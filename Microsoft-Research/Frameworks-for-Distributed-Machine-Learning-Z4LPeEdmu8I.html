<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Frameworks for Distributed Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Frameworks for Distributed Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Frameworks for Distributed Machine Learning</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Z4LPeEdmu8I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I would just like to thank CJ for
inviting me here i have to say that I'm
something my friends from Ohio
describing this morning as an outlier so
what I'm going to talk about is not
necessarily about what you've been
listening to so far but hopefully within
it all you'll get a perspective on some
new problems perhaps that you might want
to apply what you've been doing too so
most of what I'm going to talk about
revolves around the workbench whacker
but things that you probably haven't
heard of that we've we've done with it
since and so that's that's really where
the starting point is we we're here
about an hour and a half south of the
capital capital the the most populous
city in New Zealand which is Auckland ok
so the University of Waikato is just
just south of south of auckland so I'm
going to talk a little bit about
application development because
everybody should be interested in that
then a new streaming framework that
we've developed and then talk about a
project we did last year and published
in machine learning journal that you may
not have come across on experiment
databases so they they're an example of
something that hpc could certainly be
applied to if we could get hold off at
very large data sets to do similar
things with it ok so I've got various
people that I've been collaborating on
the talks in three parts I try and go
through the first part quite quickly
because I think you'll be more
interested in the second two parts so
the two guys have worked a lot with me a
dell Fletcher and Peter roiderman and
then the others on the streaming stuff
Bennett firing her Albert the fête
jesse reed and indra slow bata and then
the people from lurvin are the ones that
really got the experiment databases
databases working at hendrick block Elam
is student working
sure okay so what I'm going to be
talking about a little bit is just and
this is the faster part what application
development we've done so that the
things we've been tending to do in our
group our applications novel algorithm
development and evaluation those are the
three areas that we've mainly
concentrated on and as I'm just going to
talk about that that branch of the
application development and where it's
led us to and talk a little bit about
where I think the future is going I
think almost the last question that was
asked there was talking a little bit
around the problem you've probably got
an HBC of fusing together all the
different aspects of developing an
application not just simply
parallelizing the algorithm and then at
the application I'm going to go through
actually is is one that meets kiri wicks
Wagstaff's rather stringent challenges
that you may have come across if you
read to the icml proceedings 2012 if you
haven't it's a good paper to read on the
reason why the subjects heading off in a
direction which isn't particularly
healthy ok well it's really the focus on
accuracy as being the measure of
everything and really whether or not
that's useful ok so this is about how we
started we started by getting a grant to
develop the wacker workbench but it came
with the price of having to do
applications and so what we had to do
was to try and go and get some and so we
did a all points bulletin asking for
people to supply datasets not the best
way to in fact get data sets often they
were data sets that various
statisticians already poured over and
found nothing in and we were able to
confirm that at least that that was the
outcome for many of them what we did
determine though that was where we found
datasets involving any human input
that's where they typically went wrong
for us so data says where people had to
supply an opinion
or any some data of any kind often led
to missing values or or you know noisy
information and often partly because of
that also because it was human input the
sample sizes were too small really
typically okay and often the other
feature which is a feature common of UCI
data sets is there was already
pre-processed so you didn't actually
know what what could I have done if I
just had the original data the raw data
we did eventually though find a good
application environment this is a
testing laboratory it test soil and
plant samples for various analytical
properties of those samples so farmers
are interested in these samples they
send their soil in the results come back
telling them how much available nitrogen
is in the soil they then may apply
fertilizer to increase that so so that's
the sort of thing implants you might
want to you want to might not define
that what the nutritional content is of
some maze that you're feeding to your
stock now one of the things about that
was that there was already a
pre-existing laboratory information
system so they had ways in which they
went about doing these tests and when
they got results they fed them into this
system and eventually that system sent
out a result to a farmer to tell them
what the values of the analytes were so
an important thing for us was to try to
find a way to deploy machine learning
work inside or alongside that the
laboratory information management system
and I think that's another area really
in machine learning where we have not
really spent too much time or worried
about how we actually deploy the models
certainly systems like our and wacker as
well and many other don't really tell
you what to do once you've got a decent
model for a given data set well what I
do then
well you can take it and try and fuse it
into some software somewhere and get it
to work but that might be an extremely
hot problem you might face all sorts of
issues with the people who own the
existing code that you've got to try and
integrate with so one of the things
we've developed is a system that enables
us to deploy models in a very light way
so one of the things it's just worth
mentioning is that these people this is
the source of data that we ended up
looking at rather than human input
looking at the data you get from various
instruments that are found commonly in
these laboratories now it turns out that
there is a field a kind of intelligence
laboratory system field there's a branch
of that field called chemometrics in
which chemists play with machine
learning algorithms particularly only
one of them in order to do similar
things and that that algorithm is
partial least-squares regression I'll
get on to that in a minute okay so this
is a type of signal you typically get
this isn't this is something that comes
from an nir device now the way this
works very roughly is that the soil
comes in and what happens is it's dried
and then crushed and then it's typically
in in the past anyway what happened to
it it was it gone through a laborious
process to determine what the analyte
value for that sample what so if we're
looking at available nitrogen this might
be the value here and this took three or
four days typically so if you can find a
laboratory which has stored a lot of
past samples with associated analyte
values then what you can do is then run
those samples the same samples with the
known targets through an nir device to
be like a proxy for that wet chemistry
and then of course you can build a model
from all of these signals associate them
these
the x-values this is your y value and
then you you've built a model so for
future soil samples you just have to dry
them crush them put them through the NIR
instrument which takes fractions of a
millisecond and then you can use your
model to predict what the analyte value
is and that's the system we constructed
so having constructed that we needed
obviously we found that we could for
most of their problems produce pretty
good r-squared values I can see it as a
regression problem and so what we
developed towards this notion of a plug
on architecture if you like just so that
we didn't have to get into integrating
in with mem system and so the sorts of
scale we're talking about here are
features of around a thousand amplitude
values as you could see from that
previous signal and then numbers of
spectra of around 5,000 and in this case
minimal pre-processing the previous
spectrum that you saw had some smoothing
glide called Savitsky goal a smoothing
its first derivative stuff that we then
work done to another instrument called a
good gas chromatography-mass
spectrometry and there the feature space
explodes on you significantly and you
into one of these not sparse problems
very dense problems where m is very
large and then is is relatively small
and there you need significant
pre-processing so what we did to kind of
come up with a principled way of
developing these things was to develop
the system called atoms I keep wanting
to press them and adams is a tree-based
workflow engine it's a programming
paradigm allows you to construct
workflows which solve problems but also
include elements that allow you to
deploy models as well and as as you you
program it as the last speaker said
beauty of program of course as it leaves
a record of
you did whereas lot of machine learning
experiments don't of course unless
you've you're very disciplined in in
your file system so it's a nice way of
remembering what you did in order to
come up with a result and then we've
along the way integrated it with various
other systems so this is what it looks
like it allows you to take a flow here
is a data set which gets basically
passed to three different pre processing
methods this is just do nothing here is
first derivative and then here's a
wavelet transform and then once all of
that's been done then there's a section
down here which listens for the results
of this earlier stage collects the three
datasets and then kind of integrates
with Weka to produce an arf file and so
on optimizes the classifier within this
element here does a cross-validation and
tells you which of those three pre
processing stages was the optimal one
for the given for the algorithm you've
got here ok so it's an it's a way of
doing that so here you of course we're
not deploying anything this is our
standard what's the best thing to do in
this circuit what's the best thing for
this data set here we can now deploy the
thing in a in a workflow so we can
define a workflow but basically
generates a system if you like so here
loop forever wait you you wait for files
to come in your process of using that
previously discovered technique and then
you know you check that their samples
correct and email the results if they're
not this was a problem that the
laboratory we work with had in that
samples were being swapped and if you
swap a sample all samples look very very
similar and so if the label on it says
it's a grass but it's a piece of
days you're going to get really
different results and then the farm is
not going to believe in what you you
know because you're going to run it
through a model for grass when it's a
really amazing and vice versa and if you
get into that situation and no one
believes your work so this was a very
nice way of testing you know kind of
providing quality assurance okay so the
system looks a lot like nine it's got a
different philosophy and then it's a
graph-based more of a programming you
know sort of philosophy then nines
layout and hook up sort of thing and
then it's got us with nine but it's hard
actually to find out with nine whether
what its deployment methods are anyway
that's what we've done so that system
has been in place since 2006 it's been
responsible for over 30 million
predictions so it's being used both in
Europe and in New Zealand the good thing
is that only we do have checks in the
system to make sure that if numbers are
outside what what's normally expected
then we send it for the back to or wet
chemistry to do it in the old way as it
were and as you can see a very small
percentage gets done that way so it
actually meets number two which was
curious challenge number two which is
100 million dollars saved through
improved decision-making so it is a
significant project and as I said you
very rarely hear them yeah
I can't hope Oh ranking what ranking
them relative to
yeah that's the best because we're not
so it's like if you've got a point
estimate right and and you've got a
tolerance on that and it's beyond that
tolerance it's beyond that confidence
interval then you would just send it
back and say do that one the old way
don't trust the nir in this case so nir
can be vulnerable to things like sand if
there's elements of sand in a sample the
reflectance isn't as you know it sends
out outlier numbers that we haven't seen
before so we don't just blindly predict
everything
yeah we we actually only have a very
crude if it's beyond our confidence then
everything goes to wet chemistry yeah
it's an area actually where there's a
lot of accreditation and so what you
have to have is a sin is a system that
gets accredited by a body that does that
sort of thing and say I want anything
complicated one of those industrial
realities
that's over that period of time right so
it's 30 million so I don't have any that
would be a day that's 30 million over 78
years
hmm
no no what are several several and they
dotted throughout Europe so what what
they have is a system where they collect
spectra and send it through so yeah so
so it's not caused any problems so far
we aren't well with we're not quite at
the HPC you know level but it's not far
off yeah okay so there'll be something
at the end which is a royal for anyone
wishing to download that software and
play with it so if you're interested in
developing applications seriously and
you do a lot of work in that area it's a
good system ok the next thing I want to
talk about is something we developed for
streaming and it's a framework like Weka
for streaming data and we call it
massive online analysis just because
that name gave us another bird so
whereas the previous bird was flightless
this one is it was flightless and now
extinct and I think you can see by just
looking at the size of that leg there
that would that give you a clue as to
why it's extinct like this guy so
Richard Owen it just gives you an idea
of the height of the things Oh almost
twice asked a statting a normal height
and the remarkable thing about this is
that this used to be preyed upon by an
eagle and that's a true story so imagine
the size of that eagle okay so that's
what Mara is it's it's related to Weka
but it's it's not definitely not Weka
one of the criticisms we fielded over
many years which now could possibly be
answered a little better was what do I
do with very large data set that won't
fit in main memory and so our answer to
that was to head off in a slightly
different direction really inspired I
think by a paper by domingos and Halton
in around two thousand
on basically an incremental algorithm
for decision trees which was if it's a
paper I'd recommend you read excellent
paper anyway so what we've done is do a
similar thing collect together a bunch
of algorithms in a framework where you
can evaluate them much like wecker and
we've got various branches I'm going to
mainly speak about classification here
but there's also the ability to do
synchronous clustering of streams so
that you can view how two different
algorithms are clustering a stream in
real time so it's it's got some nice
features but it's a relatively new
venture ok I don't really need to tell
you why we need it in my view there are
two reasons one particularly is the kind
of idea of sensors and collecting data
that certainly been something that's
going to lead us to ever more needs to
do these sorts of things but the other
thing is that most people out there in
the world believe that they can predict
something from that data and that that
will be useful I mean that's somewhat
contrary to a lot of evidence actually
when when a lot of people ask me when
people talk to me about big data I
always ask what's the big question
because a lot of people get excited
about the collection but not so
interested really in what it is that
they're trying to find out ok so this is
the setting there are four points that
are worth bearing in mind especially if
you're thinking of hpc solutions for
this training examples arrive all the
time you've only got finite memory you
have to accept that the concept that
you're trying to learn will change so
you don't have iid data and you must be
able to predict at any time so those are
the constraints that you're faced with
with developing your algorithm
the evaluation procedures are to either
do this for a while and then hold out
some data and then test your model which
is kind of standard or a method called
quench evaluation where for every
example that comes in you immediately
predict it and then you use it for
training what happens if you do a study
of these two methods is you find that
this one is more conservative than that
one so you find that you get basically
the same learning curve but there's a
margin between them it's a certainly
smoother one because of the fact that
you're doing it very very incrementally
what this encourages of course is
development of incremental algorithms
and it's the kind of alternate approach
in a sense to do taking what we
currently have and making them parallel
in some ways and it's a slightly
different paradigm so many and this is
the area I don't know a great deal about
so I'm not an HBC person so this is all
that's happened with moest and it's been
a very brief introduction and go at
trying to do something by distributing
in a producing distributed string
processing with mark what we've
discovered is just looking around that
people have tried to take Hadoop and
produce online versions of it so there
are a few systems around that do that
but that seems tricky in this
environment because of the fact that
most of the current methods seem to take
the data and then distribute portions of
it to the nodes whereas we can't we
can't really do that because we have to
take instances at a time and do
something with them so what we've looked
at more recently is using and this is
the only word I noticed was on the
previous slide that's also online which
is quite good on the previous talk so if
you look at what the
big companies are doing who are doing
streaming then Twitter and Yahoo are
both employing these systems rather than
how do so what's happened at Yahoo
Barcelona where Albert Pathet was until
recently is they've developed on top of
s4 but also storm they're trying to
develop an environment which allows you
to plug any stream processing
environment into it this system called
salma and that allows you to do some
form of distributed processing with
streams young Yahoo labs Barcelona a guy
called Francesca yeah I'll give you the
name I can't remember is now I haven't
had anything to do with this project
anyway that's their first attempt now as
I said on the previous slide instead of
seems to me it's this can be a hard
problem though because what you have to
do with the algorithms in this system is
paralyzed the algorithms themselves and
that in itself is going to be very very
difficult ok so this is a kind of model
of where everything is out in terms of
streaming and non streaming this is
where that one's currently sitting so
this system is an architecture that
allows you to plug in both stream
processing environments and also up here
generally machine learning environments
so what we've done so far or what the
people at Barcelona have done so far is
they've implemented something called
vertical decision trees plus bagging and
boosting and then there's there's one
implementation of a frequent pattern
mining algorithm and also k-means
clustering so it's very early days but
what I can see is going to be a hard
road I'm not sure this is the answer
from what I've read of it so
but anyway i'll give you the URL at the
end if you want to have a look at what's
been done
yeah so so essentially this is MOA here
yeah but you can't just use it directly
so everything we've got in my won't work
you can use it you can use it but it
won't be a parallel implementation in
other words to get those you have to
implement them yourself that's why I'm
saying that looks like hard work to me
here is the range of it's not all of
them but these are the things we've
implemented so far these are the stream
algorithms we have inside lower so we've
got the Pegasus system that was talked
about previously so that that's been
something that's come up here there's a
range of ensemble techniques and
basically variants and adaptations
improvements on the original very fast
decision tree algorithm of domingos and
halton so that's this here one of the
method
yeah ah yeah yeah yeah yeah it's not
there's another there's another branch
right cool you know things like winnow
that it's not that because that's
strictly iid the these are algorithms
that have to take into account the fact
that the concept may change so some of
them mostly so what happens is that in
in these trees here you get finite
memory and you have to handle it so your
algorithm has to make decisions on
cutting branches or you know making room
so so as you go you've got to adapt yeah
well they're not easy to just to specify
I'll show you some results later and
you'll see your season Abby
yeah yeah yeah yeah yeah yeah they're
not yeah they're not expensive no okay
and then one of the things you'll see
later is that you accumulate sufficient
statistics in every node of these trees
so you get a naive Bayes model for free
because you have to because otherwise
you can't calculate information gain so
one of the things that's being done is
to not just use majority class but to
create naive Bayes leaves sometimes also
an adaptive strategy of working out when
it's best to use this one over that one
has also been employed now if you've got
a method that doesn't adapt to the
concept drift you can employ these
things drift detection mechanisms and so
some of those have been implemented in
the mid the ones that have been reported
in the literature the main ones they're
in there as well okay now here's one of
the things that you yeah
yeah yeah yeah yeah yeah all this works
in exactly that way you look at it you
do something with it throw it away get
the next one predict it yeah yeah yeah
yeah so usually there's a paper by Ikey
hula meyer on k-nearest neighbor I'll
show you some results on that later of
course you can just use the sliding
window and and standard k-nearest
neighbor right and that works really
well so that's a bit of a challenge i'll
talk about that later okay so here's
another challenge i mentioned this the
data is not I ID now so that means it's
not identical because of the concept
range but it's also not independent and
this is what this study showed that we
did recently this is the a data set
that's been used very commonly in the
field it's not typically it's not very
big which is another problem there
aren't many publicly available vast data
sets to work on anyway this one is an
Australian electricity data set it
predicts the prices whether they'll be
up or down using a 24-hour previous
sliding window anyway you can see the
majority class bumbling along there and
these algorithms the very fast decision
tree and naive Bayes are kind of keeping
above that what we did is develop
something we called the magic classifier
and as you can see you know we'd pretty
much out did all of that work then we
had to look at the literature between
2004 and 13 anything that's been
published using this data set in either
a journal or a conference and we found
that we Came whatever it is seventh or
something
which isn't bad really given that our
classifier is to predict the label that
we previously saw so this is a
classifier it's magic because it makes
no use of the feature space at all it
just uses the previous label and says
that's what the next one will be this is
we really started by calling it the
weather classifier because that's what
it's the still the best classifier for
the weather right you just look out the
window say tomorrow is going to be sunny
in Mysore so like what that talks speaks
to really is the fact that accuracy is
not not the best measure to use in these
sort of situations and so we turn to
well something call it there's something
called Cohen statistic or the kappa
statistic which sir measures the overall
accuracy but it normalizes it by the
accuracy you get by chance that's sir
typically what the cup of statistic is
and so what we did was just substitute
that that chance element majority class
with the no change classifier and then
what you find just looking at algorithms
when you use that as the measure as you
can see that some of them certainly for
periods are doing better than the magic
classifier here and then they'd lose the
plot so what you can of course do is
just create a wrapper which makes use of
the fact that the previous label is
available to you and then include that
in the feature space with with X and
then n those algorithms improve there
are a bit slow at times to react to the
fact that you're going through a period
of no change but nevertheless they still
improve I'm going to have to sort of
speed up a bit now just to give you a
picture of where this field is at in
2012 we did and this is probably
something you're all sitting there
thinking well I could develop one of
these instance incremental methods you
know like I've described that's
available in Weka or I could simply just
collect a batch of training example
balls and a sit on the side of the
stream and then from time to time build
my model and deploy that and then just
keep it predicting y collect the latest
samples and build a new model then have
some way of swapping the models or
whatever okay and that that way I can
make use of all the recent developments
in machine learning because I can use a
standard machine learning algorithm okay
that's an obvious thing to think about
anyway what we did was we did the first
study of comparing these two things
together and this is another so I just
quickly go through this so we use these
instance incremental methods which is a
range of you know basically the best
things we had in in MA against something
where we could use wacker and some of
the well you know these aren't really
state-of-the-art well this one of course
is but the others aren't anymore but we
can use that inner in a wrapper using
this average weight accuracy weighted
ensemble and this just simply replaces
the oldest model with the newest one
built on the latest batch so what
happened well just in summary if you
just look at the average rank down here
you can see this is number one but it's
a bit funny because it get or did not
finish on this larger data set here and
this is leveraging bagging using
k-nearest neighbors what you can see is
as I mentioned to the question before
his k-nearest neighbors and it's
basically rank 3 so it's on average a
pretty good classifier in this situation
and that's because it always keeps up
with the changes because of the sliding
window there have been some papers
improving what you can do in a streaming
environment given that you're going to
implement us you know such a situation
you don't necessarily have to take the
most recent one thousand samples you can
do clever things knowing that you've got
a thousand
data structures as well to make that
more efficient so this is a very
promising algorithm this is a our own
ensemble technique using hoof ting trees
and that did pretty well you know using
either huffing trees or k-nearest
neighbor but these are the sorts of
accuracies you can get sorry how you can
try yeah how you build the trees okay
online setting how you build them okay
so what you do is you have a root node
you accumulate statistics and then you
make a decision at some time once you've
accumulated a number of whether you're
going to split on on now this is where
domingos came up with the brilliant idea
so what he does is he uses the
difference in the so you compute the
information game and then you find the
best the best split bit node a split
feature and the second best then you
work out the difference between those
and you treat those as a random variable
and you use a Chernoff bound or that's
what it is but they could it have thing
bound to determine whether you're safe
you know that you'll have no regret
about splitting over that gap that that
gap will still be the same in ten
thousand a million samples time yeah I
want or dividend or mentor yeah so
they're all doing
here and so for the online setting
when
so then the FBI
yeah yeah yeah so you keep filtering
every new thing that comes in you filter
to new nodes and then you wait till
they're ready and then you expand again
yeah oh yeah yeah
I yeah yeah
2000 here Holton and domingos yeah it's
actually a methodology you can use any
algorithms they've got a framework for
yeah yeah yeah yeah no no they just
wrecked sorry they just into ranks on
where they where each of these eight
nine methods came on each one so this is
the average rank over that yeah okay so
here's the what you might conclude from
these experiments in assoc is only the
first one if you're unsure then this is
still a very safe bet this was an
interesting result you know because we
use very fixed parameters for this that
was a question earlier so we used we did
some sort of experimentation to find
good parameters then just fix them so
1,000 instances was the size of the
sliding window we use and that work
pretty well across all those problems
for instance incremental it's vital that
you find the concept drifter you make
errors and don't notice it methods
themselves are very slow to react and
that that's a problem and ensembles are
better as they are in the stand standard
setting but you need resources for them
one of the things we've been looking at
trying to develop is well in my view a
better way of analyzing algorithms
that's in terms of their efficiency
rather than just their accuracy so we've
been looking at how much memory time
does it take to achieve accuracy okay so
if you want to look at the papers that
was the magic one and then the state of
the art was in 2012 okay the final part
I think I've just got enough time to go
through this it's on this idea of
experiment date
basis so as you know what you do is you
have a bright idea you collect together
some data sets you may or may not do
some pre-processing you do some
evaluation you get some results you have
a look see if you're better than the
rest and then you try and send the paper
off and of course what happens is that
along the way some of your best ideas
end up here because they don't tend to
look that good and then others and find
their ways into the icy MLS their nips
the KD DS and so forth around the world
but of course nothing happens to them
after that so there is a sense in which
all our results end up here right
because none of us typically plow
through proceedings and then copy them
out that you know the results of running
see 4.5 on the mushroom data set or
whatever we repeat the experiment
endlessly so how many times is everybody
run those standard experiments
repeatedly so if we're interested in
cutting down the amount of computing we
do in machine learning we shouldn't keep
doing this cycle we should find some way
of just looking up what the best score
is or what's you know I'll just get my
data sets and get the scores that
everyone else can currently get on them
and then compare my new method on those
data sets that's the way it should work
so people have pointed this out in
various papers about how we can't really
reuse anything that we've been doing and
not the best way of doing science so
what we want I to do is to find a way of
just like they do in in number theory
and in analysis of perhaps having ways
in which we could generate hypotheses
that we could then produce some theory
for because we can see it in data so
that's one thing you get from having
this kind of unified shared model of
you don't get the you know your save
time and energy reproducing the same
benchmarks you might find that all the
tools that we have will integrate better
because they have to kind of interface
to this standard thing and then we might
be able to look into looking at times at
which the algorithm that we consider to
be the best may well not be the best
what conditions what happens when it
scales and so forth those questions were
being asked yesterday okay so these are
all good reasons for having this sort of
thing it certainly helps to have your
algorithms visible in searches and
perhaps going back to the negative
results thing you might not repeat the
mistakes of others okay so what we did
or particularly what wacom did was to
develop an ontology 4x4 our experiments
and then produce a markup language which
described that in order to basically
provide some way of mapping from a range
of experiments produced in these
toolboxes through to an experiment
database that was the idea once we have
the experiment database and we can do
these these sorts of queries of it and
we can also look perhaps at doing some
meta learning of the things that it
contains all right so this is what the
markup language looks like this is what
the database actually looked like it's a
uneasy thing to come to terms with the
terms of query but what we did was we
produce this big experiment so we took
these are all wecker experiments 650,000
experiments 54 our rhythms that many
data sets that many evaluation measures
blah blah blah and this is the sort of
thing we had a look at so from a metal
learning perspective what you're
interested typically is in how an
algorithm performs when it you know
which kind of data does expect it to
behave this kind of when questioned and
then the why question why is now grow
them behaving in a certain way I'll just
show you some of the things
we looked at me so here's the letter
data set for example and these are all
of the 54 methods and where they how
they perform once you've looked at
parameter of optimization and so forth
so you can see that here we have Platts
version remember this is the dual of the
primal here the RBF but of course it
varies a lot so as you if you've ever
played with an RBF kernel you'll know
that those two parameters are tricky in
it certainly the default value that you
often use is not the best one and you
have to explore the parameter space
that's what this tells you here with
these variants what's interesting here
is look at this random forest has got
some variation as well with relatively
parameter free method so that might be
worth exploring and we looked at that
later okay so it down here are the
stumps which is what you kind of expect
that's the sort of this is just one data
set however now you might believe that
the colonel width just going back to the
RBF question if you look at different
data sets and the learning curves when
you set the colonel width you get these
kind of curves coming out so basically
the arviat the sport vector machine sort
of gives up at a certain point for
certain data sets and and results in the
majority class so if you look at these
there's this one here and this one here
and these are these two you might
hypothesize that it's due to the fact
that these are larger have a larger
feature space than these two which have
more similar curves you know just sort
of slowly getting better oh well this
turns out not to be true we haven't
found the reason why this happens
yeah yeah yeah yeah they are scaled yes
yes they asked out there'd be scaled in
weka okay one of the things we did just
running through this rather too quickly
but was to repeat previous studies which
show pretty much what this what would I
find interesting on this one is that
pretty much these are different measures
here and they also roughly the same
pattern and basically confirms what we
know their ensemble techniques tend to
dominate and we had to look at the free
lunch theorem no free lunch theorem to
see you know how close can you get with
this it's a bit misleading but anyway
with bagging and boosting and
potentially with support vector machines
as well you know you can get down this
is to point something which is which is
a pretty low rank really so the question
is if I'm if I've got any data set what
should i do first well you know by and
large this might well be the best thing
to do first so here's that question that
I raised earlier how come a parameter
relatively parameter free method like
random forests can produce such
variation in its performance so we had
to look at whether data size had an
impact on that and you can kind of see
that that's true generally down here
when the data set sizes are small you
get lots of variation you know enlarge
in the large trees specifically that's
the sort of redder it gets here they
dominant they're better than using small
trees generally but there's a lot of
variation whereas over here when they
get larger that's still better to use
larger trees but there's less variation
the only anomaly is this one which is
there one of the monks problems is very
strange because the single tree does
better than all very rich you know
obviously you can't see that in anything
else that's a single tree outperforming
an illness in inverse order more trees
you use the worst you get
if you want to read that's the monks to
problem i can i can tell you which one
it is there's an explanation in there ok
here's various learning algorithms on
the letter dataset the thing i like
about this the most is is here so if you
just look at logistic regression which
is this one and then you look at sea 4.5
and you know here we've got a learning
curve cross at this point this was the
inspiration for our method called
logistic model trees so what they did
was they took a standard decision tree
and put logistic regression leaves on to
that tree with the because and it tells
you why you should do that here because
when the data sets is small this is
better than j48 right so if you've got
and this is what's going to happen at
the leads because you're going to get
smaller data sets at the lead so it
makes sense to combine these two methods
just because of this area here and the
fact that generally this one is going to
be better than this one that paper on
logistic model trees got the impact
10-year impact best paper at ECML this
year but that was the observation that
was the insight they use just that
little thing there five more minutes
okay
I think but I would also time as one of
the dimension yeah because many times
you are quoted by trying on to it yeah
yeah yeah they're they're in there yes
but they're not used here yeah yeah
they're in there yeah i'll give you the
you are at the end you can do the you
can have a look yourself yeah yeah yeah
it's all kind of published in that sense
here you so here was an a question
asking when does one algorithm perform
better than another so here you can
actually perform a find a decision tree
this goes right back to halti i don't if
you're old enough to have remembered
this paper but there was a paper in 1993
which was basically criticizing the UCI
data sets at the time because this
algorithm which halti developed just
used a single attribute for prediction
often did better than this method so and
if you look at the data sets over time
you see that after hall tease paper data
sets with greater number of classes
started to emerge in the UCI repository
and when you've got greater than five
classes you always get a big win for a
seat for c 4.5 so that's another thing
you can do with this finally we have the
another study that we this is this is
the question of finding you know why
does an algorithm behave in it in in the
way it behaves and this study was really
to look at the percentage of bias error
of the total error in for for a given
problem and its effect on data set side
so as the data set size increases what
happens to bias error so you can see
that when day the data set is small it's
all over the place basically but it all
smoothed out and the bias error tends to
increase and so what these two guys were
arguing back in 2002
that if you've got large data sets then
you want algorithms that have good bias
management and so you know you put these
three in that category it's not yeah I
don't know what the yeah so a trees tend
to do better in the long in the long
term that's what this study is it was
was kind of concluding but for the large
data sets not sure this is entirely true
SVM's don't look too good on this but
but generally they still perform very
well on large datasets I'm not quite
sure what this is whether it's an RBF or
you know again depends on the domain I
mean per text using a linear kernel is
very very good and the data sets can be
as large as you like oh sure yeah so
yeah what might also get is that if you
if you increase a regulator on the
philosophies on the lock me inside you
got a tiny go buy it hi Gary it's
 fire yeah and then yeah that's
near like
yeah no no well what what would be
really nice would be a study just on SPM
oh okay excellent okay so what I've told
you about is a framework for developing
applications a framework for
implementing and testing streaming
algorithms and then finally a look at
this experimental data experiment
databases for encouraging more open
science I guess as with the previous
speaker I'd like to conclude with the
same message but add also this one that
the field would be so much better if we
had more especially more bigger data
that would be my message that's the
machine learning paper and there's the
URLs of the three systems that I spoke
about this is the new brand new thing
this year so it's very wobbly but that's
the thing that attempts to do a
distributed stream processing okay and
this is the true map of the world we're
here
okay thank you very much Morales yeah
please visit a community like people try
to move are so common language and see
all of the a high levels exhale wake up
are they they just want to have because
good is it bigger what the yeah yeah
well that's a good question it'd be nice
to think that that could happen the only
thing I've seen since 2006 which has
been the best thing that's happened is
the branch of jml are and particularly
the am lost site where people have put
all of their implementations and I guess
that's where it rests at the moment I
don't know of anyone working on a common
language as you can see even with the
previous speaker someone mentioned C++ I
mean I find that the temptation to get
into religious war over language is just
amazing amongst people so I find it hard
to believe we would ever come up with a
common language to be honest but it but
that my site is fantastic but the other
thing is I'm an editor on there on that
branch and I'd really encourage you to
try to publish in the journal and
obviously make your algorithms available
yeah yeah
yeah so so for example what isn't a data
instance ask yourself that question what
is an example in machine learning well
it's hard to come up with an abstraction
that covers everything that currently is
and everything that it might be because
structured learning is coming big time
you know we're going to have structured
in we have some structured input
structured output yeah and then you end
up defining you know like I don't know
it XML input and output and then it just
kind of comes anything anything and
nothing so it's a pretty tricky thing
that's just on the data
I
now
you might wanna so good how do you feel
awfully
we
you
you can have it
these experiments per se
no no we we reran the more yeah yeah but
that would be the best thing the best
thing would be yeah yeah and that's what
it's all about and that you know the
community should just chuck it at that
and just build it up there so this yeah
well it's been anything that's publicly
available plus you could have a look at
that paper to see what the scales are we
set for mower in particular 1 million
instances as the lower bound because
that's what we felt at the time was
necessary but like I'm a fan of doing
machine learning on streaming video I'd
love to get to that point and really for
that so it's the application I had in
mind was soccer where you track every
player on separate streams and then you
compute their statistics but plus the
ball and then you compute their
statistics on you know just
automatically on how many times they
touch the ball how many times they
passed it to someone on their own team
and all these sorts of things that would
be a nice application but don't know how
you could do it now I mean there are
obviously more industrial applications
you have to think of cricket because the
money is
ready
it's not multipass we've looked at the
order issue so we've and we've thought
about the whole experimentation thing in
terms of that you know what happens are
they are the trees stable when you
change the order and generally they tend
to be yeah so yeah we have looked at
that but that is a good point</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>