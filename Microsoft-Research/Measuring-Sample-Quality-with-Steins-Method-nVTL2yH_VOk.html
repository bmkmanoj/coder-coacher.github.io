<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Measuring Sample Quality with Stein's Method | Coder Coacher - Coaching Coders</title><meta content="Measuring Sample Quality with Stein's Method - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Measuring Sample Quality with Stein's Method</b></h2><h5 class="post__date">2018-04-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nVTL2yH_VOk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this is joint work with my former
graduate student Jackson Gorham and when
I thought I would do today is present
some tools that we've been developing
for measuring how well a given sample
approximates a given target distribution
so the motivation for this this whole
project is coming from large-scale
posterior inference and particularly
this question of how do we scale Markov
chain Monte Carlo algorithms two massive
datasets so the benefit of Markov chain
Monte Carlo or MCMC is typically that
you can approximate intractable
posterior expectations like these
potentially high dimensional integrals
in red here with asymptotically exact
sample sequences so just sample averages
like these in blue and one drawback of
MCMC is that typically every new sample
point these X is that you need to draw
require that you iterate through your
entire data set so this can be very
expensive if your data set is large okay
so what can we do about this well this
template solution has been emerging over
the past I don't know half decade and
the idea is you can approximate MCMC
with what I'll call subset posteriors so
you take a typical Markov chain and you
modify it so that on every step you only
look at a subset of the data and the
benefit is that now you can sample more
quickly so it reduces the variance of
your Monte Carlo estimate so that's
great
but the drawback is that you're now
introducing this asymptotic bias even if
you run this chain forever it may never
converge to your target distribution so
the hope is that overall for a fixed
amount of sampling time which is all you
have in practice that the reduction in
variance will actually outweigh the bias
that's introduced okay so I think this
is very promising but it introduces a
lot of new challenges so in particular
how do we actually compare and evaluate
the samples that are being generated by
these approximate MCMC procedures how do
we select the samplers how do we pick
our favorite one yeah example of the
approximate procedure or example of just
most area Markov chain was the Markov
chain in gold
okay yeah
examples so simplest example that still
has this problem it would be Bayesian
logistic regression so you have a
logistic regression problem you have a
data set of kobarid vectors with
associated labels you you posit a model
which is that you know each label is
generated according to a logistic you
know Bernoulli distribution with their
just logistic mean you don't know the
parameters you put a prior distribution
on the parameter vector and now you're
interested in the posterior distribution
of that parameter vector so you might
run something like a good example would
be stochastic gradient Lanterman
dynamics so this is kind of like what's
called the metropolis adjusted London
algorithm where on every step to draw
you're doing a random search through
space so your vector lives in R to the D
you start by picking a random point in R
to the T so anyway conditionals
initialize it in any way and to choose
the next point you take a step a
gradient step with respect to your log
density the posterior the posterior
density that you want a sample from and
then you perturb it with some Gaussian
noise and usually that would become a
proposal in something like metropolis
Hastings correction but petrópolis
Hastings Corrections are expensive and
even just computing the gradient of your
log density is expensive and in each
case you have to run through your entire
data set to just compute that thing and
so what people have been doing instead
is using a stochastic gradient so
they're replacing this grad log P you
can write that on the board so here's
your prior PI of theta theta is the
unknown parameter
hiya Thetas your prior you have a series
of data points why I so we have a
likelihood function given the given the
parameter vectors this is your hood for
each data point and you have a posterior
distribution which I'll call P of theta
which is proportional to the prior times
the product of all the data points
you've observed so I equals 1 to M of
the likelihoods and a Markov chain Monte
Carlo algorithm would be start with a
given point let's call it theta 0 and on
every step you proposed a new candidate
point so that would be theta I plus 1
and theta I plus 1 is going to be theta
I plus grad log P theta I plus sum step
size so I'm some Gaussian vector of just
call Zi that's called D this is a
particular Markov chain Monte Carlo
algorithm it um it's called the
unadjusted Lanterman algorithm it's
expensive in particular because just
computing this grad log P in this case
requires it you know touching every one
of the data points in your data set and
so the way that it's often approximated
is to use a stochastic estimate of this
gradient much like much like we do in
optimization so you just choose a random
subset of your data you rescale the
gradient appropriately and you plug that
in Z in this case is going to be a
Gaussian vector will say standard
Gaussian so identity covariance the goal
is the sample from the sphere in
particular the goal is to approximate
expectations on the posterior so you're
joining the sequence of points with the
hope that the sample estimates the
sample averages will approximate
actual expectations
are there other questions about this
example yeah the unknown parameter is in
a high dimensional space and why it's
also a high dimensional space and so
this isn't thinking this isn't parts of
the deep okay-y could be anything but
yeah I haven't haven't specified exactly
what the likelihood is but it could be
it could really be anything so in the
logistic regression example you could
think of hyi as actually a pair of two
things let's call them I don't know WI
and VI and each WI is again something in
our to the DS with this might be a
covariant vector this is a vector of
features and this is a binary label this
is plus or minus one and essentially
what you're doing is finding the
parameters that underlie a
classification model should say I did
have an example in here but I was told
to take out as much motivation as
possible actually give so yeah you still
want to think of it in context for
example so yeah try to classify what
yeah okay great
so
this
right so here's um here's a typical
vision logistic regression setup
trying to do time to do what for what is
the actual yeah um yeah so I'll try to
detail that so this is the setup is
Beijing logistic regressions you have a
data set is your data they are wi-vi
pairs inch WI as a feature vector in r2
the D each VI is a label that's a binary
class label
we're gonna posit a model for how this
how the class tables were distributed
given given the feature vector so in
particular the expectation of V I will
say in this case is or what's just a V
is Bernoulli with mean given by the
logistic function of theta comma WI
so so far as a standard logistic
regression problem and now you don't
know the parameters so you put a prior
on the parameters let's just say it's
coming as distributed according to
normal
zero identity this is a part of the
parameters and now I'm just interested
in the posterior distribution of theta
given all the data I've observed and I
want to compute say the posterior mean
the posterior variance and posterior
predictive probability is like the
probability of Y given a new exit walks
on the door okay even in these Kievan in
this pretty simple setup we don't know
how to compute any of those expectations
in closed form and so what we usually do
is try to draw sample points from this
posterior distribution such that that's
those sample points when averaged
together will give us expectations that
converge to the true expectation so I'd
like to know what is the se the
expectation of theta theta transpose
given the whole data set that I've seen
so keep in mind one v1 up through WM VM
I don't know how to compute that so I'm
going to approximate that with sampled
with a sample average so I'm gonna run a
Markov chain usually you'd run a Markov
chain that actually is converging to
this the posterior distribution so some
of my equals 1 to N and I'll have
samples of my of my data vector and I'll
use that as an approximation to this
expectation
and that's the typical way in which from
otic Markov chain Monte Carlo is used
and that's fine and it's particularly
fine when your dataset is not that large
but most Markov chain Monte Carlo
algorithms at least require iterating
through your entire data set to draw
every new sample point because you're
conditioning on your entire data set
just to you know reason about this
posterior distribution and so of late
people have been exploring other
algorithms that try to get away from
that that expensive operation and an
example of an algorithm so this would be
an example of a more standard Markov
chain Monte Carlo algorithm although
this well this isn't isn't exactly
correct but this is called be whatever
written down here this operation of
choosing the next sample point is called
the unadjusted Langevin algorithm
and if instead of using this actual
gradient I replace it with a stochastic
gradient that only uses a subset of the
data points on each on each step that's
what's called stochastic stochastic
gradient Langevin dynamics and if you
run it with a fixed step size say even
if you run it forever your expectations
will never converge to the true
expectations there'll be this asymptotic
bias that's left around it's related to
the step size here you think of the
gradient and then screw this over to in
front of you gosh yes sir yes yeah
yeah on abilities both yeah that's right
okay just for people who are used to to
present Santos to guessing great in the
sense the difference is that here the
parents of the noises of the order of
the steps yeah that's right that's right
designers let's thank you that's been
said keep some variability here she is
and sometimes you want a sample from the
whole distribution and not just go to
the mode
okay we'll tell us so tell me is that as
a setup clear clearer yeah good okay so
this is so the stochastic gradient
legend and dynamics that's one example
of one of these approximate MCMC
algorithms their variety of others have
been proposed many of which take a
standard MCMC algorithm that is designed
to converge to a target distribution and
just impair it in some way and if some
way that makes it asymptotically biased
but much faster to run and so now you
have this whole zoo of algorithms and
there's this question which one should I
use which one is actually producing good
inferences for me and how do i tune the
various hyper parameters that are
associated so in the case of stochastic
gradient London and then I mix that
hyper parameter might be the step size
and other algorithms there typically
there's typically some trade-off
parameter that trades off between the
bias that's introduced and the the speed
of the algorithm and so we need to set
these in practice and one question is
how do we do that I guess a bit more
generally you might ask how do we
actually quantify this bias variance
trade-off that I've been highlighting
intuitively how do you how do you
quantify that explicitly so there are
actually standard criteria for
evaluating Markov chain quality and
convergence these some examples that you
may have heard of are effective sample
size trace plots asymptotic variance
Diagnostics these are also can be quite
useful for MCMC but they also all assume
that your Markov chain is actually
converging to your target distribution
so they ignore any sort of bias that
could be introduced by the chain itself
and that's the problem here in our
setting so what I want to introduce are
some new quality measures that are
suitable for comparing the quality of
different approximate MCMC samples
okay so here's gonna be this is going to
be a challenge develop a quality measure
suitable for comparing the quality of
any two samples that are designed to
approximate the same target distribution
and this will be our setup so we're
gonna work with continuous target
distributions I'll call I'll call the
target P and we'll have a density little
P for now let's just say the support is
all of our tivity like in our logistic
regression example and P will be known
up to a normal a normalizing constant
but we don't typically don't know that
normalizing constant in particular we
don't know how to integrate under P for
most functions of interest so what are
we going to do we're going to
approximate the target distribution P
with a sample so if you give me a series
of sample points x1 through xn in my
space X then these define a discrete
distribution which is just the empirical
distribution over those points so let's
call that QN and associated with QN is a
sample average and we're gonna use that
sample average to approximate
expectations under P and I should
highlight at this point that I'm not
actually going to make any particular
assumption about where the excise came
from so you could have generated them
from a Markov chain
you could have generated them
deterministically if you're doing a
quadrature rule or yeah typical Monte
Carlo we're not gonna make any
assumptions about that and our goal will
be to quantify how well these sample
average expectations approximate
expectations under P we have a few does
it errata so we want this procedure this
quality measure to detect when your
sample sequence actually is converging
to the target distribution and we want
to detect when the sample sequence is
not converging to the target
distribution yeah predefined classes and
in other words to you ativ fools for
engine yeah yeah so I'm coming to that
oh yeah we need some notion of what what
conversions do we care about
it'll come to that and then we also want
something that's computationally
feasible so you can actually go up and
use this when you're running your
approximate MCMC algorithms okay so
given that we care about approximating
expectations I think a natural metric
for measuring the quality of a sample is
an integral probability metric so this
is a it's a metric that measures the
maximum discrepancy between your sample
and your target expectations over a
class of test functions and when your
class of test functions is very large
convergence of this IPM is integral
probability metric which i'll call d h
to 0 implies that your sample sequence
actually is converging to your target
distribution weekly okay so some
examples of IPM that you may be familiar
with our total variation distance that
would be using test functions that are 1
bounded by 15 distances the one barstein
distance falls to this class for your
test functions are 1 Lipschitz the
Dudley metric or the bounded Lipschitz
metric is another example and a number
of these have a problem so a typical
problem that you'll have with standard
into IPM is that you don't know how to
compute them in particular the
definition of the IPM involves
integration under PE and that's exactly
what I don't know how to do and so in
practice I can't compute these water
steam distances or bounded Lipschitz
metrics so what can we do well one idea
is to only consider test functions that
we know a priori are mean zero under the
target so if we could do that then this
EP term and the IPM definition would
just drop out and we only have a have a
measure that depends on my sample so
potentially that's tractable to work
with so that seems like a reasonable
idea you might ask you know how do we
actually find such a class of test
functions and even if we had that would
it actually track sample sequence
conversions in the way that we wanted to
and even if it did how do we compute
this quantity it still involves a
supreme over
all the test functions in your class so
we're gonna try to answer all these
questions using some ideas from Stein's
method so Stein's method due to Charles
Stein is a typically a method for
controlling convergence and distribution
it was used to prove central limit
theorems initially and to prove rates of
convergence for the central limit
theorems and you could think of it as a
three-step process and so in our context
what is Stein's method would mean first
I want to identify an operator let's
call that T that generates mean zero
functions under my target distribution
okay so T will take in input functions G
in this case they're going to be vector
valued functions for convenience and
it's going to output a scalar valued
function and that function is always
going to be mean zero under my target
and I'll equip that operator with a
domain and I'll call that G that's the
set of functions that are passing
through my operator okay so together
this T and G are going to define what
we'll call a stein discrepancy so this
is an IPM type metric but that it has no
explicit integration under P by design
so I'll call this time discrepancy s of
QN our our sample measure t and its
domain J all right and I'll tell you
I'll give you so many specific examples
of this discrepancy in it in a second
but first I want to show you what we're
going to do with it once we have it so
typically the second step in Stein's
method once you have your stein
discrepancy is to lower bound it by some
reference IPM that you know and trust
like a Vajra stain distance and the the
goal of that is to show that if you're
starting scrip NC is going to 0 then
that reference IPM is also going to zero
so that means you can be sure that
you're detecting non convergence which
is our second requirement so this
typically requires some intensive
analysis but you can do that once in
advance for large classes of
distributions and then you can go off
and use use your stein discrepancy
happily knowing it has that property and
then the third step in steins method is
typically to upper bound your stein
discrepancy by any means necessary and
there a why
variety of tools for doing this in our
case this is the least important step
because we actually are computing this
tiny discrepancy exactly but it is there
to show you that at least they give you
the knowledge that your your discrepancy
measure would converge if your sample
sequence was converging to the target
and so I'll show you how to do this as
well so usually Stein's method is used
as an analytical tool to prove
convergence and here what we'd like to
do is co-opt it as a practical tool that
you could use to measure the quality of
samples being produced by your mark of
chance so let me give you a more
specific instantiation of this pretty
abstract recipe okay so we said step one
was identify a Stein operator an
operator that generates mean zero
functions under my target and to do this
I'm going to use this beautiful idea do
two barb or and this idea sometimes
called the generator method and his idea
was that if I can identify a Markov
process that has my target distribution
as its stationary distribution then the
infinitesimal generator of that process
just generates mean zero functions for
me basically for free and I've given you
the definition of the infinitesimal
generator here but we don't need the
general definition for this talk I'm
only going to focus on one process and
one generator and I've written that here
so we're going to consider the over
damped Lantern and diffusion associated
with a target distribution P okay and
for a wide variety of continuous
distributions this exists and has P as
its stationary distribution and the
generator of that distribution is
written here and our operator is going
to be a mild modification of that
generator so I'm going to call T P of G
this quantity this is how we're defining
our operator T P and what the main thing
you should take away is that it only
depends on P through the gradient of its
log-log density so this is something
that I can compute even if I don't know
the normalizing constant of P and for
those who are familiar with Stein's
method you could view this as a
multivariate generalization of what's
called the density method operator in
that literature
yeah in fact this has expectations zero
the true distribution just a result of
like doesn't correct me fall like that's
a score that's right yeah you can just
do integration by parts and that will
tell you that you get me and zero
functions out exactly so it's the
easiest way to check that this is always
generating means your so you know
there's some constraints on the G's that
you pass through but for the reasonable
G's this will give you means your
functions because of integration by
parts yeah so that's great okay so we
have an operator we also need a set or
we'll call it a stein set the domain of
the operator that we're going to work
with and for this we're going to appeal
to reproducing kernels and make use of
them so a reproducing kernel is a so
it's a function from an our case X to X
from X cross X to R its symmetric and
it's two arguments and is also a
positive definite function which means
that if I have endpoints and I formed
the kernel matrix of all pairwise
evaluations then that's a positive semi
definite matrix so common example the
reproducing kernel is a Gaussian R
squared exponential kernel that I've
written here will be considering some
other ones as well and what we want from
this kernel is the fact that it induces
a function space called the reproducing
kernel Hilbert space and I've written
down the definition of the space here
but I've purposely faded it out because
it doesn't matter that much for the talk
and there's an Associated norm this is a
Hilbert space that has an Associated
norm which I've also written for you and
we're going to define our domain our
stein set with respect to this kernel
and its function space and so in
particular each function G is vector
valued so it has D components and our
stein set is going to be the set of
function such that each component
belongs to a unit ball or each ballon it
belongs to this R KHS this reproducing
kernel Hilbert space and their norms are
jointly bounded that's basically you
should take away from this definition
there's a specific way in which it's
bounded but that's doesn't matter too
much for the talk okay so vector value
function each component belongs to this
reproducing kernel Hilbert space
and the norms are all bounded jointly by
one and in particular for this class of
functions if you pass it through your
operator we're going to get mean zero
functions out from an it and you can see
that from integration by parts okay so
the beauty actually of picking this
particular stein set is that computing
this stein discrepancy is going to be
very simple very straightforward and so
remember that we have we're building up
the stein discrepancy we've now chosen
an operator this operator induced by the
lender in diffusion we've chosen the set
associated with a kernel function and
together we'll call that that quantity
of kernel stein discrepancy or KS Deever
short and the benefit of this particular
choice is that this kernel stein
discrepancy is actually computable in
closed form and in particular the way
the way you compute it is you take your
original kernel you apply the operator
TP to it twice to each argument and then
you evaluate that kernel at every pair
of sample points and then you
essentially just add them up add up
those pairwise kernel evaluations so
that's something that you can that's
both in closed form and it's very
parallelizable so you get this induced
stein kernel which I've written in the
compact way but you could just view this
as applying the TP operator to each
argument of K you evaluate that stein
kernel at every pair of sample points
and then you sum together the results so
it can be arbitrary so it can be any
norm any vector valued apart and in some
sense for our purposes all these norms
are equivalent in the usual way that
vector value norms are equivalent and so
for the rest of the talk I'll just focus
on the l2 norm but you could do this for
any norm
I I'm not I'm not quite sure yet I've
mostly used the I've mostly used the l2
norm there could be some there could be
some advantages if you can there could
be some computational advantages if you
pick the right norm really has to
something to do with a skelion dimension
so if you can get away with computing
this for some of the dimensions it could
be valuable but I haven't I haven't
tease that out yet and that should say
that the stein set choice was inspired
by an analogous kernel that was used in
the context of designing control
functionals that is control variants
that are infinite dimensional for Monte
Carlo integration that was due to oats
Jeremy and Chopin and if you used the l2
norm in particular this recovers a
version of the KSD that's proposed by GL
Kowski at all and LU at all
ok so it's just we'll just use notation
which is plug in the l2 norm the last
couple of minutes of this talk ok so
there are a couple of things that we
want to show about this quality measure
in particular we wanted to show that it
detects non convergence and that it
detects convergence right so first
detecting non convergence so we want to
be the case that the stein discrepancy
is going to go to 0 only if your sample
sequence is converging to your target ok
so here's a first result in that
direction and it's for the univariate
case and this is for probably so this is
not going to be true for an arbitrary
target P but it will be true essentially
whenever your target has a fast boster
Steen coupling has a fast about your
stain coupling that's decaying about
your stain coupling and in particular
certain set of distributions that
satisfies this to be concrete our
targets that have Lipschitz grad log p
and what you might call distant strong
wall concavity so you have to find that
here to be explicit but you could think
of it as a function that strongly all
concave outside of a ball so this would
include bayesian logistic regression the
example that we've been talking about
students T regression which is actually
non convex leads to non convex
non concave density but it's concave
outside of a ball Gaussian mixtures with
common covariance so in this class this
result essentially says that if you use
all of the standard kernels Gaussian
kernels materials inverse multi quadric
kernels that the K polynomially then you
have the property you want you detect
non convergence but yeah so the way this
works explicitly under the hood is that
we've upper bounded a more standard
probability metric by the stein
discrepancy and that just gives the
result so okay so there there are two
questions there's how does how does an
scrips tree actually relate to the
underlying metric and that depends
greatly on the kernel so you can get a
polynomial relationship if you something
like a maternal you'll get something
like a logarithmic relationship if you
use a Gaussian kernel it depends greatly
on that and there's also the question of
how it depends on the target yeah so
with this sort of distant strong low
concavity property maybe it's helpful to
think about Gaussian mixture so they all
satisfy this but the separation between
the modes you know is kind of like a
measure of the violation of concavity
locally and the bound will depend seems
like exponentially on that separation
yeah which i think is what you would
expect yeah okay but this is also just
for the univariate case and also I think
I'd probably overstepped my time so
maybe I'll just give it one more oh okay
great next slide okay so that's the one
dimensional case and typically when
you're using Markov chain Monte Carlo
you care about more than one dimension
at a time so what can we say about the
multivariate case well first of all in
higher dimensions the kernel starts to
matter a whole lot more and in
particular common kernels like the
Gaussian kernel that we talked about
before actually fails to detect non
convergence even for really simple
targets like a Gaussian a standard
and distribution so here's the result
what essentially says is that if your
dimension is bigger than three or your
engine is three or bigger than a
Gaussian kernel a matern kernel which
has exponential decay and even these
inverse multi quadric kernels which
decay polynomially if the BIC if the
decay rate is fast enough you're going
to fail the detection on convergence for
some dimension for all dimensions large
enough so for gaussians it's actually
all dimensions bigger than three or
bigger than two sorry and for inverse
multi quadric s-- it's a more it depends
on the decay rate and the sample
sequences that violate this that you
know but send your sign discrepancy to
zero even though they're not converging
or actually pretty simple to construct
you can just put a bunch of points on a
sphere and grow the radius of that
sphere so you can essentially just shoot
points off to infinity
your sign discrepancy will go to zero
but this is not converging to your
distribution it's not really it's not
converging to any distribution so
intuitively the problem here is that
these kernels with light tails these
kernels that are decaying very rapidly
actually ignore what's happening in
their tails and so they don't detect
that you're sending a bunch of mass off
to infinity so this means that tightness
is important and you know a sequence a
sample sequence or a sequence of
distributions is tight if you can for
every epsilon find a compact set such
that every distribution in that sequence
has at least 1 minus epsilon of its mass
in that compact set and so it means that
none of your mass is escaping to
infinity and tightness is the piece
that's missing here so if you use any of
your favorite kernels like Gaussian
return inverse multi quadric they will
detect non convergence as long as your
sample sequences tight so that's good to
know that's that's a reasonably good
result but ideally your your discrepancy
measure would just detect that for you
so you didn't have to know ahead of time
whether your sample sequence was tight
it'd be nice if that was just all
automatic and so maybe the last maybe
the last result I represent
just probably the main one is that what
we said that these imq kernels is
polynomially decaying kernels failed to
detect non convergence if they're
decaying too quickly but if you decay
rate is is basically just right if your
decay rate is between minus one and zero
then they do detect on convergence and
for all sequences so essentially they'll
tell you if your sequence is not tight
if you're shooting mass off to infinity
and they'll tell you if you're not
converging in any other manner so you
could just use a kernel like this these
inverse multi quartered kernel with a
decay rate between minus one and zero
and then you'll have the detection of
non convergence that we wanted well
actually the complementary result is
that you can also detect convergence
under much weaker conditions here this
will work for just about any kernel you
you would think to use and it
essentially says whenever your boss your
steam distance is converging to 0
between Q and P your sign discrepancy is
also converging to 0 so those are the
main theoretical results I want to share
with you I have various examples of
using this for different problems but
maybe given the time I should stop stop
here ask you to present your what your
most exciting the example is but before
that do one classical problem is just
involved seriously sampling from a
convex body in high dimensions so that's
a local kid that's a key but a high
dimensional and as far as I know it's
open there to detect you know when
you're converging so there are
theoretical balance or how long to run
the various chains but you know if you
have any few actually are running a
chain when you want to the disk
have I converged there's no good
okay throw light on this potentially so
okay so now you're not an arbitrary
convex body and so how do you want to
map how do you want to measure
convergence so what is the notion of
convergence that you care about
well deliberation but we'll get the
whole solution okay so you care a total
variation but is that that's on average
across the outfit of a of a Markov chain
well we're running the actual bigger
margin you're looking at the sequence
what I think though is this if this
chain is the sequence for building so
far
noxee me so so here's yeah you think you
and easy and return address so so you
cancel it event so this one never cannot
yeah
so as you're not going to get
convergence but he had something like
dr. Stephen is dense then you could then
you could reason about that yeah this
front of this theorem would apply is
that you guys talking about but I
presumably the dependency of the
dimension is it's mention for this
direction it's there's not a bad under
this particular concern that about
dependence but I think what you want is
the other direction so this is saying
that if you're watching if you're
sequencing so it could yeah so it could
be very bad and particular these these
function classes aren't that big so if
you're really cared about something like
Boris teen distance like one box in
distance versus like all Lipschitz
functions there's a there's a mismatch
in the side besides of these function
classes so to relate the kernel sign
discrepancy to that you have depending
on your kernel it could be like well
logarithmic dependence or a nasty
polynomial dependence that would depend
on the dimension if you use a much
larger function class then you would
have much less of a dimension dependence
actually
it's harder but we do have some work so
some work that preceded this Colonel
sign discrepancy version was on like a
solving a linear program to obtain a
quality measure and those function
causes are much larger and there you
basically have a linear dependence
between that's done discrepancy and the
washer stain distance so we can yeah so
be happy to talk about that if you're
interested</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>