<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stan | Coder Coacher - Coaching Coders</title><meta content="Stan - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Stan</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LpfpDJt1-58" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I'm going to talk about Stan which is
our probabilistic programming language
so this is a slightly different approach
to a project today on what we've seen
before I'm Bob carpenter one of the core
developers for Stan we've got a fairly
big core development group which is
listed up here so we're doing these in
two parts so what I'm going to start
with is telling you what Stan looks like
now and we're currently in the 2.4
version we've been released for a couple
years now so what is Stan from a high
level it's an imperative probabilistic
programming language so there are other
kinds of probabilistic programming
languages out there like bugs which was
sort of our previous the previous state
of the art I think before Stan which is
declarative and then there's a language
called church which is functional which
is sort of looking forward a bit but
isn't really particularly practical for
real problems yet and there's an
object-oriented alternative that I just
learned about but don't know much about
so what does the stand program do well
it declares variables and codes a log
posterior basically or a penalized
likelihood if you prefer to think in
frequentist terms and then what a stand
inference do on top of a program well it
does sampling for full Bayesian
inference or it does optimisation and
curvature estimates for maximum
likelihood estimation and uncertainty so
let me give you a simple example to sort
of ground what we're doing here of what
Stan looks like this is a very very
simple example just for estimating a
Bernoulli probability right so we have a
data declaration that declares the
number of instances each each instance
of data is boolean so it has a lower
bound of 0 and an upper bound of one we
declare that as an array why there's a
single parameter theta which is bounded
to be between 0 and 1 and the bounds are
important here and then we have a model
that says why is distributed according
to the Bernoulli distribution
conditioned on theta so the assumption
in this model is that the prior on theta
is going to be uniform anything that's
declared as a parameter and doesn't have
a declaration is going to get a uniform
prior but you could add priors in as
well and the expression for y here is
vectorized
automatically loops over all the
versions and adds it so in general the
goal and stan is a user has a particular
kind of probabilistic model they want to
explore or a space of models where they
want to explore so rather than wanting
to take some kind of off-the-shelf
learning method and apply it to my
particular data our users are typically
fairly sophisticated statisticians
mostly grad students faculty members
research people at companies who want to
build fairly involved statistical models
not something like this and they often
don't know the model that they want to
build so they often want to explore the
space of models as part of doing this so
our emphasis is mainly on efficiency but
efficiency constrained such that we need
to give users the flexibility to express
the kind of models they want so what
happens well I'm showing you now input
and output using our our Stan interface
so this is the interface to stand
through the our programming language so
we set a number of observations to five
we set our observations which are two
positive and three negatives and then we
call a fit by calling the Stan function
bernoulli dot Stan was the name of our
model we provide the data which is n and
why using ours funky environment stuff
and then we do a print here's what the
print looks like on the output we print
the mean of theta we print the standard
error this is the mcmc standard error
and our estimation of the mean then the
posterior standard deviation some
intervals the number of effective
samples which is very important this is
the thing that goes in the denominator
square root of n when you're computing
the mcmc standard error and then
convergence metrics then we can do
things like extract the fitted data and
plot a histogram of theta so this looks
like the posterior plot for theta for
the fit given this data and running
through this model oh how long do I have
by the way 20 more minutes okay stan is
open source so the core of it's written
in c++ and we have a command line
interface those both have the new BSD
license the PI Stan and our stay on
interfaces our GPL version
three and then we have dependencies on
the eigen C++ library for matrix
operations and boost for a lot of
template metaprogramming parsing things
like that and we use google test for
unit testing so we're hosted publicly on
github we've got issue tracking and so
forth run through github we also run
everything through pull requests and
code review with continuous integration
hooks into our Jenkins server and we
also have public mailing lists separated
for the developers and for the user so
the platforms that we run on are pretty
much all of Linux Mac OS windows we need
a fairly recent c++ compiler meaning
something written in the last five or
six years not something that's like 20
years old the C++ API is portable and
C++ 2003 compliant right now and then we
have interfaces for the command line for
our for Python these are ones that are
developed by the core developers we also
have a couple user-contributed light
interfaces for matlab and julia who's
using stan right now where as I hinted
at earlier it's mostly statisticians we
have 800 some people on our mailing list
we're getting like 10 or 20 messages a
day so it's kind of overwhelming us it's
pretty much across all the physical
biomedical and Social Sciences plus
applications in engineering we just got
an education grant from the Department
of Education to do education models for
tests and teacher performance and also
for marketing so we've got a list of
these are things that we know people
have written and published papers about
using stands so there's a lot of other
you we've got thousands of users out
there this is just what published papers
have been we have a long user's guide
describing the length of the programming
language itself and then for each
interface installation getting started
guides then we have a ton of examples
out there most of the people we're
targeting learn how to do this stuff by
looking at examples of models like
there's I'm Ana College estai want to
build a population model for docs for 50
years in a particular wetland where I
have data well I usually don't want to
figure out how to program that from
scratch I want to go look at somebody
else's you know implementation of the
Cormac jolly sorber model
whatever it is for doing this kind of
thing so we've got tons of examples
we've got all the bugs and jags examples
we've got all the examples from Gelman
and Hills regression books I would
highly recommend this new book by Eric
Young wagon mockers and Lee called
Bayesian cognitive modeling one of their
students has ported all their code to
stand this is a really nice introduction
to bayesian modeling that's not super
heavy on the math side there's a couple
books in progress whose authors have
contacted us because they're going to be
including steen on and we have a ton of
user contributed examples in our groups
and our mailing list for scaling an
evaluation which is important to us I
really want to separate three kinds of
scaling here that people want to talk
about one is sort of you fix your model
and you just throw more data at it
that's easy scaling right if you're a
statistician getting more data just
makes your life easier at worst you can
subsample it more parameters are a
little tricky right typically when the
number of examples grow we're doing some
NLP classification problem the number of
parameters grow with it that's still
fairly simple what we're targeting is
more complex models that is you want to
throw a multi-level model on you want to
throw some kind of complex prior on you
want to take your LD a model instead of
saying adder ish late prior I want to do
some kind of multivariate logistic
normal prior on it that's the kind of
thing that we're really targeting is
scaling up in terms of more complex
models gaussian processes for instance
with various kinds of covariance matrix
estimate all kinds of time series models
that we're building well I should say
our users are building we're not really
in the model building business ourselves
so the metrics that we care about since
weird mainly doing MCMC sampling and
doing optimization is time to
convergence the mixing that we get after
we converge the amount of memory usage
and what we found is we have anywhere
from zero to infinitely many orders of
magnitude improvement on the competition
doing this and there are some problems
either zero it's actually goes to a
little less than zero in some cases
there are some product some conjugate
models and Gibbs are really easy to fit
with Gibbs samplers as soon as you get
to any kind of model complexity you
start seeing benefits to using Stan over
any of the competition
and the reason we really develop stan is
we had a bunch of hierarchical
regression models that we couldn't fit
with anybody software right so the
reason that Andrew hired me and Matt
Hoffman originally was to start building
something that could fit these
hierarchical models that no other
software could fit so that's where the
infinity comes from is there's lots of
problems that our software can fit that
nobody else's can so when there's more
improved the more complex your model is
the more improvement you're going to see
with our programming language the way
the models are set up is there's a data
declaration everything's everything
strongly typed here so there's a data
declaration that declares the data these
get executed in terms of reading once
from a data source validating the
constraint then there's declarations for
the parameters those declare the
parameter sizes and importantly the
constraints on the parameters because
every execution of the log probability
we do a mapping from the parameters to
an unconstrained set of parameters
though actually it's the other way
around we're doing our sampling on an
unconstrained space and our optimization
on an unconstrained space so we apply an
inverse transform from that back to the
constrained space where we evaluate our
model and automatically apply the
Jacobian applying the Jacobian is
optional if you're doing a doing
optimization because if you maximum
likelihood as you may know depends on
the scale right maximum like unlike
expectations in a Bayesian model maximum
likelihood estimates aren't well-defined
until you pin down the scale at which
you're doing them if you change the
scale that you're operating on then the
maximum likelihood estimates are going
to change right uniform uniform on the
log scale is very different than uniform
on a linear scale for instance the
Jacobian will adjust for that but we
don't always apply that the model
defines the log probability for the
model and that's something that gets
estimated every every gets executed
every time we also allow blocks to
transform data so often you want to once
take your data transform it into some
slightly different format we allow
transformed parameters which transform
the parameters and again you can
optionally apply jacobians there if you
want and then we have generated
quantities for doing things
like posterior prediction event
probability estimation decision-making
and these get executed once per
iteration we just added a functionality
which I'm very happy with because I
mainly the person who deals with the
low-level programming language issues we
have other people who specialize more in
the optimization and other people who
specialize more in the in the sampling
but I'm mainly the language person so we
just added an ability for users to
define their own functions within our
language which may seem relatively
simple but it's very complicated with
all the automatic differentiation that
we do right we have a bunch of different
built-in variable and expression type so
there's primitives integers and reels
there's matrices and we like MATLAB but
unlike something like our differentiate
vectors and row vectors this is
important for type inference if you
multiply a row vector by a vector you
get a scalar you don't want to get
another matrix out you don't want to get
some big overloaded object out for
efficiency we allow bounds to be
declared lower and upper bounds on our
matrices and we also have constrained
vector types simplex asst things that
sum up to one non negative things that
sum to one ordered vectors so if we want
to do ordinal logistic modeling right we
have some kind of ordered thing we're
getting data where people are ordering
it we typically want to do a logistic
regression and then there's cut points
that decide where the order falls we
have constrained matrices you can
declare covariance matrices or
correlation matrices which is useful if
you're building hierarchical models you
often have a couple say we're doing drug
trial data we have the reaction of a
patient to a particular drug there's a
couple features of the reaction that
vary by patient right but there's
covariance among the way those features
of the drug reaction vary by patient so
we can model that kind of thing using
these kinds of constrained matrices we
have the basic set of arithmetic and
matrix operators logical operators then
we have a very extensive library of
built-in mathematical functions this is
includes all the built in C++ functions
including all the libraries that have
become part of the standard but we also
have an extensive library of statistical
functions things like soft may accent
things that you need for normalization
constant slike log gamma functions
Bessel functions of the first and second
kind but you need to normalize some
spatial statistical models and then we
have efficient and arithmetic Elise
table compound functions which the
theano team was mentioning earlier we
don't unfortunately do this
automatically but we do have things like
log 1 plus ax so we can evaluate that
efficiently things like logs some of
exponential shows up when you're coding
up mixture models for instance inside of
Stan right then we have a bunch of
built-in matrix functions including
basic type inference which I mentioned
earlier all the basic element wise
arithmetic this is very much modeled
after MATLAB so if you're familiar with
MATLAB it's pretty much the same sort of
syntax there we have the basic
decomposition slicing and broadcasting
functions reductions like sums and norms
things that take matrices or vectors and
return scalars compound operations like
for instance very often if you're
dealing with elliptical distributions
you need quadratic form something that's
going to multiply a vector transform
times the matrix times the vector we
want to be able to do that efficiently
with respect to the automatic
differentiation that we use and we need
specializations so we have special
operations that apply to positive
definite matrices like you find in
covariance matrices triangular matrices
like you find in shull s key factors all
of this stuff lets you efficiently
implement inside of asti on program
multivariate models right which is one
of our key focuses one of the things you
couldn't do easily with existing
technology right for instance and bugs
and Jags you can only do conjugate
multivariate models whereas we want to
have much more flexible conjugate models
non conjugals for this so for instance
each we have a distribution library that
pretty much has all the distributions
most people have heard of and a lot that
I hadn't heard of till users asked for
them and we implemented them they have
the usual operations with them we also
provide alternative parameterizations
which can be a lot more efficient so we
have like a chalice key factor
parameterised normal distribution we're
very excited about a new multi multi
very
correlation matrix density this allows
you to constrain control it's up we
typically use it as a prior on matrices
rather than something like a wizard or
an inverse Witcher and the degree of
freedom parameter control shrinkage to a
unit matrix which is a more natural way
of thinking about a prior for a
covariance matrix to think of an
independently constraining the
correlation matrix with some shrinkage
toward a unit and then independent
scaling of the of the scale factors so
we're very excited about that and have a
fairly efficient implementation of that
the language itself is made up a bunch
of statements we have sampling
statements which increment the
underlying log probability in a way
that's built in or you can do it
directly with expressions yourself we
have assignments loops conditionals
blocks and then we have printing and
exception statements well I should say
we will have exception statements at
least in our next release it's it's I
think it's cute I don't think it's in 24
I think it's going to come out in 25 but
it's already past testing and is all
cued up and merged so the two kinds of
inference that we do for full em for
full Bayesian inference where we want to
do all of our inference by keeping all
of our posterior uncertainty and
integrating through any expectation we
calculate we're doing that with Markov
chain Monte Carlo methods and in
particular we're using an adaptive form
of Hamiltonian Monte Carlo Hamiltonian
Monte Carlo is really nice and that it
uses gradient information instead of
doing a random walk it actually walks
around the posterior in a way that's
sensitive to the to the contour the
curvature of the posterior there's a lot
of problems that we needed to overcome
with this i should say matt hoffman
needed to overcome with this one was
adaptation during warm-up so we
basically use a dual averaging algorithm
to adapt our step size and to adapt a
mass matrix so basically the hamiltonian
monte carlo is treating the parameter
value as a particle it's treating the
negative log posterior as a potential
energy field and it's imparting a random
momentum term and then we're actually
solving the differential
asian using the Hamiltonian mechanics to
move from one side of the distribution
to the other side of the distribution
during sampling but to do that
efficiently we need to estimate a mass
matrix right which basically you can
think of as seeing what the scale of the
parameters are and if you do it in a
dense form what the correlation of the
parameters are right and then we do a
depth eight and these things are both
estimated along with a step size during
warmup yeah yes yes we are we are we
have it for everything but our
probability densities now but that is
also you will hear more about it this
afternoon in the coming soon features so
right now we're doing it in a fairly
crude way we have something like L bfgs
which is doing it during optimization
but we're actually rolling something out
so we'll be able to do real Riemann
manifold hmc just to give away a bit of
this afternoon stock but the really
clever thing that Matt came up with for
Hamiltonian Monte Carlo an hmc is very
very sensitive to these tuning
parameters so you want to be careful
with people telling you my algorithm
runs really fast once I figured out what
the tuning parameters were figuring out
what the tuning parameters are is a
large part of the work and we're doing
that now fully automatically during
sampling right as opposed to during the
warm-up phase what's going to happen
here is we adapt the number of steps
basically the time that we simulate this
Hamiltonian for this is done with a very
clever algorithm based on the geometry
of the posterior that tries to walk
until it starts to make a u-turn now
it's very complicated to make that
observe detail balance to make sure that
the posteriors are really being sampled
from the right density but it all works
the posterior inference that we can do
is we can use the generated quantities
in the in the model to do predictions
decision make decisions but do Bayesian
decision theory based decisions
calculate event probabilities we have
extractors for these samples to do
something very general then there's a
popular package for bayesian analysis
and i are called Cotto and we do a
posterior to your code alike posterior
summary which I showed you an example of
and we like to do model comparison
either using
cross-validation if things are efficient
enough are using wake which is an
information criterion that
asymptotically simulates
cross-validation using just the in
sample data that you've got we also do
penalized maximum likelihood estimates
which i think is more similar to what
everybody else was talking about earlier
you throw up some kind of likelihood
function you maximize that subject to
some regularization criteria basically
shrinkage typically and we do posterior
mode finding which will find either a
map estimate a maximum a posteriori or a
posterior mode or it'll find a pure
maximum likelihood estimate depending on
how you want to think about this
philosophically does the same thing
computationally and we use L bfgs for
this so this also uses the model
gradient and it efficiently approximates
the Hessian as it's going along we have
to disable the Jacobins on the transform
which are necessary for mcmc every time
we transform a variable we have to take
into account the non-linearity of that
transform and the curvature of the
variable transform say going from minus
infinity to infinity down to zero and
one I do a sigmoid function that sigmoid
function isn't linear has a different
curvature I have to adjust for that
curvature when I'm doing sampling but
you don't want to do that when you're
doing maximum likelihood it's not really
a quite a proper probabilistic technique
based on expectations so instead we have
to disable the jacobians which is
allowed and what we're going to be do
what we're doing right now is we're
using the Hessian to estimate the
posterior covariance which everybody
wants with you're doing something like
hypothesis testing you need to get the
posterior covariance out right now we're
using curvature to estimate that based
on finite differences but we're about to
roll out a more general tool for doing
that and then one of the things that
that people have been using stay on for
rather than just fitting models is
people have been using Stan for a
research tool and it can be used to
explore algorithm so one of the nice
features of stan is that it takes a
model defined with constrained
parameters like simplex Azure covariance
matrices which are these really
complicated positive definiteness
consider
and they transform to unconstrained
support on our K on RN sorry and you
know basically that makes means you
don't have to deal with boundaries in
your random walk algorithms and your
optimization algorithms then once a
model is compiled you get the log
probability gradient and hashing out as
features we deal with data i/o and
parameter initialization which can be
tricky mapping between the constrained
and unconstrained spaces so we wrap that
all up with simple wrappers with access
to variable names and dimensionalities
and the transforms and in the very near
future we're going to be doing
second-order higher-order derivatives it
really third order derivatives as well
so that was actually where I was going
to stop this morning and take questions
no we can do Gaussian processes if
that's your that's your nonparametric of
choice but we have a fixed number of
parameters so we can't do now in most
applied problems you can do something
like a Jewish slave process by
approximating it with a big finite thing
but we can't technically actually jump
in the dear ashley process base and i
should qualify this there are no
discrete parameters at all if you want
to use a discrete parameter and stand
you have to marginalize it out of the
model which winds up being clunky for
users but very efficient for sampling
because the raw blackwell theorem it's
much easier to sample when you
marginalize things out but harder to
formulate the models just kind of a big
conceptual difference between Stan the
other
project link for the other projects
least conceptually it's relatively easy
to imagine combining at least the
algorithms and possibly even the code
bases to some extent right but maybe I
don't understand the details of Stan
well enough you can you see how you
would so for example in VW there's a
good online learning algorithm right do
you have a sense of how you could take
advantage of that or how that could fit
into the stand or is it just like
completely different that's been put
together bummed I think you're
foreshadowing the talk to you you had us
think about for later um we can
incorporate other methods and we can
quote I mean assuming they have open
enough licences we can incorporate other
people's code as well but we haven't set
this up to be extensible to be like
something that lets you just plug
arbitrary pieces together and part of
the problem there is everything's going
through our language right so for
instance one of the things we need to
add is sparse matrix representations
which we don't have now there's lots of
libraries but it's really quite a pain
to do because it needs to be threaded
through not in the threading sense but
it needs to be sort of woven through all
of the language to allow expressions for
sparse matrices and so forth
the variational stuff yes that's one of
my slides for this afternoon in fact you
want to talk to alp who's sitting right
back there wearing wearing a name tag
that says visitor oh yes I do we have
plans to incorporate variational and
France and and the answer the answer is
yes we're we're working on that with
Dave Bligh and without yeah since you
got your hand up Dave
tool for for taking a model compiling it
into a box that gives you
requires derivative you have an addition
complex
I feel like Stan plus
very powerful combination or let
domain experts express themselves
that may not be holding you I've noticed
a lot of the libraries
reimplemented basic statistical
functions
things like le package of years to get
ready he's using a newly hash live like
three years in them boom a miracle
stability in such a is it as a developer
how do you decide when to you know go in
something that's been used for long time
versus war when you're over the factors
and being how do you assure people of
something as subtle as miracles to
to answer the former question when do we
go with something else when it exists so
for instance we went with eigen because
it had a nice templated matrix library
so we could plug our auto-da-fé right
through eigen no problem the problem is
auto differentiating an iterative
algorithm like inverse or something is
very inefficient so we needed to
actually implement something when we
wanted more efficiency we had to extend
that and actually do something more
efficient for matrix derivative so we're
still using I ghen to compute the basic
inverse operations right and we inherit
whatever stability eigen has which is
improving over time but then we also
have to sort of paste on our own thing
for doing doing matrix derivatives the
problem was with particularly the
density functions and almost all of the
other functions there was no
sufficiently templated version so if you
look at something like boost which has
all most of the well not any of the
multivariate stuff but at least most of
the univariate densities implemented it
didn't allow us to template separately
on each argument which is really
critical for auto diff efficiency I
think to answer the question about how
can people verify that things work well
I mean we have the C++ code we have
pretty extensive unit testing for both
the gradients and for the particular
values and we encourage people to test
things themselves right so you can call
these functions separately you can you
can test them if you know C++ you can go
test them yourselves for most of our
users they can just see it in the fact
that their models don't crash any more
so the previous generation of these
systems like bugs and Jags had all kinds
of problems on boundaries you do
something like you multiply two things
together and then just due to the
vagaries of floating-point not being
quite associative you wind up with
things that should sum to less than one
being one point 0 0 0 1 then you throw
them into inverse loggia it crashes
right that doesn't happen in Stan and at
least when it does in cases we don't
know about we tend to fix it but
otherwise I think you've got your you've
got to go kick the tires yourself to
really convince yourself that you know
you can look at our unit testing or you
can
try yourself so yeah this is super
interesting father project right be in
particular the authoritive and the
distribution representation in C is it
technically possible to to expose this
in the form of a library without having
to go through the Stan language yes yeah
it is all exposed in its own library so
it's fairly modular C++ so there's we
try to minimize the dependencies among
our namespaces and packages in C++ you
can pull these things out you can't
really pull any of the multivariate
stuff out without also grabbing eigen
because we use that for our for our data
structures but otherwise you can
independently pull out just our auto
diff system in just the basic C++
functions and then you can optionally
throw the matrix library and the
probability density library on top of
that as a sub project that you can you
know use independently of sensei you
don't want to do bayesian modeling you
want to do something else alternate
library yeah we would we would like to
wrap this up first we need to create
developer facing doc that's more than
just us chatting on our own little
mailing lists and during our meetings so
that it's a little easier for people to
figure out how to use that right now we
tell people to go look at our unit tests
go look at a few simple examples that we
have and figure it out from there then
there's then we could wrap up separate
releases we could pull what basically
the auto diff is most of what stan is if
you actually look at what the
implementation is so if you download all
of stan you have to throw away the
parser and you have to throw away some
of the mcmc stuff but that leaves ninety
ninety-five percent of the code is the
numerical library underneath so you're
not if you think you'll save a lot of
space that's not going to going to
happen but it would be nice to release
it cleanly as a separate library because
people will be there for the next
session okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>