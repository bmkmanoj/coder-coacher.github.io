<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deep learning enables large-scale computer image recognition | Coder Coacher - Coaching Coders</title><meta content="Deep learning enables large-scale computer image recognition - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deep learning enables large-scale computer image recognition</b></h2><h5 class="post__date">2014-07-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MSzjFHBOAaQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right we're here with Joshua tell us
a little bit about yourself ok so I'm a
researcher in microsoft research I've
been there for over 15 years now my
background is in high performance
computing making computers more
efficient most recently I spent the last
two years in bing building a scalable
web search engine from scratch so that
was very exciting it got me hooked on
distributed systems and you know like
something that's relevant is I really
got into computers because of my life
for AI and I was seduced by a I in the
80s and then I was very disillusioned
when I realized that a I at that point
was just smoke and mirrors and clever
programming and so I went off and
started you know working on making
computer is more powerful and and when
you got started what was the first
computer that you used so my first
computer was a Commodore 64 all right I
know I did all the peaks in pokes yeah
actually the second programming language
I taught myself was list because I was
like in love they are ya and in see your
backgrounds in high performance
computing distributed systems and how
did you get back into AI at Microsoft
yeah so that's one of the wonderful
things about Microsoft Research the
sense that you have this incredible
critical mass of experts in all areas so
I came back I knew I wanted to work on
distributed systems and that was the
point where deep learning was in the air
and I looked at the literature then I
said I can build a better distributed
system for training deep learning
systems and looked around where I didn't
know much about deep learning so John
Platt was a machine learning expertly on
Bo tools and expert on scalable
algorithms were training machine
learning systems there at Microsoft you
know I just looked up who the experts
her and then they turned out to be at
MSR so took a crash goshen machine
learning from these guys in a John Platt
actually for the last two years he met
with me every every day once a week and
so I learned an incredible amount from
him and discussions from like you know
leading people Patrice Simard it's just
it was I think MSR is the only place I
could have done this yeah and how many
people are on your team so it's a team
of four people and I told Harry Shum
this and he said that his predecessor
ken moss I told them that you need for
people to change the world yeah 3 is too
little and five is too many so it was
just a lucky coincidence i guess so tell
us about project adam okay so so project
adam is an effort to build a scalable
large-scale distributed system for
training deep neural nets purely from
commodity pcs and it's much more
scalable and efficient than existing
systems and we've used it to train a
model that does image recognition
large-scale image recognition it's a
very large scale task recognizing around
22,000 different categories from an
image and you know we have the best
performance on that task using project
Adam and what is a deep neural network
so so what a neural network is first of
all is it's a computational framework
for automatic pattern recognition that's
inspired by what we know about biology
and how the brain works and what deep
neural Nets is it adds one layer that
are saying that you have multiple layers
of these neural net networks that are
connected to each other and the true
magic of deep neural Nets is that it
turns out that this hierarchy or these
layers is very important because it
allows you to learn these hierarchical
features and that turns out to be key
and different from kind of general
neural nets and that's when the deepness
comes in and how do you teach a computer
to see and maybe a better way to phrase
that would be how do you teach a
computer to understand what it's seen
that's right so so I think one way to
look at it is well how do we learn how
to see right and the way we go about it
is we see a lot of information around us
eyes are constantly taking things and if
you look at a baby they're very curious
looking around at things and our vision
system takes about 12 years to develop
and constantly be seeing things and we
get information about what we're seeing
so you know that someone tells us that's
so and so that object is so and so and
that's exactly how we train a computer
system to see using project Adam we give
it a lot of training data a lot of
images and we tell it that this is a
tree or this is a king penguin
this is a pembroke welsh corgi and this
is the magic of deep learning just with
that information a lot of training data
a lot of labels of what the information
is it automatically learns so it
automatically learns how to extract
features from these images so that when
you show it an image it's never seen
before it can accurately categorize it
and you know one of the categories you
are retarded to do you have to train
every single thing I mean will it think
that a horse is a big dog no actually
it's incredibly accurate and it turns
out that you know you require some
amount of scale so if you give it to
little training data it doesn't learn at
all but once you hit some critical mass
if you've given sufficient training data
and you have a sufficiently complicated
model that can learn it it's actually
incredibly accurate that not just
telling horses from dogs but much more
fine-grained classification like you
know that's not a Justicar d that's a
pembroke welsh corgi it's not a cardigan
welsh corgi and it's actually better at
some of this fine grained classification
than I am and what are some of that what
are the things that it looks for so what
it really looks for and and you know
like this is kind of what we found out
by examining the trained neural network
after the fact yeah and what we see is
kind of you know like it has these
hierarchical features so the lower
layers of the network low and edge
detectors the higher layers of the
network kind of learn things about you
know like textures and some kind of
generic shapes and as you go higher and
higher up the network what you find is
what you learn is kind of concepts that
are recognizable to us so there are
specific neurons at the top level that
respond when it sees human faces there's
neurons that respond when they see
images of dogs there's neurons that
expand when they see kind of text in an
image and so it automatically extracts
these features that it puts together to
come up with an ultimate classification
for an image and how different is this
from how how human would discern what
something is it's like a young child so
so we don't fully understand how the
human vision system works but actually
the
the neural network you're trained is is
hugely inspired from there and so to the
best of our knowledge like you know it
uses five or six layers of a type of
network called convolutional neural nets
and understanding of the human visual
system is there's also about five or six
levels deep of these neuron layers and
the thing about convolutional neural
nets is they have this notion of local
receptive fields whereas neurons rather
than looking at the whole image they
look at just narrow portions of the
image and the higher levels of the
hierarchy kind of see more and more of
the image and that's how we think that
the human visual cortex is also
organized so it's remarkably similar to
the extent that we understand the cortex
so as a developer I might think that if
I was building a system like project
Adam I I might have it like look for
features on the dog like here are the
eyes here's the nose look for floppy
ears is that the way that you start with
a system like that or do you just start
feeding it pictures and allow it to
decide I think that's the really
interesting thing about the system is
that you know as a program where you
would think that's how you want to
program this right you know you will
train you know like this is what an eye
looks like this is how the shape is and
stuff like that and what we found out is
with vision systems like that have been
done and people have worked on it but
the accuracy of those are significantly
worse than system trained by atom which
way you do is you just present images
and just say this is what the images and
nothing more than that and the deep
neural network is automatically able to
figure out you know the features that
are important for characterizing the
images that going to the labels you give
it and that's kind of the magic about
deep neural Nets we don't fully
understand but one thing that's really
interesting you know for a programmer to
think about is you could look at the
model Adam strain as representing a
program that the system is synthesized
that's more complicated than any program
we were able to write ourselves so it's
kind of you know maybe a new paradigm of
programming where you can kind of
program certain sophisticated things
that you couldn't in any other way just
by using large amounts of training data
in a system with the learning algorithm
and once you
trained to system is that something that
you can leverage and copy to other
systems or does each one have to do its
own level of training so that's the
that's the i think really interesting
and exciting possibility with deep
neural nets is once you have trained it
on a large amount of diverse data it's
not task-specific the features its
learned actually transfer over to either
tasks so i can train it on task one and
leverage that training and retrain it
without losing the information of all
three trained on task two and the
interesting thing is when you train it
on task to you can actually also improve
the performance on task one so it's kind
of additive which is which is beautiful
yeah I've heard when peter lee the head
of microsoft research talks about how
we're training computers to speak we
introduced a second language like
spanish the first language improves and
that that's kind of like what II just
said that's exactly right so how does
that happen for vision so i think the
way of thinking about it is you know
depending on the images you present to
the system it learns kind of types of
features so one thing we notice is when
you're starting to train the system is
our input dataset didn't have really i
think almost any cartoon images and so
it performed very poorly on classifying
cartoon images because it had never seen
them and that was kind of very alien to
it yeah and once you gave it cartoon
images not only did it get that
correctly but its performance and other
images improved because what it does it
had found certain other features that
really could only learn when it was
given that domain of information but it
could kind of transfer that over so a
project Adam uses vision I made so it's
one sensor are there other sensors that
would come into play like would you use
sound and hearing absolutely I think so
so you know like we definitely are
looking at using it for texts for speech
but more interestingly multimodal
combining these all thing all of these
things together like you know I've never
seen the Eiffel Tower you describe the
Eiffel Tower to me then I have a chance
when I first see it that oh yeah I that
that looks like the Eiffel Tower from
the description and so this transfer
learning across multiple modalities is
very exciting and
interesting so microsoft research has a
focus on getting AI ready for everyday
use how does how does project adam how
is that useful to us I think the
possibilities are are immense and so let
me just mean outline a few may be so so
one thing that you know Peter estar
would be very useful and I agree with
them it's like you know you walk around
with a cell phone take a photograph of
what you're eating and it tells you the
calorie content the nutritional value
and you might decide based on that it's
like wow those fries a way to you know
eat less of them things like I'm just
wondering in my backyard and I might
look at a plant and say well I'm curious
what is that you know tell me what it is
and then more future looking is like you
know if you could take an image of
something and describe a scene it could
really help blind people you know what's
going on around you just kind of
describe things around it and i think
the ultimate goal is to to use this to
help machines understand represent and
explain the world around us and
understand us and so putting those two
things together you can think you know
the ultimate goal is like you know your
best personal assistant that's always
around with you because you carry all
these devices but now these devices know
you they know the world around you and
what can you do without it and this is
it possible for these different devices
with AI to speak to each other and
that's absolutely I think that that's
that's the real possibility because you
can have all these different devices
kind of learning on different data and
with this transfer learning I told you
and also looking at kind of combining
multimodal information so I could learn
something just from images I could learn
something else from text can I put them
together and so these different devices
can kind of talk to each other
communicate and share learnings I think
and have you seen any examples of like
where you've you've just been really
impressed or surprised by something that
it's it's recognized or understood yes
they've been some like incredibly
amazing things so actually uh my boss
Eamonn Wang who was actually an early
supporter of this project shared with as
a photo of he had taken in his backyard
and it was really a very abstract seen
in grass and stuff
there was a tiny rabbit in the corner
and he just did it jokingly to say lad
you know like do I I don't think any
system would say that's a rabbit and lo
and behold it got the grass and it got
the rabbit and was pretty amazing Wow
and on this you know image in that task
I've been talking about I kind of
compared my performance on it with the
computers and it was like it blew me out
of the water so like I'm really
embarrassed about how bad I am at
recognizing objects thanks so much for
talking to us today welcome thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>