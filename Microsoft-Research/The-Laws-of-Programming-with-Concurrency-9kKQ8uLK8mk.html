<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Laws of Programming with Concurrency | Coder Coacher - Coaching Coders</title><meta content="The Laws of Programming with Concurrency - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Laws of Programming with Concurrency</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9kKQ8uLK8mk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
and first of course my pleasure to
introduce my colleague Tony Hall tony is
a consultant principal researcher at
Microsoft Research now working part time
for us and when I saw the summer school
webpage just earlier this morning as I
always been quite humble in his
biography because there's a lot of
detail missing so I will fill in some of
that detail but not all of it because
Tony had a long career in computer
science you've studied classics and
philosophy and Oxford afterward he had
an academic career at various
universities at Queen's University in
Belfast he was an Oxford and also at
Moscow State University in Russia
he's been now a researcher with us for
quite a number of years he and the
Cambridge lab and Tony is well known for
quick thought the whole logic and CSP
and he did he warned the ACM Turing
award in 1980 and a number of awards
afterwards too many to go into detail
and welcome to me and well thank you
very much scarlet for that pleasingly
short but very flattering introduction
and thank you all for your for your
welcome and your attention I consider it
my privilege in certainly as a pleasure
to address an audience of this summer
school yes you are all researchers
nurses researchers from universities
around the world just at the beginning
of your research career I have exploited
this opportunity to tell you about a
topic in computer science which has
interested me throughout my route
research career which is the present I'm
a little bit longer than yours and I
hope it and it continues to do so
and at the same time I will tell you
something about myself something about
my motivations
as the ideas which helped me in my
research and maybe some which did not
I expect my talk to last about 50
minutes I spent about half of it talking
about the laws of sequential programming
and the second half on concurrent
programming I hope to leave 10 minutes
at the end for questions but you're
welcome to ask questions before that if
you wish and I'll start by asking you
some questions about yourselves first
question before this school how many of
you had heard of the laws of programming
Oh excellent
well I hope the end of 50 minutes you
will know quite a lot about them
in fact more than most programmers do so
no really point in asking the remaining
questions except that I hope that in my
talk I will be able to answer them
myself and so at the end of the talk you
will be a little marginal marginally
wiser my own answers of the questions
could be summarized very briefly the
laws of programming are collection of
mathematical equations roughly the speed
the level of the algebra that you learnt
at school simply simple algebraic
equations involving familiar algebraic
properties the laws are widely used
although maybe not everybody knows it in
the development of software that you use
every day operating systems compilers
program checkers and program analyzers
and also to a lesser extent in the
software for the cars and the trains and
the aeroplanes on which you travel
finally the laws have been proved true
of a simple programming language is what
I will describe in my lecture
which was invented by one of the great
logicians and mathematicians of the last
century Stephen Kleene II in 1956 he
published a paper entitled a
representation of events in nerve nets
and finite automata one of his aims was
to represent events occurring in the
nervous system of organisms he therefore
made his contribution not just to the
theory of programming which is what I
will be interested in but the
foundations of computational biology
which I know is of greatest interest to
many of you and has been treated well in
previous lectures of this summer school
great so I will talk about clean ease of
regular expressions it was sort of a
programming language which is used to
program finite state automata or to
specify the behavior of such machines
execution of a program was represented
as a string of events or actions which
occur sequentially while the program is
being executed regular expressions are
widely used today to specify protocols
for dynamic interactions of components
of a distributed or concurrent of
computer system these are the notations
which I shall use for regular
expressions the semicolon denotes
sequential composition of strings of
actions execution of the first operand P
is followed by execution of the second
operand q the Union operator denotes a
choice between its operands only one of
them is actually executed and the other
one is ignored the numeral one denotes
the program that does nothing
here is a sample of the laws of regular
algebra which apply to regular
expressions they all appear to appeal
strongly to your programming intuition
which you have developed over several
years of practical experience of
programming associative law at the top
of this slide says that if a program
asks for 3 C 3 actions in a sequence it
does not matter in which way the actions
are bracketed two by two the next law
states the obvious fact that if you do
nothing before doing P this is the same
as doing P the laws for choice are the
same as those for disjunction in boolean
algebra a choice is associative that is
commutative and it is idempotent a
choice between P and P itself is no
choice at all you have to choose P the
final law is a distribution law which
describes the interaction between the
sequential operator and the choice
operator left hand side describes the
execution of P followed by just one of
either Q or R the right-hand side
describes a choice between the execution
of P then Q or P then R and the
distribution law Millie says the same
thing as it does in ordinary algebra
that the choice can the choice can be
made before the sequential composition
as it were at compile time or it can be
made while or even after the execution
of P at runtime but choice must be made
now going slightly more sophisticated
I'm going to introduce what is known as
the refinement ordering in programming
said you what if you heard of the
refinement ordering of programs Oh
lovely so here it is P I will call it
below P below Q means just that every
execution of P is also an execution of Q
in other words the executions of P are
just contained in the executions of Q
the route of this P has less executions
it's more determinant it's easier to
predict what it's going to do whereas
the and furthermore it's easier to
control what it's good what is actually
going to do is going to be more tightly
controlled by P and by Q I conversely Q
is more abstract it describes more
phenomena than P and therefore it's like
an abstract theory in which a lot of the
detail is is ignored and the essential
abstraction is made clearer abstraction
of course is a property of
specifications and all my P cues and
ours are going to apply to regular
expressions whether they are used as
programs or our specifications and the
refinement ordering applies to them all
equally for example if P is a program
and our R and s of specifications P
below our means P satisfies our
specification
everything it does is allowed by the
specification P and R below s means that
R implies s if R is a design a slightly
more concrete specification of the
eventual product of the eventual program
then our below and s is a more abstract
Cashin then our below s means that our
is a more concrete description of every
product that is described by s how is it
slightly unusual to use relations
between things that are so different as
programs and specifications in fact they
are often expressed in different
notations we can use more powerful
notations in our specifications and we
do in actually writing programs but the
justification is that in lis the algebra
of them is the same and all the proofs
of the refinement relation are the same
no matter whether the operands are of
the same kind or whether they are of
different kinds and going to be used for
different purposes for example
specification and design rather than
just execution refinement can be defined
algebraically in terms of the choice
operator as saying P below Q just means
the union of Q and P is equal to Q since
P is contained in Q its union with Q is
a leaves Q unchanged because all the
executions in P are already in Q now the
law that I have given you can leads
enables me sorry the definition enables
me to prove some theorems on the basis
of the axiom of distribution and the two
theorems I show at the top of this slide
are immediate consequences of the
definite of the distribution of
semicolon through Union but I could also
Express the covariance property by a
proof rule rather than by equations
or in this case in equations can I use
the word equations to stand for any
equations as well please because of
course there's no real distinction every
equation is just - in equations every in
equation is just half an equation the
proof rule says if you know that P is
stronger than Q then you also know that
P followed by R will be stronger than Q
followed by R and similarly R followed
by P will be stronger than R followed by
Q this this rule is a special case of a
very general Prince of prints appalled
engineers have an intuitive belief if we
take an assembly and engineering
assembly say a board of a computing
device and we replace one of its
components by a stronger one a component
that has tighter tolerances and will
accept more variation in the environment
is more deterministic in this sense and
produces a more reliable result if you
change the component improve it in this
way then you can only improve the whole
it's a whole assembly and this is basic
product whenever you replace a bulb or
any component of a system by one that is
better the resulting result will be
better than the original now a proof
ruled out a ssin the proof rule tells
you how to extend a partial proof by
adding more lines by adding more
statements at the end of the proof so a
proof rule comes with a line in the
middle the antecedents are written above
the line and it says that if you have
proved the antecedents then you may also
write any of the
once written below the line as the next
line in your proof so proof rules help
you to write proofs but they are all
based on algebraic laws and they can all
be proved from algebraic laws like this
one can using the definition of the
refinement relation here are the more
proof rules from before the first of the
rules on this slide are a formal
statement as a proof rule of a very
important property of refinement that it
is an ordering it is transitive
and the second states that it is
anti-symmetric that if truth if P and Q
are below each other then they are equal
now let's move on to applying some of
this knowledge to the proofs of programs
I will begin my talk with an account of
how the laws can be used to explore and
prove properties of the programs that
you that you write also describe how
they give guidance to an implementer or
a programming language which amazed the
laws right at the beginning of my career
my academic career I wrote an article
entitled an axiomatic basis for computer
programming which essentially describes
my interest in this subject in 1968 I
moved from a job as a programmer and
researcher in the computer industry in
in Britain to a chair of computing
science at the Queen's University
Belfast my motive for making this move
was that I wanted to conduct long-term
research into the theory of programming
and I realized that my toes and topic of
research the theory of programming would
not be
likely to be put into practical use by
industry for at least 30 years ahead
because 30 years was Dayton which I was
due to retire as an academic at the
standard retirement age I wasn't worried
by this in fact it was exactly a point
that attracted me to this subject of
research because it meant it would last
me all my life as an academic because
once the results of research have been
taken over by industry and incorporated
in industrial products the results are
no longer original and you can't conduct
academic research into topics in which
the results already part of consumer
products the the other reason is that
any further development of the theory
would be conducted in industry itself
using far greater resources that are
available to an academic researcher so I
would have I was protected from this
problem by the fact that nobody would
use results of my research until after I
retired so I wouldn't have to change
over in the middle of my life and nor I
did I actually retired from academic
research in 1999 and very fortunately
took up an offer of a post at Microsoft
Research in Cambridge here I found that
my prediction that the results of my
research would not be used in industry
was entirely correct in the early years
of this century when I was working for
Microsoft the scene changed dramatically
at that time Microsoft windows were
suffering from a series of malware
attacks several of which destruct
disrupted the entire commercial and
economic life of the world for several
days
they were estimated each of them to have
cost the world economy the some around
four billion dollars which Microsoft
thought was rather a bad idea
so Microsoft developed a program
analysis tool exploiting the results of
academic research most of which of
course had been done after I'd started
this line of development and the tool
was able to detect the vulnerabilities
that posed the most risk of repetition
of the malware attacks and similar and
indeed they were installed and they were
used on all of Windows code and they had
the required beneficial effect and
similar tools are now provided as
program analyzers in modern integrated
development environment like Visual
Studio or exchange the basic concept of
my axiomatic basis of computer program
programming was spell was expressed not
algebraically as I would now express it
but rather as in a new notation called a
Hall or triple the purpose was to help
in the proof that all possible
executions of a program q when started
with a given precursor P will exhibit
some desirable property are so PQ are
are the three operands of the triple and
it's written with curly braces as shown
in my definition the definition the
algebraic refinement equation on the
right hand side of that is a definition
which I discovered only quite recently
enables me to translate algebraic
notations into the notations of the
whore triple the interpretation of the
notation is
if P is which is called the precondition
describes what has happened so far and Q
is now started and executed to
completion and the trace of overall
execution when both P and Q have been
executed will be satisfied any
specification are now the rules for the
proof of the correctness of programs
were mostly expressed as proof rules of
the form as I show on this slide this
slide covers the reproof rule for
sequential composition it says that if
you have proved that P followed by Q
satisfies the specification s and s
followed by Q dashed satisfies the
specification R then you can deduce that
the sequential composition Q followed by
Q dashed will satisfy the specification
which has P as its precondition and R as
its post condition now that rule can be
proved quite quickly from the law of
associativity but I cannot prove I
cannot prove the associative associative
law from the sequential composition law
now a calculus of communicating systems
was the subject of research of my friend
and colleague Robin Milner and he
published this his work first in
Springer note in in computer science in
1980 the question that Milner wanted to
answer was one concerning the
correctness of an implementation of a
programming language this question is
formalized by giving a set of triples
which are known as transitions which
collectively form an operational
semantics for the language he gave a
canonical example of the rules in the
definition of an abstract implementation
of his concurrent programming language
or process algebra which came to be
known as CCS a calculus of communicating
systems
now the Miller transitions he introduced
to show how an implementation can
generate a single execution of a program
R because an implementation only needs
to find one of those many possible
executions are depending of course on
its input parameters it doesn't have to
consider all executions which is what I
considered in my triple because they all
have to be correct
the notation Ian introduced used an
arrow with us writing the first step of
execution above the arrow if R is a
program it can be executed by first
executing Q with P is a continuation for
later later execution when P has
finished in other words I could I can
express the I can define the triple by
simply saying that Q followed by P
satisfies our it's very similar to the
whore transition Port render or triple
just that the operands are in the
opposite order R QP instead of P Q P Q R
a Robin Miller did not actually give
this law for sequential composition
because he he didn't need it
hmm but it's a very obvious law it it
says if R can be executed by doing Q
dashed first and then doing s and s can
be executed is it the second antecedent
the S can be executed by doing Q first
and then P then R can be executed by
doing Q - first by doing a composition a
sequential composition of Q - then Q are
still leaving P as the continuation to
be executed afterwards now this law -
can be proved very simply from the axiom
associativity of semicolon but the
reverse equation the rows implication
does not hold so what we have proved so
far in summary is that in the case of
the law of sequential composition the
algebraic law of Association is
sufficiently strong to prove the
validity of the proof rule for the
Milner transitions and also the proof
rules for the hor triple it says the
same thing there was really no need to
introduce a new notation just to express
these two proof rules it could all have
been done with an algebraic law no the
final so this is how the algebraic law
is used is not used or it can be used
directly in a program
optimizer which follows the law or even
the refinement law by replacing an R by
something which is more efficiently
executable and these are validated
directly by algebraic laws and algebraic
transformations and they can be computed
by simply symbolic execution again using
the laws the algebraic laws are also
used indirectly when the Millner
transitions are used to specify or guide
the design of a compiler or a generator
for the programming language and the or
triples can be and the same algebraic
laws can be used in the design of a
program analyzer such as those which
remove the vulnerabilities of from
software that is still being written
today now is it are the algebraic laws
true well my argument here is based on
the fact that there exists a simple
programming language the one designed by
cleany which obeys the algebraic laws
which cleanly called a regular algebra
everybody calls a regular algebra these
days and all I have to do now is to show
that the same techniques can be applied
to the other operators of a modern
programming language of in which there
are great many like conditionals and
loops and so on this work has been done
some of it has been done and some of it
remains for future research but I will
just concentrate on how I can use the
same strategy to deal with concurrency
concurrent composition thank you
I'm going to introduce a new operator
for which say that two programs must be
executed concurrently I use parallel
bars because this is sometimes called
parallelism and the informal definition
of the parallel bars is the two operands
start and finish together but while they
are executing they may communicate with
each other and also interact with their
common environment the algebra doesn't
say all of this but the algebra is all I
believe true of programs which have
communication between concurrent
processes as indeed in communicating
concurrent systems of milna am I going
to treat this parallel combination as
entirely on the same basis as the
sequential compilation Combinator it's
just two ways of executing a program
either concurrently or sequentially and
therefore I'm going to specify I'm going
to formulate the algebraic laws for
concurrent execution in the same ways
that I have done for sequential
execution unfortunately they are the
same laws the concurrency operator is
associative it's commutative as well and
it's idempotent and it distributes
through Union and it has as its unit the
same unit as sequential composition
which is do nothing if you do nothing in
in parallel with something it's the same
as doing something doing nothing in fact
there's nothing now there is a new law
which I discovered only recently well
five years ago perhaps now called the
exchange law which plays a very
important role in the theory of
programming
the exchange law is written as an axiom
and the top of this slide its purpose is
to validate a possible method of of
implementing concurrency by interleaving
of the atomic actions of the two
concurrent threads the you can see that
in a certain sense the left-hand side of
this law is more interleaved than the
right-hand side the right-hand side is
more concurrent than the left-hand side
well when a concurrent program is like
this is is run the actual choice and
which interleaving is executed is very
often made by a scheduler at runtime so
I regard the exchange law as expressing
one of the decisions that the scheduler
is allowed to make when it is executed
or a compiler for that matter I don't
mind whether the choices are made at
compile time or at runtime the
right-hand side calls for concurrent
execution of two threads each of which
is a sequential composition and so on
the right-hand side I have two
sequential compositions marked in in red
supposing the scheduler makes a decision
like this I will run this parallel
program making sure that the two
semicolons which I have marked in red
will be passed by both the threads
simultaneously as though they were
synchronized at this point and that is
my choice as a result of that choice the
actual execution which results is shown
on the left-hand side it is a sequential
composition of two parallel
positions the sequential composition on
the left-hand side represents the
simultaneous reaching of the two
sequential come to semicolons on the
right hand side and therefore everything
that happens before the semicolon on the
left hand side is the same as what
happens before the two semicolons on the
right hand side and those two things are
done in parallel and similarly that's
the P and the Q are executed before the
read semicolon on the right hand side
and on the left hand side and similarly
the P dashed and the qu dashed occur
after the read semicolons on the right
hand side and after the single read
semicolon on the left hand side that is
an informal justification for this law
from the exchange exchange laws is the
most complicated law that I really want
to show you today because it has four
operands three operators on each side of
an equation you very often want to use
the frame law in the context in which
you've only got three operands or even
only two that's okay we have a number of
coronaries of this law which i call
frame laws we which I've written down 1
2 3 on this slide the they are very very
simply proved consider for example
number 1 number 1 is the same as the
exchange law except that it doesn't
contain one of its operands P dashed so
from the axiom we are permitted to
replace P dash by a special case name is
a do-nothing instruction if when we've a
play when we've replaced P dash by
do-nothing we can cancel all the do not
do
nothing's and this will give you the law
law number one on on this side on the
other theorems the third of these
theorems describes the two ways in which
the could the actions of the two threads
can be a to two threads consisting of
two atomic actions can be interleaved
one of them goes first or the other one
goes first
they are both in equations which means
that it is also possible a third
alternative for example two atomic
actions may be executed simultaneously
absolutely at the same time and that is
not excluded by the exchange law or
either of these axioms and it's this
that means that the my model of
concurrency can be given the title true
concurrency as opposed to Sequeira the
concurrency which is defined by
sequential composition sorry
the sixth sequential consistency per
principle which says that concurrent
execution is equivalent I equals the
union of all the interleavings of atomic
actions
there are many it seems very beneficial
to allow true concurrency as well as
false concurrency in my model here's an
example of how to use the exchange law
to actually compute an interleaving
I'm going to write two strings a red
string ABCD a black string X Y Zed W and
I'm going to assume that I've just
omitted the semicolons I'll put them
back shortly
these two programs are executed
concurrently and I would like to see
how to do how to derive at least one of
the interleavings
well the first thing I have to do is to
act like the scheduler I have to choose
at what point do I wish these two
threads to synchronize and I will write
semicolons in those two places that's on
the second line of fear of the slide and
then I can apply the exchange law this
match is exactly the right hand side of
the exchange law so I can move the
components X Y and a to be done in
parallel and BCD done to be done in
parallel with Z double Z W and that's
done on the third law third line by
application of the exchange law now I'm
going to make another decision about
what which of the semicolons the twos to
do the scheduling in fact I can now make
two two decisions one applying to the
left-hand side of the main semicolon and
one to the right hand side and that's
done on line four on line five I've
applied the frame law to the three
operands before the semicolon and on the
last line I've omitted a lot of steps
I've chosen to do to perform X before a
and so that the resulting string begins
with X a you can see that this is an in
to levy we we can get many other
interleavings
of the same string by making different
choices at each step where are we going
to put the semicolons that's an arm
Detroit choice should we apply the law
of kamma tivity of
concurrency before making this step or
not and these choices will affect which
interleaving I get as the result of
course there's no calm utility law for
semicolon and that is why the original
ordering of the two threads ABCD and XY
said W is preserved there's no law that
enables us to exchange any of those as
you can see quite easily from the fact
that I've put one of them in read read
type and the other one in black proof
rules for concurrent composition well
just as I want in the next three slides
to derive a proof rule for concurrent
compositions similar to those needed for
sequential composition the proof rule is
modular just like the rule for
sequential composition in that it splits
the proof of a complex formula written
after the line the consequent into two
simpler proofs which are written before
the land line in the case of the law
that I have on this slide the
antecedents of the proof rule are
simpler because they only contain three
operands instead of six and they're
simpler also because they contain
sequential composition rather than
parallel composition so it's it's win
win is a very good way of splitting
proofs of concurrent programs you prove
the individual threads separately and
this rule is proved from the exchange
law
and this time we can actually prove the
exchange law from the role here I'm
going to give you a proof that the
modularity rule implies the exchange law
it really is very much an undergraduate
exercise I've written the modularity
rule first because that's what the what
I'm allowed to assume improving the
exchange law and my first step of the
proof is to replace the are of the are
dashed by P semicolon Q and P dash
semicolon q dashed the antecedents are
now as a result of this substitution
just statements of the axiom sorry the
the the the basic theorem of reflexivity
of refinement ordering so we have
already proved those we can therefore
deduce the conclusion of the rule but
the conclusion of the rule after the
substitution of R&amp;amp;R dashed
makes the root makes the conclusion
equal to the modularity rules therefore
sorry equal to the exchange law so using
here's a little proof which uses the
exchange law as a proof rule and it
concludes with the exchange law sorry
I'm getting confused for the next slide
it's a little proof that uses the
modularity rule to prove the exchange
law the exchange law implies the
modularity rule you can see the proof is
no longer than the one I've already
given you and therefore since as usual
time is pressing I will omit it but just
summarize if we take the modularity rule
which is shown as the third rule on this
slide and we translate it
using the definition of the Milner
transition in one case on the second
line and the hor triple on the other
just translating the PQRS into our Q P
or P Q R and into Milner notation or the
whole notation they are all the same the
modularity rule I have shown is
equivalent to the exchange axiom and
therefore both the rules and both and by
definition by the very definition both
the hor triple rule and the Millman
transition rules the verification
semantics of your program and the
execution semantics are the same so
summarizing in the case of the exchange
law we have proved it equivalent to the
Milner transition which he used is in
his definition of CCS to explain
concurrent composition of two
communicating concurrent processes and
we use it again to prove the correctness
of the poor triple which is for proving
the correctness of programs again the
regular languages are a support for
believing the law is true they are at
least as a logician say a model for the
axiom so in praise of algebra
I think algebra is wonderful algebraic
laws are so simple they can be stored to
school children they are much
appreciated by mathematicians and by
theoretical computer scientists
algebraic laws are actually used by
engineers for symbolic computations
mathematicians tend to rather down
rather despise symbolic calculations if
they have a proof that they can do by
symbolic calculation they never tell you
how they just say by calculation
I don't follow this example I'm glad to
say they are used indirectly by software
tools software tools use the mill no
transitions to define compilers and
interpreters and they use the hore
triples to define verification of
principles but nevertheless since both
of those are defined defined by the
algebra
they are indirectly they are using the
laws and as we saw we play play a
central role in the unification of the
theories of Milner and whore and so
anybody against let me ask you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>