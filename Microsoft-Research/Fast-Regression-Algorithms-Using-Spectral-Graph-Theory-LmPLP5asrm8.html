<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fast Regression Algorithms Using Spectral Graph Theory | Coder Coacher - Coaching Coders</title><meta content="Fast Regression Algorithms Using Spectral Graph Theory - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fast Regression Algorithms Using Spectral Graph Theory</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LmPLP5asrm8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
alright good morning we're happy to
welcome Richard penguin tell us about
fast regression algorithms using
spectral graph theory
thank you well so the talk will be
structured as follows I'll first start
off by talking about some regression
problems and especially why and how to
solve them so why do we solve solve them
and how do we solve now move on to talk
about fast hours for linear systems and
I now finish by describing some of the
graph theoretic tools needed for such
solver such as treating weddings okay so
let me start off let me start off with a
problem that you have probably probably
all seen in one way or another which is
the learning / inference problem for
this problem you're given a signal
that's potentially noisy but you know
there is some very good underlying
structure to it so you want you want to
extract it out you want to remove the
noise so you want to extract both the
real signal so there's kind of step
function on the right and you only want
to be able to find the hidden pattern as
well so that because this is a important
problem they're helping a lot of tools
that has been that's used to solve it
and the one of the most commonly used
tool is the idea of a regression our so
the regression minimize so a regression
objective so this kind of objective is a
half I put some constraints on of our
vector X and I want to minimize some
norm of this vector so usually the pin
or so p is usually pick to be at least
one for this to be a convex 2224 this to
be a convex problem and the constraints
on X are usually convex constraints so
the simplest form of these convex
constraints are linear equality's so X
is setting some subspace and usually the
X can be either a the underlying
structure orico could be the not noisy
signal so this this kind of a problem
has a many applications and let me just
describe a few of them very quickly so
the first way is probably more
well-known than the object itself which
is blah so so this is this actually this
actually is part of a an area known well
this is actually our key tool in an area
known as compressive sensing and it's
it was the one key objective is this
last webbed active by table shawnee
which aims to minimize the one norm of X
subject to ax equal to ax equal to s the
signal so this has a this was shown to
lead to extremely structured and sparse
output and those are very very grow past
it's extremely resilient of noise so
this is one this has a lot of
applications also the other application
won't talk about is the image processing
so here this is an example of a process
known as possible image processing the
idea is that I have two images that I
that really don't belong to each other
for example a polar bear and the picture
of Mars taken from a space rover and
that is I want to blend them into each
other so I don't somehow merge the
boundaries of these two pics of these
two things so the object that could be
solved is I can find some i define some
underlying graph on these on these
pixels and I and I try to minimize some
energy function depending on the
difference of neighboring pixels and the
ideal value that I want so just
objective up there summing over i J of X
I minus XJ minus the standard boundary
and you ended by solving this you get to
the picture on the right so you can
create these fairly strange settings and
the third application of this is a
problem that perhaps the more
theoretical oriented of us are more
familiar with which is the minimum cut
problem this problem asks to a given a
graph have two special vertices smt and
i want to remove the fewest edges so
that s antique becomes disconnected in
this graph so this is a classic problem
in combinatorial optimization and it's
not very clear that this is a regression
problem but as it turns out you have
formulated as one so consider the
following variation I want to label the
vertices of the graph so that s gets
labeled zero T gets labeled one and I
want to minimize some her all edges of
the difference of the labels of
endpoints of that edge so it's not so
the Commission is still not as clear as
I would like to to show but but one side
of it i think i can try to convince you
which is i can label everything at once
cut to be zero label everything else on
the other side to cut to be one and the
size of the cut is simply summing over
all so any edge that crosses the cut has
a difference of one any other edge has
difference of zero so summing over all
these edges gives me the size of the cut
so hopefully I convince you that these
problems exist in a variety of context
which means that if which means that
there is reason to study very efficient
I'll win for them the exes are strict
201 so you can you get to relax them two
real numbers and there are results show
that if they're the optimal fractional
solution equal to the optimal integer
solution so this is studies Odin for
minimizing this type of objectives are
have been studied for a very long time
in computer science start so ended the
history of these are the algorithms can
essentially be divided into 20 year
intervals during the 40 to 60 s mr.
Robbins known as simplex methods and KKT
conditions we showed that these albums
well these problems are tractable so
there exists our than a converged fairly
fast board in the 60 to 80 s they were
working on a lips with algorithm and a
variety of other methods that show that
these problems can be solved in
polynomial time which is kind of a gold
standard for a theoretically efficient
and the between the 80s in 80s and 90s
there were further developments that
which led to the beautiful idea of
interior point algorithms which shows
that these from can t be solved very
efficiently so general idea of an
interior point algorithm is that it
solves one of these problems using about
square root m newton steps and she is 40
to describe efficiency i'll let em to be
the number of non-zeros you can think
about that the size of the problem and
this is a tilde notation I will use the
height log factors and I will use that
consistently throughout this talk as
well so before I continue talking about
oh and let me first explain why do we
need fast outwards the reason is a lot
of data that being used to that a lot of
the data that come for these regression
problems are big for example this
picture which is kind of almost like a
cell phone camera picture has about ten
to the six pixels so m is at least 10 to
the 6 and if you go to videos or say 3d
medical data
you can easily get to data that's about
10 to the 9 or bigger and end it and if
you look at the key subroutine that's
used in interior point audience is this
idea of a Newton step so enjoy your
point all going to solve one of these
problems using about square root m
newton steps and what is the Newton step
there's a fairly simple characterization
of it which is I'm solving a linear
system so the key bottleneck in getting
fast algorithm for solving regression
problems at least the way we know them
today is we need fast linear system
solves two people to give further reason
that we actually want fast linear system
solvers I'm going to put a i'm going to
use as quote from boyd Vandenberg which
states that if you look at the number of
iterations for a randomly generated the
typical SDP instance it only grows very
slowly as the problem size grows so that
the chart on the right is this figure
that shows that plots the number of
iterations against the log of the
problem size and as you can see it
doesn't really grow much but for the
more sharper sharp-eyed among you you
you may also notice that the size of the
problem only top side I've out at about
10 to 23 so it song about a thousand and
I should good reason for this which is
that we don't know we really don't have
good ways of solving general linear
systems very fast so the inner system
algorithms are actually linear system
solvers are actually a man wonder wonder
one earlier study all burning problems
they will work on this as early as the
1st century and also I think Newton
worked on is also happens way one of the
most rediscovered algorithms in that the
oven of that were discovered is known to
ask also nomination and the in modern
terminology i order n cubed algorithm it
was shown by a strategy that is this
exponent of 3 kind to be lowered to 2.8
many work followed getting to the state
of the art today which is about two
point three seven two seven but if you
think about this number even if you
analyze in conjunction of in conjunction
with interior point algorithms we're
still looking at a quadratic time out
with and what and what's more
problematic about this type of approach
is that
these albums also need an M Squared
space which is even more problematic
when you're at about 10 to 12 so what
what's often used in practice at least
in practice today are actually metal
that get around this issue so the method
that's often used in practice our first
order methods so some type of coordinate
gradient descent or subgradient methods
so these methods have the nice property
that the cost per iteration is very low
but they do try this for slow
convergence and also what's happens in
them is that they run force for a number
of iterations and because of time
constraints they're cut so what me to
think about these ovens is that they're
trading essentially a solution quality
for time because time is such a crucial
a crucial resource for these out for
these minimization problems on the other
hand there has been some work recently
which show that at least for some
instances we can solve linear systems
extremely fast this is due to a work by
experiment 10 12 and four who showed
that for linear system thats related to
graphs you can solve the mean nearly
linear time roughly and Polly locket I
will get our top talk more about this
algorithm in 12 slides but for the but
first let me explore some of the
consequence of this so so the
consequence of this now is that first we
have that that these the second order
methods that you does that usually in
your system self have a have a low
complexity so have a low number of
iterations on the other hand these
linear system solvers gives us a
low-complexity per step as well so if
you combine these two albums together
you can get extremely efficient
algorithms with fairly with very good
guarantees as well and in the setting of
graph hour winds this led to the idea of
a laplacian paradigm so some
representative works there are is a work
by eight I shan't Spielman show the
state-of-the-art minimum cost flow
algorithm runs about em to the 1.5 time
and also there was a more recent work by
Christian on Keller matter experiment n
who showed the state-of-the-art
algorithm for approximating maximum flow
and minimum cutting graphs so these also
this all when writing about em for the
Four Thirds time so ups so some boats
the work that I've done in this area are
related to extending these albums to the
more general setting and regression
graph-based regression and the first
extension that I've worked on was a
droid working a joint work with wuhan
cheering alexander modhri and gary
miller what we showed that it's possible
to extend it there they are all with to
the more general type of tour to the
more general classes of regression
objectives that come out of image
processing specifically we showed that
it's possible to solve the group the l2
objective which is a close cousin of the
group lasso objective and this this
turns out to capture almost any image
processing objective as convex that we
could find so here is an example of a
denoising an image so by setting up the
objectives correctly you can first
remove some of the small noise and if
you overdo it it also get to the image
on the far right which is a cartoon-like
version where the person's ear is
considered noise as well and also
removed on a more theoretical citing a
drunk or a joint work with John Keller
and Gary Miller which oh that's possible
to extend this I want to solving okay
commodity flow which is that you're
trying to route you're trying to ship
goods through our network but these
goods aren't exactly exchangeable so
think about reckon about apples and
oranges through the same graph and this
problem actually is very relevant to a
lot of the practical regression
objectives because they're related is to
multivariate labelings of graphs so
instead of labeling every vertex on a
single number as in a minimum cut
setting you're assigning essentially k
labels for each vertex and minimizing
some objective depending on their
differences and a more recent work join
with with my advisor gary miller we
showed that for the more structured
graph such as the graph that you get out
of images or videos so basically any
graphic with a good separator structure
you can actually speed up the channel at
all framework to even faster running
times so this is probably also relevant
for a lot of these image related
applications because images they do tend
to have fairly good separator structures
so the so the implication of fast
solvers with if you take these
generations do into account is that they
give fast algorithms for solving a
variety of these graph-based regression
objectives and at the same time as I
will talk about later they also lead to
very they also be two parallel
algorithms
okay and there are actually a variety of
other applications of fast hours as well
such as such as planar embedding solving
finite element systems and get obtaining
good separators for graphs so finding
good balance cuttin graphs due to time
constraints I will not be able to talk
much about them I'll be happy to talk
about these I'll fly ok so for the next
wed i'll try to talk about how does how
these past hours work and the problem is
that given a matrix a and a vector B I
want to find a vector X such an ax equal
to B so to describe running time I will
say is that i will let n to denote the
dimension of a so as m by n matrix and
it has em nonzero entries so it's the
problem size but because because a is
related to a graph it actually has a
very specialized structure and the
structure is is that it's a graph
laplacian so there are many form of many
possible formal designation of graph
laplacian the simplest one is that it's
the degree matrix diagonal matrix can
set containing the degrees minus the
adjacency matrix of the graph till the
way to think about it is that for each
entry ki jai this entry is the degree of
the vertex if I if it's on the diagonal
otherwise it's the negation of the
weight of the edge so it's negative w IJ
so consider this this is the example of
a graph laplacian the left the graph
below is the graph corresponding to it
so here the this vertex the fur this is
the first vertex has degree 3 3 edges
coming into it so it responds to its
entry of three in the matrix this first
edge corresponds to those two negative 1
entries because the grab the matrix part
buses where the matrix is symmetric so
you put one on both sides as well so the
next edge this then this edge goes one
through these two negative ones and this
one corresponds to the third there is
actually extension of graph laplacian in
numerical analysis know as a symmetric
Divac dominant matrices it was showing
work by grandma Miller that
a solving graph laplacian essentially
equivalent to solving a CD matrices so
for in terms of stating the result this
is often talk talk as SD solvers for STD
linear systems but in terms of
algorithms it suffice to just think
about graph laplacian and then what the
issues are with dealing graphs are the
WI JS once or maybe different for every
entry they can be different for every
these are weighted graphs in which case
it's not the degree it's just the row
sum right yeah it yet it's a weighted
degree yes so for I I think it's all
various points in this talk I will
actually go to unweighted grass for
simplicity but please all generalize to
weighted cases so the one thing is
challenged with dealing with graphs is
that they can be very unstructured so
for example this is the graph of the
corresponding to a social network these
are graphs that at the top level look
like almost like complete graphs you
know there is very little good
partitioning that happen but we know
that there are some underlying structure
under there and the other problem is
that if you're using this kind of
algorithm inside other more
sophisticated routines the intermediate
linear system that's being solved
because they're so few of them almost
captured the entire difficulty of that
optimization problem because we know
that these regression probs are
difficult which is why we want to solve
them but difficulty has to go somewhere
so in some sense a lot of difficulty of
the problem metric goes into solving
this linear systems so the the result by
a Spearman 10 formal it can be stated as
given an M by n graph laplacian em
nonzero entries such that I know it I
know that the exact answer is X so that
I know that being equal to ax for some
vector X I'm going to this always be
able to find you an approximate solution
to some close to some epsilon such as a
rustling nearly linear time so notice
that this autumn is approximately nature
in that there is a error guarantee that
happens there but if you look at the
running time the dependency on epsilon
is on a log 1 over epsilon so in
sometimes it's a very strong
approximation and by nearly linear I
mean M poly log n so log on to the power
of sea for some constant C
so also the error is being measured in
the a norm so you can think about it as
the norm being induced by the matrix so
it's the square root of the transpose
this G H it should be a a by the way and
so this is this is a great algorithm
from a theoretical point of view but
there's so but from a practical point
there's a caveat to it which is that
considered so notice that there's this
log n to the power of sea that's in the
running time so personally I've never
been able to track down what exactly the
value of C is so I'll just resort your
authority for for some estimates so
that's meant by a 10 30 men is 70 by my
advisor Gary Miller is 32 co-author jana
chuda stays 15 chaahat NSA is 12 and the
lowest i've heard is from Lorenzo reikia
which is 6 so consider the case where m
is about ten to the six so n is 10 to
this you're dealing with a million by
million in your system here log in to
the power 6 is more than million so this
is this algorithm in for a lot of the
practical instance that we're dealing
with is essentially a quadratic time out
with so in some joy into one works with
a with the audience coders and Gary
Miller we showed that essentially this
the same kind of guarantees can be
obtained with running time dependency at
roughly M log squared in and also in a
working in tossing 11 we showed we can
get to em log so we essentially improve
the runtime exponent to a single log and
order for the next bit of the talk I'll
try to describe as briefly how this
algorithm works so the album has
essentially has three key components
iterative methods graphs power suppliers
and low stretch embeddings I'll talk
about talk mostly about that at the last
two which means that I'll try to give
you the two-minute version of iterative
methods next so iterative method is one
of these great ideas from numerical
analysis which states that if I want to
solve a linear system in a in a matrix a
I can solve it by solving linear systems
in B and iterating so I can
of it by making us number of calls to a
system is similar and solving that
system instead the idea of similarity is
defined through a spectral spectral
conditions which is that which is this
written using this this squiggly
spectral notation so the less or equal
to respect or less or equal to formally
this means that formerly the spectral
less or equal to means that for any
vector X the a norm of X is less than a
normal than a B norm of X but but but
all the use that I will make of this
special notation in this talk I think I
can appeal to your intuition about
scalars in that the operation that will
do to them is essentially manipulating
them in ways that that's almost
identical to many plane enter the less
than with the less or equal to sign for
scalars and one of the key ideas in
these solvers is that because a is a
matrix that's obtained from a graph we
might as well pick P as a graph and
analyze this kind of spectral less or
equal to conditions spect using graph
theoretic notations so I will replace a
with G H with lb with H and deal with
grass from this point a now so because
we want to find an easier age and we're
dealing with graphs there are two
obvious candidates for considering
what's easier graph the first one is
fewer vertices the second one is fewer
edges as it turns out we can reduce
vertex count if the egg count is small
so I will just focus on focus on getting
H with a smaller number of edges after a
tree is similar to G so what I want is I
want graph sparse fighters these are
sparse equivalents of graphs that
preserve some property these objects
have been studied extensively in
computer science they met in various
forms examples are spanners which
preserve distance or diameter of your
graph cuts parse fires which preserves
the value of all the cuts up to some
small factor but what we actually need
is spectral sparsa fires which seems to
preserve the
values and eigenvectors so essentially
the spectrum of the graph formally what
we need is an object Nolan's ultra
sparse fire which is for a graph with n
vertices edges along with a parameter K
I want to graph with n minus 1 plus M
poly log n over K edges such that these
two graphs are within a factor K of each
other so essentially want to reduce the
number of edges by the same factor as
the similarity factor but i'm willing to
lose some logs poly log factors as it
turns out this poly log factor is the
test direct dependency on running time
it was shown by spearmint n that this
that getting ultra sparse fire with this
type of quality imply solvers with with
essentially the same exponent of log in
a running time also the order K that
will be used is slightly larger than
this poly log so it's rough so it's too
so the valley ok that's used for these
algorithms in the US parent m framework
is setting k2 to a poly log factor so
it's a mildly sparser from a theoretical
point of view but it's still more that
constant factor reduction in a problem
size and the dispute my time working
essentially be viewed as obtaining ultra
sparse fire constructions for a fairly
large value of P so ultra specifier for
a fairly large poly log factor before we
continue further let me just do a sanity
check and consider one example of a
graph that we want to build altars parse
fire for so this graph is a complete
graph so those record formula with
solving linear systems in complete graph
you're probably aware that today this is
a fairly easy problem in a sense I can
just read off the answer but let's just
say we want to get ultra sparks of fire
for it I claim here random works in that
I just pick about n log n random edges
but rescale these edges so that the in
expectation the total weight is roughly
the same so we scale them up by a factor
of roughly and over log in and it can be
shown that with with high probability
the resulting graph is going to be a
ultra sparse fire so it's going to get
about n log n edges which is far
the factor and also its actual constant
factor approximation so the but this
only works for comfy grass because all
the edges are roughly the same so for
general graphs we do something slightly
different which is that we want to have
different probabilities for keeping each
edges because edges aren't exactly the
same as each other anymore so the
general graph sampling mechanism this is
due to due to a framework by vengeance
or kharghar is that I want the sample
address such as each edges cat with
probability P of e I want to associate a
probability value for each edge and I
want to assemble these edges with our
probability and if I do pick that edge I
want to scale it up to maintain to
maintain the expectation so I want to I
want to scale by a factor what roughly 1
over P so that in expectation I got the
same edge back so there are a few quick
observation to be made about this
framework the first way is that the
number of edges that's kept is equal to
the sum of the probabilities just
because in expectation I'm getting each
edge with probability P of e and what we
need to do is we need to prove the
concentration of this higher sampling so
before before try to describe the
concentration result I need to take a
slight detour and describe what a key
quantity has need it for measuring to
this concentration which is a notion of
effective resistance in graph so this is
another one that those notions that
handy defining of a bunch of ways and
the physics definition is that I'm going
to view the graph at a circuit so each
edge is a contact is a conductor which
conducted according to its weight and
the way you measure effective resistance
is you literally take a voltmeter and
you plug the two endpoints into those
two vertices and you could run one unit
current from one vertex to the other
vertex and enter the resistive value
between these two vertices it's just a
number you read off your your-your-your
voltmeter in general the way you compute
effective resistance is that you solve
you solve DX equal to some indicator
director and you read off the difference
in its solution of the two between those
two vertices this is something that I
think is the
parting in 11 but I don't really
remember it and I think for I could
remember this this leads to fairly
complicated calculations because once
you're asked to do Gauss elimination on
5 by 5 matrix and a test leads to some
horror stories so let us consider a
simpler version which is that I have a
single edge here the effective
resistance of this edge is 1 over the
weight of the edge and also i will say i
will use some some rules about
registering series which is that if i
have two resistors await a chain
together in series one after the other
the resistive the relative value of
their combination is just as some of the
resistive values of those so sort of two
rules that that's going to be helpful is
that resisted value is inversely
proportional to the weight of the edge
and if I have a chain of resistors the
the resistive value of them at a single
component is just some of the resistive
values so now I've revealed a little bit
of definition I can introduce the rid of
experiments very vast about what they
showed is that by letting the sampling
probability equal to the weight of the
edge times its effective resistance time
extra log M factor this will give you a
graph that that's within a factor of two
of the original graph with high
probability so some probabilistic issues
I'm hiding on the rug but what's more
crucial is that is this fact about the
sum of these probabilities there is
amazing fact which is that the sum over
all edges of the wait times effective
resistance in a complete graph it is
exactly a minus one it's a bit of an odd
result but consider is some let's
consider some simple cases concertation
where the graph is a tree every address
unit weight the only the only for each
edge the only path is between two
endpoints by itself so each edge edge
has effective resistance one and if you
deal with a graph you have a minus we
have your doom the graph that's a tree
you have a minus 1 edges so you get a
total sum of a minus 1 what's amazing
that this is actually true for general
graphs so and the implication of it is
that every graph has a spectral sparsa
fire with n log n edges so
so we constructed spectral spar spiders
but we haven't really gotten to ultra
specifier or solvers and the reason is
that there actually is like have yet
which is what I which is a century of
chicken in it chicken and egg problem
the issue is how do we compute effective
resistance notice that as I mentioned
earlier computing factor is you need to
solve a linear system in general for
general graphs and this was later shown
to be the case this was actually the
case in experiments reversible work as
well so the way they up they obtained
effective resistance is a they use the
solver to solve linear systems but if
you look at the experiments and sober
what they need is they need spectral
sparsa fighters so let's come back to
yourself very quickly so most of our
work can be considered as a way to work
around this issue and there were two
things that we do to do this the first
is that instead of using effective
resistance we use fairly a fairly crude
outer bounds for them and a second thing
we do is we actually modify the problem
to one where we actually can get fairly
good upper bounds so to get upper bounds
we use a result from electric
engineering known as we're ice mountain
in Italy or otherwise known as I just
unplugged my computer which is that if i
remove edges from a graph or if i remove
components from a electrical circuit the
thing will stop to stop working
eventually which means have made it the
resistive valid go to infinity at some
point the slower version of this is is
is formally is that if i remove a
components from electrical circuit the
realistic value bring two vertices can
only increase so to get other bands for
effective resistance we can actually
measure effective relations with mr.
expect to only a part of the graph and
where do the most aggressive version of
this which is to calculate the effective
resistance with respect to a tree so as
so we don't want the infinite values
because that's really bad for sampling
but what we can do is we can measure
them with respect to a tree so we toss
out all edges except a small part of the
graph and here to measure in fact a
resistor in two vertices only the path
between the matters so these two edges
hanging off the
off the path they don't really help for
effective resistance so all we need to
do to measure the effective resistance
is count is to sum up the resistive
values along the path turns out the
section known quantity this is known as
the stretch of an edge with respect to
the graph and what we need is we need a
tree for the graph such that the total
stretch of all to all the edges is small
and what's even more amazing is that for
any graph turns out there exists a good
tree so for any graph there exists a
spanning tree for which the total
stretch of all the edges is roughly M
log n this is hiding a lot of all gun
factor and i'll get i'll get to more
details about this in 12 slides again
I'm just summing over all that this is
given to you something but all you gotta
do they could I guess this in the
spiritual haces on floor or every eight
play look for some sense so there is
this expectation type guarantee which is
a stronger guarantee and the it turns
out that the from a from a from a metric
embedding point of view these are
interchangeable notations these notions
but for in terms of getting a fast
algorithm is that you it's actually
Saudi you always have we know to date
welder this hour the M log n algorithm
it's a crucial that is the weaker notion
of some and is actually open to get a
expectation but it's probably possible
to get expectation as well so what is
going to imply though is that our upper
bound for effective resistance so our
upper bounds for wait times effective
resistance that's going to some to M log
n which means that if you combine with
sampling you get about M log squared n
edges back which is more than the graph
we started with so this is the problem
and the question and the this actually
this this situation is actually most
evident in the case of an unweighted
graph who every edges unit weight so
consider this tree
in red I'm not sure even how clear it is
but if you use this tree to stretcher
that edge that I'm pointing to because
it's taking two edges to reach it azle
is a length two paths so that resistor
value is 2 and and because every Edge
has length 1 the the shortest path you
can get me any two vertices is one which
means that the lowest sampling
probability we're going to get if we do
this to our weighted graph is at least
one which really doesn't mean anything
because if you sell for everything with
probability 1 you don't get the same
thing back so the question is this we
even a good tree so let's slip out now
let just step back a bit and see what
we're missing with this kind of approach
so what we needed was an ultra sparsa
fire so this is something that has a
factor k difference with the original
graph but has about em poly log n
divided by K edges when we just
generated has about em n minus 1 plus n
log squared n address the N minus y I
just put their just just to make them
look about the same but if you recall
the Spielman's reversible guarantee they
say that G&amp;amp;H are within a factor of two
of each other so it's a very good
approximation and the other thing that
to notice is that we have not used k at
all so the what we needed has this
factor of K but we have not been we have
not introduced you may introduced okay
in our bounds so we need to use case
somehow and I propose that we just use
it in the simplest way possible which is
a notion with stew which is going within
this notion that the tree we got is good
it's a good part of the graph so let's
make it heavier by a factor of K and
because by doing this we're going to get
a different graph G Prime and this graph
is going to be waiting a factor K is a
factor K approximation of our original
graph and now I claim something good
happens so now eat your eggs in the tree
is that having wait what has weight 1
over K has weight k
means that they resisted but because
resistant values are inversely
proportional to the edge weight these
two resistors now have resistance one
over K each so you can look look at the
same edge its stretch now went down by a
factor of K so this is actually true for
any edge and what you get is that the
tree based effective resistance between
any two it between any two points now
decrease by a factor of K so what we
need to do is now is to do a counting
and we give that we're going to pay for
the tree edges because the tree edge
they didn't really change themselves so
they got heavier but their path also got
heavier so they're still going to be
there so can I get n minus 1 tree edges
but the off created is all that
probability now went down by a factor of
K so we're looking we're looking at M
log squared n over K edges now and it
once example you're going to get a
partial graph but the issue is that we
kind of cheated in that we modify the
problem so we we didn't really sparse by
G we sparse fight G prime but notice
that we have these two inequalities we
have the g and g prime are wasting a
factor k of each other and the g prime
and HR waiting a factor of two so a
plane notice of my intuitions you try
what is the typical size of k is it
smaller than two bigger than two big
very big number very close to one what
is it so it's it's like poly log n so
think about as about a thousand or so ok
so it's a big number and it's that it's
small compared to him yeah it's very
small compared to em but it's much
bigger than most constants
so there are actually more gradual
versions where smaller constants of K
are used but to describe the i wouldn't
i think there's the easiest theoretical
description of the algorithm uses a
reasonably big but not too big value of
K so if you combine these you get that
GNH are within a factor of 2 K of each
other this is just appealing to your
intuition about scalars and if you look
at the what we needed we need ultra spar
suppliers so we need about M log n to
the to the p over K edges and a cape and
a factor K difference what we just
generated we generate H that's within a
factor of 2 K of G and we got about M
log squared and over K edges so if you
look at these two these two definitions
and you look at those two conditions
they're up two constants they're roughly
the same condition with p equal to two
so will you what you obtain if you
combine this with the framework given by
Spielman 10 is you get an oven that runs
thing about M log squared n time and it
turns out you can do even better by
taking the notion that the tree is good
even further and idea there is that
instead of me instead of on the smaller
graph so you have now got to yourself to
a smaller graph instead of computing
another tree of another tree from this
from scratch you can compute the tree
you can reuse the tree from the current
iteration are using district you can get
to a m log n time out with there were
some some extension that we have done to
the with this work as well enjoying work
with Jana scooters and Alex 11 we show
that a mildly dense graphs can send to
be specified in linear time also in a
joint work with with with Karen Miller
we show that it's possible to extend the
sum of some of these ideas to general
matrices so the idea there is you want
to find for jet perform general positive
semi-definite matrices simpler versions
and you want to do this kind of
specification you have a proportion to a
solve so dealing with fairly you're
dealing with a class of matrices is
formed by outer product products of tau
and thing matrices
which happened quite a bit in machine
learning so too to summarize solvers
what happened here is that by using the
ideas from spectral graph theory we were
able to construct similar graphs but
will we and using these similar graphs
we're all able to obtain extremely fast
I'll winds for solving linear systems
but if you think about this algorithm
there's a key backbone in its algorithm
which is that we need these good treats
for grass so to get these fast overs a
crook a critical component are these
elements from comment or a graph theory
which aims to to to generate these good
trees and for the next two and further
the resident of this talk I'll talk
about algorithms for getting these good
tree embeddings so the two the two these
trees in in theory are known as low
stretch spanning trees and what they are
is that the same the Ricola the sampling
probability of an edge equals to the
wait times the effective resistance of
the tree path so for simplicity that let
us consider unit weight case again so
here the stretch of egg is simply the
length of the tree path connecting two
end points and and the low stretch
spanning tree is just one where if you
sum over all edges there n the distance
between their endpoints is fairly small
so I claim this is fairly different than
most of the definition of spanning trees
that were used to a good example of this
is the square mesh so the square root m
by square root M square mesh the first
tree to consider is the hair comb which
is taking a first column and every
single row I claim that because the
edges are is unit waited this is a good
tree from a lot of the sense that we're
used to so is a max weight spanning tree
it's a shortest path tree if you start
from the say the top left vertex but I
claim that the stretch of this isn't
that good actually for quite a bit of
the edges the stretch are good so for
all the editors on the left call
the second left column the stretch of
these edges are three so you just take
one left go one up and go around well
but the problem happens when you cross
the midpoint because once you cross the
midpoint you ought to go all the way to
the left go to one down and come back so
the stretcher of half the edges in the
graph is about square root of n so you
get a total of roughly n to the three
halves and as it turns out the tree
that's good for the grid is this kind of
recursive see construction so you want
to part it in the graph up into smaller
diameter pieces and then combine them
together to join to join these trees and
there were a lot of work done on this
starting with a work by Alan Carr palak
west and the city art results are
combines two works by al khazim
experiment n and Abram Bartel naman
mucho that it's possible to get
spreading through the total stretch
about M log n for any graph this
actually the state of the art when we
were working on a solver and also is
hiding a la gaga in fact but if you look
into look closer at these ovens you
realize there is a slight problem with
them well from an efficient solver
perspective which is their running time
is roughly am block squared and this M
log squared n can be explained by the
simple explanation is that you're
essentially running about log and
shortest path on this graph so you're
taking apart the graph running for this
path and repeating and you're doing us
Bob log and levels and then the running
time is essentially for this path times
log in but to get a to get a faster so
we're all withem because our sovereign
training about m log n time which you
need to get around this bottleneck as
well so the way we did this was we we
drew drew some ideas from a work by
Orlin madurese obrim subaru mommy &amp;amp;
williamson who showed that if a graph
has only kate distinct edge length so
has a small number of edge lengths so
recall that BFS our graph is about
linear time so what they showed is that
if a graph has about a distinct edge
length the running time of forest I'll
can be reduced to about em la que and
will be the way we took the way we use
this fact is that we read we
rounded all the edge ways to powers of
two so if you think of a polynomial a
boundary edge weights yo graph now has
about log and distinct edge weights so
in short it has blowing about M log log
n time so get a total world class
roughly of M log n ignoring poly log
factors but well we actually made some
improvements to this ordering a doll
result as well in a way to actually
improve the improve this M log of M over
N ok to actually a M plus an L oo K so
almost a kind of the Fibonacci heap
style band so generating these trees
actually is even more interesting from a
parallel perspective and the reason from
a parallel perspective this is
interesting is due to a work work joint
with a guy block anupam gupta jana chuda
scarran gary miller encanta Wonsan what
we showed that the spearmint an ultra
sparsa fire framework can be readily
paralyzed to roughly about em to the
one-third depth and nearly linear work
if you combine this with the the
laplacian paradigm for getting graph
algorithms thrown earlier the result is
that you get theoretically fast parallel
wooden form any graph problems and
somewhat were surprised this was
actually a very very difficult thing and
the state of the parallel graph
algorithms before our work was that the
state of the art of parallel one for max
flow actually had a higher runtime depth
had a higher time then a total work by
the goldberg rao algorithm which is the
state of the art max flow out with and
what we obtained will actually parallel
algorithms whose work are close to the
study are sequential and have a
reasonable depth so the parallel death
of these albums are roughly about em to
the two thirds and for an in terms of
parallel graph albums this is actually
very interesting in that our current
existing frameworks for getting parallel
algorithms so for example that well the
most use mine is the MapReduce framework
and to Kota Sergei Vasiliev s key from
google that these frameworks actually
seemingly not very good for graph
computation
so this actually leads to some ideas of
getting interesting ways of doing doing
parallel graph graph albums on massively
parallel scale and this cool leg was
actually made in the context of specific
more specific specifically to one
problem which is the parallel shortest
path problem directed graphs so I want
to compute Sadie a BFS and directed
graph actually even computing reach
ability SG which ability of entirety
graph I don't think there are methods
known to give speedups over sequential
outward but but if you if you then look
at the framework by al khazim XP on 110
what happens is that the first step
using this algorithm as I mentioned
earlier is that your shortest path so
generating these trees in parallel it
turns out to not be exactly easy and we
we got we were able to get these
embeddings was we was of several we did
several things we first went and looked
at an earlier idea by Alan Carr palak
West so there's actually the first loss
for spanning tree construction and then
we combine that with the idea of
repeated local clustering of the graph
so you repeatedly cluster our small
pieces of your graph combine them and
cluster again also use sample at
different densities to find these
clusterings so these were ideas that
we're used in approximate parallel
shortest paths an unpaired graphs by
cohen and the pictorial version of this
all of the attention looks like the
followed we have a graph vitam centers
jump off some pieces of the graph get
rid of them find more centers jump off
the rest of the graph so with this we
were able to get a good parallel
algorithm for solvers and therefore a
lot of these parallel graph algorithms
as well so the big picture to
something's up though is that too so to
solve a lot of these graph-based
regression even comment total graph
optimization problems we need fast
linear system solvers well fast linear
system solver is a good way of getting
these algorithms but to get these fast
linear system solvers we also need a
variety of combinatorial tools from the
graph embedding literature and I think
some interesting
questions for future work are can we get
better regression algorithms can we get
a faster and more parallel solver does
so one one one way to view a parallel
solver is i carry it's a sparse
representation of the inverse of a
matrix the sect is not the case in the
current of the current parallel
paralyzation socially question is can we
actually get a sparse and approximate
pseudo inverse of the matrix and also
can we extend solvers to other type of
systems that's all any questions
we can take this offline because we
really need separately but I'm very
interested in the kinds of things that
arise in image processing which are quad
meshes you know you can think of them
this planar graphs things like that you
mentioned you have some recent results
so i can ask you about them unless you
already have slides the other thing is
the theoretical results for a regular
mesh is the order and multigrid software
so it seems like there's still a log and
what's the current gap between these
tree based approaches and multiplayer so
far so for planar graphs there was a
work by by by my two co-authors yeah
let's go to thank every Miller with a
new show that linear time algorithm but
for sport is regular planar meshes and
there were demotic with albums are are
shown to be to run extremely fast
because they they run what's called a B
cycle so they're recursive structure is
much more compact than our algorithms so
if you so they're actually have so jana
sangh Direction has some code which
combine some of the ideas from multigrid
solvers with some of these ideas and my
impression is that to get to get fast
algorithm for these well-structured
graphs probably ideas from both of these
needs to coming at some point and and
also i think they have a fairly
efficient solar packages kind of scene
still combinatorial multigraph worker
it's you it's it's updated version of
the CMG silver ok because Backman did
not have order and as the graph to the
larger
the question</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>