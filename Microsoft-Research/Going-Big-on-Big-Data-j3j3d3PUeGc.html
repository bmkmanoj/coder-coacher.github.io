<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Going Big on Big Data | Coder Coacher - Coaching Coders</title><meta content="Going Big on Big Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Going Big on Big Data</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/j3j3d3PUeGc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it's my great pleasure to introduce Sam
Madden today so Sam was actually in this
area for a completely different reason
he came for a wedding but we managed to
sort of hijack him to give you a talk
not my wedding now my way little
introduction he's a professor at at MIT
in the sea sale in the computer science
department his won numerous awards
including the TR 35 award NSF Career
Award and then of course a whole bunch
of best paper award and probably many of
you have seen who attended Sigma this
year also heard the talk that he gave
when he received the the tenure the test
of time award at Sigma this year for the
work that he didn't is pH D at Berkeley
on the tiny TV work so again so great to
have him here and over to you today he's
going to go big on big data cool nice
perfect so I've given this talk at kind
of silly title giving going big on big
data I know we're probably all it's
pretty sick of hearing about big data so
I won't spend a lot of time talking
about it i'll give just a little we have
been running an initiative a couple of
initiatives at MIT under the heading of
big data this big data at csail and we
have a big grant from Intel that do work
in the big data space so I'll talk just
a little bit about my perspective on why
big data is actually an interesting
thing and not just sort of an overhyped
trend before I start maybe I'll say just
a couple words about csail for those of
you who don't know about see sales so
see cell is the largest
interdepartmental laboratory at MIT so
that means we're about a thousand people
about a hundred kind of professor
professor level people about five to six
hundred graduate students at any point
in time mostly focused on sort of
computer science very broadly defined so
csail is computer science and artificial
intelligence laboratory it's everything
from robotics to hardware to HCI much
like say Microsoft Research so it's a
big place and it's you know pretty fun
and exciting place to be a part of it so
um we all know you know what is this big
data thing it's explosion of data coming
from every possible source of
you know things in our lives from our
cars to our smartphones to our computer
games to our government to our cities we
have a tremendous amount of data way
more data than we know what to do with
so that's where big data comes from you
know this digitization of information
the tremendous availability of sensors
from you know things like smartphones
and location-based devices access to
cheap computation and just increasingly
connected devices the sort of Internet
of Things so I listed some apps here but
rather than that maybe I'll talk about a
couple of applications that I think are
cool big data applications not
necessarily for my own research but that
people around csail are working on just
to give you a flavor of some of the
problems that we're trying to solve in
this big data at csail and then I'll
really spend the bulk of the talk
talking about my own research but I just
kind of want to it start off with some
big picture big visions guy kinds of
things so his first example is from
somebody named Jim Michaelson so Jim
Michaelson is a mathematical pathologist
at the Massachusetts General Hospital so
what he's done is assembled what he
claims is the world's largest cancer
patient database so this is a database
actually a sequel server database you
guys will be happy to hear that contains
information about 173,000 cancer
patients and it has a bunch of
interesting information in it all their
medical records all the treatments that
they have received all of the billing
and sort of insurance reports if they
died it's linked to the national death
registry there's something called the
National tumor registry which provides
information about the type and size and
location of all the tumors in these
patients so it's a pretty interesting
and rich data increasingly is starting
to have genetic information in it anyway
Jim was very interested in the problem
of medical costs and so he started using
this database to try and understand
where medical costs go to in the
Massachusetts General Hospital and this
is I think this is an interesting
example because just to produce this
graph took Jim and his team about three
or four months he's not a computational
person he did a lot of you know putting
data sets into Excel and making
different plots and running different
regressions and it was very very painful
and one of the things that he's come to
us to ask is no can you help us to do
these kinds of analyses over this data
in a much more
efficient way so that's where we're
starting to collaborate with him but
this result is his result that he came
up with so he asked the question wide so
this is the distribution of costs to
treat lung cancer patients at the
Massachusetts General Hospital so the
median patient you can see here costs
about thirteen thousand dollars the 90
90th the top ten percent of patients
cost around sixty thousand dollars so
there's like a factor of four spread in
the from the median to the most
expensive patient does anybody want to
guess what the difference between these
patients is so it turns out these guys
all die with about the same probability
lung cancer actually is a terrible
disease people don't mostly don't get
better from it I don't believe I don't
know that but I don't believe that that
is a correlating factor here one more
guess whether they're good yeah I don't
think it has anything to do with their
you know lung cancer predominantly
doesn't but I think doesn't affect kids
mostly so these apparently as far as I
understand it no and they and in fate so
the other thing they offered you might
think I might be related this is the
sort of stage of presentation like if
somebody comes in you know very sick
versus not sick they might cost more or
less turns out that doesn't matter
either it what's that it is the doctor
in fact so it turns out that the thing
that is that there if you have there are
about six doctors who teach you treat
the lung cancer patients at the
Massachusetts General Hospital if you
have dr. Jones or dr. Smith not their
real names you will cost four times as
much money okay so why because these
doctors have a practice which is kind of
a weird practice like you come in and
they will give you chemotherapy give you
x rays every time you come in whereas
the other doctors are only doing this
one out of every three or four times you
come in so these doctors just have
different norms for what they think is
the right way to treat patients in this
particular case these you know dr. Jones
and dr. Smith's treatment regime is not
actually more effective so this is I
think kind of an interesting result it's
also you know it's a data-driven result
and it's you know increasingly our
feeling is that medicine and these kinds
of analyses even you know everybody's
talking about genetics and I think
genetics are pretty interesting but even
absent genetics there's a ton of these
interesting kind
analysis problems and these people like
Jim Michaelson really really really want
computer scientists to help them solve
these problems so it's actually that's
been a really fun fun collaboration that
we've kicked off their studies
statisticians but you know I think that
to me this is a computer science problem
because this is this huge multivariate
database right and there's this search
problem over you know what combinations
of variables are correlated with these
you know outcomes that we're seeing
whether their death or survival or you
know cost or a whole bunch of other
things and you know there's all these
theories we can each take one of these
theories but if it takes you know a lot
of manual analysis to test those
theories then that's not probably a very
good way to think about this and they
don't apparently have tools that make it
easy for them to kind of automate this
search through this big database it may
be that this is not you know I'm not
sure that this is the world's hardest
computer science problem but i think it
is actually these these kinds of multi
multivariate analyses are probably
non-trivial to get right and my guess is
that a lot of the explanations here are
in fact complicated ones that are you
know many different things interacting
with each other all right so here's
another example and this is a slightly
different example this is the el taco
let me let me tell you what the the
graph is showing here and then i can
talk a little bit about why i think this
is an interesting sort of computer
science big data problem for us all to
think about so this is a result from
somebody named andrew blow who's a
member of csail a very well-known
economist some of you may know some of
his work he did this analysis of credit
scores so he was interested in
understanding whether credit scores were
a good predictor for default when you
get a loan from the bank so what he did
is he took this database was in this
case a one-percent sample that
represented about 10 terabytes of data
of all the financial transactions of
customers from a major bank in the
United States on the y-axis here he
plotted our sorry on the x-axis here
he's plotted the distribution of credit
scores and then they're color-coded by
whether or not the person defaulted was
you know my rig red being more than 90
days late on their loan yellow being 60
days late more than 60 days blue being
sorry less than 60 to 90 30 to 60 and
green being less
30 days less than 30 days late so you
can see that the credit score if you
sort of look at these green people it's
not doing a very good job of separating
the sort of defaulters from the
non-default there's a lot of people with
low credit scores are perfectly up to
date on their loans and the people who
do default although they're sort of they
don't have a very high credit score
they're sort of smeared over a pretty
wide range of the available credit
scores so what Andrew did was built a
classifier or machine learning
classifier that does a much better job
of separating these people you can sort
of see if you draw a line through the
middle here that the red people are
cleanly separated from everybody else so
this model does anybody want to guess
what this model learns so if i showed
you ever if i showed you somebody's
translate financial records what would
you look at to predict it whether they
were about to default on their loan yeah
okay so how are you going to tell they
just got sacked from their financial
records well if they stop getting direct
stopped having direct deposits at the
beginning of the month that turns out to
be a pretty good indication so that's
basically what the model learns is if
you stop seeing regular deposits we're
seeing regular deposits and stop seeing
them so this this particular result I
think is interesting because andrew is
he's actually quite computationally you
know mathematically sophisticated he's
capable of using these models but he
this particular you know just running
this model generating the model takes a
long time he doesn't have tools that let
him scale this parallel eyes this
generation of the sort of learning of
this model and so that's something we
started to talk about is you know
building doing this sort of parallel
machine learning on these kinds of big
big data sets and so here andrew is
actually the machine learning expert and
the domain expert but you could imagine
different worlds we'd have a domain
expert and a machine learning person and
then their systems people like us who
are working on helping them scale these
problems
very actually this case it's too late I
guess actually but if the banks had a
way I'm right the question is does it
you know presumably why does the bank
care about this because they would like
to understand whether they can write or
they might want or they might want to
intervene and say boy we gave you a low
and it looks like you just lost your job
you know maybe we should do I mean it's
not I think banks don't actually want
you to default on your loans right it's
not in their interest to do that so if
they can find some way to help you to
you know fix your situation whatever it
is defer payments who knows right right
to get more of your money before the
something else get your money they'll be
happy okay thanks David for the cynicism
there all right let me move on so okay
so you guys all know the sort of the big
data you know mantra velocity volume
variety our take on is too big too fast
too hard but you know I think to me that
the kind of the interesting action and
the big data space and when we've all
been working on too big and too fast or
volume and velocity for a long time and
these these two hard problems these you
know cases where you need complex
analytics to find patterns or you need
to really combine diverse data I mean
this is clearly these are the problems
that we don't really know how to solve
and where a lot of the hard action is
and so so our kind of take is that if
we're going to solve these problems you
know what we really need to do is to
work with people who are both the sort
of algorithms experts the people who
have the domain expertise the people in
the industry who understand the problems
to solve these problems that this my
feeling and this is the really i think
the most interesting thing about big
data to me is my feeling this like
conventional view of the world as you
know there's database people and there's
OS people and there's machine learning
people it's kind of a it's kind of a
screwed-up view of the world so I'm
hoping we can you know make some changes
inside of csail that reflect the fact
that these groups need to talk to each
other more and I think you know you look
at the way things seem to be evolving
here in the database groups at Microsoft
Research and you see I think you see a
similar evolution of people recognizing
I need to do
so I'm probably preaching the choir a
little bit I just want to say before I
move on to the more technical part of
the talk Microsoft surojit in particular
has you know helped us to join as join
this big data at csail initiative it's
an industry supported initiative to sort
of share or collaborative research with
our share and collaborate with a bunch
of industrial partners so we're really
happy to have Microsoft as a founding
partner of that thing and as a part of
that and one thing I'd say isn't you
should talk to Serge it but we'd love to
have people come out and visit and
interact with us and there's definitely
you know if you see problems that appeal
to you or papers that we've written that
you like please make an effort to reach
out all right so with that I'm going to
move to the sort of technical part of
the talk so I one of the things that
I've been thinking this is really more
my research as opposed to overview about
our take on big data so I and I think a
lot of traditional database research has
really focused on what i would call
scalability and performance problems you
know there's this sort of prototypical
systems ecig mod paper that says you
know we're going to build we have a new
storage manager for our database system
and it's you know an order of magnitude
faster than everything else that has
come before it right and then you you
know stamp out a bunch of performance
graphs and it's a it's great but you
know one of the things that I think
there we don't do enough of is thinking
about how we can make data kind of more
accessible to users so I'm going to talk
about a set of what I would call data
accessibility problems that are sort of
three research results one of them is a
you know Eugene woohoo some of you may
have seen Kim hog talk about his bit of
some of the visualization work one of
them as Eugene's work but to other
systems that I'll talk about in a little
bit of a little bit more detail than
Eugene's work so the three systems so my
sort of take on this is you know this is
like this is not what I what I want to
say is Big Data Systems is not about you
know MapReduce right I think you the
conventional view of big data when you
go talk to people outside of you know
people who do research is that well you
know what do I need to know about Big
Data Systems I need to know MapReduce
because that's the platform that matters
and I just you know I like to hammer
home the message that I think MapReduce
is a perfectly great platform for
certain kinds of things but there's a
lot more to be done and it's not
the silver bullet here so so I'm going
to talk about three kinds of things one
of them is Eugene's work which I like to
characterize as helping users understand
why answers are what they are and their
data so that's the system called DB
wipes another is a this some work we've
done on crowdsourcing and using people
to integrating people into databases so
this is the system called quirk and then
finally I'm going to talk about an
interactive data visualization system we
built called map d that uses GPUs to
really really efficiently visualize and
interact with large data sets and I'll
give a demo that so with that maybe I'll
just take a couple minutes and like
there any questions i know i went kind
of fast to that but right cool so onto
the onto the good stuff so DV wipes so
here's my take on DB wipes and out of
this probably is not Eugene's take on it
so maybe you'll get a very different
story so first of all you know I think
visualization is really really important
it's something that I've started to feel
like is something that we as a community
there there were some interesting data
processing problems that underlie
visualization obviously it's kind of
probably the first place that a lot of
people start when they're analyzing a
data set is just to look at it and
understand what it looks like right plot
it right you make some distribution
right and one of the things that often
you find with visualizations are
outliers so data points like these guys
up here what is it about these people up
here who have you know very high credit
scores and yet defaulted on their loans
right these little red dots that you see
there on the upper right so what we're
trying to do in DB wipes I think of what
what's important about this tools it's a
tool that tries to help you identify why
outliers exist so what do I mean by why
so given a visualization which would
typically be generated by some
computation that you ran on some
underlying data set right you're what
we're trying to find DB wipes is to try
and find a predicate so you've got input
data set you've got output data set
you're trying to find a predicate over
the input data set that one applied a
filter that one apply to the input data
set will make the outliers and the
output data set go away okay so why is
this an interesting question because it
you kind of want to understand what are
the properties of the inputs
that cause the outputs to look bad right
and that's sort of what these predicates
if you can find them will explain right
and this is a challenging problem
because these data sets are often like
this data set is this huge
multi-dimensional data sets got hundreds
of different dimensions and so this
search problem over the input data set
is quite complicated right you have to
you might have to look at sort of all
sub all correlated sub-ranges of you
know all sub-ranges of all input
attributes that you know might
potentially be the ones that cause this
some data to be outliers for certain
classes of sequel queries for example if
I'm you know these points were generated
by doing a group by and then taking an
average in the group's or something you
may be able to find efficient ways to
figure out the relationship between
inputs and outputs but in general if
this is a black box function like it is
here that produces this visualization
solving this problem is challenging okay
so that's what this DB wipe system tries
to do I'm going to sort of go through
the system by showing you a little demo
a little interactive demo of it rather
than going into too much technical
detail about it but that we have a
beaded be able to be paper coming out in
two thousand this upcoming vldb that
discusses this problem and gives a
preliminary solution to it I think
probably our algorithmic that the sort
of algorithm we have for doing the
search problem that could be refined and
if you know people are interested I
think it sounds like an interesting
problem I'd love to talk to them more
this is eugene by the way made me
promise that show a picture i'm every
time i presented his work so that's
eugene didn't see him before ok so
here's the here's the interface so did
he show this before he didn't show ok
good all right so the the inner for this
this is a data set it's actually not a
particular large data set that was
collected from it's a set of sensors
that were measuring light and
temperature and humidity in a building
and what this visualization the blue
dots here represent the temperature
variation throughout the building over a
four day period so the peaks and valleys
represent the building as it warms and
cools in the morning and afternoon on
the bottom we're just plotting the
variance so each one of these points is
actually generated by taking an average
in a group by of a whole bunch of data
points right and so on the bottom here
is the variance of each one of the
points on the top ok so you notice that
in the variance there are some
interesting
outliers some points where the variance
is very high so you might say well
what's going on with those particular
sense of reading what was the variance
so what DB types let you do is select
those points and you can tell the system
that those particular points are
outliers in some way you can either say
they're just wrong I don't know why
they're wrong or you can say I think
these points look higher than I would
expect you can also give DB wipe some
positive examples so you can say these
points down here they look more like
what I would expect so you can select
those and say those are good outputs
okay so you sort of give it this
information and it uses this to solve
this this search problem basically of
searching over the input data set in
order to find points that if you were to
remove them would cause the bad points
to go away and hopefully would leave
this good set as it is okay so when you
run the system it gives you a set of
predicates that are sort of like this it
basically searches through the space
find ones that remove all the points and
then tries runs a simple decision tree
basically to try and describe what the
properties of those points are that if
you remove them would fix the outlier
problem so if you click on the first one
of these things here for example you can
sort of see as I toggle back and forth
but the first one of these makes the all
the variants go away so we did a good
job of finding this what is this
predicate in this case well it says what
guys probably can't read it but it says
not mode ID equal to 15 so these were
these little sensor modes and mode ID 15
is one particular device so in this case
sort of the explanation is that there's
one device that's malfunctioning during
these time ranges and it turns out
actually if you drill into this a little
bit more it has something to do with the
voltage on the device so this device is
battery when as it starts to run out the
voltage drops and when the voltage drops
it starts producing haywire crazy
readings okay questions about that yeah
okay so so so it's sort of like what I
described before so what we need to do
is we go into the input data set right
and we need to search through so we know
for those inputs that the user
highlighted for this set of inputs that
the user highlighted right we know we
can figure out what what all the points
that contributed to any of those are so
that's those are the points that we sort
of need to search
because those are the ones that we might
need to eliminate from the data set so
then we the paper proposes three or four
different kinds of search heuristics
basically like you basically what you
want to do is to so the the sort of the
most naive algorithm what it would do
would be to take say take half randomly
subdivide all those input data points
into a good set into a candidate good
set in a candidate bad set and then you
would rerun the query over the candidate
good set and the candidate bad set right
and then you would sort of you figure
out whether which of those two sets was
better and then you would recur somehow
or you could do this in kind of a
bottom-up approach where you could pick
one point at a time you could remove all
the points in this entire set and then
add them back in one point of time until
you found the one that you know most
eliminated these things and you know
best kept these good points in place so
you can kind of that kind of give you a
sense of and you can imagine a whole
bunch of different kind of you know
search algorithms in between that in
order to you know navigate through the
space and find the one that best
explains that however yeah yeah so in
fact if you go look at the paper the
paper talks all about it we started
working on this problem because Eugene's
past work was really on he was working
on provenance for these array oriented
database systems and then he started we
started talking about the thing that we
think is interesting about this is this
is like provenance but where the
function where you're doing a these
aggregation operations so its provenance
across aggregations where the functions
are not necessarily you know simple
sequel functions so if there's simple
sequel functions like average it's a lot
easier because I can sort of you know or
take an even like Max is the obvious
case right if I'm asking for the maximum
well I want to make a maximum go down
well I can trivially identify the
highest point in the input data set and
remove it right but
sure I mean like any we're using a
decision tree algorithm basically to
build credit explain the program like
any decision tree algorithm you have to
kind of cut it off at some point you
don't want to you don't want to generate
too much of a predicate in fact you can
see that the way that we've solved it
here is to basically prefer at your
mitting predicates that have fewer
clauses in them that actually these
these are five candidates and they get
progressively more complicated as you go
on because the system is kind of
proposing things that are like it's
basically overfitting a set of input
point so I think there's a classic
overfitting problem here yes it could
and we in fact tried to use a classifier
for this and it didn't work very well
and I don't remember why sorry we did
think of doing I mean we tried using
svms or something and it just it didn't
work and it may be that we didn't use
them right you know oftentimes there's a
lot of kind of part in getting these
models right but okay so um what what
Eugenia is working on next which i think
is pretty cool is to then build this
into a general-purpose visualization
language so what he's trying to do is to
come up he's got this sort of simple
declarative language if there are
several people were proposing these
kinds of grammars of visualization now
so he's building his own little grammar
for visualizations that looks like this
it says you know I've got some data set
and I can specify the you know x and y
axes and what the scales look like and
what the colors you know how colors
correspond to different attributes in
the input dataset make pretty looking
pictures like this but what I think is
really interesting about this is these
now integrating this what is essentially
a provenance system into this thing so
there's a relationship between each
point in this visualization and each
point in whatever the input data set was
that was used to generate the
visualization right and so what he can
what the idea is this doesn't work yet
he's very close to having a demo but
it's not quite there the idea is that
you'll be able to for each data point
sort of a work backwards and see what
inputs it were in the input data set
that correspond it to it as well as do
this sort of DB wipes thing where you
can select some outlier data point and
it can run the analysis that we have in
order to sort of identify common input
predicate so we're trying to build sort
of a general-purpose visualization tool
that lets people
write their own visualizations this this
demo that we had here is sort of a
canned visualization not a it's not a
flexible visualization system we're
trying to build this kind of support
into a visualization engine so I think
this is a really cool problem and we're
something we're kind of starting to push
on a little bit all right ok so that was
DB wipes um what I'm going to do now is
talk shift gears a little bit so that's
sort of I you know thinking about how do
we make big data accessible well
visualizing data is an important thing
to do another thing often that you run
into with these systems is that well you
need some way to take human input into
account so in data integration systems
for example this is you need this
support right you need the ability to
ask a human which of these things is
correct right and so even if you have
some automated tool that does a good job
you're going to want something that can
help to you know kind of take human
input and so we my student Adam Marcus
spent about but a couple of years
actually working on building a
crowdsourcing system that we call quark
and so I'm going to talk just about one
kind of result that we got and that's
just so and give you a little bit of an
introduction to what quark can do ok so
quirk is a mechanical turk plus querying
or something all right so you guys
probably all know about crowdsourcing
the most classic crowdsourcing system
that all the people seem to do research
on anyway Mechanical Turk this is a
system where you can pay people pennies
to compute simple tasks like go look up
something on the web for you ok there's
lots of other crowd sourcing platforms
out there and there continue to be more
and more of these and they vary and the
types of tasks you can ask people to do
some some of them are like like
TaskRabbit like interact with the
physical world some of them are you know
more you know design RT tasks like build
a website there's a whole bunch of these
different platforms Turk is appealing
because it sort of has this it has a
programmatic interface and you can ask
people to do simple things and in
particular you can ask people to run
what our observation was you can
actually write tasks that would let you
do sort of database Colonel like things
with people so that's what we did and
quark so our question was could we use
people sort of as the inner loop of a
database so you know suppose whatever I
want to find you know I
have a table of people and I have some
unknown values in a table I might want
to use Mechanical Turk to you know get
people to supply the phone numbers right
so this would be have people go do web
searches but the interface that we're
going to provide is one that's a table
oriented interface it's like a database
you can say here's a database and here's
some unknown values and then the system
will automatically go out and post these
tasks on mechanical torque in order to
be able to get the missing supply
missing information ok but there's other
things that you might want to do with
the system right like you might for
example want to you'll rank people right
so maybe I want to say who are the
ugliest people at Microsoft Research
right so who knows what the answer would
be but you know you could show people
pictures and get them to run that as a
ranking function and that's something
that you know it's a matter of personal
opinion you're never going to be able to
get a computer to do this but you know
it's a task that you know maybe you want
to actually use a computer to do more
seriously I think people are often faced
with very subjective ranking criteria
and if you're trying to prioritize
what's the most important you know bug
to fix in a project or whatever it is
you could imagine you're having
programmatic interfaces to groups of
people that would let you do that all
right and then finally you know maybe
there's kind of join like operations you
want to do like is s chowdhury the same
as Serge at sea right and you have these
two pictures and they're labeled and you
have to decide what these things are the
same or not it basically a data
integration you know entity resolution
problem but in many cases people are
just going to have more context and
they're going to be you guys would all
be much better at solving a problem in
any algorithm ever right so probably all
right so so what we built is a simple
sequel like interface it looks a lot
like Pig if you guys know pig you can
say things like you know for each
researcher a professor or generate name
comma get phone of phone of name and
this get phone is a UDF that we have a
simple you know a simple domain-specific
language unless you describe what that
UDF looks like it will you know go and
result in tasks being posted to
mechanical turk you can say other things
like if I've got pictures of cats and I
want to know which ones are smiling i
can say filter the photos by some other
predicate right and and so we have this
if we had a table like this of images
right so this is just an example of the
syntax so yeah but we have task types so
there's a filter task type there's some
prompt that has some HTML in it
and then there's some way to combine
input from multiple users who because
users might disagree so oftentimes in
these systems you need some way to sort
of reconcile multiple different users
input I'm sorry about the examples here
at these like these are the world's
silliest examples and is these are Adam
slides and he just came up with the
world silliest examples so unfortunately
it's only going to get sillier so you
kind of are gonna have to bear with me
but anyway so this is an example of the
kinds of interfaces that users would
create these hits that would look like
this that would be posted in mechanical
turk so when you start thinking about
this problem actually there are a
surprising number of pretty interesting
problems here that are it's not obvious
how you're going to solve all of them
like how much would you charge people
how much did you pay people for each one
of these tasks and how does what the
relationship between payment and quality
and response time and how many workers
should you get for each task and how do
you determine whether our worker is a
good work or a bad worker or their
workers trying to spam you or take
advantage of you or not and what's the
right user interface for building these
and what if you H lots of these
different tasks together into one of
these hits or if you do them as separate
ones or how do you deal with algorithms
like joins that sort of our you know the
naive or sorts where they're sort of
super linear and runtime and it seems
like even for small data sets you could
quickly end up having to do a lot of
tasks to do anything very interesting
with them and what's the right ordering
of operators and is it different than in
a conventional system and you know how
do you build a user interface that
tolerates the fact that responses are
going to come back slowly over days or
hours rather than you know right away so
there's lots of interesting questions
we've worked on a bunch of them I'm
going to talk today just about one
simple example problem which is sorting
ranking a list of items with people okay
all right so ranking I've already given
a couple examples of ranking like rank
researchers or vapors or you know bugs
by some report but there's lots of cases
in the world where we need people to
rank things or the web this happens all
the time right so you know Yelp you rank
restaurants by their ratings you know
Amazon you rank or you know school rent
rank things by rank rank books by their
user scores or likes so the this these
interfaces actually suggest one of the
ranking algorithms that we're
think about which is this the idea that
you can ask people to give a score and
then write things by their average score
right so the other way that you might
rank things is to i'll show you how let
me just tell you since I just started
this very silly example ok so this silly
example is suppose you're stuck in the
wild and the wilderness and there are
dangerous animals all around you have to
figure out which ones are the most
dangerous to avoid them ok so you want
to sort animals by dangerousness all
right so these are some example animals
and we're going to show them to people
and run them so this is a very simple
example the reason we picked it is
because we wanted to pick some examples
of things that we thought people on
Mechanical Turk aren't likely to have a
ton of domain expertise about and that
everybody can kind of me know like
people know mostly know what different
animals are mean maybe people don't know
that this is a lemur and that's a high
you know but you know people like have
some sense of animals they can look at
them they can make some assessment of
them and we asked them some different
questions like we asked dangerousness
which is kind of a weird thing to ask
about animals we also asked him about
size which we thought was an easier
thing to ask so we asked them some
different tasks that were so we sort of
what we're hoping that this would be a
domain that wouldn't you know be too too
polarized by experts and non-experts
anyway so we built these two interfaces
so the two interfaces one is a rating
interface as i already suggested the
other one we i did a direct comparison
interface so this is i call this the hot
or not interface right but you know
basically you can pair wise comparables
to all the other animals and you'll get
some ranking or some result right these
things are kind of these this is kind of
eat both of these are kind of
interesting right because if you just
ask everybody to rate some set of
animals and give a score for it you
haven't actually compared every animal
to every other animal unless you you
know maybe if you asked everybody to
pray provide a rating for every animal
you could then argue that you had a good
ordering of things but you're not really
comparing one animal to another you're
just sort of you know getting you know
one rating per animal in this case you
are directly comparing animals to each
other and so in that sense maybe you're
getting more information but the
interesting thing is that people may
disagree right so some people might say
a hyenas
dangerous than a hippo and some people
might say a hippo is more dangerous than
I you know and so you get these
conflicting answers that actually when
you think about sorting things makes the
world you know sort of unclear what the
what the right sort is you have this
sort of people in theory have actually
studied this problem this sort of noisy
noisy comparator problem and it's it
makes it difficult sometimes to apply
naive sorting algorithms like if people
apply a quick sort where you like you
know pivot on one animal you know pivot
on you know is dangerous is not
dangerous you can end up with these kind
of weird incongruities basically so
anyway that wasn't that's that's one
sort of interesting result that came out
of this the other thing we looked at was
batching so we built these interfaces
where we let people compare multiple
animals to each other or rate multiple
animals at once which is one way to deal
with this super linear complexity you
present multiple things to people and we
looked at whether people would we sort
of looked at people's tolerance to is
kind of a social science phenomenon but
you know how tolerant how many people
how big how big are the batches that
people will tolerate and as you go to
bigger batches how does result quality
fall off and how does response time fall
off we looked at a bunch of a different
kind of results like that so what I'm
going to do is I'm going to give to just
give you two results from the paper
there's a bunch of other these other
things that are reported on the paper so
the first one is the quality of results
so how does comparing compared to rating
as a sort of way of ranking items and so
the way we the way we evaluated the the
ranking is to use this thing we call
Kendall's tau rank correlation so we
took what we believe to be the true
answer for a set of questions and then
we computed the ranking that our system
generated using human workers to the
true answer and we use this measure
called Kendall's tau which basically
gives you a number between minus one and
one that's in a sort of the agreement of
two rankings with each other just is a
measure of how many items are out of
order basically okay so we took the true
answer we compared the answer from the
crowd and we looked at these two inner
faces the comparison based interface
where we compared every item to every
other item and then used majority vote
for workers so we use workers every item
i think was ranked by through compared
by every item was eventually compared to
every other item three times and we took
the majority vote in that case the
particular the first task we tried was a
set of squares okay so if these this was
meant to be a super easy task that
everybody would get right and in fact in
the comparison interface everybody does
get it right we perfectly order the set
of squares compared to the ground truth
and this towel rating thing where we
asked people to rake the side we show
people the smallest square in the
largest square and we asked people to
rank on a scale of one to five which you
know how big is this square doesn't give
a perfect answer but it Cal point eight
is actually pretty reasonable the kind
of interesting thing about this rating
thing is that we only have to look at
every item sort of rank every item once
we don't have to rank all pairs of items
okay so it goes from a quadratic to a
linear time so we then looked at a
hybrid algorithm where we tried using
rating to sort of get things mostly in
order and then we use sort of local
comparisons in order to reorder things
up to a fixed budget basically and we
saw how well you could do for a certain
budget with that kind of using if you're
willing to spend a little bit more money
to fix things up how much better can you
do and so you can see that the so this
is the tao of point eight which is the
rating based interface and this is the
comparison based interface which is
perfect and then we sort of saw as you
so this towel point eight takes whatever
it is in this case some some number of
tasks so it's a you know it in this case
I think it's per item we have nine tasks
and here per item we have 81 tasks so
it's the hype the comparison interfaces
you know the square take is n squared
and you can see the comparison interface
basically as we add more tasks as
quality improves so we were able to come
up with this interface that was able to
kind of progressively improve its
quality eventually kind of and
interestingly it gets to a perfect
answer before it is actually used up the
same budget as this
this comparison based interface okay so
we then asked people we did some silly
things like we asked people to rate
animals by dangerousness this is the
rating that people came up with so flour
and grasshopper rock be turkey dolphin
parrot I don't need it's some of them
they basically make sense the ranking
that people came up with why you know
whether we all would agree that a whale
is more day less dangerous than a
panther or that a Turkey's less
dangerous than a dolphin I don't know
but you know people did actually do this
and they came up with a ranking for
animal dangerousness that seemed kind of
reasonable yeah well what people or just
what people's personal fears are right
there's probably some people are really
afraid of sharks or something yes cuz
you're surprisingly dangerous yeah i'm
not sure i would call skunks all that
dangerous but yeah we're people than
anything else that's right those are
power I think that probably the most
dangerous by lives lost right although
you know the on the other hand most of
us don't interact with hit but you know
there's also probably a bias here about
where these people are coming from right
like most of us don't have occasion to
interact with hippos on a regular basis
though where's you might have a skunk
problem right okay alright so i think we
looked at some other interesting kinds
of questions about whether we could
detect whether or not a particular
question was ambiguous one of the things
we're interested in is whether we could
learn that a problem was a question was
people were not having not doing a good
job of answering a question or a
question was hard for people to answer
and so we phrased a set of questions on
animals from less to more ambiguous so
animal size we thought was a low
ambiguity question dangerousness is a
medium ambiguity question and then to
ask something that's completely
ambiguous we asked them how likely would
you be to find this animal on Saturn
okay which is a question that makes no
sense right and the idea was we wanted
to understand this way we wanted to see
if we come up with a metric that would
in fact let us understand was the
question we were
Asking made no sense that people are
answering it right so we use this metric
called Kappa which is just a measure of
agreement review or a rater agreement
that again basically varies from 0 to 1
based on how much people agree ok so
basically if you look at squares people
agree most of the time on the size of
squares and actually on animal size
we're happy that you know they basically
agreed on animal sizes two people seem
to understand what these animals were
and be able to rate them and you can see
that if you ask people just like a to
pick a random number from a list for
example you see that they you know they
don't agree very much although they
agree a little bit more than zero which
is surprising I'm not sure what that is
but you can see that in fact this is a
pretty good metric for detecting whether
question is completely ambiguous ok so
the kind of interesting takeaways our
first of all we can use this rating
versus batching interface I'm sorry the
rating versus sorting interface and the
hybrid or comparison interface and the
hybrid version of this too as well as
batching to reduce the total dollar cost
number of questions we have to ask
pretty significantly and we looked at a
bunch of different metrics to understand
how good a job we're doing using this
tile thing as well as this notion of we
call this notion of Kappa to see if
we're asking a question that users don't
understand so is this big data no it's
not really but one of the things we have
been in the sense that it's hard to
scale these interfaces up to large nut
mounts of data but one of the sort of
natural questions to ask is how would
you integrate a system that uses people
into a larger system right so one of the
things we've been looking at is whether
we can use these crowd-sourced ma data
to for example train a classifier which
could then be used to do prediction so
could we you know build a predictive
model that would allow us to estimate
based on the parameters of animals which
ones are dangerous or not by using
crowdsourcing to do this so this is kind
of an active learning problem and so
we've kind of been asking we started to
ask the question well ok let's take that
as the starting point can we figure out
which crowd workers we should ask
to give us information that will allow
us to or which questions we should ask
users about which say animals and our
dangerousness algorithm we should ask
users about in order to best improve the
accuracy of a classifier ok so the
intuition is that if you have a
classifier and you have some items that
you haven't labeled yet in your training
and you're in your set of items right
then you can ask the crowd to label some
of them the ones you want to ask about
are there lots of different proposals
you might come up with you might say I
want to ask about ones that are close to
the margin you know between dangerous
and not dangerous if I'm just asking a
binary classification problem or you
might want to ask about the ones that
the algorithm is most uncertain about
which and what do we mean by uncertain
what we had a specific definition of
uncertainty where we tried leaving we
tried taking the training data we had so
far leaving you know iterating over
different subsets of it and then
measuring whether items classification
changed from different iterations so
this is a variant of this current
nonparametric bootstrap to estimate the
uncertainty of certain items and then we
came up with somehow or than it does a
pretty good job so the takeaway here is
with that I don't want to go into too
many details but kind of the takeaway is
there is this big data connection which
is you want to figure out how you use
people in these database systems and
oftentimes this could be for a machine
learning task or even just for to supply
a bit of an uncertain information in a
data integration task and I think it's
really interesting and important for us
to think about how we integrate these
things together so does anybody want to
ask any questions about that part of it
and then basically what i'm going to do
with the last part is do a kind of a fun
little demo and i can talk a little bit
about the technology behind it but i
mostly just want to use it as an excuse
to show this demo so does anybody want
any asked questions about crowdsourcing
yeah so when you but starts together do
you how do you letter the coast
is it like the same or do you like so
yeah so there it turns out there been a
lot of studies where people looked at
people's price sensitivity to tasks and
the difficulty of tasks and there are
some rules of thumb in our evaluation
what we did is we fixed the cost of task
set I think two cents apiece and then we
just looked at basically at what point
do quality start going down or the
latency like the way I don't know if you
guys have used mechanical torque but the
way it works is you work or see a list
of tasks that are available and they see
they can basically these tasks are
grouped together so like in our case we
would say there are a thousand hits that
are Adam Marcus's you know ranking task
and you could pick one of these hits up
and do it so people might do one but if
it took them a long time they might not
come back and do another one for two
cents so you can see what the latency is
and the rate at which their completed
and we measured some of those things
there been other people who have looked
at well what if you adaptively bury the
price these experiments turn out to be
kind of maddening to run because you get
you don't have any reproducibility like
the set of people who participate one
time is different than the set of people
who participate later and in fact a set
of people change you know the sort of
people who want to do your tasks is
biased by the kinds of questions you've
asked in the past before so they the
workers have these like forums that they
talk about task posters on and so you
know one of the problems that Adam had
is when he first started running this he
started doing some things that the
workers didn't like and they said bad
things about him and then his reputation
was bad and people didn't want to do his
tasks and then later he came back and
you know managed to get some people to
say nice things about him and they did
all his work but you know you can
imagine how hard it is to get
reproducible experiments in the face of
you know this kind of a human human
system so
there are all there are quality controls
the Amazon provides so one thing you can
do is you can say only one I use people
who have you know successfully completed
at least X of these things in the past
they have a certification they're like a
master certification you can ask them to
you can limit the tasks to there's lots
of different things you can do you know
and in some in some of these cases we
were actually interested in
understanding we looked at some problems
like could you build systems that are
robust to people trying to game or spam
the system right so could you ask
questions in a way that prevented people
from being able to you know
significantly skew the results are cause
the system to provide answers that don't
make any sense but if you use some of
those those quality controls maybe you
can make some of those those spammers go
away if that's what you want right yeah
it's still though is like it's not I
mean you don't have any again you don't
have any way to say i want to send this
to these hundred workers you can just
say i want to make it so that this
category of workers is not able to
complete this test so you can't there's
not a way to specifically recruit the
same workers over and over and over
again this is a research on this problem
primarily in the database field or do
you found that there are other areas of
see it well science that have also been
looked so there's an API people who've
looked at interfaces and there's been
some social science and economists who
looked at you know various aspects of
this right incentives so it's not just a
bunch of database people i would say in
fact that you know there's a group of
people at MIT you been doing this HCI
work and the space who built some really
pretty nice interfaces and tools they
you know if you guys have seen michael
Bernstein's work he was just hired at
Stanford he built this really cool
system for using crowds to summarize
documents and it was like it's a really
nice and proofread document so you can
take your research paper and it
decomposes the paper up into a bunch of
sentences and ask each sentence to a
crowd worker and
you know the crowd workers all suggest
edits and fixes and then there's another
crowd worker who's responsible for
verifying the fixes and saying yeah this
this the or voting choosing which of the
fixes is best anyway you know they said
for twelve dollars or something and you
know eight hours you can have your
entire you know research paper proof
read and you know in fact it finds the
obvious grammatical mistakes i mean the
results are you don't get beautiful
prose back but it does fix errors so
it's certainly better than word
you know it depends a lot on that you
know I think what I think this actually
probably is most relevant in the reality
is in organizations that have relatively
large groups of people who may need to
you know collectively contribute to
something or you know collectively you
know make decisions like order things or
rank things I mean you could imagine in
an organization like you know an
engineering group at microsoft that
might have under the people in it that
you know there are certain kinds of
tasks that would be be interesting or
maybe expert finding inside of
organizations who's the person at
Microsoft who knows about X Y or Z you
know it's not quite a database problem
but it is sort of a you know how do you
you know sort of a crowdsourced
computation problem eating how big will
this scale to you know I mean we ran up
to you know databases of thousands of
elements in limine so it's not scaling
the billions of things it's just it's
never going to happen right this
particular work I mean the data came
Dave tamer does have a crowdsourced
component to it and I think you know
data integration is a natural place
where you want us try and use crowds to
help you answer some of the questions so
they're using some of these things I
don't know that they have a need for a
ranking your face but you could imagine
that it might be something you would
want at some point but I think data
integration is an obvious place where
you would you want to integrate crowds
and they have a model they claim that
they have experts who are doing this so
they've been working with you know
genomics and genetics companies and
there they want to use people who really
understand you know what is a you know
this particular column in the database
that has some genetic significance what
does it mean right and so you can
imagine there that using Mechanical Turk
might be difficult all right so with the
last few minutes i'll just talk quickly
about this map d system and so this is a
system that somebody named todd mastec
is not actually a graduate student but
as a programmer working with me has been
building it's a gpu-based graphic
visualization system it's actually
underlying it is a sequel database
system but it the manifestation of it is
is this the interactive visualizations
that the demo I'm gonna show is running
on top of Twitter so why why are gpus
interesting so I I sort of for a while I
don't know how many of you guys have are
doing work and gpus I know there's some
people here who are actually probably
building a system that kind of looks
like this but the you know I sorted for
a while my take on gpus had been well
you see see these papers or people like
I have a hash join implementation that
runs on the GPU you know and the
performance on the hash joint itself is
awesome but you have this problem you
have to transfer the data out of this
you know how does CPU on to the GPU do
the computation and then transfer it
back and you end up you know with a you
know net gain a twenty percent or
something so our take in in map d is
that these gpus are becoming
increasingly they're getting large more
and bigger and bigger such to the point
that where you can now buy GPUs a single
card with eight or sixteen gigabytes of
RAM which means you can put say 12 of
these cards in one box so now you can
talk about a box that can host you know
hundreds of gigabytes of data and small
nut you can see can start talking about
having terabytes of data on you know
small numbers of servers which is
probably interesting it's at the point
where you know if you were a government
agency for example that had a terabyte
size data set you really wanted to
interactively explore it would make
sense to start thinking about this kind
of technology so that's what we're doing
in map d we're basically building a
parallel a parallel database system that
it's column oriented parallel database
system that basically lets you load
certain columns into the memory of these
gpus and then run queries on them inside
of the GPS so and it has some
interesting compression and other
techniques to try and take advantage of
the data that's available as much as
possible the sort of awesome thing about
these GPS which maybe some of you know
and maybe some of you don't is that
these things have an incredible amount
of parallelism inside of them so um the
GPU programmers talk about cores which
as I understand it aren't quite the same
thing as CPU cores but they talk about
core accounts in the you know 4,000
cores per GPU okay so we have a level of
parallelism that goes way beyond what
you could get out of
national conventional processor and the
bandwidth the bus that the inter GPU
intra GPU bandwidth SAR higher
significantly higher than on a cpu
although not orders of magnitude higher
they're like you know two to three x the
bandwidth that you would get on them
between the memory and the cpu and a
conventional architecture so that's
going to mean that we can get you know
cool you know pretty remarkable
interactive performance out of these
things so i'm just going to show you
this demo I can avoid falling unlike but
because this chair is missing a wheel
I'm going to show you this demo or
Twitter so um hopefully you guys can all
see so this is a map of the United
States so those these are it's going to
be a little bit unresponsive because
we're coming over Wi-Fi to a server on
the east coast but we can do this
interactively query in this case it's I
think 100 milli about a hundred million
tweets that we have loaded which is over
you know a couple weeks of data in we
can get about 30 frames per second so
what we're doing here is none these
queries are pre computed and we're
exhaustively scanning all 30 ml all
hundred million tweets to evaluate every
query that I'm about to show you okay so
the system I'll just give you one simple
example we can ask a query like tweet
text I like rain okay so those are all
the tweets that have rain in them you
can click on one individual tweet
fishing and it's kind of chilly standing
in the rain right so you can see the
text of an individual tweet but more
interestingly you can build this kind of
a heat map visualization which shows the
relative frequency of the word rain
versus the background frequency in each
region so we can render this heat map
and then we can interactively visualize
it so now we're animating frame by frame
again each frame here is being exhausted
generated by a sequential scan of the
whole data set but you can see you know
these rain storms as they move across
the United States in this
animation so well this was in I don't
know when does it stop raining and when
does it stop raining in Seattle you had
a good couple weeks yeah well so this is
May and may late May and late April and
early May so it looks like it's stopped
raining you know people kind of stop
complaining about it over here by
mid-may so all right so but there's
other sort of interesting and surprising
things you can discover from this so it
is there's just another simple example
so if we type Central Park where you see
Central Park lights up there there's
also apparently a central park in Maine
somewhere but you can zoom in on this
and you can see in fact it does a pretty
awesome job of localizing central park
right that's exactly because well what
do people do they go to Central Park and
they tweet about it right and I'll give
you one more example and then I'll stop
and let you guys ask a few questions I
like this one which is i typed Red Sox
there so for those of you who don't love
in the Northeast you may not know I
don't know if there's a similar effect
here but that there's this kind of
imaginary line that runs kind of
diagonally from through through hartford
but between New Haven and Hartford that
sort of is the dividing line between Red
Sox fans and Yankees fans or Patriots
fans and jets fans or whatever it is and
you can actually use this to kind of
visualize it so if I switch this to
Yankees you'll see the you'll see that
you know its wings over the other thing
so um so I'll leave it at that I am
pretty excited about the system just
because I feel like it gives us you can
see why having this kind of interactive
ability to explore our large data set is
really pretty awesome it's the ability
to run these kinds of analytic you know
sequential statscan style queries on
large data sets in milliseconds is a
capability that is
is surprisingly powerful and it's one
that I think existing database analytic
database engines probably really aren't
optimized towards you know they people
haven't been thinking you know
millisecond latency people you know say
well a second latency would be great
right or if you're in MapReduce you say
yeah 30 second latency would be great
right and that those differences are
although they sound you know an absolute
magnitude or small you can see why in an
interface like this even that little bit
of lag that we're getting going over the
Wi-Fi network to the west coast makes
the experience not as nice it is as it
is when you can do this on the East
Coast and we're looking at a bunch of
other kind of analytic style things in
this visualization space we can do here
so questions
yeah so my I'll actually say the reason
that this guy built this system begin
what I don't know about geo flow I don't
know what I don't know I haven't tried
it my guess would be typically things
with Excel or not you know it's not
designed to scale the hundreds of
millions of points although I know that
there have been some cool extensions and
you know research results have come out
word it lately where people have done
really neat things but yeah right right
which is sure yeah so I'm you know I
think I mean obviously I don't think I
know what I'm not so much saying that
these kinds of visualizations I think
they're the interview interface we've
built but I'm not trying to claim the
interface we built is what's exciting
about this what I think is exciting
about this is the ability to use these
gpus in a really efficient way to do
this kind of computation and again you
know when it comes down to when we go
write research papers about this the
research papers are going to talk about
all the tricks we had the the convoluted
little things we had to do to get good
performance on these algorithms and
compress the data in a way to load as
much data in is getting you kind of
imagine all the systems talent you
moving stuff on and you know these
columns on and off of the thing there's
a bunch of systems challenges that you
have to deal with in order to build a
system like this so deletion so this
should this 100 million tweets in this
to over two weeks this is two weeks
there's about 100 million weeks thinner
geocoded how bigger have maximum size of
the tweet it's complicated so it's is
160 characters right so what is it
hundred and 140 character sorry yeah but
the there's metadata so it's and then it
compresses some I think you called a
kilobyte would be a good I think I
believe that the records that Twitter
sends you are in the order of a kilobyte
because if million isn't terribly
a hundred million one hundred gigabytes
the raw data 100 billion I'm like 100
gigabytes right how many words you did
you know in this case we're using I
think for GPUs each with 8 gigabytes of
RAM but we're not loading you know so
the trick is that we're compressing the
text because there's a lot of repetition
in the words we don't actually need I
called it a kilobyte but when you you
know it's a kilobyte because it's like
they ship it to you in some JSON format
and they've got like the you know ascii
text of the person who posted the name
and ascii text to represent the latin
long and you know there's a whole bunch
of you know kind of goofy and coding
tricks that you play them do you have a
sense of kind of what the what the
speedup is form all the techniques so
we're actually getting we've got you
just pumping the data in raw through the
GPU we've got these oh ok so there's I
thought you're going to ask how much
faster is have been a cpu pumping data
through the GPU is generally the thing
with GPU processing in the past for any
kind of database operation is it costs
more to get it in now if the copying it
off right so you can you know whatever
it is you get your three gigabits a
second or something right into and out
right and so that's that is the gating
factor to copy if you're streaming the
data in and then streaming the results
back to the CPU freaking Vince a second
it's an okay number but if you have is
that 30 gigabytes of data it's going to
take you no no I'm just saying that's
this real I think three gigabytes is PCI
I don't know CeCe you guys probably know
very gigabytes sec I bus bandwidth right
I mean maybe these gpus have a little
bit better than that now but it's not
it's not a ton better so we that I would
question I thought you're going to ask
us how much how well could you do in a
carefully optimized main memory
implementation which i think is a pretty
interesting question we have these guys
at Intel who are working with us who
basically reporting this thing from GPUs
two CPUs which sounds a little bit silly
but there
these high-performance computing guys
who should be running this on board
right you've got the GPU with local
memory yeah so interested in hell intel
has this this thing the xeon phi board
and the next generation Xeon Phi is
going to be an on-chip on it's going to
be a coprocessor located right next to
the CP or even in the CPU that you know
is like sort of like a graphics
processing unit that could be used to do
it has much higher bandwidth than the
PCI bus does is how much data transfer
let's say you have a 4gb you as soon as
the three gigabit per second so the
trick here is were not transferring any
data the data has been pre-loaded into
the GPUs and that's you know at some
level that's the trick which is we're
saying let's load the data that we want
to query into the GPUs and then we can
you know in this case the tweets and
then we can interactively explore that
data then are you using
so the current implementation that's
drawing those heat maps is not actually
using kind of ironically not actually
using the GPUs to draw those things it
turns out that for whatever reason
rendering these PNG's that get sent back
over the web is something that GPS are
not good at so they're good directly
rendering to the screen but because we
need to send data back that's these
compressed encoded pixels that's
something that's easier to do on the CPU
so we basically send you know a
histogram to the CPU which then competes
the graphics and in fact if you look at
the Layton sees per frame something like
half the time of each frame of this
animation is spent doing the rendering
of the PNG that get sent back
so we're we have an implementation of
group by that he didn't he was trying to
get in place for the demo but it
basically a way where you can do a
spatial aggregation so you can group by
you know region and so then and then
compute distributions and so you can
compute like the top k trending words in
every region for example so that
requires you to you group by region and
then within each region you can compute
an aggregate like the top K and then you
can do differences between adjacent
aggregates so we can do that sequel
query for example there are certain
cases where key value I'm sort of key
foreign key joins would obviously be
useful like you want to look up some
metadata about some particular thing we
don't have that implemented but it's
something we're adding so over for
example like if the tweets have images
references in them you might want to go
to a look up to an image an image table
and display that although it would
probably would actually do that on the
cpu is thursday you sort of start to
when you start thinking about these you
now you have to think in this world of
split execution where it's like is this
something that makes sense to do on the
GPU or do i store these images on the CP
or even on the disk and just fetch them
when the user asks to render them but
you know i think this idea of being able
to render hundreds of millions of points
like we're literally drawing a hundred
million points on the screen here and
it's like it's pretty cool like it's um
it's a big data set and this guy who'd
built this he basically built this
because he was a Middle Eastern Studies
major at Harvard and he was trying to
get arcgis to let him visualize tweets
and the Mideast and he was pissed off
because our das you know fell over at
ten thousand tweets or something and so
he went the system showed up in my
database class one day let's built this
so that's what we started working on it
anyway so that's basically all I have
guys there's a little bit more about
those things but um you know happy to
take more questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>