<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intelligible Machine Learning Models for HealthCare | Coder Coacher - Coaching Coders</title><meta content="Intelligible Machine Learning Models for HealthCare - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Intelligible Machine Learning Models for HealthCare</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2Lp2LYUUxhc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
if you saw the talk I did on deep
learning shallow learning you know a
year ago this will be as different as
possible from from that talk and this is
very much new work
it's work in progress so if you see
connections with your own work if you
have ideas for directions that we should
take this you know I encourage you to to
get in touch with me okay so this is
joint work with Yin Liu was a grad
student who I worked with at Cornell
yo.hannes used to be at Cornell but
we've managed to steal yo.hannes and now
he's in Redmond as well and then this is
a dev at MSR in Redmond and Mark Sturm
is a tech employee at a major hospital
whose name we can't divulge so and I
just want to thank the medical experts
who have helped us with this Greg Cooper
Mike find in our Corbett's has also
helped us and then these are other
collaborators on some of the papers we
published a few ktd papers and what I'm
going to talk about the kdd papers are
kind of technical you know they're the
usual computer science here's the
detailed algorithm here's why it's a
hundred times faster and still as good
as other things this is not going to be
that kind of talk this is going to be a
very untechnical sort of demonstration
of what you can do with the methods and
I'm actually going to spend only sort of
one or two slides telling you about the
machinery that's behind the scenes okay
so and the motivating example for me is
going to be healthcare so although you
can can use this for other things so let
me just sort of give you a thought
experiment so you've got a lot of data
you know million 10 million patients
you've got say thousands of good
features for each of those patients
you're very good at machine learning
you've trained a state-of-the-art model
on the data and you know the accuracy of
the RS he just looks phenomenal you know
ROC 0.95
we never get that in healthcare data but
imagine that you you do something that
good so now I guess the question is you
know is it really safe to go ahead I
mean the model looks great and the day
it was great is it really safe to go
ahead and deploy this model and start
using it on real patients is high
accuracy on test data is that sort of
sufficient to say that it's safe to use
this model I mean you know what what
could go wrong right right you you know
it's it's the ideal setting for machine
learning well what have you really gone
so what you've really got is this black
box
maybe this model is boosted trees or a
deep neural net or random forest you've
got this black box you've got incredibly
high ROC on it on a test set so it's so
that's great but you don't actually
understand what's in the black box and
I'm gonna talk for a while give you some
motivation to understand why high
accuracy on test data is not sufficient
in in critical applications like
healthcare or for fielding a model and
then you have to be able to do much more
than than just get high accuracy so let
me tell you I was involved in a study
this is back when I was a grad student
at CMU was a multi-institutional study
so in fact Greg Cooper who was a he went
to grad school with Eric orbits and
David Hecker Minh so these guys all know
each other from back then Greg Cooper
was in charge of the study and our goal
was to compare different learning
methods that were available in the mid
90s so deep learning didn't exist yet to
compare these different learning methods
on this pneumonia prediction task and we
had the usual kind of learning based
methods back then you know we had
rule-based learning back then we don't
use that very much these days but we had
that we had simple neural nets you know
one layer we had some Bayesian methods
hierarchal mixture of experts was a new
method at the time and our goal was to
compare these different learning methods
and see how well they could do compared
to say logistic regression you know the
good old standard learning method that's
used in healthcare so much and you know
things like SVM's were just sort of
happening they weren't really available
yet - so we don't get to compare against
those things and I got very lucky
although there were a dozen different
teams competing on this data set the
multi task neural net that I ended up
training ended up being the best model
of all the different models it had the
highest ROC the highest accuracies
things like that
so you might think
we would then you know maybe go to
clinical trial with this neural net and
we were discussing doing that and I
actually stopped them from going to
clinical trial with the neural net and
instead we ended up doing further
experiments with logistic regression we
we didn't use the neural net in fact we
used one of the poorest performing
learning methods for future work as
opposed to the highest accuracy model so
now the question is well why do we do
that right obviously it's going to have
something to do with the intelligibility
or lack of intelligibility of the neural
net but let me give you more detail
about about what happened so the goal of
this experimental work that we were
doing was to train a model that would
predict your risk of say dying from
pneumonia or from something very bad
happening like you end up you know
needing a lung assist or you end up in
cardiac failure or something like that
so a really bad outcome basically from
pneumonia think of it as probability of
death so that's our goal is to predict
whether you all have pneumonia we
already know that diagnosis our goal is
just to figure out which of you are high
risk because then we should admit you to
the hospital and if you're low risk in
fact the proper care is chicken soup
antibiotics and call us in three days if
you're not feeling better that actually
is the best care for you if you have
pneumonia and you're not you know very
high-risk you don't want to go to the
hospital s you really have to so so okay
so we're trying to train a model to
decide whether you're a high risk or a
low-risk patient so we can decide
whether to put you in the hospital or
not and the guy who is doing the rule
based learning learn to rule one night
which which was if you have asthma it
lowers your chance of dying from
pneumonia right and you you don't have
to know a lot of medicine to question
whether that makes sense
so of course we went to the next you
know huge meeting with with the doctors
and we asked them about this and they
said oh that's that's interesting they
said in the data that that we're playing
with we consider asthma to be a very
serious risk factor if a patient with a
history of asthma presents with
pneumonia they not only go into the
hospital you know they often went into
the IC
you back in the mid-90s they went into
critical care they received very very
aggressive careful treatment because
they're considered very high-risk and
the good news is that that treatment is
so effective that it actually lowers
their chance of dying compared to the
other pneumonia patients who don't
receive that aggressive treatment so you
know chalk one up for for health care
right there doing something great for
high-risk patients they're saving them
so it is a true pattern in the data
right I mean I mean it is true that the
asthmatics in the data set actually have
a higher chance of living than non
asthmatics in the in the data set so it
is a true pattern the role based system
learned it I can only assume that
anything the rule-based system could
learn the neural net would have learned
so we assume that this is also in the
neural net it's a real pattern in the
data there's no reason why the neural
net wouldn't learn it I'm a smart guy I
told them hey you know I can probably
figure out a way to make this problem go
away in the neural net
so it's oh I can probably fix this
problem and there are different ways
that you might try doing it it's
actually not so easy it would it would
take research and publishing new papers
which is you know a good thing actually
for us but that's not what made me say
we should not use this model on real
patients this is what scared me was I'm
just gonna make up a story our data
actually doesn't have any pregnant women
in it but imagine that a pregnant woman
who has pneumonia also receives very
careful attention and treatment and then
that makes her also less likely to die
from pneumonia
also there's selection bias pregnant
women would tend to be younger patients
who have a much better chance of
surviving pneumonia so you could imagine
that pregnant women also would have a
lower probability of death because of
the treatment that they would receive
and maybe the neural net is predicting
lower risk for pregnant women but the
rule based system didn't learn it so now
there's this problem like well I don't
know what the neural net learned that
the rule based system didn't learn so I
don't know what I need to fix in the
neural net
you know that something else didn't
learn and tell me about and that's why I
stopped them from further consideration
of using the model on real patients was
I said you know we don't understand this
model it's it's a one of the most
complicated models people would train
sort of in the mid 90s and I said we
just don't know what harm it might do to
certain classes of patients we already
know it's making a big mistake on
asthmatics
maybe it's making big mistakes on other
patients and we just don't know about it
let's not use it so and that's why we
ended up using logistic regression
logistic regression it's very easy to
read and understand the model ok so so
that's the big problem I mean maybe we
just can't use some of the most accurate
learning methods in healthcare because
because we're just not going to be able
to understand them maybe we're going to
be stuck with using standard
tried-and-true methods like logistic
regression for these sort of critical
labs because it's very very important
like you might think oh well you really
should just collect better data right
you you should not you should collect a
data set where you don't send the
asthmatics in for treatment right then
you'll realize that asthmatics really
are higher risk and and then you'll in
the future do the right thing of course
that's not ethical right we we can't
withhold care from from mathematics in
order to train a better machine learning
model here so so we have this problem
you know it's not easy to know what the
model has learned that's bad we can't
sort of collect the ideal data because
we have to live with the data that's
actually available to us where treatment
is already being done so we're just in
this sort of conundrum maybe we're just
gonna have to stick with models that are
just very simple even though they're
less accurate and what I'm gonna
hopefully show you is that we can
actually get models that are much more
accurate than logistic regression but we
can get a lot of intelligibility oh so
that's where we're going ok so you know
maybe there's this sort of trade-off
right models can be low complexity high
intelligibility
unfortunately low accuracy or the
trade-off can run the other way we can
have high complexity high accuracy but
at the expense of intelligibility and if
you think about it you know maybe it's
something maybe it's a very fundamental
thing like
constant right you you can't no an
electrons a momentum in position
accurately at the same time maybe you
can't have high accuracy and high
intelligibility at the same time and
hopefully we'll will see that actually
this isn't true that it's not going to
be like this right so so there's no
doubt that being able to Train causal
models would be the ultimate cure for
this problem if we had truly causal
models than it would recognize that it
was the intervention the asthmatics was
were receiving that made them low risk
and that without that intervention they
would be high risk causal learning of
course is very difficult and I don't
know how we would even do the causal
learning without withholding the
treatment from some asthmatics to
understand what the effect of them not
receiving treatment would have been so
that's a bit of a problem so but there's
no doubt if we could do causality that
would solve the problem but but I'm just
going to assume that causality is too
hard for us to do and then
intelligibility I think of the
rule-based system as having been
intelligible that is we could
immediately see there was a rule asthma
lowers risk we immediately questioned
whether that rule made any sense and
then it's very easy in the rule-based
system to eliminate that rule to just
certainly when we want a model to be
accurate the ultimate most accurate
model would be something that's causal
but it's not necessarily the case that
you have to be causal to be accurate oh
yes absolutely we're using medical
expertise and their causal understanding
to conclude it oh there's no doubt about
it yes and I assume that this will be
true in many uses of machine learning
both in healthcare and in other
applications where we'll have background
knowledge which would let us look at a
model if we could understand it and say
good good good huh that's interesting I
didn't know that good oh oh that looks
wrong let's get rid of that that's bad
if you actually have this causal
information why not just use that causal
information directly well it's awfully
hard to write down all the causal
information you know beforehand right I
mean there's a big difference though
between looking at the model seeing what
it's doing and having an expert say oh
oh that looks questionable and doing the
opposite which is saying expert please
spend the next five years writing down
everything you understand causally about
the world experts can't do that or
official extreme yeah I think it's much
yeah I think you'll see when you look at
the models it's much easier for an
expert to check a model to look at it
and see if it makes sense or not then it
would be for an expert to explicitly
elicit all of the what they know in
advance especially since the majority of
its probably not relevant to the model
you know the alternative you're
describing is not happy to talk about
this more of this afternoon yeah yeah I
mean the causal step is something worth
thinking seriously about but it's
fraught with challenges so we don't know
how to go that way yet okay so let me
just talk a little bit about model
complexity to sort of show you this
space of models that we're gonna gonna
think about so here's good old linear
regression and logistic regression these
are sort of pretty intelligible models
for the most part I mean in very high
dimensions of course everything gets
complicated but basically we just have
sort of weights times individual
features and we can look at those
weights and if the weight is positive it
you know increases risk if it's negative
it decreases risk if it's zero the term
isn't in the model so these are pretty
intelligible things if the features are
intelligible of course if the features
are not intelligible then the dnal is
lost
so this is sort of simple reasonably
intelligible at the dimensionalities not
too high but unfortunately they're not
always that accurate so now we've got
much more complicated things like deep
learning random forests boosted trees
you you know all these sort of new
methods that we've developed a machine
learning over the last 15 years and
these things are are much more accurate
typically on most data sets but sadly
they're they're pretty opaque so so what
we're gonna do is hope that there's some
ground in between where we can get sort
of as much accuracy as possible while
not losing too much intelligibility so
if we have this space that goes from
sort of very simple linear models to
these full complexity models there's a
something in the middle called additive
models which many of you may have heard
of so and what additive models are
instead of there being a wait times a
feature plus a wait times a feature
there's a function of this feature and
that function can be a pretty complex
nonlinear function but it still
functions of individual features there's
no function here that's a function of
all the features it's just individual
functions of individual features which
are then being added together just like
they would be in a logistic regression
model we can make that a little more
complicated this sum over functions of
individual features is this part of the
model and now we've added a sum over
functions of pairs of features so this
would capture things like pairwise
interactions between features and then
this would capture say three-way
interactions between features and if we
if we have n features and we go all the
way up to n weigh interactions then
pretty much anything can be represented
by this model class now what we're going
to do though is we're not going to allow
ourselves to even go to three-way
interactions we're going to stop with
functions of individual features this
term and we're going to allow ourselves
a small number of functions of pairs of
features and we're going to see what we
can do with that and the reason why
we're going to stick to this model class
is I think as I'll be able to
demonstrate to you these remain pretty
intelligible they're pretty easy to
understand and if we succeed and get
pretty good accuracy out of these things
and
remain intelligible then we've sort of
accomplished our goal okay I didn't
invent generalized additive models in
fact there's a very nice book by dipsh
Ronnie and hasty back in 1990 which
beautifully goes through through this
class of models so statisticians have
been here long long before us so what
we're going to be doing is using this
class of functions and then we're gonna
put it on sort of machine learning
steroids so so we're gonna do this class
as best we can so let me talk a little
bit about the shaping that's going to be
an important part of the learning
process here imagine features like body
temperature respiration rate pulse rate
things like that partial pressure of
oxygen in your blood a number of these
variables like body temperature there's
a normal range 37c and then if you're
much higher than 37 see you have a fever
and you're presumably at high risk
because you have a high fever or you
could be much colder than 37 C maybe 32
C and that also would put you at higher
risk because you're hypothermic
now clearly a linear model like logistic
regression can't handle this with a
single feature for body temperature
because there's no way to multiply
temperature and have it be low in the
middle and high at both ends right so so
what do you typically do when you do
logistic regression is you end up taking
temperature and breaking it into
multiple variables so you have a fever
variable which starts at 37 C and you
know so it's zero when you're normal and
as your fever gets higher that number
gets higher and then you have a
hypothermic variable which also starts
at zero for normal temperature and then
as your temperature goes down it sort of
gets higher indicating you have more
hypothermia and then you do logistic
regression on these multiple variables
and what happens is only one of you'll
either be hypothermic or you'll have
fever you won't have both so only one of
these will apply and then you'll have a
wait times this so even that doesn't
work well enough and that's because
you're sort of risk with fever goes up
rapid
you know it's almost like an exponential
rise so you actually need to create sort
of low fever medium fever high fever
very high fever you need to create these
multiple variables so they have
different weights on these different
terms so and that's typically what's
done and you use experts we use doctors
you know to say what these different
ranges should be when we take a
temperature variable and we break it
into a dozen variables which we're going
to do with logistic regression on we use
expertise to do that now the expertise
doesn't does end up hurting
intelligibility a little bit what was a
single variable is now you know maybe
half a dozen or a dozen variables and it
also requires expertise to do this and
maybe the expertise isn't perfect maybe
it's not done as well as you can do it
if you learned it from data so and this
is the pneumonia data set that we're
going to spend some time talking about
and some of these variables you can
actually see how they've been
discretized so let's see heart rate it
looks like less than 125 is normal 125
250 is considered high this is
considered very high systolic blood
pressure less than 60 61 to 70 71 to 80
81 to 90 greater than 91 is considered
normal
here's temperature you can see it's
broken into you know half a dozen
different variables so this is the way
things would normally be done and in
fact the logistic regression model for
pneumonia this is exactly how it was
done these variables were broken up into
these different quantile you know ranges
and then logistic regression was done on
these these variables so that's the way
you would normally do it whoops sorry
back one so what we're going to do
though is we're going to be able with
these generalized additive models of
doing a more complex shaping function so
let me just give you as some example
this is just synthetic data here imagine
we had a problem with six features and
let's say everything truly is linear on
each of the features so ignore the graph
for now just imagine it truly is a
linear function of these six different
features and if you learn the write
coefficients you'll get zero error on
the data
now imagine that unbeknownst to you I go
in and I take all the values of x2 and I
do like a square transformation to them
or risk
we transformation to them and I I go to
another variable and I do a sine or
cosine transformation to it so I do this
I play with the data okay I transform it
now obviously if you're trying to train
a linear model you won't be able to get
you know zero or in fact a linear model
of data that's been transformed in these
ways you get a pretty high residual
residual error but if you're able to
learn functions that capture the
transformation and undo it do the
inverse transformation you'll convert
things back to the original linear space
and then when you fit a linear function
you'll actually do extremely well right
and and and that's what happens you get
two if they're the better you can do
that the better you get back to zero
error so that's the kind of game we're
going to play the only difference is
it's not going to be contrived in this
way it's not like all have done some
transformations to the data and you play
the game of trying to figure out what I
did and undo it instead we'll have no
idea what the real transformations are
and the model is just going to have to
learn it from the data so that's what
we're gonna do so this is a this is my
computer science slide I'm just going to
sketch the algorithm that we're going to
use so that you have an idea but we're
really not going to talk about the
details under the hood any more than
this because it's more important I think
to show you what we can do with the
model so the first stage is we're going
to build the most accurate a dative
model we can where we just shape
individual features so this is no
interactions we're just shaping
individual features we're going to do
that under the hood we're using boosted
decision trees the decision trees are
controlled so that they can only each
tree can only look at a single feature
that way you can't do any interactions
of features by testing multiple features
in the same tree so we might have you
know hundreds of thousands of trees but
every tree is only allowed to examine a
single feature at a time so if we want
to look at different features that'll
have to be different trees that do that
so if we do this really well and then
you subtract from the targets that
you're trying to predict everything
that's in the residual is either noise
or it's something that depends on
features you just don't have measured
which you'll never be able to model so
that's kind of like noise or it's some
sort of interaction it's either a
pairwise interaction or a three-way
interaction or something like that so
what we do is we do this as well as we
can then we look at the residual and we
have a technique that we've published
which is a very fast mechanism for
scanning for pairwise interactions so
and there can be a lot and one of the
datasets we're gonna have 4,000 features
that's 4,000 squared over 2 right so I
guess that's what 8 million pairs of
features and what we're gonna look for
is just sort of the top 250 most
important pairwise interactions but
there's 8 million things we have to test
so it has to be done quickly it's a
large data set so we have a fast method
for doing this then we'll take the top
you know 100 200 300 pairs and we'll
just use cross-validation to pick the
top n and then we'll do the same sort of
shaping on those pairs and we'll add
that to the model and then we'll adjust
the whole model a bit and then we're
done okay so that's don't worry about
the details it doesn't really matter
that much there are papers that we would
show you how to do this in if you want
to write your own code and then we're
gonna repeat the process you know ten or
a hundred times and that's to further
reduce the variance of the process
because it can be a high variance
process and also because we want
something that's like a confidence
interval so this will give us pseudo
pseudo confidence intervals they're not
very trustworthy confidence intervals
but but they'll work reasonably well so
that's the algorithm slide let me tell
you about the data we'll spend some time
with this pneumonia dataset from the mid
90s it's a model it was a pretty big
data set back then about 15,000 patients
we only had 46 features all the patients
have pneumonia and the goal is to
predict probability of death so in about
11 percent of the patients do die from
from pneumonia the larger data set that
we'll look at is a very recent data set
here we've got a hundred thousand
patients a year from a major urban
hospital and the goal here is completely
different I'm predicting probability of
death at all you've all been in the
hospital
you've just been released and our goal
is to predict if you might need to come
back to the hospital within 30 days and
in the US they're starting to penalize
hospitals where too many patients have
to come back within 30 days because
presumably you didn't do something you
know right if too large a percentage of
your patients have to bounce back in
so so this sort of puts the pressure on
hospitals to either not release the
patient if they shouldn't be released or
to ensure that they have proper care
during the time after they're released
or that they know how to take their meds
and and things like that so okay so
those are the two different data sets
just give you an idea this isn't that
important on the pneumonia data these
are our OCS on the pneumonia data set
the GA to M model this is our
generalized additive models with pairs
is is doing pretty well it's a pretty
good ROC it's better than the neural net
that I trained back in the mid 90s so
this this is a high accuracy model even
logistic regression if we do it well
does OK on this if we discretize the
variables into the proper bins so we're
doing better than logistic regression
but the difference is not astoundingly
large on the other pneumonia data set we
have which is much smaller it turns out
the difference between logistic
regression and any full complexity model
is huge it's ten points of RSA and then
on this readmission data set we do a
couple points better you know ROC than
the simple models like logistic
regression so so and I just want to so
we're talking about shaping features you
can only really shape a feature that's
you know got multiple values if it's a
boolean feature
there's no interesting shaping to do in
fact if it's a boolean feature our
method is just logistic regression so
it's just putting a weight on the
feature I'll show it as a graph but it
is just let's just logistic regression
so it's only the features that are
labeled with a C here for continuous
that we're actually going to do some
shaping okay whoops so now we're
actually looking at a term this is the
term for age this is the pneumonia
dataset we're trying to predict
probability of death and I'm going to
show you a number of these graphs so
let's spend a little time with this
graph so you get comfortable with it so
this is and this is completely learned
from the data there is no human
expertise fed into this model so it's
it's got 46 features this is what it
learns for one of the features so this
is age in the x-axis this is a data set
that has just adults so there's nobody
younger than 18 in the dataset
the risk here this is a value that the
models predicting for different age so
the way to read this would be if your
age 70 then it adds a zero to your risk
if your age 80 it adds about a point one
seven to your risk if your age 50 it
subtracts about a point two five from
your risk and the way the risk works is
roughly speaking if you add one to your
risk it kind of doubles your probability
it's like log odds it kind of doubles
your chances of death and if we subtract
one from your risk it kind of cuts your
probability of death in half so we
roughly speaking right in fact it's it's
an ensemble of thousands of decision
trees all of which are shaping just the
age variable and then this is the
aggregate of that ensemble of thousands
of trees but if you think of it as a
single tree it's kind of right this is
just smoother because it's an average of
thousands of them in fact it's one of
the things we're looking at is having a
little bit more explicit control over
the smoothness as opposed to just being
an artifact of the learning method yeah
yeah now I think your intuition is
exactly right that controlling
smoothness is important yes then that's
like a piecewise constant approximation
to the shading right and he made the
bins fine enough of course it would look
a lot like this so when we have thought
about interpolating yeah yeah well we
haven't done it so far but we are
thinking about different ways of
imposing smoothing further smoothing
onto onto these graphs yes yes in fact
I'm sorry I should have said this is a
histogram this is the density of
patients of different ages in the bottom
and you can see the ER bars tend to be
smallest where we have the largest
sample size yeah so you're exactly right
yeah and the ER bars you know they're
coming from our bagging process so you
know these are these are not
statistically valid our bar
they're just sort of to give us a
qualitative idea of what's going on here
so so you should scoff at our error bars
okay so so let's look at the graph and
see what it says
so well it's good to be young so it's
very nice to be under say 50 so that
lowers your risk and in fact it doesn't
seem to distinguish between patients who
are forty-five in patients who are 35
and 25 it just it just seems to consider
them all to be uniformly low low risk of
dying of pneumonia if they have a
pneumonia then risk seems to rise slowly
as we go from sort of 50 to say 65 so
that that's nice
there's an interesting jump at around 67
66 67 and it's interesting in especially
in the mid 90s this would have been
retirement age in the US for the
majority of the population so in
retirement not only means that you know
your daily schedule has changed your
activities have changed but it also
means your healthcare provider has
probably changed I mean you're you're
now on Medicaid Medicare as opposed to
receiving insurance perhaps through a
company so a lot of things have changed
at that point and it's interesting that
the model seems to detect that remember
it doesn't know anything about this it's
just looking at the data for 15,000
patients and coming up with this so
maybe it has detected retirement age and
then risk seems to rise pretty rapidly
as we grow older than then say upper 60s
into the 80s there's another interesting
jump at exactly the 85 86 boundary
there's nothing there's nothing explicit
that happens in healthcare in the u.s.
at age 85 and there's a chance we're not
sure yet what causes this and this is
where our causal interpretation of the
data would be so valuable if we had it
an interesting question is well there is
a sort of thought that you know doctors
sort of say well you know grandpa is is
87
Scott pneumonia hasn't responded to the
first two rounds of antibiotics do we
really
is it really in grandpa's best interest
for us to continue aggressive treatment
so there's a chance that it's a purely
social psychological unconscious
decision that's happening basically for
patients who are over some round number
threshold like eighty-five in the US but
but we don't actually know if that's
real so so that's something we have to
further investigate it's something the
model when it makes predictions does do
so this is an accurate statement of what
the model does now whether it's doing
this correctly or incorrectly is the
thing that we don't know so and then
risk seems to flatten off and then it
may be dropped now the error bars are
getting large at here there's very few
samples out here at this age and I
showed this to some doctors for the
first time a couple weeks ago and they
said are you familiar with the class of
people called successful agers and
apparently this is a genetically
identifiable class there are markers
that indicate you're in you're lucky
enough to be in the successful agers and
they said they wonder if this is
successful a juror showing up in the
graph very little sample size you know I
wouldn't want to bet my life on that
so but but it's interesting that right
away when when we showed this to doctors
they said you know there's a possible
explanation for why that happens so they
also thought this was very interesting
because I think they were all feeling a
little guilty that maybe they were
unconsciously causing this small
increase in risk for patients above some
threshold for which they really had no
medical justification for doing they
also sort of laughed at this and they
said you know causally it could be that
we just know how to set retirement at
the right that this is natural and we
just know where to put retirement age
okay so this has just learned from the
data and what I want to emphasize is
here we are we've trained a model that's
as accurate as any other model we know
how to use in this data it's as accurate
as boosted trees neural nets all those
sorts of things in fact it's more
accurate than some of those methods and
all of the sudden we're having an
interesting conversation about very
specifically what the model has learned
about one of the variables age and
even sort of generating questions like
oh I can sort of understand that I
wonder if that's real and if it's real
why is that happening boy that looks
dubious let's look into it it's really
really interesting that we're having
this conversation if this had been a
neural net or random forests we would
have said well the ROC is 0.86 let's use
it and we wouldn't really know that
presumably those other models would be
doing something vaguely like this inside
but we just wouldn't know that it was
inside it's because these models these
terms are sort of independent and
examinable as graphs that we're able to
have this sort of interesting discussion
and and peek at what the model is doing
Oh get there in a second yeah good at
the neural not a bunch of data where we
tile the space and then kind of done an
analysis and recovered this graph from
the neural net there there's no doubt
that with enough effort we could
probably get this back out of the neural
net it's not easy though in the neural
net it's really entangled with all the
other variables in a way that's hard
like you might think you can just
marginalize over all the other variables
say in the neural net or even in the
data without having learned the MALDI
maybe you just do marginalization over
everything else in this graph pops out
and it turns out that's not the case you
don't get this graph if you marginalize
over all the other variables you have to
learn all the terms together in order to
get this this sort of graph so
marginalization alone doesn't do it but
clearly it's in the neural net and with
enough effort we could find out the best
way I know right now for learning it if
it's in the neural net is to train this
model to mimic than there on that and
then look at this model but I mean
that's not a very good answer well I'm
sure it's in the neural net in one way
or another we could tease it out yeah
the beauty of this is there isn't
anything this is the model like the way
this model works is you really do find
the patient's age you go up on the graph
whatever that value is you write it down
you do that for all 46 terms you add up
the column of numbers convert to a
probability in the usual way and that is
the model there's there's no extra
complexity anywhere so it's completely
transparent so this was different than
doing marginals
why why is it better to look at this
does it look at their organization so
the problem with the marginalization is
one of the things marginalization will
do is it'll multiple count evidence from
different variables so these things are
additive in the sense that if we have
three variables we'll take the effect
from this variable the effect from the
next variable and and add them together
and then that gives us the probability
marginalization could easily show us
that for each of those variables you
would predict that same total
probability like you would get the same
risk score from doing the
marginalization oh whoops I'm sorry it's
interesting what would happen is the
marginal plot would have vaguely similar
structure right I mean especially for a
variable like age the marginal plot will
recognize that you know risk is higher
for higher age and lower here it'll be a
little Messier because the effect of
other confounding variables haven't been
subtracted so carefully but but it'll
still be qualitatively somewhat similar
the difference though is you don't
actually know exactly what impact it has
on the model the marginalization because
you'll be multiple counting the evidence
from all these terms so if you were to
if you were to do this predict the risk
score and then add them all up you'll
end up with either much higher a much
lower risk from the marginalization than
this which is constrained to making sure
that these things all line up properly
to the right probability so but I agree
in some cases marginalization actually
can can give you qualitatively similar
results it's not the model though
marginalization let's see so the graph
is the model so so the model the model
let's see so so what I've done is I've
trained a model that predicts accurate
probabilities it's as accurate as any
other model we know how to train and you
can completely understand the model
right there's there's nothing hidden
here if you disagree with no no the
purpose of the intelligibility is not oh
oh let's see yes yes yes it's it's
definitely to allow the expert to bless
the model and to basically say we we
trust that this looks reasonable or this
looks very wrong yes yes it's definitely
to do that and we wouldn't necessarily
get the same thing from the
marginalization right because the
marginalization wouldn't be the making
the same predictions as this model I
mean the marginalization is not itself a
predictive model it's it's a tool for
looking at data whereas this actually is
a model because there could be other
correlated features correlation is
definitely the curse of all high
dimensional data and it'll be a curse
here as well just as it's a curse in
logistic regression right logistic
regression also becomes difficult in a
thousand dimensional space I mean all I
can say is we have an accurate model
it's as accurate as anything else we
could train in this model unlike
anything else we know how to train
that's this accurate you can actually
understand exactly where the prediction
comes from and because of that you have
a chance of deciding that some of what
it's doing is not correct and I'll show
you examples of places where it's sort
of obvious it's it's doing the wrong
thing but but I'm not sure how to say
more than that so it's definitely not
causal itself and in fact it's one of
the concerns we have is that the model
is so intelligible that we're afraid the
healthcare professionals I've shown it
to recently that they often leap to a
causal interpretation and we sort of
have to keep reminding them sorry it's
not that smart it's it's it's not causal
it's interesting if it suggests causal
things that you want us to study but but
the model itself is not not causal not
necessarily
okay so this is age let's go on
so statisticians write generalized
additive models have been around a long
time the reason why we're getting extra
horsepower out of them is because we're
doing all the tricks that machine
learning people do statisticians if they
had worked on this feature what they
would have done is fitted it with a
linear model so this is popular code
where they would have done a spline fit
a linear model and they would basically
say that you know the risk sort of just
increases with age so they would have
missed a lot of this detail so and I
think this is an example of just
statisticians being very conservative
when they fit a model statisticians
don't like to add a term to their model
if they're not sort of 95% or more
confident that that term really needs to
be there in that shape and I think you
can't make a statement that this graph
is correct like to some 95% confidence
interval we don't know how to do that
now not everything is so rosy so this is
a blood pH again we're doing pneumonia
risk and normal blood pH is sort of
around this range and it's interesting
that it sort of drops here and then goes
up and then drops again further and I've
asked the the doctors about this and
they said one possible explanation for
why it's low here and then goes up and
then comes back down is there might
actually be a threshold where they start
applying treatment at this point so so
it's sort of like you know you're fine
if you're here and then your pH is going
higher you're getting higher risk but
they haven't applied the treatment yet
then they suddenly apply the treatment
and it helps you and it lowers your pH
so that's one possible interpretation
but the truth is it's just a pretty
messy looking graph right it's not it's
not beautiful and clean like that age
graph might have been I don't really
know how much of the detail and
structure that's on that graph is real
meaning a sort of accurate statement
about the data I don't necessarily mean
causally real now statisticians would
have fitted you know with this
particular spline and that's actually
perhaps too simple it also misses sort
of interesting detail that's being
captured over here but you know I don't
really love this graph either
intentionally picked one of the bad
graphs from from the model and you know
I don't really really love that one I
don't like that one either I'd like to
have something probably in between which
is why we're talking about ways of
imposing better regularization but you
know if statisticians are too
conservative machine learning people
tend to be too high variance right so
and here's here's sort of my joke about
that which is uh so-so statisticians you
know approach the edge of the precipice
very very cautiously and a 95%
confidence interval kind of prevents
them from ever getting to the edge right
because there's they're so careful so
they end up sort of stuck here and
because of that they don't get all the
accuracy that they might have gotten by
including more terms in their model now
machine learning people were the
opposite we're like lemmings we just run
you know headlong full-speed at the edge
jump off and then we count on something
like cross-validation maybe to save us
before we crash and burn at the bottom
but the ideal place to be would be you
know the best view is right at the edge
of the cliff and I think something in
between what statisticians have been
doing and the sort of overly complex
models where training is actually ideal
and and then we're trying to sort of
pull ourselves back to get there but
we're not there yet so okay so that's my
joke about machine learning people
versus statisticians this is another
graph for age but this is now the model
that's 30 day readmission okay so this
is bounce-back Hospital bounce-back
there's ignore for now we have patients
now that are under age 18 for this data
set ignore that for now we have
something qualitatively similar we have
pretty flat risks sort of in 20s 30s to
mid 40s then risk goes up somewhat
slowly as you you you know start hitting
50s and 60s again we see this sort of
interesting jump here it's delayed by a
year or two and what's interesting is
that's probably true in the u.s. now
since this is a modern data set that
retirement age has sort of moved in
extra year or two to later we see a sort
of another jump there's this interesting
drop that I don't know what to make of
that and then we do start to see some
interesting jumps it's not exactly the
same
as the other graph now it's a very
different prediction test the other word
prediction probability of death this is
just you know maybe you had a car
accident and you had your bones fixed
and you know do you need to come back in
30 days so this is a very different task
there's some similarity between the
graph there's some differences between
the graphs one of the big differences is
the vertical range is quite different
there are now four thousand features in
this data set and age is not one of the
most powerful features anymore whereas
for pneumonia it's a very powerful
feature so so the range of score
predictions that comes from ages is much
smaller than the other the other model
and then there's this interesting detail
that's happening here near zero and let
me just zoom in on that and show you
what what's happening so now we're just
looking at age zero to two years because
now we have infants and in fact if
you're a newborn you're born into the
hospital so you are a patient and then
the question is would you need to return
within 30 days and it's interesting this
is the largest difference from zero
prediction this minus 0.05 it basically
says most infants are born healthy into
the hospital and will not need to return
to the hospital within 30 days and that
seems to match the doctors understanding
and then they also thought this was very
interesting that the risk is slightly
elevated once you sort of hit the
three-month - one to two year period
which is where a lot of sort of infant
conditions are detected and need to be
treated so they found this sort of
interesting and the only way you end up
coming back here is if you were in the
hospital one month earlier so so this is
again 30-day readmission it's not
readmitted from this point it's it's
reading within a one month period and
then a risk sort of drops and you're
usually pretty healthy it looks like
until you know 18 20 30 and starting to
get into the 40's so again it's sort of
interesting that the model has learned
this kind of detail about age from the
data set so and that's kind of nice that
we're able to see this so so if you're
born and this is 30 day readmission then
basically you'd have to be readmitted
within this small window
of one month to be readmitted at three
months you'd have to have been in the
hospital for some reason to two months
or less yeah yeah I hope I answer your
question so it's just interesting that
the model is learning this sort of
detail that we're we tend to be born
healthy that there is a period in
childhood where where we do tend to have
some illness and then we sort of come
back to the normal long period of health
of youth and middle age and then things
get worse
so I think everybody in the room knows
about parity and interactions so let me
just jump and show you an interaction
and it's sort of a sad one this is uh
this is the pneumonia data set again so
now before we were looking at graphs
that's because we were shaping
individual features now we're shaping a
pair of features and in this case we're
shaping this pair on this axis we have
cancer and on this axis we have age and
the cancer is just a boolean so it's
just zero or one so no cancer has cancer
and then this is the usual age which
goes from 18 to 100 in this data set and
the other factor for age is already in
the model that graph that we saw and
there's a factor for cancer which is in
the model which I haven't shown you this
is the interaction between the two it's
what could not be captured by by by
those individual terms and it's kind of
interesting the reason why it's an
interaction is we have sort of high risk
here high risk here lower risk here
lower risk here that's the classic
parity diagonal effect and what's
particularly striking is the high risk
at low age if you have cancer and it
turns out we've looked into this a
little bit to try to understand it it
turns out what we're seeing is childhood
cancers you cancers a view of leukemias
and things like that which sadly for the
most part have not responded to
treatment so you still have them as
you're going into age 18 20 22 and not
only do you still have them but you have
pneumonia now you wouldn't be in the
dataset if you didn't have pneumonia so
it means you've had a childhood cancer
you're probably not not all patients
have had a childhood cancer
probably most of them you're now 18 20
22 years old you have pneumonia and it
means that the cancer has not responded
to treatment in youth and that sort of a
sign that you're at high risk so what
we're seeing is the tale of the
distribution of children which are not
in this data set who have made it to to
youth past their teens and sadly have
not responded they're still diagnosed as
having cancer whereas if you had had
cancer at age 14 received two years of
treatment and were considered cured you
wouldn't still have it in this data set
so it's sort of sad that that's what
we're seeing yeah
okay so let me actually just go to one
of these models and and then we're just
gonna let's see okay so this is the
model for pneumonia this is age that we
were looking at and what I want to do is
I want to scroll down to asthma okay
remember this whole thing started for me
with the asthma problem in a neural net
and it turns out that the model does
learn asthma is a boolean it learns that
having asthma lowers risk so model has
learned exactly what we learned back in
the mid 90s and that's why we didn't
ship the neural net was because of
asthma so it has learned the same thing
so how would we deal with it the
interesting thing is the way we would
deal with it is if you think asthma is a
problem which it obviously is you want
there to be a variable which is asthma
in the model the last thing you should
do is hide it or take it away you want
it to learn as much as it can on top of
that asthma variable and then you just
cross it out in the model you sort of
set its weight to zero and do not
retrain the model if you retrain the
model then correlation will kill you and
what will happen is the asthma effect
will spread as much as it can through
the other correlated variables and you
won't even know it's there
so you don't want that to happen so you
want there to be an asthma variable you
want it to learn about asthma you want
to then recognize that what it's learned
is wrong and you want to either
just put a 0 which is the like
eliminating the rule or if you wanted an
expert could say no I want asthmatics to
be very high-risk
I want this to go from zero if you don't
have asthma I want it to go to plus ten
redraw the graph that way and and it'll
all work the model will work if you do
that so you can either edit the models
by changing the graph so it would take a
lot of expertise to change some of these
graphs like the age graph or you can
just eliminate in effect and here's the
interesting thing so there's a asthma so
this model I was very happy to see that
this model was this you know bad or
smart as the model before here's
something interesting history of chest
pain turns out that's also a good thing
to have if you have pneumonia and we
think it's exactly the same effect and
we never learned a rule that said this
so this is something that presumably was
in the model we didn't know about this
is an even stronger effect in asthma so
it's interesting that the rule based
learning didn't didn't learn it so again
we would probably just eliminate this
factor just like this is what they did
in logistic regression so when they
actually they did something less correct
than this and logistic regression they
removed the variable asthma and then did
logistic regression without it which
meant that the leakage through
correlated variables probably happened
it would have been better if they left
the variable in and then said its way to
zero as we would suggest doing but
there's more so chronic lung disease
also seems to be a good thing and we
think this this is exactly the same
effect so so all three of these terms we
would probably just eliminate them from
the model so so this is the kind of
thing we're being able to quickly look
at the terms is very valuable I'll just
find one or two more interesting things
here then I'll go to a different model
and then that'll be uh oh it's a really
good question we would probably throw
away its interactions as well because
we're scared of it yes although it's a
little darker when we start throwing
away interactions it's it's a very good
question we need to do more research on
that yeah but we sort of have a model
right now that things are TIF if they
looked anted we tend to eliminate the
whole thing
this is just sort of interesting yeah I
knew this data set pretty well 15 years
ago it turns out there's this incredibly
flat spot for a heart rate that's kind
of normal and as soon as I saw this
graph I thought oh my code is broken
yeah something's not working I looked at
the data it turns out there are no
patients in this range all patients who
had quote normal heart rate which is a
pretty wide range
we're just coded as zero which is common
especially in older medical datasets so
there's actually no patients here so one
that points out a weakness of the
current model which is I mean there's no
patients here so it's not really making
a prediction for anybody but if a
patient did fall here in the normal
range it would predict a plus point 2
for them and I'd really rather have
predicted a 0 for them if they fell
there but what was interesting is by
seeing this graph I just sort of
instantly knew there was something funky
going on that I didn't understand and it
only took me a minute to then realize
that there was this property in the data
that I had never realized before so so
this turns out to be a very valuable
tool for understanding the data ok so
now I want to show you one other thing
and then we're actually done ok so now
I'm going to jump to we've been looking
at the models overall trying to
understand the entire graph and all of
the models let's now look at the models
for some specific patients and now we're
gonna look at 30-day readmission okay so
we've got hundreds of thousands of
patients and 4,000 different terms so
here's a patient their probability of
readmission is 0.08 turns out that's
about baseline on average about eight
nine percent of the patients readmit why
does it what we've done is we've sorted
all the terms there's 4,000 plus terms
in the model we sorted all the terms by
how much they contribute to increasing
your risk and at the bottom by how much
they contribute to decreasing your risk
so we're just gonna look at the highest
risk terms I'm not gonna show you 4,000
terms we're just gonna look at the top
five or a ten so why does it think this
patient is you know what what increases
the risk for this patient well they've
been in the hospital six times before
that's a fairly large number so that's
interesting so that adds something to
their risk it turns out electrolyte
imbalance is something that's not
rate it does increase your risk a little
bit it turns out they've been in the ER
also six times which is different from
being an impatient six times it turns
out they have combat fatigue and they
have drug intoxication so they have a
drug problem so now think about this
here's a patient forty-two hundred terms
for this patient and we've just sort of
looked at five or six of them and I
think pretty quickly we've identified
what are likely to be the most important
factors for this patient being at risk
right now of coming back to the hospital
so so I think that's interesting that by
looking at the term sort of this way and
this is a standard trick in logistic
regression you look at the terms that
increase risk the most and decrease risk
the most but because we get to see
graphs it's it's just a sort of richer a
richer thing to do and I'll show you two
more graphs and then we'll just stop
here's a very high-risk patient so this
patient has a 92 percent chance of
returning to the hospital in 30 days so
why fat intravenous preparations so
they're trying to get food into this
person intravenously as much as they can
and it's a very high score this is
adding 0.44 to their risk just that one
factor is adding adding that much to
their risk they have sadly cyclical
vomiting syndrome this is something that
young children have sort of aged one to
three so they're having problems keeping
food down and they have a particularly
high risk value of it so it's adding
that much to their score and they're
receiving I've looked up these things
they're receiving preparations to help
prevent nausea and to try to keep food
down so they're trying to treat this
this patient the patient they've been in
the hospital they're young it turns out
they've been in the hospital 64 times
okay so they've been in an amazing
number of times so of course that
increases the risk all by itself and if
you look at the next graph they've been
in the hospital
what eighteen times in the last twelve
months so their average time between
return is less than 30 days so of course
the probability of returning within 30
days it is gonna be high okay so I think
I think that's it for looking at these
models we can look at you know here's a
patient who's also very high-risk
they're very high risk because they're
receiving abnormally high doses of
chemotherapy agents and the only way it
turns out to receive those very high
doses of chemotherapy agents and agents
with which help you tolerate
chemotherapy the only way to do that is
if you're sort of receiving a second and
third round of chemotherapy that you're
not responding to treatment okay so let
me just go back and wrap up so so that's
what I think I think you can see by just
looking at sort of the top ten terms
we're able to get you know a pretty
quick summary of what's happening with
this patient that's relevant for the
prediction tasks for 30 day readmission
it's kind of like you know two doctors
walking up to each other and saying uh
good chance this patient's coming back
they've got this this this and this so
so it's kind of like that story despite
the fact that there's over 4,000 terms
in the model we've only looked at a few
of them and we feel like we have you
know maybe some understanding of what's
going on with this patient the vast
majority of terms for the vast majority
of patients are in that sort of longtail
in the middle which have sort of very
little consequence because they don't
really apply to most patients so okay so
let's uh let's just summarize we're
getting remarkably high accuracy on our
healthcare problems with this this
method higher accuracy than we ever
expected to get in fact and that's
because logistic regression itself is
not so bad on some of these problems
especially if you shape the features you
know with expertise we think the terms
are reasonably understandable
intelligible transparent I don't know
what the right word is they're
definitely not causal though in some
cases they probably are causal but it
would always require a lot of extra work
or expertise to be able to validate
whether what we're seeing on the graphs
is is causal I'm very happy that on the
pneumonia dataset it rediscovered the
problem
that made us worried 20 years ago and
it's kind of nice that it discovered new
things that look just like that problem
so we were actually you know probably
justified in being worried about
releasing that bottle so that's great
and I'll just jump there's a lot of work
to do
it's this is an interesting class of
models also for engineering because if
you're doing machine learning as part of
your product it's important sometimes to
understand why the model works what it's
doing inside so that you can then better
better improve the model the next month
I know one of my co-authors used this
for a problem and within a day they
discovered a problem in the data set
that they didn't know about just because
it learned something that seemed
completely wrong they investigated it
turned out there was a very important
feature that was broken in the data set
so it's kind of nice to be able to see
your model and understand what it's
doing it can be valuable in a lot of
ways but we need to do all sorts of
things like user studies before we
release this in the wild and you know
assume it works I'm particularly
concerned about people trusting the
model too much and interpreting it as
being causally correct because it's it's
not if we were to just delete some of
the terms and retrain the model
everything is going to shift around it's
not like what it learned about age is
correct about age it's just that what it
learned about age is exactly what it's
doing its predictions based on so you
can see exactly how it's making its
predictions it's not that that's like
the true graph for age that you would
learn independent of all of their
features so it shouldn't be interpreted
that way ok if you have any suggestions
for how to do causal analysis of this
where to apply it how to improve it how
to better regularize it all those sorts
of things we're really in the early days
with with this is it's a it's a it's a
new technique that we're just working on
now so we would love to get feedback
suggestions criticisms things like that
so thank you for staying past time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>