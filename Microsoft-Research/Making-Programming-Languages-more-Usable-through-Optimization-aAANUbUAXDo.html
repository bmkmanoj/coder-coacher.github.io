<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Making Programming Languages more Usable through Optimization | Coder Coacher - Coaching Coders</title><meta content="Making Programming Languages more Usable through Optimization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Making Programming Languages more Usable through Optimization</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/aAANUbUAXDo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay so it's a my pleasure to introduce
raw state here who's interviewing today
and so Ross is a student of soaring
learner at the University of California
San Diego where they do have nice
weather and he's well known here at
Microsoft already because he did two
internships here and he won the
Microsoft fellowship one year he did a
lot of work on a category theory of
effects and compiler optimizations and
he worked with the redhead on a new
language called Salem and today is going
to talk about to us about usability to
optimization so low as Dan said I'm from
UCSD and I research program languages
and one thing I've been working towards
making programming which is more usable
and how I've been going about doing that
is by improving the technology for
program optimization but before really
get into all that I want to give my
overall perspective of program languages
so the way I see it people have this
amazing ability for intuition and
creativity and they've applied those
abilities in order to build computers
which complement them with the ability
to process mount large amounts of data
and calculate with high precision and
which it since their invention have
become ubiquitous in our society yet
these computers are really useful to us
if we can't get them to do what we want
them to do and so I view program
languages as it sort of means
communications between these two worlds
that enables people to enable computers
to enable society now having this role
this means that programming languages
suffers from all the problems that
people have all the problems that
computers have and all the problems that
communication hats and so there's a lot
of ways that we can work towards
improving programming languages one of
these problems is that programming woods
always have the balance between human
usability and computational efficiency
and so this is where my work comes in
I'm working on proven technology for
rural optimizations that way we can take
the emphasis off Optimus on
computational efficiency and focus more
on human usability the reason why I
think this is important is that in my
experience I seen that a lot of people
will sort of always keep her
keep efficiency on their mind as they
program and had a recent example of this
pop up at UCSD in our grad student
lounge where someone had written this
program here and the details of this
program aren't very important what is
important is that another random person
came along about a week later in settle
by the way your program sucks it could
run faster by taking this out of the
loop and then another week later another
random person came along and said well
this is only case of the optimizer sucks
typical optimizer will take care of this
for you and another person says yes
that's rely on optimization technology
but then another person came along and
said well how do you know the string is
immutable right if the string is being
changed in this valley of sterling can
change as well but then another person
came along another week later and said
well actually that's a matter presumably
because you can look inside this loop
and inside sterling and see that needle
and modify the string and so it's
effectively immutable through this
through this block code and so the value
won't change and so there's this month
long discussion amongst a bunch of
random grad students about this yes yes
sorry Fred I usually say that I forgot
things besides numbers well so so yeah
why more detailed than we were thinking
at this point uh-huh so the interesting
thing that is that there's a month-long
discussion amongst random grad students
about random program and this whole
month a single person knows off by one
error here and so this bugs me because
it means that we're focusing on very
detailed things like efficiency and
whether it's being a pile optimizer not
compiled when we don't even have a
correct program yet and the prove this
actually happened I took a picture and
you can see I remove some of the
inappropriate comments and with a goal
of my research is to make us that you
can write the code do what you'd like to
not worry about performance efficiency
you let the Copiah take their care that
for you in that way you can focus on the
more important things like optimal I
correctness is automatically initialized
for you yes I think there should be
somewhere up there and yeah there's
there's many things wrong with this code
again is someone like I guess probably
just teeing
okay oh yeah yeah but um right so
there's already a lot of stuff on
program optimization right and the issue
though is that a lot of this technology
out there relies on sort of a lot of
repeat Emmanuel imitation effort that
makes a sort of a black box that the
typical programmer can't do anything
with it's just something that works by
magic and they can't affect it and so
what I've been doing is going through
these technologies and replacing them
with these more reusable axiomatic
systems that are automated and have a
more open interface that programs
programmers can interact with them and
so some areas have looked into so far
things like translation validation which
make sure the compiler or the optimizer
does what it's supposed to do and
doesn't actually change the semantics of
the program and looking into things like
extensible compilers making it so that
we can easily add new optimizations to a
compiler and even inferring
optimizations entirely automatically so
that optimization will be available for
new languages as they're being designed
now these three applications here all
sort of unified by this one technology I
came up with what we call equality
saturation in this sort of an axiomatic
way to reason about programs now this is
the overall layout my talk so feel free
to ask questions any point right now I'm
gonna start off with translation
validation and I'm going to do so by
asking you all the question so how many
of you have had the compiler actually
inject a bug in your code that is you
went through all this effort to make
your code nice and angelicum pletely bug
free only to hand off to the optimizer
which through some fault turned it into
some demonic code that does that does
something that you didn't tell it to do
so for those of you have been so
fortunate as to avoid this situation not
cause the situation let me let me
enlighten you as to how this goes so if
you run into these bugs are absolutely a
nightmare because you can look at your
code for hours and say it should be
doing this it should be doing this I
don't get why it's doing or not doing
this and the fact is it should be doing
that because your code is fine it's the
compiler that's wrong and you just don't
realize it and furthermore whenever you
try to observe the bug say by inserting
print lines or running the debugger it
shuts off the optimizer so all of a
sudden the bug goes away so it's like
this quantum physics happening inside
your code which like quantum physics can
be very confusing for a programmer
furthermore once you finally figured out
that the compiler is at fault they need
to figure out how to rewrite your code
in some weird way to make it so that
stop introduces stops introducing this
bug anymore and all your co-workers get
confused as to why your code is so ugly
so as you can see this is frustrating
for a programmer but it's not only
frustrating for programmers also
frustrating for companies and many
companies have a policy of not using
these optimizers because they can't
afford these kinds of books and they do
so I'd rather high cost after all this
means if they pay all their program is
to do these optimizations by hand and
head optimized cozen be more difficult
to maintain over time and so these costs
accumulate over time as well and so
these companies would really like to
able to use one of these optimizers even
though it's a little bitchy because
typically they do work right so how can
we go about doing that well I can
consider a single run of the optimizer
we have this original program in
optimized program we can incorporate a
technology called translation validation
and what this does is it looks at these
two pieces of code here and tries to
figure out whether or not they're
equivalent if it succeeds that means you
can use the optimized codes safely
because you know it's the same as the
riddler code you haven't introduced any
bugs and if it can't figure this out
well they need is default back to the
original code just to be safe now
there's many ways the bill translation
validators most common one is to use by
simulation relations this sort of try to
figure out how these two programs walk
together step by step but these things
have some difficulties with bigger range
rearrangements of code and so we've
looked into another way to do
translation validation using equality
saturation and so they illustrate my
technique and you start off at the very
base
kind of program basically just simple
expressions here and will elaborate some
more complex programs as it go and we
consider okay this I times P plus J
times 2 will hand it off to the
optimizer and it turns into I plus G I
plus J let's just buy one and we want to
figure out whether or not these two
programs are equivalent now the idea I
had was let's take these programs and
turn it into ninth nice mathematical
expressions and once we're in this this
nice mathematical world we can start
applying nice mathematical axioms the
reason about them so we know for example
that anything or electrician by one the
same thing as multiplying by 2 so that
tells us that the optimized program is
equivalent to this intermediate program
here similarly we know that
multiplication just use their addition
and so it tells us the immediate
programs will come into the original
program and so just by using this very
basic language axioms we can figure out
these two programs were actually
equivalent to each other now this works
very well for these nice clean
mathematical expression programs but we
wanted to make this approach work for
more realistic program programming
languages like C and Java and so we had
to figure out how to accommodate
challenges like loops which are going
until later on and effects and the issue
is that a typical imperative languages
have things like statements which can
not only do mathematical equations but
also can read from the heap and modify
the heat so we want to figure out how to
represent these statements as
mathematical expressions and so to do
this what I came up with this sort out
this concept of effect witness and when
it does it takes us dereference of our
here and it represents us as the
expression load from location are from
the current state of the heap Sigma and
so all uses of heap are explicit in our
representation similarly when we modify
the contents of our here well then we
map this to a store which Nala takes the
value store and the location store too
but all sticks estate of the heap it's
modifying and the returns the new state
of the heap after the modification is
done and so all modifications are deeper
also explicit our representation now if
we consider this program here it's a
more detail what we're doing is we're
taking the contents of some location and
then paying those contents back into the
same location and so assuming a
reasonable memory model this program
really doesn't do anything whatsoever
and so we'd like it will do reason about
that with our mathematical expressions
and the way we do so as we say whenever
you have expression that
looks like this one well then this store
is actually gonna be equivalent to the
original incoming heap Sigma and so with
these kinds of occasional reasoning not
only can we reason about the values of
programs will be given reason about the
effects of programs so with this
approach I designed a translation value
that works for see and implement so far
for c and java and it takes these two
programs the original optimized program
and these programs come in the form of a
control flow graph right the standard
representation of compared to programs
but this isn't very good for algebra
reasoning as we found out and so what we
do is we came up with new representation
one that I call a program expression
graph when you converge to that and the
reason why we call it a program
profession graph is that it represents
the entire program a real entire method
as a single expression that forms a
graph rather than the tree because it
has some recursive loops in it and so
once we have this nice mathematical
representation well then we can move on
to recall the saturation and what this
does is it applies as algebraic axioms
in order and further publicist and
hopefully you can figure out these two
programs are equivalent now they get a
more detailed picture of this let's
consider this program here and don't
worry about the details of this program
because just contrived to illustrate how
my technology works but let's suppose we
hand this off to an optimizer and to
spit out this optimized program here
well we can all look at this and say
well what it did is it took these two
differents dereference 2p here and sort
that into a temporary local variable
that use twice similarly we had these
two multiplications by be here and we
simplify that to a single multiplication
by to be and so with the translation
value has to do is figure out how to
figure that out entirely automatically
and the way it goes about way ours goes
about doing that is first taking this
original version of F and converting it
to your program expression graph our own
representation and so to do that we take
this due reverence of P here and
translate that to a load that takes
location P and the incoming say the heap
Sigma and then this called a stir car
here gets translated to this call with
these two parameters s and the result
that load operating on the current say
the heap Sigma and has these two outputs
and what these two outputs correspond to
is this pi v node is the returned value
of the function call that is the value
being explored leading to this X here
whereas Pi Sigma is the resulting state
of the heap after the function call
completes in particular when we had this
neck
reference of P here well then we use
that new state of the heat better than
the original state of the heap and so we
distinguish these two loads in a
representation now then we go on to this
edition multiplications here which we
store into the heap in order to get a
new state of the heap for the function
and lastly when we return next we mark
that this pi v is the returned value of
the function whereas that store is a
return resilience a the heap out of the
function call completes once we're done
with the original program well then we
move on to the optimized program and do
the same process which I'll skip over
but we're going to reuse nodes as much
as possible in order simplifies and
speed up the process and at this point
once we have this nice mathematical
relation we made sure it's complete so
that we can actually throw away the
original control flow graphs and just
work on this mathematical system and so
we can start applying axioms so one is
the fact that multiplying by 2 is the
same thing as last written by one and so
knowing that we can add this equivalence
here and note that we're adding an
equivalence we're not throwing away the
original program and this is how we
differ from a lot of other techniques
which are more destructive and it's how
we make it our system able to adapt to a
variety of compilers in a variety of
optimizations but taker in this
situation this lush it by one isn't
actually useful for for the translation
valid to validate this translation
rather we want to use as addition
multiplications in applied
distributivity once you're done with the
math up there we can move down to the
function call and we can incorporate
some knowledge that we know that llvm
this optimization framework that we
targeted is using in particular olivium
knows that this called stair car doesn't
modify the heat and so the stated heap
after the function calls can be saying
the save the heat before the function
call and so we can add this equivalence
to representation and once we've done
that well we have these two loads from
the same location and now we know that
we're operating on the same state of the
heap and so we know the same the result
although it's also gonna be the same and
the constantly the result is addition
doing the same there's all these
multiplications are going to be the same
then transitivity tells us this they
quote this edition's cousin that
multiplication so this leaves us to
these two stores and we can see they're
operating on the same state of the heap
at the same location and now I know the
story in the same value and so they're
going to result in the same say the heap
after a soaring as well and so we just
proven is that the original f an
optimized f have the same overall effect
and so all this all we have left to
prove that they have that return the
same
explicit value this is really easy in
this case because they actually start
off with identical values in a
representation and the reason why that
happened is that a representation being
mathematical gets rid of all the
intermediate variables and so a lot of
syntactic Vince's differences between
control flow graphs sort of go away and
in fact many optimizations are validated
just by translating to a reputation
without even how to apply these kinds of
equivalences so Bailey's house will use
to know if we have a loaded store that
they are resulting effect witnesses sort
can commute with each other so we have
two stores and they can be passed each
other if we have one load we know that
on to if we have a load and store and
they're the same location that we just
use the same one but if we can prove the
different locations then we just reason
that the loads it's going to be same as
well before the store has after the
store there's been severe
so I mean you have you talked about how
do you make an app like the alien house
itself floral reckoning there's the
aliasing on each program but there's
also sort of global Susan boss oh it
seems when you're moving to so I like
the fact that there's two programs
actually doesn't really create too big
of a problem Braley's now sing because
yeah they're still values within the
programs and so it's usually within the
programs and you're the operators
commute and then once you figure that
out then you'll figure out okay these
two operators are the same across the
programs so they'll say it now seems has
to be within two programs or within the
program itself and so it's not a big
problem of this two programs i goes is
program one specific at least
information that you used to find your
so we use so so what i'm doing here is
actually very simplified like we have
things like ordering analysis and stuff
like that so knowing whether one injure
is bigger than other and stuff and we
have ALS alias analysis as well and
these can all work on top of this kind
of basic system yeah usually yeah yeah
so ailis analysis because we're doing
this entirely automatically we often
don't have a alias information so
sometimes so actually synthesis problem
we had with ella vm was that even though
we told it not to it would do an inter
procedural analysis to figure out alias
information and then do optimizations
that weren't valid from what we could
know but if we had to set up so if you
gave it to the gave it to us if he
actually gave us the information then we
could start applying it so you have a
way to incorporate it is yes
so you're kinda cool because under not a
freak on history what if there are
preconditions that able the opposition
right so this is no mrs. base is seeing
things alias right away this is there's
a there's some precondition some fact
I'd your inputs that you don't that we
don't know about and so if we if you
give us information then we can and we
have the collage it for reasoning about
it like we do prale's now sing a few
other things then we can prove sup about
it but we can't wait we can't infer the
context we're entirely interprocedural
system so basically what you're saying
is give me any kind of proof that your
program and if I can use it to add more
equivalence arrows there you have to be
equivalences we are there's actually
systems we owe you can put arbitrary
logics on top of this one arbitrary but
mini logics I'm hoping it's you can only
use it if you can improve your branch on
that right I gave its turf heart you
improved it because you added this
yellow equivalence says oh it doesn't
multiply the Prairie an alias you would
do the same so what other things we can
do besides adding oh I mean with aliases
well the alien house is what you do is
you try to tell us these things are
distinct and then will infer the quinces
from that so we'll do the olin further
equivalences you have to give us the
equivalences way so we like same with
Lenore inequalities and so like that
sometimes we'll take advantage of
inequalities and use those infer things
or particularly bounds and injures to
deal with overflows and stuff like that
okay so going back so I've just we've
proved that the returned values are
identical and the overall effect is
identical so we basically just validated
optimization right and we're able to
validate a wide variety of optimization
so desist this out we ran this on the
lvm through the spec 2060 benchmark
suite and this last column indicates is
that if llvm made some sort of change to
the program then three out of four times
we build validate that change and what's
cool is that another team of researchers
actually came along and implemented
their own translation validator
specialized toward elevate even
specialized vs. exact configuration of
l'a BM and while this did get them this
did while this did get them significant
performance improvement they actually
were only able to match our validation
rate that does they actually weren't
able to improve their success rate be
cut by the specialization process and so
this isn't poor sorry to get question
okay so this is important to recognize
because if you consider these two
program or approaches side-by-side
specialization requires a lot of
knowledge right you have to know how the
language works but also you have to know
all the optimizations the compilers can
be applying and you have to even know
what order those outspends is going to
be in and so using the special edition
purchase requires a lot of repeat
emmanuel implementation because this
means that you have to make a different
translation valor not only for each
compiler your validating but also for
each configuration the compiler you're
validating whereas with the quality
saturation all we added know where the
barrier where the basic axioms about
these language and we're able to take
care of the rest I honor entirely on
their own and this person particularly
important is that a quality saturation
going to even validate optimizations
it's never seen before and this will be
important for this next application i'll
be going into but question on the
previous chart yeah are you one of those
percentage
that referring to
series of optimizations that are all
done together and about it or individual
organizations are involved this is this
is method in method out you know how
many methods were able to do so there
could be many optimizations and usually
that's what gets messed up is when some
too many optimizations happen and some
important intermediate state gets lost
that sort of tends to cause the whole
app that goes on so if it's if there's a
train that doesn't sort of like erase
and pass information then it tends to be
fine but if it erase the past
information we and then basically all
the other approaches that we know about
get stuck at some point we did we turned
on everything interrupt isidro so as I
said they actually so that some of these
are because actually use interprocedural
information as well so that's just
something we can't do anything about
without somehow painting lbm but uh so
this this is that like you know run into
procedurally and then then we validated
what we could
it's all my artists technical
translation validation is that the
queries to the theorem provers sorry
three very simple once win for the
simulation relations right whereas in
your case that constructing this igri
we're introducing new thoughts on flight
right so is there a danger of sort of
getting to a loop where you keep
anything goes to the ground yeah yeah so
so for our thing it isn't it doesn't
necessarily terminate always we in our
practice we've had into that or in
practice if there are no loops of the
things and then we will terminate but if
there are loops then one of our basic
axioms actually getting to later on well
guarantee that won't terminate so we
back we do it we found a dep restore
yeah yeah because you there's just
infinite number of options at that point
and so what we found is that a
breadth-first search through the
sessions works better much better than
death for search because it sort of
prevents you from going down a rat hole
make sure that you try your options
broadly rather than getting that we go
through this loop okay keep going
through the loop keep going through loop
we don't get stuck into that kind of
trap the one thing if you add this put
your expression to need an eternity
notice page
um one of the big things that fit is
that our stuff will reuse nodes as much
as possible so we get LC somewhere that
later on but even if you put the axioms
for like the associativity and community
that's like a huge blow-up with number
expressions at equivalent but a
representation that's actually quite
compactly represented all those
equivalent expressions so yes there's a
live variety but because we had to sort
of additive approachin because we reuse
things and say here's equivalency ozan
equipments and we have this nice
locality aspect of it right you can
reuse subject Russians a lot tens
actually not blow up in our
implementation until we start having
boots can you explain the difference we
hold an optical engine so this is all I
we're considering all methods and is
that we're only three methods where they
optimize are actually made a change so
last time the optimizer doesn't just
says you know input output can't do
anything and so this is just the only
cos ring optimized once what was the
proportion of the sale 1864 functions
that were actually changed get out of
them just grow grow from this table I
don't know it okays just looking the
numbers of say it's not huge but yes
unfortunate any more questions before we
go on yes so your conjecture list of all
the failures or because of
no not all the theaters like talking
with that people have worked in this
area stuff like that things that come up
are basically too many optimizations
have been applied and or I mean it's a
good thing I levelers going to apply but
it's done so in a way that's somehow
some important intermediate state got
lost and we can't figure out that
intermedia say that connects the two
sides together so as you saw like often
it's often you've sort of drift from one
side and to the other side and you
converge in the middle but if something
if something has been done that makes
that middle state just not there anymore
we can't figure it out and basically I
racing to get stuck in that situation
all right good to go so back so with
this approach now with translation
validation we're making its that
companies can actually use the compiler
or use an optimizer even though it's
unreliable but still still safely do it
with translation validation and after
doing that I worked on making so we can
actually extend optimizers with new
optimizations and make this accessible
for typical programmers and the reason
why I thought this is important was that
back in my energy days what I remembered
having to do is a lot of optimization by
hand I'd write some program like this
image processing program here and I'd
realize that this item 50 + J isn't
really the best way to access this image
rather be doing image + + in order to
get rid that Mulligan from side the loop
and so once I recognize this I have to
have this choice to make between this
keep you around this program that's easy
to understand not only for me but more
importantly for my co-workers and this
other program that likes to keep more
efficiently and this choice comes up a
lot because of many optimizers including
geez three pieces 03 won't actually
perform this optimization and so if
you're running an image processing
you've probably run in situation and you
may have thought okay well why would I
extend the compiler in order to take
care of this optimization for me so that
way I can keep around as intuitive code
but execute this more efficient code and
let's consider how much work is involved
in that well if you're so fortunate that
the optimizer actually is open source
well then you have to do is check out a
copy the source code learn the architect
for the optimizer and then implementing
optimization even though you were doing
image processing so you may not be
aware of programming language techniques
then you have to integrate that into the
pipeline and compile your new compiler
distribute to your co-workers but before
you do that you should make sure the
debugger implementations since after all
we just talked about how compiler bugs
and be quite annoying and you don't to
be guilty of those and then you have to
deal with the fact that this compiler
you just checked out is going to be
being upgraded by the community as well
so have to merge all those upgrades with
your changes in order to make sure their
team has an update compiler and so you
can see this is quite intimidating
amount of work that's scared off a lot
of scares have a lot of programmers
including myself and so am i this why I
did is make a train a train of optimized
ER and for this in order to extend it
with that optimization I showed you all
I have to do will give a single example
of the optimization in fact that example
I showed you just showed you work just
fine and from that alone will take care
of the rest for you and there's a little
catch this though which is that sure I
can learn this exact optimization you
here right but how often you can be
writing programs that work on 55 50
images not that often so really what you
intend me to learn is some more general
versions of optimization one that would
work for any W neh and a youth any any
use of image i times h + j and change
this into a use of image plus + and
furthermore you want me to learn a valid
optimization when that doesn't introduce
any bugs and so you hope I learn sigh
conditions like this HS of your loop
invariant and this use can't modify IJ
your image now recognizing this is
really your intent oh yeah
you know the image to impose plus juice
yeah with many more site conditions
which is why we weren't with one don't
want people to have to deal with this so
so recognizing this intent where it is
or is make a system that will take this
concrete example and automatically
figure out how to generalize it into
this more broadly applicable form and
the inside I had for doing that was that
these two does this optimization he gave
us here needs to be correct otherwise we
don't want to learn it in some
particular these two programs need to be
equivalent so we can understand why
these two programs are equivalent you
can understand why this optimization
works and how we can go about
generalizing it and so with that made up
an architecture for a trainable
optimizer and we assume that the
programmer gives us these two snaps out
to the program before and after for
which they want us to infer the
optimization that they applied and what
we do is we hand these off to Jason to a
translation valid which diode li make
sure the programmer didn't make any
mistake in this optimizations what else
it gives us a proof these two programs
are actually equivalent and this proves
important because within it we can
determine what details of the programs
are important which ones aren't
important and can be generalized and so
by generalizing this proof you can
actually get a generalized versions of
the input and output programs giving us
a generalized optimization for which the
original one given to given to us by the
programmer is concrete instance and so
with this architecture where we enable
is a way to teach or is a way to enable
program is to teach compilers Bradley
click off the musicians that are
guaranteed to be correct by writing just
one example of the optimization being
applied written language they already
know rather than some compilers specific
optimization and so these I believe this
makes extensibility very accessible to
typical programmer now this get some
more detailed Ernst training of how this
actually works let's say this programmer
knows I'm giving repins our presentation
and says Ross please learn that a plus a
minus a can be transformed to eight and
so I'd say ok well I'm going to hand
these optimization evaluator which is
going to prove that a plus 8 minus eight
actually equals eight and then we want
to generalize this proof but before you
do that we have to look inside the proof
and so let's take a look at what this
translation evaluator actually does so
the translation validate really starts
off the sauce starts off nothing
absolutely nothing about the
equivalences of these two programs right
really all noses properties about the
overall language is working with so it
knows for example that anything mice
itself equals zero and you can use this
fact to infer that eight minus 8 equals
zero and so as it runs is going to
construct a proof of this database in
particular is can say that imply this
axiom we could add fact one to the SATA
base we also know that is something is
equals zero that anything plus that
equals itself and so using this axiom
and this fact we can infer that a plus 8
minus 8 has equal 8 and again we make
note that by using fact one and plain
this axiom we're able to add back to the
database this fact to is important
because it actually proves this
translation valtor's translation or
transformation given to us by the
programmer is in the fact that oh yeah
sort of assuming that this parentheses
there so if the parentheses are there
then we're not used seeming
associativity but yes that's another
axiom we can add on sorry going back oh
yeah so we just proved that this
translation given Super Grover is
correct and so we can move on to
proofing ization now the thing I found
out is that proof foundation actually
works best by going backwards to the
proof so I'm going to going from right
to left and to see how this works we're
going to move these axioms out of my way
and you can start up with some dinner
program a transforms in some general
program B and I want this to be valid
and so I need to prove that a equals B
in the way I'm going to go about doing
that is going to look at this concrete
proof here to figure out how I can
refine a and B so is that they actually
are equivalent as I do so I'm going to
maintain the invariant this is the most
general program transformation for it's
a portion of proof I process so far
applies since I haven't processed any of
the proofs of power start off with the
most general program transformation now
to start refining things as I said works
works best by going backwards so we're
going to look at the last axing that we
applied this one here in order to apply
this that means there had to be some C
so set C and D so sorry news to equal
zero and we use that to infer that d
plus d is equal d and so we can look at
a concrete proof to see how we use this
in particular our proof tells us that we
apply this axiom using fact one and so c
equals 0 has to be fact one and so we
can add that to our databases is a
future goal to prove similarly if proof
also tells us that implying this
accidentally added back to and Saudi
plus two equals D has to be fat
too but this time you're I do you have
effect two namely a equals B and so to
reconcile these differences what we do
is we can unify these two facts we can
take all uses of a and replace them with
D plus C and we can take all uses of B
and replace them with D and after doing
so it actually makes complete sense to
transcribe our notes from below to above
from below to above after all of you use
this fact one you can get that back to
and note that when I was making that
substitution I also made the
substitution within the generalized for
information and so I restore the
invariant it's the most general
transformation for which the portion the
proof that process of our applies a
particular if I can figure out how to
refine see so that actually equals zero
then that transformation will be correct
so to go about doing that again let's
say we're going backwards so we're gonna
look at the previous axiom I apply this
one here and in order to do this there
had to be some e so we inferred that emi
c equals 0 now this actually makes no
assumptions and so we can start off with
the empty databases what do you expect
from a good proof and then we can look
at our notes to see that the plan is
actually a defect one so you might see
equals zero has to be fact one but once
it again we already have a fact one
namely c equals 0 and so once again we
reconcile these differences by unifying
these two facts in particular we're
taking all the uses of c and replacing
them with e- e and once we've done this
we could transcribe my notes again and
say though if you add fact one or you
could buy this action to add fact one of
the database and in so doing what I've
just built is a generalized proof that
this strand lies transformation over
there d + cmic transform de or 2d is in
fact valid and our original concrete
proof and concrete transformation are
just an instance of this realization by
taking all the uses of dnd replacing
with hates so by understanding why this
transformation given to us by the
programmer is in the back valid we're
able learn a more broadly applicable
optimization particular by examining the
proof of equivalence of these two
programs are able learned opposition
from the programmer and so to recap what
I did at a higher level I hand these two
programs off to a translation valuator
which gave us as proof that they're
equivalent and this proof said or proof
works because these weights are the same
and because these weights are the same
and so we had this proof off to the
proof analyzer at then inferred or
maintain those equivalences in the
device transformation however
the proof didn't need all four eights it
to be the same and so we could use two
different symbols DNA in the generalized
transformation and what I proved is that
using this process well I'll automatic
well sorry we'll always learn the most
general optimization for which is a
proof of validity and so with the strong
guarantee weibull are in a wide variety
of optimizations from just single
examples of them being applied this
inter lip strengthener reduction is in
fact that and that image processing
example or optimization a showed you
earlier we dry food as equivalence you
used it the technique you just described
like thanks again the uses so so these
are like last days have loops and stuff
and so if we want to do loops that we
have to use pegs and same as if there's
anything of side effects you have to use
pegs so yeah so I showed you as a very
yeah a little sore so use the same
techniques JSO rebuilding after the
college separation can refer so
underneath to make this sort of to make
that technique i showed you actually
work for realistic programs that we use
pegs in the claws hydration on top of
everything
and so here so the cool thing is with
this approach and in fact that well in
the fact that our technique actually can
adjourn rise through other adverbs a
nation as well what this does it tells
us that since we build up translation
validation as a technology for
translation validation improves so our
ability to learn new optimizations from
programmers these things like putting
the multiplication inside the loop and
to get rid of sometimes you can put a
multiplication side of loop to actually
get rid of it I just mean you through
everything and other times it's better
to pull factor it out and put it at the
end of the loop so that's just sort of
moving operations into and out of the
loop so can you
or how hard would it be to do something
let's move interchange rolling um
unrolling should work right well so
there so by enrolling do you mean the
one way to take the loop and then make
sort of double copies of it or do you
mean the one where you pull out one
iteration of the loop lot of what's
called okay some people use the lad I'm
rolling to the ladder 12 so that's why
we'll ask okay so the other one we
actually haven't done that because
basically our representation I should
i'll show you earlier or later on sort
of has iterations kind of tied into the
semantics of it all so we have looked
into ways to get rid of that we figure
out you can do sort of meta operators
that allow loop unrolling but we haven't
actually tried putting that into
practice loop healing is easy and I'm
feeling thank you oh and it's actually
dependence testing to prove it correct
it's probably go beyond the purge system
at this point so looking at lip
interchange it depends on what how
exactly how they're bundled so we can
figure out often what we'll do is alou
if we do or enter a list i'll sing ahead
of time in order to figure out how to
turn it into a peg better and in that
case we'll we actually get these two two
loops actually be completely separate
expressions and so loop interchange is
extremely easy to do in that situation
because they're actually already
separated in other situations is much
more difficult because you have to
figure out there to sort of figure out
after the fact that they can be
separated separated when it's a little
harder to do sometimes we can do it
sometimes we can't and so compared with
my simulation relation like my
simulation relation can do things like
loop loop unrolling better but they loop
peel or loop interchange they have a
terrible time with so there's some pros
and cons to the the two different
approaches and what's interesting is
that the two different approaches seem
to have the same walls I was talking
about earlier in the translation
validation so again because this all
works out translation validation if we
use a by simulation relation translation
validator then things like loop and
rolling would be would be just fine but
here since we're using our quality
education approach things like I'm
rolling our difficult but loop
interchange is better
this work practically I mean if you can
do this why have all this code in a
normal compiler handwritten that you
know that implements these different out
positions so so what there is there sort
of like a detail here which is that as I
said there's the most general condition
for which this works but what does
generality mean and so in our situation
we formalized what generally generally
means and depends on the logic that
you're working with forth like that for
that proof and so generally for this
process you need sort of a first-order
logic to do all this stuff and so up
when you implement optimization by hand
you can basically do things that require
higher Dora logics and so there's some
optimizations that are better done by
hand because they actually will work to
more broader situations but the reason
you don't want to do that all the time
is because there's a lot of
optimizations that are only useful for
certain domains and so you really only
want to learn like if someone's in that
domain then they'll say ok learn this
optimization and have it there so I
wouldn't say you should go all the way
this way and once they should go all
their their way really it's a matter of
balancing the two shows us far worse
results problem for optimization later
on the issues that come with performance
or about optimization well part of it
was that we because we chose lob em and
Java or a bike code that they're a
little too high level for a lot of
things so if you can find some programs
that have some key bottlenecks and you
actually put these optimizations in then
then you'll get good performance results
the issue is is there a bottleneck in
the code that's actually something that
you can optimize if there isn't which
often the case then you're not going to
get big performance results so there's a
big issue with evaluating optimization
in general so I can go into more details
about this horror optimization
evaluation problem i found out when i
start doing this research but later on
i'll actually show you something of some
success we had with the ray tracer where
there was a big bottleneck and i had to
do with these kinds of optimizations
okay so now that mates that we can
extend the compiler with any
optimizations by just giving example
another thing I've looked into is making
so we can for optimizations entirely
automatically or given the properties
the language the reason why I found this
to be important was that when you make a
new language optimization tends to be a
big hurdle in order to get that language
adopted and you might wonder well there
aren't that many languages being made
every year but in fact many companies
like the old video game companies I
worked with actually have their own
in-house language that's maintained by a
single person who would really like to
have an optimizer but doesn't have time
to implement an optimizer and so the SEC
knology would benefit them and also
domains is igg languages are becoming
more more common and so by covering my
technology you could learn the main
specific optimizations for those inmates
pacific languages and so to see how this
works i'm going to sort of focus on one
very classic optimization known as into
loop or sorry its loop addiction
variable strength reduction and what
this does it takes this program here and
it translates into this program over
here and particularly get rid that
multiplication from inside the loop the
way it goes about doing that said okay
well this four times eyes can turn this
loop variable J and accommodate that
changing our changing comment by 12
degree by four and your change the
bandits hand to a pound of 40 and reason
why you might typically we go about
doing this is because it's very useful
for things like array optimizations
where typically this four times is like
the size of the array elements and so
you want to get rid of application from
the side of the loop and so if you were
to go through effort of actually moving
this optimization for your language well
then there's still one more subtle you
have to deal with it's called phase
ordering and the issue is that you could
start off this program here and sure you
can play Alou conductive over Lucan loop
induction variable sweet deduction in
order to get this ideal version here but
you're also gonna be writing another
number of other optimizations such as
the fact that four times will be
replaced with left foot by two and if
you apply those optimizations first well
then they'll block out the supernatural
strength authorization in particular
because they get ready this
multiplication from inside the loop so
this issue of optimizations sort of
conflicting with each other causes
what's known as the phase ordering
problem and so recognizing this let's
consider how much work is involved and
implementing an optimization for your
language well with the traditional
techniques what you do is first identify
all the multiplications inside the loop
and then filter out all the non
inductive cases I
variables being modified in subway
besides incrementing then you decide
which of the remaining cases you want to
actually optimize because you too many
of them you'll overflow you're right or
overload your registers and then add to
add new liver loop variable for each of
the remaining cases and insert the
program it's preacher in many cases and
replace their appropriate
multiplications with appropriate loop
variables and so once you've done all
this you have to integrate the
optimization into your pipeline and the
rest in that phase wearing issue and
then if make sure debug everything
because there's a lot of little things
that can go wrong here and again we
don't want to just compiler box because
they're very painful and so you need to
do all this process basically for each
kind of optimization you do and so this
reproach requires a lot of repeated
Emanuel imitation and so in lie that I
figure out a way to apply a quality
saturation to this problem in particular
if with my approach all I have to do to
get lupine there's real strength
reduction is to just three basic axioms
of your language disra's it easy row and
identity and from that loan we learn
loopnet relative introduction
automatically more generally it helps us
if you also give us sort of estimates of
what your operators cost in order so
that we know how to prioritize them and
then once you've done that we can
automatically for a large variety
optimizations all of which are
guaranteed to be correct because will
actually be able to output a proof that
the transform programme is equivalent to
the original program and so you don't
have to worry about this debugging
problem here and so to see how our
approach works we start off with some
control flow graph right the standard
reference agent for impaired to programs
and again this isn't very good for AXA
Matic reasoning so we're going to
translate it to our own reputation the
program accession graph I talked about
earlier and once we have this nice
mathematical representation well then we
can move on to equality saturation and
infer a bunch of equivalent ways to
represent this same program by applying
those Abrego danny's and once we have
all these equivalent representations
well then we can incorporate what we
call a global profit heuristic which
analyzes all these equipment
optimizations and picks out what's the
optimal optimal one according to the
cost model that you gave us and once we
had this optimal choice well then we
convert it back to a control flow graph
in order to get a standardization that
we can move on to other stages the
compiler like lowering down the assembly
level now to see how this works in some
more detail let's consider this lupin
texture of real strength deduction
example I showed you earlier
and there's a new challenge here which
is that this comes in the form of
control flow graph but again I said
control flow graphs aren't very good for
this kind of acts ematic reasoning and
so you want to figure out how to
represent this as an expression and the
issue is it that this program has a loop
in it this loop really represents an
infinite number of values so how do we
represent represent an infinite number
values as a finite expression so to
solve this problem the idea I came up
with was to use or expressions with
loops in themselves essentially
recursive expressions and so this four
times I loop value here who represent is
this expression over here in particular
we have this data node that says that
loop variable I start stop at zero and
second wind by one in each iteration of
the loop and so once we have this nice
loop representation or lice mathematical
position of the loop we can move on to
these familiar quality saturation
process w talked about earlier so we can
apply an axiom say fact that left
shifting by 2 is the same thing a
multiplying by 4 and so we can add this
equivalent representation here and note
that were once again we're adding
information we're not throwing away the
original control flow graph and so this
is very different from the prior
approaches in particular it's additive
and it's how we deal with that phase
warning problem because we're still free
to explore in another direction
particular we can distribute or you can
buy another axiom that says that any
operator distributes through their theta
nodes and this results in this
multiplication of additions here then it
allows us to apply distribute Iffat e
and that results this four times theta
note here and there's an already at four
times four times i know'd over here and
so we can reuse that same node in order
to keep our reputation as a compact i'm
doing that we can apply 0 no Danny in
order to apply those expressions then
we've just built here is what we call an
e peg or an equivalence program
refreshing graph and what this does it
has all these sort of equivalent classes
or values here and it says that here's a
bunch of equivalent ways to represent
these various sub components of this
program and so once we have all these
equivalent options well then we can
incorporate a global property heuristic
tell us which of these options is in
fact the best one and so it analyzes a
peg in order in order to figure out 11
reputation for each equivalence class
that optimizes the cost of all that you
gave us now once we have this we can
simplify down to a program efficient
graph and this gives us sort of our
optimized results
and lastly I'd find those translate this
back into a control flow graph and so
this corresponds to this graph here a
particular it says that there's a loop
variable J that starts up at zero and
secure met by for any trades in the loop
yeah 1040 coming oh yeah yeah so sorry I
just for focus on this thing here that
you start adding less than the party
which start going to much bigger picture
this mute they like you even the trigger
point sedated yeah you have an axiom
that status at it in would be smaller
than forty and four times being more
than 40 yeah okay so Antonia this is
where you also have to know that the
upper bound is small enough to make sure
there's no overflow and stuff like that
so all the axioms I showed you so far
actually hold for modular arithmetic so
they're not an issue but things like
inequality axioms the standard ones
don't hold for Monta jeweler too excited
to make sure the bounds are appropriate
this representation just static single
assignment person
this is dip it's a more complete form of
it so it's actually one that so static
single assignment you can't throw away
the CFG it's now there's not enough
information things all values that are
different actually will get completed
yeah yeah and gated SS a you can say
this sort of as very similar to a
version of gay desta say we're all
looping the seas here are explicit
there's versions that don't have
explicit loop indices but you can't
throw away the CFG because they'll
actually merge values again that or not
that are not the same so this is sort of
one that's been fleshed out all the way
to make it's a completely independent
confront from the control flow graph and
so we figured out a way like way to
actually convert from this repetition
back into control flow gasps we also
gave it the notational semantics stuff
so and showed that gave a transformation
and we approved in that like moving
from to representation actually
preserves semantics so it's sort of they
say very thoroughly done as I say but
along the same lines when you do this
challenge of reasoning about loops the
way people reason about loops in the
control flow graph is that right or no
another 18 of the kind of opposition to
have this talk about earlier if it's in
this Moro and have you reason about
about those who are those so this is
what so if you had another loop down
here say and if so their effect list
loops this becomes much easier because
actually in our thing they'll just be
one loop print expression another look
expression they won't even be in any
sequence and south on top of each other
and so that's where some things get much
easier when there's a defect going
through it then that's forced them to be
sequentialized and that gets things slow
messier because a little reason about
them do the kind of opposition
um we've had miss mix luck on that one
so it depends on just how how
complicated the story is so this or
simply enough simple enough it works out
if it's too complicated than we at least
automatically we haven't real do it I'm
not automatically you can do it by hand
and that's nice that's another thing
more questions yeps anybody in line so
is it isn't it just totally trivial or
is there some subtleties about that our
development so so actually so I've been
talking about Maxine's here but really
we are engine actually allows arbitrary
what we call a quality nalysis is to
come in and they'll do fancy or things
and so one of them is an in liner and so
it'll say okay here's things that are
like have been approved for in lining
because they're just you know you
something lying here sick whatever and
then we'll in line them and it's fairly
standard how in language add the
equivalents for the coolers between
function call and the function color
place with all the expressions or work
with all the expressions replace some
things we found so it's very important
to give intermediate variables we found
things that I try to do like lambdas
within the thing it's possible to do but
in practice because of this sort of
exponential kind of exploration this
additive approach it just becomes very
very messy when you actually try to use
like lambdas and try to do stuff like
substitute this inside the lambda that
doesn't work very well so lack of
intermediate variables there's actually
a big thing towards getting this for
getting this approach to work
okay yeah you've just described it
generally speaking but I'm curious Steve
would you consider optimizations like
sipping iteration for j 0 or what
backwards
replacing the whole thing with
so we had the first one is easy the
first one the first one we can do that
you know that won't happen automatically
all plenty um that's a loop healing
thing the going backwards one is not
easy so that one I wouldn't that one
requires you to know so basically we
represent these as sort of like this
they to note is a seat this is where the
iteration thing comes in its right to
sighs okay iteration zero at zero
Direction one it's what four plus what
is it eration zero so our sort of
semantics our reputation incorporate
scintillation counting to it so if the
little shifts are fine but things like
reversal but you can't even reverse an
infinite sequence right so you have to
really have to know reversal respect to
some maximum point and similarly sort of
taking every other one you can do we
have sort of an evens and odds thing
that's how you get the loop healing or
the loop and rolling thing but we
haven't tried putting that in practice
so so some bloop things no problem other
loop things difficult I could spend it
was the first although that is the total
FML question so you have a Denali the
super out yeah okay so this is how close
is it so Denali so they they're
basically very similar fattiest this
they call it kind of equality agitation
thing Denali is within a block and
within that block of having just a
single block very much changes the
picture of things so you can use very
different techniques for doing this
process and you can basically they work
on like things like six instructions at
a time so the scale of things is
completely different so we're doing like
a whole Java methods at a time or hole
hole so you methods at a time and so
they don't have to worry about loops and
stuff like that so I really view them as
this is more meant to be the generic
purpose compiler or you know smart
generic purpose compiler where sonali is
a you know here is a six you know small
chunk of 66 angeles assemblies records i
need to be optimized to all hell and do
go through it as detailed possible the
state stations for
possible so they have finite state press
right so they can actually explore the
entire database so that's why they can
use a Sat solver in or actually say is
this true or not true whereas we can't
because our estate space is infinite so
that's that's why the depth or the
breadth first search is important rather
than in depth for search because we
can't explore the whole space oh sorry
I'm into open so it first absolutely
this is a very structure so you're
saying x by 2 g's two pegs hey so does
that work for
it's the term useful yes reasonable
thank you ok reduce will see a fuse we
can do and there's a way to translate
irreducible strands t up to use to
reducible civilian fuse with some
duplication but any reducible one we can
handle any facts something I took a lot
of struggling a separate suffering month
was figuring out how to make loops that
came from non-structured loops and still
revert them back into non fresher loop
so if they had breaks and continues
actually restore them to a loop that
still has brakes and continues rather
than it has a lot of duplication and
unwanted branches so that was messy but
it's been figured out just just full of
theirs I mean these things just just
happened right like I mean we didn't
finish ins we're like let's just see
what happens and we threw him in and
they just they worked just from the
axioms and some of them are they didn't
work is because either is either because
there's some sometimes it's because
there's like something big like reverse
the loop is just not doesn't really work
for this kind of representation and
other times it's because we're missing
axioms so it's a tad axiom where you get
it so like interloop doing the boundary
action example a max you require
something along with what you mentioned
is like basically at the end of this
loop we know that Jay is 40 right and so
this actually has that knowing that fact
require sort of a higher order axiom and
so we added a higher axiom into the
system and then we can do some fancier
stuff that way so so something I've
learned in general or just come a few
times that the reputation you choose and
for optimization is a big factors to
what kind of optimizations are easy to
do and are not easy to do and so you
know I chose this one here but there's
many other ones that could be valid for
different kinds of programs and
different kinds of things and fire that
proof demolition process actually I made
sure that the algorithm i came up with
actually could be generalized to other
kinds of reputation as well so it's not
just stuck with pegs it can work with
other kind of other kinds of programs
so this is obviously fairly akin to
verification techniques amuses usually
program verification when you have loops
you summarize them via loop invariant
and there's variety of techniques
foreign currency I was wondering whether
you think you have all the power need or
if you were to apply loop invariant
inference techniques and incorporate
that into your technique somehow with
what you get so what we found is we
actually already have and our pre
actually like the say the anti-aliasing
there's a few things that are best done
before you start doing optimizations or
it'll look basically learn loop and
variance and because the reason is
because it's hard sometimes it gets
possible but it's hard to do loop and
variance dynamically because you have
all these equivalent representations and
you're basically trying to induction
over his equivalent representations and
so it doesn't really work for you while
we found out in practice so we'll do
live in variance beforehand and then we
then we use those ones and once we have
a few beforehand we can figure out how
to meant them dynamically as well but we
still need some like with we found that
we need some starting points to go from
in order to get those
okay oh good okay so sort of the zodiac
so now to look we were probably has seen
this but looking back where he came from
he started off the sport times I they
got skates to GA and hips had this
increment by 1 it's changed it to
increment by four I made this banner
tender and change it to bound at 40 and
this looks familiar because it's in fact
loop induction Braille strength
reduction and what's cool is that when I
didn't add I didn't program the thoughts
magician explicitly write it just sort
of happened and we call this emergent
optimization and we found that there are
many optimizations that emerges from
these basic axioms automatically and so
this is how we're able to get sort of a
language optimizer and automatically in
for optimizations now I've been talking
about language sort of optimizations
here but we actually found out that we
could also apply this to libraries and
the reason why this sort of came to me
as some being important is that I'm REE
bad racking my undergraduate days I had
to write a ray tracer and had this
towards between mutable vectors and
emulators now mutable vectors meant i
had to write big sequence of commands
like this in order to complement this
very basic expression with immutable
vectors furthermore middle vectors if I
chose them meant or meant that my code
to be error-prone because I had to track
things like ownership make sure that the
wrong person doesn't modify the wrong
vector at the wrong time and where the
email vector is right they don't have
those kinds of problems and so in light
of that you might expect me to choose
immuno vectors however I was worried
that amido vectors would be inefficient
a particular even this based expression
here allocates a number of intermediate
objects and that is objects artist no
made and then thrown away almost
immediately and so because of this I
decided to go with me two vectors in
order to get better performance and I
remember decision years later when I
started working optimization and so I
did to see whether I could apply my
optimization techniques to these life to
this library design I went back to that
ray tracer and re-implemented to use
amino vectors the way I would have liked
to have implemented and I did find out
that actually ran seven percent slower
so I was justified in my performance
concerns and so what I wanted to do is
apply my techniques in order to replace
these sort of very manually intensive
libraries modules and gets the sort of
nicer ones that and still have the same
performance here in
tease as these manual manual ones so the
idea I had was that using my techniques
where I can able is library uses
optimizations in particular that idea
that we expressed the various guarantees
that we have about using your library as
axioms so for example if I had a vector
library that has these two you know as
these two vectors together and gets the
first component well that first
component to be the same thing as
getting the two factors force components
in the summing them together and so once
we incorporate all these axioms into
these quality saturation process then
we'll automatically infer optimizations
for using my library specialized for
using my library and so I applied this
to that translation through that ray
tracer and it's actually able to get
email vectors to run faster than the
mutable vectors because they are more X
and friendly particularly optimizations
we learned were able to reduce the
number of allocations by forty percent
and get rid of all those intermediate
objects and so by using this technology
we can make it so that you can design
your libraries the way you'd like to and
still get the performance concerns or
performance issues by taking advantage
of these axioms and this is tht
for compiling high level especially our
heads of things like fast fourier
transform down to local ships until
libraries so it's a ticket I mean
rewriting the strictly more powerful
right so this is so disappointing this
is within the same language so we're
optimizing so they would take Java code
and we rewrote it to Java code and so we
dint was so this is a performance on top
of that even have to worry about
lowering down and is actually also all
these metrics I showed you or showing
you are on top of the JVM optimizations
as well so this is optimization the
debian couldn't do and the JVM optimizer
is actually is fairly advanced we found
out thinking of it and I'm just thinking
in terms of comparison to a rewriting
system means the ones your notice of
what is the pose so you know it's what
is the expressiveness things
well so we can I mean rewrite system
this is essentially a kind you can think
of as a rewriting system but with with
this that google Peter billable property
hurt profitability heuristic which
allows you to explore many rewriting
simultaneously whereas with typical
rewriting system you have to worry about
only like basically as the phase Orion
problem if you rewrite things in the
wrong order you get pal good problems
and so we've sort of addressed that
issue by using the Equality saturation
keeping this out of approach and also we
also we figure how to extend it to loops
and stuff
oh good all right so there's also i mean
i'm sorry stuff that the whole the whole
idea here is reno does there's over
second golly stuff right all about
abstract data types and sort of doing
higher-level transforms it seems like
related to
I'm not matter with the works are can it
could be like essentially the ideas of
you did you have sort of yeah if you
have some knowledge of the higher-level
synaptics new data structures you can do
community things like these
optimizations yeah I know they will be
basically with the cool thing is that we
figured out that or you the experiments
show that that actually makes a
difference we can just do that stuff
automatically right we don't have all we
have to denote the axioms as you're
saying and well at least my tent with
all this is to make a set people can
actually program differently not just
like take existing programs and run them
and give them faster but actually make
it so people can write their libraries
in a way that's nicer so this at least
substantiates that would work trusting
essentially or whatever the ability I
mean we're trusting the accent our
friends that's right so but you're
inferring them from the time signatures
yeah so like so like I've written the
library so I just went through it said
okay here's the Mac seems I would be
youth that I imagine would be useful and
then then it's okay and for
optimizations so the library writer
would provide back seems we can't infer
them automatically in fact that actually
ideally the ideals wasn't so much about
that cause it was really involved
security like now
that the function was pure
hey Anna guess I read that be an axiom
like the reason why i like the axiom
thing is that it means you don't have to
have the library code okay so particular
with ello systems where you're dealing
with interfaces you're not addressing
the problem we'll check the code no no
yeah so that is a whole nother story
yeah library writer also have to
recommence functions
we didn't have him do that the we have a
very well our custom was actually very
naive but we found out that naive house
models actually work pretty well it
would be better actually the one thing
where would be better though in practice
is knowing that some methods are more
sense than other ones so right now it's
just treat all calls uniformly and
really be better to say like no no these
call that function is nothing like
calling this function please you know do
more of these ones and less of these
ones but we didn't actually do that in
our system we we they're the that I
think we use can Canada accommodate that
but for the amount of automated
automation were you trying to go forward
that wasn't something we go for all
right so so we even talked about
performance but Evan really talked about
the performance of my own tool right so
sorry for ebook aliasing the last
emotion what do you need a
access to the view object to their abs
and sometimes you might want to rewrite
them on
Oh so to do some technically related
objects or we're not rewriting objects
or anything where does we read in code
so this so here I mean here that you'd
still have you would still say you and B
would still stay around it just be this
thing so there's no we're not to worry
about aliasing or anything like that for
the for the optimization that we're
dealing with here the only things that
were the things we're aliasing would it
was useful is knowing once that staple
operations can commute and basically
almost all the times in aliasing use
people was so knowing that state law
operations commute so so yeah sorry so
aliasing isn't really too big a deal for
these kind of library axioms okay so so
so going on to performance this is where
that tool that that tulle train i showed
you in the various stages the compiler
and so it evaluate how effective this
tool chain was how efficient it was we
ran is on the specs to a vm 2006
benchmark suite and we found out was
that the equality saturation actually
runs by quickly the slope part is
actually global poverty ristic and to
figure out why we did some some
investigations we found out that the
quality streams actually color
saturation was actually doing such a
good job that even though we stop it off
early by the time we stop it up would
find a trillion trillion trillion ways
represent a single method on average so
you can imagine takes a while to figure
out the best option out of a trillion
trillion trillion options and so that's
why they go far both gulp ravu global
profitability heuristic takes a while
now and I the fact even though there's
you no good reason this takes a while we
still want to figure out a way how to
get over this hurdle right in order to
get the sick ecology adopted and so the
idea had the idea of combining this
technology with the previous technology
showed you earlier in order to speed up
optimization in general this isn't the
so this technique that we showed you
works for really any optima that's
optimizer you're not just our own
there's a lot of these things out there
such as Denali and these things are
smart but as a consequence are slow and
some of them are really slow and you
have a lot of code you want to compile
but you don't want to wait around
forever so the other things out there
are things like rewriters which are
quick and efficient but also rather
naive and so what we would like to have
is the sort of ability to combine the
these advanced atomisers with the speed
though he's equipped fish and rewriters
and so the idea had it for doing this
was that I could take off just a piece
of your code base and ship it off to the
dance optimizer in order to get a very
well optimized version of that code base
we then ship that off to your
optimization generalize ER in order to
learn optimizations from the advanced
optimizer they're going to tack on what
we call a decomposer in order to break
this up proof of equivalence up into
lots of a bunch of little lemmas so from
each of those lemons we learned a bunch
of mini optimizations specialized to
your code base which we could then
incorporate into our fish and rewriter
and pass the wrestler code base through
the rewriter using these lessons that we
learned from this advanced optimizer and
to see whether this is effective or not
we ran this on that right place where I
talked about earlier we found out that
the rewriter was actually able to get
the same produce the same high quality
code as the van Tata miser just using
this lest we learn from the sympathetic
from just part of the code base and
furthermore is able to do so 18 times
faster in the advanced optimizers so
significantly address that performance
problem i talked about earlier and so
this is recap sort of made us that
weekend and throw out some positions
automatically and efficiently and
overall you know made so we can actually
use optimizers take advantage of them
even though they're occasionally broken
maids that we can extend out the message
nor address are sort of more domain
specific needs and made so the atomizers
are available for new languages and also
for library writers and design their
libraries and so by continued design of
research what I'm hoping to do is make
it to discussions like this are no
longer necessary we can focus on you
know the more important things like
correctness and something else say since
you know this job back and all something
else oh just talk about is that fact
that this is really only one line of my
research another line of research I've
done actually here mostly is type
systems in particular thanks to John and
Chris I learned all about existential
types for dealing with a type stimulant
which is for C sharp and well wait you
know chris is working on is is this
operating system verb that's guaranteed
be memory safe and the big issue is that
they want to be able to take C sharp
code either for things like the
scheduler or for user programs and run
them in their operating system but you
know C sharp code is memory safe but we
compile those are broken and so you want
infer types add the similan able to make
sure that even the assembly code is
still memory safe and so
what I did here and this got me
introduced with exit realty made this
big category theoretically Merck and
this would not be very useful for
dealing with Java particular actually my
students told found this out for me that
Java has all sorts of problems with
wildcards they had this piece of code
that the writing for my class project
and they were very frustrated because
his code was in compiling and they have
no idea why I'd look at the code and I
found out that the code is the code is
actually correct the the type checker
was broken and so if desig ate some
further and the found out that wild
cards a particular job it just does not
do a good job with so I apply this
existing framework because wild cards
actually kind of accidental type in
order to prove that algorithms that job
is using for type checking and also
figure out how to refine a subsystem a
bit and showed that if practical do
these refinements I were to make sure
that the jobsites wasn't actually is
decidable well type argument prints we
couldn't can't solve but at least the
rest of it subtyping basic things we
actually some typing wasn't known to be
sizable before so now it's a sizable huh
the water was this thing or bow ties I
shuttle doubles after we're really well
courts come that was the Java five so
generics wildcards all came together
scared some wild Kurtz came at the same
time
no no interest and go for well so that
before job I'm saying the same time get
it
such a bitch at the generic Java
prototype oh yeah I have a lot of time
to write some lovely that's what I
thought yeah okay okay good there's good
reason foreground cards and what they
address there's also some messiness is
so they definitely start like with the
sale on stuff so I guess I go on the
sale on today so so sale on is this
there's a see my red hat working on
making a new language but together there
are people who worked on the hibernate
project at people familiar with that
they've had tons of years experience
dealing with Java code and that you can
very frustrated with some of Java's
problems like wild cards and so what
they want to do is sort of clean up java
and general low enterprise primer
enterprise programming so we're try and
learn all lessons on these languages so
they're aiming for things like
decidability and so that's how my work
got involved we figured out that wild
cards have some good aspects and so have
some weak aspects or incorporating both
of them or what you know throwing away
throw away the weak aspects in violent
coughing the strong aspects stuff like
that and so I just by working with them
on making sure the types of Sims nice
and clean has all these properties that
they they told me like so things like
principal types they want decidable
types and decidable inference where they
allow it they want stuff like that so
I've been helping them design their
language and make everything look nice
and pretty and meet their guidelines so
they give envy so so this is sort of one
line another line of research I'm doing
and then and sort of a longer-term track
very very far down the line what I'm
some problem I've seen is that if you
have some tool for C sharp well you
don't really know where that tool
transfers over to Java and seemed
relieved you have some cool proof for
Java you don't really know where that
proof fans over to c sharp and this is
frustrating because you know these
languages well they have some key
differences they also have a lot of
similarities and so you'd expect that a
lot of tools of proofs can be
transferred across these languages we
don't really have a way of formalizing
that and so what I'm looking want to do
is make a sort of metal language for
programming languages in which we can
formalize requirements of proof tools
and proofs and foremost properties of
languages in order to make it so one
tool can transfer across many language
simultaneously and similarly to make us
that when you make a new language you
know what kind of properties you want to
aim for in order that app access to
these dis ting tools and proofs that
I've been made for other language is
already out there and you know I've done
some work in this with effects here
and also looking into foundations of
computation and mathematics and looking
into resource-constrained computation so
just sort of scoping the landscape right
now but I'd be happy to talk without
talk with any of you guys up at today or
tomorrow but at this point I want to
think out thank all the people have had
a chance to work with that UCC and and
here and well I guess think all you for
the fellowship that let me do all this
research and then open up to any
questions you may have
translation validation but why didn't
you to immediate termination
what do you mean by sorry
translate your
seemed to me that that you
and everything so we
instead that's a translation into movie
and then you get all of the nations into
formula
perhaps up from
oh yeah yeah so we tried that didn't go
very well the reason so this is also
years ago so this is when these
technologies were younger so may you
actually work now but um the reason why
we think it didn't work well is be
particular because we had recursive
expressions and the algorithms at things
like the e graph that you guys solvers
and south ends III and then simplify and
something that don't seem to really like
recursive expressions they just they
just run forever go down a rabbit hole
and then you never hear back from them
you say something wanna fire somewhere
it would do so that like we had that
operator operation that distributes to
the theta node that can go on forever
and ever and ever and so I we think that
if we had didn't have programs i didn't
have that of that operate that that
specific axiom or didn't have any loops
in it then it'd be fine so so didn't
like those kind of things so those are
politically kind of things that we
wanted to actually have work you wrote I
mean one of mining station yeah yeah use
the really pattern matching out of my
new from undergrad so yeah yeah so
that's how I was how I got that to work
if I ask that one more question okay see
you know there are like amazing results
with the optimizer and stuff nu X
adrenaline like like real job fight
girls rights I've been a pretty
realistic tool but but do you feel like
now my friend zone was also a Sableye do
you feel like I could really incorporate
it in a in a Java compiler for example
but
what is it practical or not I don't
think it's Trish precision but take it I
wouldn't say is ready for gym Kylie
actually I think I was just saying like
can I bill would I be able to build a
compiler with no optimizer right Kirsty
I'll just give it exeunt and I integrate
your framework there's certain
optimization that I wouldn't put in that
class like because I mean this rather
high level so things like like register
allocation all those low level things
don't really like once you get this
level abstraction that you can't see the
differences between those so i wouldn't
try doing those kinds of things he's
kind of in this system also as you saw
like you know there are performance
problems actually probably better now
because it's years later and the solvers
that we've used to probably gotten
better but um actually I think what
would be better is rather sort of a
compromise of systems they can trigger
the compiler that has like many of the
core things down there add the
extensibility aspect to them and then
make it so I particularly like the
strategy i've showed you with the
learning optimizations from the super
optimizer i think will work best is
running super optimized around your code
base say like once a month learning all
these axioms from at once a month
applying those for the rest of the month
and that way you got the various cuit
like i've seen all these things their
level as much smaller set of axioms that
run much more efficiently and then you
know another month later after your code
change enough learning again and go
again I think that's a better approach
towards going about this sort of
accommodates those things and weaknesses
of the system
so I see that you haven't even llvm up
there and as wondering if you can
compare a little bit with the work on
vellum which formalizes the llvm
intermediate representation here just
managing student and so on what
kind of semantics means well yeah um we
didn't there's no from we didn't use a
formal semantics for olivium so actually
so Mike steps the one in took care of
all the llvm specific stuff so I did the
general the general purpose kind of
stuff so he died out there with the very
he dealt with like by codes and and and
big coat that goes in big coats and then
he would talk to me when he'd run into
some problem like okay can't figure
burbs in this how to purpose in this and
then we talked about better ways to
represent those kind of contracts and
stuff but overall he was the he was the
more detail-oriented part of that part
of the project so unfortunately I go
into details of the lod I'm thanks to
the transition metal vm into your
representation that you believe this
faithful fellow p.m. semantics can you
work for your ok then I recovered back
to IBM for and the Xu's right yeah bring
our translation from lv m to our
reputation was very very spacek it was
like you know here lv m expression maps
to this you know it was very simple ones
so it's more the axioms are really these
things i incorporate the semantics of
olivium and they're mostly things like
integer axioms is up to that we had
there was a bit of a mess with the fact
that there's many different size of
integers and all the other operators and
operations lvm are typed and so getting
all those two interact well with each
other was a bit messy but we figured how
to deal with them
yeah there's a classic loop
optimizations that our optimizations
cousin cash locality
tile
thought about it any thoughts on
expressing that sort of thing that's a
loss of function so that so that cash
locality and oh actually peril is
absorbed come up a lot when I talk about
the stuff and basically they have two
common issues and project the back see
that yeah sorry the two things i have
the same same issue behind them which is
generally interprocedural stuff
basically for cash like how do you
typically have to know how memory is
laid out and that's typically not
written the funk that's not available in
the function you're working on so it's
hard to make a cost model that does
hasn't does doesn't even know what the
how the memory is laid out and stuff
based assessment are like what we've
done some investigations of this this is
the problem we always ran into is that
you didn't know like how the memory
would be layout at least not
automatically so again is another tool
came along is that here's some
information then we might try
incorporating that but with that one
that's one where we didn't have any luck
because we didn't have led such a tool
okay let's aim speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>