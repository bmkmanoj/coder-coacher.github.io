<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Lasserre-Based (1+epsilon)-Approximation for Makespan Scheduling with Precedence Constraints | Coder Coacher - Coaching Coders</title><meta content="A Lasserre-Based (1+epsilon)-Approximation for Makespan Scheduling with Precedence Constraints - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Lasserre-Based (1+epsilon)-Approximation for Makespan Scheduling with Precedence Constraints</b></h2><h5 class="post__date">2016-06-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VAUkoefYGCs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by Microsoft
Corporation may be used for internal
review analysis or research only any
editing reproduction publication
reproduction internet or public display
is forbidden and may violate copyright
law
okay yeah it's a pleasure to have Thomas
tell us about the scheduling problem
which is I guess one of the four or
three or four remaining open following
some carrier Johnson and so the problem
is really on Thomas want to tell us
something new yeah thanks I think
technically speaking we're not going to
it like completely close it anyway yeah
thanks for getting up so early
no this isn't the time for everybody in
the room okay yeah this is try to
address the audience more person more
personally thank you James for asking
the first question before I start okay
so this is joint work with with Elaine
Libby who is also at u-dub okay so this
is this is the problem when I talk about
you you have some unit size jobs so you
have some jobs that have like processing
gain length one and the the jobs have
precedence constraints so you have two
priests process let's say this job
before you can start any any of the
those jobs here and I give you i give
you m machines that are all identical
and all the goal is you want to you want
to schedule the jobs in a non
pre-emptive way so that the makespan is
minimized and the makespan is the time
that the last row finishes so okay um
okay so just a warm up let me just
explain you a simple to approximation
algorithm that's 50 years old and it
works as follows you just can't really
to anything everything greedily so let's
say you just select any job that you
that you can select and you just well
put it on one of the machines now
there's another word job that you can
just process and now you see that well
in principle you have a you have a third
machine here but you don't have a job
where all the other processors are
already completed so you leave that's
not empty and you move on with
next time and and with the next storm so
just do everything greedily the way you
would imagine it makes sense
ah so this is the algorithm would would
work and okay so here you see that well
there's this chain this chain we didn't
pay a lot of attention to that so we we
have quite some some some idle times
here and now this is the time that the
last chopper finishes you can see that
actually this this is the this is the
make span here so this is at that time
that the last job is finishing you can
also see that this is not the optimal
solution there's this chain of five jobs
we probably should have started this
like a lot earlier and just work work on
the chain all the time ah yeah but this
so this is an approximation algorithm
and it definitely does not always give
you the optimal solution but the claim
is this gives you a to approximation so
how can you how can you see this so
first of all if you if you stare at the
times where there is some idle idle time
where you didn't use all of the machines
then why did you do that why did you not
process this job earlier well actually
the reason is that there has to be a job
that it depends on coming earlier and
that's actually the only reason why you
could schedule this earlier so to be
more precise there is a chain that goes
over over all of the non busy times and
if you wanted to be formal you could
construct this chain from from the last
job you could wonder why did I not
schedule this guy earlier well because
of this job why did I not give you this
guy earlier because this shop why did I
not do this guy earlier because this job
ends on time
now you can wonder if you have a chain
what's the length of the chain well a
chain the length of any chain is the
lower bound or the optimum because of
the optimum kind of needs to process the
chain somewhere um okay so that's good
so this gives us a bound on the the the
non busy periods actually bounding the
busy periods is even is
because you can imagine that even even
if the optimum will would be allowed to
ignore all the precedence constraints
even then just to process all the jobs
you need at least that amount of time
okay good so and now you just add things
up and you get that the schedule we're
getting has length at most two times the
other actually the algorithm isn't much
better than a to approximation you can
be a little bit more careful and
depending on how large the your number
of machines is you can be a little bit
better than two but if that number group
goes to infinity then the ratio goes to
two like the approximation ratio of this
Headroom okay so we would like to beat
this algorithm and it's kind of what's
what's like the standard technique and
approximation algorithms it's you write
on some media program so we could do
that and this might be the natural in
your program like experience would
probably call that a time index version
so let's say you have a variable X DT
and this is this variable is going to
tell you whether you wanna run the job
at time T and I hope you see that you
only need to consider integer times
they're just integer time slots so what
would be the constraints like every job
should be scheduled somewhere at any
time you can schedule at most M jobs M
is the number of machines you have and
then you should have some constraint
that that implements the the presentence
constraints so you should have if some
job I should be finished before J then
it's kind of the fraction that's that's
finished until some time T can't be
bigger than the fraction that's that
scheduled for that other job until time
T plus 1 okay and good so this is this a
this is a final in your program but can
you can you beat the gap of - no not
really
it's not hard to come up with some
anti-gravity gap construction just
imagine in this case I give you two
machines but you could have some some
kind of blocks of three jobs let's say
this is like a long sequence of of jobs
and it's kind of the other other jobs in
the ice block depend on the I - first
block and now the the fractional
solution that you could construct would
be that you can you can kind of process
all the jobs in the same block in
parallel and so this would mean like
these three jobs in an integral solution
you would need two time units but the
fraction solution could essentially
schedule this in 1.5 time units it's
it's nice to observe that in the
fracture solution the the support of
jobs can overlap even if the jobs have
prisms constraints so this is something
you can't rule out that makes the whole
thing difficult so the integrity gap
it's also essentially two so if the
number of machine grows then then the
the gap will tend to - so this isn't
really going to buy you that much you
could probably come up with some
rounding algorithm I I doubt that you
could beat the simple greedy algorithm
for this problem it's extremely hard to
not get a factor of two in some sense if
you so yes you probably in exercise I
haven't done it myself and fractional
times I mean that's what it is what this
takes advantage of right like one two
and three started at second two and then
they go to second two point five and
then you start four five six
I'm just saying then you use reserve and
at least for this you can respect the
president's constraints but just be able
to is this the same I don't think so no
I don't think so I don't think it's okay
good okay so we we did see that little -
approximation algorithm by Graham and in
fact it's it's unlikely that you can
beat the two in general so there's a
paper by Biola saying that well I'm not
some variant of the unique games
conjecture you're not going to a you
won't be able to beat the faculty then
you're not that's the question what I'm
going to talk about in the next two and
a half hours but okay so in scheduling
it does actually make a lot of sense to
to consider cases like what happens if
the number of machines is a constant so
think of MMOs being a big constant maybe
then we can be the factor of two and
there hasn't been in much success
actually so this is this is what we want
to do okay and we've mentioned already
that this is essentially this is one of
the four open problems and Garey and
Johnson so now it's maybe three and a
half open problems now that graph
isomorphism is kind of solved to grow
with the input length you can't be too
welcome how are you gonna fit a result
in between those two bullets that's what
I'll give you everything but the running
time will depend exponentially on on M
on the number of machines and there was
no such algorithm known before that's
better than two okay okay good turn it
in fact if you have let's say three
machines it's unclear whether this is an
NPR problem
there's a nice survey of humanity
working about open problems scheduling
it's now sixteen years old a couple of
problems have been solved one of the
problems that has been open is whether
it's a 4:3 machines there is a pitas and
so billion machines it's it's it's also
open whether this is NPR yes and the
result I want to talk about here is that
actually you you can take the data
linear program UK you can throw it into
your favorite LP or STP hierarchy and
after some number of rounds the gap goes
down to 1 plus epsilon that number of
rods is a weird number that's a little
more than poly log and the epsilon at
the M actually goes here into the hole
okay so this gives you also an algorithm
that that finds a that finds the
schedule in some kind of weird a little
more than quasi-polynomial running time
no there is there's an M it's really an
M here and I think times 1 over epsilon
square I think is the the dependence
here this is the exponent so yeah good
okay good so this is this is what I want
to talk about okay so yeah okay so let's
what's what's the dal-su hierarchy the
idea is it's a very general idea you
imagine you have some some linear
program let's say with binary variables
let's say the the set of feasible
fraction solutions is K and now you want
to add in some more constraints so that
your your LP becomes better so what you
would ideally want to have is that into
that that convex hull of the integer
points in K yeah you're not going to get
there but you can write down some some
bigger system and it's going to be kind
of stronger than your original in a
program that system will depend on some
parameter that's called the number of
rounds of the hierarchy and as you
increase the parameter the the hierarchy
gets stronger and stronger and then
eventually after
number of variables many iterations it's
going to converge to the the integral
Hana okay so many of you might have seen
this already so the idea is that if this
isn't just a general general in your
program then U and T is your number of
rounds then you would just take
variables for all subsets of at most T
many variables and you've write down
some kind of meter variable which is
supposed to tell you the like the the
fret with the extent to which all of
those variables have to be one so you
have some extra variables this is why
it's called so called lift and project
because like things are variables they
give you lift and in particularly you
will have your original variables in
there and now you just need to write on
some constraint system that gives you
some kind of consistency and you can do
this explicitly it's not I don't want to
go into details I at this point I would
like to treat the let's say hierarchy as
a black box
we know what properties it has but if
you wanted to write down explicitly the
constraints then you would take your
variables you would throw them into a
big matrix that's called the the moment
matrix and then you ask that this moment
matrix has to be positive semi-definite
you also will take all the constraints
that you have and you would put them
together into another big moment matrix
and also that has to be positive
semi-definite for when any constraint
works we surely Adams yes so what is
Remington deserve one of the authors has
a survey on the laissez hierarchy and
wanted to copy/paste the sexes that I
actually I find the I find the
definition more elegant than for sure
Charlie Adams it's it doesn't matter so
we're using yeah
the lezzy hierarchy has a lot more power
than we need okay if I hope that doesn't
make you uh doesn't cost you a headache
okay we're not using any kind of fancy
like polynomial whatever
are you okay good James doesn't look on
Vince just for the record for the people
who can't see imagine this is yes and
you can just imagine for yourself this
ryeom's
it's not going to be any different the
algorithm itself okay so if that is that
number of rounds T is a constant you can
solve those systems in polynomial time
and well for us it's not going to be
constant it's going to be that like
weird somewhat more than poly log terms
so this is going to be yeah a little
more than quasi-polynomial okay good
more question so if if to minus epsilon
was np-hard
then your result says that the running
time of the reduction okay assuming that
likes a stab requires exponential time
your thing says the running time
reduction has to be at least something
yes do you have an area that is just
like some function of F it's not a damn
yeah it should be sometimes I'll never
figure it out but you could have you
could have some argument in that yes
okay so the LSE hierarchy enter Shirley
Adams hierarchy they have some nice
property which is as follows if you if
you look at a solution which is valid 40
rounds then and you look at one of the
variables that's strictly between 0 and
1 then it is true that this is actually
a convex combination of two solutions to
Lasserre solutions one where the
variable is zero one where the where the
variable is 1 and both of those are
valid for one round less okay in
particular a way to reinterpret this is
as follows that if you have a let's see
a vector you have a variable that's
strictly between 0 &amp;amp; 1 then you can say
oh I want to have that this variable is
actually 1 so you can what's called you
can induce on that variable being one
which geometrically means you're just
replacing
ulis a vector by one of those guys where
the variable is actually 1 and this is
going to cost you one round
okay and we will do this we will do with
this a frequently you could do this yeah
we're doing this with with one obviously
the same thing would also work if you
wanted to set the variable to zero yes
yes oh good there's one observation
that's worth making that looks trivial
but if you do this conditioning then the
support of your leg less the effect was
only going to shrink now if you okay
imagine imagine this would be an actual
probability distribution and if you
induce on on some event then another
event that had a zero probability before
it's still going to have a zero
probability no conditioning will make
the probability suddenly positive okay
good okay fine so this is okay let me
now go to the algorithm and okay so
here's a little bit what we don't do so
imagine imagine we kind of guessed the
time that the optimum needs let's say
this is capital T so imagine this is
this is the the time horizon we kind of
give this this parameter capital T to us
here and we find some let's say solution
with a large enough number of rounds
which is kind of valid for this for that
scheduling a P and now we want to round
that it that's scheduling let's say our
LP s dB
okay so first let's okay if this is the
the time horizon let's look at some kind
of a binary partition of that himer isin
so we just partition everything always
into interval to two intervals of the
same length and then like obviously okay
we need like locti many many many levels
and now let's let's imagine that these
are jobs and let's look at the support
of jobs so these are the times where the
Lasserre vector has some positive
probability of of of scheduling
okay so they don't have to be intervals
they don't need to be consecutive to
support okay now we want to do the
following imagine for the moment we kind
of assign the jobs to the to the minimum
interval containing containing all the
support okay
now the idea is we we would have an
algorithm that essentially search at the
bottom of that partition and schedule to
the job so we would like to like first
schedule the jobs that have a very small
support and then later add in the jobs
whether the support was very long okay
kind of hoping that they had like more
flexibility okay okay now this is the
actual algorithm um this is the overview
but the actual algorithm in the first
step we do the following we we look at
the first let's say a log log n square
many many of those are levels and we we
massage the the Lasserre vector by
conditioning on certain events we will
get that the maximum chain of jobs that
they're assigned to those levels is
small and I will explain this a little
bit in more detail and on on the next
slide let's take this as given for now
okay now after we do this let's say
these are the supports of the jobs and
these are the intervals where they are
signed to yes okay very basic what does
it means these are the time units these
are the time units where where this guy
is scheduled so let's say this is JT
where this is positive okay like imagine
this is this is the support of a job and
now kind of with respect to the current
Lasserre solution
no it's not the laziness of the person
drawing the picture which is me suggest
that it would be okay I think one's here
here yeah and then from now they just
look like intervals but I don't have to
be yeah okay good so okay yeah so it's
like for example like did this rule
rectangle mean that yeah there's some
probability that the the drop scheduled
here some probability that the drop is
scheduled there and and so on okay um
good so now we want to create some gap
here and we take log log and level it's
kind of somewhere in the middle and we
delete all the jobs okay with with we
throw them away now you might be saying
that I'm forced to schedule all the jobs
but if I have a small fraction of jobs
then I kind of threw away I can kind of
just insert them at the very end and I
you can just imagine that for every job
you can just insert a private new time
for him and you can just insert him and
that that is well you just pay pay this
extra in the back span but that's not a
big problem okay so as long as we're
throwing away only an epsilon over m
fraction wig of the jobs were good okay
good now this creates a gap in the sense
that now though the intervals at the top
are a lot longer than the intervals
being down here and if you do the math
and actually this factor is like a poly
log term of this thing and I need that
pulley log term that's why I have log
log n levels here okay good now I do the
following I have that I have mice let's
say a vector and please remember that I
did some conditioning so it's not or not
any more of the original as a vector but
I do the following I now recursively run
my scheduling algorithm and I schedule
all the jobs in those in those bottom
intervals so I go through every of those
bottom intervals and I just scared
the vectors schedule the jobs and I do
this independently for all those
intervals and you the important thing is
that I I have my las a vector and I give
I give a I give a copy to every of those
intervals here and now I since I do this
recursively there's going to be some
conditioning going on here some
conditions of independent conditioning
going on there so schedule the jobs
recursively what you first need to
understand recursion described I
described it here kind of for the the
top interval for the largest possible
interpret for the holy time horizon but
you can imagine you're on the same
errand that I just described here you're
on a just for that's more time horizon
and you run it only with the jobs that
had the complete support in here I look
at the jobs for supported here and then
it looks like the original okay it just
only only on a smaller scale okay and
okay good
so that looks fine but there's a problem
so there are my top jobs and now I also
have to schedule the top jobs I and the
problem is I kind of have to insert them
in the gaps that that remained and there
isn't really a reason why this should
work and so what what could be the
problem the problem is that we're doing
a lot of conditioning add way in the
Lycian solution that this is not
necessarily consistent so it might be
that an act in any actual integral
solution it's like if you schedule this
job at the end then
in the other in the other interval you
would have to schedule another job kind
of at the beginning so that or are also
at the end so that you can schedule
those top jobs in between so there's
some consistency that we are losing okay
but it will still be enough and why does
it work the reason is that we have
decreased the length of the maximum
chain among the top jobs the one of the
jobs that are assigned to the to the top
in ohms okay good okay so first things
first let's let's start about the first
at the first point what was what did I
want to do I had those jobs that were
assigned to the first put in lock lock
and many levels and I wanted to to
decrease the length of the maximum chain
by conditioning on certain events just
look at Graham's algorithm just okay the
bound here is essentially you get the
optimum plus the length of the maximum
chain which is that that itself is
bounded by the optimal but if you give
me a better bound on the length of the
maximum chain you can get something
better than two yeah so in particular
from from this you already see that you
only have a you only have a major
problem if you have long chains so we're
kind of getting rid rid in some sense of
of the long chain problem well they did
they will still exist but yeah okay so
what's were we we were precisely not
here okay this is the team ah okay so
that don't look at the statement of the
dilemma that we're going to prove look
at the picture so imagine this is this
is one of the top intervals and imagine
there are a ton of jobs assigned to it
what does it mean they are assigned to
it it means
their whole support is contained but the
whole support also kind of goals kind of
contains the separating line between the
decks subintervals and okay now imagine
it happens that some of these jobs has a
lot of other jobs dependent on him so in
this case there's the blue job and let's
say there are a lot of other jobs
dependent on him but still the support
can overlap this cannot be ruled out
okay now we do kind of the obvious thing
we spend one of our Lasserre rounds and
we condition on the event that this job
with the high out degree is scheduled in
the right hand side interval in the
right hand side subinterval now after
this conditioning all the other jobs
that depend on him will also be fully
scheduled in that right-hand side
interval so in some sense after one
conditioning the support of of a lot of
jobs actually moves down by one level
okay and so now well you just repeat
this argument and the support of jobs
always moves down you do this often
enough you count how many intervals we
actually have among the top intervals
and then then you get bound on the
number of rounds that you're losing so
there's some little calculation but it's
this is the basic argument you take one
of the variables and you condition it to
be mas ya but the condition fraction
solution might not be a solution I don't
have any other question
ok good ok so I hope ok
what I hope I could convince you is that
I could actually if you look at the
picture I can actually decrease the
maximum chain length of jobs assigned to
the top intervals to something well as
small as I like ok and that number well
I will lose some number of rounds
proportional to that but I'm good ok
good good good ok so that's fine
but ok ok yeah let me let me go back to
the to the next thing that we want to do
ok so after I recursively scheduled all
the other jobs assigned to bottom
intervals now the crucial argument is
how do you insert how do we insert the
the jobs assigned to table intervals and
now the only thing we know is that
actually the the length of the of any
chain among those jobs is very short is
that short as we like okay now let's
move on with that ok so where is
actually the problem imagine imagine
this is this is a job assigned to one of
the the table intervals but the job may
actually depend on on some jobs that are
scheduled in Batam intervals and it also
may may have some of those job dependent
on him and the problem is that now we
would recursively schedule this guy
somewhere in this interval but maybe
we're scaling him at the end and we
might schedule this guy somewhere here
but maybe we scared you him at the
beginning so we don't have any control
over that so in principle I I can
imagine that for my top job the only way
to be saved is that that I insert him in
some slot that comes after the end of of
any interval where a bottom dependent
bottom bottom job was scheduled
okay so I could imagine that I define
that I define a release time for this
job
and this is some some kind of scheduling
term I define a release some for this
job
which is the end of any interval where
we're dependent bottom job is scheduled
and I schedule I i define a deadline for
my top job which is the beginning of any
interval where like the earliest
interval where a dependent bottom job is
scheduled okay and now okay and now I
will try to schedule my my top job
somewhere between that release I'm a
deadline and if you if you look at the
support of the top job there's a little
bit of a problem because there's some
fraction of its support where I'm now
kind of forbidden to do to schedule the
job okay good okay now the point is that
the the bottom intervals are a lot
shorter than the top intervals and the
gap is some poly log term um okay and I
want to essentially schedule the top
jobs in in imagine in in two phases in
the in the first phase I want to just
prove you that I could schedule the the
top jobs if I would be allowed to ignore
any precedence constraint other than
that I have to schedule the job between
release some and deadline this is what
I'd show in the first phase in the
second phase I show that okay now
imagine I I know that I can schedule the
job between release time and deadline if
I ignore precedence constraints now I
can also schedule it with precedence
constraint knowing that the presense
constraints that I have to respect don't
have a long chains okay so let's start
with the first with the first argument
if if the top jobs wouldn't have any
presence constraints with each other
what could I do I claim that then I
could actually find a good assignment
now this is an easy problem this is
essentially a bipartite matching problem
where you can imagine that for every top
job there are some slots some time slots
when you could schedule the job and you
know that the last sales solution it was
a fraction solution for that bipartite
matching problem the only difficulty is
that there are some some parts of the
fractional solution are cut off now if
you have a you have a matching problem
and there is some part in the matching
graph that's cut off but you can check
Holt's condition and you can just see
what is the amount of edges that are
denim that I'm losing and okay so
essentially the only thing you need to
check is the following you look at some
subset of the top jobs and you lose what
is the neighborhood what is the size of
the neighborhood you're losing and now
the argument is that for that out of out
of queue intervals out of cube bottom
intervals you're only losing two
intervals you're only losing the slots
in two intervals and the reason is that
every of the top jobs must contain the
one of those border lines and you know
that you have one of those border lines
only every every Q of those small
intervals okay so in particular it means
that you cannot lose more than two
bottom intervals in any of the that are
contained in one of the top intervals
and I'm not convinced that this was a
great explanation but I hope you believe
that this is not a hard argument so this
is essentially it's a bipartite matching
problem you can bound the number of jobs
that you can't schedule anymore we're
just looking at halls condition and just
counting by how much in your
neighborhood could possibly shrink and
here you use that well you're kind of
talking about intervals okay
okay so this is the first phase now the
second phase is this is might look
familiar to you if you're into
scheduling essentially we're using the
earliest deadline first scheduling
policy which is kind of it's a standard
thing and scheduling it's it's kind of
optimal in many settings it's not
exactly optimal in our setting but it's
good enough okay so what is what is what
is the problem now
imagine these are our top jobs these are
the jobs that are assigned to top
intervals and now what is this setting
what do we have left now we know that
the the length of the maximum chain is
not too long we can make this as small
as we like we we know that well each of
those jobs may have a release time and
may have a deadline that we we know that
actually the time horizon is partitioned
into a not too large number of of blogs
which correspond to where the top
intervals begin and where they end so
we're the largest of the bottom
intervals begins and ends and we also
have some capacity left and please
remember that we already schedule all
the bottom jobs recursively so there are
some slots are left so you can imagine
that there's some kind of capacity curve
and we have to fill in the top jobs
somewhere in the remaining slots and
from the last slide we do know that if
we would be allowed to ignore any
precedence constraint among the jobs we
won't be able to schedule everything
okay and now the argument is that well
we can still schedule essentially all of
the jobs losing only a small number and
here the crucial argument is that the
length of the maximum chain was founded
okay good so what is the earliest
deadline first algorithm you you
essentially you sort the jobs according
to according to the deadlines and now
you just schedule them greedy
and let's imagine the following if you
have a job and his deadline is coming
and we didn't manage to schedule the job
in one of the slots then we just throw
it away
and we want to count how many of the
jobs did we throw away okay and there's
the last slide and this is essentially
this is essentially a modification of a
standard argument at scheduling so if
you ever look into scheduling book you
might find something very similar so
imagine this is the time horizon these
are blocks and you only have deadlines
at the beginning and at the end of of
those blocks
okay now let's assume for the sake of
contradiction that there is some
interval where we have discarded more
than K many many jobs and we want to get
some powder K so let's imagine this is
very very large and they will come to a
contradiction
okay good so for some technical reason
imagine that the last job that you
discarded in this block it will really
was the the lowest priority job and
making you you just delete everything
else the trick is that the earliest
deadline first policy doesn't actually
change for other the job that came
earlier so that's without loss of
generality okay so we know that this
block is kind of overloaded for some
reason that we don't understand yet we
were throwing a lot of jobs away now
let's let's backtrack and let's let's
see let's go back to the last to the
last block where we had less than see
many idle times and see was the bound on
the maximum chain length meaning that
actually the block before had more than
see many idle times and now the crucial
argument is if you have a job that we
did schedule in one of those blocks here
then I claim that the deadline could not
have been before this point because if
it would have been before this point and
it would have been at this point
or earlier and I had more than see many
idle times meaning I could have
scheduled him here so the point is the
observation you should make is okay so
let's say in in this picture the maximum
chin okay imagine the maximum chain
length is two and now here let's say I
have three I have three free slots now
imagine you want you want to schedule a
job and you want to schedule it as early
as possible then the only reason why I
cannot schedule him in one of those
three slots would be that well he has to
depend on this guy but why did I not
schedule this guy earlier what he has to
pending on this guy well then DSO depend
on this guy so then you can again
construct a chain of length more than C
very similar to to the two grams
algorithm to the analysis of grams Algar
and then you would get a contradiction
okay so what happens so every job that
we actually schedule in this in this
area
it had release time and deadline here so
it had really and any of those K that
jobs that we couldn't schedule it has
released some and deadline here and that
gives you a bound okay and the ball
depends on C and and that number of
blocks here yeah I think it should be K
should be bounded by PMC and so you know
that in no block you discard more than
bad many jobs so you probably have to
multiply that with another factor of P
which is your number of blocks and
that's actually the end of the whole
thing okay so there are some
calculations you just need to count how
many jobs did you throw away in total
you choose the parameters correctly but
every idea is actually you you did see
okay it's any question
okay there are actually some open
problems so it's it's not so clear
whether not a constant number of the
seiran's would be enough or she'll rally
items rods or at least an actual poly
log number of rounds so for us this is
like a little aw this is M divided by
epsilon square times log log n so it's
not so nice actually also a nice problem
is what happens if you have running
times that are not one but you have
arbitrary running times some of the
arguments still make sense others others
don't so that would be nice whether one
can have some extension here and yeah
that's what I wanted to tell you thank
you for listening also works if you have
general running times - maybe 2 - 1 or M
something like that but nothing why did
you need this break up because I thought
you just needed it agree you just need
you to be large right yes yes but you
could be a large constant as well right
it depends on epsilon why he does it
so you want to say you want to throw
away like an epsilon or epsilon gram
fraction of jobs throwing away would be
fine there is a technical problem which
is that the level where the jobs are
assigned its it can move because you do
the conditioning
and this is makes it very hard rule to
only cut a constant fraction of the jobs
so this is what we had to choose the
parameter larger it yeah it does look
like a technical technical problem we
really thought for some time about it
but we we couldn't get rid of it yeah
that that's the reason why there's the
log in to the log log in term it's one
of the things where you write it down
you think okay it's not going to take
you more than 20 minutes to find a
solution and two weeks later you say
let's let's be happy with it yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>