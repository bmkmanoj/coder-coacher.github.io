<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MSR Vision Faculty Summit - Artistic Stylization of Consumer Media Collections | Coder Coacher - Coaching Coders</title><meta content="MSR Vision Faculty Summit - Artistic Stylization of Consumer Media Collections - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MSR Vision Faculty Summit - Artistic Stylization of Consumer Media Collections</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uleJx65XVMk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so so we are starting the last session
of the workshop and it's a talk by John
calamos with artistic stylization of 10
media at elections voltage on ok Thank
You introduction ok so I'm John
columnist for the University of Surrey
I'm a lecturer there I've been there for
just over two years now and the work I'm
going to present today is briefly a
summary of what our group's done in
those in those two years and I'm going
to focus in particular on one project
which is on the artistic stylization of
consumer media collections so this is
another talk where we've got the
computer graphics and the computer
vision coming together and the graph
exposing new requirements that motivate
new work to be done in computer vision
and in particular I'm going to present a
video segmentation algorithm so more on
that later so just some brief motivating
context I'm sure we've all been in the
situation where we've been in our loft
and come across an old box of media of
old photos and started to leave through
them and had a very enjoyable experience
reminiscing but and those those photos
have a lot of value to us they encode
our past memories and experiences but
they're largely useless because they're
inaccessible and they're not in a
structured form and indeed we probably
got old videos as well which arguably
might even be more useful since we can't
play them on normal hardware now of
course we have digital photography and
digital photo frames and the like to
present our media but our personal media
collections really any more accessible
the work I'm going to present today
focuses on something we developed with
some HP Labs funding we call the digital
ambient display and this is looking at
ways to add value to those media
collections through structured
presentation of the media and also
stylization of that media in artistic
forms now what I mean by artistic
stylization is transforming images or
video and videos the focus of this talk
into paintings or cartoons that kind of
thing so before I go into detail on that
project I just want to summarize some of
the
work that we've done broadly our group
is looking at ways to increase the
impact of visual media collections of
image and video collections we do that
through either techniques for visually
searching those collections or
techniques for either presenting those
collections in different ways or for
editing those collections one of the
projects we've looked at recently is a
new segmentation technique distinct from
that operas in today called touch cut
where we have a system on a touchscreen
device with a single touch we're able to
segment an object in an image and also
use that to bootstrap video tracking
process and the idea is that enables
editing on touchscreen devices you can
see here we've selected this object and
stylized the object using a simple
artistic rendering filter we've also
been looking at visual search as I
mentioned and looking at sketches for
visual search we've produced a system
for sketch based image retrieval that
was the first system to adapt the bag of
words framework commonly used for for
photo realistic image query to sketch
based image query and we did this by
introducing a new descriptor we called
gradient field hog and that outperforms
sift hog and various other descriptors
by about ten to fifteen percent at the
task and sketch based retrieval I can
take questions on some of these I won't
be covering these today we also looked
at sketches for video retrieval where we
sketch not only the object but also the
direction the object moved and we use
that to index a database of of sports
video and contribution there was a new
probabilistic model for labeling super
pixels in the video to one of many
objects that have been sketched and the
way that we treated that was as a hidden
label problem trying to label these
these super pixels to as i said the
objects all the null objects which
hasn't been sketched whilst also trying
to produce a trajectory across a
panorama of the video that increase
monotonically treating this as a state
space this was presented at ICC V in
2009 another type of visual collection
we've looked at is dance we've got some
work under review at the moment looking
at a new form of descriptor for matching
hose in photos of dance and also videos
of dance and this is able to work over
very low resolution archival quality
footage this has also got a benefit
beyond computer vision where we
digitized a lot of dance collections and
put them online for the community and
we've got a web demo of this technology
and then finally we've also looked at
visual search and I guess more of a
traditional sense looking at the
identifying photo fragments in large
databases and this is disk funded work
it's looking at developing a solution
for visual plagiarism detection in art
coursework and it's been trolled now it
for UK universities and it's more of an
engineering project this looking at
cloud solutions for visual bag of visual
word search so that was a very brief
summary let me go into detail now about
some of the work we've been doing on
these digital ambient displays this
project was motivated by questions well
the question of how we can present our
digital media collections in a setting
perhaps commensurate with a shared meal
or a living space where the media is
engaging whilst at the same time always
paradoxically not being too distracting
so we've all been in the situation where
perhaps we've been in a cafe and we've
seen a TV in the corner it's showing a
program and we always distracted by this
program even if we're not interested in
what the program is so it's alway kind
of hanging a video on a wall that gives
us the content but isn't isn't so
distracting and we feel that one way of
doing that is to use artistic style
ization techniques so this display does
a couple of things it's not just a new
average digital photo frame it presents
the media in a structured way by
clustering the media using keyword tag
similarity and also clustering on visual
similarity to create a hierarchy out of
your content so if this were trips to a
trip to London it might cluster your
photos into like London big soy big ben
buckingham palace and so on and the
system will walk around the tree
presenting that that media and it's also
equipped with a camera so it will dive
down deeper into the hierarchy if you if
it seems that you're interested in
something that's showing I'm not going
to talk about that side
work today what I'm going to focus on is
the artistic stylization and as I say
that this motivated some new computer
vision algorithms that I'm going to
present so because I guess many people
here won't have looked into the artistic
stylization literature I'm just going to
give you a very brief overview of how
these techniques tend to work I guess a
lot of people would argue that artistic
stylization sometimes called NPR
non-photorealistic rendering originated
from the work of paul habberly in
siggraph 1990 and he observed that
people when they're using digital paint
programs which really haven't evolved a
lot in the last I guess 40 years it
least in the simple case they can't they
can't present they can't develop these
rich kind of images that we might see an
artist produce in real life and he
attributed this to the large time to
palette to go back to select new colors
in the paint program so he came up with
this idea whereby if we wanted to
produce a painting of this photo we
would click on the photo and a color
would be sampled at that particular
pixel we've clicked on and then on a
canvas that shared the same geometry a
paint stroke would be splattered down
and that would be done in succession to
produce a painting over time so this is
a times ten speed up of someone using
such a system here not only the color
but also the stroke orientation is being
sampled from the underlying image using
just a sub L operator kind of gradient
thing and then the user is choosing the
size of stroke and obviously implicitly
the ordering of the strokes as well but
this can also be automated this scale
selection using some sort of edge
strength measure or so maybe an image
salience measure of some kind so you can
produce very quickly especially when
you're playing it ten times feed
something that looks very much like a
painting but it's still a manual semi
manual process and if we want to extend
this to video which we're interested in
this can prove to be non trivial
exercise so the main challenge in
creating paintings from videos is what
we could obviously temporal coherence
which means that the strokes should
basically not flip
so they should be stable but also the
motion of the stroke should match the
motion of the underlying objects in the
video so what was being shown there on
that video is that these strokes were
but this this initial attempt by the
vinovich was basically pushing strokes
from frame to frame using an optical
flow estimates and because the strokes
can move independently to the objects
underneath them they tend to pull apart
and not match the motion of the
underlying objects so we're seeing
motion that's not consistent with the
underlying object and also not
consistent within a given object but if
we look at a real painting we can see
that actually the way the artist lay
down strokes is consistent with in
semantic regions of the object so in
this paper in this painting around the
cake by thea boat from 1962 we can see
that these strokes have been laid down
consistently within this region of the
of the image and it immediately makes us
think in terms of computer vision about
image segmentation and how that could be
applied to the problem so this is a
manual segmentation of a bat image but
not too far away with what you might
expect an image segmentation algorithm
to give us so if we were to use image
segments to constrain our stroke
properties to be consistent we get
stability over time if we're looking at
a video and also we would get
consistency within the region and so the
problem of painting a video is very
closely related to the problem of
segmenting a video in a temporary
coherent manner so once let's suppose we
were able to segment the hand of this
boy over time what we could then do is
perhaps establish the correspondence
between the contour of this frame to
this frame and then using something like
a shape context and then we're able to
produce some sort of dense motion field
which we can walk The Strokes around in
effectively producing a rotoscoping
system so stabili segmenting the visual
structure in a video basically leads us
to higher quality stylization so this
motivates the video segmentation
algorithm I'm going to present today
this is work that's just been accepted
to I Triple E transactions on multimedia
so this is
my chance to showcase it since it's not
in the conference the key features of
this system are it's basically a
multi-label COF based segmentation
approach so multi-label graphic art
based approach and it incorporates not
only a unity and pairwise turn but also
multiple super pixels segmentations
which increase the spatial coherence of
the system but of course we also want
temporal coherence and in order to do
that we do some bookkeeping to keep a
color and texture model updated as we
move through the sequence it's a
progressive algorithm that goes from
frame to frame and we also have to do
some bookkeeping to manage the birth and
death of different regions okay so this
is the energy term that we we optimize
first of all i'm just going to explain
how we segment a single image and then
we're going to talk about how the motion
comes into it so we're looking here as i
say with a unary term a pairwise term
and a high order term that relates to
super pixel segmentations of the scene
and here we're introducing notation I
subscript time for the frame data for
the pixel data and we're trying to label
each pixel I in the set of pixels V in a
particular frame with a label from this
set here I'm going to walk through the
different terms briefly to give you an
overview of how the technique works the
first the unit erm initially isn't so
exciting because the first couple of
terms are kind of our guest fairly
standard the unity terms made up of a
color model a texture model and this
temporal model which is the interesting
part which will come to in a moment the
color model is effectively a GMM learnt
over time for a particular region label
so it's a it's learnt just as a simple
color model driven by the pixels that
have been labeled as that region in
previous frames there's a texture term
okay which is a standard text on base
term we we distill a number of feature
vectors from the the first frame of the
video and a 40 dimensional space to me
codebook that space okay so that each
label then has a histogram or
of those code words that represents the
kind of texture we would expect to find
in that label and then finally there's
that labeling prior that I mentioned
briefly before okay which is basically
information that's been propagated
forward from previous frames and that's
that's the interesting part the pairwise
term is fairly standard it just
basically will penalize contrast change
within a single label region so this is
seen in in many papers and the super
pixel term relies on multiple over
segmentations of the video frame using
in our case reviews mean shift and an
end cut super pixel technique we run
these algorithms with various different
settings to get in our case six
difference over segmentations and what
we do is we penalize situations where if
we have so particular label and that
label lands within a super pixel what we
do is we count up how many other pixels
disagree with that label and that gives
us a penalty term that we apply for that
for that particular labeling and that's
averaged across all of our over
segmentations I'm just going to give you
a preview of the output of this
algorithm and I haven't described the
motion propagation yet just the just
this just the static term this is the
source and this is our output
yes yeah that's so yes so thanks to
Michael Co for releasing the stage set
okay so right so so this this is a
single frame the source and alpha posed
energy term and remember this
incorporates the motion propagation
which I haven't yet explained so i will
in a moment i just wanted to show first
of all the influence of the different
terms so if we just had the unit erm in
our optimization obviously we can see
there's a lot of spatial incoherence
over here because there's no constraint
known as enforced by the pairwise term
and with the pairwise term we clean a
lot of that up we can still see some
incoherence here and especially on the
finer features some difficulties around
the legs if we include one super pixel /
segmentation we can see a significant
improvement but still we have some
difficulties here around the leg regions
but then when we do incorporate multiple
segments over segmentations we get a
much better quality definition around
the legs so this is the justification
pleasing multiple over segmentations
because some of those over segmentations
are likely to be incorrect okay so
moving forward then to the motion term
our first idea was to basically
propagate forward the the labels from
frame to frame to basically initialize
the graph cast on the next frame so
let's say we had a frame and this is the
result of our single frame segmentation
and I should say on a very first frame
we initialize using just the standard
mean shift segmentation then what we do
is we take we use the dense sift flow
algorithm to compute for every pixel a
new location at at the next frame
instance and then we skeletonize those
regions and use those as if you like
scribbles to initialize the graph cuts
on the multi-label graph cuts on the
subsequent frame ok and that gives us
then a basis to segment the next frame
and so on iteratively so these are then
in the in this initial attempt are hard
constraints the other thing we need to
do is update that appearance model over
time the car
texture appearance and we do that by
basically updating our gaussian mixture
model over time by recomputing it over
the last few time steps with a
contribution weight that decays over
time and this gives us also a convenient
way to detect the region birth and death
when regions appear sorry when regions
yeah up here it means that we have
allocated a label because we've always
got as many labels to allocate as we had
available to us on the previous frame we
have to allocate all of those labels and
a new region will therefore be allocated
one of those pre-existing labels so that
means there was somewhere be a region
that basically two two regions that have
been allocated the same label and that
will cause very different gaussian
mixture model in the color space to
occur so by measuring the chi-squared
different distance between these two
gaussian mixture models the current one
and the previous historical model we can
detect the appearance of regions and
then resect meant that region again just
using standard mean shift the region
disappearances or deaths a handle it
implicitly because we needn't use all
the labels we used on the previous frame
to label the current frame these are the
results of that first approach this is
per frame mean shift which you might
well with a bit of help from an edge
detector which you might expect to
perform poorly this is our approach and
then this approach from ECC be 2008 from
Paris hotel so we already get an
improved coherence and this is a rather
artificial example showing the region
appearance being handled so this was the
first iteration of our system but it can
fail especially when the sip flow is
computed incorrectly so what we then did
is we went and enhanced it with by
moving these skeletons from being hard
constraints to soft constraints and
diffusing them probabilistically over
time rather than just propagating them
forward on a per pixel basis so given a
particular region which could be say for
example this region with label n we are
propagating forward as I just described
a subset of those pixels
Oh n which represent the skeleton of
that region okay and the system is also
going to fail if the skeleton that's
propagated is wrong so what we do
instead is we diffuse forward those
labels so that each pixel has now a
probability distribution over it as to
which label it may email obtained in
more detail this is our skeleton then
for a single region and it contains
these pixels at the previous time step
okay and we're labeling those pixels one
to the number of pixels in the skeleton
okay and we're computing from every one
of those pixels at its previous position
in the previous frame okay we can
compute for any position in the current
frame the probability that that that
label has moved that pixel labels move
to that location and we do that by
simply centering a Gaussian on its
predicted position based on that stiff
flow and that Gaussian has a standard
deviation we choose the standard
deviation based on the local estimate of
motion coherence around that pixel okay
and we do that by computing in a small
window okay a seven by seven window
believe it is a histogram of motion
vector directions and we look at the
entropy of that histogram and if the
entropy is high it means is uncertainty
in that region which means we would also
which means we want to have a lower
spread okay so obviously if we do this
for every pixel in the skeleton we can
sum up those contributions and we
normalize here assuming to the uniform
prior here to get basically the
probability that a particular pixel has
been assigned the label from that
skeleton so that's our probability that
we propagate forward and becomes this
term or an approximation to this term
which was our third term in that you in
that you know a term of our graph cart
that's only taking into account the
previous frame okay actually what we do
is we look at multi
frames we look at the current frame and
several frames before it and we compute
all of this basically for thats that
frame and one before it and then that
frame of the one before that and so on
in a paris fashion I can we can average
these together and we wait the average
according to some confidence and that's
a confidence between the current frame
contents and the Warped content we got
from the SIF flow okay which hopefully
you're following obviously yeah so okay
so here's the proposed method then on
that sequence there's a URL in this
paper that you can download these and
the data as well
okay this is a comparison then of our
proposed approach to the hard assignment
approach so this is our soft assignment
motion diffusion approach this is the
hardest islands this is that Paris
algorithm from eccie be 2008 and this is
the Quattro tell algorithm from CBPR
2010 so we can see at least subjectively
that there's a greater degree of
coherence there oh I think you've seen
that result previously so I'll skip
again for the moment just subjectively
we can look at the results of the the
hardest Islands which was that that
first approach and I'll proposed motion
diffusion estimate and you can see that
we get rid of this kind of smearing
artifact that we can see here on the on
the on the legs if we look at the
quantitative data then and we use the
but the Berkeley methodology to evaluate
this by manually marking up frames every
ten frames so that my PhD student
doesn't get too tired and then what we
do is we use use basically the Berkeley
methodology to evaluate how close our
segmentation and various olive
algorithms segmentations are to that
frame we can see interestingly that
despite this being a progressive
approach we don't have a decay over time
that you might expect with accumulation
of error so these are some of the
results this is obviously the purpose of
this was to produce artistic style is a
shins so here are some kind of paper
cutout stars that we presented from
various videos and these are some
examples of painterly renderings that we
produced from the sequence notice the
problems with the faces which will come
back to and there are well several
minutes of this really these are some of
the paper cutout kind of highly abstract
approaches
from that from that part of the work we
can see that although we've adopted a
forward propagation strategy for these
labeling friars by extending this
initial idea of the hard assignment to a
soft assignment with motion diffusion we
don't have the kind of accumulation of
error that we might expect such an
approach to have over time and also that
the algorithm performed competitively
relative the other algorithms that I
showed you and we've also shown that
that coherent video segmentation can be
applied to the computer graphics problem
of the artistic stylization something I
haven't shown you in detail but I just
pretended the results for this is
published already so you can you can
look this up is that we did some user
studies looking at user engagement with
these ambien displays looking at a
random presentation of the of the videos
rather like a random slideshow you'd get
on any digital photo frame a structured
presentation using our semantic
structuring of the media which again I
haven't had time to go into and looking
at the structured and artistically
rendered version and in all cases we saw
an improvement in a statistically
significant improvement in the level of
engagement based on the amount of time
the user spent looking at the thing
which we measured using the onboard
camera okay I've got a couple of minutes
left so I just want to give you a
preview of some additional results that
we produce recently looking at the
rendering of faces which were weakness
in a segmentation based approach
rendering faces is an especially
difficult problem for an artistic style
ization algorithm because we have a very
high perceptual prior on what we believe
to be an acceptable rendering of a face
if we run one of the state-of-the-art
algorithms for painfully rendering over
a face we get a lot of blurring here
around the facial features that we would
normally find unacceptable we've created
a new system that will produce
high-quality portrait renderings by
fitting a model to the face and treating
regions independently another
interesting thing about this system is
that it learns artistic style from
example so the system can be given a
photo and painting but the user has
produced and then
we'll learn basically models for the
properties of strokes on a per facial
region basis we get this by fitting it
out to a shape model and then those
parameters are learned based on the
underlying information we've extracted
from dense feature extraction from the
image and that model can then be applied
to render new images and it can take
into account shading and introduce
things like complementary color if it's
been used and produce a variety of
different artistic styles so these are
our rendering outputs and these are some
of the input paintings that were
produced and in particular the hair and
the I preservation is much better than
some of the state-of-the-art algorithms
okay so that's my time up i think this
work is obviously due to a large group
of people these are my students and
postdocs that work in our group and yeah
thanks for your attention and be happy
to take any questions so we have time
for a few questions ha deserve a really
impressive results especially on the
emotion segmentation a question I had
your your spatio-temporal CF model you
had quite a few potentials how did you
learn the weights or did you set the
weight mmm ok right that's a good
question so I didn't talk about the
inference step actually the inference
step for the spatial term is Boyd very
heavily from the robust PN model but
history and colleagues presented in cvpr
I think 2008 and we use we use that to
optimize on the spatial spatial
segmentation in terms of the temple
that's just propagated information from
from previous frames so that's put in
place but what about the weights of
dividend shots okay all know on the unit
earn yes once and there was an eye on
empirically my understanding is they
don't require an awful lot of tweaking
but yes I I actually I believe that the
univ ii and pairwise turn the weights on
them which is one i think the the super
pixel term took a bit of tweaking yeah
so do we have any more questions okay
cuz different computing because you used
yeah yeah i think optic flow is really
crucial right yeah definitely I mean we
tried quite a few techniques when we
were developing the initial hard
constraint version like box and various
things like this but we just know I've
got a dim dim view of what's cooler
tellings is very difficult to find one
that worked very well for a data but we
did find that the SIP flow was perhaps
one of the more more generally useful
useful techniques and robust I mean so
we just adopted that in the end but the
purpose of the motion diffusion was
actually to mitigate errors in the in
the dense flow estimate but you never
know it's always fed forward so one of
the so this was commercially sponsored
work and one of the potential
applications for this were to run fast
enough was was for video stream
processing actually so we were kind of
avoiding that by design so like to focus
a little on the artistic re-rendering so
what have you explored alternative kinds
of rear ndering to the well look like
large largely was just recoloring all
your super pixels here but yes the
region map that we get out is our basis
for stylization and we paint with the
same algorithm for every region but i
guess we could we could explore why the
gamut of stars by just selectively
painting certain regions or applying
different algorithms but but really
we've been focusing on the painterly
rendering yeah further well and also we
saw that paper cutouts tell which was
literally just the mean I mean shading
of the regions so yeah you said
something
either you start here son he was that
people media would be more particularly
yeah they do this kind of stuff I mean
is that actually chance so the about the
validation in that respect that we did
was through and the user studies of
engagement that I only briefly presented
the results for but that's published in
computers and graphics in a journal
version is a long piece on that study so
yeah I mean we can say that they're more
engaged with their collection question
is whether that's just due to novelty of
the effect but yeah it's very hard to
judge whether someone likes a technique
more I mean this is the problem with the
whole of artistic stylization really a
validation aspect of it but but yeah or
something well the initial motivation
was to to create a a system that
produced a rolling visualization of of
your video collection in the way that
didn't just basically replay the video
which seems to be how things are done so
especially with that paper cutout effect
I didn't really play the full length of
it but so the paper cutout kind of
morphs into another paper cutout and
morphs again and it's meant to be this
kind of flowing almost like a
screensaver visualization that's less
distracting and then perhaps just
replaying the video i think that degree
of abstraction is yes it is arguably
attractive for presenting that media in
a ambien setting but whether it be very
difficult to judge how to validate that
yeah we haven't we haven't looked into
that okay okay so let's tank John again
and</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>