<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Phase Transition in Random Graphs: A Simple Proof | Coder Coacher - Coaching Coders</title><meta content="The Phase Transition in Random Graphs: A Simple Proof - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Phase Transition in Random Graphs: A Simple Proof</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HtBWmw4d8tg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
everyone it's a pleasure to host Benny
silikov who's visiting us almost every
year from UCLA and Benny we'll be
talking about a new simple proof for the
classical phase transition in the air
the shredded raining random graph Thank
You al so let me say as that what I'm
going to talk about is a joint work with
Michael cleaver leverage and indeed the
idea would be to present an approach
which we think is pretty simple approach
to several classical result in random
graphs since we're talking about random
graph so let me start with a basic
definition so the model we are looking
at is jim p it's a probability space on
graphs on n vertices in which
probabilities at I J is an edge is P
which can depend on n and this is
randomly and independently for all
possible pairs of edges and another
piece of notation so this is 1 this is 2
so given property graph property P we
will say that GNP has key with high
probability insistence with high
probability
if probabilities at GNP satisfice he
tends to 1 as n constant changes we will
have always asymptotic notation anything
so this is our basic object so this area
the area of studying of properties of
random graphs goes back 50 years to the
work of erudition Rainey and one of the
first result erudition Rainey proved was
addition Rainey that if you look on the
edge probability P around 1 over n then
you process that the properties of
random graph experience a very
interesting behavior so called you know
once you move from less than 1 over m to
above 1 over N sudden is a giant
component emerges so we will have two
regimes okay so in the regime when your
edge probability is a little bit below 1
over n say we have with high probability
all connected components
have size Big O of log N and I put her
epsilon indicating that the constant B
depends on epsilon and when P is 1 plus
epsilon n then with high probability
there is a component of linear size let
me say that this is probably one of the
most well known result in not only in
random graph but in the whole
probabilistic combinatorics and
obviously since work of erudition Raney
it was revisited many time and extended
and people understand it very well so
they know precisely what is the size of
the component here and here they know
what's going on if you you know slow
down and kind of see what happening in
the threshold window how you move from 1
minus epsilon 1 plus epsilon lots of
information is known ok this is one
result I want to talk about another
result is a theorem by I take on Russian
samurai D so they wrote in 80s it's also
very old result a paper which appeared
actually in a celebrated first issue it
was first paper in the first issue of
combinatorics
the answer the question of L dash
improved that not only you have a giant
component but actually you have a long
path a long cycle both of linear size so
what they proved that if your age
probability is 1 plus epsilon divided by
n then with high probability
GNP contains path and actually can show
once you have passed also cycle of wing
your legs
and so what I want to say no no not
nothing here you just just effects it
you know your giant component can be
very complicated structure tree like
mostly tree like structure here actually
talking about a path which is harder to
get so this is the second result and
what I want to do in this talk I want to
show you an approach which actually
gives both of these results and I think
it's pretty elementary and because of
its elementary it works in some other
circumstances and I will mention in
which circumstances it works so just to
be precise in terms of formulas here the
results which here is the statements
which which I will going to prove claims
okay
so again in the regime when P is 1 minus
epsilon over N using this approach you
can prove that with high probability in
GNP all connected components have I have
sighs it must and all right precise
numbers because I want to say something
about this numbers it was 7 over epsilon
square log N and then the next thing if
P equals 1 plus epsilon over N then with
high probability
GNP has a component I will stop writing
connected component so it's obviously
what I mean has a component of sighs at
least epsilon n over 2 and the third
thing is that if P is 1 plus epsilon
over ends and again with high
probability
GNP has pass / cycle of length
at least epsilon square and over five
okay so that's the results which easily
follow from this approach which I will
present now why I am writing a precise
formulas because what I want to say is
that in addition to giving simple proofs
for the qualitative statements
surprisingly this approach also it
doesn't recovers completely the precise
constant it recovers at least a correct
dependence on the epsilon so indeed the
best dependent on the epsilon in this
regime is log in with a constant log in
with the coefficient one over epsilon
Square and in this regime giant
component behaves like constant epsilon
N and indeed the length of the longest
cycle is of order epsilon square n
because so that's I need carefully check
what if epsilon is constant is at least
but small it's recover says the funding
another thing which I want to mention
which is kind of more interesting in
this regime and you'll see from the
proof it works not only for epsilon
constant but you know if you know a
little bit more about the topic you know
that the critical regime where where
things happening is when you take
epsilon n to the power minus one-third
so this all works above when epsilon is
n to the power minus one-third let's say
x squared log so all this results still
continue to work so kind of interesting
so it's a as I said there's a new thing
is it just the proofs are simple and I
think it's a bit different compared to
other proofs which people used and they
give you a lot of things pretty cheaply
okay so that's what I'm trying to
present so now what we are going to do
another nice thing about this approach
we are going to use some very very basic
tools the tools which probably you teach
in a class of algorithm the first year
in for the students in computer science
so the algorithm which I'm going to use
as a graph of exploration algorithm
called depth-first search
so so of course most of you know but
just to be on the safe side let me
briefly describe the algorithm and let
me briefly tell you what are the
properties of the algorithmic I'm going
to use okay so this is graph exploration
algorri so you start with a graph okay
and you explore it basically discovering
connected components but in certain way
so you fix some ordering of the vertices
so Sigma is some fixed order in which
you explore there's a graph and at any
moment you have a partition of the
vertex sets into three sets S Plus u
plus t3 disjoint sets where s is a set
of already explored vertices T is a set
of yet not discovered vertices
and you is a set of vertices which you
currently exploring under exploration
and it's important how we handling this
you see you see you is a stack so it's a
it's a list so they kept in stack last
in first out okay so what does it means
that every time you add vertex to you
you put it in the end
every time you remove vertex from you
you take the vertex homes and in you
remove it okay so you is always will be
this linear list where the vertex were
always adding to the end and we're
removing it also from there now how this
algorithm works so here is a brief
description you start with you empty and
in the end in so with you empty SMT and
T everything and in the end as is
everything and U and T are empty so you
basically explores a photograph and at
each round
you do the following if you're--you is
not empty so if you have vertex in so if
you have vertices in your list then look
on V which is last vertex in you if
there is a W if there is a neighbor of V
call it W integer and first such
neighbor to you so you remember all my
vertices are ordered so if there are
more than one neighbor of V which I
currently didn't discover just pick the
first one and add it to you so you
remember what does it mean so that you
put it to the end of the list otherwise
what means otherwise it means that V has
no neighbors which we didn't discover
here so all neighbors of V we already
see otherwise move v2 s and continue
and the last thing which I need to tell
you that if at some point you becomes
empty empty pick first vertex from T and
move it so you basically took the last
vertex on which you look which you
discovered and you're trying to discover
its neighbor if there is no new
neighbors you said okay I already
completely explored this vertex I move
it to s if at some point my stack become
empty start with arbitrary new vertex
and continue some very basic algorithm
again I just want to be make sure that
it's written on the blackboard so you
can look at it because everything would
be self-contained enough for properties
of this algorithm going to use for very
basic properties the first one which
from definition clear each round one
vertex moves from T to you or from you
to s so why this is important things are
continuous ID at any point I can stop
and say okay I want is to be of certain
size I can stop my algorithms at moment
so things changing from the point of
view of sizes is continuously now
another important thing is you always
forms pass in G but because the way we
build you it's always a pass okay so
whatever is in you it gives us a pass
the shirt thing is no edges from s to T
that's obviously whenever we move vertex
to s from you it means well
explore its neighbors there are no
neighbors of this vertex left in T so
there were no edges from s to T at any
given moment in the algorri and the last
thing which I need is the following
between two times when u is empty we
exploring same connected component of
Jesus just once we start exploring the
connected components once we touch a
connected component we continue to
explore this connected component the
fact that u becomes empty again means we
finish exploring there are no edges
going out in any direction yes so say
any two moments between which u is not
empty this this gives us the whole
connected component this would be also
useful is this clear these are four
properties I'm going to use that's now
let me let me show you show you the
proof of let's say long patterns and I
mentioned something about the logic
connected component and so on so how I'm
going to kind of I'm going to use DFS on
the random graph and the way in this
case we're not very original everybody
does it so there was a way you kind of
use any graphics flourish and algorithms
a random craft you generate random graph
on the fly so what I'll do I'll take
okay so now this is my model it's fine
so take my big end to be anxious - these
are coin flips for the edges I'll look
on the sequence of random variables X X
I I runs from 1 up to M and X I this
binomial random variables 1 with
probability P and 0 otherwise
okay and I generate GNP by feeding the
sequence eggs to DFS what it means
DFS let's say with order one two and so
on
yes I said DFS should fix some other fix
arbiter or the fix or the one up to n so
what does it means you start running DFS
and every time the FS wants to ask as an
edge you just take your next coin flip
from the sequence the one which you just
didn't saw and based on this coin flip
you tell me the edge they or not okay so
obviously generate a random graph and
the only thing what I want to do I want
to analyze what what happens with this
algorithm and now another nice thing is
that although you know we're talking
about things which kind of may be
complicated from probability point of
view the only probabilistic I mean of
course there should be some statement
probabilistic statement here's the only
statement we are going to use at least
for the past it's more or less something
like chebyshev inequality so here is the
only property of the sequence which I'm
going to use everything else would be
deterministic so here's a property which
I'm going to use so I fixed my P 1 plus
epsilon over and I fix n 0 which would
be epsilon n square over 2 and I look
how many ones I see in the sequence from
the beginning up to the moment and 0 so
what I'm looking I'm looking on some of
X I I runs from 1 up to 0 so this is a
binomial random variables obviously this
thing is concentrated concentrated
around its expectation so its
expectation is epsilon 1 plus epsilon n
over 2 is just probability times this
number and the only thing I want I want
that
with high probability this is at most
let's say L well the only thing which I
need about L that its Omega something
tending to infinity of let's multiply
this numbers it's a square out of
epsilon and so again this is a most
basic result you can imagine because
what these are some independent random
variable so here is a proof so the
variance of some of X I I runs from 1 up
to n 0 is up to a constant epsilon n
because it's you know M 0 times P times
1 minus P so it's a + bi championship
inequality probability to deviate by
more than square root of the variance is
just 1 over the square of this number by
which you deviate if I take this Omega
Omega means this L divided by square
root epsilon n tends to infinity it
means that probability of this is
tending to 0 that's the only thing I'm
going to use any questions so far
ok so now now basically what I'm going
to do I'm going to take this algorithm
going to take this Fed and show you that
with high probability in the random
graph JMP with edge probability P 1 plus
epsilon I expect to see paths of length
epsilon square n over 5 ok so here is a
proof of the result ok so again let me
recall you claim claim is with high
probability
GNP P equals 1 plus epsilon over n
contains
path of length at least epsilon square
and over five and then getting cycle
from path would be easy okay
so here is a proof so on DFS on G yes
some to make it shot IGMP I will write
on G 4 and 0 steps and 0 steps mean uu +
0 steps means you until you see and 0 PS
ok so you basically asking 0 question so
you run your process generating
on-the-fly random graph for in 0 steps
and then you stop and you analyze what
what's going to happen ok so the first
thing you say if at any moment you you
yes you remember you have this vertices
in stack which form paths is ever is big
bigger than epsilon Square and over 5
you done
if ever you is at least epsilon square
over 5 done so assume you is smaller
than epsilon squared over 5 during the
whole process that's the first thing
now the second thing I want to persuade
you is that you far from seeing the
whole graph at time n 0 so so your s the
set of already explored vertices is
really small and small I don't need very
small so we claim that my s is at most n
over 3 ok why at time yes again at time
and 0 so suppose not
so you remember now I'm using the effect
that the process is continuous from the
point of view of size of s so I can stop
the a process at time when s is
precisely anniversary okay at at point
when s is precisely n over three now
let's look how big is T then my T has
size n minus s minus u obviously at
least an hour three because U is small
thing about epsilon small constant and a
less than half so U is tiny so s is at
most a novice recent is big so then we
checked
all peers from s to T which is s times T
at least n square over nine much much
bigger than n zero contradiction so let
me stop for a second here because that
would be the main thing I'm going to use
so remember when we stopped at time and
zero we only look on in 0ps only n 0
pairs of the graphs were checked whether
they edges or not so if at that time my
s reach size n over 3 and my T's of size
also bigger than an Aussie it means that
you remember what algorithm said there
are no edges from s to T no edges means
I check all these pairs and I discovered
they are all known edges so if Zed the
situations and already at that time I
looked on n square over 9 pairs which is
impossible because I stopped my process
at a time in zero okay so that's a main
tool which I'm going to use that you
know I cannot look on too many edges so
if the structure of my graph is so that
the corollary I looked on too many edges
that's a contradiction that's the only
thing I'm going to use so this implies
that as small as an anniversary okay so
now the now kind of the finishing touch
so now time gain trying to get a
contradiction and if I get a
contradiction contradiction to what it
going to be it would be contradiction
that my you was always small so I'm
still going to show in the end that I'm
getting a contradiction now what I have
in the beginning of this stage I know
already that my s is of size n over
three and I know red is thatmy U is of
size at most epsilon square n over five
and if I know that my T is not empty
okay
moreover I know that each X I equals 1
increases S Plus u by one every time you
discover an edge you move it to you
maybe later
remove this vertex to you in the
neighbor every time you discover a new
neighbor you move this neighbor to you
maybe eventually this neighbor moves to
s but definitely every time X I equals
one it means the new edge comes you
discover new vertex and this vertex
increase the size of s Union you is this
clear it's because you know what are the
edges what are the real edges on which
you looked you're never looking on the
edges inside s of from u to s you're
always looking on the edges from you to
t so every time such edges discover it's
a new vertex which added to the stack
and then eventually maybe this vertex
end up in s but it increases the size of
s Union you so therefore the number of
Excise which were 1 and this why I need
this inequality basically tells me what
is the size of s Union you at the time
and 0 this is precisely there's a number
ones which I saw so this implies that s
Union
put Plus because the Union looks like
you as well the S Plus you size is at
least as big as epsilon 1 plus epsilon
divided over 2 n - the cell
and since
you was less than epsilon square n over
five
I'll get importantly that s is at least
as big as epsilon 1 plus epsilon over 2n
minus epsilon square n over 5 - any
questions and now now it would be
exactly the same computation now the way
I'm going to get contradiction I again
will just compute how many pairs are
with one vertex and s and one vertex MT
and I'll show you that I saw too many
pairs I look on too many pairs compared
to the water already promise you with
the number n zero so so let's let's look
on the computation okay so if this is my
situation then number of peers
with one vertex in s and another in T is
so we know that this is at most
this is s times T but it at most 1 0
because n 0 was the total number of
pairs about which I asked questions and
definitely about all the eight pairs
which have one vertex in s and one
vertex in G I asked question and get the
answer no H ok so I know that the
product of this resizer should be at
most M 0 now it's equals s times n minus
s
this is legal so let's first substitute
you so it's s times n minus s minus
epsilon square n over 5 yes
trying to do it slowly to see that I'm
not cheating you and not switching the
inequalities because you can prove
remarkable things if you some points
which yes I'm substituting a bigger
number for you so definitely it's still
inequality and now this is a coach'll
moment look on this function because
that's why I needed to show you that s
is smaller than ever in our three so up
to basically some roughly n over two
this function is increasing function of
s so they thought to take its minimum I
will substitute the smallest possible
value of s so that's important to notice
yes so I'm substituting because this
function is increasing and my ass is
smaller than n over three I'm
substituting the smallest value of s so
what is the smallest value of s is
epsilon times 1 plus epsilon over 2n
minus epsilon square n over 5 minus L
you know if I'll substitute it here then
I'll get and minus EPSA 2 yes sorry to
here and and here this epsilon 1 plus
epsilon over 2 and epsilon square over 5
epsilon square of 5 cancels plus ur
and now let me tell you what every
single thing here is so this equals so
let me see what I have here I have here
epsilon n over 2 now let's look what
happens with epsilon square so here
we'll have epsilon square over 2 which
is 5 epsilon square over 10 minus 2
epsilon square over 10 so I have plus 3
trans epsilon square and minus M and
here I will have n minus epsilon and
over 2 minus epsilon square n over 2
plus L and now let's look so epsilon n
over 2 times n this is a main term it's
epsilon n square over 2 so so far it's
good this is this guy and now the
important things would be to look on
there epsilon square so here I have plus
3 tenths minus 1/4 so 3/10 is 6 over 20
quarter is 5 over 20 so I have here
epsilon square n square over 20 and
that's really what will get me a
contradiction because everything else is
so you also have some constant of order
epsilon cube and square and you also
have some constant of the order L times
n
so that's all that's all numbers and now
what I claim as long as this number
bigger than these numbers you get a
contradiction so you proved your result
and now why why keep L to the very end
because you can check that if epsilon is
bigger than n to the 1 minus shirt this
number bits is number obviously if you
only care about constant epsilon then
you can take L to be any function you
can take L to be n to the 2 shirts but I
claim that you can actually check yes as
this you remember this is thing is
epsilon power half n to the power
three-halfs
and if epsilon is n to the minus 1 so
this is exactly n to the 4th shirts and
this is exactly into the for set so if
you move it a little bit you're looking
well that's a halt any questions so as I
said the prove is really once you have
this statement is very deterministic so
you're basically getting a contradiction
on the fact that ru is small said you
look you just look on too many non edges
now now let me say something about ok so
this is kind of maybe more easily
believable because you know if I'm
looking for a long path then my U is
indeed by definition of DFS is the pass
or so therefore it's kind of easy to
believe that I'm getting large you so
now I want to tell you a few words about
why this also works for giant component
but before I do this let me only just
say a few words about how you getting
cycle from the past again probably most
of you know but just one remark
psycho
so no to get their cycle from the
pasture you already got your pass of
size epsilon square n over five so what
you can do you can take n to the 2
shirts vertices in the beginning let's
say 2 shotz arbitrary n vertices to
shut-ins n so if you get me an edge from
here to here you get a cycle but this
gives you n to the faucets pairs so in
order to get one edge in this interval
you need to sprinkle
probability you know P prime let's say 1
over N to the 7-6 so this will not
affect anything and and and and you get
a cycle yes because this is much smaller
than 1 over n so it your probability
virtually remains 1 plus epsilon away
and but now once you found this edge
found this path you will also find this
cycle
sososo cycle you're getting for the same
price as the past so now one word about
about component so let me remove this
so for the component you need to do a
little bit more work so instead of
chebyshev chebyshev already will not
enough or at least for for our approach
so you need a little bit more you need
channel bound because instead of talking
about that for one value and zero i want
to claim that this proposition halts for
all t between i know some theta n Square
and epsilon n square over two so it
doesn't matter you can put here anything
you want but you want to say that they
would be very long interval during which
your georgia you you will never becomes
empty because you remember how you
showing me that there is a large
component if you find me a long interval
of time during which you was never empty
it means during this time he was
exploring the same component so if you
start with something really really small
and you end up here and you show me the
during this time you was never empty
then you exploring the same component
and what is the size of the component
the size of the component is the number
of once you see you remembers it's a
number once you see during this time and
the number was is epsilon and square
were two times P which is roughly
epsilon n over two so that's how you get
it so so what I want to show I want to
shows that P again is 1 plus epsilon
when T is this and I want to show that
some of X I I runs from 1 up to t minus
1 plus epsilon over N
T
less or equal what it should be square
root of epsilon T over n Omega I want to
shows that this happens all the time so
basically you around your your value and
of course Omega is now not enough so
what I instead of do all I'll multiply
this by let's I squared
Logan was some constant and now I can
use a large deviation in equality so
again binomial distribution probability
that it deviates by square root of
variance times square root log n it's
smaller than power of n so I can indeed
take a Union bar now once I do this
approve would be exactly the same
because how I'll show you so now the
claim would be claim during times from
you know any constant and square
till epsilon n square over to you
never empty okay and why you never
emptied so exactly this computation it's
even easier than that because now I even
don't not assuming this there would be
this no error term epsilon and square
five if u is empties and everything is
an S and it means that you know
precisely what is the size of this
because the number the size of this is a
number once you saw during this time
it's roughly say there's a time times P
and now you're comparing s times T where
basically s times n minus s with the
number was a T with reserve is there how
long you are in the process and it's the
same contradiction so basically it's
precisely the same proofs and and again
if you carefully check error chance it
works up to n to the power 1 minus
so so that's that's all the ideas and
similarly you can show that in
subcritical regime you don't have giant
component exactly the same way as before
now you show that it cannot be long
interval where every time you see
someone so at some point you will become
empty after some constant log n steps
you become same jeans and empty again
and then empty again and again the same
computation okay so that's that's all
the idea another things that says that
it's a very similar to the proofs people
did think what simplifies is a lot the
typically people do or the branching
process or in an in in the language of
computer science they trying to explore
the Brad's first church from the random
coven surprisingly depth-first search is
works at least if you want a very rough
estimates you don't care about constants
much much better it's much simpler proof
I think just the computations are really
Elementary so you know you can you
immediately see what's going on so now
let me to finish let me tell you but
breadth-first search if you do it one by
one instead of just just looking of all
the neighbors of all projects the same
right so it's exactly the same there's
no as far as proving say the block
component it's exactly the same argument
or easy so the passage from death
research to that person doesn't simplify
at all it just gives you an additional
yes
see if you do breath for 30 meters its
growth the children on but when it's
exactly the same argument become a
depth-first search no no you still still
do breadth-first search my sexy but just
instead of exploring so you don't go I
see you just explore the children of a
vertex one but one until it's all all of
them at once and then it's exactly the
same very same distribution of variables
you have to just order in a different
way it's essentially you don't check you
don't change anything actually just the
same inequality you just have a simple
error jumps now so you can apply this to
other settings so let me mention to my
settings which kind of will follow the
same way and give you some simple proofs
for what people already knew so there is
a model
which people look at it's a random
director coughs a DNP so it's a directed
graph on n vertices and again the same
as before the probabilities at IJ is an
age he surrender million independently
for all pairs I'm J but now you're
looking into you taking into account
boss space I J in pairs ji so for every
such pair you toss a coin and was
probability IJ you put a direct with
probability P with a directed edge from
I to J so what you can show using
exactly the same machinery because
depth-first search you can do all some
directed graphs which if P is
probability 1 plus epsilon over N then
with high probability d NP contains pass
cycle of length constant epsilon square
so this is not new result this was
proved by car but I think it gives a bit
different roof to what cop was doing and
again it's exactly the same argument not
not very different now one more result
which I want to mention is so this is
one where the same thing works
so one model people looked at so so if
you think about random guards so what
you do you start with some arbitrary
graph which in this case is not how
better it's a complete graph and you
toss a coin for every edge in this
probability P you put it in an was
probability 1 minus P you leave it out
so you can ask what if I ask all the
same question about some random sub
graph of other graphs and where where
these things that you know my graph is
complete what is important about being
complete and one thing people observed
is that you can do the same thing on
pseudo random graph so let me just tell
you the definition and the lambda graph
is d regular on n vertices such set all
eigen values so you look on the
adjacency matrix of this graph
so if since it's directly the first
eigen value of this matrix is d and all
other eigen values except the first one
I absolutely lambda okay so this is one
way to define a graph which is have
strong pseudo random property so if you
lambda is much smaller than D it's an
edge distribution of this graph
resembles the edge distribution of
random graph is edge probability of n so
what you have is you have the number of
edges between any two sets SMT is
roughly what they should be so it's
deviates from D over N times st by some
small number so if your lambda is much
smaller than D from this for for large
sets SMT this value is much larger than
this value so this is a natural
definition of pseudo random graph if
your lambda yes
look on the lambda much much smaller
than D and so what you can do you can
say okay start with AB pseudo-random
graph and look on this random sub graph
of it yes so GP is just random sub graph
of G where you keep every edge with
probability and then then you can have a
very similar result which obviously bit
stronger than thing for complete graph
because complete graph is exactly n D
lambda gravity lambda is just 1 it says
so so so what you can prove that claim
if G is and D lambda and lambda is
little of these it will work not
necessarily for very dense graph
obviously digit tend to infinity because
I want my lambda to be a little of D
then G P where P is 1 plus Epsilon now
you should normalize it by D not by it
by n because that's a very natural place
where you will start having a large
component yes let's say pass or cycle in
this case
of ranks that epsilon square this is one
thing which I want to mention so the
proof is exactly the same thing prophets
before because you know once you control
a little bit distribution of edges
that's the only thing we're using here
yes we were using that from s to T there
are so many pairs and for all these
pairs I gave you answer no I didn't put
edges here that's a one thing another
and the set thing which I want to
mention which again people look at is
so-called maker breaker games so what
you do so you look so your board is
edges can so look on the edges of the
can and then at every step
maker let's say takes one edge
let's say colored it red red and breaker
takes say B edges color colors so why we
give break a little bit more power
because again from historical rhythm
people when they study this games they
see that for most of the games it's
actually very difficult for breaker and
much easier for me if they play
one-on-one so basically almost every
game would be the win of maker so what's
the goal so you take your favorite
property for example you want to make a
once to create a giant component he
wants to create a red a component of
linear size you want that red graft you
have a component of linear size and the
question is for which value of B breaker
can stop him and for which value of B
breaker cannot so you kind of many times
getting the parallel results you're
getting in random graph so there is some
kind of threshold value of B such that
if B 0 if be much smaller than B 0 then
make a winsome be much bigger than B
zeroes and wake up in something so so
this source of goal let's say goal for
maker is to get component
of Alinea size so now this is not now
with no randomness here because both
maker and breaker can play very
deterministic and very clever strategies
still again experience shows that you
roughly for many problems expect this B
to behave similar to what random graph
results suggest you and this is indeed
the case and actually again this is a
serum so people study this Bednarski and
luchek they proved it that if your B is
1 plus epsilon times n then largest
component of red graph it's constant so
if you give a lot of power to break us
if you give the breaker 1 plus epsilon
times n edges at every step then he can
make a largest component really we're
not a logarithm constant size which
depends on epsilon and on the other hand
if B is 1 minus epsilon times and then
maker creates a component of linear size
so this was the result of the mask and
Lu check and what you can get using the
same machinery is actually not only you
can get a component of winner size you
can create a path yes
if B is 1 minus epsilon times and then
makeup can
create a path of size out of epsilon
square and exactly the same of because
you know there's the strategy of Mirko
would be just to kind of every time to
keep of course this would not be a DFS
but he will keep this you and every time
will try to extend you to the longer
path and if he cannot if he already you
know some vertices in the beginning in
the end of you would be killed he will
just you know runs exactly the same
analysis comes to the first vertex from
which he can still get out and get out
and the same computation works till one
interesting remark that here we're
really talking about pass and not cycle
because for cycle it's actually not
enough so we can you can show that that
maker even with with that kind of bias
cannot create any cycles of a cycle you
know this thing should be n over two you
know 1 minus epsilon is not enough
so if breaker has more than n over 2 at
every step he can prevent maker to
create any cycle at all not even a long
cycle just any cycle so here the
problems is very very different and let
me stop here
so what what can you say about any
lambda class when lambda when if these
fixed and I tell you that they a lambda
is a absolute constant over the course
of time say x 1 Steve these large I
don't think what we're doing is kind of
delicate enough to deal with this case
for example you give me three regular
under graph I don't think at least this
naive approach is sufficiently strong I
think I need I need this I need really
kind of a good control how becomes so
somehow just because of computations no
the result is two we were only oh yeah
but but but what I'm saying is just
because they're you know if you know
concentration of edges up to little
order of main term then this approach
will easily gives you large pass but if
you only will deduce your your your edge
distribution from the eigenvalues and
nudies fix and this is not strong enough
you don't get a contradiction
let's think</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>