<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Brains, Minds and Machines - Andrew Saxe | Coder Coacher - Coaching Coders</title><meta content="Symposium: Brains, Minds and Machines - Andrew Saxe - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Brains, Minds and Machines - Andrew Saxe</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XbjHT_FC3-g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so let me introduce our next speaker
it's andrew sex he's a swats posture
postdoctoral fellow at the Center for
Brain Science at the Harvard University
he did his PhD at stanford way was
advised by jay McClelland and entering
and where he held standard stanford
graduate and india SE chi fellowships
his research focuses on the theory of
deep learning and its applications to
phenomena in neuroscience and psychology
and today he will talk about a speak
about hallmarks of deep learning in the
brain so Andrew thanks and thank you to
the organizers for inviting me it's a
pleasure to be here ok so it's a
remarkable fact that pretty much all you
have to do to get better at something is
continue to do it whatever it might be
tennis learning math if you do it you
get better at it most of the time and
this is a key challenge for theory i'm
sure it drives many of us in this room
is to understand how human level
learning arises you know from changes in
synaptic strengths and linking this to
behavior is a key key challenge i think
actually a key difficulty is the brains
death and by that i mean layered
anatomical structure and computationally
we know that depth makes learning
difficult so anatomically in the object
recognition pathway for example you have
inputs the coming from the I they go to
the retina to the LG N and then up
through v1 v2 v4 and I T so you have
this succession of stages in the anatomy
maybe order five of them and of course
if you were to zoom in with in any one
of these regions you'd also see that
there are inter laminar structure within
that layer and so you can have another
two to three layers within each area
perhaps so overall you're talking order
10 layers in this object recognition
pathway there's also evidence for depth
just from the physiology if you record
we've heard about this several times at
this point if you record from different
layers of the visual hierarchy you see
simple responses the beginning layers
and you see complex responses in the
final layers and that's suggestive of
this
sort of staged computation and
computationally it won't surprise people
here to know that deep learning is
difficult so here I'm showing artificial
5 layer Network trained on a handwritten
digit task and you can see it sort of
stays not learning very well at all at
the beginning before it sort of
nonlinearly bumps down to zero error
eventually so what I'd like to suggest
in the talk is to the extent that you
agree that the brain is layered
structure to the extent that you think
that deep learning is hard then you
should be wondering how depth impacts
learning in the brain ok and what I'm
going to try to do here is identify
hallmarks of deep learning so this
should be an empirical question right
does depth impact learning in the brain
to know that we need to understand how
depth changes learning dynamics what
could you measure that would convince
you that depth is a relevant factor here
so really we need a theory of deep
learning that explains the impact of
depth on learning dynamics so that the
bulk of this talk is going to be an
attempt to give such a theory for a
simplified model class and then that
will be sort of general points so I'll
specialized it and show that you can
specialize it to specific experimental
paradigms to give very specific
predictions for individual experiments
okay so what do we want from a theory
well we want to know things like how
depth impacts learning dynamics learning
speed and learn representations for
instance and our strategy I'm going to
take is to use a very simple model class
I think particularly for the brain
sciences this is crucial you need a
simple model class that you can get
unambiguous predictions from one that
will isolate the contribution of depth
so that you know that if I find
something that was due only due to depth
and nothing else so here's what I'm
going to do well here's the deep network
takes its input in on the right over
there and it passes through nonlinear
layers and of course it's nonlinearities
to make it hard so I'm going to do
something drastic and just take out all
the nonlinearities going to look at a
deep linear neural network so what is
this thing well it's just produces its
output by multiplying a bunch of weight
matrices times it simple
and you could of course always rewrite
that as just a single linear matrix W
total so here this this model class is
completely controlling for
representational power you add layers
you're getting nothing in additional
computational power sits just isolating
the impact of depth itself which is this
layered factorization into matrices
you're probably thinking okay but I
don't really care how learning happens
in that because surely that's trivial
right but I'd like to convince you
otherwise so these plots that i showed
you are actually from deep linear
networks so you can see they have these
plateaus and sudden transitions
nonlinear learning trajectories they
also show interesting behavior like
faster convergence from pre-trained
initial conditions and so the hope is
that if we can understand exactly where
this comes from the linear case it will
give us better intuitions for the
nonlinear case okay so why aren't they
trivial well it's a question of okay
it's a question of what is linear it's
the input output map that is linear all
right but the error function is
nonlinear so when you're doing learning
you're plugging this linear input output
map into nonlinear error function and
the minimization problem that you're
trying to solve is non convex actually
provided that you have a deep network
and so the gradient descent dynamics
that arise from this are non linear and
coupled if you look at that equation
you'll see there's cubic interactions in
the weights and another interesting
property of these gradient descent
equations is that the only way that the
training data enters is to the input
correlations and the input-output
correlations so our central goal here is
to solve these equations to reveal the
full dynamic trajectory of learning as a
function of the input and input/output
correlations and for different depths so
deep linear networks are not new there
was a lot of has been a lot of work on
them so for instance the critical points
were worked out by Baldy in Hornick in
1989 shallow dynamics have been studied
three layer dynamics have been studied
these are all fascinating papers and I
suggest you you have a look but the
dynamics analyses
don't generalize to deeper networks so
we need a new approach that will take us
to deeper deeper networks and the way
that we're going to do this is via an
SVD change of variables okay so
essentially what we have here the world
on the top is giving us this input
output correlation matrix that specifies
the the data set we're going to
decompose that in the product of three
matrices where it's going to the SVD is
breaking it down into a series of modes
each mode contains three things it
contains an input singular vector which
is determining which inputs are
participating in this mode and it links
that to an output singular vector which
is determining which features sort of
are mapped on the output side and then
the strength of that interconnection is
the singular value which is on the
diagonal along that matrix so when we do
this SVD change of variables essentially
we're analyzing the networks learning
dynamics in this reference frame and so
what's going to be changing is effective
singular values denoted by a here and so
we're going to be tracking how that
changes over time so just to to put it
in equations the network is doing in a
time-dependent SVD and what we're going
to be looking at is the coefficients of
that time dependent SVD okay so what's
the intuition behind what's going on
here well essentially we're pulling out
these hidden 1d neural networks okay so
they're all mixed together in the
standard deep network but we have just
the right change of variables and
initial conditions to pull apart this
into a set of completely independent 1d
networks and by one deny recommen
there's just one neuron at each layer
okay so if we look at this look at one
of these 1d networks what does it look
like well it basically it's taking the
projection of one of these singular
vectors onto its input and it's
multiplying it by a bunch of scalars so
the bees are now scalars indicating the
layer strength in in each of those 1d
chains and then you're projecting it
onto the output and if you multiply all
those bees together that brings you to
the overall effective singular value
strength so again just to show what
learning looks like in this setting play
this video
and you'll see that the singular values
pop up over time in the network and it's
excessively refines the network's input
output map until it's fully learned the
input output correlations okay so now
that we have this concept of mode we can
look in more detail about how a deep
network learns let's start with the
error surface so the error surface is
given by this for one mode of course you
can be summing across all the modes but
we just look at just one of them and if
I additionally assume that your layer
strengths are roughly equal to start
then I can just plot for you the error
surface of these networks as I increase
the depth and here they are so some
things to note if you look at the
shallow network it's just this nice
parabola its convex normally has one
global minimum but then as you increase
the depth you see this shift where now
it's non convex okay and as far as the
fixed points go again this is known on
just translating it into these this
formalism the fixed points are scaling
back as you increase the depth that
that's the those are the global minima
but then there's also this interesting
saddle point up here which corresponds
to all of the layer strengths being zero
and there's a plateau next to the saddle
point and that's only present for deeper
networks so as long as you have one
hidden layer or more and this is the
structure of the error surface so now if
we're moving to the dynamics I'm going
to plot here the great ascent dynamics
imma just imagine dropping balls on the
surface that are going to roll down but
they have no momentum and on this axis
I'm on the the lower plot I'm going to
plot the effective overall singular
value strength and what you'll see is
that as they they come in it's the
shallower networks that learn more
quickly in the deeper networks the
deeper networks are stuck on that
plateau and eventually they expel so
already we have one fundamental
distinction between shallow and deep
learning shallow learning shows roughly
exponential approach behavior on each of
these modes whereas deep learning shows
fundamentally sigmoid behavior it can
look like nothing is happening for quite
a while and then
all of the sudden you get a roughly
stage like transition to fully learning
the mode and in red here so I have been
introducing some small technical
approximations in red i'm showing
simulations full networks from random
weights just to show you that these
analytic descriptions are due accurately
represent that case okay so that's great
can we actually get the analytic form of
that curve the answer is yes so here's
the differential equation describing how
these effective saying their values
evolve you notice that depth isn't what
is one of the key terms there so for
different depths we get different
differential equations describing how
the effective seeing their value changes
I just want to pull out three special
cases in shallow network as I said you
have exponential approach in a minimally
deep network you already have a sigmoid
elope in a very deep network the limit
is depth goes to infinity now you have
this form that involves the lambert w
function just to show you what those
look like they're they are so you can
see the switch from exponential approach
to a sigmoid okay so once you have an
analytic trajectory that describes the
entire time course of learning you
basically ask whatever question you like
about the network and come up with with
a crisp answer so for example how long
does it take to learn a mode well each
mode is learned in time 1 over s where s
is the size of the singular value so
it's very simple idea that the stronger
a correlation exists in the environment
the faster you learn but we can also ask
much more central questions for example
how does deep learning speed scale with
depth this is a critical question if I
train a three layer network and then I
train a hundred layer network on the
same task how much slower is that
hundred layer network ok so to get at
this we also need to understand the
optimal learning rate scaling with depth
because you can't just keep it fixed so
you can do that by looking at the
Hessian and so it turns out a skills is
one over the depth ok so when you put
that into the equations what you get is
this this result for the time difference
the time it takes to learn a very very
deep network
compared to the time it takes to learn
shallow network is order 1 over the
initial mode strength raised to the
depth of the network so this is a
fundamental prediction actually of the
theory and it's it's quite remarkable
because what it's saying is deep
learning does not need to be slow
necessarily it depends critically on
your initial condition it will be
exponentially slow if your initial layer
strength is small but it will only be a
constant factor slower if the layer
strength is not small and the intuition
here if you look at the point above one
you can see that all the curves are
swirling around that point at one
essentially if you have an initial layer
strength that starts at one you're
always beyond the edge of the plateau so
descent will be easy whereas if you have
anything to the left like point five you
can see that as you increase the depth
you're getting more and more stuck on
the plateau and mathematically the
intuition is the gradient norm is
actually growing with depth but the
learning rate is decaying you multiply
the two in its constant speed okay so
this is a prediction but we have to go
test it so we looked at deep linear
networks learning the M missed a
classification task and here's the the
scaling of learning time with depth and
you can see that the deep networks are
slower but it's saturating it's not
exponentially slower this actually
appears to be only a finite amount
slower so this is giving us some
traction on what initializations do
right if you initialize with small
random weights then you're initializing
with a layer strength that's small and
so the training speed is going to be
very slow its going to scale
exponentially with depth however if you
use a strategy like on supervised
pre-training say with auto encoders that
corresponds to learning orthogonal
matrices in this case and the initial
layer strengths will be approximately
one and so training speed is now only a
constant time and there I'm talking
about the just the fine-tuning time so
you also have to deal with the
pre-training time so so far this has
just been an attempt to understand
learning dynamics in a deep network but
often when you try to understand
something you end up figuring out how to
control it
we coming up with engineering
contribution as well and so from this we
thought well why do the pre-training why
not just initialize straight to the
orthogonal initialization the theory
predicts that that should have depth
independent training times so don't
bother with pre-training just jump
straight to an orthogonal matrix is your
initialization and if you do that indeed
you do see these depth independent
training times this again for deep
linear networks where the red curve
there shows that these orthogonal
initializations it doesn't matter how
deep it is there there's always very
fast okay so to summarize the theories
predictions about the effective
initializations small random weights
will scale exponentially with depth it's
very bad pre-training and fine tuning
skills linearly because you have to pre
train d networks but orthogonal
initialization is potentially depth
independent all right but to really cash
this out in a engineering context you
need to be able to extend it non linear
networks and the right thing to do is to
add in a scale factor so it's a scaled
orthogonal initialization with a gain is
going to counteract the effect of
non-linearity and we have a way of doing
a prediction for that non-linearity so
essentially you tell me the activation
function you just turn the crank and it
tells you the right game to use and so
now we can train potentially much much
deeper networks in a quite a quick
fashion so here's for instance 10 h
networks trained on the on the this is
the left you can see the red curve there
is the scaled orthogonal initializations
and it hits zero training error around
at bak two hundred and on the right as a
depth 100 network it's quite deep but it
still is hitting zero training error on
that same at bak two hundred and this is
just showing that indeed these
orthogonal initializations have
approximately depth independent training
times and it's better than using
gaussian random initializations and
finally the prediction our theoretical
prediction for the optimal g as you
increase it up the network the
empirically derived optimal g is indeed
converging to our theoretical
prediction alright so that's about
learning dynamics last thing we can look
at is deep representations how our
inputs transformed across a deep network
and this has been widely studied and
nonlinear nets and the nonlinear case is
admittedly much more interesting in this
context but but we still say something
about it so here's an example of how
it's been studied you show an image to
both an artificial network and a brain
and you record from the brain you record
activities in the artificial neural
network and then you make comparison
between these two it's very interesting
work but what happens in a deep linear
neural network well here's the intuition
if you have an input that has high
variance it's a high variance input
direction but it shouldn't link up to
the output so it's not relevant to your
task at hand then in the hidden layers
of that neural network the presence of
that input is going to exponentially
decay away in the hidden representations
conversely if you have a small variation
in the input that you need to boost to
get to the output then it's going to
exponentially increase across the depth
of network this is a single network
looking at how the representations
change across laters so that's for one
mode and of course the actual network is
gonna be mixture of that so you have a
mixture of modes and there some are
amplifying some are decaying and so over
the course this deep network you can
look at the changing correlation
structure of its hidden representations
and it's going to shift from a more
perceptual set of correlations to more
task based set of correlations by the
end of learning because you get this
progression and here's just quantifying
that at the beginning you're very close
to a correlation matrix is just the
perceptual correlations and by the end
you're very close to a perceptual to the
correlation matrix is just governed by
the task correlations and this is
consistent is broadly consistent with a
recent experimental results so this is
showing the black bars here are sort of
a perceptual Gabor model showing how
much variance that explains in the
across the cortical depth whereas the
white bars here are
higher conceptual distinction between
animate and inanimate items you can see
that it's sort of exponentially
increasing across depth okay so some
hallmarks of depth that we've identified
well it seems like at least in the deep
linear case saddle points are the the
the trouble that can slow down learning
and you because of those you get these
sigmoid ille irregular learning
trajectories and we've seen that there's
a very strong effective initial
knowledge depends very sensitively on
exactly how this network has been set up
and as far as the internal structure
goes if you look across hidden
representations you can see a
transformation from more perceptual
representation to a more task oriented
representation and just one caveat while
I'm discussing this is that we're only
analyzing training error so there's
probably a lot more to be said about
generalization error so that's just
where we are now so I just want to
mention that I haven't been talking
about training a generalization error
anywhere okay um so this is deep linear
network you should ask if any if it's
even remotely similar to what you see
and nonlinear networks I'm not going to
talk much about this but just note that
there is recent theoretical work
suggesting that the conceptual picture
I've just painted for you actually does
transfer quite well to the nonlinear
networks but everything I've been
talking about is is quite a course
prediction right sigmoid a learning
curve snotting something that you could
really go and test with experiments but
the theory I think provides the tools
which now you can go specialized to a
specific experiment and I'm going to
take perceptual learning as an example
okay perceptual learning is just the
observation that if you practice and ask
for example this visual discrimination
task you you get better over time all
right so in the standard experiments you
have a monkey focus at a fixation point
and then you show it this sort of
rotated Gabor filter and it's a visual
field and you just ask it was that
clockwise or counterclockwise it's about
the simplest thing you could extract but
as the monkey practices this gets
better and better and actually get very
very goods it's very fine small
orientation differences that it's
detecting and this has been very heavily
studied empirically and the the sort of
the broad conclusion from many papers is
that you see larger changes in higher
areas like v4 and IT as compared to
lower areas like v1 and many people have
found this surprising because if you
think v1 is orientation selective than
the most orientation selective of those
areas this is a fine orientation
discrimination task so many people have
thought well this should really be
what's the v1 would be the site of the
most plasticity but does not seem to be
the case so still a relevant question
today is what determines the
distribution of changes across this
cortical hierarchy all right how will we
build a deep network theory of this i'm
just going to instantiate three basic
assumptions the first is deep chain like
structure the second is stronger
orientation tuning in lower layers
that's an assumption about the
initialization right which we've seen
can be very important and then the third
is great into sent printing and so a
minimal model will look something like
this we have an input which is one of
the two angles that you're being
presented that's processed by a layer of
orientation tuned neurons in the input
layer and then we have a decision layer
which I'm going to initially start off
at zero so it's completely untuned so
this is sort of an extreme version if we
have an orientation tuned input layer
and we have absolutely no tuning in the
in the higher layer and just to note
that these bell curve tuning curves
they're not gaussians they're not
parameterised you could put in whatever
you like there and the model will give
your predictions and when gradient
descent is acting it can change any
point on this curve so it's not
parameterised and yet right so even
though we have all this flexibility in
the model the simplicity of the task
means that we can still get an exact
reduction okay so we can take the full
gradient descent dynamics but reduce
them to just two scalar variables here's
how it works your input matrix is going
to be
original bell curve tuning but now plus
a term which is basically this that red
squiggle it's the difference between the
two population responses and that's can
be scaled up over time by this alpha
scalar and the weights in the second
layer also scaling up Cepeda so it just
happens to be the case that this
reduction works you plug it into the
equations in check and so we we end up
with these reduced dynamics so again
these scalar equations are describing
the trajectory of every single weight in
that entire original network so now that
we have these we can ask what changes
within a layer and how to changes
compared across layers and that's really
what this theory is buying us okay so
within a layer the prediction is the
most informative neurons changed most
this has been looked at in detail
experimentally so what I'm showing there
is data from a monkey's where they found
that the neurons were the highest slope
which means they're most informative
increase their slope a little bit more
when they were trained so it's the most
informative neurons are changing most
and that's the prediction in this deeply
neural network it's also the prediction
of many previous shallow models but now
we can look across layers and hear
something interesting happen it's
actually the exact flip it's the weakest
layer that changes the most now so in
green we have the decision layer and you
can see that it changes more and it also
changes earlier than the input layer all
right so that's a two layer case but you
might ask does this generalize and
deeper networks and answer is basically
yes so now we have these layer wise mode
strengths and if they're ordered in some
way so one is bigger than another's
bigger than another if you reverse that
order that is the order of the sizes of
the changes okay so there's this
fundamental idea that it's the weakest
layer that's changing the most and so to
the extent that you think higher layers
or less orientation tuned in this
context you should think they're going
to be changing more not less the
intuition here is just the deep network
behaves like a chain if at any link of
that chain you drop away the signal it's
gone and it can't
come back so you have to change the
weakest link and experimentally there's
lots of evidence for this reverse
hierarchy of improvements but I'm just
going to show a one example so here
you're looking at the that what you're
which you should be looking at is the
increase in slopes this is before and
after training and so the increase in
slopes in these lower two lines is in v4
there's a twenty-eight percent
improvement whereas in v1 there's only a
seven percent improvement so essentially
we have these two fundamental topologies
parallel structure and serial structure
in parallel structure you have the most
informative neuron changing the most but
in serial deep structure it's behaving
like a chain and it's actually the
weakest layer that needs to change the
most okay and finally a key focus in
experimental work is spatial transfer so
suppose I trained you one position but
then test you at another position they
found beautiful patterns for how you
will transfer to these these different
locations so what I'm going to do is
take that minimal model but now
replicate it at em different locations
and then I'm going to add in some
spatial pooling xand I have a v4 that
has fixed spatial pooling according to
some bell curve tuning and that this
I've structured it in a way so that the
dynamics are no different but now we can
ask how well you will transfer to
different locations so here's an example
the blue curve showing your performance
improvement at a train location green
curve is showing your performance
improvement some distance away and
general intuition is you're going to
completely lose whatever is at the lower
layer when you transfer and you're going
to transfer some of what's at the higher
layer ok so this theory now gives a rich
set of predictions about how these
things interact so for instance remember
I told you earlier that the decision
layer changes earlier in the input layer
so this immediately suggests that early
learning should transfer better than
late learning because it's the only when
the input layer changes that you're
going to lose that and this has recently
been confirmed experimentally and it
also makes a prediction for task
Precision's
so if you're doing a very high precision
task then you're going to follow these
curves out all the way maybe to that red
line there so the input layer has
changed a lot so you're not going to
transfer very well whereas if you're
doing a low precision task you
essentially get to stop quite early
where most of the change occurred in the
decision there and this has also been
shown in a number of experiments okay
and as always and this is all these
predictions are from completely linear
neural network so we need to go verify
it you can actually get it to work in
the nonlinear network and so we have
done that with a source more typical
deep architecture and qualitatively all
these findings replicate so what I hope
I've convinced you of is the value of
minimal models so in much the same way
that in control theory we studied linear
control theory in electrical engineering
we studied linear circuits that sheds
light on the more complex phenomena and
hopefully we can use these simplified
models to shed light on the more complex
and nonlinear networks I think the key
thing is but you know what do you get
from linear control theory no actual
system is linear right so what do you
get from it and I think what you get is
conceptual structure like the right
conceptual understanding clarifies
things like stability that's a key idea
controllability reach ability things
like that and we want those similar
kinds of concepts that we can import
into our understanding of learning
systems okay so to conclude it's been
observed that learning and a deep chain
like structure is hard and my suggestion
is that overcoming this challenge may be
a key factor shaping learning in the
brain in mind and I presented a theory
that yields through rich predictions
about the effective depth on learning
dynamics but still remains tractable and
I seen one example sort of goes with the
theme of this symposium where just the
desire to understand these systems did
ultimately lead to an engineering
advance with the orthogonal initialized
asians that permit fast training of even
very deep networks but I'm most excited
by these models for their ability to
connect computation neural structures
and behavior so we have all these
ingredients we have their learning they
have a neural link and they are giving
us interesting behaviors and we can now
start to understand this in a variety of
contexts some of which I've talked about
but many of others I haven't had time to
so thank you very much thanks a lot um
I'm for quest so I thank you for the
showing the power of a simplified model
but my question is that that you are
nicest is that on the basis of a
supervised learning with the target
signal given at the top layer so the
question is whether that's the right
framework for brains learn so if you
think about more like unsupervised
learning so how much of your result
carries for such a case yeah it's a
great question so again I view
supervised learning is another
simplification that's clearly not
exactly what's going on in the brain is
probably something more like
reinforcement learning and it will I
think it will be possible to extend
these results to reinforcement learning
it's something i'm currently working on
but you have to get the right framework
to get it all to work out but i think it
will be possible but what about the on
slope was running like in the linear
case little bit some type of pca yeah
yeah so this is why I just sort of
briefly mentioned today there's this
caveat generalization error I didn't
look at it so to me the unsupervised
learning it's definitely in the brain I
actually have some other work comparing
on supervised learning models to
representations that you see in the
brain and I think that's probably
sculpting how you generalize and in that
way it's very important but you know in
the brain the there's this critical
period epoch when animals young where it
does really seem to be statistics driven
but kind of in adulthood it really does
need to be associated with attention or
reward so I would say it's more
you know way to supervise their so these
feed-forward networks don't capture the
fact that in the brain all of the
feed-forward connections have feedback
connections and I wonder if you've
thought about how that would change the
picture yeah I see if I ok right so
depth versus hierarchy you we often say
the brain is hierarchical is depth a
different concept I contend it is so a
hierarchy you're basically saying that
lower levels connect only two levels
above them all right and what you don't
allow is loops back by depth what I mean
is serial sequential stages so I'm
completely fine with allowing feedback
connections backwards going all the way
back down provided there are no level
skipping connections from the input to
the output I think that's the key thing
it's about serial chain like structure
and you also have recurrent connections
all of those things again i started with
feed-forward networks just because that
was the easiest place to start and i'm
presently working on these extensions i
think it is important and another thing
to just throw out there is although i
haven't explicitly included feedback
connections maybe i have implicitly done
that because it's learning by gradient
descent so somehow has some kind of back
propagating signal well why don't you
like level skipping I'm I I don't like
it or dislike it I just think that in
the brain as an empirical matter it
looks like you don't have too much level
skipping so in the fella in Venice and
diagram that AB you do have level
skipping with the average number of
levels you skip is only about 1.8 so it
seems like there still is really this
serial structure yeah so I'm sorry we
have one time for one more question and
maybe we can have them pick me in the
panel oh ok ok so you start off with
random orthogonal weights why don't we
keep them orthogonal why don't you make
the really great question great quad
yeah I'm glad you asked that because I
like this I have to find it though
okay this theory I think explains why
you need all these adjectives on the
pre-training scheme why is it layer wise
why is it unsupervised why is it free
training and this will this will get to
your point so why is it layer wise well
if your pre-trained multiple layers your
back to the deep learning problem so
that doesn't work why is it unsupervised
you can actually analyze supervised
pre-training it doesn't give you a
balanced initialization but now to get
to your question why pre-training well
the formula that I came up with depends
on the initial scaling it doesn't it so
the fundamental quantity there is the
initial scaling so if you regularize
towards your your suggestion force
things to be orthogonal you're
preventing the learning process from D
orthogonal izing when it needs to and so
it will help you if the task requires
forethought no matrices and then it's a
prior you could add but it won't help
you in general and the point is just
having the initial scaling is all you
need to do the learning will be fast
after that thank you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>