<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session: Probabilistic Line Searches for Stochastic Optimization | Coder Coacher - Coaching Coders</title><meta content="Oral Session: Probabilistic Line Searches for Stochastic Optimization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session: Probabilistic Line Searches for Stochastic Optimization</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/heFG1Yye_HA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay well welcome everyone to this next
session we have two papers in this
session they'll both be 15 minutes each
and we're going to carry on sort of the
first part of this theme of optimization
but from a very different kind of view
so for the first talk we're going to
hear about probabilistic line searches
from marin mazzie she from the from the
tubing and NPI thank you very much right
so the general setting is a nonlinear
optimization we want to find a local
minimum X star of a potentially very
high dimensional function f of X and now
typically an optimizer would keep around
an estimate X I of that local minimum
and then update it iterative ly by
greedily descending so at every step the
optimizer chooses a search direction s
in which it wants to step and then
conditioned on that search direction it
adjusts a step size T which defines the
length of the step so there is a lot of
practical work going into finding good
search directions but it is equally
important to also adjust good step sizes
because they crucially affect
optimization performance as well so for
example in deep learning these steps
eyes are called learning rates I'm now
in deterministic or noise free
optimization there is a well-known
paradigm that automatically adjusts
these step sizes and they're called line
searches um so we will quickly skim over
a typical workflow of such a line search
um every line search starts with an
initial function and gradient evaluation
at the current position of the optimizer
so the top plot shows function values
the second blood gradients and initial
evaluation
is already plotted on the left and on
the x-axis there's the step size so the
line search now tries to find a good or
appropriate point on the positive x-axis
now acceptability our appropriate point
is encoded in two inequalities called
wolf conditions they are written on the
bottom of the slide and I'm Dame 180k so
the first row of condition on the bottom
left label w1 and imposes linear decay
on the function values and the second
Wolfe condition on the bottom right
existed two versions a weekend a strong
one and the weak version only accepts
points which gradients are larger than
the initial gradient and strong version
only accepts points when the gradients
are flattered and the initial gradient
so in the plot um the weak and strong
versions of the Wolfe condition or the
acceptable regions of the weak and
strong wolf conditions are shown as
light into our green shaded areas so if
a point lies inside of the green shaded
area then it will be accepted by the
light search all right so now our
typical workflow is some quite
straightforward first the line search
proposes a point for example at this
vertical red line then it evaluates the
point there and it checks on the wolf
conditions so this point does not
fulfill the wolf conditions but the
gradient is still very negative so it
excludes like the line search excludes
the whole search space to the left of
this point and it's looking even further
to the right at this vertical red line
then it evaluates the function again and
this point also does not fulfill the
wolf condition but this time the prime
gradient is positive so it excludes the
whole search space to the right of it
and makes the cubic interpolation in
between the last two points
so the next candidate point would be at
a local minimum of this cubic
interpolation the line search evaluates
the function again and there we go on
this point indeed fulfills the wolf
condition and would be accepted by the
line search so this is also the point
now where the optimizer is going to step
2 next all right so in summary many
classic line searches model the
one-dimensional objective with a cubic
polynomial arm the search for candidates
is done by excluding complete regions
and collapsing the search space and
points are only accepted if they fulfill
the wolf conditions this works really
well for noise free deterministic
optimization problems but it fails as
soon as we introduce the cast icity many
machine learning problems though are
noisy for example if we have an
exchangeable loss function which is a
large sum and we approximated by a
smaller sum so this is actually the
thing that I'm ask you as the largest
talk about exactly that loss function I
know this estimator for our loss
function and it's derivative is
approximately Gaussian distributed now
with this knowledge we can try to extend
the line search paradigm to the noisy or
probabilistic setting a do 2d got unity
we're going to assume a Gaussian process
a surrogate for our one-dimensional
objective as search for candidates
instead of excluding regions and
collapsing the search space we will
leave the whole search space open and do
on Beijing optimization and third as
acceptance criterion instead of having
binary wolf conditions we are going to
have a probability that the wolf
conditions are fulfilled
so these three steps in more detail the
gaussian process we're using is an
integrated wiener process this is a very
nice colonel it is robust and flexible
and the posterior mean are piecewise
cubic splines which have analytic minima
and this is a feature that we are going
to use as well in the next step so for
the search of candidates we do Beijing
optimization and we use a well-known
utility function called the expected
improvement now it's self optimizing the
expected improvement globally we will
only evaluate it at a few well-chosen
candidate points which are the analytic
minimum of the posterior mean and one
extrapolation point and then we are
going to choose in between these all
right now as acceptance criterion the
important observation here to make is
that the week wolf conditions are in
fact positivity constraints on two
variables a and B which are linear
combinations of function values and
gradients now since we implied or impose
a Gaussian process on function values
and gradients this map implies on at
each point t a bivariate gaussian
distribution over a and B now for the
week wolf conditions to hold a and B
must be positive so the probability for
the week wolf conditions to hold is the
integral over this probability density
over the positive quadrant all right
there is also a straightforward
approximation to the strong wolf
conditions and it basically does exactly
the same computation but it exchanges
one limit of the integral
alright so with this machinery at hand
we can now look again at the typical
workflow this time of a probabilistic
line search it also starts with an
initial function and gradient evaluation
at the current position of the optimizer
and the top lot shows the marginal
belief over function values and a bottom
plot marginal beliefs or gradients and
again the initial evaluation is already
on plotted um now the lancers proposes
and initial candidate it evaluates the
function there and then it updates the
goshen closes and now at the bottom you
can see an orange Gauss blob and this
red box and the skulls blob is the
probability density for the wolf
conditions so we will accept a point if
at least on thirty percent of this
probability mass lies inside of the
green shaded area so this point clearly
does not fulfill the wolf conditions so
it will be rejected for now the line
search proposes another candidate point
it evaluates the function there then it
updates the GP and again it checks the
wolf conditions so also the Gulf blob of
the blue point does not lie sufficiently
inside the green shade area so it will
also be rejected for now the next for
the next candidate points we in fact
have two of them one of them being the
local minimum in between the red and the
blue point and the other one being one
more extrapolation point so now we want
to choose in between them where we want
to evaluate next and this is done by the
solid dark blue line and the solid dark
blue line is the product of the expected
provement at every point times the
probability that the wolf conditions are
fulfilled at every point
and in fact we don't compute this blue
line at every point we just computed at
the positions of the candidates and then
we choose the one with the highest value
so for our candidates here the one on
the Left wins so we evaluate there and
we update the GP again and now we accept
the green point because about seventy
percent of the wolf probability density
lies inside of the green shaded area
alright so we tested the line search on
two different architectures of a two
layer your network and combination with
stochastic gradient descent on one
network being on cipher 10 which is on
the left side of the slide and the other
one being on em nest which is on the
right side of the slide now top let's
show test error versus initial learning
rate r410a parks and so this basically
performance and the lower is better and
now every plot shows two curves I'm the
light gray one with X markers is Sgt
with a fixed learning rate and the dark
gray horizontal one is STD governed by
the line search um well there is the
qualitative difference in between the
two crafts and the performance of SGD
with the fixed learning rate is
dependent on the choice of this learning
rate and the STD or the performance of
STD or in combination with the line
search is not so for different choices
of initial learning rates the line
search controls these learning rates and
yield similar good results now the
bottom plots show test error versus
Deepak so this is an evolution of
performance and time
and the left plus respectively show on
sh t instances with fixed learning rate
different colors are different learning
rates and again they vary in a high
range and our highly de performance is
highly dependent on a choice of it while
under and right plots respectively the
instances governed by the line search
but will up very nicely and independent
of the choice of the initial learning
rate all yield comparable results
alright so um there's a lot more to say
about this and a lot more theory to talk
about if you like please meet us at
poster number fourteen this evening and
thank you very much so if there are any
questions then maybe we can just get
ready and wave your hands but I'll
actually start with the first question
actually so we choose the cubic
interference spline because it has is
good analytic properties that make the
whole algorithm linear linear complexity
effectively does it so use this veena
carnal that because it makes a order n
00 n complexity algorithm so that's what
computational reasons are there other
reasons it would you would use that that
give you particular robustness to
outlines in the gradient estimators or
other kinds of kernels that we might
want to use yeah so we don't use the
kernel for the complexity reason because
if you think of complexity reasons I'm
dtp with using a few the complexity just
does not matter in this case because our
GP is very tiny so we are just inverting
a 4 by 4 or 6 by 6 matrix and this is
virtually on
it's very inexpensive in comparison to
actually validating a gradient and so
this is not a reason the reason is
because really that this convo has some
very nice properties in comparison to
our for our purposes very nice
properties um and just also pronounced
straightforward extension to what a
classical I've searched does anyone else
what's the ask a quick question ok let's
dive Marin again
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>