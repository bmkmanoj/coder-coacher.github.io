<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>BDA - Querying Big, Dynamic, Distributed Data | Coder Coacher - Coaching Coders</title><meta content="BDA - Querying Big, Dynamic, Distributed Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>BDA - Querying Big, Dynamic, Distributed Data</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1a6pKqoXHZA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay so so this talk would be about
quitting big dynamic distributed data
and it's certainly going to be about
strings but stimulation sort of a
different context that the one we've
seen so far so strains in a distributed
system and this is going to be this talk
will be about work that we've done in
the context of the lift project so lift
stands for local inference in massively
distributed systems and it's a fête
open project that's actually winding
down at the end of this year and of
course you know a lot of things i'll
talk about are not just my work but also
work done by a cast of folks and i'm
just some you know giving you some some
some important names here okay so you
know again I I don't i'll go over these
first slides very quickly i don't think
i need to convince you that you know big
data is big news and big business let's
hit the popular press you know everybody
is talking about big data and the idea
that you know we're really gathering so
much data with all these new information
gathering technologies and of course
this raises a lot of questions about how
to effectively manage and analyze all
this data right and so when you hear
about you know the challenges that big
data imposes you know you hear you often
hear about these four beats and you know
i think all of the almost all of the
talks that we've seen in this meeting
have actually touched upon some of these
fees okay so the four vs first of all
volume also scaling from from terabytes
to go and going to exabytes or
zettabytes okay so we're going to really
closing in on the exabyte era and I
think the seta bite there is not that
far the second way is velocity you know
processing so big data is really going
to be static you know just sitting there
some database along us to do multiple
passes so big data is dynamic which
means that we really need to deal with
massive amount of streaming data the
third way is variety and you know I
think a lot of talk starts upon the
squareheads talk surajit stock you know
we really cannot assume that our data is
just very nicely structured lying in
some relational database we need to deal
with many many different types of data
but with varying levels of statute in
the data ranging from text to fully
structured relational data and something
that wasn't mentioned so much but I
think certainly a gearhead stock touched
upon was with this issue of veracity ok
so we're acid it basically means that
when you have big data you're going to
have inevitably a lot of noise so you
need techniques that are able to deal
with you know the noise and the
uncertainty that lies in the deck ok so
if when you do information extraction
that's an inherently uncertain process
you know anything matching you know
obviously you need to some effective way
of dealing with this answered now I'll
go with this four piece I would like to
propose a another dimension a fifth
dimension that I think is very important
and this often overlooked and that's the
dimension of distribution so big data is
really going to be static it's very
often going to be actually distributed
in widely distributed systems you know
many many data centers possibly around
the world and it's going to be in many
situations going to be impossible due to
communication constraints or to other
constraints like administrative
constraints of privacy concerns you
actually centralize all this data in a
single data set ok so you somehow need
to deal with the distributed nature of
big data as well okay and basically this
is exactly what the lift project is all
about it's basically focusing on the
volume velocity and distribution ok so
basically we want to deal with massive
distributed data streams effectively
okay so streams I guess again I don't
need to give you too much context here I
think the basic ideas you know you have
all these new technologies that you know
continuously generate data you need to
sort of have continued squeeze over this
data that continuously monitor the
incoming streams for different patterns
or events of interest ok so this sort of
continuous creating of dreams are sort
of a well-known paradigm and the way
that this is modeled typically in so the
stream the streaming algorithmics world
is something looks after something like
this so you have sort of this stream
processing engine over here that's
observing these continuous data streams
which can be you know petabytes or
exabytes of data and of course you
cannot afford to store all these data so
what you do is you build these dreams
synopsis that you keep in memory which
are orders of magnitude smaller than
your original
streams and then when a query comes in
or a function that you want to compute
over this thing basically what you do is
you go to the synopsis and you give an
approximate answer hopefully with some
guarantees on the approximation error
okay of course a surge it also mentioned
this morning your approximate answers
are orphans sufficient for many of these
problems when you're looking for
Elephants when you're looking for trends
are normal ease you're right you don't
really need answers that are precise to
the last decimal okay so clearly a lot
of the smarts here needs has
concentrated on building these dream
synopsis and the problem here is
building the synopsis you know module
given all the constraints that you have
in the streaming mode so this means that
basically you need synopsis that work in
a single pass so you have to build these
summaries you know with one pass over
the data you cannot go back and forth
you need synopsis of our small space so
orders of magnitude smaller than the
streams typically your space
requirements need to be logarithmic or
poly logarithmic in the stream size they
need to be in additional small space you
also need small time okay when the
stream is really rapid you really want
to be able to update the synopsis very
quickly so the paracord processing time
has to be very low okay and another
thing that's also very important is to
have these structures that are delete
proof basically you need so in the very
genetics remodel you can have not only a
pens and inserts you can also have
delete okay so you need synopsis that
can gracefully deal with the leads in
this dream and so something that's also
very important and you know that's
something that also then the next talk
will focus on is this idea of merge
ability or composability and this is
very important where you apply these
dreams of a distributed system that you
can build the synopsis this synopsis in
an independent way in a different size
of your system and then at the end of
the day put them together and get a
global view of the overall state okay
okay so just just to make sure that we
understand where I'm talking about you
know when we talk about the stream so
the model that I'm assuming is basically
a relational stream is basically just a
very very long dynamic array okay and
you can think of this array essentially
as the frequency distribution vector of
you know your relation your favorite
relation of course the relation doesn't
have to be a one dimensional it can be
multi-dimensional but you just need some
way of server unfold
the dimensions on the line you know row
major column major pick your favorite
but the basic idea now is that you have
this very very long frequency
distribution vector and basically when
you have inserts and deletes the
components of the vector which are the
counts for a given double combination or
going up and down okay so basically the
you're basically observing is very very
long signal that's implicitly rendered
through a stream of updates and then
update basically says you know go to
location X of my array and add see to it
okay so X is basically the combination
of attributes for the incoming topple so
which basically is the index in this
array and c is basically the insert or
delete so if i have an insert it's a
plus one if I have a delete it's a minus
one or you can have bulk insert cindy's
okay so this is basically relational
strip rights it's a very very very long
vector with dynamic components okay
thats components going up and down and
what's the goal of these old is trimming
algorithms our goal is to basically
compute queries or functions over this
very long vector using space and time
that is much much more than n okay so
how can we compute interesting functions
in the same queries over this vector
using space and time that's much smaller
than that required binding target so a
lot of work has concentrated on this on
this model sort of the previous model
that I talked about well now what I will
argue is that you know really this this
model is not this sort of centralized
model is not very sufficient okay
because basically in many many stream
processing applications so for instance
these large-scale event monitoring
applications that everybody is talking
about like sensor net monitoring your
power grid monitoring and so on you
rarely get this sort of complete vision
of the entire stream okay so typically
what you have is something that looks
like this okay so you have these sites
that are sort of can be distributed in
the wide area and each one of these
sites can actually be observing its own
local stream so think of these are sort
of the monitors that you incorporated in
your network monitoring infrastructure
and the coordinator is basically the NOC
the network operations center that's
trying to figure out what's going on in
the network okay so basically each one
of these M sites is observing its own
local a rapid massive volume data stream
and the
coordinator wants to monitor a function
or equity over the union of all these
dreams okay so clearly this is a very
simplistic structure you can have sort
of a multi-level hierarchy you can have
a sort of a peer-to-peer network or you
know your favorite distributed
architecture but for time being let's
just concentrate on this okay so our
goal is to continuously track a global
query over the union of the streams
observed of the sides and as you can see
here we're introducing additional
dimension so there's always the knife
solution that says you know I'm just
going to push all the data up the
coordinator and turn this into a
centralized problem but we really want
to avoid that because of all the issues
that I mentioned so really you know in
the early so you really want the
distribute solution that doesn't really
want to centralize all the data this
centralized the streaming data all the
time because of data volume because of
various constraints and so on so
compared to the earlier model now we
introduce an additional an additional
constraint which is basically that of
communication so we really want
solutions that are not only space and
time efficient but they're also they
also need to be communication efficient
okay so in this distributed them in
context communication starts playing a
major role okay and what are examples of
quiz that we might want to monitor over
the set up things like for instance join
aggregates variants entropy you know
complex functions over these unit the
union of all these dreams okay so this
is basically are going to be our setup
okay and if you think about it it seems
very hard to really say anything
interesting right so if I want to really
tackle value of the function
continuously as the streams are
continuously changing it seems that the
only way I want I can do this is
basically by pushing all the data up to
the coordinator okay well if you sort of
monitor the class of problems that you
you're focusing on it turns out that you
can be much smarter and basically that's
what we'll do so we'll focus on two
classes of monitoring problems first of
all what we call a threshold crossing
problem so we're not here we're not
interested in knowing exactly the value
of the function we just want to know
whether that function has actually
crossed a certain threshold okay and we
want to generate an alarm that that
happens okay and a second and more
general setup is basically we don't care
about the exact value of the function
which is that care of
you know the value of the function
within some guaranteed accuracy bound
epsilon so we don't care about knowing
the exact value of f at all times we
just want to know that it's within some
certain aerobar okay and then that gives
us the opportunity to basically
trade-off accuracy and communication
processing costs okay so clearly the
knife solution is centralized all the
data but this is something that we
really want to avoid you know we don't
want to centralize the data and apply
the known streaming algorithms instead
what we're going to do is wanna design
solutions that sort of process the data
at the edge right so we want to process
as much as we can at the edge what we do
is this this idea of sort of in situ
stream process and the basic idea is we
take this global query and we try to
sort of push it down to local constraint
to save local constraints that we can
install at the site ok so this is the
basic idea so you basically you know
take your global query we try to sort of
map it into safe local constraints safe
meaning that you know as long as none of
the sites speak up then you're
guaranteed to be safe you haven't
crossed that threshold right so and when
something happens something bad happens
then you need to push information to the
coordinate ok so this is a very simple
sort of pictorial representation so you
somehow build these local constraints or
filters that you push down to the site
and then as long as the readings that
you see here within your filters you
don't need to do anything but then once
you get an outlier reading then you need
to push that information the coordinator
and then the coordinator may come back
and say you know you need to adjust your
filters to make sure that the system
again operates within the desirable
range and do something like that ok so
again this is a very very simple view
and and as you can imagine you know the
smarts here comes into how do you build
these filters ok and there are
situations where that are very simple
and the simpler situations are of course
when this F function is linear so if
you're f function is linear like a sum
or you know an average right then things
become easy because what you could do is
you can get some slack with respect to
the threshold for instance and you can
serve distribute that slack across the
sides and you're you know as long as you
with your within your slack you're fine
the problem is what happens when you so
so linearity means additivity and it
it means that we were able to basically
get these filters very easy the problem
now is what happens when you have
complex functions that are not
necessarily in there how can we generate
these filters in an effective way and
this is something that I'll try to cover
today and just to give you a sense of
where we are and where we're going so
basically a sort of try to introduce
this continuous distributed streaming
model that you know I present in the
earlier slides and now i'm going to talk
about the geometric method which is a
very generic method for coming up with
these local constraints for general
functions okay not not necessarily
linear for any any type of function okay
then I will discuss some recent work
that actually combines the geometric
method with sketch summaries and in the
end if I have enough time i'll try to go
over some challenges and complete and
the Lexi in this area conclude the top
okay if you have any questions you know
please feel free to raise your hand okay
so the problem will kind of focus on
just to begin with is basically we want
to figure out whether a non-linear
function or query over a union of
distributed streams so each one of these
streams is basically you can think of it
as a sort of a dynamic vector as we
discussed right so you have a function
over all these vectors and we want to
figure out whether that function
actually crosses a certain threshold
okay if the function is nonlinear as I
said so you can imagine something like
information gain for instance right then
you know coming up with these local
filters is a non-trivial problem and
actually it's not difficult to see that
you know if you have a complex enough
function you can have situations where
you know even though globally the
function may have crossed the threshold
the local values of the function can
actually be anywhere you know below the
threshold above the threshold whatever
you want okay so then this is just a
very simple example okay so how do you
get these save local constraints when
your function is in nonlinear so this is
basically what the heart of the
geometric method okay so let's let's go
over the basic ideas so this is was
actually work that was done by some
partners in the project you know a few
years back and Sigma 2006 and the basic
idea of the method is since we want to
be completely agnostic about the
function that we're monitoring we are
not going to monitor the domain the the
range of the function so we don't care
about the values of the function we're
actually going to monitor the domain
okay we're actually going to monitor the
domain of the function so where the
function is define these streaming
vectors rather than the range of values
of the function okay so this is going to
be the basic idea so let's try to sort
of formalize the problem a bit first of
all we assume each site is tracking a
local statistics factor so each side has
this sort of dynamic local statistics
vector this is the local data stream
that it's monitoring so the local
frequency distribution if you want okay
so site is striking this local
statistics vector V I and the global
statistics vector so the global stream
we're going to define it as a complex
combination of the local statistics
vectors okay so for instance the global
strain we're going to say it's going to
be an average of the local district okay
and this is like a vector average right
so this is again a vector along better
okay so this is basically the problem
will change so the global stream is this
convex combination of the local
straining vectors and the condition we
want to monitor is whether the value of
a function of every goes beyond the
certain inertial okay first of all let's
assume that you know we've done a sink
we've done initial sync so the
coordinator now has an estimate of the
local of every local statistics vector
okay and at any point in time the
coordinator will have an estimate based
on the updates that it's received from
the site and let's say that the latest
update that the coordinator has received
from site I is this VI Prime okay so
this is the latest update of syndrome
said I I and I have an estimate at this
point based on the updates I've seen I'm
the coordinator I have this estimate e
which is basically the convex
combination of all these VIP range okay
so this is my view my current view as a
coordinator of what's going on in the
system okay of course you know by the
when I have received the ipad there was
something in the past of course you know
the site may have drifted right so after
it sent me this VI Prime updated may
have drifted by some Delta VI so again
this is a vector that basically tells me
how much side I has drifted from the
latest update that it sent me okay and
of course I don't know Delta VI as a
coordinator but the side knows that okay
okay so the key observation here is that
if I look at the current global vector
so this is the current global picture of
the string okay it's always going to be
a complex combination of these
translated local drift we're basically I
take these local drifts these Delta V is
so how much its its site has drifted and
I translated by the current estimate
this e okay so just to sort of give you
a picture what this looks like let's say
that we have two dimensions in our
vector okay and we have one two three
four five sites okay and this is the
previous estimate and these are the
drift in two-dimensional space so this
is basically this is basically how far
site one has drifted from the latest
update site 2 and so on okay what does
this tell me it basically tells me even
though you know I may not know exactly
where he is I know that it's always
going to be since it's a complex
combination of these translated local
drifts I know that it's always going to
be somewhere within this convex hull
okay so the basic idea would be to try
to monitor what's going on in this
complex world and whether some point in
this commercial has actually you know
gone beyond the threshold okay the
function value of some point of this
cover charge well the question is how
does this help me because if I if I need
to build this convex hull then I
actually need to collect the Delta V is
then actually need to know exactly the
drifts and I know the global vector the
question is can I can I get sort of a
way to cover this convex hull using just
local constraints and the answer is yes
so basically what we what they showed
was that in any dimensionality if I
the union of these spheres but I
basically center around this translated
local drift okay so basically what I do
is I take these local drift vectors and
I design a sphere around each one okay
then you can show that in any
dimensionality the union of all these
balls the union of all these fears will
always cover the convex flood okay and
of course you know these these spheres
are going to be centered in the middle
of this vector and can have radius half
basically the length of the vector okay
and why is this important because these
fears are now completely local okay so
basically what you can do every site can
actually build its sphere completely
locally and can check that sphere
completely locally and we know that you
know as long as none of the sites speaks
up the convex hull is safe so that's the
basic idea okay now just to formalize
this a little more so basically what we
can do is you can view the sort of this
threshold crossing problem as sort of a
coloring problem of the domain plane
okay so let's say that we have you know
basically the our threshold crossing
problem is going to check whether f of X
grows beyond a certain tau okay we can
view this basically as a coloring of the
space where you have this inadmissible
region here this this red region and the
rest of the region is safe so as long as
my v stage within the the rest of the
region everything is good and of course
this is admissible it this inadmissible
religion can actually be very complex
you can have holes you can depending on
the value f as a function f that you use
it can be arbitrarily complex okay so
the basic thing that we want to do here
is we want to check whether the convex
hull is monochromatic whether it stays
on the right side of the constraint okay
so how do we check that we basically
check whether each sphere is
monochromatic okay so each site will
basically build its own sphere based on
its local drift okay and will
independently check the monochrome icity
of its own sphere okay
none of the sites speak up then we know
we're safe and we can continue how do we
check the monochrome icity basically
what you need to do is solve an
optimization problem within that sphere
so essentially what you're doing here
again you're going to have more
processing done at the edge right to
check these local constraints but maybe
what you need to do is within your
sphere you need to check the maximum and
minimum value of the F function so you
solve a constrained optimization problem
within its sphere and as long as both
these values around the right side of
the constraint you're fine okay and so
this is the bit going to be the basic
idea of course in certain situations
what will happen is you know you can
have some spheres or piercing into the
inadmissible region so at this point
what will happen is side three is going
to speak up because its sphere is no
longer more chromatic and when it's
picks up it's going to send it Delta V
up to the coordinator it's Delta V now
become zero so by definition its sphere
is monochromatic right but by sending
when u Delta V your email have changed
so things may move around so in the
worst case what will happen is every
cell will speak up all the Delta Delta
bees will become zero and you just get a
current reference point which is by
definition monochromatic so the convex
hull which is going to be reduced a
single point okay okay so of course you
can play a lot of games here in terms of
the resolution protocol so if you have a
local violation how do you resolve it
how do you pick sides trying to balance
each other out and so on there are
several different games you can play you
can allocate slugs two different sides
to try to minimize these sort of local
balancing operations but I'm not going
to go into the details here but
hopefully you get them the basic idea
and just to drive the point further you
know that there were some more recent
work that actually showed that you can
actually generalize this idea you can
use ellipsoid instead of spheres try to
take better advantage of the flexibility
that the ellipsoid regions give you you
can actually use any point to design
these ellipsoids other than e you can
actually have a reference point that you
can push away from the boundaries if
you're close to the boundary you don't
need to design your spheres or your
ellipsoids around e you can actually
push your e away from the boundary and
get the reference point that's far and
the covering theorem still falls okay
and in general there's a more it turns
out that this is actually a more general
theory behind this idea so
this is just a very special special
implementation of this safe zone idea
where a safe zone can be in general
defined as a convex sub sub region of
the admissible region I'm not going to
go into these details here because I
want to move on to the next part which
is actually the more recent work that
we've done that talks about how to take
this geometric monitoring idea and
combine it with sketches ok ok ok I'm
going to try I'm going to try to be a
little quick here I think but the basic
idea is this so so far the method
basically assumes that these local data
stains are complete right so I maintain
the full vectors locally well if you
have really massive data streams you can
really cannot afford to do that and you
have to use some kind of summarization
in sketching a th ok of course and in
addition what we want to do is we don't
want to don't care about just threshold
crossing but we want to monitor the
value of a function approximately now
obviously you know wondering the value
of a function you can map it into these
sort of paired thresholds crossing
problems right so you can that's not
difficult to do but you need to account
for the errors as well as the sketching
error so sure if you summarize the
streams you also need to sort of account
for the sketching errors ok so the key
problems that we came up that we have to
deal with in this context its first of
all trying to minimize the data exchange
volume when you're using these sketches
because sketches are of course summaries
but they can still get pretty large and
secondly how do you deal with the
nonlinear nature of the AMS estimate ok
and just to give you a sense of of what
the function return to compute are they
look something like this it's sort of a
generalization of the frequency moment
idea that has been repeated again and
again so let's say that we have you know
two different data streams that are
distributed across perhaps different
sets of sites and we want to monitor the
the size of the inner product of these
two frequency distribution vectors which
is basically the size of the equity on
earth ok and this is a very generic
query class because it captures things
like even ranges you can do heavy
hitters you can do wavelet you can do
all the all different types of things
range queries or integra gets us all
okay and the way we do this typically in
the streaming world is using this idea
of sort of
randomized linear projections which are
basically these AMS sketches so what you
do is you take this very long vector and
you take inner products of that vector
with a random variate and so you get you
know the first random projection next
one you can repeat this with many
different families and you get in
eventually what you get is the sketch
which is basically a much shorter vector
that contains the random projection
random linear projections of the
original vector okay the nice thing
about all the summer is is that the
linear very easy to compute of a stream
you can very easily compose them so
they're measurable you can do just
audition and get the picture of the
whole stream and based on that you can
also compute things like joints okay so
here what I'm showing you is the
estimator the AMS estimator function for
the self job the special case where you
just have one stream and you want to
estimate the self join which is
basically the l2 norm squared right F 2
F 2 norm squared okay so what do you do
here you basically look at your sketch
as a two-dimensional matrix and you have
1 over epsilon square copies of the
sketch in this diamond in this direction
here so you have rows that have size 1
over epsilon square and you have a loved
one over Delta such rose and what you do
is over each row you square the random
projection and you take the average and
then you get load one over Delta
averages and after all what you do is
you take the media that's so this is
basically what this dysfunction is
saying right so the final estimation I'm
going to get for the F 2 squared right
the l2 squared norm for this or the self
to in size is basically going to be the
median of a bunch of raw bridges sketch
and this turns out to be a very very
good estimate okay so if you think about
it this is basically the function that
we want to monitor right so what we want
is you know if we want to monitor the
self join over it distributed stream
this is basically the function right so
your your your your local statistics
vector is going to be your local sketch
and this is the function that you want
to try to monitor to see whether your
function has crossed a certain threshold
or is within certain bounds okay and it
turns out not to be such a trivial
problem first off as I said these
sketches can get large
so and especially what happens is that
these matrices get you know they're
short and they're very very fast right
so this one over epsilon square can
actually get very large if you really
want thai terror guarantees whereas you
know the low bond over delta because of
the look there is actually pretty small
right so this one over epsilon square
causes a lot of problems and the
sketches can get very large which means
of course a lot of communication okay so
what we showed in this paper which again
hopefully will appear in vldb 2013 it's
you know it's undergoing the last round
of revision so hopefully if everything
goes well it will appear in BL to be
this here we actually show that you can
actually reduce this problem to just
looking at the log 1 over delta
dimensions that correspond to the norms
of the rows of the sketch matrix ok so
if you define this their own or if you
define this row nor error vector this di
error vector over here you can actually
show that the AMS estimator can be
bounded by functions of this di vector
and the nice thing is that this di
vector just has all Logano where Delta
dimensions so basically we're just
monitoring the log 1 over delta
dimensional space which is much much
cheaper okay a second technical
component its second technical
difficulty is dealing with the media
nest operator so the median operator is
a highly nonlinear function and we need
to somehow effectively deal with that
and the problem here is remember we have
this optimization problem to solve
within every ball that we get and
actually what we show in the paper is
that there's actually a different way of
thinking about the problem where you can
use a greedy algorithm to actually get
the distance of the ball center to the
inadmissible region to the closest point
in the inhibition region which basically
allows you to tell whether you're bald
is monochromatic or not okay and there's
a fast greedy algorithm not allowed to
do so it's not even that computation
intensive okay now of course taking this
idea and extending it to non-self joints
like general choices can be done it's
non-trivial but it can be done and we
show how it can be done and we actually
compared this technique against the work
that we had earlier with Graham a few
years back showing how to use a ms
catches for monitoring such functions
which was a
very purpose specific solution for these
types of queries and not as general as
the geometric method and we actually
show that the geometric method is much
smarter it actually beats the bitter
earlier algorithm and it beats it by a
lot if you don't consider if you have a
broadcast channel at the coordinator
which allows you to do control of the
remote sites more effectively so for the
last part I had a plan to talk about
some challenges I'm not going to go over
them I just want to say that you know
this this continues disturbing
monitoring area is really picking up
some steam over the past few years and
you know there's a bunch of people who
have worked on different problems in
this continues distribute through model
people are around this room like eben
Graham and milind and others and there
was a recent workshop about again half
go in initial none Japan that talked
about exactly some of these issues and a
lot of the attendance here were also in
that workshop sir that's good to see
that you know the there's some
continuation of interest in this area
and you know again there's many
challenges like you know going to
peer-to-peer systems if you go into
peer-to-peer network it's you know it's
a really hard problem because there you
don't have a notion of a coordinator so
you need some other way of communicating
information around what does it mean for
the system to stabilize and so on
there's also problems in terms of
applying the theater systems theoretical
foundations I mean talked about lower
bounds it's sort of the traditional
streaming model but this in this
continuous distributed streaming model
this is sort of a completely new thing
so communication complexity applies to
just sort of one-shot computations but
for for these sore distributed
computation problems but you know if you
have continues distribute computation
it's not clear how exactly to prove
lower bounds here but certain lower
bounds have been proven but it seems
like there should be a more foundational
theory behind all this and in general
again you know with this is the basic
idea so we talked about you know the
geometric method as a generic tool for
dealing with these continuous
distributed streaming problems actually
the the method is very powerful so we
can I've shown you a sort of a summary
of how to combine it with sketches we
had some work in Sigma last year that
talked about how to combine dynamic
prediction model
method and some Morrison work that
actually shows how to apply the
geometric method for even more complex
functions things like you know
monitoring a distributed skyline over a
set of strange there's a lot of
interesting work that remains to be done
and I just want to sort of log in here
this workshop that we're organizing in
and bl to be this year which is exactly
on big dynamic distributed data and you
know the deadline here there is may 29th
but i have from good sources that it's
going to get extended so if you have
something that you want to send you know
please consider submitting it i think
it's going to be a really exciting
workshop and hopefully we'll get some
submissions from this room as well and
with that no i just want to thank you
all this is this is the lift website
where you can actually get some more
information about the project and and
papers presentations and so on and my
webpage of course feel free to contact
me for anything</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>