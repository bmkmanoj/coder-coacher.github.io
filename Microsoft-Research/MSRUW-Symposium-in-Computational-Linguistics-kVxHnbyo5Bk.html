<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MSR/UW Symposium in Computational Linguistics | Coder Coacher - Coaching Coders</title><meta content="MSR/UW Symposium in Computational Linguistics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>MSR/UW Symposium in Computational Linguistics</b></h2><h5 class="post__date">2016-06-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kVxHnbyo5Bk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi everyone welcome to the 39th
symposium on a Friday the 13th and so
far everybody everything's gone smoothly
most of you probably know the drill but
drinks are through this door there's
also coffee out there restrooms through
the back door and then in this direction
I hope if you're parking in the garage
you registered your car with a
receptionist if not you might want to
dash out and quickly do that and I think
we have once again we've got two really
interesting talks today and Fay is
actually going to introduce our first
speaker Dan Garrett okay it's my
pleasure to introduce Dan Garrett as the
speaker from our website he received his
PhD from UT Austin last year and he's a
postdoc at CSE Department you de and
he's working with look gentlemen so I'll
turn to then hi all right so I'm gonna
be talking about a project that I've
been working on for about two years now
sort of a side project what I've been
doing but it's really interesting and
fun so I thought it'd be fun to share so
this is work that I've been doing with
Hanna Alfred Abrams who's a student at
UT and the project was sort of
originated by Dan kind Taylor burger
Patrick and Greg Dorit at Berkeley and
we've worked with them as well okay so
the talk here is about document
transcription so transcribing historical
documents so just so we are all on the
same page this is a historical document
and our goal is to digitize it into text
we've specifically been looking at a
collection of text from the primeras
libros project so this is books that
were printed before 1600 in Mexico on
the first printing presses brought over
from Spain there's a website primarily
Bruce Oreck where these you can view
these
books they're all high quality scans a
lot of these books are from private
collections so it's really nice to be
able to have access to them so we're
gonna talk about a couple of things here
so I'm going to talk about you know what
what the challenges are with this
problem how we bottle it and our
technical contributions and I'm gonna
talk about something I think is really
interesting which is the implications
for Humanities in particular digital
humanities all right so we'll start with
the challenges okay
so one of the big deals with historical
documents is dealing with the visual
noise so this is not stuff that's
printed on a laser jet so these are our
books that were printed by lining up
little pieces of metal type and then
slapping ink on there and pressing them
down and so you get places where the ink
is splotchy the places where there's not
enough ink and it's hard to read the
letters you also get places where um the
alignment is off because they're they're
winding up these letters and sometimes
they're the words are kind of smushed
together and so you end up with all
these interesting challenges the fonts
that they used are also unknown to us so
they're not you know they're not using
Helvetica so you know these are fonts
from my computer but none of them
exactly match here so we've got a word
unsupervised what the font actually
looks like so the books we're looking at
you know they use not even a
standardized font within the the small
group that we're looking at so you can
see all sorts of things so the older
ones use this black letter font and none
of them really look the same the other
challenge in recognizing the letters is
that they look different all the time so
knowing what an a looks like you know it
could look like that or that or whatever
alright so we we're building off this as
an ocular which was the paper that the
Taylor Greg and Dan originally gave us
so I'm going to give a little bit of
background on that before moving on to
our stuff so ocular works as a
generative model has three parts so
first there's a language model then it
type sets and then it renders the
letters so the language model part is
going to model a sequence of characters
and then each character is going to
produce a bounding box and space in
between and a vertical offset because
some letters are going to be higher than
others and an amount of inking and then
based on that it's going to produce a
set of pixels that's going to represent
the letter our focus is gonna be on the
language model aspect so we're gonna
reuse the other parts okay so one of the
challenges with these books that were
printed in Mexico is that they are
mostly multilingual even the ones that
look monolingual still have multilingual
aspects to them and not only that but
they switch readily between these
languages so so here's a page from a
book called the advertencia s' it was
written in 1600 and so it's got Spanish
on it it's got Latin on it's got nollet
on it and they're just all jumbled up so
there's no chance for us to you know
segment into pages that are specific
languages we've got to handle this
jointly in a single model oculars
original language model was was really
straightforward it uses a smooth
character 6 grand model they estimate
the parameters in the usual way so they
have a bunch of text they count up the
engrams and so while when we first got
the the model of first got the software
from Taylor we were really excited and
we ran it through the system and of
course the language model that he used
was in English and so the transcription
was really bad and you got things like
English words okay that's you know to be
expected so we we did the natural thing
which was to take corpora from the three
languages that were relevant to us smash
them together do the same estimation and
what we get is is a really bad
transcription and with exhibiting what
we're calling the multilingual blur so
the statistics are getting all confused
and it's not doing a very good job so in
order to handle this we developed a code
switching model in which we develop
three separate models three separate
integrals and the characters here are
annotated with the particular language
and the key here is that we allow
switching between the languages at the
character level and this this got such a
much better transcription and it also
gives us a labeling of what language
each characters have so some some more
technical notes we want to learn the the
transitions between the languages and an
unsupervised way along with the the
standard training procedure and there's
this key of having a hyper parameter to
prefer not switching so when we didn't
have that it switched
readily because it's trying to figure
out what the letters look like and it
picks a word and if we tell it that you
know by default words that are next to
each other are the same language then it
does a lot better by trying to force
those to stay the same so here's this
this page from the advertencia sand so
we're looking at a string of characters
here and so we're gonna have a set of
potential characters for each language
and so these are language specific
because for instance Spanish has accents
and now it doesn't and so we're gonna
come up with a bunch of states and so
when we do our decoding either for a.m.
or for you know the Turbie thing we're
gonna have a lattice that goes for each
character now this is a simplification
because the system is actually doing
joint segmentation as well as figuring
out what the letters are so keep that in
mind so the goal here is to find a path
through the lattice and so here it's
choosing to call this now odd and then
we hit a space we allow changing between
between languages we don't allow
changing between languages within a word
because obvious so this is a Spanish
word and of course we hit another space
but we're gonna stick with Spanish the
other challenge that's really
interesting here is this concept of
orthographic variability so spelling at
the time really inconsistent so here
they spelled a word like D say as DS a
so there's a lot of these letter
alternations so there's U and V and B
and V and and some others and they also
didn't use accents the way that you know
standard accents are now standardized in
Spanish but before they they just left
them out all the time the other really
interesting one is this printing
shorthand that they did so the paper
that they printed on was really
expensive was brought over from Spain in
order to conserve paper what they did
was you know photo with a line almost
fit they would weave out some letters to
make it fit in there and there are
patterns to this but the patterns you
know vary across who the printer was so
these these very abilities are region
specific time specific author printer
document specific and so if we were to
try and produce language model training
data that
was representative of each of these
things we would be stuck trying to
figure out what all these possible
variations are is a non-starter for us
the other interesting thing is that the
indigenous languages so so now uh before
the Spanish arrived had a pictorial
writing system which does not lend
itself to printing out a printing press
so they developed a latinized version
and it was being still developed at the
time obviously so when we need a
language model training data we go to
Project Gutenberg they've got text and
lots of languages and that's easy for us
however they use modern Spelling's you
know they're not gonna have the short
hands in there as somebody's typing it
out because it would be really hard for
us to read as an example here just for
the challenges so this is one page of
one book we see the word mentor it's
spelled two different ways so one it
leaves help the end and puts a tilde
over the e to mark that the other one
doesn't so there's a field in the
humanity of scholarly editing and they
thought a lot about what it means to
produce correct output from a system
like this and so there's sort of two
choices that we boil down to there's a
literal transcription which they refer
to as diplomatic and there's a
normalized transcription which might be
modern it might be something else and so
in order to figure out what we should do
we asked people we asked the book
historians we asked scholars of nahuatl
what they would prefer and half of them
told us one half of them told us the
other because some want to study the way
that people wrote and some want to be
able to search for a particular word and
see all the places it appears without
having to think of every possible
spelling variation and so the modeling
these things is really important so if
you don't do any orthographic variation
handling this word mentir it gets
rendered as as Marita which is also a
Spanish word it's just not this Spanish
word and the reason it does that
obviously is because it doesn't know
that there's an N that's been left off
there and the R and the T really look
similar and so it's it's you know trying
to fit that to a word it knows so when
we when we model this we want to make
sure that we understand that that n is
left out so our approach is to induce a
probabilistic mapping between the
normalized writing and the orthography
of the document so we'll see what that
looks like so it's a generative model
obviously we are trying to figure out
where the pixels came from so we're
gonna have a sequence of states now our
states are actually character level but
I'm just glass over that for now so we
might call this out it should actually
be Spanish I'm sorry
and so we generate the word persona and
so this is a standard spelling because
that's what we sort of expect to see and
the letters in this are going to produce
if they're going to generate the letters
that are actually going to be rendered
so the S here is gonna get rendered as a
long s the O is going to be rendered as
an O with a tilde because the N is being
left out and so for each of those
letters to be rendered we're gonna
produce a bounding box we're gonna
produce a vertical offset we're gonna
produce a amount of ink and then we're
gonna write into the pixels so what's
really interesting here is in our model
we actually end up with two
transcriptions not the one that we would
normally have so we end up with a
literal transcription and we end up with
a normalized transcription we also end
up with a tag on the language of the
word so the having this joint thing is
really cool because the standard way to
do this is with a pipeline so typically
you run an OCR tool you get a literal
transcription then you run a post
processing rule based system to
normalize it these rule banks are
enormous and they have to be produced by
hand and so it just doesn't work very
well because as we've seen if you don't
handle the orthographic variation the
literal transcription is really terrible
and so trying to normalize from that
it's just not gonna work very well when
we learn the parameters of the model we
use expectation maximization I'm not
gonna say more about that so in order to
know how well we're doing we ran some
experiments so we evaluated our system
on seven books in the primary works
project from different time periods
different printers different you know
readability you know in terms of quality
different languages we wanted to have a
baseline so that
we could see if there was an easier way
to do this and so our baseline was to
take the modern Spanish that we got for
Project Gutenberg have a list of
replacement rules because we want our
language model to look like the the data
in the book and so we basically
transform the modern Spanish into
something that looks like what the
document has so this is kind of
interesting because so so we do better
on the normalized transcription so if we
don't do any orthographic variation
handling basically we're just kind of
targeting a normalized transcription
because we are trying to basically work
we're trying to model the word as the
old language model tells us if we do
this
rewriting then we're trained changing
the normalized version to the version
that we expect to be in the document and
so we're sort of targeting the
diplomatic version but our model
actually does both and so we are able to
dramatically improve the normalized
version without losing too much on the
diplomatic side to see some some outputs
you can see that we we capture lots of
things so we get this you know qual to
koala that with a C so they used to
spelt with a Q in the word esta they use
the long ass which if you don't handle
the orthography it gets confused about
what that is it captures that qu e is
sometimes written as q tilde that
accents get dropped and sometimes they
would double letters so this happens
frequently another cool thing that we
didn't really think about but totally
happened was that we caught typos so
this this one here they wrote a an R
instead of what should have been an e
without the handling it tries to fit
that and it comes up with an eye but in
our version we recognize that it's
really an R and that it should be an e
and so our output contains that there's
some stuff that we just don't do very
well so hey Zhu Cristo where crease
dough is written as X P tilde Oh so
that's written because that way because
it comes from the Chi Rho which is a
Greek way of writing Christ and so our
system has just no ability to do it
because we don't we don't know how to
work a mapping from creased out to
pío another kind of interesting thing
is that because the Nahuatl orthography
wasn't standardized at the time they
didn't use a lot of spaces in these
documents and so our language channel
training data has spaces the original
document has these really long words and
the evaluation is funny right because we
do pretty well on the character
evaluation but not the word evaluation
because it's unclear what a word is now
I'm going to talk about the sort of
broader implications of this in terms of
humanities and particular digital
humanities so as we were doing this
project we wanted to constantly be aware
of our goal here our goal is not to
produce transcriptions our goal was to
produce interesting data that people can
use for further work the transcriptions
by themselves are just a means to the to
that end and so the dual joint
transcription thing was really exciting
and particularly the indigenous
languages so this this field of
scholarly editing is concerned with how
to prepare text for publication and so
trying to figure out how we can take
this this indigenous writing and turn it
into something that we should be
publishing it's a good question so for
these Nahuatl texts the original people
who worked on easily who develop these
latinized scripts you know 500 years ago
referred to not novel language as
barbaric and described it as being
deficient in certain sounds so this is a
really interesting concept they viewed
Latin as being the perfect language you
know handed down by God or whatever and
they viewed all other languages as
deficient forms of it so you know
Spanish was missing certain sounds and
now it was missing certain cells now of
course we know that now it has sounds
that Latin doesn't have and the the
monks who were doing this probably
didn't even notice that or care so the
you get this this aspect where the
colonialization of this language is sort
of wiping out important aspects of what
that language looks like and so when
people want to write things now when
they want to publish things in modern
orthographies they're developing new
orthographies that that decolonial is
the text and so they're adding things
like tonal markings that
should have been there originally and
warrant and these orthographies are now
being produced with the help of native
knowledge speakers another great thing
about the fact that this is unsupervised
is it allows us to target any normalized
form so the orthography is as I just
mentioned are changing there's a book
that was just published pretty with a
new form of orthography and so our
system is able to you know we just take
more training data of that form and it
will produce transcriptions in the
orthography that we want and so this is
important because you know the original
orthographies are sort of in this
colonial context and we want to produce
things that are what we would say is as
actively anti colonial and the great
thing about this dual transcription
aspect is that we are still preserving
the original orthography so their
historical and societal implications to
the way that people wrote things and we
don't want to lose that you know we want
to keep that because that's part of the
historical record so we want to produce
transcriptions that are true to the
language but also capture the historical
implications another nice thing is that
we get more output than if we just did
the transcription by itself so we are
producing language taggings
right that are not sort of the in the
text it's a weight and aspect and
there's metadata associated with all of
these books you know on the website or
whatever and the metadata is often wrong
so you know a book might be it might say
it's written in nahuatl and have huge
Spanish sections because you know if
somebody was looking at the book maybe
they didn't notice that but our system
can can find that and actually quantify
how much of each language and because
it's word level tagged you can study
things like the context in which each
each of these languages were used this
would be totally impossible to do by
hand because you're not going to read
through thousands of books and thousands
of pages and try to find all this
information the other thing it outputs
well is because we have this
probabilistic mapping but
queen orthographies we can actually
quantify the ways that people wrote so
we can see here a bunch of major
replacements in a Spanish book and so
this is really interesting because it
allows us to ask questions and so one of
the cool things is because these
orthographies are specific to regions
and printers and and different things we
can use this information to try and ask
you know / - for instance who printed a
particular book this is something that
people in the humanities and people in
the digital manatees so trying to find
out who types that particular
Shakespeare text is a project that
someone's done it's also related to this
this other interesting thing of how
scribes earlier scribes who wrote things
by hand are said to have reinterpreted
text and so trying to figure out what
that means in our context you know how
the orthographic changes what they say
about the ways that people chose to
print things so to conclude we started
with scans of historical books and we
produce two transcriptions that our
language tagged and have the orthography
the original orthography in the text and
a normalized format the orthography and
so we want to take into account these
multilingual aspects the the fact that
these orthographies change in order to
not only improve the state of the art of
OCR but to also produce data and produce
tools that can be used to open up vast
resources for people to study these
these interesting questions further and
if you're interested in reading the more
technical aspects or trying the software
out you can find links to these things
on my website and the code is all open
github thank you
yeah so you've said that wonderful
aspect of this is that you can detect
languages in the text that almost
impossible to do manually but your
training data is language models from
specific languages so how do you decide
which language models to use I so so I
so you could do two things you could you
could throw a language models in for all
the languages that you expect but really
what I'm what I'm talking about is that
it would be impossible for you to go
through and quantify these things you
need to read thousands of pages just to
find out where certain languages were
used is sort of a more easy question but
definitely if you threw in language
bottles for Nahuatl in Zapotec and
Mixtec you would hopefully be able to
define those things because there's
nothing that says that every language
has to appear every document yeah that's
really quickly but when you talked about
finding the transitions between
languages is that just process yeah yeah
yeah so we you know count up the the
places where it appears and we do a
Hardy M update on that too if there was
any interesting looking into where that
happens no so we we thought about that
it's it's pretty darn good at finding
those language transitions and so we
thought about things like oh maybe we
should look for word cues that tell us
when you're gonna trance when you're
gonna change and it we haven't looked
into it because it doesn't seem terribly
necessary there are some some kind of
interesting ones one of the books that
we were looking at is a book about
Nahuatl grammar and so it has sort of
nahuatl words thrown in or not there's a
place where it talks about how it
prefixes so it kind of throw these
throws these in amidst Spanish text and
it's not good at finding those because
they're just too small and too ambiguous
to to capture and so it just tries to
transcribe that has some weird Spanish
thing
yeah so I don't know what the practice
is like in the digital humanities but is
there room for a back move actually
we're in a balancing would go through
the transcription and then you know
maybe manually great things and this is
great this is critical for us critical
it is it is not our goal to produce
things and hope that they are useful it
is our goal to ask people what they want
produce things talk to them about it see
what they want to do with it and
absolutely do that so things like the
you know finding replacement things we
we talk to people who work on on an a
lot text to find out you know what what
are the things that we should be
expecting what what are the things that
we need to build our systems to be able
to handle so definitely that's a that's
a thing that we always want to keep an
eye and even after an email after all
this is done right produce transcription
set up being used by by scholars right
and that is at least a potential that
they would they would manually correct
the text right yes supervision signal
that yeah so one of the one of the
issues that we came across
early on it is still kind of playing us
is that there's not a lot of nahuatl
text there just isn't and so this thing
is actually really useful for us because
we can we can produce more Nahuatl text
and it's not going to be perhaps great
but we can give that to people and they
can they can correct it which then
becomes language model training data and
so that's definitely a thing that we are
trying to do yeah so how do you
distinguish between like code-switching
versus borrowed words first it's like
proper nouns we we don't distinguish I
mean borrowed words you know where we're
calling code-switching I mean the phrase
code-switching is not technically
correct to to encompass all of these
things because there's you know mixed
language use and borrowed words but if
we come across a borrowed word in for
instance not a lot which is extremely
common because they're you know words
that they just adopted we would manually
are you we wouldn't you know the system
would would
label those as being Spanish or Latin so
in this in this collection of books is
the vast majority of them like intended
for monolingual audience or I mean not
necessarily right so a lot is booked
news most of these books a lot of these
books are religious in nature and so
they were written by monks for monks and
so they contain things like Spanish text
but they also contain religious writing
that's in Latin so it would be expected
that somebody would have to be able to
read Latin or the books that are written
about not water in not what what would
also have to have knowledge speakers um
whether they'd be you know the ads that
population or people who had learned not
what later I apologize if you already
mentioned this but how do you determine
what the normalized for is it makes
sense like the original form is just
what is there on the page but what is I
need you to remember yeah so that's
that's a great thing about the
unsupervised case is we can produce
whatever you want where what you want is
is characterized as what the language
model training data comes in as and so
if you want for instance for the Spanish
case right you can feed in modern
Spanish and it'll produce something
that's more modern or you can take books
that are hundreds of years old you can
feed in Don Quixote and it will produce
something that's that's slightly
different and then with the knowledge
you can give it you know an older
orthography or as people produce new
orthographic conventions you can feed a
new stuff and it will be able to keep up
to date with the the ways that people
want to transcribe things
yeah so what was the language Holi video
using because you mentioned the six gram
character model yeah you just stick with
that
so we yeah so we used the the
code-switching model so we we had three
six gram character models that were
wrapped in together and able to switch
between yeah so yeah the the six gram
model worked pretty well we haven't we
haven't messed with with trying to
change the the six but you could imagine
that you know with now I'd you know
because we have lost data but those are
things that we haven't tuned
all right thank you so good afternoon my
name is Lucia Vander Wende and so I am
the Microsoft representative for this
part of the talk we always have one
presentation from u-dub and one from MSR
but I'm kind of I sit on some both sides
of the lake so I'm going to introduce
Maleeha yet Tuscon to you my colleague
she and I started a project several
years ago which was funded by MSR
outreach and and so as a result I
thought now also become faculty at u-dub
so I don't know quite which which way I
am so Malia is a wonderful colleague
we've had the pleasure of working
together last couple of years she got
her PhD from at University of Washington
did a postdoc there did some work in
industry at Siemens so kind of also sits
on both sides of kind of the fence there
and now is associate professor in the
department of biomedical informatics and
medical information or education at the
University of Washington and she leads
the u-dub bio NLP group which is you
know enjoys we enjoy talking with each
other and we should get to share a lot
of information about this space so
pleased to introduce really have to you
Thank You Lucy for the interaction so
today the title of my talk is extracting
semantics from clinical text and today
I'll be talking primarily about our
research ongoing research at our u-dub
by an LP lab so all the content that
will be presented today is available at
this website all the papers tools and
and hopefully we'll some really really
some datasets soon so what we do in our
lab is we do process electronic medical
records EMRs or capture the healthcare
from various perspectives so it's
contained structured data about patients
that involve problem lists lab results
pharmacy orders
and many many other parameters and also
it contains a best amount of
unstructured data in the form of free
text so some examples would be radiology
notes of reports admit noise discharge
summaries so they all describe the story
of a patient during their hospital visit
and and in our institution more than 80%
of patient data is actually in the form
of free text so this represents us
really good opportunities are too
difficult to formulate and RP problems
and work on them so this is an example
discharge summary this is coming from a
publicly available data set so it's okay
for me to share with you so as you can
see it represents many of the aspects of
the hospital course from multiple
different dimensions but because this is
free text if we captured this
information in the form of only key
words we would get some knowledge out of
that such as like most probably this
person might be suffering from acute
leukemia or maybe having problems with
headache nausea vomiting diarrhea and
maybe has some history of like colon
cancer perhaps depending on location of
the document but if he add like section
information on the top of that we would
know that I could leukemia is actually
related to the present illness and colon
cancer is actually is related to
somebody in the family so if we add more
information on the top of that
for example temporal information we can
build a timeline negation is very very
critical in our research because most of
the symptoms they are that are critical
for diagnosis is like presented as
absent if the like condition do not
observe them so there are so many
negation mentions in clinical records
for example if I may add negation we
would know that this patient is not
suffering from hay to headache nausea
vomiting or diarrhea actually so it's
quite important for us and also another
piece of important information is
uncertainty where cognition cannot
decide whether any one symptom is
present or absent but like questionable
or like hypothetical conditional or
something like that so
on the top of that there are other
layers of information of course I
reported in these texts for example
medical tests are interesting to be able
to extract with medical tests I
mentioned I mean that like the test
itself and the value outcome of the
tests also medical treatments apply to
the patient during their course of visit
so our research interests in our lab is
to make information hidden in vast
amounts of clinical data meaning in
Amar's more accessible to improve
clinical research and patient care so we
are after not the EMR design but the
secondary use of EMR data
so these are some clinical applications
that we are working on actually when we
think about like like formulating a
problem that revolves around EMRs we
look at usually the problem at different
levels so one that will be the note
level right you have a given note and
you can extract all medications
treatments tests findings reporting that
report and one example application would
be is billing not that interesting for
us but we are more interested in quality
improvement part so we have an ongoing
project right now that involves like
processing radiology notes for
extracting incidental findings and from
incidental my findings we mean that you
go to see a radiologist for a particular
test for example like totally like
perhaps a checkup
and then they see something very
irrelevant to the purpose of the test
such as a like an lung nodule or
something and then they would put a
follow-up information there like saying
that like this is suspicious it needs to
be followed up in certain number of
months and if this information is not
well communicated to the referring
clinician then bad patient outcomes like
unfortunately happens and it's real
problem so NIH funded our project so
this is one project we have at the North
level we only look at patients at the
North that was for this one at the
patient level there are multiple
questions we can ask
one question we are very interested in
is to be able to answer does a patient
have a given disease based on the a
bunch of like a bundle of data for
example this may be their whole Hospital
visit their whole ICU stay or like
things like that so being me
we try to answer the question by looking
a although accumulated data above for
that patient and this is for disease
surveillance and I'll be talking more
about this ongoing project predicting
the onset of metro situation pneumonia
so I will not talk more about that now
ah
and the last level is the population
level where you look at the EMR as a
whole and you try to extract a group of
patients that like me me meet certain
characteristics for different purposes
this may be for cohort selection for
clinical research or for outcomes
research we have two ongoing projects
right now relates to that and one of
them is predicting liver cancer stages
for the mini like for treatment outcomes
such as like if you can predict a liver
cancer stage it goes from I think one to
found five or something I always forget
about that I want four or five and five
so four if you can map that like the
patients to that stages and you know
what kind of treatments apply to those
patients then you can estimate what
would happen if another patient would
come with similar characteristics so
this is four outcomes research for at
the patient level and a new project
speech which we recently initiated is
extracting environmental and social
factors for disease outcomes and I'll be
talking more about this project or so so
for all these kind of like projects we
have to look at ya comes back multiple
times we'll each of those generate a
different free text field or will it
just be one big long record so every
time like a report is generated it's a
separate report it doesn't bundle up
yeah
so to answer these questions it's quite
important for us to be able to extract
the structure and the semantics from
these three text notes and we have done
some research on this prior for our
previous projects or going ongoing
projects and we will leave some tools or
one of them being is a statistical
section chunker which for a given
clinical note it will identify the
boundaries of the sections and then for
each section it would map it to a
section header and another tool we
release and it's very popular actually
is the assertion analyzer where we have
a sentence and a medical concept and it
will tell us whether it's a present
absent conditional hypothetical possible
or not patient so these two two tools we
released and we are currently working on
an event extractor and also a core
efference analyzer the cover reference
analyzer I will be talking about the
event part but for Co reference analyzer
we are interested in core efference
extraction for tumour information in the
radiology notes so it's very very
interesting because like this captured
like tumor information apparently very
important for us to extract for the
tumor cancer stages because number of
tumors like however they are located and
like and they're mentioned everywhere so
we are building that and we are
releasing that hopefully mid-summer so
in the remaining of the talk I'll be
talking about two clinical applications
mainly I'll be providing some
information but will not go very into
detail of the technical details
I would be happy to talk with you after
the talk if you have any questions about
that
so the first application is phenotype
modeling in the ICU this is joint work
with Lucy faith and we have two clinical
collaborators mark were fool and Heather
Evans from u-dub medicine we had a bunch
of suppose a current students who worked
on this project and our motivation is to
develop an automated screening tool that
would accurately identifies critical
industry know types in the ICU in this
context I used the phenotype as a
disease name and our diseases manager
associated pneumonia we picked this
disease because this has been the most
deadly disease
in the hospital acquired infections so
and it's kind of quite interesting
because it's a consensus definition it's
consensus diagnosis so some information
will come from 3:6 some information will
come from structured data so it's
perfect like prediction problem for us
so for this project we've created a
really nice dataset which included all
three texts not generated during the
patient's ICU stay from admit to
discharge and we had all the structured
data that is related to our cars for
ventilator associated pneumonia so we
can capture vital signs all the patient
related information but also we captured
ventilator bundle elements because this
infection happens because these patients
are intubated to ventilators so the
angle of the bed or like what kind of
respiratory treatment is given also
being captured here before the
annotation we of course looked at the
what like kaliesha stick a VAP patient
and as I mentioned this is a consistent
consensus diagnosis like the first piece
coming from radiology notes mainly
talking about it's like lung problems
and the second piece talks about like
fever and other problems that is
captured in the structured data and the
last one is coming from the microbiology
results so mainly it's a semi structured
text currently it's captured in
structured format but at the time we
built this data set the text was like
the report was is a text blob so we had
to do something about that for
annotations we had a clinical
collaborator and we created a almost 800
patient data set where 123 word
impulsive or VAP we captured more than
14,000 ICU days these patients some of
them are really sick because they state
like 163 days in the ICU which is crazy
long to me and we capture 27 more than
27 thousand 3 text nodes and
looked at different scenarios in this
project because this is a really
interesting data set so we had we
explored some other of some options in
the first scenario of the task is to
decide whether the patient had a
specific phenotype so we looked at the
data retrospectively we bundled all the
data and determined which ones are
positive or negative for the given
disease and this was for clinical
research and cohort selection purposes
one of our clinical collaborators was a
geneticist and he want to create cohorts
of patients so this B did the study for
that purpose the second scenario is the
hospital type one where for a given
point of time you try to decide whether
the patient has the disease or not and
this is for phenotype surveillance for
the hospital case and the third one is
we'll have no type where you have it
your at times empty and you try to
predict the future whether the patient
is a high-risk or not and this is for
clinical decision support so for the Hat
phenotype one we build a very basic
baseline we use uni grams and then we
use some some clinical terms we didn't
use structured data in this one because
usually because when you bundle all the
data together like usually a mention of
pneumonia F happening in the pretext
already so our performance mummy did the
baseline in a very very simple format
this was an SVM classifier it was 50%
when we added like section information
and assertion information negation and
everything our performance went up to
85% so it really mattered so for
documentation document classification
tasks usually you don't need to add more
context to an grams but in this context
it really helps to improve the
performance so we did some error
analysis and we realized that it's not
good to add microbiology notes as an
grams because as you can see you cannot
get much much out of this when you
represent this kind of beer texting with
an grams so we built a information
extraction tool that would capture these
components of these test results in a
more structured way so we did that
it's a very very basic like information
extraction task so I won't go into
details of it that for the second part
we realized that assertion classes were
not powerful enough for most times for
example we are trying to capture present
absent conditional hypotheses to go
possible not associated with the patient
but if you have the second example the
report - like it says gradual
improvement of this very very important
indicator of pneumonia and when you run
a certian crossfire on the top of it it
will say present so so we had to do
something about that
so this is our ongoing research one of
our students like working on mainly
extracting clinical events with change
of state information so this example is
coming from real data actually where
three ICU days same patient five reports
and this is the event if you lung
capacity is consistent with pulmonary
edema and it says like no change versus
no change gradual improvement and the
task is to be able to capture that like
event and its current like changing
correspond to the previous reports so to
be able to do that we built an event
description of course oh we capture
these like radiology radiological events
from anatomical measurement value and
change of state and also a link to the
previous report kind of attributes so we
capture these pieces of information for
each event and we created a corpus
mainly composed of more than 1000
sentences be annotated them for this
event definition and these are some
example annotations so in the first one
as you can see we have this like
radiological event like captured with
all pieces of information that we
defined and in the second one we have
two events actually that have shared
entities so this was the extraction task
and we also enhanced that with a
coordination and negation
after doing some initial analysis of the
corpus so in terms of performance like
we built a name and to recognize her of
course or to be able to extract the
pieces of highlighted text and this is
these results are coming from Stanford
any art package we had to define some
domain specific features of course but
our performance is currently quite good
for entity extraction and we use the mod
parser to be able to extract the links
and our current performances at this
level so this is one piece oh that's
ongoing research our like our student is
working on that's right now but like if
I return back to our like phenotype
extraction task so we are doing research
currently also on the will have
phenotype scenario and the task is like
to be able to trip predict VAP in the
next eight hours based on data collected
to infer our hours prior so what we did
for us is at one of our our new students
is working on this project where we use
on structured data to be able to extract
like to print to this prediction as the
baseline and we mainly trained a
Bayesian network by using a Microsoft
tool of in mind and our current
performances at this level the baseline
performance and of course the next steps
will be is like because this is the
conferences this consensus diagnosis we
are going to add feature text features
coming from reports we will most
probably true first like Eng grams and
then we are going to true like these
event features and also the micro
features ok so I'm going to move on to
the second clinical application in the
clinical a second clinical application
what I'm going to talk about is like
extracting very different from the first
one this is about extracting lifestyle
and environmental factors from clinical
records so this is the population level
1 this is joint work with Lucy Hezbollah
is like a new faculty in our department
david claussen who is a biostatistician
and
here the motivation is lifestyle and
environmental factors play a significant
role in in on our health actually and
they are very well recorded in our
clinical records however when people do
like disease outcomes research they
usually focus on like genotype phenotype
extraction type of tasks without taking
the environmental factors out of the
equation although most of the time
genome stuff is like kicked in because
you add something from the expo like
outside so in clinical care like five to
ten percent of cancers are attributed to
her three factors and while nineteen
ninety five percent have been found
correlated with these elements so we
thought that like it would be really
interesting to look at the clinical
records to observe how these elements
are represented we looked at u-dub data
we we looked at some existing literature
on the topic and we found out that like
the doctors actually like put a really
good effort to record these elements
like they report like when they ask you
questions about your daily life actually
they are writing really serious details
like that you are not aware of so so the
purpose of this project is to be able to
automatically identify those elements
and then do some sort of like outcomes
research at the population level so what
we did was like we looked at it of
course some existing literature also the
EMRs and we came up with this list that
we thought would be interesting to look
at also we had some guidance from some
clinicians while we formulated this list
and we define these l-like factors with
many attributes so what we are after is
not to know that whether a person is a
smoker or not but we want to know that
how much smoking the person is like
experiencing or like if they quit
smoking how much like they smoked in the
past so that we would have a gauge of
like like quantify
you need the exposure amount so we
mainly defined these attributes to
capture all the possible details
represented here in this clinical text
and we created a data set mainly we
mined from a publicly available web site
empty samples which is medical
transcription samples website we picked
history and physical notes because they
are very rich in social history
definitions and then we had annotator I
met
a medical student who agreed to do
annotation for us and we did turbulent
ation a small set and we done like
minion notated the whole dataset and
these are some annotations examples uh
as you can see these are coming from
them the publicly available data set
however we compared this with our own
data set at University of Washington to
see whether this publicly available data
set is like a good representation of
what they were really like represented
in a clinical environment and we agree
that it's very similar and the reason
why we decided to annotate first
publicly available data set is then we
can release the data set because
otherwise we cannot do anything with it
so this is an annotation example it like
across various factors these are some
statistics about annotation of course
substance abuse has been annotated more
frequently than the other factors and
it's also partially because of the size
of the current data set is a very small
data set so we decided to work start
from substance abuse extraction first
and then these are the attribute
statistics so the ongoing work is like
we are in the process of building the
extractors and I do use this data set as
part of the clinical and RP class I do
teach at University of Washington right
now so some of you actually working on
this data set so after we built all the
extractors we are going to apply this to
the u-dub medicine clinical history for
disease our
comes research of course and then
another aim would be is like to apply
this to another project which is called
emerge actually this is a really cool
project which is it which is like
involves ten institutions across the
United States and they do or meaning
genome phenome analysis work so if we
can add environmental factors as a as a
parameter to this equation it would be
really interesting research so I would
like to acknowledge my collaborators I
want to acknowledge of course my lab I
kind of I'm very privileged to work with
Lu CFA and many of our students so and
also we have many many clinical
collaborators I just listed people who
do research who did something about this
project so and questions yes in existing
so some of the work that we've been
looking at where people are doing like
large-scale Hospital record of trying to
predict something like heart failure
readmission yeah what is typical
practice so they just do a keyword
approach are they just like auditioning
the first thing you said thank where
they're not paying any attention and
negation or anything like that and just
chopping up all the words and throwing
it in or are they doing something more
sophisticated so it depends on who is
doing it right so it's it's kind of
sometimes oh I don't know you are asking
a very white question some people just
to actually regular expression searches
like they would have a certain phenotype
or disease names like in their minds
unable to regular expression searches
they would add negation on top of it
perhaps like if the phenotype is very
very clearly written in the text like
people's preference would be at first
and if it's not that clearly written in
the text and you would need to add more
or more information from the rest of the
like reports or other information such
as medications they can go with building
like cause fires like based on the whole
data yeah so it's it's all over the
spectrum actually and it all depends on
your disease type for our disease Vemuri
definitely cannot get we'll have type of
prediction by only looking at the text
or by only looking at structured data so
we decided to combine them both so that
talks to ask about sanitation tool you
are using today with the training data
or we use Brad that from Oh north of
Manchester yes so it's quite a cool tool
actually it's very easy to set up and
depending on of course it depends on the
type of annotation you want to do it
doesn't let you to do everything but
it's pretty sleek actually so it's yeah
it does not do this between documents oh
yeah looking the links between documents
that you can link any sentence within
the one doctor
okay be honest about the evaluation when
you humble the ground truth to the
extracted entity is it exact matching or
soft matching so that one's exact
matching if you miss one more do you
consider it false
yeah but thank you just a second I need
to double-check the table we do
calculate both actually we do calculate
oh this must be a token level so it's
not X in exact it's yeah yeah because
like ah if I don't you can correct me
because like this should be in exact
match right oh really
okay your differential between it in the
beginning of the continuation of saying
so for this one like a space that is
restricted it's bigger but I get exact
but like for token match I don't think
we do oh it's like mini-me calculate at
the token level like everything and then
like if there is it like gap between
them it will still get a credit this may
be a little bit of a crazy question but
it goes into politics like because I
mean you know this is the kind of
research that obviously and you could
have an incredible positive way what's
the level of like National Coordination
of of these things or the level of
support or the awareness amongst you
know funding agencies what is that and
what would be sort of Union your top
wish list you know you could make the
policies
so I think it depends on the application
for example for there's like there is an
agency and I under NIH called hrq which
is about improving the quality of
healthcare that who founded our regular
work actually so for that one there
about that so there are many try to
increase the decrease the number of
fault like mistakes make made during the
hospitalization and things like that but
like it's for do for that one it's like
for that those kind of projects is
easier to get funding but phenotyping
projects if it has direct impact on the
patient it's harder to get funding that
they usually claim that it would be how
can I say it would be a little a hard to
implement in the real case like real
Hospital case although it's kind of the
project has been initiated by a
clinician actually so our our clinical
collaborators came to us but to meet
that like we would like to have a
predictor system that tell us who are in
like high risk for this phenotype but Oh
from funding point of view it's harder
to get funding for that one than other
one the radiology project where we try
to identify the incidental findings and
things like that because that highlights
that project is about like highlighting
to report the other one has direct
impact on the patient care so they I
think the varèse are more on one side
yeah but and I would be more happy of
course if we can get more funding of
this kind of research and is it is a
core native at a national level so let's
say you know the u-dub with with
Microsoft is working on you know some of
the specific problems and then another
place is becoming the Center for working
on you know
other phenotypes or other problems or is
it right now is it basically just
whatever the research bubbles up you
know that's where some some funding goes
and the best one sort of shakeout and in
the end I think the NH funding like way
is a little bit different than NSS one
like they Triton they tend to fund
bigger projects more so if a project
involves multiple institutions
you'll get more chance for example to
get funding for a phenotyping project
for example emerge is a really good
example for that it's a really really
big project that involves like 10
institutions it's crazy big but they're
all what are their all doing is like
they mean II like extract phenotype
information from EHRs and they also like
collect the genome information from
patients and they do genome bite like
genome phenome wide analysis of these
things so I didn't know like it's it's a
mix I think like it's sometimes of
course like some institution would be
more powerful in terms of in certain
areas and then they may just draw more
funding in age I think tend to like give
bigger amounts of money that would
involve like lots of institutions and
it's understandable I guess because then
perhaps to have more impact you also
know that research may be strong to
wherever there's data right so I to p2
has made some small medium sized data
sets available so there are a number of
institutions across the US and worldwide
due to smoking cessation detection
obviously
and so on but that's just where the data
is and the real problem was working in
this space is the creation of data
because the hospitals don't have this
data and there is a huge concern for
privacy with you know we are working on
a Mars so it's it's challenge that's why
we try to undertake publicly available
data so that we can make baselines more
available but thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>