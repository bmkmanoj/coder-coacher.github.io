<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using FPGAs to Simulate Novel Datacenter Network Architecture at Scale | Coder Coacher - Coaching Coders</title><meta content="Using FPGAs to Simulate Novel Datacenter Network Architecture at Scale - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Using FPGAs to Simulate Novel Datacenter Network Architecture at Scale</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JjrLu9_Mwbs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hey it's my pleasure to introduce sangee
tin who is a PhD from UC Berkeley who is
actually pretty well known I think
you've been here several times and you
have a you got a Microsoft fellowship no
you didn't well you should have and has
done a lot of very interesting work
using FPGA is to simulate data centers
and you'll talk about that work today
okay thanks Jim thanks for the
introduction
so the talk runs about 15 minutes so
feel free to ask any questions I can
answer on the line and the table II
didn't present is my dissertation work
I've done in Berkeley with Professor
Cristo sandwich and David Patterson the
table you present is called a diablo
using IP JS to simulate normal data
center network architecture at scale so
this is our line agenda on my talk I'm
gonna briefly talk about motivations of
this work and things we're building an
emulation on tasks bad so we're gonna we
trip data centers the whole computer
system so they're in a course with
project we systematically look at how
computer systems should be evaluated and
we have our own methodologies and then
I'm gonna talk about the beef part of
the talk which is the Diablo stands for
data center in a box low cost and talk
about implementations talk about
architectures and then I'll show you
three very exciting case studies we have
done and some of them you wouldn't be
able to see with karna tools and finally
since this is a really long time
assistant building project there are
lots of experiences and life lessons we
learned I'll talk about that and then
conclude my talk so datacenter Network
is very interesting topic right the
first on the first I want to show you is
a traditional data center network
architecture this is from a Cisco's
perspective this is the traditional ones
right so this is sliced up Francisco as
you can see data center network usually
tiered architecture right have multiple
tiers down the bottom a rack of servers
and you track of server you track
contains roughly around like 440 usual
machines and you have table of rocks
which and further this table all racks
which will aggregate
which people call a grey switch or
aggregated switch and their multiple
levels of those and just to give you
some other side the idea about the size
of the whole network the typical center
usually you see right 10 mm even more
machines and plus like thousands of
switches so this is really really large
network there's no doubt about that so
take that center networking
infrastructure apparent is very very
important this sly site ISO from James
Harmon from Amazon so basically on high
level he bill the data center network
infrastructure is the SUV of the data
center this is use an analogy of the car
as UV cars really large and hy-vee's
poor gas managed and first of all
networking infrastructure pre expensive
right source accounts for of third
largest cause of the whole center and
more importantly it's not about the cars
is about supporting like applications
and supporting about data intensive a
job like mac reduce type of jobs and
they're so very important right so
therefore there are lots of people
looking at this networking design space
there are some people from resource I
believe Microsoft and also there are
some people from products I especially
like from the Google's efforts and I was
Facebook then looking at like 40 gear be
like a honey 100 gigabit squish designs
if you look at all of this designs and
you'll find like lots of distinct design
features like for example in terms of
switch design and people have different
micro packet buffer microarchitectures
it does even the sizing of the buffer is
it completely different right and also
some of the switch has the program of
flow table some don't and even in terms
of the applications and the protocols
people support it they're quite
different right some people support the
different versions of TCP and has the
congestion bit support so on and so
forth
however if you look at all of the
designs they're pre the very nice
everybody claim they have the best
design rate compared to others but the
problem is if you look at all the work
done so far there's one problem has been
largely ignored so far which is the
evaluation methodology right so if you
look at how people evaluate their nuovo
designs right there are a couple of
issues first
well the scales write the scale of
people having right now it's that's very
small order and reality that Center
network I showed you in the first slides
if you look at the experiment people
done so far typically are you see a test
pad of less than hundred nodes right and
especially for academia they things are
even worse because they don't have money
they just use less than twenty machines
and put some of the virtual machines the
pretend they have lots of notes and the
second usually is about a program and
people use right people tends to use
synthetic programs or micro benchmarks
even some people claim the ones back
micro benchmark because micro benchmark
and drivers switch to full and that's a
really bad assumption but if you look at
real data and their application programs
what they're running is really the web
search web search Gmail oh and like
macro to establish things and last is in
terms of the switch design itself right
so there lots of networking researchers
today they're trying to use
off-the-shelf switch and unfortunately
on those which is a really a proprietary
there's almost nothing you can change
there on for example um you you couldn't
actually easily change like say the link
performance delays and even the sizing
the buffers you won't change the
computer switch okay good luck have a
some talk with Brockhampton and da they
were lawyer to that to do that and style
very easy to it so here I raise a really
valid question how do we really enable
network architecture at innovations at
large scale like an order of like
thousands of nodes it's not a hundreds
of nodes without really building a
spending lots of money people in real
data center
large-scale data center infrastructure
so this way so before I start project of
is a couple places including here and
also talk to people like large corpse
like Microsoft or Google and Amazon's
ask them or what kind of features you
want on if you wanted to networking the
evaluations apparently one common
conclusion is that networking
evaluations those are at least as hard
as evaluating order large-scale system
designs so first of all you need decent
skills right so Taylor Center an order
of 10 mm knows but you need
at least like couple thousand no to see
really interesting a phenomenal secondly
if you look at the real designs rather
real thing you want look how I say this
switches right there massively parallel
if you wants it run something these
forests is a nice place that means you
really need some decent performance in
your simulators I think about large high
ridic squish or usually or even
sometimes close to the hand reports
think about number of flows through
support and the virtual cues whatever
else concurrent events just per clock
cycle anything trying to look at and
also if you look at knack work itself
it's a really nominal second not no
second time accuracy this is really
accurate think about the typical user
case for 10 Gigabit Ethernet usually say
transmit a 64 byte packet right on 10
Gigabit Ethernet it takes about 50
nanoseconds right so this is really
comparable DRAM access that means if you
want to have a simulators or whatever
similar things you there you will see a
lot of money fine grain a
synchronizations
just I'm just doing simulations last but
not least is people really really want
their production software they need
extensive application larger people are
not content with micro benchmarks at all
especially from product side and so here
is my proposal I'm gonna use that PJs
all right so this is sounds like crazy
for you who are not like familiar with
our PJs let me just give you a very
brief overview about what is that VGA
this is true from a digital circuit
design class from Berkeley
so I feature is a configurable
reconfigure allowed architectures you
see there's a interconnect and this is a
big functional block if you zoom in this
block you see a structure like this and
typically you'll have a lookup table
with a fixed number of inputs and that
will be used to implement and incoming
torii logic and to store some state in
the circuit and you have a flip-flop and
the people use IP on for many things for
example people use XJ to build initial
Hardware prototypes and people use IP GA
to do basic design valid
you can really use this highly
structured device to build a target
Hardware system today very easily easily
okay so my idea is trying to use a
visual little bit different compared to
people who use IPA for computer than
people use at beautiful very occasion
what I did here is I build abstracted
execution performance driven models that
are performance model but realistic
enough to run through software stack and
I'm overall after a build system when
calculated the cost per simulated data
center knows is really low it's like
twelve dollars compared to the Raspberry
Pi right so you spend like 25 bucks you
can you can buy lots of those but you
still have to so you have to buy the
internet and there's nothing you can
change or just an implementation now
also I believe there's similar efforts
you built some like hundreds nodes tasks
that using atom processors in academia
so because that I mentioned like Luis
Barroso from Google that datacenter is
really a warehouse computer so we're
gonna take the data center problem is
the computer system problem right so
we're gonna look at how people look how
people deal with this computer system
evaluation right one common approaches
computer architecture use is come to
architect accuses trying to run some
simulations right before I start this
topic let's get familiar with the Jaguar
terminology I would like to use in
simulation world people often like to
mention hosted versus target right so
you know what I mean by target is the
system is actually being simulated in my
case I'm looking at the data standard
designs of that means draw servers and
switches and network interface card and
a host usually when talking about
simulation means at the platform on
which the simulator itself runs in our
cases I PGA's use IPA to implement
architecture simulators if you're
interested in software simulators
that's gonna be you access six machines
right we actually look at many of the
computer architecture similar coming
architecture simulators system
simulators out there
primarily we
I think there are two types major major
simulators so the first one is a pure
base is the most popular ones based on
software right now we call it a software
architecture multi-skilled and ensure
same and the second one is like our
approach is use IP J's and we call that
in short Fame we're actually over the
paper about a couple years ago just
talking about differences and why you
have to do it on the IP GA architour um
same style simulations to the explore
the designs place why the sulfur is bad
okay so first let's look at the current
state of art right so the evaluation mat
law is same this is almost everyone will
use it right right so there are lots of
issues we software simulators so the
number one issue gives the performance
so to figure to show the problem we
actually look at how computer
architecture the simulations in their
research right so by doing that we were
looking at the papers published from the
premier company architecture conference
which is east cup attack indigo and also
the paper all the papers and people
publish a couple years ago I think the
data is slightly out but I don't think
there's a significant change today in
terms of the way that people simulate
their benchmarks so we're actually
looking at how many instructions people
simulate the pre benchmark across all
the paper people publish and also look
at the target the complexity over
targeted design in terms of number of
CPU cores its support and also calculate
a number of instruction people actually
study the procore so here's some
interesting thing we found like a
backhand decade ago like in the last
millennium right so everybody is looking
at a single court design and people
simulate around the medium instruction
people simulated for benchmark is around
260 US 267 million structures and going
back to a few years ago moving to the
multi-core year as everybody's looking
at multi-core design so the median
number of course people studies around
16 cores and given the complexity of the
system grows over the past decade and
people do sin
more instructions like close to building
instruction 825 million but if you
divide by the number of tours people
looking at this number a hundred million
is like not going up by actually going
down
sadly thinking about call frequencies
people are running today writes for the
for the process or back decade ago so
everybody runs to couple hundred
megahertz but this recently a processor
is actually running at Aqaba gigahertz
if you convert is one hundred million
instructions to raw walk-up time and
this is really sad this only translates
to less than 2 min 10 millisecond so
does that mean that means you couldn't
even simulate a typical on offering
system scheduling corner which is
typically around millisecond and if you
look at data standard vacations and iOS
and all sorts of things you at least has
to assimilate the couple hundred missed
a few minutes to see really interesting
phenomena and also another big problem
they of the software simulator is
usually times people tends to build
unrealistic models right for example we
have an infinite fast CPU so everything
just consumed one clock cycle which is
very dangerous and so therefore we are
really betting on this idea based
architecture emulation just a high level
there are simulators build on my PJs so
now that we confuse it with the standard
IP future computers you regularly and
I'll be confused with ipj based
accelerators people are interested so
there are actually simulators right
architecture simulators also when we
look at the architecture simulator
design of Fame design actually founder
lots of distinct the design features and
there are lots of dimensions right so we
summarized like to three basic
dimensions right so the first one is
direct versity how about I mean the
Kobalt means you can actually decoupled
the whole cycle from your target cycle
you you so for example we can use
multiple host cycle to simulate the one
target cycles and the second one is for
RTL versus abstract the RTL so for very
sick verification guys it tends to use a
modified AC car yell but in terms of IP
to simulation you're interested
performance you can actually have a use
like abstract abstracting a performance
art
artyom performance models and so that
has many IPG a friendly structures to
simplify your designs and also increase
your density and along with the
decoupled timing model and you have a
much simpler design perches take
multiple IPG cycles seemingly the one
target cycle the last feature is a
single threaded versus multi threaded
host and this is really crucial for me
to release I'll be able to simile a
couple of thousand our server knows with
the scriptures I'll show you what I mean
by male threaded hose I'll think about
this case write what we call this is
wholesome of that threading and by the
way so you find the similar in your
target trying to simulate like for
independence if you write one of the
easiest way to build your model is you
build for CPU pipelines right but that
will consume like more resources and a
single thing you can do is you'll build
a single processor pipeline input but
you put four Hardware threads on it use
each of the Hardware threads data cable
simulate one of this CPU CPUs in target
and you have a timing model so you can
synchronize all the Hardware threads and
make sure they in terms of simulation
they ask you in parallel so as approval
accounts type actually a bill of fame
simulators a field called run gold and
the goal is to simulate 64 spar v8 core
with a shared memory memory hierarchy
it's a really little cost that runs on a
750 dollar
Sonic's eschewed in the board and now so
it has sufficient hardware Union say you
have to refuse and them use it's capable
boot into Linux
it'll actually compared this simulator
with the the state of our selves
estimator which is simek's we found this
actually toward thank you faster it's
more efficient and this is also the
basic building block for my data center
a project Diablo so next let's look at
the data center project rise of the
Diablo of simulator so so I'll holla
what is Diablo Diablo is really a wind
tunnel for the data center networks
built with of course with a PGA's and so
the goal of the outlaw
very aggressive we're trying to simulate
a couple 10 mm notes on your prototype
we have a couple of thousand notes
running and each note in this system is
actually running a real software stack
they are now running running micro
benchmarks and also we're looking at
switch interconnects so focusing on a
switch interconnect will be able to
simulate a couple hundreds even
thousands of switches with Diablo at all
levels and with enough details
architectural details and accurate
timing so I want to point out so in
Diablo they're drill instruction
executing on there we're really moving
real bytes in the network when a sending
point fake corners so and also Diablo is
fast enough even like say overnight is
capable running a scale of 100 seconds
just a few minutes and also we know I
feature a really hard to program so we
pre build many runs and configure
architecture parameters for the user to
to modify with their control software so
there's no need to do everything to this
we can allows user to change say link
performance right link a throughput link
bandwidth and link latencies and also
even the switch buffer act layout and
policy buffer dropping policies things
like that so of course this could be
built with a same technology I mentioned
so they're actually uh three so I
mentioned Diablos it's all about
performance models right so there are
actually three basic models in T hablo
the first one is the server models it's
actually built with a run gold as
simulators so for the server models
right now we are using a very simple
timing model we're assuming it's a fixed
CPI timing Allah means in yellow
instructions take one cycle and the rest
of memory instruction takes fixed number
of cycles which one
it supports Part V I say because I
mentioned this actually measure this is
spark certificated a processor so we run
their verification suite and um
it runs the floor Linux run with word
the linux 2.6 they're on right now like
bring up linux 3.5 which is very recent
and also in terms of the others ii types
of models we have in diablo is the
switch models right if you look i did
the center networking
and there really are two types of
switches the first one the circuit
switch which is more like from the
research side and the second one is more
conventional is this is like packet
switching like production table central
run so we actually have both models will
build both models from their abstracted
models but our focus is more on the
switch buffer configurations instead of
the switch routings so the switch
reaction model is looking after a cisco
nexus switch this is a nexus 5000 and
plus a brought on patents I plan out by
micro cell research and and also the the
last model in this Diablo is the network
interface card models because realize
that network interface card model is
really crucial so we have a very
sophisticated network interface card
model the link model is support scatter
getter DMAs and zero copy drivers like
many features you found the performance
optimization features you've found the
production NICs
and also support the pulling api is from
the ladies kernels so this is not a
trivial
so now let's we have three models I see
how I'm at the three models to use the
three models to model a real data center
just recall on the first Lysa this is
the abstract view of your data center
i'm using a it's like a factory or close
whatever clothes on topology just as an
illustration so you have thousand
servers like thousand switches and for
sure your centers instead of building
thousands over ten thousand different
nine future pitfalls I only have like
two types of bit felts this is very
marginalized designs so the first time
bit fellows I use is to simulate
rack of servers flat top of rack switch
so given a current chip which is long is
perhaps why I was able to simulate a
floor racks of servers plus a top of
rack switch and the second type of
ipj design is we dedicate just too
similar to switch itself we use the IP
GA to simulate the monster switches like
ten thousand ten two thousand years tens
of thousands like a switcher designs and
a for aggregated switch
erase wage and data centers which
switches so you can really do a detailed
architecture modeling there we connect
this to type AI PGA's using the high
speed 30 links well now the connection
is based on the physical topology say
you want to have a hypercube then you
just connect IP in the harbor cube way
so there's no restriction just just
wirings okay so we actually build this
Diablo prototype using a multiple of P 3
port this is actually a called developed
by Berkeley and a Microsoft Research
probably have heard of it the six the B
word each p3 boards has a Sonic's 4 x 5
PG A's
so six words we I use like 24 IP Jason
toll so this is the photos of our
cluster and we actually populate all the
ports the maximum memory so has a decent
memory capacity it's a 384 gigabyte the
Rams in total but think about number of
nodes or trying to simulate that's still
not that much it's roughly around 128
megabytes per simulated note but we
think it's kind of interesting enough to
run some networking applications we're
actually working on the second
generation trying to solve this problem
and also if you look at this the
bandwidth of the memory that's really
great
right there are 48 checked around to
ddr2 channels on so you have roughly
around a hundred twenty one hundred
eighty gigabyte per second bandwidth and
each of the IPG has this one gigabit
this you see the yellow table this is
the one gigabit links we use this link
to just for the control so you have 24
times one gigabit links well a couple of
axes six servers just to drive all the
boards and which will be used for like
say the consoles right you'll see you
council through similar linux and also
provide us some disk of functionalities
and we connect this boards using sure
first bullet 928 expert notes oh yeah
those have to be scaled down so I think
yes so for the one like memcache you
were looking at on it doesn't actually
matter so in terms of the worldö size
this is I think this is really minute by
the board define self cuz yes when I
talk to like some Latin people they
think like if you have a hundred
megabytes and you can run most of
interesting protocol studies but I know
for some like storage applications you
definitely the gigabyte because just for
the cache is very large yeah and so is
this a balanced system and Department
user do I have to manually make these
partitioning decisions captain we just
use the naive partition like this you
don't have to worry about that too much
and I can tell you the performance
simulator is very scalable when we try
to simulate a 1000 notes versus 2,000
notes we saw zero performance loss I
don't think so yeah so we don't do play
trick with like a fancy on partitioning
so which is used most the simplest one
most of us great for one that will still
give you some performance and also I can
tell you the performance simulation
bottlenecks it's not actually simulating
the switches it's actually simulating
the server computations so you have to
optimize your server commutation
simulations in that case okay so back to
this we actually connect this the
prototype actually connect all the
boards using this like you can see this
asserted cables we're our own custom
protocol sorry protocols your loss of
those usually is running like 2.5
gigabit per second with these on your
second version which will have a passive
backplane to handle all the connections
you didn't even this cables and all over
all the whole system pretty nice it
consumes only slightly over one kilowatt
7.2 kilowatts but in terms of simulation
capacity what you saw here is that beast
is capable simulating slightly over
2,000 servers and they're also 96 3096
racks and there's also 98 60 minutes
which there so there are 90 think about
3,000 copies of the unix run
they're running real applications it's
very interesting in terms of instruction
simulation throughput on the whole
system can deliver roughly around eight
point four billion instructions per
second so just a little bit about the
human tation because lots of things
going on there I'm showing what I'm
showing you here is a tie photo of the
IP J's I used to simulate rack of
servers so on high level this is a full
custom FPGA design is lots of hand
design there are we use minimum
third-party IPS including so the only
thing we don't use on our self is the
Sonics IP use and this is pre patent
food design so we started ship to the
limit and so it's like a 90 percent of
block from utilizations and also we
close the application around closing to
95 percent look up look up tables would
be used and I'll keep in mind the IP to
use here is like five years old you 2007
PJs if you have this year you can
potentially have more pipelines there
and we're actually running the whole
circuit like a 90 megahertz / 180
megahertz this is a double palm today
that happened just make sure I have
enough pores on the block runs and if
you look at design closely right there
are basically two partitions right so
each partition has one host D run
controller and of 16 gigabyte di Rams
and have two server or 2 rango pipelines
plus to switch pipelines there and in
terms of size of use this model probably
can see so the NIC bottles and switch
models and processor in your pipeline is
roughly the same size and with the
folding point it's double the size
so you can you can guess if I have a
larger ipj how much I can put there and
so this is a picture has four server
pipelines and four switch switch and /
NIC pipelines there and it also has a
couple of sometimes they go one to five
a 2.5 GB transceivers there it depends
on which type of ypg which type of hours
you want to use so I show you some proto
a prototype with with a 6b bores
let's say what do you want to go with a
10,000 note system right so there are
some pack of little calculations and if
we stick with the current technology
what is really old 2007 five year old
technology you probably ends up using
mmm and I'm using like 20 to be bores
means 88 I PGA's and but if you look at
this year this is really promising right
so look at the the recent technology 28
nanometer generations on your dad
similarly 10,000 CPU systems you only
probably need like 12 boards you
actually calculate the cost of the board
assuming you to work house around 5,000
of actually doing the board be fine
right now it's the overall cost is about
the hon 20k um that's include the board
cause with the drm cost and the cost is
roughly half and half and next I'm gonna
show you three exciting case studies all
right this is very interesting
okay so the first one is we took we took
the habló to simulate a novel circuit
switching Network this is actually to
work I'd done with MSR and Mountain View
or with Chuck Thacker I try I was trying
to use that the upload is simulated he's
the initial version of his circuit
switching Network probably you familiar
with its network just to bring back
everybody up to speed and this is a
circuit switched networks more like a
ATM and he has more levels of switchers
and it's designed so I'll during the
period of three months my internship
there we actually a model though all
three level switches all level switches
and also we try to run hand called the
drive Harris or kernels with some device
drivers so we think this is very
interesting because we the outlook you
can actually capture some a performance
bottleneck in the early designs for
example we found some problems in the
circuit switching center town circuit
and also we think there are lots of
interesting things from the software
perspective while chucks design is the
last leaf network and there's the
harford doesn't draw any packet but that
we've found actually that because
everything handled by the processor
eventually and the software processing
speed can actually lead to the packet
loss and the second case study idea is
like going back to the
traditional packaging world so I'm
looking at a classic problem people
called TCP in caste throughput collapse
issue on this problem is actually
reported by CMU is the claim they found
it in a real production gigabit network
attack network attached storage but on
the setup the problem is very simple
right so you have a this is the set up
set up to powers you have a shared
switch and there's a shared file server
broth-based the file server they also a
couple sign their senders connect to the
same switch so when you measure the
throughput the bottleneck link which is
the link between the shared server and
the shared switch and you plot the
throughput curve versus number of
different number of senders or receiver
whatever um number of clients and you
see a curve something like this right
this is the throughput and this is
number of senders and ideally you won't
see the throughput curve is when you
increase number sender is gradually
going up and cetera to this peak but in
reality in practice this is the curve
you saw and people conclude it right so
people have done natural simulations
using in nice to the people try to
experiment on small scale caster's
researchers conclude that this is caused
by us small switch buffers and also TCP
retransmission time out in the kernel is
too long and so that's their conclusion
okay so when I look at the this problem
I actually talked a couple people's and
especially industry side I'm asked them
if that's a real problem some of them
told me well they are pretty artificial
will only happen a specific set up so we
actually look at how people did their
experiment I actually found this problem
actually only happens under us really
specific like pre setup it's happened
with a small block size usually less
than 256 kilobytes a request site and
happens only on a switch with Rochelle
oh buffers so the first thing we want to
do with the app load is to see if we
recreate this set up like particular set
up can we reproduce this through craft
problem right so this is the graph this
result you see from the D uploads from
the simulation is actually we're trying
to model a gigabit switch with Michelle
a buffer do you be the squidge and we're
trying to use around the DC being cast
the coal actually mentioned just to be
fair
we're not using Berkeley coal we're
using coal from Stanford networking
research group and this is the the
throughput curve EOC and also we try to
change some like a confucian data point
conversion points in the system say
scaling of the cpu performance to see if
that affects I also changed or Isis
calls people using applications
well actually didn't see a lot of
difference than the gigabit level so
this is a throughput claps and that's
what you see on a real comparable
physical setup so this is great so we
can reproduce problem okay since what
the app allows is really a simulator the
question we like to ask is what if we
upgrade the whole interconnect to a
tanky a bit all right change the switch
back and change even the server was the
sort of performance I are saying what if
we that's how the system will perform
what if we have a faster server and also
there are lots of things you can play
with right you can say recall this
program right the original program is
written using standard pthread plus
socket blocking Cisco's but if you look
at the most recent that server
applications they are actually using a
pooling versions of with more responsive
versions of OS Cisco which is usually
this called a pool and so what if we use
a pool right it's like many other many
current services like memcache you are
actually using this versus a pea threat
and what if we changed this and what
redo the same experiment now what a
throughput collapse will get right so
here I show you a simulator thank you
switch and with a bunch of things I
mentioned like say I'm simulating a
forgiver servers and also versus a
cookie her service if you look at the
original color I using the Stanford I
modified the code and just change your
switch and this is the one you get right
so under you still see a through foo
class that very early
and there's no significant difference
between the Fukien hers and to give her
server with foggy her slightly better
but not that much but once you start
looking at always right let's let's
change the Oasis color use let's change
the Cisco to be poor right you got a
drastic different curve on the left-hand
side and you see the great difference
between four gigahertz versus to get
hurt and moreover if you see this like
collapse on site points actually make
shift it to the right so this is very
interesting so first we were happy
because we can reproduce this simple
problem even if people call it
artificial but second we found once you
trying to scale up the whole system say
change your switch your system
performance bottleneck my shift to a
very different skill and we thinks
looking at a wise implementation human
application application larger extremely
important like many of the current work
done with this simple problem people
just run and that's - there's no
competition model
there's no wise at all and also we think
fixing the retransmission timeout timer
is not really like fixing the real
problem you have to look at where the
root problem of years the little problem
the through procaps is then you start
developing some methodology or some
probably new system design to solve this
problem so the last one the last case
studies everything is very interesting
and we actually run modified memcache
service on a really large scale setup so
we think this is exciting because it
reproduce reproduce someone phenomena
people saw in your large-scale
environment in production and also on
this is probably the first time academia
will be able to look at problem I
thousands of scales and previously I
talked to people people can only afford
to see a couple hundred nodes from
academia but now we're moving a holder
magnitude better it's it's like ten
thousands of versus hundred so mam
casually are just like probably you
familiar with that it's just like very
popular distributed key-value store
application you
by many a website like say Facebook
Twitter and any large website there's a
memcache basically likes its name and
actually the code we use is a modified
memcached this is now the Neyman cache
the tiny meanie whatever this is a
modify the cold download the from their
site and compile our platform with the
comparative spark and the clients we use
it's actually colder with a middle wire
later on top of leave non-cash this is
the application level that will be used
to handle application level view twice
and things like that so in terms of the
workload we use ideally we want to have
the real workload but you know well
universities we have some limited access
to the to the locks but Facebook
actually publish their ups that this
were closed at this in last year's
signatories so we look at the paper and
build our own workload generator and
verify our own accordingly real Facebook
data so we think this is probably the
best thing we can get ok so the first
thing we have a we did with this is
memcache is set up is to trying to
validate our result at small scale right
large scale is pretty hard about it but
for small scale we can we can build we
could have a relative equivalent system
physical system set up and run the same
experiment same a programs on both this
system and see if we can get the shape
of the curve right so I'll so in our
validations we do is we're trying to
build a 16 node cluster with the 3d
heard Xeon processor just pretty regular
with a 16 port a give me the switch and
in terms of configurations all the 16
servers we use two of them to be the
memcache server and the rest them will
be the clients and will scale up the
scale that number of clients from 1 to
14 and then plot the throughput curve
and latency curve and so on so forth and
also memcache is a real application
right there also parameters
configuration you can just change for
the server side so what you can change
here is you can actually change the
protocol use TCP versus UDP and which is
better and also change number worker
threads of the server say for threads or
a threat and keep in mind on the
simulator server we have
the single quorum are all running at 40
Hertz with a fixed CPI Tanya model
versus this guy this is a different I
say and different CPU performance we
don't expand with an absolute number
right but we actually expecting we're
getting what we're looking at is the
shape of the curve the trend wide scale
so that's what we care about most
so the first thing we look at is trying
to do the validation about from the
server's perspective right look at on
the server application throughput so
this slide shows you the location
throughput we measure the out load which
is simulated versus the thing we really
measured on the real cluster we set up
so x-axis of this graph is number of
clients like 1 to 14 the y-axis is
application through prove measured from
non caches with non stat in their face
so it's from like a smattering kilo
bytes per second so you can see under
different side all right so Diablo can
successfully reproduce the trend of the
curve and to our surprises because this
mic I opened it on the absolute value is
this more close to the real thing so if
we're happy so now let's look at from
the client side right so we measured for
each of the client we run we actually
measure analog old replies the queries
will measure the client quarry
latencies and we did a similar
comparison graph as the previous slide
and right again right hand side is a
real class or life hand side is the
Diablo as you couldn't see again we got
we work out the trend right so the most
of the quarry finish reasonably fast
less than 100 microseconds the only
difference is that
Diablo the set up we have here is
probably slightly the latency started
better than the real set up because we
didn't actually calibrate our nose but I
mean if you want dude absolute value you
can definitely calibrate you know how
you can get a closer curve but in terms
of the trend of the whole system this is
very good so the next experiment is the
most exciting part of the thing we have
seen so far it's actually worth taking
that
and really is trying to study problem I
large-scale like we try and study the
problem to scale up to 2,000 notes since
this is simulator so you can do many
tricks I can you can play around many
hardware setup there right so you can
similarly different interconnect for our
case we're looking at gigabit
interconnect with a regular micro second
level poor poor latency switch and also
we look at what if we change the whole
inner connected thank you a bit and
improve our switch latency by a factor
of 10 on everything like throughput
latency effector time will happen to
application I know this setup is a
little bit more aggressive but show you
some ideas what we can do with it Diablo
at the scale and when we scale up this
application and us go up this experiment
we're trying to be moderate we're trying
to maintain the server versus explained
configurations so we still have to
memcache is servers per simulate Iraq
and with rest of the clients and while
make sure all the server load are
moderate and we don't want like pushed
everything to the clave that's not where
people are running today so the server
utilization we measured after simulation
is roughly around 35% and there is
actually no packet loss actually measure
night but when I create this experiment
okay so so one problem so like a
large-scale is this problem people
called request latency long tail right
this actually identify what Luis Barroso
from Google should talk about this
problem and it couple two years ago
after CRC keynote so we're actually
trying to use the able to see what if we
can see the similar power my Laura skill
so this slide actually shows you the
client request latency distributions
this is the PMF graphs on with x-axis is
in log is the microsecond y is the
frequency or and also you see two types
of there are lots of this graphs
actually tell you many of things right
first there is a front end giving there
canal which is the dash line there also
solid lines for a given their connect
and the first thing is we actually see
this throughput oh sorry the latency
long tail problems so which is
most of the quarry's line requires a
finished very quickly and on order a 100
micro second
but her still some query maybe 1% of all
eyes 11.5 percent whatever finished a
lot slower than the rest of the chorus
and for some reasons right to order
mining to slower and also if you look at
this like gigabit switch we actually
classify the types of quarry into three
categories right so the Y is just you
because this is really sizable system
some lacroix just keep the local
memcache server some we have to traverse
once which one aggregates where some has
Travis true so actually plow the later
latency of distributions of all types of
queries then you'll find once you have a
sizable system most of the core is
actually going across your rack is going
off the rack so that's very common so
that's the where it dominates so and
also you will find if you traverse more
switches and he gets in this guy and you
have at this peak is getting a lower
than that this is a local cult this is
the one hardest to high significant
lower that means there if you travel
with more switches and they're the
variations your quarry will be more I
think Louise this year he wrote a paper
talking about the cause of this latency
long tail one of the relations that you
have once you have more switches or more
mounting dumpty revenue sticks in your
system so that actually capture that and
another great thing is is you can
compare it right the result would thank
you of is switch versus one give you
switch
keep in mind everything is 10x better
than 10 giving set up actually does help
on this latency long tail issue and
fewer requests will finish slower but if
you look at absolute value of this the
requests are finished by the time you
begin your connect there are now ten
acts better in terms of vacation latency
stride is just no more than like two
acts if you again look at Louise talk
right he mentioned you look at a portion
of time the query latency you spend
right it's the time you spend in a
switch in hard word only accounts for a
very small portion of the overall there
out of the time you're just spending the
Oh as a kernel stack and this again
Diablo help to identify this problem and
also OS networking stack conversation
again very very important okay so since
we have this excellent vehicle right so
you can see problem a large skill so
what one big question like to ask the
impact of the system skill on this long
tail this is what makes the deal is
unique so let's look at the result we
have in a previous slide you saw the PMF
distribution now let's look at the CDF
curve right it took one and one curve in
previous slide so if a Google right so
you know you're not actually interested
in this part where most the core is
finished reasonable fast what are you
actually interest in this part this is
the tail part of the whole request this
is where God makes the whole thing more
interesting in terms of Remini or
anything right and so first let's see
okay let's do mean this CDF curve a
little bit right so so let's do mean
this like curve had answer questions I
say if we have a larger system does he
like hurt the latency problem will help
lacing problems you can see a long tail
problem so apparently I'm using here
just you this is to mean that to me in
the curve of the tail part is 96 this is
like a hundred and now you only use like
that also data points I don't use a one
configuration because other confusion is
similar just show you the ideas then one
I use here is thank you beginner can I'm
running with the UDP protocols and
ideally you want this curve closer to
the rule that means fewer things falls
in the tail but if you plot this curve
resuming curve with tanking a bit here
right
you see clear trend right so this is for
500 note this is a thousand note this is
what 2,000 note so you have more notes
more servers in your system apparently
this tail problem is more obvious so
this first thing okay
chanax because if demon a special
academia they've really found out like
looking at a system shows like hundreds
of nodes because the physical
limitations
and they try to draw some conclusion any
people draw conclusions just with a
hundred notes and so let me ask this
question so we will draw the same
questions if you I give you a platform
that's capable of looking at problem I
get thousands notes just let it I say
for this case look at the tail
so we all have a different conclusion
different answers if we have a larger
scale system now one very simple
experiment we can perform here is they
change all the protocol software
protocol we use for the servers like say
tcp vs UDP right which is better given a
specific hardware setup which is better
in terms of the minimizing the long tail
right this is a very simple experiment
we're going to so let's first look at
the experiment and we have done with a
gigabit the traditional gigabit
interconnect right so again I'll plot
this like zooming the curve the CDF and
I will you keep the same coloring scheme
consistent with rat represents the UDP
protocol and with a blue one represent
the TCP protocol so clearly at 500 knows
a gigabit interconnect setup you see
well
UDP is definitely better twice because
this curve is closer to the roof now
let's move a little bit Stepford life's
code book had a thousand notes
well apparently very interesting right
don't up to a thousand notes you don't
see significant difference between the
ECB and UDP and will physically slightly
better right now let's move a little bit
further I took two thousand notes but
this is a result for two thousand notes
in this case TCP output for show up
perform UDP okay so I can draw a
conclusion now so TCP is a better in
terms of scale but what if we change the
inner connected to ten gigabit right so
whereas you have the same conclusion
like TCP is much better choice in terms
of latency long tail let's let's redo
the same experiment right so first again
five hundred nodes right here you can
see TCP already outperformed UDP right
so for it is like five hundred note
setup either 10 gigabit now let's go up
to a thousand knows what this time what
you
conclusion reverted it's apparent UDP is
better now let's move - mm no let's see
if UDP is do a better choice okay this
is the curve we got 2000 notes there not
much difference it's almost the same so
this is really interesting because we
think that long tail problem it's really
a convoluted problem during multiple
factors contribute this issue and it's
now the linear function so you
definitely draw the answer which one is
better
so there many are I believe there are
many other things happen at scale and
diabolos haven't able to help that and
and when we did this like large-scale
experiment we also found many other
interesting things at large scale and so
sure yeah I have some like rough ideas I
think there's the problem is like some
mentioned like your loss of IQ software
Q's in the networking stack and the way
the current networking stack is the
handle is not really deterministic so if
you look at the drivers right you don't
just look at it as switched itself there
multiple factors of I believe but
software the wise networking stack is
the big problem right there's the way
the design is it's actually optimized
for your internet it's now different
istic its work in the pouring pulling
models for most of the queues the way
that the OS drain the queue so it's now
it's not like you have a packet and you
will send to the software it's really up
to the OS and decide when to notify on
vacation so I can't tell you the exactly
why this happened there I believe there
are more things you can do but this is
not a single cost by a single problem
it's probably multiple it's a probably
system design issue right so let's look
at the other issues sure how much rows
or anything you can use with the
software you can do that and and also we
use hardware yes this is a standard
Linux whatever thing whatever you can do
with the Lin as you can do it
yes you can actually do that we can
actually generate I actually didn't use
it on out but I can give you the sum
distribution of the Hult the way the Q
is drained of the time and that's done
through the hardware performance
counters so you can you can you yeah
what have this like debugging
infrastructure there but not now for
this experiment I just like spending my
time finishing up I didn't look at those
but it's yes the answer is definitely
possible you can see more things than
you couldn't see on real switch design
yeah so yeah potentially this is a
architecture simulators right you can
you want to probe whatever you want and
and also the nice thing about ipj is the
debugging infrastructure running
parallel to the main stuff so it has all
0 perform of its impact yeah so that's
good okay so let's let's look at other
issues right other small issue so the
first issue is on because Facebook if
you look at there's a Facebook publish a
blog about their memcache choice like
the protocol choices and Facebook saying
we're running the UDP the reason for
that is because you need to consume more
memory TCB consume more memory than UDP
but vaction didn't see this right we
actually look at the server memory
utilization SATs kill they're pretty
much identical right so between two
protos but we think what really matters
is the the dynamic patterns of the tcp
versus UDP right so it could be cause
the memory consumption could be caused
by the server not evenly balanced like
we have more in a concurrent of more
in-flight
transaction half the handle for TCP
versus UDP but you can't simply draw the
conclusion TCP is definitely better it's
really little about load balancing and
another thing we found is well we look
at how people do especially system
networking folks too similar experiment
they tends to focusing on the transport
level protocols like say they they're
not happy with the vanilla TCP they do
lots of hacks in the transfer for their
control theory based transport protocols
and also the trying to hack the kernel
you say tweaking the timeout values
through the protocol but we found some
cases like when I met him it might be
just fine you have to what the original
focus is not just the protocol itself
you have to focus on CPU processing like
the case of
at easy being cast right you have to
focus on your neck designs that also
even Ohio's medication loud voice on
vacation in August coup shil
and also like I mentioned answered
Eric's question like we think the loss
of software cues like when you start
building the system even though when you
start writing the driver you figure out
the way the Karl in this out networking
stack process in the packet is very very
complicated on the loss of cues and
buffers just in the software stack it's
not just like there's a buffer in the
switch there are more software buffers
which is more known different istic in
software stack so why don't you just get
rid of this buffers unnecessary buffers
then build a super computing like a unit
rack connect directly on your sturdy
fabric right so now also we saw some
many other interesting like say we try
to change the internet hierarchies like
sometimes you have one like one one
layer somehow my two layers we found
adding another layer it's actually a
slightly olfactory server host theorem
usages I think that's affect your flow
dynamics this is also very interesting
to see a large skill and so in
conclusion so we believe ice looking at
evaluating the data center networking
architecture I knew things looking at OS
amplication logic and computation is
definitely crucial you need that and
second thing is you can't generalize
your result you got from a hundred nodes
100 100 nodes to a couple of thousand
those you see complete different things
that's a couple thousand scale and we
believe Diablo is is good really good
enough to generate this relative members
versus the absolute number because we
wanna figure out the trend right and we
think this is really a great designs
place proration
to i'll our skill before we really
invest shell like a million couple
million dollars to do the real system on
so apparently I learned a lot on doing
there's building this they're like this
is like a year lifers by the way so um
so be habló you can see now is this
actually designing for large-scale
system we're capable simulating
thousands of instances so there are some
people who asked the question so I mean
only interesting things happening within
the rack so we'll what Diablo still help
me
because he's using ipj is it supposed to
be slow my answer is yes
the reason for that is because we have
like 3,000 instance here on the
prototype we have you can actually
potentially run your experiment in
parallel um old experiment we run so far
finished overnight think about if you
have a real physical design and when
you're trying to run your experiment on
real physical design and sometimes you
couldn't afford a castle to do it in
multiple racks but you can you know if
you have to run experiment in sequential
but overall it's not necessary faster or
significant faster than running in the
simulator so once you have a 3,000 as
similar the instances great massive like
a simulation bandwidth that will
probably change the whole story and the
second thing is because the goal of
Diablo is very aggressive actually
running the real software code it's not
the micro kernels like researchers use
we're trying to bring up the whole the
office Sheldon is kernel so we found
there's some lots of issues in the
software that the real hour they have
box right so the first actually solves
very interesting things when we pour
Linux or there's a couple of times we
have the changes to change the hardware
to work around this Linux issues Rises
just because we some time to change the
software and then the problem show up
again and then when they are decide okay
we have to change the hardware to make
sure we can run this legacy Linux and
also we actually found that there's some
Linux actual is it's a product of the
hackers write the software hackers they
don't actually follow the hardware spec
but when we design the processor wait
what's wrong
okay so when we what's our weight is on
the Diablo no well actually follow the
spag from read I say it's work all the
verifications we expect the follow stack
and wow thing we found is therefore
sparkers a processor stay register right
but there's a third ability stir but
some of the business for reserved means
for future use software shouldn't use it
but the current implementation with spar
processors terms of its power processor
they they allow some of the reserved
bits this is writable and readable and
the Linux kernels actually use are one
of the bits
or really crucial information like say
to indicate whether or not the current
processor in the OS u OS kernel
application mode that I will be used to
clean up the whole OS kernel stack so we
actually found this is what we found out
this bug purely based on Locke because
we followed the harbor's back but
hackers know they they have a machine
they just called for it it works it
works so now also we look at the outlet
of a6 IPG system that oak who has half
of the server rack right so this is a
massive scale simulations this is like a
real machines like think about a member
of the Rams you plug in the numbers from
a Google Google's number four for saw
priority around you will see real
airsoft errors in the system we actually
saw dirty reminders and also we saw
errors of from the certain links we have
lots of Serling's write it down not a
hundred percent reliable we'll see won't
wearers for sure per day and also we
actually descends up to like because
we're using the memory in different ways
and stuff some of the node just dies you
know simulation um we think the fame
methodologies are over is great right
this is a it helps your society problem
like large-scale help us to get closer
to the hardware side and but what do you
think is a need a really a better to
deorbit and now I called everything with
the velocity in Verilog so I'm a hugger
so I have to be frankly saying is it's
not a very productive to to build things
so we're actually working on second
version maybe there's something else can
help us like say I'm a hardcore very
long guy right even even I would say
this is hard so you have to think about
whether or not you want build a system
just rely on your lock so again Diablo
is actually available from this website
you can go look down for more
informations and also we will plan to
publish all the source codes out there
and there will be paper upcoming and I'm
currently building the second generation
of the Apple hardware and we'll talk
about how we seemingly 10,000 CPUs now
with enough memory the next versions ok
so with that I'll be happy to take
questions
so you said you try to avoid all the IP
from the FPGA that particular reason
yeah table actually works that's my
Franca's yeah so like say the memory TV
I'm controller now reliable enough once
you have lots of clocks clocking a clock
domains for science controller it's now
reliable not besides I don't like their
controller because there are lots of
hard-coded like routing constraints in
those things and also the reason we
these are our own IP products for
example assert ease right you're not
really using all the future in the PCI
Express you're now using all the future
inside that drives and also you won't
make sure the whole thing is reliable
enough so we build our own
retransmission schemes reliable schemes
there and basically going to take it
like TCP is equal to what he like the
hardware TCP with window size equal to 1
and also for the control product was an
Ethernet right once you hook up lots of
ethernet cables to the servers you see
software running on the access server
will drop the ethernet packet so there
must there are also much some reliable
guarantee and the reliability guarantees
in your hardware design so you need to
redesign the whole it's not just like
one yeah so we have to think about
don't-don't-don't crack up the shell
sonics do you whenever work I mean for
some people but not for me ok does your
target OS support it's a it's a it's a
full Linux you support anything
supporters yeah yes I just didn't turn
it off yeah
so is it possible that you can
virtualize yeah yeah actually uh now
through the second version the second
version will keep you physically
you bite person rate the servers and the
new server is not gonna be single core
model it's gonna be multi-core and we're
gonna the idea is like you mentioned
we're gonna use external flash as the
main service to a while you're really
multiple gigabyte DRAM storage and use
the DRAM was the cache so this is
through pool optimizes multi-threading
these lines of latency will be hidden
hopefully and based on our current setup
like say I mentioned like 1000 knows
like three boards versus six ports I
don't see many actually the role --roll
perform a simulation going down so it's
very it's great and besides the
synchronization between the models the
high-level model run on tea from my PJs
since we control the protocol design
ourselves we've done that the round-trip
time is like 1.6 microsecond cooling all
the payloads I don't think you can do
and if you I don't think any of the
existing parallel soft simulator sim
self saw parallel software simulator
will beat that and I don't think I don't
see to the best of my knowledge there's
a equivalent software simulator that
allows you to the same thing yeah
question in the back I think most people
give us a repair speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>