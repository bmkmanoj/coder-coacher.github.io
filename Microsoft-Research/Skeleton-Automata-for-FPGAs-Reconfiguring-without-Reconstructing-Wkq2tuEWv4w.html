<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Skeleton Automata for FPGAs: Reconfiguring without Reconstructing | Coder Coacher - Coaching Coders</title><meta content="Skeleton Automata for FPGAs: Reconfiguring without Reconstructing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Skeleton Automata for FPGAs: Reconfiguring without Reconstructing</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Wkq2tuEWv4w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
hello everyone I am candygirl from the
embedded in rican credible computing
group here in research redmond and today
it's my pleasure to introduce luis woods
lewis is a third year graduate student
at eth zurich and he also will use to be
one of our interns here is research
interests include fpgas in the context
of modern databases parallel algorithms
and stream processing and pattern
matching and with that loose thanks Ken
for the introduction and thanks everyone
for coming so this the work that I'm
going to present this joint work
together with Jen's toy dinner and chong
Ling Lee from the systems group in ETH
Zurich and the title of this work is
skeleton automata for fpgas
reconfiguring without reconstructing so
let me just start with a very brief
introduction to FPGAs I know many of you
know what they are but them I like to
think of them as soft hardware so soft
in the sense that you can change them
after manufacturing but you know once
they are programmed they behave like
real hardware and so they consist of
conceptually of these two layers a logic
layer which is just a pool of hardware
resources and uncommitted pool and then
a configuration layer on top which
allows you to you know define how these
hardware resources interact and together
construct the real circuit and so by
software update the behavior of a
circuit or actually really the layout of
the circuit can be changed now in in my
research I am particularly interested
how these types of of hardware can be
used in database systems also how can
data processing systems benefit from
this and one question is the question of
you know how do you integrate it into an
existing system do you use it as a
coprocessor where you load data over
there to do a lot of number crunching on
it or do you do something like I'm
proposing here where you say you
integrated in an existing data path so
this data path could be the network or
it could be the direct link to storage
and you can take a chip and put it into
this path and have it process in the
streaming manner the data as it flows
through to the CPU anyway so for
instance as I show in this example you
could use it to maybe pre-filter and a
lot of data or you could use it to you
know do pre-computation or auxiliary
computation on the data as the data
flows by so how is the work between CPU
and fpga divided you know the goal is to
extract the parts of your program which
the FPGA can handle efficiently but
these are probably rather rather simple
things you still need the GPU because
it's much more flexible you still in a
full-blown query engine you cannot
upload everything to the FPGA so so the
key challenge sort of is how do you make
this separation and for this to work
well so there there there has been some
research on using FPGAs for databases
and there are also some commercial
systems that do this so at the one end I
want to sort of present to two extremes
innocents on the one hand there is
something like this glacier compiler
glacier was developed in our group a
while back and is it is a query to
hardware compiler so you give this thing
and i'll get an algebraic query plan and
it translates this into a circuit
defined in a hardware description
language
Verilog or VHDL then however you cannot
put this directly on the FPGA you have
to do a number of steps on it such as
synthesis place and route to map the
circuit you define to the particular
FPGA device and this takes some time
this is not something that you can do on
the fly so so this approach here works
well in in for instance a streaming
scenario where you have long-running
standing queries which you want to have
implemented in hardware I'd say you're
in a true in network intrusion detection
system and you have certain patterns you
want to detect on a data stream so so
you might have new patterns you know as
time evolves which you want to put on
there on a nightly basis or on a weekly
basis but but not very frequently so at
the other end there are systems like
mateesah which are mateesah is a data
warehouse appliance which also uses
fpgas inside its architecture and here
you don't compile query full queries
into hardware but you rather put a fixed
set of operations in on to the FPGA so
things as decompressing data efficiently
to speed up transfer times between disk
and in deceit and the final system or
you can do simple filtering of data such
as projection based or restriction or
selection based filtering you can do
that type of stuff but it's a very so
it's it's a you don't fully compile
queries you just do a little bit of it
and the more complex stuff will be done
on the cpu and so the work I'm going to
present here is is um about this type of
filtering done on XML using a technique
called XML projection this technique
extracts
filtering expressions from a query and
then pre-filters the data before the
final x query engine then runs on a
reduced set of data and the goal of this
work is that we don't want to have the
compilation overhead which we saw in
glacier and yet we want to make this
filtering as expressive as possible so
we don't want to just simply filter you
know on columns or so we have here a
more complex problem where we really
want to have thrown yet because it will
be operational and some liquid
joint video some benchmark plays on TV
schedule
so how long does it in the nation
fremontca generator okay so so if you do
the full compilation this will take
definitely several minutes and then it
depends of course how how how complex
your circuit is then it can even take
hours but but it will definitely take a
number of minutes yes your optimizer and
get a plan yes and laser takes as input
a final physical plant translates into
HD yes and then there's a cost or so and
that is one pockets I can just take you
to be HD programming right so also the
compilation that first part is not not
an issue but but the second part is the
issue because the complicated thing is
you're now taking an abstract circuit
description and you have to translate it
to the to the to the hardware which the
FPGA provides you and you have to do the
rotting and this is a really is a
complicated process so this is not
something that you could do that you can
do efficiently so so yeah so from from
from very low go VHDL to bit stream and
loading the bitstream ulsan the fpga
that's a process which takes quite some
time
so the alternative is to not reprogram
the fpga but to just you know have a
generic circuit which you can modify the
way you like so i think i have to give
now next I'm going to give an
introduction to what is xml projection I
already highlighted the idea a little
bit but let's run through an example so
here i have an xml document and on the
right-hand side i have a an xquery so
this exquisite does it from this
document basically selects all the item
elements which are descendants of the
region elements and it then returns for
each of those item it returns a new item
you can do this in xquery like this you
can generate a new item which contains
the name tag which match so the name
with all of the the entire subtree of
name here it's just a text fool but it
could be entire subtree and it generates
num categories new element nom
categories which contains an aggregate
which is this the count of these two in
category element okay so we say okay
this is a complex query which I mean
xquery is a is a very complex language
and we cannot run this full query on an
FPGA however in 2003 there was a paper
by Marion and Simeon who talked about a
technique for projecting XML documents
and they showed that you can actually
from any query infer aesthetically a set
of so called projection path now these
are this is a subset of X pass used to
express these paths and these paths
define exactly which parts in the
document will be untouched by the query
so you can actually before you run this
query you can quickly extract these
paths and then filter out all of the
irrelevant parts
I'm off the document and then you can
run the xquery on the final filtered and
result in a document and it will give
you the same result so if we take this
idea to our hybrid architecture which
I'm proposing then the server running a
full-blown xquery engines such as Saxon
or amex query or you name it before it
runs the Experian junne it extracts this
pass and puts them on the FBG and this
has to happen obviously quick enough for
for this approach to make sense and then
the data is streamed not directly to the
FPGA or it's it's it's not not directly
to the server but it's stream through
the FPGA and the idea is that the FPGA
really operates as a stream processor in
the network which just reads on the 1n
droid original document and then the
other end produces the filtered document
that's that's the key idea so with that
the next question is so how do we put
these filtering expressions on an FPGA
so how do you how do you how does the
corresponding hardware look so here is
an here is a such an expression where I
say from the root I want to match
descendant which is a which has a child
be which has a child see and then that
shall have a descendant that means the
descendant means somewhere in the
subtree an element D so one this is a
similar problem to regular expression
matching if I if I in the regular
expression i can express as a
non-deterministic finite state machine
order a deterministic one and the
example here is a non-deterministic
finite state machine where I say okay if
I match a I go into the next state if a
match be I going to make state a match
see then I add a clean closure meaning
i'll stay in this state now I've omitted
one thing here on purpose namely that's
not quite the full story what is missing
is we're not keeping track of closing
tax here right so in XML we actually
have to keep track of once I'm in this
state see once I matched ABC those tags
are open when taxi closes again I have
to I have to take care of this somehow
and this is typically done with a stack
and I will later show how how we added
this stack as an implementation detail
into our cyst anyway if I if I for now
for simplicity if I just want to take an
NFA like this how can I translate it
into hardware and here I give you an
example so you have what you have on the
FPGA is you have flip flops and you have
logic and you have a lot of that so we
can store the states of this non
deterministic finite state machine in a
flip-flop flip-flop for each state
telling me whether this state matched
right so if i if i go from q0 into q1
then I and I so when I match this
predicate aid then I can put a 1 into
that into this flip-flop and end with
the gates I basically implemented
transitions between all of these states
right so so if I here I'm assuming an
external tag decoder so I mean external
some black box which sort of reads tags
and outputs them in our implementation
actually we will have local replicas of
tactic odors because that work more
efficiently and so the gates anyway they
just take the previous state from the
flip-flop and the current tag and then
define whether they should go into
active statement
you will need a parser yes and I'll come
to that in a second so this here I'm
just looking at the xpath expression
assuming I already have a parser right
in front of it yet okay so so so okay
and again I said this now already
several times if we want to now of
course we could compile these XPath
expressions into you know particular
circuits and load them onto the FPGA but
we don't want to do this because this
just might take too long but what we
observed is that actually these xpath so
this is a restricted set of X pass we
have for instant only forward only
navigation so the automatons or these
nFA's which we construct are always of
the same structure they might change the
semantics might change in the sense that
whether I'm matching attack for matching
attack bar and what also changes is
whether I have whether i'm doing a
descendant navigation step which is the
child step but other than that sort of
the really the hardware structure really
stays this stays the same so so we can
we see that we always have pairs of a
node test and a navigation step yeah so
it's always descendant and a or child
step and B that's that's a repetitive
pattern and this we want to exploit
exactly for our idea of skeleton
automaton so this is this is really the
key idea behind this approach namely
that you define this skeleton which is
the same for all the XPath expressions
and load this on the f onto the fpga
ones and you leave those parts which are
expand which are which are specific to
to to a specific xpath expression you
put this on their load later so you just
have this skeleton which you load on the
fpga when you boot the engine and later
at runtime
all you have to do from your XPath
expressions you have to just extract the
semantics basically and load them into
there and this can be done quickly
because all you're doing is you know
updating a little bit of memory okay and
again this should be quick so you can
have highly dynamic workloads and and
and you can do this so so in numbers you
can do this in you know two or three
microseconds versus several minutes of
compilation time so that's the key idea
and from here I will now in the next
couple of slides before i go into a
valuation show a little more of the
details and in hardware because there's
some things missing which i haven't yet
talked about so in terms of architecture
you you asked about the parser so yes we
have a parser in front of this so that
the xml stream runs through this parser
or it's actually more of a lecture which
annotates the data stream with lexa
graphical information which is then used
by these expose circuits okay and so
this parcel reads one bite of one
character per per clock cycle and it's a
just a large state machine which says
okay this is the tag start or this is um
here is the end of the tag or the end of
a closing tag yeah and then this
so-called cooked XML flows through these
segments matchers and this is actually
how we're exploiting the parallelism of
the FPGA so it's a pipeline parallelism
which we're doing here so we're
streaming this data through this
pipeline of segments matches which are
daisy chained and then a small detail
here at the end is the serializer that i
don't want to say too much about it but
that just makes sure that what comes out
of this engine is valid XML because we
don't we want the xquery engine to be
completely oblivious that in front of it
an FPGA is filtering out
so we cannot just give it the the
matched part you know we cannot just
strip everything out of it which did so
we cannot just give it the parts which
the XPath expressions matched we have to
embed it in a valid XML document so we
have to keep the fool pass to leave
notes and that's what this Cyril Iser
does ok so this segment the skeleton
segment this is now the this is as
technical as I'll get is at the core of
this and here is a diagram how of the
architecture of this segment skeleton
segment so again it represents a
navigation step plus a node test so
saying am i doing a descendant or a
child step and you know which tag am I
matching so the configurable parts of
this are tagged predicate that's a i'm
going to write in there into a ram you
know if a magic tag foo i have to store
this in there right if I'm matching
their tag bar I have to start that
information in there and a configuration
parameter which says what kind of an
axis am i doing and each segment matcher
can do both it can do both or all for
navigation steps but and we configure
which one to do by writing into this
thing and then you see these matters
have data in and a match import than the
data out and a match output so they're
all Daisy change the data is passed from
one match or to the next to the next to
the next and the same holds for the
match in and the match out signals and
the segment core takes all of this
information and defines whether it has a
match or not and then the final missing
part here is this thing here at the
bottom of this history unit and what
this is is the stack which I mentioned
what we're not taking it into account
for if we just look at the regular
expression so
if we have a match we put this into this
stack it's essentially a 1-bit stack we
write a 1 into there and as we go down
in the tree we shift we shift this
information to the left and as we go up
into the tree we shift the other way
around and if it's a if it's a child
navigation I'll just shift that one up
and down and if it's a descendant
navigation step I'll you know once I put
in a 1 I keep shifting in one's yeah and
so here we used 16 CS 16-bit shift
registers so meaning we can go up to a
depth of 16 this is reasonable you don't
you typically don't have very you know
extremely deep documents if you have you
you could change this on the other hand
you could also say well if I have an
overflow here then I just you know i'll
stop filtering you know you can say
that's always my backup that's just the
worst case send you all of the data okay
and this is a detail about the
configuration how is it done configuring
at runtime we use for this processing
instructions that's a feature of it is a
in the XML specification and we the key
thing is we embed this configuration
also in into the byte stream and this
can be recognized and then we actually
configure you know as this flow through
and and in terms of how fast this is
well we can process one byte per clock
cycle so depending if your ex pass you
know consists of 50 bytes for instance
then the reconfiguration in terms of
throughput is um yes 50 cycles which 125
megahertz clock would translate to 400
nanoseconds okay
so so this was all about how to build
one all of the details of how we build a
single pass now we want to support more
than one path we when we analyze such a
query we typically have something like
15 paths or something like that now we
we don't have to change this
architecture to do that a lot we still
keep this chain of segments matchers and
all we have to do is pay a little bit of
attention at the beginning and end at
the end of the pants so at the beginning
at the beginning we have to make sure
that this segment at the beginning
behaves as a as a root note basically
and and that's not difficult to do the
details are you can just initialize this
history thing with a 1 and and make sure
that you are not dependent on the
predecessor and the other thing is now
the question is what do multiple paths
mean for the end result so here we're
saying if either one of these paths
match then we have to keep that part of
the document so so we what we want is
the union of all of the past so we just
have an additional global match signal
and we can configure a segment at the
end of the past we can say you're an end
of change segment and so at those
segments what will happen is what you
see here in the bottom that the previous
global match signal is merged together
with the local match so if any one of
them has a match at the end it will
propagate all the way to to deserialize
or seeing again which is responsible for
outputting then the data yes
sorry hanging here PJ I have them in
sequence and they really sequence or
today working bad about forgetting
so they're laid out in sequence they
operate in parallel but but pass one
will match before path to but so so but
they've passed one matched together with
this data stream that match will
propagate along so the data just flows
through this pipeline and matching
information is merged into the stream
basically so it's all the sequence
operating in parallel that's what it is
actually okay so yeah then this is sort
of the big picture we have this parser
end and the indie serializer at the two
ends and then we have this chain of
segments matches and we put as much on
there as we can and and the FPGA is a 2d
array basically of resources and a
pipeline like this tends to map nicely
to the hardware because there's only
small neighbor to neighbor communication
between the elements so the tools are
very good in figuring out how they can
put this deer all with other designs you
can construct other more complex designs
with transitions all over the place and
which don't work as well but this one
works very well so a little bit of
evaluation first some performance we
looked at we we measured the saxon y ESO
saxon is a is a is a sax based XML
engine or experiencing and the e is the
commercial version so we we measured the
speed up in parsing time on this engine
with and without this projection and
what you can see is on a hundred
megabyte XML instance the speed-up was
was a around 66 a takes somewhere around
there so this speed up in part time
it was actually significant by reducing
the document and part time is indeed
really an issue for XML applications in
particular it's something that's not
it's it's a inherently serial process
which is hard to paralyze and so if you
can just make the document which has to
be parsed smaller then the departing on
the xquery engine which we didn't change
at all will obviously run faster the
benefits in the execution time of the
query which we measured were not
significantly or not significant this is
known about projection I think the
reason is that once you have your
optimized data structures in main memory
then the queries run very efficiently
over these data structures anyway so
they won't touch the stuff which you
don't need anyway so there's no benefit
here in many of the queries the
execution time was much the smaller part
in the parsing time but you have also in
this exmark benchmark which is a is a
standard benchmark for xml benchmarking
you also have a few expensive join
queries where the execution time then
dominates but in most of the queries the
execution time was less dominant than
the part and then finally and this was
the original reason for projection
memory consumption is again the
improvements there are quite cific
significant since you have a smaller
document and the data structures in
memory are a multiple of the original
document size so often it this really
blows up yes
what does a refill agenda combined with
the correspondence of resolution
and yes I have a backup slide on that
yeah we can I might as well show this
one now so that's true saxon this is
unfortunately a bit graph which is a
little bit hard to parse but saxon has a
switch so the commercial version has a
six switch for software based projection
so it implements this and and we ran
this as well so that's what you see with
these striped things here so this is not
this is not this is not a stacked graph
or anything and the message here is so
it has no effect software projection has
no effect on on a part-time right
because you did the document departs
it's the same size execution time it
doesn't have so much of effect anyway
and memory consumption we were a little
bit surprised that the did the consumer
II consumption improvements from the
hardware projection were much more than
those from the software projection okay
so i have a few few more results before
I conclude on this talk so the one thing
is about one question is well how many
you know how much of these segments
things can you put on the FPGA so here
we use the not a very large FPGA this
was a xilinx vertex five so it was on
the eqp v5 portal so i shall say a
medium-sized FPGA and so i did I didn't
talk about how we use beer am at all
that doesn't matter but you have you
have basically two types of resources on
the FPGA and we use those sort of in a
in an equilibrium and the messages we
then could put 750 of these segments on
the FPGA and this is sufficient to do
like the first 10 x mark queries so so
so to put this in numbers may be to run
10 15 X paths which each you
reach is using something like another 15
or 15 to 30 segments something like that
so this is what this was a sufficient
and the other result that I have is
scalability so I said I said at one
point in the talk that this maps well um
to FPGAs and one way to measure this is
to say how how does the clock frequency
behave does it degrade when I start
filling up the chip so it's 750 segments
I'm really saturating the chip quite a
lot and what what you see sometimes is
that then the clock frequency really
degrades while here it stays more or
less stable which lets us assume that
this would also scale to a larger chip
very nicely that we wouldn't have any
problems there and also this is the
clock frequency I'm showing here is well
above our target which was just 125
megahertz for gigabit and so this this
brings me to my conclusion so what I
what I talked about today was I was
showing a hybrid xquery processing
engine with the approach the
architectural approach of putting the
FPGA into the data path so rather using
as used rather than using it as a
coprocessor which does heavy number
crunching on the side where you have to
load the load data on there and get a
result back here we put it into the past
which exists between server or between
source data source and and server anyway
and we let it sort of transparently to
the to the xquery engine process pre
process the data and the problem that we
encountered was that reprogramming the
fpga and such a setting is not an option
because we want to change what we put on
the fpga very frequently and very fast
and the result that the solution
that we came up with for this is to put
a skeleton that's the part which doesn't
change for all the queries put that on
the FPGA ones and at runtime you only
change the semantics of it that those
parts which are specific to a certain
query and finally this this work is part
of our Avalanche research project which
you find under this URL and in this
project we're looking at we're looking
for we're aiming for hybrid cpu FPGA eco
designs for data processing that's our
ultimate goal and we're trying to figure
out what are the right abstractions what
are the right interfaces foot is to work
well and with that I happy to take any
questions yeah which is the first of
okay this is entirely about the program
billion so how much venereal was a
research how much
corporate or do you expect this cause is
their cost decision-making program next
hour
okay I see what you mean I think in this
particular case the cost so I don't have
an accurate number but i think it was
fairly small so so like the navigation
step so so on the one hand you have a
lot of resources there anyway right the
way that's the way the fpga is built
whether you use a multiband additional
multiplexer to decide whether this is
going to be a child step or this tendon
ship I think that won't have a lot of
impact and and then yeah they could be
Rams I mean we for instance we store the
the predicate information which tak your
matching in beer and blocks okay and
again these blocks they are there I mean
I mean the question is what would you do
with them I mean here we're assuming we
can really use the chip as we like
there's nothing else we have to do on it
so again they are there and i don't
think i don't think that like if you
would say okay I do now a a solution
without this return without this online
reconfigure ability I don't think that
you then could say okay then i can put
10 times more pass on it I think it
would be rather something like well I am
not even sure if it would be more but I
think it wouldn't be so much was that
your question
actually I second or should you go back
to your area ground
this one
um so this is it something here now so I
understand why sort of the zero
intersection point is not zero you need
some some things just to suit boot
controllers right right the bars aren't
ethereal eyes are they say what is the
sort of the less than 1 slope kasbah
factor drops off as you get farther oh
ok so I think you're right it's not
linear but it's you know it's closer
well I think I think this has to do with
the tools i think that you know the more
you're filling up this stuff the tools
start doing a better job in using the
resources that's that's my guess here
these media points we play sort of that
the 400 what it could be using is
actually considerably lower than that
but it's like well have the space so why
not why not expand
that's that will be my first
I mean it's visually it's a visually
discernible people when people report
logic utilization members they're not
here on a chip that's not in fact that
they're being really best
the results could be quite better well
who knows whether
maybe the circuitry of timber to live
appoint hosting us its synthesis
estimates as well how many eligible
answer that that's enough questions is
the block and synthesis or a city place
proud I guess it'd be in place and route
I would guess because you start out with
synthesis synthesize the law did that go
to translate and then the world number
the synthesis step sees oh well you know
your 2% utilization ok I'm going to
double all of these pieces to make you
faster because you have plenty of room
well there's said this is options to say
optimized for area regardless right I
guess you can try or anyone at blue
graphs with all its options section X
area that's trivial so I think this from
here we used to just out of the box
options now other designs of that you
know you've eloped on Peggy percent and
then keep adding stuff to it'll stay in
eighty percent and you know just you
know so it is definitely is fighting a
quick solution and that's good about
fish farms is concerned I mean the
routing all of these steps also take
consistory the time it takes to to
produce a final design also I guess
isn't linear right in general you see
the super linear grass not a subsidy
around and that's the big
spanish thing about your program
abilities physical just a special twist
in the data path right when I the
programming pocket something special
so in acting as a circle so the overhead
in dec four four segments
going to do that before
% x I in the point-wise that's that's
the same
you don't have it complicated extra
use the regular
so as you're falling over
though right
then you've been happy about block
around twisting dodgy
fixed tokens
yes benchmark speedups to another skills
there's a couple of really stand out the
05 06 i believe it was you got there
Anna yeah Robin says do you mean and the
shapes of the first and second are sort
of yeah you know that did you did you
work into why my son had better speed up
to others uh I mean not so much I know
that like I think you sex just really a
lot of data is filtered I mean in the
past time I think the speedup is really
too how much is filtered out and that's
dependent on the query so the queries
are progressively harder evidently
right uh well I've I don't I don't know
yeah I actually I don't remember exactly
every query what what it's doing in the
execution time oh no I don't have these
ones here yeah some queries are joined
queries which are hard on the execution
time and and some there's also some
queries which yeah which you just can't
filter out as much because yeah but yes
i know i don't know i don't have this
graph but have a second i have this
information with me just have to look it
up real quick so if you have hope I
marked it that I can find it I know it's
I know it's in this in this thing but
maybe we have to take an opera offline
because i'm not sure if I can spot it
right away I think it was something like
between 15 and and 73 and and the median
I don't know anymore what we have for
medium there's a lot of numbers in here
but I think that was it was something
like that
purchase a turn for complexity reasons
right so the system will never say for
complexity reasons I can't support
anything there's always this fall back
good whatever you bring I'm just gonna
I'm just gonna let house on the right to
the first so I'm I know nothing about
next month so is there anything like
that there's been the language that you
just simply can't support in xquery
language yeah right oh yeah absolutely a
lot of stuff with me which we would have
troubles too okay commenters and so you
know any fraction of the elements I'm
sorry I'm just make sure that I got the
question right you mean support on the
fpga right yeah okay what fraction of
those types of things are actually in
the benchmarks i delay they assume that
there's whatever however large this is
one sweet and probably some other sweets
so how much of that actually gets
expressed and how much of that is
actually built into the runtime of
performance ability I mean I think
fairly Allah gets expressed I mean take
you know they have they're doing joins
for instance that would be difficult to
do in an FPGA you know there's a back
references you know from a child back to
a parent again here on our expands we
only this works well because we only
have forward we only go down the tree in
the past we never have then when we're
in a child a reference up to a parent or
just parent of the parent or stuff like
that I mean you can do xquery is a
language which allows you to basically
express anything it has function calls
and all sorts of stuff and and so
there's there's a variety of things
which we can put on an effigy
they're there but they may not be
so if you switched
those the six and seven that barely make
it 2-2 I mean at best you're doing
you're making you're making the job
half-ass they tried I i really think the
reason for this is that you know once
you once you build up your main memory
data structures did you so the filtering
effect the effect of filtering on a
query execution is just not that great
so let in the Black Sea
tokens one two three instead of the
streets
haha this was so so on doing doing this
before the FPGA so like binary XML or
something like that no we haven't we
haven't looked into that so yeah so yeah
I mean this yeah then then I don't know
how parsing parting speed up with change
with binary XML that's actually an
interesting thing as an extension to
this to to look that yeah mm-hmm doing
this lesson and the cavitation and
pipeline fashion so it's all going on
there on and I guess there's also
nothing that says that you couldn't run
the entire system horse-trading peril
and just saying okay
I have 1 parser and 10 and 1 series of
sequence managers that are still wrong
children there's no real way of sort of
catalyzing within because sequence
matches are already sort of parallel yet
in the sense that they're all looking
for different things
I mean I guess the other thing is that
this is interesting because this because
the problem statement allows you to
basically make one long pipe and cut off
wherever you want it's like getting
guarding though it's kind of like all
right um if you want me to but if you
would need to be able to produce certain
parallel outputs where it's this is
interesting because I don't care which
one you matched all I know is that you
something in this thick mattress that
means I have to take this entire sort of
half down right student is element and
take it and put it on the element but if
you couldn't if you had to figure out
which one it came from here soon idea
how you would actually maintain that at
some cuddling what but which one it came
from I think I think that could be could
be done by adding you you would have to
then add this information into the
stream but you you would then I guess
you would have to then stop the previous
one ad in yours and this would sort of
you know the all of these that matched
well you know you would add up this kind
of information the problem is so then
you know which one it came from if you
want to know this the problem is if you
want to route if now your problem is
like such that you say okay this doesn't
all if it passed one match then I need
wanted you know then I want to send that
data over there with passed to match
they want to send it to somebody else
then you have there a problem of you
know if all of a match you have to send
them to all of them and you know you how
do you do that but then that's a
different problem
I think so too the what is it by the
engineer running from a great extent
the curry engine is a Saxon here and
it's not like very plans just about
anything I get my question different so
country the design is sort of clean in
the sense that input is excellent now
put us again when it's right here so in
principle you can store it and then
again right here to my right but is the
very engine is sort of upgrade eccentric
and what you are doing is something like
a filter analogous to a filter Riley you
can feed the output of this straight to
the operator that becomes even filter
rather than making it well well form
doctor again passing sure I mean yeah if
you if you're willing to to modify your
engine you can do a lot more also we're
parsing twice basically when we're
parsing on the fpga and we're partying
again on the engines you know you could
use this lexicographical information
which we have maybe already for a final
part but then you have to modify the
engine
it was morning could say something more
about the autonomy via telephone yes
from day from the example understood
that they and also thinking that they
always look like this they have some
closure at the beginning when you check
you know some occurrences of Taylor
again or or is it sort of more more
complicated and then in relation to that
does the when you say so what you mean
by semantics is in different label right
so language there echoing Frank plug is
different actually labels that different
semantics transtheoretical exactly of
course if you choose choose different
enables you might actually get a non
determinism Miss Helen which you perhaps
in
just wondering if once you fix the
skeleton does the effect of watch how
you choose the somatically labels have
particles constraints that's okay so so
first of all yes this is a not you know
this is a specific type of non
deterministic finite state machine so
it's one where were you where you have
this forward only I'm path and so here
the predicates that we are supporting
here are always just tag names yeah if
we wanted to we also had some what we
didn't evaluate it thoroughly but we
also saw how would you do more complex
predicate swear you say okay I'm also
looking at attributes I'm taking I only
want to look at those items which have
an age attribute which is larger than 50
I think the concept is the same but it
makes this predicate major thing more
complex right you have to then convert
strings to binary and do the comparator
so um I don't I don't think I mean it's
if you as long as you don't support
stuff like back references from child
from a child back to the parent and so I
think you you won't have anything that
you cannot handle and in particular than
I mean the non determinism that you have
here is is is not a problem because you
you do all it is in parallel so so so
while what I want to say is if if I have
an a non-deterministic state machine in
software and now i have i have a lot of
active states then suddenly this can
become a performance bottleneck right
because i have to update all of these
parallel States on one single input item
while in hardware I don't care how many
states are active I operate them all in
parallel anyway so
you will get a you might be 0 you might
beat you on right so the choice is so
open exactly and that doesn't breathe
estate so like a subsequent states that
are active yes and and and because each
state is an individual circuit that's
fine each stage kissed just you know
looks for himself and the overall result
is correct yeah I don't know so if you
go back to your site I'm wondering about
basically the air balls law firm here
okay so you have these things broke it
out as being sort of the speed everyone
it's beautiful
how much time do I spend so what's it
what's the wall up so if i put this
scrap together with the other graph
right if I said what's my part in this
okay yes did this one is a very query
dependent like I said um you know in a
lot of these queries you you know you
can just maybe stack them on top of each
other but there's there's in this
benchmark also a few queries like long
joins where we're really the query
execution time is dominant so so there's
a few a few I don't know if it was kute
q10 or whatever where where you know you
would be then 40 seconds for 40 seconds
joining right so there there you would
get a very different picture yes okay
good again this is the views day huh yes
I can do this so not here so here you go
actual query execution time varied
between 68 milliseconds and 41 seconds
and the median over these was 390
milliseconds and this dis versus part
time of 2.5 seconds okay to put this in
proportion and the speedup is yeah what
you because you have 700 segments you
use the most powerful
so you could have seven of these engines
rut sell those
provided that salmon queries in parallel
yeah provided two things either the
engine the other side stands who married
and be that the ratio between equal they
have the least 17 Emmys so so which this
there is yes I think yes but there's one
issue namely how do you so when we when
we just load the pics pass all of the
eggs pass for for one query on to the
FPGA we basically swipe and then append
all of the path right so now you have
multiple in parallel okay you could do
this but now one query terminates you
take this out you leave the other ones
there you have a whole so you would if
you would do batches of queries in
parallel I'd say yes but you if you just
want to you know okay I have six queries
are still running but here I finished 1
i'm going to send the next one there
would be an issue of you know of
allocating allocating this thing in a
proper manner and the other thing is
then of course you have to also decide
on okay you could always Union all of
these queries but then the filtering
effect of course goes down because the
queries might not look at the same data
so so so that's another issue you know
the first seven you would be running one
variant
I mean surely independent work right
this game chess educator of the first
second third
you have played this base like
how many how many how many quarter to be
running on the same document then it'd
be very question apart is eating it up
how you do that yeah well you also meet
them a different unique multiple Parsons
multiple multiple parsing engines yes
yeah yeah so Reba is not 76 yeah yeah
you're not understand but I sniffles MPG
is correct but be able to that would
mean that you'd have to partition the
system have insertion points we can
certainly a break after the car isn't
into multiple cover you know that we
rate leave one car into two cars or put
two cars in make one car right so yeah
sorry but it's a program with the
program and yeah yeah okay yeah there's
a certain granularity if I guess you
probably also have doing increase your
i/o as well I missing if you rent some
middies at a time then you put out when
we pick on the bottom line pretty good
thing well see citizen who doesn't you
oh you're saying it's the same place as
I'm doc yeah yeah oh so assuming that
the speedup is roughly
to the smoother collaboration a that
enjoy depressing it then you could just
be all of the sudden applets on price of
park 07 that we met yeah that's true
last August
yes thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>