<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Crowd Computing | Coder Coacher - Coaching Coders</title><meta content="Crowd Computing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Crowd Computing</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FaiX7lvQCZM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
we're very happy to have Rob Miller from
MIT telling us about crowd computing at
MIT for the last couple of years in the
area of crowd computing which you know
we choose as the name it's an
intentional analog to cloud computing
because we well in the same sense that
cloud computing is about having access
to highly available and elastic
computational storage resources over a
network crowd computing is the same
thing but with human resources
and in particular we want to look at how
to use those human resources to solve
problems that single person would
struggle with and that we don't know how
to do with software yet so this is sort
of an example of an incredibly hard
handwriting recognition problem that
what's it did I write this now one of my
grad students read it but you are used
as non-dominant hand and and tried to be
shaky but we do see realistic examples
of this this is sort of a toy example
we'll see later more sort of practical
ones but the notion of crowd computing
is is getting a large group of people on
the web and these people are now
available in a variety of different
systems some of them are mentioned here
to make small contributions each so
we're not depending on a huge amount of
work from any one person to do things
that we don't know how to do with
software alone ai is not up to the task
yet and that one user alone would
struggle with and there are lots of
motivations that drive that power these
crowds wikipedia has you know made huge
hay out of purely volunteer work some of
its small contributions some of it large
fun is a great motivator this is a
screenshot over here in the right of
Foldit game at University of Washington
made by University of Washington that
has random people on the web folding
proteins and they've actually published
a number of papers in nature and science
that came out of the players of this
and have much longer author lists than
most most science papers social is
another way that you can draw a crowd so
one way that I think about Facebook is a
large global face recognition problem
that people are solving because they're
tagging their friends in in there in the
pictures that they're posting on
Facebook and then there's pay and Amazon
Mechanical Turk turns out to be a really
nice platform for prototyping many of
these ideas I mean people have seen or
used or heard of Mechanical Turk before
good way best features about it is that
it's programmable you can write code
that will post jobs and Mechanical Turk
collect the results and pay people and
do all of that automatically so you can
actually think of using this kind of
crowd behind a system that you're
building we also think about using it as
as again a prototyping platform a place
to try out ideas for crowd systems that
maybe someday would instead be
implemented on a different kind of crowd
not one driven by pay but maybe one
driven by volunteering or social or or
fun so that's the big idea I'm gonna for
the rest of the talk I'm going to show
you first of all sort of some of the low
levels of this the way that we think
about programming a crowd and then I'll
show some examples of crowd powered
systems that we've built so systems that
have human intelligence behind them in
the form of one of these highly
available crowds and I want to finish by
pulling back sort of another step in
thinking about what the design space is
now when we have crowds as one of the
components that we can use in a system
design and where we're going from there
so for the first part what I first want
to illustrate is a feature that it seems
like almost every crowd has this is a
task I like to put on Mechanical Turk
but chemical turk is particularly
demonstrative of this feature this task
just asks a person to flip a coin and
tell me whether its head or tail and
i'll pay them a cent to do it
and I do this task over and over this
particular one I only ran a hundred hits
but we've also done it for it with a
thousand hits and this is pretty much
what you get either shows there's a lot
of unfair coins out there or there's not
a whole lot of coin flipping going on in
this in this group I'm celsa doesn't add
up to hundred because the hundredth
person that a binary coin or something
idea something who didn't even ask for
us so they're there many interesting
questions you could ask about this right
if you change the instructions it does
actually change the results if you put
tp4 tails before h4 heads the effect
kind of goes away and it goes back to
5050 that's actually I'm not interested
in any of that I'm an engineer from my
point of view this is noise right and so
we need to design our system so that it
tolerates this kind of noise that we're
getting from the crowd in Turk some of
this noise comes from genuine spammers
people who are answering randomly just
hoping to get paid in any crowd it's
going to come from people who don't
understand the task or who not giving it
their full attention it human beings
make mistakes so that sort of is the key
problem that we're going to try to solve
with with more careful system design I
want to come back to this example that I
presented at the beginning this very
hard handwriting transcription problem
one person alone turns out has struggles
with this can't do a great transcription
so we developed an algorithm that uses
iterative improvement by many workers in
order to get a decent answer and I'll
just briefly illustrate how that works
by showing sort of the middle of the
algorithm so we have a partial
transcription right here that a new
worker is being hired to improve
basically being paid five cents to just
fill in one additional word here or
correct a word that was mistaken and in
this UI and this is someone older
version of the UI but workers can
indicate when they're uncertain about
things by putting parentheses around
them and leave question marks for places
that they don't know so that person is
asked to make an improvement and so they
make that improvement and then what we
do is take their input in there
output feed it to a group of voters each
of whom are paid a small amount to just
say whether this input is a better
transcription or whether this output is
a better transcription and these are
presented in a random order so the
voters don't know which was the input
which was the output and then the winner
of that vote either the input the output
goes on to another phase of this
algorithm and hopefully we're climbing a
hill here one of the challenges here is
figuring out when to stop the thing we
can have another phase where voters are
deciding whether it's complete or not
the moment we just sort of cut it off
when when they request or when it looked
to us like it was complete and at least
in this particular example the crowd
actually does a pretty good job so they
still have some uncertainty in some of
these words these red ones here are
places where the crowd got it wrong this
should been flowery that should mean get
instead of God should been verbiage and
that should have been the person's grade
b-minus
but one thing I point out here is that
groups of people are good at solving
problems and that even when they get it
wrong what they the the way that they
get it wrong is actually somewhat
sensible and fairly close to what the
original was here's another example so
this is a blob of printed text that we
applied a Gaussian blur to I have
trouble making anything out of this but
apparently there are enough people on
Mechanical Turk Ken because after eight
iterations of that process they pretty
much got it all right this word here
should be wedged come from here I don't
know human beings working groups are
somewhat magical if you can coordinate
them the right way so that's our first
example of a simple but fairly effective
crowd algorithm that at least we call
improve and vote algorithm workflow
design pattern you know they're there
words for this when human beings are
involved it feels little if he calling
an algorithm probably yeah we didn't I
mean yes I would not be surprised given
that it was a very mechanical
transformation that we did what we were
mainly doing here is these are toy
examples that are mainly designed to
test the idea of the improvement vote
algorithm and not not intended as things
that we don't think could be on no no
they were not so they may have tried to
plug this into Photoshop for example and
under it that's a possibility I think
that's fairly unlikely for the
sophistication of the crowd that we have
but certainly for tasks like translation
from one language to another you do see
the crowd trying to reach out for
automated tools that are available to
them and take the easy way out and in
general we don't want them to do that
and we have to that sort of design in
things that prevent them from doing that
it's possible it's possible my sense is
that that was not happening that is yeah
that may just be a gut thing you're
right they are still interesting
questions they're willing to know you're
right and you actually pointed out
something that I do which is that I I
sort of assumed that Mechanical Turk
workers are going to put less effort
into it than what you just described
because we are paying them five cents
for this on the other hand somebody who
is really an awesome Photoshop hacker
might be willing just for the fun of it
to crank up Photoshop and solve that
problem even though it's like there was
a sudden transition great question I
would have to look at the data because
that would answer that question if there
was one person in there who is like a
ringer that had some tool they knew how
to use good I would have to go back and
look at the data okay few things I
wanted to say about this my main thing
is sort of what are the criteria that we
want to think about for comparing
different workflows different algorithms
one of how well it deals with noise and
one of the reasons I like Mechanical
Turk as a prototyping platform is
because it is very noisy our rule of
thumb is that for any given piece of
work any given hit you put out
Mechanical Turk 30% of the time you're
gonna get some bad work back as a result
it's not always gonna be like spam it's
not just gonna be like garbage they've
typed in there but it may be somebody
does not paying attention to tasks
the instructions so we have a fairly
high noise rate which really forces us
to think about quality when we're
designing the system and think about
quality control my rule of thumb is if
it works on Mechanical Turk it's likely
to work on any given crowd because Turk
is kind of so bad for that this
particular algorithm runs very slowly
because it's serialized that's passed
through all of these stages and you're
not really taking advantage of the fact
that you have a big group of people who
could be working in parallel and we have
to think about the incentive so and Turk
this is simple to measure because it's
the number of it's the amount of money
that you had to pay these people in
order to get them to do the work for you
it's harder to quantify that when you're
talking about say a social crowd how
much social capital that I expend by
making my friends do this bit of work
for me we'll see a couple of other
algorithms later when I when I talk
about these other systems I just wanted
to start with that one because it's
simple and shows some interesting
results so next a couple of systems that
we've built that are a little bit more
practical and show the value of having
human beings inside a system so this is
a problem that we unfortunately still
have a lot I don't know why we still
have page limits in in the age of
digital publishing but we nevertheless
do and so 30 minutes before a deadline
you know this is page 10 we find
ourselves with a little bit over and
we've already played all the tricks we
think we can play with font sizes and
line spacing and margins and things like
that and waste a little bit over and and
the author you know the the the user the
single user that is working on this
paper they really should be paying
attention to things other than the
length of the paper they should be
focusing on getting the content right
because it's gonna be accepted for its
content
not for its length right to be rejected
for its length won't be accepted for
being on ten pages so the idea is to let
that user focus on what's important and
hire a crowd or obtain a crowd somehow
and focus on that other problem how do
we deal with its length
and I'm afraid my video did not seem to
be working when I was
trying the beginning but I'll sort of
talk through how the thing actually oh
video is working good what we do is we
take the users text and that we just
sent off a bit of the conclusion to the
crowd and some minutes later we come
back with suggestions from the crowd for
how to shorten this thing and it's now
embedded in a user interface with a
slider here that automatically chooses
from those suggestions in order to fit
to exactly the length that you want so
you can kind of drag your paper down to
ten pages
and you've got exactly what you want and
that you I also showed you what was
actually changed right so that you can
compare the before and the after and
basically monitor the crowd so this is
something that will come back to you
later sort of this synthesis or this
symbiosis between an end user and a
crowd of human editors human
intelligence and software in this case
doing something fairly simple that is a
greedy search among all of the
suggestions in order to fit the length
that the end user is trying to shoot for
that brings it all together that
coordinates so yeah so the comments
actually have already been put through a
filter and I'll talk about that when I
talk about the workflow algorithm so all
the slider was really doing is saying I
have a certain desired character length
give me a subset of the of the
replacements that we'll get to as close
as that character lengths at that point
it's the length and yes we haven't we
haven't we haven't graded the quality
anymore finally other than other than
throwing out the worst and that's what
the algorithm about to show you does so
the way that this works that the
workflow that the crowd went through
when it got that pile of text to work on
is
is this fine fix verified pattern that
that actually splits up the editing into
two different phases that allows us to
do some more quality control and treat
those edits sort of independently and be
able to choose between them
independently so the first step is just
to find a place that looks like it needs
editing in this case it's a place that
is kind of fatty right this sentence
could be reworded to be shorter those
workers simply have to highlight that
they just have to mark that as a place
that could be shorter and the advantage
of that is that then we can get sort of
independent judgments about places that
are fatty look for at least two people
saying that that is a fatty bit of text
and pass only those on to the next step
which is to rewrite them and make them
shorter right so this gives us some
quality control for that if we were just
and we did through some experiments in
which these were combined and said that
here's a paragraph shorten it you ended
up with a lot of changes to the
paragraph that we're much harder to do
quality control on so this allows us to
get what you saw in the in the in the
demo lots of little changes to the text
which is actually very interesting
because it means the crowd is behaving
differently than a single editor would
behave and then the fix is make a change
that will shorten it without changing
the meaning of the paragraph and then
because some of those will be bad you
know at least on Turk our rule of thumb
is that 30% a third of these answers are
going to be bad we pass that on to a set
of voters who basically throw out that
lower 30% and that way what we've got
back are hopefully fairly high quality
choices that that we choose them all so
we've run this on a number of different
kinds of texts including technical
computer science papers some that were
already published and people were able
to shorten them actually interesting
thing that that that the Mechanical Turk
crowd does and most of them on
Mechanical Turk parently don't have much
computer science technical knowledge I
say stay away from those parts right
they don't delete things they don't
understand they go for the wordy English
and shorten that and that's good at
least for the parameters we chose we
tend to get about 15% cut from the text
on average which means that we take an
11-page paper and cut it to about a 10
page paper and it does it again in an
interesting way it's kind of like
shooting a shotgun at that paper rather
than taking out an entire section which
is what a you know a single author under
a time pressure would do it takes costs
about as much as a professional proof
reader to do it but instead of you know
taking an hour or more for that
professional perforator to turn around
we can turn it around in 20 minutes in
fact a lot of this time this so we
actually split the time that it takes
into wait time which is time with the
Mechanical Turk hit waiting for somebody
to pick it up a lot of time is spent
with nobody working on the task right so
this is basically wasted time that the
kind of the the process is stalled
waiting for some resource to discover it
and and work on it the actual work time
the critical path of the process is only
about two minutes to get through the
fine fix and verify for preferred for a
given paragraph for an average paragraph
so this could really run very fast you
know this is actually close to something
that feels like it would be interactive
speed you push a button in your we
actually did implement this as a word
plugin push a button in word and ideally
two minutes later you know gone for
coffee you're back and your paper is
shorter another feature of it by the way
we did we don't just shorten things but
we also do proofreading funding spelling
and grammar errors and things like that
and it actually is it's compatible it's
complimentary with the spelling and
grammar checker built into word finds
things that that does not and in fact in
our we said the paper itself threw that
thing and found a grammar error
nobody else had found even though the
paper had a lot of authors they didn't
actually find this bug so the bug is
this word here and actually this makes
perfect grammatical sense to word
because you just have to put it in
parallel with produced so shorten should
produce many alternative rewrites or
introduce grammatical errors right word
is perfectly happy with that sentence
I'm not happy with it because I don't
really want our system to be introducing
grammatical errors this really should be
in parallel with changing right without
changing the meaning or introducing
grammatical errors right so word will
not discover that because it's
semantically you know the difference
between those is a semantic difference
CarProof that is the crowd found this
and I think it's interesting that you
know we had a small crowd of authors
they didn't find it but when we sent it
to the crowd of Mechanical Turk they did
and one reason for that is that this
paragraph was down the bottom of page 5
and the author's you know we're reading
the paper from the stock right they were
reading from page 1 they had to read it
all the way through so they're pretty
tired by this point they were also
paying attention a lot of things other
than just grammar
well this is a fairly important semantic
thing everybody missed it
the crowd was given only this paragraph
and the people who did the find were
told you got to find something wrong in
this paragraph right you have to find
something to complain about in here so
they found this good question false
positives I don't think they found those
in our paper they're certainly in some
of other tests there were false
positives yeah this was the only mistake
that came out of this car ah good point
probably not so one of the one of the
issues with this system is that we did
have to pull the text out and present it
to them in a
in a textbox so yes that's another way
that the presentation to the crowd was
distinct you know different from the
presentation to to the author's good
good point yeah we told them we had they
had to highlight something wrong and
virtually every paragraph has something
that you could quibble with and yes we
wanted one thing what that's happened
three times and it's capitalized I don't
know everybody knows about Flickr I'm
not sure um it's a good point yeah and
they may have complained about that
that's right
certainly in effect that we that we did
get with spelling was the spell checker
in people's browser would underline
things and they would highlight that so
yeah it's again a sign that that workers
often try to do the easy thing okay so
that was a word processing plugin that
draws a crowd in your and your word
processor and helps you with editing
this is this is a system that Jeff
Bigham who's been at University of
Rochester and is is going to Carnegie
Mellon in the fall developed while he
was a visitor in my lab a few years ago
and what it is is a a smartphone app
iPhone an Android app that allows a
blind person to take a picture of the
world and speak a question about it and
that spoken question and picture are
posted on Mechanical Turk where workers
type in a reply to that question and
that is sent back to their were blind
person's phone and read aloud to them so
it's a way to use the this human
resources crowd resource to provide you
know an ability that the end user
doesn't have it's been deployed so it's
actually been out there for a couple of
years now and it it is back now
not only by Mechanical Turk Mechanical
Turk is the fallback crowd for vizwiz
now when no one else is available but
it's also it also has volunteers that
are answering vizwiz questions so you
can get crowds from from other different
other sources other than other than
paying for them and it's getting lots of
questions a day from blind people these
are a few of the questions that it got
during the early days you see picnic
tables across the parking lot this is
the this is a time that it took for the
answer to come back one of the things
that the vizwiz system was really trying
to push was to reduce this wait time the
wait time of 18 minutes that we had in
the Soylent experiments where those
tasks were just sitting on Mechanical
Turk waiting to get picked up and visit
was has gotten to down to 13 seconds
which is actually pretty fast right it's
about the time for somebody to kind of
answer the phone if you're calling them
and you know give you the answer aloud
so that's very fast and there's two
things that vizwiz does in order to
reduce that time one of them is kind of
a Mechanical Turk specific thing if
you've ever used Mechanical Turk you
know that it's got a very bad user
interface for workers it's just
thousands of posted hits in a you know
in a long scrolling or long paged
display and many of the workers sort of
just look at either the jobs that have
the most available work or the jobs that
are the newest once you fall once your
task falls off the newest the first page
of the newest results it's basically
invisible to workers and it's response
rate is going to shoot way down so what
vizwiz does and a lot of requesters are
doing now is it repeatedly posts new
tasks in order to keep itself at the top
of or at least on the first page of that
display the second thing that vizwiz
does is is it hires people before
they're actually needed so as soon as
the blind person gets
their phone and switches into the app
it'll start posting tasks on Turk in
order to hire somebody and have them
ready to answer this question as soon as
possible
and if the worker arrives before the
question has been asked then what vizwiz
does is it asks some old questions in
order to keep them busy and also partly
to train them up and get them ready to
answer the real-time question that is
eventually going to arrive NSF right now
I think it's I mean there there are many
business models you might imagine using
for this but no the blind people are not
subscribing to this service yet working
on a video one but I don't know whether
he's ready for that yet this is in
seconds the other thing I wanted to
point out here is some of these are very
long so this is an example of very slow
answer this happened in three minutes
for the first person and it's also
actually a very hard question because
this is the wrong side of the can
turns out blind people don't take great
photographs just one thing that we
learned from this project
many of them are dark or blurry the
human workers are good at detecting that
and making suggestions like you know the
image is difficult to see or could you
please turn on the lights but this is an
example I think of how great human
beings are just at the high end right
that human vision systems and human
problem solving can look at this can and
sort of see the edge of the label here
and the edge of this logo and figure out
that it's Goya beans and that it's
chickpeas so people are great at doing
that but it takes them longer moving
from there so that was an example of a
system that responds in sort of tens of
seconds the the next system I want to
show you tries to push this sort of as
far as we can imagine pushing it
and get human help within seconds within
within a time that might be feasible for
interactive use in yeah in a user
interface this is a crowd powered camera
shutter the idea is to get with human
help the camera to taking the the
picture at exactly the right time what
is one is just the right moment when you
should have taken this this action shot
for example so the way we do it is to
leave the shutter open we take a 10
second video that video is sent to a
crowd of workers that we've got and
roughly in the time that it would have
taken you to review that 10-second video
on your phone the crowd has picked a
frame from it as sort of the best one
the moment when you should have snapped
that picture and there are two ways that
we do that one of them is sort of an
outgrowth of of the vizwiz techniques
that I was talking about which is in
order to say the the time of your post
hanging out not being answered on
Mechanical Turk we hire the crowd in
advance and in fact to simplify things
you know we don't just show them tasks
but we give them we give them some money
just to hang around so we give them a
retainer but instead of being a retainer
like you'd pay a lawyer for months or
years it is for minutes and this
actually shows for various different
retainer lengths that is the times that
workers had to wait before the actual
task was ready what the probability was
that they would actually come back so
pretty much at the at the five-minute
retainer mark here we get half of them
back with a few seconds half of them
ready to work within two seconds the way
that works is they're allowed to go off
and do other things but we really want
them to stay there in front of the
computer when the pop-up appears to be
ready to dismiss it and actually do the
work so if you hire people for a
five-minute retainer and hire about
twice
as many or at least initially hire about
twice as many as you need then you're
probably going to have the right pool
ready in two seconds and we don't have
to pay them very much because we're not
actually asking them to do very much
just to stay in their seat for five
minutes and they can do other Mechanical
Turk tasks in the mean time but just
having them there turns out is not
enough because if we did like an
ordinary improve invoked kind of pattern
with this we would be waiting for the
slowest person in this group to make
their choice and that would not be great
for latency and we'll see in a moment
when I show you some comparisons that
that it produces relatively poor latency
so what we do instead is we guide the
group through a process of iterative
refinement looking for a good place in
the video and eventually narrowing it
down to a single frame so the way that
works is we imagine this is the slider
of the video and we have a small group
of people that have been hired each one
gets dropped in the video at a random
place it's not playing they have to
scrub around in the video in order to
find a good frame and we watch where
their scrubbers are where their sliders
are and when some fractions some
threshold of the group is dwelling for a
threshold time around the same region of
the video we make a guess that there's
something interesting right there and we
shrink the area of interest from the
entire video down to a neighborhood of
where those people are looking and bring
everybody else into that region as well
and then we recurse and repeat and for
10-second video three phases of this are
enough to get you down to a single frame
with the particular parameters that we
chose which were we're looking for at
least a third of the group to be within
a neighborhood of I think
a quarter of the size of the current
window something like that work where
the most interesting it should still
work because we're positioning people
randomly initially are you worried about
sort of boundary conditions of the of
the neighborhood because they've just
crossed over each other and I mean we
are looking we're looking for also a
12-time we're not just looking for them
to strike each other we're looking for
them to be within a neighborhood of each
other for a minimum of two seconds I
think what's the actual threshold that
we use so we are looking for some dwell
but there is actually interesting point
here that the beginning of a video is
sort of a salient point right and it may
be that some fraction of our workers
don't like to just start from where they
are and instead jump back to the
beginning of the video and do a scan
through it and that's a good question I
don't know whether we saw a lot of that
behavior scrubbing they only scrub like
sort of nobody actually goes all the way
to the beginning right interesting
question
one way we might solve that is to change
the timeline so that it's not the
timeline anymore yeah and then there
wouldn't be any salient points and then
there wouldn't be any beginning points
here's a comparison of the algorithm I
just described to you the rapid
refinement algorithm with just getting
the first answer and at least on
Mechanical Turk you know this is sort of
30% likely to be bad somebody who's not
really paying attention or not or or
we're actually spamming us versus a sort
of traditional way to do quality control
that is you collect frames
the whole group and then you have
another group that you've also hired
vote among them I'm about which ones are
the best and what you're seeing here is
the over a number of trials what the
frequency is of various latency so the
mean latency for rapid refinement is
about 12 seconds that mean goes up to 16
seconds when you're just getting the
first answer back when you're not sort
of forcing people to work fast and and
they're they're sort of picking at their
own speed but you also get a very high
variance in that first answer sometimes
it's really really long and then when
you go through a whole process where you
have to wait for the slowest person to
make your choice the the mean is even
larger in the variance is larger as well
we've already retained these people so
all of them are basically starting at
roughly the same time right we've got
that crowd ready to work with in two
seconds so they're all they're all
they're ready and it's it's about five
people here's some examples of actual
photos actual frames selected using the
rapid refinement method and using this
is actually the algorithm that YouTube
uses for a video to choose what the
thumbnail is going to be by default and
we also had a an expert photographer
look at these videos and try to pick out
a frame so this is an example up at the
top we see places where a rapid
refinement did really well this is sort
of a typical range and so you can
compare it against what the computer
vision algorithm is doing down here we
have an example of a failure by the
rapid refinement which in this case was
caused by having sort of two salient and
interesting points that happened to be
closer to each other than our threshold
and or just just outside the size of our
threshold so that when the workers sort
of were exploring the boundary of those
interesting points they ended up
narrowing us down to the valley between
them right and
than the best we could do in that Valley
was this blurry photo right here so that
maybe could be tweaked by choosing the
parameters a little more carefully based
on what's going on in the video but it
might also be sort of an inevitable
compromise between quality and latency
that that this algorithm is basically
making okay the last system that I want
to demonstrate is a bit different from
the other ones that I've shown but one
of the reasons that I have it in here is
- it's because though it's three other
three systems they're all using
Mechanical Turk and I hate it when
people walk out of a talk like this and
think all of that work was about
Mechanical Turk because this is really
about crowds and this is a different
kind of crowd there's can be no Turk
workers behind or inside this system and
what this system is is a code reviewing
system that we're using now in MIT
programming classes where we have the
problem that we assign students
thousands and thousands of lines of code
to write and we don't have the staff
resources to actually read most of that
code and certainly not in a timely
fashion so they hand in a programming
assignment and they may not get feedback
from their grater before the next
programming assignment is due and then
they haven't had a chance to improve so
key thing we're trying to provide here
is faster feedback for students and auto
grading is really not at the point where
we can you know give good comments about
you know why this code is bad this is
actually a perfectly correct factorial
function but nobody would actually want
to maintain it so the system works by
drawing on a crowd of reviewers not just
a single greater that is looking at this
person's assignment but instead this
person's assignment is chopped up into
bits and similar to how we chop up your
your paper into paragraphs and sent to a
mixed crowd which consists of not only
staff but also other students in the
class another alumni who have taken the
class before maybe they're upperclassmen
now at Mi
maybe they're actually graduated and
gone out to the industry jobs so this is
what a reviewer is sort of assignment
looks like we've got these little chunks
of code that we have to review and the
reviewing process happens also on the
web in an interface it's sort of very
similar to commenting that you would do
in Microsoft Word or in Google Docs or
in github for that matter where you can
also comment on and review code a few
things are different one is that by
default the the reviewer only has to
look at a small chunk of code and we've
experimented with a number of sizes for
how large this chunk should be and the
right chunk not surprisingly is
basically whatever a module is in that
program so in some of the programs some
of the assignments that we submit
students are basically writing
standalone procedures in which case that
procedure is sort of a chunk that is
reviewable some cases they're writing
classes and then the reviewer really has
to sort of look at the entire class so
they're focusing their attention just on
that module they have a way to go beyond
that and look at all of the program if
they need to
most of our reviewers don't need to do
that they find they don't need to click
on that review all code second we have
we have an automatic checker and going
in here and making comments so it's
basically a lint that we're using a
Czech style for Java in this case and
then the reviewers have a threaded
discussion and they can reply to each
other an upvote and downvote all of them
are marked by names and sort of an
important thing for the system as a
social system the reviewers are all
identified but the person who wrote the
code is anonymous so that so that they
retain some privacy we've used this in a
couple of semesters now in the Soph for
software engineering class in at MIT
some of the semesters you know we work
harder than others to recruit alum
and get them to participate this does
require effort students we can motivate
to participate because as part of their
participation grade alums need other
kinds of incentives these alums we
recruited mainly out of altruistic
impulses a lot of them were former staff
members of the course you sort of
remembered how great it was to tell
people what they've done wrong but what
we're doing now actually we're in the
process of building this system out into
a more substantial social networking
system actually in which the alums would
have an incentive to come in here in
order to in order to develop contacts
with current students and potentially
recruit them when they graduate we've
generated a lot of comets in the system
students are getting about an average
ten comments per submission which is
actually better than they were getting
from graders so that's an improvement
but a key thing here is that the
comments are coming back very quickly so
students hand in their assignments at
the end of the week code reviewing
happens basically over the weekend and
they get their feedback by Monday which
is a very fast turnaround and single
graders were never able to do that
before and one of the things that
enables us to do is give the students an
opportunity to revise and resubmit their
that problem set so that they can
actually improve their grade yes well
we'll come to that actually we have we
have some comments at the end but yes
for the most part students actually
demand this kind of feedback they and
occasionally when the system sort of
breaks down and we can't do code
reviewing which happened a couple of
times in the early days there were just
tons of complaints that they wanted
their code reviews we we assign multiple
reviewers to each chunk of code and in
fact each chunk of code typically has
two reviewers from the
an alum pool and one reviewer from the
staff and that staff reviewers job is
basically to moderate basically to go in
and review the reviews to make sure
people aren't saying things that are
that are that are wrong or boneheaded or
rude for that matter we see but we see
that that interaction is actually useful
for maintaining the quality control of a
system like this and that's important
because much of our crowd are people
that are still learning right people
that are still learning how did it how
to program and how to do software
engineering so we see examples of
students correcting each other of
students clarifying what other students
have said on something I like to see is
evidence that the reviewers themselves
particularly the student reviewers are
learning something from the experience
so it's not just we are getting grading
work out of the students but the
students who are doing the grading work
are actually getting some value from the
experience there's no any things about
Java or they're learning things about
software engineering design by reading
other students code okay
so that's four examples of systems up
other there are definitely pure feedback
and peer grading experiments happening
in MOOCs so Scott climbers HCI course
from Stanford uses peer grading pretty
much exclusively for all of its
assignments Eric Rabkin --zz science
fiction course at University of Michigan
also uses peer grading they and they use
multiple peers as well so you get
created by a handful of your other your
other peers one way that this system is
different is that it's trying to draw
not just people who are currently in the
class but also people that have left the
class and have graduated and I think
that's going to be you know as MOOCs
mature
going to have to figure out ways that we
can draw on those those populations in
order to notice sort of keep the keep
the the crank turning because for us
both both kinds right right I mean
traditionally we have drawn on the
people the upper classmen already right
I mean the the standard model of
Education right is that you're a student
in the class then you're like a lab
assistant or a greater for the class and
then maybe if you become a grad student
your TA for the class and then you know
years later you're a faculty member
whatever it's a it's a sort of climbing
pyramid right and I think we'll see the
same thing with with MOOCs but in a more
obviously distributed by yes that would
be an interesting business model and
they may be very motivated by that yeah
that's a good question and it's one that
we could probably answer by looking more
closely at the data because there there
certainly are examples of chunks where
no staff member was assigned to it for
various for various reasons just for
instance we didn't have enough staff in
the system that that week and so to see
whether for example there are
misconceptions that are never corrected
in those are there any major you know
problems
a good question and I don't know enough
about what has happened in those in
those MOOCs to be able to answer that
there is definitely a fear even in even
in you know widespread QA forums like
Stack Overflow that crowd question
answering can sort of propagate
misconceptions around without unless
there is unless there is you know sort
of adult supervision that it's keeping
that from happening but that is just an
that's just a hypothesis so I don't know
I don't have any hard answers for but
it's a question we should all look at
okay one of the reasons I wanted to show
you those those four systems as examples
was to talk about what I think the
future of this space looks like and why
I think crowds are gonna be an important
part of how we think about designing
systems in the same way that machine
learning has become a part of how we
design systems graphical user interfaces
have become a part of design systems
databases and and client servers have I
think we're gonna be thinking about
crowds as a component as something in
our toolbox that as software designers
system designers really we should be
able to pick up and here's one reason
why because crowds enable us to think
about taking The Wizard of Oz
prototyping technique it's been long
used in HCI Nai
and pushing it out in the real world so
you're not just having your wizards in
the lab your wizards are now hired from
a crowd and they're and they're allowing
you to push that app out collect some
data from real use find out how people
are using it iterate on the idea on the
application and and then build in some
AI later for performance and cost
improvement and vizwiz is business and
soil and are both good candidates for
this you know I should mention what I
what this space is by the way this is
the end user that single person that
wants this thing done this is the this
is the automation the computation the
software that that is helping them do it
the history of computer science really
has kind of looked at the trade-off
between these two dimensions more and
more taking
things that used to be done manually in
paper processes for example and trying
to automate them trying to push
ourselves down along that axis so we
have a new axis now which is groups of
people hired on demand and contributing
small bits unnecessarily hired but
obtained on demand and contributing
small bits of their work so vizwiz in
this space you know has very little AI
is very little intelligence not a lot of
software there it's all pretty much
being done by the crowd but as we
collect more and more pictures from
blind people right this is a corpus that
is not well represented on Flickr or the
kinds of pictures that blind people
taking the kinds of questions that blind
people need to ask it's something you
really need to deploy an app out in in
the world in order to discover in order
to be able to train a good AI system to
do it looking at the sort of end so the
user peak of this I think the end user
is still going to be a crucial part of
this system there there have been
experiments that kind of look at try to
look at systems that are you know purely
small contributions from from random
people on the Internet mediated by some
software that's coordinating them I'm
not too optimistic that you know a crowd
is ever gonna write the greatest
American novel I think you're always
going to need some kind of unifying mind
some kind of designer to bring it
together and at least for practical
things this person provides the right so
it won't really matter how good it is as
long as it's grammatically correct so
that person I think provides the
coherence and they also have the final
responsibility for the results so we
have to give them a user interface that
allows them to understand what the crowd
is doing and be a check on it and and
the guide to it and I think we've seen
there have been a number of systems that
showed that the users monitoring of
what the crowd is doing is is very
important Soylent this is sort of just
one of them there are many different
crowds yeah and I don't want you to walk
out of this thinking that crowd is just
Mechanical Turk because there are lots
of them
Viswa is actually in particular has also
been put on top of Facebook some
interesting results they're actually
blind people want to have the control
over which crowd their question goes to
and they all they actually feel more
comfortable sending certain questions
actually very private and sensitive
questions they feel better about a
random person the Mechanical Turk
answering then about having that
question end up on their own Facebook
with their name attached to it so
different crowds are gonna have
different purposes and different uses
and matter differently to the user
you know our student and alumni code
reviewing crowd is an example of one
that we've sort of built up in the
context of a single organization so
crowds can also be brought together that
way and motivated by a variety of
different incentives you know their
grade their reputation maybe their
desire to to recruit a to hire students
few things that we've learned about how
to design crowd systems definitely helps
to divide work into small chunks like we
did with with the Soylent paper editing
and it gives you more parallelism it
allows you more opportunities to to do
quality control and that you should plan
for noise that your crowd is not really
gonna flip all the coins that you tell
it to flip they won't pay attention to
all the instructions so you have to
think about quality control think about
building it into your algorithm or try
to refine your crowd try to get rid of
the people who are not flipping their
coins that's what many people try to do
I would rather pay more attention to
this because I think in the long run it
will give you a more robust system now I
think on this angle that you know this
user is a human in this crowd also
consists of humans you know this is a
very different beast over
here what actually is the difference
between those two points why are you
bringing a crowd into this system so
that crowd should should provide some
kind of compliment sometimes it's that
they actually have a different
competence right that the end user may
have Anna Mae lack and ability that that
the crowd has like eyesight but another
might just be that there are more
diverse or they were looking at the
problem in a different way so we had
this example where since the crowd was
just looking at this paragraph and since
maybe that hyphenation wasn't actually
there they do a better job or at least a
different job and they bring some
different perspective and and different
intelligence here's some projects that
we are sort of continuing to do so
building on the idea of vizwiz we're
putting a crowd into the draw of the
passenger seat not the driver's seat the
passenger seat of a car because the
driver is you know a driver as an end
user is situationally disabled in ways
that are very similar to where a blind
person is disabled you know you're
supposed to keep your eye on the road
not on your phone or your tablet or your
laptop and your hands should be on the
wheel and your attention should
certainly be on the road so we are
mustering a crowd up in the passenger
seat to give the driver help with
essentially information needs like
searching the web we're also looking at
generalizing that the sort of notion of
Soylent - - programming so that you know
a single programmer in their in their
IDE can draw on a crowd of skilled
programmers from across the web and get
them get their help for small amounts of
time so we call this idea actually might
grow outsourcing rather than what's
sourcing a huge or large module for you
know hundreds or thousands of dollars
and days and days work instead you're
hiring somebody for 15 minutes to write
a function for you there's a bunch of
open questions how well does this scale
how can we take advantage of crowds that
are out and about and there's quite a
few startups that are doing things like
that
raising the expertise that we expect
from crowds you know I
I have some assumptions about Mechanical
Turk that it's going to be low expertise
there may be some ringers in it but we
want to we want to refine those crowds
so that the ringers are not ringers but
are instead you know that we have a lot
of Photoshop experts so we have a lot of
programmers in their tasks routing is
hard Mechanical Turk interface is awful
awful awful we need much better ways to
actually get a task to the person who is
the best one most likely one to do it
and do it well and then the sort of big
question how do we do these transitions
from having a crowd behind your system
to sensibly incorporating artificial
intelligence so that's what I've got a
bunch of places funded this but I also
want to thank the 20,000 people that did
work as part of what I've shown you how
many slides oh I did not have them
shorten my talk no no no but they
contributed to all the other loops all
the other systems yep there's oh they're
out of so I haven't done any work on
this but work has been done in this Jeff
Bigham at now at Carnegie Mellon has
done exactly what you described now with
a car on the road but with a robot
driving around on the ground in the lab
where the crowd is controlling it
basically by clicking left and right and
forward and back buttons to accelerate
and decelerate and Jeff new students
have come up with actually some very
nice strategies for mediating for
basically collecting the the directions
given by the crowd and using those to
decide how to turn the robot and the
best strategy that they've come up with
is one where you act
we are electing invisibly a particular
member of the crowd you've got right now
to be the driver that person doesn't
know that they're the driver you know
it's a bit like you know that Douglas
Adams trope about the President of the
Universe is some guy on a planet who
doesn't realize who's making all the
decisions for everybody that's exactly
the way this works and that person is
elected by how well they're agreeing
with what the rest of the crowd has done
in the very recent history and so if
that if that person who's elected and
they only serve a short term you know
like 15 seconds or something like that
very harsh term limits why well I mean
they can continue to be they can be
reelected they can be real like I
shouldn't have described his term limits
they can be reelected arbitrarily many
times but that way if they disappear we
can fall back now to the next person who
has the next highest agreement with the
rest of the crowd over there over the
recent epoch so that seems to work very
well where it where it sort of breaks
down is where our politics breaks down
you're heading for a tree and half of
the crowd wants you to go left and the
other half wants you to go right and you
hit the tree so but you should you
should certainly look at that work it's
great work the system is called Legion
probably very yeah I think there are
actually probably three startups doing
the thing you've you initially described
the security-camera reviewing
particularly after the fact when an
incident has happened and you need to
figure out who was strange and and
connected them together through a
sequence of cameras I think there's a
couple of startups working on that
problem
individuals the summer flutter
distinguish them but is there any effort
at all to make team sort of them or you
know start thinking of them as ah
interesting yeah not not in the kinds of
crowds that we're thinking about right
now but that is something that is
natural that to start - yes I'll be
there certain we certainly have people
working together and in many of our so
like the small group in our code
reviewing system that comes together to
talk about one particular chunk of code
it's like a team that has been formed
kind of on the fly and that's the way a
lot of these systems have been done
there been some experiments with a
machine translation of poetry that have
brought a small group of people together
and have them work together and think
about how they're gonna how they're
going to solve this problem but but
about these interesting questions about
making those things last longer bringing
those groups back right one of the
assumptions I make about crowds to
distinguish them from other kinds of
human organization is that crowds are
extremely fluid and that any particular
person in the crowd has a high
probability of not being in the crowd
the next time you look for them if you
would if you if you lower that
probability you're getting to something
that's more like a firm rather than a
crowd and I think different
I think my question might relate to that
point which is it seems like having a
system the retainer model requires kind
of an ecosystem of other tasks yes in
some ways it seems like you that we need
to reproduce the sense of other work to
be done in order to have somebody who's
available yes I mean so in the larger
scheme of this what well I can imagine
that that there would be some tasks that
have real time requirements and some
tasks that don't and that you know a
worker would be sort of subscribed to a
feed of real-time tasks that they that
they are willing to act on quickly when
they arise and in the meantime they're
sort of filling their time with other
tasks I mean this is a bit like what
many sort of workers in the real world
do right there
they're supposed to answer the phone
when it rings and they shouldn't let it
ring for minutes minutes before they
answer it but that doesn't happen too
frequently and then in the interim they
are doing paperwork um yes maybe um it's
sort of my feeling that people already
that we already have a fairly effective
system for moving people up through
education and getting them into firms
and whether if you want to speed that up
fair enough it would be different I mean
the way that I think of crowd work and
maybe I'm just not being maybe I'm just
not being forward-thinking enough I
think of it as something that actually
fills the nooks and crannies of and the
inefficiencies of our of our firms the
fact that many people are actually
underemployed and underutilized
many of the people on Mechanical Turk
are actually gainfully employed this is
what they do instead of doing Facebook
and solitaire and YouTube at their
boring desktops they make a little extra
money on Mechanical Turk that's what's
servicing the show no so good very good
point
Mechanical Turk actually has two
different crowds in it right there's a
majority sort of Indian crowd that you
get when you post your tasks at night in
in American time and then there's a
North American crowd we did not filter
our crowds so we tend to we tend to just
post our tasks without any filters at
all and that gives us basically this
very bad behavior of 30% bad work if the
rule of thumb is that if you if you take
only US workers and you require them to
have at least a 95 percent approval rate
for other tasks you're gonna get a more
refined and high-quality crowd we did
not use that because we want sort of the
bad work and we want to be able to
figure out how to so right now I you
know human computation can definitely do
things that artificial intelligence is
not ready to do that machine learning is
not ready to do computer vision is just
full of problems that it's just very bad
at and human beings are much better at
so in a sense your question is asking me
whether it will ever catch up and I
think the answer is no I think that
human beings will always be inventing
new crazy ways at least to interact with
each other just an understanding of
human culture and of human humor is
likely to always be ahead
of what a machine learning system is
likely to be able to do that's my gut
knee-jerk reaction to that at the very
least you're gonna need the crowd to
give you the training examples that
you're gonna have to train the system on
for a little while so you know the
actual no gap between the machine
learning and the and the crowd may
narrow so that it's razor thin but I
think we'll always be there</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>