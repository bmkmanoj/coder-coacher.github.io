<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Edge Detection on a Computational Budget: A Sublinear Approach | Coder Coacher - Coaching Coders</title><meta content="Edge Detection on a Computational Budget: A Sublinear Approach - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Edge Detection on a Computational Budget: A Sublinear Approach</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PBAo_EV9U6U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so welcome to the session on
computational and statistical trade-offs
I guess I'd like to thank the IMS and ms
are of course for all the support and
putting together these sessions I think
it's great that we have this forum for a
meeting of different communities that
don't necessarily come into contact and
thanks also for all the speakers that
have come and put their time I think we
have a nice line up here for this
session too so just to make make a few
comments so the session has to do with
problems that are kind of this interface
between computation and statistics we
very much seen flavors of this for
instance in the previous talk issues of
what kind of trade-offs other between
the computational cost of procedures
versus their statistical performance and
historically this is something that's
very much coming to the forefront
because of big data traditionally
statisticians at least the old-fashioned
model was you have an estimator you look
at how it behaves asymptotic normality
consistency rates but you don't really
think about how to compute it and many
estimators of course are not really
computable in poly time conversely in
optimization and algorithm designed
people haven't always been thinking
about statistical issues statistical
problems bring very specific challenges
to the table and sometimes enable you to
use algorithms that may be in a worse
case setting aren't good but in an
average case setting for that's
appropriate for statistics can be very
good so does some broad themes i think
the speakers will touch on a subset of
these lots of people are looking at
analysis of computationally efficient
procedures for statistical problems the
classical distinction in computational
complexity of course is exponential
versus poly time but much more relevant
in statistics and machine learning are
hierarchies of polynomial algorithms
things like quadratic linear or even sub
linear if someone says I have a
polynomial time algorithm but it scales
like n to the 10 and n is a million it's
essentially an exponential time
algorithm for all practical purposes an
interesting another thread that will see
is people are often using algorithms as
a form of regularization
so here there's the idea that the
algorithm choice itself is a design
parameter so things like early stopping
you run an iterative algorithm and you
decide when to stop that's a form of
regularization dropout techniques in
neural networks that's again a form of
regularization also things like bias
variance trade-offs these can also be
explored by exploring the space of
algorithms so that's again a nice sort
of interplay between algorithmic and
statistical issues computational
barriers we've heard a fair bit about MC
MC MC MC realistically is it's not
really a controlled approximation unless
you say something about mixing time and
it's pretty hard in general to say
things about mixing times for a lot of
these high dimensional and quite
complicated posteriors that come up in
in certain Bayesian models but it's an
interesting problem there's lots of
recent work these days actually showing
their gaps that for some problems if you
have exponential time in computation you
can actually achieve a certain
statistical error and there's a gap if
you only have polynomial time you can't
match that exponential time bound so
there's gaps in things like planted
click problems in sparse PCA and certain
forms of sparse regression there's a
very nice evolving literature in this
area I think the the sort of Holy Grail
here is is the following question can we
actually obtain principle trade-offs
between the amount of computation that
we can throw it a problem where
computation in a general sense it might
be flops it might be memory it might be
communication if we're in a distributed
system so can we get trade-offs between
computational resources versus
statistical performance we'd sort of
hope that as we throw more computation
we get better statistical results but
it'd be nice to be able to characterize
the shapes of these kinds of curves and
and there's again evolving lines of work
on this problem if we could answer that
question then I think we could really
get very nice answers to problems in
experimental design or sequential design
things that also known as active
learning essentially the question here
is if there's a cost to collecting more
data
and there's a cost associated with
performing computation how should I
trade off my resources is it better to
collect more samples and use a cheaper
algorithm or is it better to collect
fewer samples and spend more time with a
more computationally intensive procedure
so these are some broad questions and
each of our speakers I think we'll hit
on some subset of these questions and so
our first speaker will be Bose nadler
he'll be talking about edge detection on
a budget perfect thank you guys away for
a really great workshop and I'll talk
about a particular problem within the
instances that marked in outline
highlighted and this is joint work with
a in bulk wave who was a master student
who actually did work on end but in my
lab karuna twice one and a radius
castell it UCSD so this will have a
slight overlap with the slide of gesture
from morning in many applications we
observe a lot of data and I've me and
Martin many others have been trying to
think what L interesting statistical
problems related to that so one is that
you have so much data it does not fit or
cannot be processed on a single machine
and a very common approach nowadays is
distributed learning and then you can
ask yourself how much accuracy did I
lose by splitting the data over many
machines and many people have done work
on this I'm pointing out to a very
recent work of myself however this will
not be the focus of my talk the focus of
my talk will be a different problem
related to a lot of data and that has to
do with maybe you have so much data that
if you apply a standard algorithm it
will be too slow or take too much
computing power and then you can think
about two key challenges one let's call
it a bit more practical
there's obviously a need and we've heard
that in many of the previous talks to
develop extremely fast statistical
inference algorithms and ideally would
like them to have lineal maybe even sub
linear complexity and all of the
theoretical side we certainly would like
to understand fundamentally what our
lower bounds on what statistical
accuracy we can achieve when we have
computational constraints and this is
exactly what I want to show you today
for a very specific problem a problem
that came from my collaborators in image
processing computer vision has to do
with detection of edges in large and
very noisy images so we were luck so
these are general challenges and we
don't have answers to them for this
specific problem the way we formulated
it at least it has a sufficiently simple
geometric structure that we could
actually give precise answers to these
questions so what is edge detection you
take an image there is an image n1 by n2
pixels it's just a matrix of values and
your goal is to detect edges in this
image it's typically boundaries between
objects and essentially you can think
about it as there occurs in this image
so that if you take the gradient of the
intensities along in the normal
direction to the curve you get large
gradients okay it's a fundamental task
in low-level image processing they have
it a well study problems there are tens
or even more of different algorithms in
a well-understood theory our interest
was detecting edges in noisy and large
2d images or even 3d video you can think
about tracking and division for lounge
is clear we just have larger and larger
or better and better high-resolution
cameras so you can easily get images of
thigh is a thousand by thousand pixels
but actually there are also
service especially satellite ones where
you have images of sizes of 40,000 by
fifty thousand pixels and more
motivation for noise so again in some
applications you have images and non
ideal conditions like poor lighting or
fog a very service applications you can
also think about real-time object
tracking in 3d video where we have very
short exposures and very noisy frames ah
I am going to have one image prior and
that is that I'm going to be interested
in long and straight edges what's a good
image so here's just an illustrative
example this is actually anyways we took
out from the web it's quite large
roughly 2000 x 2500 pixels so for
example one of the most common
helicopter accident is that they crash
into power lines okay so everybody can
see this power this power line and this
transmission power and these power lines
can you see there's another one here so
if there's another one here and you I
mean there are very faint lines here as
well okay alright so what you know if
you open Wikipedia edge detection you'll
see that the most common algorithm to
detect edges is by John Kenney it's
called the canny edge detector what it
does text this image goes pixel by pixel
at each pixel computes convolved it with
a Gaussian to attenuate the noise a
little bit and then looks if they're
high gradient if there is it says this
pixel there is an edge so it's in a very
fast algorithm complexity lineal in the
number of pixels but it's locally nature
so it does not really work well at low
signal-to-noise ratios and if you apply
to the image I showed you before it
easily finds all of these power lines
but it of course it finds the bushes it
does not find anything
in the second transmission line so at
the other extreme the more sophisticated
method that are global in nature a and
we also have some theory for which edge
strength we can detect this is also
where practice and theory depart because
these methods are theoretically
efficient their multiscale they run in n
log n time it's just there is a constant
there that is quite high so they're
practically slow so I took the algorithm
developed by a somewhat collaboratively
blonde who is an expert on multi grade
and yeah in practice the trans after
like minutes on single images and hours
on video it works well it's just very
slow you need to better control the
false around but it easily detect these
power lines eh it just takes minutes on
a single frame so the challenge or this
is a toy problem but but the challenging
in similar problems is you want to
devise on the one hand algunas will be
extremely fast but on the other hand you
want them to be robust to noise and what
I want to show you is an approach to do
so with which is sub linear in other
words without I want to detect long
edges so think about the following set
up you have hard well it takes images it
may take many images you want to detect
if the long edges in them but just
transferring the data from the memory to
your computer is extremely stuff that's
one of the bottlenecks you want to
detect it very structure without even
touching all of the data in other words
in sub linear time and so a specifically
in complexity which goes like n to the
Alpha with the alto
is strictly smaller than two and once
you formulate this type of problem many
many questions arise so one is
statistical what what did you lose your
you're not going to touch off the data
so obviously you will lose something and
the question is exactly how much what's
the trade-off it's also computational
question related to experimental design
which pixels should I observe and on the
very practical side we want to design an
algorithm which you have sub linear time
and hopefully even match the lower
bounds will get from the statistical
theory so I'll skip this so here we need
to formulate some model for the
theoretical part i'm going to assume a
very idealized model later on show you
that algorithm works reasonably well in
actual images but a model to do the
analysis is that what I observe is a
noise-free image I not co-opted by noise
additive actually in the analysis within
assume it's Gaussian and my goal is to
detect the edges in the clean image I
not from the noisy observed I and my
assumptions are that my clear image
contains only few step edges step edges
just that the intensity is one constant
on one side of the edge and a different
constant on the other side of the edge
there is a sparsity assumption in there
a second assumption is that the edges
that I'm interested in are straight and
long and it makes sense if I cannot
detect in sub linear time an edge was
only five pixels in length and the
definition what is a signal-to-noise
ratio of an edge it's just the jump the
edge
contrast / the noise level Sigma okay
and so these are the questions which
pixel should a sample what are
fundamental lower bounds and what's the
trade-off between statistical accuracy
and computational constraints so I'm
going to do that the analysis I'm going
to do on a very simplified class of
images and I'm going to do a worst-case
scenario so the class of images i'm
going to analyze is the following class
i'm going to assume that my observed
image I either contains only noise or it
may contain a single long fiber that
goes from one side to the other plus
noise I don't know neither its
orientation nor its location ah and I'll
focus on a worst-case scenario and ah
here the first lemma that is very easy
to boom it says that if you observe end
to the Alpha pixels and alpha is smaller
than 1 then probably there is an edge
that you will not be able to detect no
matter how strong it is it's very easy
to prove if you observed less than n
pixels take a single column there is at
least one row that you haven't observed
any pixels in it I'm going to pass my
edge exactly through that row so you
just you don't know that it's there okay
here's the less trivial result it says
that okay another thing i didn't mention
i'm thinking here about non-adaptive
sampling meaning that i decide which
pixels to sample in advance okay it's
not like I you can also a different
interesting problem is an adaptive
sampling where you observe a pixel based
on the result you can decide what to
observe a pixel next I'm not doing that
I set up in advance which pixels I want
to observe so if I have a bye
jet of s pixels and let's assume for
simplicity that s over n is an integer
then ah we have a notion of what is an
optimal sampling scheme I didn't I don't
have time to go into the exact details
but an optimal sampling scheme is the
one in which in a worst-case scenario he
has highest detectability power so any
optimal sampling scheme must observe
must distribute it's a budget evenly it
must observe exactly s over n pixels per
row and in particular if you decide just
to sample whole columns this is an
optimal sampling scheme for the class of
images I had before okay and this is
very attractive because this is going to
the party practical side fetching from
memory contiguous columns is is is fast
okay so I was very careful to assume
that my pixels go from left to right
what we do in practice is with search
full edges on the original image I and
then we transpose it and reapply our
algorithm okay okay I mean we could have
also had a version where we sample haul
roads regarding the trade-off between
statistical accuracy and computational
budget you may realize the definition of
obscene of edge signal to noise length
we can then pull the following theorem
that if you have budget as another issue
didn't say I write your complexity but
actually for all implementation
complexity and budget done will be the
same because we have an algorithm I
don't think I'll have time to describe
that if you give me s pixels its running
time is roughly s it's s times the
logarithmic factor so one time
complexity and and budget are the same
so that's it
the budget of n to the Alpha pixels up
two constants the edge strength that you
can detect office scale as follows so
here is alpha so remember if alpha is
less than one in a worst case the strong
edges that you cannot detect if alpha
equals two you've seen the whole image
you can do whatever you want and alpha
equals one means that you can see maybe
one or two or three columns and any edge
which is below this curve you cannot
detect asymptotically at this one n is
very large and any edge whose strength
is above this curve is easily detected
if you go to our paper we show
empirically that our detection algorithm
exactly traces oh ya know so you can
talk about either worst-case
detectability or average detectability
iso another Pope to discuss availability
we need to have a prior not apply ole
ole assembly solution when I just come
from yeah okay yeah but this is this is
a worst-case scenario if you wish ah i
should say that given that pole alpha
between one and two is sample hall
columns actually we we we our detection
powerful edges is the same regardless of
the location and orientation so okay so
here is so that was the theoretical part
I'm moving I'm shifting gears to the
yeah sorry just one question sir so is
it really back sense yeah basically
specific problem this is a minimax
result other constraints allocate
exactly else which will be left to right
oh yeah don't but you know that the full
extension goes from left to right and
the width can be a little even one pixel
wide now it's unknown and it could be as
small as 1 pixel in the power lines I
really like in the power lines i showed
the image with the power lines the the
width is two or three pixels why because
it's a multiple hypotheses testing ball
I have many many different orientations
so oh sorry so then I did notice things
so you I thought you know that it goes
all ok because you still go for next to
write you get a lock factors ok now ok I
miss ya ok that's the lesson which I
analyzes all you need is a little just
fuel noise there's no edge or the reason
it I mean this is a fiber actually goes
from left to like it just need a good
starting point over 60 ok so I've come a
little bit of the theory let me shift
gears and described in two three worlds
the algorithm so what we actually do and
it also relates locks this question in
practice edges not necessarily go from
left to right so what we do in practice
is we sample several strips ok because
an edge may only go from here to here ok
in each sleep we look if there are edges
in it or not let's assume that here I I
detected there is an edge here I
detected that there is an edge then I I
also fetch the pixel values only in this
region ok so for those are knowledgeable
there's something in statistics called
group testing this is somewhat analogous
to group arresting I'm looking at the
pixels here looking at the pixels here
if I don't have any evidence for an
Edgehill awful an edge going through
here I'm not going to look at the pixels
in between ok so that's why there is a
simple geometric structure that allows
me to to have a sub linear time
algorithm now many other things that you
need to do for example a you need to
validate if there is an edge you need to
localize it if it doesn't go from left
to right only somewhat in between let me
show you some results so he
is just a simulated image you see
various rectangles signal title issue is
one it's not very low our visual system
can easily see everything here this is
the result of Kenny this is our
algorithm we don't detect the base
because they're just too short for our
method here is a compulsion of our
algorithms runtime with a supposedly
efficient multiscale algorithm to detect
a straight edges this is a image with n
so this is the end Logan or something
ours is it's it's it's subliminal so it
increases but you have this it on such a
scale and here is the result of an
algorithm on the image I showed you
before ok and this is roughly looking at
something like maybe ten percent of all
image pixels so we sampled I don't know
maybe eight strips or something by the
way ah 11 this is just an image with you
from the internet it's foggy it's not
Gaussian noise we estimate the noise
level only from the strips in which we
sampled no tuning parameters with the
algorithm other than that question yeah
no no no no no we sample this strip this
strip this trip we equispaced strips ok
yeah you could think about a
checkerboard design it would not be
friendly to the architecture on the one
hand it will not be friendly
computationally on the other you know
another few images a forest with with
the road so these are the images with
texture we can still do something
reasonable a soccer field and I'd like
to
summarize what I've shown you is on a
very specific problem that for that one
we could actually study in detail the
statistical versus the computational
trade-off I think we developed possibly
the first sub linear edge detection
algorithm were not aware of other people
who have done it there are many open
challenges and questions specifically
for edge detection you know this is
basically a proof of concept you really
need to be able to deal with slightly
curved lines with valuable contrast with
with texture etc a real challenge is how
to do this in 3d because a 3d video is
immediately hundreds of gigabytes or
several gigabytes can easily be and I
think in more general understanding this
trade-off between statistics and
potential constraints for other problems
with their in image astrophysics or
other big data probes in general I think
it's a fundamental an interesting
problem you know so this which is a very
long path come and join us the road to
infinity thank you yeah
go ahead it seems like I made you up but
it seems that to english and the subject
at higher depends on only using long
action attention so if you look at short
edges i immediately so have you have an
object anyone who find something very
small you cannot do it in sub linear
time doing the small inches it's a much
more difficult problem I think you can
probably show that you cannot do it in
something your time all right think
about the an image will there is no edge
it's just you're looking for a spike a
like a star in espoo physical ealing for
a very small region with high intensity
how can you do that if you don't observe
all of your image ok we can we can use
the same ideas but now paddle do it this
in parallel quite splitting the image
into different policies not immediately
polarizes I can swap a on a GPU
architecture you can each strip sent to
a different GPU and then gather the
results yeah sorry going to to be cursed
few students to assume the curves are
long would you I'm just curious if you
would guess that the time being
sublinear there's it would depend on the
average curvature for second degree that
I have to assume a bounded curvature
otherwise it will not do it do you think
there's a critical curvature than where
you go from sublinear to linear that's a
good question we're working on it right
now and I haven't thought about India
yeah yeah also depend on things like the
thickness of the lines relative to the
density of the pics pixels I mean you
don't need so does it I mean I'm
thinking that maybe some of the work
could ultimately have impact on the
design of the instrument used to measure
an image if you want to find a line of
us because there's no point measuring
pixels a too high resolution
detect the lines you're interested in
okay so as follows if you if you're
looking for objects which have a minimal
width you can so to speak subsample but
if you want to detect edges which can be
a one or two pixels wide then you cannot
subsample so each year microsoft
research helps hundreds of influential
speakers from around the world including
leading scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>