<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>IEEE eScience Keynote: From Genes to Stars | Coder Coacher - Coaching Coders</title><meta content="IEEE eScience Keynote: From Genes to Stars - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>IEEE eScience Keynote: From Genes to Stars</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4eoXd_ujKGs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged showing internet or public
display is forbidden and may violate
copyright law
you
welcome to the second day of the new
science conference and I'm very pleased
to welcome you to another beautiful day
in Beijing we're very privileged today
to have Alex our layers our keynote
speaker talking about from jeans to
stars and Jim Gray's fourth paradigm let
me just read you a few words from his
bio he's the Alumni Centennial professor
of astronomy at Johns Hopkins University
and also a professor in computer science
he's director of the Institute for
data-intensive science he's a
cosmologists works on statistical
measures of the spatial distribution of
galaxies and galaxy formation a
corresponding member of the Hungarian
Academy of Sciences a fellow of the
American Academy of Arts and Sciences
and in 2004 he received an Alexander von
Humboldt oh well warden physical
sciences and in 2007 he received the Jim
Gray award in two thousand eight and
eight he became doctor honoris causer of
the yacht ross university in budapest he
enjoys playing with big data it says
here well he was one of the ones who
started the the Big Data revolution in
astronomy with the Sloan Digital Sky
Survey and did some pioneering work with
Jim Gray using databases in astronomy
with the sky server project so alex is
an astronomer but his interests range
from computational fluid dynamics to
genes to astronomy and almost everything
so let's give Alex a round of applause
and look forward to his keynote
Alex thank you very much Tony and it's
very very nice to be back here in
China's so I would like to talk today
about the journey that we have
undertaken this gym over the last almost
last 20 years and so the title of the
talk is from jeans to start trying to
kind of give probably some credit to all
the different areas of science we have
tried to touch and so what we are faced
today is that we have big data in
science and the data is growing
exponentially it's doubling every year
and as a result all science is becoming
increasingly data-driven and furthermore
this is happening very rapidly one
particular additional challenge is that
we are on there we live in an age when
it is expected from us to actually
publish the data to make the data open
and public and of course we the people
who practice this know all very well
that this doesn't necessary mean the
data is accessible so the fact that we
actually make the data onto a public
website is doesn't mean that people can
use it it's quite quite a lot of work to
pass this particular hurdle and we over
the years we have come to understand
that a lot of these steps in this
process are non incremental so as the
data is doubling every year we just
can't afford to buy every year twice as
much computing we need to change the way
we approach these problems we see
another interesting thing happening for
a long time science has become
increasingly reductionist so so we
started out this natural philosophy
which is span of physics chemistry
biology and so on and then later on in
various subdivisions you know
theoretical chemistry experimental
physics but what we see today is that
through computing and big data we see
another convergence between life and
physical sciences so people who do
systems biology are using very similar
simulation techniques to those of in the
physical sciences the other thing is
that when we talk about Big Data we
shouldn't forget that it is not just
about the terabytes and petabytes of
data sitting in really big data sets in
small number of big data sets but there
is also a huge number of small data sets
lying around indeed much of the
scientific data is actually in these
small files and we have to deal with
that challenge as well but so we should
realize that today there is a genuine
scientific revolution taking place which
hasn't happened all that much in the
history of science so we live in a very
special at a very special time so
there's hardly a week goes by that in
the New York Times were in Washington
Post or in The Times there is not an
article about big data especially in the
context of genomics which is really
expanding so here several talks about
this and this is a slide that we built
together with Jim which shows how
science is changing so how a thousand
years ago science was entirely empirical
and you know the this was one of the
earliest studies in turbulence when they
will not go through the turbulent flow
and put it in one of his codices then
subsequently starting with Kepler we try
to capture nature in simple analytic
equations which also had a simple
analytic solution so that was the
abstraction derived from the empirical
data and so will be developed a
theoretical branch of science starting
with Fermi and lomb during the Manhattan
Project we you started to use computers
to solve some of these equations which
may have
very simple in form like the
navier-stokes equation but the solution
can be the arbitrary complex and what we
see today is data intensive science or
science or data-driven discoveries which
is really a synthesis of many of these
synthesizes theory experiment and
computation the statistics and all in
all this requires a new way of thinking
about the scientific process so how does
the scientific data analysis scene look
like today so first of all the data is
everywhere it's not never ended pepper
will be in a single location the
exponential doubling comes from the fact
that every year we built much cheaper
sensors with much higher capacity think
about the digital cameras and the CCD
chips in our cameras I have already gone
through several generations of digital
cameras over the last few years and I
think this is at the heart of the
sequencers the high throughput
sequencers has to put the new telescopes
and basically this is all driven by
Moore's law the same silicon technology
which enables us to every couple of
years to build devices which have twice
the number of transistors but what
happens think about a little bit how do
we do our analysis if our data is
doubling every year and say roughly
Moore's law says that the computers are
doubling the every year for simplicity
so basically as the data is coming in we
can do the pipeline processing we can
keep up with the data but if we want to
do a statistical analysis which is at N
squared algorithm like a cluster
clustering of data then suddenly if we
have an N squared algorithm the task
when the data is doubling the computers
double in size
quadratic algorithm they'll take four
times as long a cubic algorithm will
take eight times as long so sooner or
later we will live in a world where with
the big data the only algorithms that
survive are n log n anything the scales
worse than that people just not be able
to execute so we will need to rethink
how we do our statistics because a
minimum variance estimator if we have n
data points requires the inversion of an
N square matrix which is n cubed
operation so basically forget it so we
have to come up with algorithms which
are incremental and first of all their
heart is in the idea that the
computational cost is a very substantial
part of the cost function so we have to
be able to compute for a minute for an
hour and we should improve our
statistical accuracy if we compute it
for an hour and if you compute it for a
day but it will be our decision when to
stop and that should be part of the cost
so the statistics has to be computable
in finite time that's the SS at the same
time the computer architectures that we
have today are increasing the CPU heavy
and I'll pour the distance between the
disks and the CPUs is growing so we need
to rethink what computer architectures
can be used basically for such
computations and again I have to go back
to Jim who in the mid-90s wrote a paper
about cyber breaks where he postulated
that there will come a time when the
disk controllers the chips in the disk
controllers will be fast enough that
they can do the processing of the data
on the disks he called the cyber bricks
and we are getting very close the latest
solid-state disk have armed chips on
them and also basically in slot of the
tablets we have quad-core arms soon
where we all have eight core arm chips
in there so Bo can very easily imagine
that just one of those chips is the disk
controller rest of them does the
processing so the word of the side
which has come when we look at how is
the data analysis done still today most
of the scientific data all is done on
small to mid-sized valve of clusters
which are typically have pretty pure io
they don't have very much disk space so
they are completely inadequate for the
job but that's what we have and they are
putting broom closets and soon the
different departments melting down the
building's basically through power
because the faculty is trying to buy
more and more of these computers which
are inadequately cooled in it that
quickly maintained so basically this is
not scalable and not maintainable people
have to do something else so Jim
throughout the years as we work together
on the Sloan data we realized that there
is a lot of these things are coming so
his first law was our first rule of
thumb was that scientific computing is
increasingly revolving around data today
it seems pretty trivial but at that time
it was all about supercomputing so
scientific computing was about grace and
the other next postulate is that we need
a scale-out solution for the analysis
and again at that point there were so
this is what we see today so Google and
Facebook and and Microsoft they are all
building relatively low very large
numbers of very cheap servers to do the
data analysis in the cloud the next
thing was the realization that the
network speeds are falling behind the
growth of the data so we will not be
able to move the data fast enough just
in time for the analytics and this was
at the time when the computational grid
was very fashionable and the grid was
very good for certain things where you
had
small number of parameters you took
basically those parameters to where the
computers you Randy jobs and you got the
course a few numbers as the result and
you cleaned up after yourself this work
very well in this work but it doesn't
work if you have to move terabytes to
where your computers and then move
terabytes back so so Jim's postulate was
that we have to take the artists to the
data this is what we see now every day
in cloud computing he had a very good
heuristic way to communicate with domain
scientists he said that don't write a
requirements document just tell me what
are the questions that you want to ask
and this was the first thing when he
asked me when we first met and then I
said okay so I want to ask anything and
everything from my data and Jim laughed
and he said okay why don't you just give
me the 20 questions that you want to ask
and the first five was really easy and I
could do it in a few minutes next five I
had to scratch my head and after that I
have to go home and this taught me
humility that okay not every question is
equally important and there are some
questions that are trivial and I want to
ask every multiple times every day from
the data and so will everybody else so
those really have to run very fast and
this was much better than a brain dump
of a requirements document which had the
kitchen sink in it because it forced me
as the scientists to prioritize and this
worked extremely well ever since and his
last posture that was go from working to
working because in this world there is
one thing constant in the world of
computing that everything is changing
every two years the commercially the
distributed computing paradigm so we
have to build such applications which
are able to grow and evolve and so when
I started in distributed computing on
the sloan which was still a word of
korba then it was computational grid
then it was web services grid services
today it is the cloud you can take we
can be sure
in two years there will be something
they asked Anika so a lot of essays have
been written by James friend and put in
this book which is downloadable from
Microsoft I really urge you to download
the 1010 live through it so what are
these non incremental changes first of
all we don't just need to change the
computational architecture we need to
change the statistics we need to change
the way we do the data acquisition we
have to change the way we do the data
analysis and so this means that the
people who do this have to understand
not just statistics not just computer
science not just astronomy or genomics
but kind of all of the above so this is
the and it is and they are not just
doing twenty percent more we have to do
to factor of two more every year so we
have to change the ways we approach
things and the other apparent thing is
that science is moving increasingly from
quote-unquote hypothesis driven to data
driven discoveries and of course people
I always get into arguments when I say
this because in a sense so I was
wondering why our astronomers so easy to
convince that this is an interesting way
to adopt and I realized in the process
that astronomers have been always be
have always been data driven since we
cannot run experiments in the ordinary
sense on the light on the desktop so I
go in and change the mixture of the
chemicals or trika dial or that we can
do is observe the sky and we can do of
course two kinds of data-driven
discoveries the first is exploratory and
then we derive hypothesis from the
exploratory data and then we do
confirmatory analysis and bestow
confirmatory we actually then get we do
have hypotheses going in but
generally when we did when we discovered
supernova we didn't have hypothesis that
this is the end of stellar collapse or
the stellar evolution which are soaked
something doping on the sky and the
first supernova appeared i believe on
the chinese star charts so how did I get
into this so I was a cosmologists
working happily on doing correlation
functions and power spectra of the
galaxy distribution and in 1992 hopkins
joined us on digital sky survey it was
roughly at the same time when the human
genome project started it's not an
accident so basically the technology of
the large ccd mosaics has become mature
enough that you could also basically do
the processing of the genomes and we
could also be a big enough mosaics to
cover the sky it's a collab it was a
collaboration of a bunch of private
universities and SF tasks or the soul
foundation and so on and we set out to
map the sky what was visible from North
America we built a special telescope and
they took two and we plan to do two and
a half trillion pixels of images because
thought we would collect about 10
terabytes of raw data and build a
database with half a terabyte and it
also thought we would finish by 2000 by
building everything and running the
observations well we didn't even start
so everything was delayed everything was
a bit harder but as I read in the end it
took us 18 years actually 16 years to
finish but most welcome to the rescue
Kreider slocum to the rescue so
everything became cheaper so they could
be much more ambitious with the data
aspects so we double the sky coverage in
the end we have not 400 terabytes of
data that we have to archive and the
database is close to certify terabytes
and the database is run out of song out
of hopkins and so this gym we started we
actually build the structure where the
prototype of the sky so we're
essentially in two or three months in
2001 we didn't get much sleep really and
after that a whole bunch of people came
in and helped and so it there is now a
lot more toward in it and you know the
sweat and blood of a whole group of
people but in 12 years in hindsight we
have we exceeded our billion buckets we
run 200 millions sequel queries which
were actually not connected to the web
services but these were coming in from
the outside we have 4 million distinct
IP addresses of users were just about
15,000 professional astronomers in the
world so this is really remarkable how
we how broad an audience we managed to
attract and what we saw these the
emergence of the internet scientist
person who is not a professional but is
actually interested in doing original
research if there is an interesting
enough data and as a result this became
the world's most use astronomy facility
today and so this just shows some
traffic graphs and we are still growing
so the traffic is still increasing one
is the altogether the back seats and the
other one is the sequel queries sequel
queries are now flattened off the big
spikes are basically the you data
releases we have one day to release
every year and we don't throw away the
old releases we hold on to them like we
don't burn the old editions of a book we
just put next to it a new edition of the
book on the bookshelf and but about such
a way having a yearly edition of the
data release makes it very clear for a
paper to code that which version of the
data is being used okay one unique
feature that we built about two years
after the data release is the mighty be
immature there we give our power users
the people who started to run really
complicated queries and workflows
they own server side database so instead
of typing the query output through the
internet to the home computer we allowed
them to write it into their own database
right next to the main data server that
they were in full control and we have
now about 7,000 people using it so
roughly half of the words astronomy
communities using this feature and over
the years he developed into a full
collaborative environment people even
published the datasets associated with a
paper and made it word visible not just
grew principle but work visible and so
yes this is actually delivering so
sending analyst is directly to the data
fairly complex analysis workflows this
shows by the way how based upon
citations we were the number one for
several years in a row we have been the
number of an astronomy facility of the
world and the goals were to provide easy
access to exciting new data and we went
and visited David Lipman with Jim when
we were in the process of design in this
and David gave us a very interesting
paper which said that if you design a
form based interface to the database you
have to provide a backdoor for the
scientist because if the if the forms
designer was a mediocre program again
only you can only do those search
patterns that the programmer allowed you
to do you have to provide basically a
free-form access so that it nothing
limits the creativity and imagination of
the scientists in any case with the fact
that the power user cut the only this
was really the first example of cloud
computing in two thousand working in
2003
so in 2007 there was a group in Oxford a
group of students in Oxford who had the
idea that why don't we post a bunch of
images on the web from this lingerie and
ask the public to visually classify it
and it helps to have Brian May helping
from the Queen he was the guitarist a
queen who got his PhD this year and he
loaned us his web designer and who
designed a beautiful site and then also
helped us to publicize it and it hit the
BBC nightly news and the next day we had
300,000 people classifying galaxies and
what was remarkable not just the hype
about this so that was nice but there
were some genuine the important original
discoveries made on this data by
amateurs so this little green thing on
the side was classified by the telescope
software as a reflected light and you
can see there is a lot more structure to
it so and the Dutch schoolteacher the
honey fan arkin has written a blog about
this that she thinks that this may be
more interesting and it turns out that
it was an entirely unique object which
has since been observed by the Hubble
Space Telescope several NASA satellites
the big radio telescope array in New
Mexico so and she wasn't thirsty rotor
and all those papers so we built the Sky
Survey and mighty bid the server-side
analysis environment for the users and
over the years the span of into a whole
slew of different projects from
radiation oncology to environmental
science to the Galaxy Zoo to numerical
simulations to the Hubble Space
Telescope whole archival system is not
being converted to this a lot of the
virtual Observatory tools the palomar
telescopes observatory and of course
this
was actually a spin-off of the terror
server where the gym built as a torture
test for sequel server which was just
acquired by microsoft at the time from
sybase and so we turned out when we
started to work together so Tom Barclay
gave us a lot of his code we actually
recycled some of the terrorists were
called returned the telescope from
inside looking down on the earth out the
space but we used a lot of the similar
geospatial ideas and basically
generalized it and now we generalize it
even further so this is an example of
the unko space but they basically took
the sky so remember but applied it and
converted it to do radiation oncology
pursue an active database where
basically they follow the treatment of a
patient and compare it to all the other
patients in the database who has similar
symptoms and got a similar treatment how
well the patient is following the
treatment plan this is now toshiba is
not financially to into a startup so
life under your feet was another project
where Jim and I basically wrote a whole
bunch of time serious scripts in a
matter of two or three weeks and we put
out a bunch of sensors to study the
carbon cycle measured from the soil they
put out a bunch of little wireless
sensors and this shows that the kind of
we started in the middle of 2006 and you
can see the number of censored these are
the number of cents or samples collected
we are today we are out of here and 200
million samples and our samples include
Brazil the Atacama Desert in Chile
then a whole bunch of places in the US
and we are now doing the deployment in
South Africa Finland Hungarian and again
the u.s. second half of the talk I would
like to talk about a new kind of
instrument the regenerates even larger
amounts of data and these are the
supercomputers HPC instruments and so
what we see today is that the largest
simulations are approaching petabytes
and this happens from you know supernova
to turbulence and also to brain modeling
and generally a lot of the analysis is
happening on the fly as the simulation
is running you analyze the data and then
you throw it away and this is not good
enough so there is more and more
pressure from the public who want to
compare for example experiments to
supercomputer simulations they want to
access the best and latest and the
biggest simulations and this creates a
whole bunch of new challenges in how to
move the data how to look at it how to
interface how to analyze and what
architectures do we want to use so this
is an example when we had to move
hundred and 50 terabytes from oakridge
in 10 days because we got the phone call
that unless we move with it will be
deleted and it was much easier said than
done this was a few years back today
it's much easier so we were able I think
a couple of weeks ago to move 50
terabytes from from Texas in a matter of
I think one afternoon so how do we
visualize a petabyte today generally be
Duda visual analytics in taking the data
and copying it to an workstation with
the beginning of GPU on it and then
basically do the rendering there very
few people have access to the caves but
this petabytes we can't do this so again
we are back to take the analysis where
the data is staying the visualization
where the data is because almost
everybody has now a mobile device in
pockets which can receive a
high-definition video stream so
basically we can stream the results and
this is limited only pretty much by our
perception so everybody in the world can
actually get a video stream that is as
much as our brain can capture so we have
to do the rendering where the details
and i will show a few examples so what
are the usage scenarios for this big
data from simulations there is a life
cycle which we have to understand so
today most of the things are done on
this using the on the fly analysis or a
private reuse where they put some of the
snapshots onto a scratch disk and
analyze it okay but there is an
increasing pressure for public reuse the
only way for the public reuse or almost
the only ways to make some of the files
available it's okay with a few terabytes
it's not okay with 100 terabytes and
it's undoable over a petabyte and at the
same time there is if we want to keep
things it would be make sense to build
smart services on top of the data which
actually deliver intelligent data
products but the simplest this requires
a bunch of investment in time and
programming so it only makes sense if we
want to keep the data for a medium to
long term and then we have to think
about for the most important reference
data sets about the long-term archival
and curation so I'd like to show two use
cases here one is from turbulence and
the other one is for cosmology
turbulence is interesting because it's
entirely classical physics final said
it's the last unsolved problem of
classical physics and about eight years
back with Charles Minerva we started a
project to take snapshots of a large
1,000 cube simulations simulation and we
store the thousandth time steps in a
database and we built a very
fine-grained spatial index over it and
we came up with a new metaphor how to
analyze it
this was around the time than the movie
twister was popular so imagine that you
have a turbulence they bought the
tornado in a box and instead of driving
up to the tornado with a car and shoot
an accelerometer in it you take your
laptop and shoot sensors into the
tornado in the comp in the database and
then the sense was reported by the fluid
velocity wherever they are so that's the
metaphor and it worked like a charm you
can do depending on very place the
particles you can do all sorts of
analysis patterns and so for example you
can run time backwards you cannot do it
with a dissipative simulation because
they're the arrow of time is not
reversible but we can put down particles
in a loop and then moved and backwards
using the interpolation and integration
so this is the daily usage of our system
from all over the world on a typical day
we deliver about 100 million points and
over in 2011 we exceeded 100 billion
voice points deliver to the world and so
this is a recent paper that was appeared
that appeared this summer in nature
where Greg iron from Hopkins solved a
long-standing problem in naughty to
hydronium turn turbulence what it took
is to put down a whole bunch of
particles several million particles in
small volumes and then integrated
trajectories backward adding a
stochastic random perturbation term and
then once he computed the orbits that he
integrated the magnetic the evolution of
the magnetic field forward along the
trajectories and was able to show that
they satisfy a certain scaling law that
was predicted in 1923 by Richardson but
nobody was able to prove it ever since
so this led to basically really major
publication in one of the most respected
academic journals how can we do
visualization with large amounts of data
this was an experiment also with the
turbulence stage
bass so basically popped a GPU card into
the database server for maximal enticing
the bandwidth and we stream the data
from the database directly into the
visualization engine and did the
following visualization on the fly so
let me see if I can trigger this yeah
okay so basically hope and Optus that
this you could see the same movie in
real time over a lot with tens of
terabytes of simulations so so this is
embedded visualization being integrated
with a relational database and very high
speed so the data was streaming at
several gigabytes per second okay and
today the turbulence is after the nature
paper it the word has exploded so we had
basically two databases in use the 30
and 50 terabyte we are loading now from
University of Texas 100 terabytes a base
which is a channel flow then we are
getting from argon a five-minute error
by database of another
magnetohydrodynamic turbulence
simulation but with much higher
resolution then somebody Los Alamos is
running 20 terabytes rotating fluid
somebody else is doing the two fluid
mixture that simulates internal
combustion engines and then Los Alamos
somebody is planning to run a six
hundred thousand six thousand cubed box
with about a thousand time steps so that
would be a world record in simulations
and the plan is to bring it to Hopkins
and turn it into a database so in
cosmology it also started around the
same time so in Munich I was on
sabbatical in Munich and worked with
Gerard Lamson there and we took a large
simulation that was certain terabytes of
data but it turned out it was on a tape
robot in the referent centrum and nobody
was able to touch it and so we took the
top one terabyte which was just the
galaxies and not the dark matter
particles and put it into the salon
framework and made it public and put it
also a mighty big database next to it so
people could stream the results anyway
so over over this time do we have now
600 registered users in this database
millions of queries and there was a
workshop at last December in garching
with 50 top users of the system came and
gave talks about what science did they
do with it and also what features would
they like to see and it was really
remarkable so they want to push the data
sizes so people are now starting to run
petabytes I simulations in cosmology
this trillion particles of dark matter
and they want to store hundreds of time
steps of this and want to look at the
dark meta trajectories so there are a
bunch of good questions how is the data
stored how does it get there there are
all sorts of different computations that
they want to do on the data a posteriori
some of them are very localized based on
individual galaxies they just want to
create fake galaxies in the simulation
by changing the recipe of star formation
so right now there is this is pre
computed and everybody can compare but
there is only one galaxy formation
scenario and people would like to be
able to play basically in a 70
dimensional space parameter space then
there are all sorts of rendering issues
so how could people create fake
observations what would the Space
Telescope see if it observed this
simulation and of course the problem is
that cosmological distances the light
travel time is important so as I go back
on a light pole I have to look at
earlier time steps of the simulation I
have to look back in time so this means
we have to store actually enough
snapshots so that I can interpolate in
time well enough
there are all sorts of global analytics
where I have to take the whole
simulation volume and basically do a
Fourier transform alla density great and
there are all sorts of issues with data
representations we are doing another
thing that Hopkins where we are taking
not the biggest and later simulation but
we try to take a medium-sized simulation
but we are running five hundred
realizations of it so that we have an
engine where we can compute statistical
ensemble averages and covariances and
actually much of the computations are
done by driven who is here at the
National Observatory paging and we are
currently at hunt we are currently at
100 simulations and exceeding 200
terabytes so this is all loaded into the
database another thing is the Milky Way
laboratory which is a project run at Oak
Ridge or it was right the first half was
around at oakridge and the second half
will be run probably in Switzerland but
here the idea is that we will be able to
shoot test particles at West test
galaxies a posterior into the simulation
and see how they get disrupted in the
time variable gravitational field so
people can kind of play almost play God
in this simulation okay so how do it so
what we have seen in the simulations in
the database that in seven years that we
have been doing this there's an
incredible progress and so the community
especially in cosmology they are playing
the database as if it was a musical
instrument so they are not just kind of
making it's not just using as a tool but
they are using it in an artful fashion
and they are using it for things we
never imagined able to visit and we have
all these new challenges but it's not
about storage so we are way past the
stage when this when the people are
happy just to run a sequel query on this
but it is clear that we can't use a
supercomputer to do the posterior
analysis either we need something in
between which is halfway between a
database server and halfway between a
supercomputer and so we are trying to
write a middleware at hopkins called MPI
DB which would basically use MPI costs
25 data in and out into a large
distributed parallel database but so
what are the architectural challenges
how can we build a system that's good
for such a task well certainly not at
the super computers because
supercomputers have this space which is
very very fast usually but it's also
very expensive if you want to store
petabytes and we want to keep them there
for years that we have to do it cheaper
so computations and visualization must
be on top of the data so it's not a
database server I there because we have
to have more CPU cycles and more GPU
renderings available than what you
normally have in a database server and
so all in all what is utmost importance
we need a very high bandwidth to the
source of the data generally we have
been using databases very well but there
are all these myths about databases that
or databases are not scalable okay and
people said that this is why people
Google build MapReduce well the answer
is wrong google has now been building
actually proper sequel databases on top
of the big table framework so they have
several dremel tending Spaniard these
are all sequel compliant databases some
of them are global but what we also
learned the databases are very good if
we have find if we have to have fine
granularity data access but we also need
a lot of extremely sophisticated
value-added services on top of the data
so in scarcer where we have 700
user-defined functions which capturing
lots of astronomy knowledge
but basically flat files are very good
if we read the data in bulk but when we
need fine granularity access they're not
in bits databases and indexes and it
makes no sense to build master servers
so ginormous database we need to scale
out basically find kind of cheap
components like the cloud but which has
this integrated integration of cheap
storage and also sufficient amount of
high performance computing right on top
and probably we need to shove in for
some of the problems sort sort of as
some number of large memory in memory
systems so we built a bunch of
extensions to Seco server one is large
arrays so Mike Stonebreaker is building
side a B which is a relational database
where the primitive data type it is a
multi-dimensional array we rather took
this existence sequel server and we
added the large arrays to it and hoes it
is Jose Blakeley and dragon Thomas from
the sicko server group and it has a
whole bunch of matlab features so we can
take a binary area in the can slice and
dice it aggregated and sub set it in t
sequel but there is a new addition in
the new version of secrecy recruit file
table because it has been a pain to load
data into databases especially this
large binary data because everything is
stored in a trilobite pages so you take
a large data set you shard it into a
trilobite pages you put a header and
every one of the pages and then you
build a bit three of those and when you
want to read the data you have to
reverse and in sequel server 12 there's
a new feature called 5 table but you
just copy files into a directory and
they automatically appear in the
database but you can also access those
files from the database side and you can
build an index on to the file contents
in the data base side and using the
so-called sequel bytes interface in
c-sharp in the clr you can
to receive into the different offsets
inside this file so we can essentially
we can write our own native binary file
format but still use it with the
database it's fantastic and so this is
what we are working right now so here
the fast we could potentially read the
directory the simulation outputs as they
come up the supercomputers and it's
loaded the other trick is how do we use
GPUs and this is just one example so we
had a simulation with 400 million
particles of I alacrity of simulation
and one of the challenges is to compute
the dark matter and halation that we
know that there is dark matter we don't
know what it is but there is a satellite
up there which is looking at the
gamma-ray sky and it sees a little bit
of excess towards the dark tech center
which people hope that it may be the
first elusive sign of Directors in Dark
Matter so this is very hot so we took
the simulation and computed the Dark
Matter inoculation man and that spot
there is the galactic center in the
simulation this is like a milky way
galaxy box originally it was the guy who
run the simulation my coolant who is a
faculty with in Berkeley it took him
eight hours on a cpu to run a single and
halation map and then be one of my
students has rewritten it in OpenGL in
Windows and we can run it now in 24
seconds so this is fine this is a
thousand times increase we did because
we realize that this is really a
rendering problem but what the 24
seconds enables us now to play with the
particle physics we can change the cross
section the physical cross section of
the dark matter and we run 20 different
scenarios and for every one of those
with pre-compute the we can recompute
basically the undulation map shown on
the next image and it is the same patch
of the sky
the four correspond to different types
upon generation whether we have the
so-called Zoomer felt correction in it
anyway so we can at this point exclude
quite a lot of the physical processes
that are around for the dark matter
simply from these computations because
they already conflict the existing
satellite measurements so we are
submitting the phils reflectors paper
about this so this is a fact when doing
something in 20 seconds instead of eight
hours makes all the difference in how
you do your science so what are the
district's today the disk is data
intensive scientific computing and you
know in real estate people ask okay what
are the three most important thing when
buying real estate location location
location and it turns out that in
scientific computing our biggest problem
today in the data in terms of computing
this displaces space to space and and
then afterwards the next thing is how do
we move the data and kind of hundred
terabyte is magically bit still today
which is below it is relatively simple
above it is becoming very very hard and
so I we built a system at Hopkins called
the data scope this is the newer version
of the grave off that we built
originally with Microsoft's help the ear
the idea is to build a system with
multiple petabytes which you also has
extra mile bandwidth but also in up GPU
computing integrated and we got the GPUs
from Nvidia and much of the CPUs from
Intel so we could build it actually
below a billion dollars so they
discovered this capacity six and a half
petabytes but we can hit about half a
terabyte sustained read in parallel of
course and we have all together 90 GPUs
so every machine in the average every
machine has a GPU in it
the last bit i would like to talk about
this the long tail and how it impacts
science there are two interesting
lessons to learn there's Facebook and
Dropbox what is the Facebook lesson so
Facebook brought together a bunch of
totally in a sense uninterested in
relevant data sets and data and and s
this data app approached a critical size
then suddenly new contacts started to
emerge and so new associations emerged
and so what is the science equivalent
could we build something like this and
the other lesson is Dropbox they're
basically doing something which is ultra
simple you just drag and drop and then
something happens but you don't have to
read a 50 page menu off to use it that
also sometimes simple interfaces are
much more powerful than a complex one
and they are also much harder to build
and so we are trying to merge some of
these ideas in something we call the
side drive so the idea is that we built
basically a server-side storage
environment it does nothing fancy about
this life it's like Dropbox and we
actually emulated the dropbox interface
but the idea is that once people upload
data into it we reserve the right to
automatically harvest the metadata
because all that time so far where we
are scientists to publish their data and
fill out the associated metadata forms
have been an utter failure so basically
scientists are just not patient enough
they don't have the patience to actually
be bothered with filling out those forms
and so we have to do something better we
offer them something for for a session
innocent be offered a free storage and
we will have to make do with what we can
harvest from the data
so okay so what's the idea first of all
so we did cloud store closed storage for
small data in different science
communities we enable them to link all
these data sets to each other and also
to analytic services to again there is
nothing fancy so far but if you for
example upload something and we could
harvest the header or figure out some
metadata it goes into automatically into
a relational database in the user space
so the user can start searching on for
example all sorts of key value pairs
related to this if it's a tabular data
the data is automatically loaded again
into a user owned relational database
this is similar to Seco share that bill
how built at the University of
Washington but his experience has shown
that even without the metadata when
people from a given project were just
able to do basically upload a whole
bunch of small spreadsheets which were
very hard to feather it and fuse in a
database this was much easier to write
joint queries which actually brought all
these data sets together but furthermore
we know a lot more about the people so
people will have to have a user account
from the user account we can actually
link them to a general user ID so we can
find out what the publications are and
for example Alex wait at Microsoft has
been collecting the texts of the corpus
of the scientific papers over avoid a
set of disciplines so we actually know
the social network of the authors we
know the texts of their papers so if you
see the column headers endured in the
tables we can actually look up the
papers what is the broader context and
what is the meaning so we actually have
much richer information than we then you
would normally think that we have anyway
so so we have now a 400 terabyte system
online already at Hopkins and this is
linked both to add other my debian Sloan
and also today uh sure
is that bill how they offed so we can
actually point the relational tables to
it into either of those two systems
starts to jeans class river estes so
what did astronomy teach us about
genomics and jeans and there is a huge
parallel ISM between genomics today and
what astronomy was 20 years ago before
the song so before the song astronomers
typically looked at a small patch of the
sky they took their own images took them
home then spent six months to basically
do the basic image processing everybody
had a different image processing tool
and even if they share the same tool
they use different parameters and no two
astronomers would agree basically what
is the right way of doing things and
most of the time was spent on this on
this footwork down in the trenches once
flown was published we did basically a
shrink-wrapped calibrated data release
astronomers first didn't believe that
this is good enough and they try to
repeat and to the pro in the processing
and after a while they realize that this
is good enough and it was not worth the
effort trying to repeat what we have
already done over the sky and the rather
started to run database queries over the
whole sky so what is the state in
genomics much of it is based on files
and Perl scripts and the information the
better data is typically stored in the
file headers in the long filenames
everybody's upset running their own
aligners and no two people agree on what
exactly what parameter studios on both I
or bwa does it sound familiar and they
don't use databases and but at the same
time this works when you have one to ten
genomes but it will break down when we
have a thousand genomes or when we have
a million genomes so we will have to
have databases to actually organize all
these so basically for statistical
processing and collaboration
the database behind and also find a
common processing that is good enough to
do the very basic things and spend your
time on the discovery and on the
interesting statistical analysis so some
of the things that we have done here is
if we wanted to get into very massive
processing it turns out also that the
current aligners are not quite fast
enough so we wrote our own so Richard
Wilton in the physics department who is
both a computer scientist database
programmer and a medical doctor he
worked with Ben Langmead who is the
author of bow tie 2 which is one of the
most popular aligners today to write a
GPU a liner which is pretty much faster
than anything else but also has the same
quality as bwa and bowtie and so we can
actually process thousand genomes in a
measure in a reasonable time now we also
built an ultra-fast prototype the beach
business sequel server and solid state
disks how can how can we search for
short treats in the one 1000 Genomes
data and finding both the align and
unaligned and one of the things we would
like to do is to integrate this with my
DB so that again the results of the
crisco immediately again into a user
database that can then be used to join
and cross correlate with all the patient
data and the phenotypes and the next we
would like to do a side a side drive
custom version for genomics where people
just drag and drop some of the genomics
data in it and it will automatically get
processed in the background so
what we see is that there's a big
changing sociology so this I mentioned
already the convergence of physical and
life sciences what you also see is that
there is a data collection in
ever-larger collaborations we see the
virtual observatories on every scale of
the physical word from CERN to the
virtual Islamic Observatory or I boa
ncbi and me on the ocean observatories
and what we see is that the analysis is
becoming decoupled so an individual code
can in principle going into these data
collections and then write a paper from
the statistical analysis and so what we
see is also the emergence of the citizen
and Internet scientist and this means we
need to start training the next
generation to be able to thrive in this
world and traditional scientists are I
shaped so they we trained people today
who are very deep and narrow in their
own field and people who we call to the
interdisciplinary scientists are
t-shaped so they are still still have
the depths but they have sort of a broad
but relatively shallow layer on top but
we need to have people who are
pie-shaped who are then at home in the
computational statistics so basically
understand computer science statistics
but they also have to be at home in
their own science otherwise they will
have communication problems so we need
to have this early involvement in
computational thinking so summarizing
science is increasingly driven by data
both large and small but the integral is
large the Syria changing sociology
surveys analyzed by individuals we are
moving from hypothesis driven to
data-driven science we need to build new
instruments instead I this is I think
it's more than computing what
our building is the equivalent of the
microscope and telescope for data really
there is a challenge on the long tail
data changes not only science but
society we see a new force paradigm of
Science in emerging and it's incredible
to see that how the Sloan Digital Sky
Survey has been at the cusp or the
transition and you know I would like to
finish with a quote from Harry Ford so
he said if I had asked people what they
wanted they would have said faster
horses if you are scientists today what
would they like they would say more data
and faster computers but we know that
there has to be more and and I think
this is about the science that these
science is bored by just faster horses
thank you very much
thanks Alex we have I think time for a
few questions so happy to take some
questions from the audience yes there's
a couple of questions just back there so
Mike is just coming Thank You people
yeah a whole question for that intensive
science research do you feel cloud
computing and a grid computing retrying
in vital under concede so partly so
challenges facing data-intensive
scientific research thank you I in my
mind it's clearly cloud computing this
better it's the only the business models
today is not quite right for science so
the storage is too expensive the compute
cycles in the cloud are very cheap and
they are cheaper than anyone can do but
essentially in all the major cloud
computing platforms you have to pay when
you pay for stories you have to buy your
storage every month it's just too much
so if somebody arbitrarily dropped 11 of
the providers arbitrary drop the price
by a factor of 10 then I think suddenly
people would swarm
because there you can do the computing
on top of the data obviously so another
question I have a comment and a question
first you say that science has gone from
data driven to sort of more theoretical
driven I disagree with that occurred to
me I think that science basically tries
to search insights and there are there
different points of friction according
to what the knowledge that you have and
what you can do that is when you have a
problem that you have enough Norina
theory you spend your time developing
mathematical tools when your theories
are okay then you spend your time doing
data and that is just a focus of
part-time time that you spend currently
doing one activity but the overall goal
is the same which is to look for inside
now you say we're going to towards the
data driven and I think that that is
true at the moment pain points are in
data but the next faster world a
replacement further the next car is the
sort of not faster horses is actually a
better understanding of computation
we're still stuck in a model of
computation which is the von Neumann and
the whole of Turing paradigm which we
have done very little research on and
we're very happy because all of a sudden
we have a MapReduce model which is just
to functionally Punk tional ways of
looking that we know scale better than
the others and do are you aware than of
anyone actually doing research in those
fields or is that something that should
be kick-started for you because I think
that that's where the next boundary is
well
so yes there I think there are lots of
activities going on so so in berkeley
there is the what is it called the amp
center where they are really exploring
kind of both architecture and algorithms
issues about a lot of the data different
computation i think they are doing
fantastic work then again within google
they are actually passed way past the
MapReduce paradigm at this point so that
was what they did to five six years ago
so they are doing different things and
all together as in the exascale machines
will force us again to rethink how we do
high performance computing entirely
because nothing that we do today will
scale to exascale so so so I think you
are right we will have to change it will
have to be much more power efficient we
have to get on a completely different
curves so that's that's not incremental
right away so so we will otherwise we
get stuck so any other questions there's
one right at the back that's okay sorry
about the race
and I think we could take one more after
that if there is one great look Alex as
always just wondering what your comments
were a lot of the buzz and big data is
around unstructured data no sequel most
of your work seems to be around
structuring the data to make it usable
and useful source running what your
comments were on some of the current
trends and fashions in the industry so I
try to emphasize in the beginning there
are two kinds of big data searches or or
in science data-driven approaches one is
the exploratory and the other is
confirmatory so for exploratory it's ok
to use unstructured data and noisy data
once you actually want to have a
confirmation of a hypothesis you want
very controlled conditions and very well
understood systematics and so on so
there it is clear that we will devote a
confirmatory analysis we will always
have to do basically very structured
things for the exploratory you know it's
free for all so there we can kind of
take because all that it gives us a hint
and enables us to design the next
experiment which confirms or or kind of
the hypothesis so
ok well let's thank Alex again before I
let you go for coffee there's one sort
of announcement I have to make which is
the Jim Gray award for 2013 which we
normally make at this conference well
the recipient was unable to travel so
let me just remind you that the Jim Gray
award it's awarded to a researcher who's
made an outstanding contribution is this
microphone on here shall I use this one
it's for a researcher who's made an
outstanding contribution to the field of
data intensive computing an innovator
whose work truly makes science easier
for scientists a groundbreaking
contributor to the field of Hassan's and
one who pursues an open supporting
collaborative research model and
Catherine banning and who used to be in
Jim Gray's research group like to quote
Jim preferred doers / talkers and you
just seen an example here in Alex Allie
who is absolutely a doer rather than
just to talk but it does give a good
talk as well so thanks Alex so these are
our previous Jim Gray Award winners so
Alex Carol go bald yet dossier Anthony
Williams last year and and the the the
oceanographer and protein data bank were
absolutely doers rather than just
thinkers so I think this is important
and if you read the fourth paradigm
you'll find their sections on
environment where there's articles by
some of these people all right and if
you'll see in environmental science and
in health and medical science and in
also in infrastructure but there's a
fourth section and the fourth section is
about openness open access and the open
access revolution to both publications
and data so this year it's extremely
valuable and important that we are able
to award the 2013 Jim Gray award to
David Lipman who as you heard from alex
was collaborating from the very
beginning with people like Paul Ginsburg
with people like Alex and with Jim Gray
and he and his staff at the NIH NCBI
have had a huge impact because they've
actually brought a huge amount of
literature in PubMed Central to be open
access this is full text of papers it's
now by law that you have to deposit your
papers in PubMed Central and previously
when it was only voluntary there was
twenty percent compliance when George
Bush signed it into law it became
seventy percent and this year the
National Institutes of Health and said
if you don't put your paper in will
delay your next grant and so they now
have over ninety percent compliance but
that's not all that David did he and his
team created not only that the
publication's but you can go from the
publication's to the databases you can
search across different databases so
it's a wonderful example of open science
and what we need to do this is for the
biomedical field we need to do that for
other fields so Jim Gray worked with
David Lipman they worked on making a
portable version of PubMed Central
that's why there's PubMed Central Europe
for example and pubmed central Canada
and so it's very appropriate that David
Lipman is this year's award winner and
I'll be going to Washington DC next week
to ncbi to present it to david but can
we just have a formal round of applause
for this year's Jim Gray Award winner
thank you very much I think it's time
for coffee thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>