<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Beast from Below: How Changes in the Hardware Ecosystem Will Disrupt Computer Science | Coder Coacher - Coaching Coders</title><meta content="The Beast from Below: How Changes in the Hardware Ecosystem Will Disrupt Computer Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Beast from Below: How Changes in the Hardware Ecosystem Will Disrupt Computer Science</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v9xWn4nZUMo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
it's truly a privilege to be sharing my
thoughts day with such an elite and
distinguished audience I've had
conversations with many of you today and
and it's just an amazing group of people
so thank you all for attending the talk
Thomas Kuhn is classic book the
structure of scientific revolutions
coined the term paradigm shift as he
described it scientific communities
function within a paradigm largely
making incremental advances within that
shared understood paradigm until there
is a sudden and unpredictable shift to a
new model or paradigm with in computer
science we don't have paradigm shifts
caused by incomplete theories of the
underlying laws of nature but we do have
paradigm shift caused or driven both by
technological limitations and large
disruptive shifts in technological
capabilities okay for I've got the notes
there we go ok so for generations people
have been building software applications
that have largely been able to ignore
the low-level details of the underlying
hardware okay this speedboat here on the
on the slide represents you know
computer science blasting along more and
more capabilities every year making
great progress enabling applications
software new science new discoveries all
the things that we've been hearing
during this during this faculty summit
today I will argue that we're moving
into an era where the physics and the
underlying hardware will drive a
paradigm shift that will affect many or
most researchers in the field eventually
okay and for my architecture and
hardware colleagues in the audience I
apologize many of you will be familiar
with these concepts I in this talk at a
general computer science audience ok so
the beast from below is transistor
scaling but I think there are
opportunities to defeat the beast from
below if we work together across the
boundaries of the field and that's what
I'll talk about today
okay so I we have the standard computing
stack yield Pat loves to give this in
his and many of his talks and we start
at the bottom with devices and i don't
mean devices like you know smartphones I
mean silicon devices transistors memory
cells and whatnot we're a devices and
services company but in that sense we
Microsoft means the other kind of
devices and then you go from you build
circuits out of devices then you have
micro architecture which is logic
architecture on that and is n means you
know instruction set architecture
systems architecture and networks and
then on top of that you compile down to
them and then of course there's a
boundary that programmers typically used
to to write programs to this underlying
stack okay and the top of that of course
you have how you figure out the
algorithms that you need for the
applications that you want but what's
been really important for productivity
and for us to make the enormous progress
we've seen over the past five decades is
that the the lower part of this stack
has produced consistently large
exponential gains and capabilities I
mean year after year after year after
year things just get faster and more
efficient and faster and more efficient
largely invisible to the layers above
now if you if you need a level of
performance which is well beyond the
capabilities of assist you know today's
systems you you might wait or you might
build yourself a supercomputer both of
those takes some time but the important
point is that the the incredible
complexity underneath that line all of
the compiler transformations and
optimizations and all of the new
features added to the instruction set
architecture and you know now billions
of transistors in the micro architecture
and the physics of those individual
circuits and devices are hidden from the
software developers to first order and
so we've been able to move you know
orders in orders are orders of magnitude
largely under the covers and that's
really what's enabled these enormous
gains and all of these different
capabilities and this vast increase in
pro
activity so that's the world we've been
living in for the past 50 years ok now
the glory of Moore's law is is that it
has been the fundamental driver of these
capabilities and I can't emphasize the
importance of Moore's law enough and I
think we all understand how important
it's been but it's been so consistent
for such a long time that we almost take
it for granted you know and and a little
bit like you know the Japanese yen which
is called the Widowmaker trade and
currency speculation you know predicting
the demise of Moore's law has been a
fool's errand for decades right there
have been papers proving that it can't
go on much further and then of course
the proofs had a flaw and it keeps going
and so you know we almost take it for
granted and built in his keynote this
morning talked about how programming
today isn't really all that different of
course it's different but you know
fundamentally you know it's the same
model as we had you know decades ago so
really this has been largely invisible
ok so everyone knows about Moore's Law
less commonly understood or known about
outside of outside of the the hardware
fields is Dennard scaling so robert
dennard in 19 and his colleagues at IBM
in 1974 published a paper that laid out
a set of scaling rules and basically
what the set is ok you're going to
shrink a transistor from one generation
to the next here's how you shrink a
transistor a transistor is just a switch
it conducts current you know with an
on/off switch called the gate a lot like
a light switch the gate is that the
switch that you toggle and then current
will flow across the transistor or not
and when you make them smaller he set
out a set of scaling rules where you
scale everything proportionally you
scale the length by a factor of K you
scale the width by a factor of K you
drop the voltage by a factor of K you
increase the dopant atoms in the channel
by a factor of K k is typically 1.4 and
if you follow all these rules you get a
couple of really nice features number
one is that you keep the strength of the
electric field across the gate insulator
constant so that's the constant that he
drove to keep things to keep things
going and you'll see down at the bottom
there
a power density stays at one so you
shrink transistors you pack double the
number of transistors into an area and
power density stays the same even though
you're driving those circuits a lot
faster so it's almost like magic you
follow Dennard scaling rules you get
double the transistor count the true
individual transistors get forty percent
faster and they're fifty percent more
efficient each giving you a constant
power density and that's every time you
turn the crank which is pretty awesome
and again you know that paper was 1974
we had about three decades of Dennard
scaling okay so this is really the the
underpinning rules that enables Moore's
law because if you didn't have this
scaling you'd be in trouble okay so
what's the catch well there's a little
problem and the Dennard scaling is dead
okay it's failed and the reason that
it's failed I'll go into in a moment if
we look at the if we look at the axes
you see the transistors on top and red
and this was a really nice graph put
together by my colleagues at Stanford
you know mark Horowitz couldn't lay low
curtain and others you know Moore's law
is live and well it's cranking up you
know doubling of transistors every
generation and then while we were on
this train we saw in blue single thread
performance kept increasing and the
clock rates in green kept increasing and
you see the divergence between clock and
green and single thread performance that
difference is i LP instruction level
parallelism but basically we were
getting exponential gains and single
thread performance and exponential gains
and frequency and people were you know
juicing extra power to drive things
faster than the efficiencies that
Dennard scaling gave you because they
wanted the performance because there was
such a big economic incentive okay but
what happened was that the gate
insulators got so small you know the the
oxide thickness you know small numbers
of atoms that it really became positive
possible to scale anymore and so you
couldn't drop down the threshold voltage
and thereby the supply voltage without
generating a lot of leakage you know
it's like for your light switch analogy
it's like you buy a new light switch and
either the light is always a little bit
on even if you turn it off or you reach
and touch the switch and you get a shock
okay neither of those are desirable so
if you if you tried to follow Dennard
scaling rules past the point where it
failed in the
it's 2004 2005 you're just dissipating
more and more current even when the
transistor is not doing anything it's
called leakage or static power
dissipation and it's no good okay so
that really drove us to a non regular or
non-uniform scaling and that's created
all sorts of problems okay now if you if
you shrink the number if you shrink the
transistors and you grow the and you
pack the transistors much more densely
but you don't get the corresponding
energy efficiency what you see is that
power density starts to climb and you're
consuming more and more power in a
smaller area which is more heat and so
there's really only a few ways to handle
this you can just keep powering the chip
more and more and you know driving more
and more cooling you can just hold off
on power and leave the chip underpowered
and not doing as much okay but you know
you don't really have a lot of good
options and so as a response in the
mid-2000s we saw industry go and shift
to multi-core to try to extract more
efficiencies out of the transistors that
they could power and keep driving
performance forward and that was a major
shift caused in part by this failure of
Dennard scaling ok so is multi-core the
answer you know there was a consensus in
my community around 2008 2009 that
multi-core was going to save us that it
was you know we were just on this path
of scaling cores and they double every
generation with the transistor counts
and pretty soon we'd have chips with a
thousand high performance scores on them
and everything would be fine and dandy
and as long as we learned how to do
efficient parallel programming we'd be
fine of course that's very hard so I was
really interested in this question and
so my group and collaborators published
a paper in Disco 2011 that talked about
both dark silicon and the feasibility of
multi-core scaling in this power
dominated area of disproportionate
transistor scaling and and the question
we asked is what happens to multi-core
performance when you're packing more and
more transistors on the chip and using
them to generate or increase the number
of cores and then you you can't drive
them as fast because of the power limits
so what do you actually
yet okay and you know I should say that
that in this in this era even when your
power limited work because we have these
cores work on better parallel
programming more better parallel
languages more efficient parallel
architectures is still really important
because it gives us extra performance
for free just like work on single
threaded performance is still really
important and work on energy efficiency
for single-threaded performance in
parallel execution is still really
important all of those are in fact even
more important now because we're not
getting the freebies out of the
transistors ok so there's just an
enormous amount of great work going on
in parallel programming a lot of it
being done you know in our in MSRs
collaborations with Berkeley and other
places and that work remains I would say
not even as important as it was but more
important now ok that being said we did
a study that said given some different
technology scaling assumptions how much
were we going to get out of more cores
ok and so the the vertical axis there is
speed up basically how much performance
do we get given a fixed power and area
budget and then the the x-axis is years
the blue line is actually historical
performance brought forward so that's
that's what we got in the last 10 years
so so meeting that line would be doing
as well this decade to come as we did in
that past decade and we made two sets of
technology scaling assumptions what we
call ITRs scaling that's the
semiconductor industry association that
puts out a roadmap about how well we're
going to do and their projections I
think our way optimistic and in fact
we're starting to see that now that
they've been they've been much too
optimistic as we've fallen off of this
curve and then the line below it is what
we call realistic scaling it looks
really like that's what we're tracking
now ok so you know we may be small
changes but we're pretty on the mark on
that one and so over ten years if you
following the realistic scaling curve
for parallel applications assuming that
you have the optimal multi-core topology
for each individual parallel application
you get a speed-up of under for over 10
years ok and you know historically the
speed ups have been higher than even 18
x you know what we started to slow down
in the last 10 years
so this really says that there's really
not that much juice left in multi-core
now that's absent there could be a big a
big breakthrough in transistors
certainly that's possible there could be
a great breakthrough in architecture or
parallel languages that give us you know
ninety-nine point nine percent
parallelism so there are things that
could happen that would shift this curve
up okay but given a set of fairly
optimistic assumptions you know we're
looking at this so now there are a
number of ways to deal with this problem
of having too little power to power your
whole chip okay and there's really three
one is that you can say well i'm just
going to i'm just going to burn the
power and build an expensive cooling and
you know have liquid cooling in my pc
and then my pocket okay there's some
economic challenges with that not to
mention a different kind of leakage we
could decide just to turn off part of
the chip and leave it dark this is why
people talk about this being dark
silicon and part of the chip stays lit
and so you just switch over to whichever
part of the chip you need and you build
a thousand special-purpose accelerators
and you just light up the five that you
need at any one time okay we call that
Frank and chip doesn't sound very
productive for software that's the
second option a third option is just to
make the chips smaller and if you made
your chips smaller on a non linear scale
with each generation you have an
explosion in supply and not much of a
jump in capability and that really
changes the economics of the industry ok
and I think we're going to see the third
option being the most likely all right
so that's also going to put some
pressure on on silicon scaling ok so all
this multi core work is really important
but it's not going to be the driving
force generating the sort of gains that
we've seen in the past at least that's
my belief ok so I thought it would it
would be interesting just to give some
context and take a look at the past to
see how we got here so I mean these
aren't anybody's names other than names
I thought up for this talk and I'm sure
they're not entirely accurate but you
know you can think of the late 30s to
through the 40's with you know ecker and
machli and Turing and von Oy name is
what i call the age of discovery this is
when they discovered not only a lot of
the results about computability but
really lay the foundations of the
execution models that were dealing with
now the stored program concepts von
Neumann computing instruction fetch all
of this sort of thing it was really laid
down by these giants in this period so
this is this was as one of my colleagues
said you know an era where Giants walked
the earth and then you know in the in
the late 50s through the 60s and early
70s what I call the area the era of
invention these are things but a lot of
the the basic concepts that we take for
granted were invented things like caches
you know out of order execution virtual
memory stable instruction sets so you
can reuse software from one machine to
the next you know these these were all
developed in that period and then you
know starting in the late 70s and
through the 80s we had an era of
integration where we started building
single single chip computers which first
were microcontrollers and then we pulled
more and more stuff onto those dies and
then once we had integrated the things
we knew how to integrate caches
floating-point units we went to an era
of ILP we started going to out of order
execution d pipelines very fast clocks
and drove the performance of these what
at the time were called killer micros
way up that ran out of steam because of
Dennard scaling as well as we hit a lot
of limits on efficiency and we shifted
to multi-core and I think that
multi-core shift you know starting in
about 2004 will really have run its
course you know in the next couple of
years 2016 or so and so the question is
really what comes next what are we going
to use to drive this next big wave of
computing and this is the paradigm shift
that I'm asking about you know where is
this next level of performance and
efficiency going to come from okay and
that's what i'll talk about them the
rest of the talk so there are certainly
some candidates one is that we can start
specializing logic neural computing is a
great canada i'll talk about both of
those my wonderful colleague krista is
going to be talking you know how's a
talking about quantum and that runs very
cold interesting interestingly denard in
a recent paper said that you can
actually keep scaling the voltage if you
keep scaling the temperature down with
it okay so you know we go to- 50-60
centigrade da da da that provides a
scaling path it provides some other
takes some other challenges and
engineering feats too and is not
something I want to have
my pocket okay but but this this era of
X what X is is a really important
question and it may not be just one
thing but it's coming soon and I think
that's the message that I'd like to
convey to day that we really have to
work on figuring out what X is and X is
going to require breaking a lot of the
traditional barriers down okay so let me
start the first candidate which is
specialization so it's well known this
is a sort of a classic graph that's why
we use that Bob broderson at Berkeley
put together a while back he took a
number of chips from the ISS CC
proceedings between 1997 in 2001 and
categorized them by function and
efficiency so on the y-axis you
basically have energy efficiency and on
the x-axis you have just they've sorted
the processors based on how efficient
they are and the category in the left is
general purpose microprocessors these
are the things that are nice and easy to
program and they run everything they
just run everything very inefficiently
but as long as the party's still going
we don't care okay digital signal
processors in the middle but get between
a factor of 10 50 100 x efficiency and
then dedicated hardware or a 6
application-specific integrated circuits
that do one function run between a
factor of 100 and a thousand more
efficient than a general purpose
processor okay so by specializing the
logic just in circuits to the to the
function that you want to execute you
can already read large gains and
efficiency over where we are today and
this is no surprise this is well known
in the computer science community okay
there are a couple of intermediate
points though you know a six are like I
said are up there at the top there are
fpgas which are really interesting
devices that are basically programmable
logic so they have they express their
logic not in you know and and or gates
map down the transistors but look up
tables that express logic functions and
you can wire together a whole bunch of
look-up tables to generate larger and
more complex functions and these give
you you know a jump of say 100 x over a
general-purpose poor but they're
reprogrammable but you pay a factor of
10 / an ASIC okay so there's some really
interesting possibilities there now I
think as I alluded in the last slide
there are more gains to lower you
and and the last graph was really useful
for understanding the disparity of
efficiency on existing chips but let's
understand the space of possibilities as
we start to think about specialization
and new execution models okay so I think
of code specialization is giving you
potentially an order of magnitude and
obviously these are very rough numbers
code specialization is things like
taking a multi-core chip and rewriting
your single threaded application to run
over 16 processors if you do it well you
might get a speed up with 10x okay and
this is on a single machine obviously if
you have many machines the gains are
potentially higher okay logic
specialization as we saw in the last
slide can give you another order of
magnitude okay so by customizing your
circuits you can maybe go down to a
factor of 100 or more over over general
processors that's a lot of work okay and
we you know we don't have any weight
currently to do that product
productively if you go down the next
level to circuit specialization you can
start doing things like analog circuits
that that are not you know boolean gates
but are representing actual functions
these of course you're starting to leave
did the digital domain and of course you
there are some things you can do and
stay in the digital domain but there's
another order of magnitude there and and
when you start going down into devices
using actual small devices that compute
different functions there's even another
order of magnitude there okay so if
you're looking at you know ion mobility
and an ion transport memory which is 10
nanometers across they have some very
interesting properties as a function of
the current flowing through and the
resistance in the state of the cell okay
so we can build devices that give us a
lot more but right now we have no way to
specify them or use them at higher
levels of the system okay so the first
two layers I would argue are sort of
what we understand their boolean their
digital they're easy they're hard to do
well maybe but but down at the lower
levels that's where the big gains are
okay and the question is really how are
we going to marry those lower levels up
to the software stacks that have been so
productive and so stable for so long
okay so starting with logic synthesis
going down to that second order of
magnitude by specializing the logic
there are large gains in efficiency if
you can start to specify your software
functions down on logic as opposed to a
general purpose processor now that
becomes a spatial thing I mean you're
you're not running a function through a
stream of instructions through a
processor you're actually laying down
the code completely concurrent you know
dataflow like on a collection of
communicating state machines where
everything is running it once it's
fundamentally a very fine-grained
concurrent language or model and you
know again 100x for fpgas or something
called core screen reconfigurable arrays
which are more like spatial arrays of
functional units connected with various
types of networks than a thousand x4
asics but the development and
compilation for specialized logic is an
enormous challenge maybe people have
been at this a long time we've made slow
progress but what's really interesting
and starting to change is that these
fabrics of programmable logic are no
longer just the raw logic look-up tables
that you have to put a circuit down onto
the the major vendors you know alterra
and xilinx there are host of other
companies working on this are now
starting to put down things like
functional units clusters of functional
units in a sea of reconfigurable logic
around them and they're starting to
stand about processor cores with a sea
of reconfigurable logic and function
units around them and memory blocks with
a sea of processor cores and functional
units so we're getting this very
interesting stew of cores and
programmable logic and functional units
and memory blocks and it's a substrate
that's much richer than the old kind of
standard just here's a bunch of
programmable gates and and there's a lot
of work going on in tools to map down to
that all terra has a stack called opencl
which is really a standard to map kind
of data parallel types of languages down
to graphics and other accelerators now
they're targeting this interesting stew
xilinx has a product called Auto ESL
which is much more sort of logic
programming coming up and trying to make
you more productive and whether they
meet in the middle or something else
comes and and provides productivity is a
really interesting question so we see
really interesting mixes of these
functional units and these tools in this
stew
so I think we're going to see some
breakthroughs here I think we're going
to start to see wide adoption and maybe
in niche Ares at first but this will be
a candidate for getting us past some of
these challenges that we're seeing on
the scaling domain and then I think you
know given how fast workloads changing
the cloud we're like much more likely to
see programmable logic there first and
then things like CG Ras or many more
types of asics in the client which need
that extra energy efficiency and have
sort of more predictable workloads you
know since you're replacing the devices
and not rolling out new services is
option as often okay so that specialized
logic but what what is the challenge
here well it's got great potential in
the short term we have a lot of
challenges at the language and algorithm
an application barrier I mean exposing
this to software programs has
historically been very hard but it's a
problem we have to tackle and we need
help we hardware people need help from
the higher levels of the system and
figuring out what sort of concurrent
languages can actually make this
productive what libraries and data
transport frameworks and fabrics can we
use to sort of select a flow from a pre
orchestrated library then plug a bunch
of kernels in but it leaves these other
two levels unaddressed you know I mean
there are field programmable analog
arrays but they're really niche right
now and this says nothing about devices
and this is where the big speed ups are
okay so what can we do to attack these
levels there's another issue issue here
which is that in the logic
specialization space you know you have
efficiency you have a spectrum and you
have efficiency on the far right okay a
six at the extreme you have generality
where we think of a cpus on the far left
and you can think of that as
inefficiency in the extreme and they're
going from left to right of course we
start with CPUs then you have multi-core
CPUs and then many core cpus and then
GPUs and then FPGAs and then a six right
so the farther the farther right you go
the less general you get and the more
work it is to map and but the more
efficient you get and this tension is
not something that we know how to
resolve today but we need to make
progress I'm hoping that these software
tools for mapping down to these
interesting substrates will help us to
make a lot of progress but how do we
resolve this tension in a more
fundamental way or what what is that
what is the hook that's going to
give us an added advantage that we
didn't have before because people have
been at this a long time okay so a final
comment you know is that if you look at
Moore's law and I thanks to check our
Borkar dead until for this chart he gave
me a while back you know we're currently
at 22 nanometers moving to 16 or 14 if
your Intel for reference you know
silicon atom you know in a matrix is
about two and a half angstroms so we're
looking at you know figure 4 per
nanometer ok so by the time we get down
to 40 nanometers we're looking at
transistors that are 16 atoms across ok
that's the long way the the short way is
the gate on top ok so there's an
economic imperative to extract
performance from these generations of
transistors we're not getting the free
scaling and energy efficiency anymore
and the power density is climbing in
addition we're starting to pay more and
more of a tax for the digital
abstraction you know because these
transistors are getting leaked here
they're getting noisier there's more
variants they fail more often ok at each
step you take down you're getting closer
to the atomic domain and we're pretty
close already and things just get very
very tough down there you know quantum
tunneling high end high velocity
electrons just you know process
variation your manufacturing you know
hundreds of billions of transistors okay
with small numbers of atoms ok so we pay
a fairly high tax to get a clean digital
representation with enough band gaps to
keep things functioning as they should
be and the challenge is that at some
point along this road as you get down to
single atoms that tax becomes too high
ok you know you're not going to build a
stable digital transistor out of a
single atom ok and so at some point here
that tax becomes too high and if you
don't figure something else out to do
with those transistors that stops the
scaling so I think we hit an economic
and long before we hit a physics pace
and we may be able to manufacture them
but we have to be able to extract value
from them to justify the tens of
billions of dollars that you need to
move the next process node because every
node is getting harder now during this
time so this is sort of the danger zone
as I see it I'm guessing you know 11 but
prob
we ate is where we're going to run into
serious economic challenges without some
new model okay during this time of
course dram is likely to die but that
doesn't mean progress is over we're
going to be making a lot of progress on
denser non-volatile technologies 3d
stacking integration of multiple
technology into a package so there's
going to be a lot of stuff happening
through this and even after even after
Moore's laws quote-unquote over so it's
not an end of progress but the question
is how do we harness these nasty noisy
devices in a way to provide better
performance and the economics to keep
scaling going as long as we can physic
as long as we physically can okay so I
think there's an opportunity to break
this digital tyranny okay of this of
this abstraction of ones and zeros that
served us so well for 50 for 60 years
okay and that's to go to embrace
approximation some people call it analog
some people call approximation okay this
is you know there's a lot of great
debate going on in the community right
now you know we've tried this it's too
hard it's black magic okay but the world
is different now not only do we have an
enormous economic incentive our
workloads have changed we're doing
enormous numbers of things that that
intersect with the analog world our
world in fundamental ways and so if you
look at that list of applications our
classes of applications you know
large-scale machine learning vision
bioinformatics mining Big Data speech AI
nua these are all things that you know
taken in some sense an analog input or
produce an analog output or have many
possible answers these are things where
you can afford some imprecision these
are things we can afford some air in
fact you can afford a lot if you have
the right algorithms if you have the
right convergence okay as an example my
group is working with a very interesting
workload internally that we're trying to
accelerate we had 400,000 thresholds in
this workload represented as 32-bit
floating point numbers and you know we
wanted to make it more efficient so we
just did a quick thing we just took all
but sixteen hundred of them so 3398
thousand four hundred of those
thresholds we just dropped them to 20
bits okay the other 1600 we said all
those look big will leave the
alone we won't even try to mess with
them and we saw no reduction in the
quality of our output okay we don't
actually know how low we can go and I'm
guessing for each of those you know for
each of those thresholds that's carving
out a different part of the space it'll
be a little bit different we have no
formal way to deal with that right we
don't have the we don't have the models
we don't have the mathematics we don't
have the language support you no we're
doing this all by hand and by trial and
error it's really time consuming and
messy but there's a huge opportunity
there yeah we just drop the precision by
a third for almost all of them and it
was free okay this is no surprise to
people that work in machine learning
right you know sometimes you need single
precision floating point sometimes you
need uh but very often you need far far
less than that ok so these workloads can
be very robust to produce rutas
precision and they can allow us to start
computing with circuits and potentially
eventually devices that are far more
efficient than what we have today but we
need the right abstractions to reason
about how much error can you tolerate
how much noise is acceptable what
function are you trying to drive through
how do you map that under the functions
how do we characterize their
distribution what if we have dynamic
noise how do we manage all this and
right now we have no idea okay but the
potential is there there's huge
opportunities the workloads are amenable
so but we need help from across the
stack okay and I'd like to thank you
know point out that this is both this
approximate computing work as well as
the work I'm about to talk talk about
was in close partnership with Lewis says
a University of Washington who has a
great program in approximate computing
is really driving this work hottie as
malasada as our student who did a lot of
the work adrian sampson and others so
thanks to them for giving me some of the
material on the slide so i think about
approximate computing and energy
efficiency you know here i show a Pareto
frontier on the y-axis i have energy
going up as we go up and then
performance going to the right on the
x-axis and you can take a bunch of
processor designs and compute that
frontier and you can always buy a little
bit performance for a disproportionate
more energy right it's a nonlinear curve
so if i want that extra little bunch of
performance it's going to cost me some
more energy and so we of course try to
move down into the right with bed
designs more efficient architectures
that's the sort of bread and the butter
of my community but but with this
approximation we have a great
opportunity we can add a third axis of
error or in precision and now instead of
a Pareto frontier we get a Pareto
surface okay and the shape of this
surface for each of these workloads is
at this point completely unknown all
right we have no idea how far we can
push this okay but once you're willing
to take a little bit of error and you
and you break again this digital tyranny
you can start using these devices that
are noisy and you don't have to pay that
enormous tax to guarantee a one or a
zero okay now of course there are points
on the surface that don't make sense
right i mean i can get you know zero
energy and infinite performance if I'm
willing to take enough error right I
just pick a result and I'm done with the
computation so clearly that doesn't make
sense but there are there are a lot of
really interesting points on this so
I'll talk about one kind of very
incremental step we've taken in this
direction next and that's taking von
Neumann computing and transforming it to
neural networks okay which this was a
really surprising result for us we
didn't think it would be this work this
well so on the on the left and this is
again his work with Lewis says a hottie
and Adrian so on the left here there's
sort of a standard control flow graph of
a traditional imperative program C C++
whatever you want we find some target
region which is well structured so well
defined inputs and outputs no side
effects a lot like functional
programming ok and then in our compiler
we actually have a unit that observes it
ok so we're trained a neural network it
observes the inputs it observes the
outputs we run this over lots of input
sets we interpolate inputs and then run
them through if we don't have enough
input sets to work with and and then of
course we test it against a real set
after we train it and when it turns out
is that you know if you pick the
topology of a standard you know
artificial neural network or a NN based
on the workload you can actually do
really well you want a different
topology for each one of course
obviously if you give them all a giant
topology you know they'll be good in the
limit but then you you pay a lot of
inefficiency for small functions you can
you can
a small topology for really large
functions you need a much bigger
topology with more hidden layers and
more inputs and more you know and more
neurons in the hidden layers so we
observe it at runtime and then what you
can do is that when you've transformed
that representation into a neural
network then you can run it on an
accelerator and in the paper that came
out in micro last year in 2012 you know
we propose a digital accelerator that
was tightly coupled with the processor
pipeline called a neural processing unit
or NP you you know marketing speak
because it's supposed to be analogous to
a cpu and the idea was that you take
that neural network and offloaded to the
neural accelerator which was digital and
had all these properties okay but the
really interesting thing here is not
that we can do this but that across all
these different types of programs we can
transform very different kinds of loops
you know with array accesses you know
dependence chains of instructions into a
common representation that common
representation can tolerate some error
and that common representation maps
these very different types of von Oy man
code down onto a single substrate which
is neural and so that was the real the
real surprising observation that across
all of these application classes I won't
read them but you know they're there are
are six and they're all you know very
multimedia or robotics I mean all this
new analog domain type stuff they all
were able to compile into a small space
of neural networks and so that common
representation actually gave us very
good results so for these applications
and not just the inner loops but the
whole application that we're doing these
we got about a 7 X improvement in energy
delay product you know two and a half x
speed up three x reduction in energy
just by transforming the mean colonel to
a neural network and then offloading to
a digital representation of a neural
network and i show you their the before
and after of a baboon and you know i'll
take it out before you can see the
differences ok but they're small ok so
now if I look at the stack that we go
back to you know the problem is that the
npu gains are still limited I mean
there's still less than a factor of 10
because they're hidden behind all our
standard abstractions we're paying for
amda law ok now we have some work going
on now that's pushing those down into
analog circuits where
already incurring some error that error
is typically less than ten percent in
all these applications the quality of
the output was not was not deemed to be
so bad that it wouldn't be worth doing
okay you know it's subjective of course
how much error and that's another
problem for people to work on and then
eventually we can push down into devices
and there's actually a lot of people
looking at device to support these
classes of computation but not NP using
specifically so that's an example where
just by relaxing this digital
requirement and bringing in a little
error we were able to do a fairly
significant program transformation that
was able to map down on to either
software with code transformation you
know logic specialization with a digital
NP you you know circuit specialization
with the analog and view and then
eventually devices like memristors or
other things that that do well with
neurons and make computer sigmoid okay
but but to really take a jump past this
I think that approximate computing is
the key for a lot of these workloads and
the numbers and the importance of
growing every day is the key that will
lead us to some of these new execution
models I think as long as we require
that we stay completely digital you know
it's very hard to beat von Neumann
computing for efficiency or generality
and we can work on logic specialization
and I think that's what we should do in
the short term that's going to take us a
fair ways down for efficiency but it's
hard you know and and right now it's
very very labor-intensive work for
really talented people to make a lot of
progress on so I mean I guess my plea is
for the and I know there are lots of
people doing this you know there's lots
of people working on quantum here at
Microsoft and elsewhere you know I think
the community is starting to do more and
more with neural as a general execution
model which is really interesting
hardware sip this is a lot of people too
there are many other there are many
other possibilities and you know von
Neumann was one of the co-inventors of
cellular automata bayesian maybe for
biological computation for
representations but I think we really
need to think about new execution models
that can map down on devices then
provide the abstractions all the way at
these new stacks that will let us get
these factors of 10,000 or more that are
achievable with these atomic scale
devices
before we you know before we run out of
steam and just give up okay and I think
like I said I see a lot of really
interesting stuff thing happening in
neural networks not just the work that
we're doing to be compatible with
conventional von Neumann computing as an
awful at engine but people are now
taking you know scientific applications
and recoding them as neural networks
Olivier to moms excellent work you know
from inria there are many types of
interesting networks immune obvious
there's the old school and end stuff but
then there's all the the heavy emphasis
now around you know deep deep belief
networks which is some degree or
bio-inspired I guess because they're
deep and then there's some there's a
whole class of things that are bio
abstracted neural networks and by that I
mean actually trying to pick out what
the fundamental primitives are from our
brain that makes it work and discarding
all the rest getting the cleanest
possible abstraction in connectivity
neuron function role of
neurotransmitters and all of that and
topology so we can actually get the
efficiencies that we get with this 20
watt part you know with synthetic
systems and and and there are some
really important discoveries to be made
there because no one really understands
what that minimum set of primitives is
and Jim Smith is a giant in my field
who's done just you know seminal paper
after seminal paper for decades and he
made a comment in Tel Aviv a couple
weeks ago when we were having a summit
and he said that you know like the 40's
there are giants he believes that there
are giants walking the earth today who
would who are building the underpinnings
of these neural models that we will
eventually use for widespread
computation we just don't know who they
are yet and so you know it's maybe some
of them are in this room I know it's not
me but but there are giants walking the
earth so I think that's really exciting
okay so I'd like to wrap up into the
civil with with five testable
predictions for 20-25 given all this and
so looking ahead giving the trends that
we've seen and all the various work
going on in the hardware community and
and in our related communities what do I
think things are going to look like in
12 years okay so first of all Moore's
law will be dead and by that I just mean
this regular cadence of you know CMOS
transistors scaling every two years we
double the density ok we're still
we'll be making lots of progress will be
lots of exciting work to do but that end
will transform our industry and our
digital systems it's going to really
affect things so none of us have ever
worked in a world without it okay when
you turn off a 50-year exponential some
some surprising things are going to
happen okay I believe that hardware
software compilation will be common this
is along the lines of logics
specialization in synthesis so if you
have a really demanding application
you're going to run it through a
compiler and it's going to spit out some
some hardware description language that
will be run through a synthesis tool
it's going to spit out some software and
it may spit out one big glob of stuff
that goes into this this mush that I
talked about of cores and functional
units and IP blocks and and
reconfigurable logic okay but that's
that's going to be a radical departure
because that's kind of the easy thing
right now for it's hard but I think it's
one of the clearest in terms of its
value proposition I think we're going to
start seeing a complete stack around
neural evolution and execution so that
stack that you know the abstraction
stack I think we're going to have one
for neural execution if you're going to
be well-defined interface is at every
level from devices all the way up to
algorithms and programming languages I
think physics-based computation this is
harder it's longer term but it's going
to be a really hot topic so extracting
you know stochastic behavior of atoms in
different devices and trying to extract
those functions and make those functions
sort of higher semantic function is that
the old risks Wars will have sisk for
Adams okay and people will be investing
a lot because the potential gains are so
high so it'll be an explosion people
trying to create device to have
interesting new functions as the
building blocks for some new physics
based model and I think that's going to
be longer term and then finally I think
given where I see the neural stuff going
and I don't think this is surprising to
anyone who's certainly working at MSR or
attending the summit the machines are
going to be beating humans that many
many more tasks than they are today
whether it's speech recognition vision
etc etc and this this will be
transformational not just on our field
but on society and I think we need to
think very hard in reference to the
question to Bill Gates this morning
about the downside of Technology I'm an
optimist too and it's just amazing
things we're going to be able to do
somebody is going to have to take
responsibility though and it can't be
politicians
for thinking about how we guard or gate
the impact this has on society because
technology is now so pervasive and so
powerful and it's moving so fast that it
will have proof i mean it's changing all
of us you can go to the bus up and you
see people everyone's staring into a
little box right so it's it's changing
us as we speak and then i guess one last
comment since I I have the podium I I
worry a little bit that the publication
requirements in computer science now are
so onerous that it's very hard
especially for junior people to take big
bets okay if you want to jump out and
work on one of these new models which is
going to take a decade or more a very
hard work and you're expected to turn
out four or five or six top-flight
publications every year that's one every
two months you know it's really hard to
kind of detach and do a lot of the
fundamental work if you're expected to
say on that flywheel and people do it
it's possible but I worry that our
publication requirements are so high
that it may be inhibiting some of the
really important long-term work that we
want to see and so something that I've
tried very hard to do in my group is
give people the freedom to publish when
they have something amazing to say and
not worry so much about showing a
certain rate of publication and I'd like
to see a lot more of that because I
think we have some really hard
challenges ahead I know in my community
you know the at least the rate of
requirement is just it's just crazy and
it's been growing every year okay so I
really hope that together we have this
great opportunity with devices with
circuits custom serve gets you know
approximate computing analog interesting
new software stacks you know working
together I think we can define the new
abstractions that will create the new
stacks of the future and give us three
four five six orders of magnitude more
capability and jump ahead to some of
those you know grand challenge problems
and great visions that the future is
stream of I'm incredibly excited and
optimistic you know I think I think that
we're going to get there there's a lot
of hard work to do but you know there
are at least some paths forward you know
even though the traditional ways that
you know the the free lunch is slowing
down a little bit you know so my hope is
together with the you know all of you
the software community
the language community the algorithms
community you know we can all work
together and we could turn that beast
from below into something tasty and keep
the party going thank you very much and
I see jignesh with the first question
this is a longtime friend I i will start
with him coming down number two that was
a fantastic talk as usual great to hear
from from you I have two questions first
one is you talked a lot about how to
make computing more efficient with the
ideas you mention what about getting
data in and out of those computing
devices what do you see happening with
memory buses to connect all of these to
make it scale the way you are saying
that's the first question the second one
is Wall Street has been doing
algorithmic trading and rumored to have
been using a six and FPGAs yes for four
or five years so should we be following
the money and do what Wall Street does
I'm sure I'm going to go to hell for
suggesting that we follow the bankers
okay so let me unwind those so for
getting the data in and out it's clearly
very important I did say I think we're
going to see an explosion in
non-volatile memory devices and if you
look at some of what's happening in the
nan space with flash they're starting to
build you know t'kul chains of cells as
opposed to you know horizontal stripes
so we may be seeing 4 8 16 layers you
know so there will be a lot of storage
that you can attach to chips with
through-silicon vias and get very very
high bandwidth you know the the
intersection of 3d stacking silicon
photonics with chips I think are going
to give us tremendous jumps and
bandwidth in capacity locally and then
you know a lot of the new micro second
scale networks coming out the data
centers will give individual servers
access to a much broader pool story you
know a flattening of the data center if
you will
yes yes so there are a lot of
technological directions we could take
and there's a lot of work to be done I
mean I think all of those are promising
I don't know if they'll be enough okay
as far as following the bankers I mean I
think I think they certainly have been
building custom systems because the
economic incentive is so high maybe by
having us all build some of these tools
and make it easier to do that we can
level the playing field and give high
frequency trading to the masses give the
power back to the people you just need
an office within within one city block
of the training floor and you're good to
go let's take number two again and then
we'll take a question from the live
stream number two so I'm a label from
Columbia tremendous talk thank you I'll
it's a pleasure to meet you by the way
we had hybrid digital analog computation
back in the 70s yes we did and one of
the things I hope that you'll do is
unlike a lot of hardware designers who
have inflicted upon us software people a
hardware architecture that's very
difficult to write algorithms for and
compile into the multi-core arena um
what will you offer us to make writing
programs and compilers much easier for
your vision of the future that's a great
question so the glib answer is I think
we need to work together rather than
having us inflict something up because
we don't really understand what those
primitives need to be you know in the
early days of in the 40s and you know
the age of discovery you know we were
just working on stored program concepts
and generating instructions but there
was no reusable instruction set it
wasn't till IBM had a research project
in the late 50s to be able to reuse
instructions from generation to
generation and then of course there were
all the debates about compilers versus
assembly language program as the 70s
that you know much better than I do I
think the most important thing is to
understand initially a set of primitives
that we can expose up and then think
once we understand the primitives look
at the compilers in the language is to
think how can we start stitching those
together
there's some actually great work if I do
say so myself going on at Columbia with
one of my former students sympathy
Madhavan who is actually starting to
build those primitives you know
integrators to do integration
differentiation multiplication
trigonometric functions out of analog
circuits and and you know he's starting
to think about how to push those
abstractions up so I don't have an easy
answer for you other than other than it
needs to be we need to start with the
primitives think about how we compose
them and then think about what those
interfaces ought to be a higher level
it's a hard problem yes thank you so do
i so let's take a question for the
streaming audience we've been live
streaming at this talk our proxy for the
for the Oscar how do you think the
changes in the hardware ecosystem will
affect job opportunities in computer
science in the next ten years I think
they will increase the job opportunities
because they're going to be a bunch of
really hard problems to solve a new
technology spinning out and if you look
at the industry ecosystem right now you
know we have so much data and we have so
many machines and so many advances you
know that people are building
applications that don't that take a
combination of maybe new networking or
new data or new uses and they don't need
that really cutting edge performance you
know so I don't think this is going to
reduce in any way the demand for example
web programming or software jobs I mean
the digital ecosystem is marching
through the economy eating all these
analog industries maybe it's a bad
metaphor to use given the talk i just
gave all right but so then we have to
have an analog wave behind it but you
know so that's what's driving a lot of
the job growth in our feeling i don't
see any signs of that slowing down I
just think that now we need to make the
investments so that we have those new
capabilities 15 20 years from now
because any of the stuff you know even
in architecture in architecture such a
weird field because it's a deep funnel
you come up with an idea you know in a
general purpose core it might be a small
idea you put it into the funnel 10 years
later if you lucky it comes out the
other side but then it gets applied
across billions of people i mean what's
the value of one percent across all of
computing it's huge so but there's a
long funnel and so I think the stuff
we're talking about now we won't see in
production for
15 or 20 years we just need to make
those investments now thank you other
question one time for one more question
then we'll go to the buses number two
please yes so is this one so is there
any reason in particular that the
approximate computing idea that you were
mentioning couldn't entirely be done in
software where the hardware and
essentially the bitwise computing is
still entirely precise and you know in
recent years of course many examples is
this nobody for example would do a
lossless video and audio compression
people do always do lost lossy
compression and so in some sense it may
be the approximation could stay within
the software which is entirely what
happens in example machine learning and
optimization and many many other
fronting approximate algorithm fields so
that's a so that's a great question
thank you the question was why can't we
just limit approximation to that highest
level rather than paying all of this
pain going all the way down and I think
the answer is that the if you go back to
the stack of code specialization and
logic specialization in circuit device
specialization with an order of
magnitude jumping at each you know I
think what you're talking about is code
specialization where you can you can
come up with algorithms that exploit
that approximate e2 to reduce the
computational overhead of the number of
discrete computations you do and of
course you know when you look at if you
look historically for example scientific
computing in over the years they've seen
five to six orders of magnitude growth
in hardware capabilities and they've
gotten five to six orders of magnitude
from algorithms and the algorithms are
still incredibly important and that they
actually talk about 12 orders of
magnitude you know half from the systems
and have from the algorithms so anything
you can do at the hardware level at the
at the software level to make yourself
more efficient is fantastic but then
when I talk about going from 32 for
example to 20 bits you know we're
reducing the bit with you know you don't
have the primitives on a digital
processor to do that you need logic
specialization to exploit those those
narrow bit widths for example where FPGA
is really really shine is when every
computation has a different bit with and
you strip out all the stuff you need and
just squeeze stuff down it's sort of
like shrink-wrapped execution at every
level and then if you go down to the
circuits and the devices you know
they're just large large gains there so
4.jpg you know encoding for example
you know some of the algorithms that
you're using to compress you could push
down into a 10 nanometer device and do
it rather than with a software loop
running you know using more efficiency
you'd get a big speed up I don't know if
that's possible that's so I think that
that doing what you're describing is a
great thing to do and we should do every
bit of that that we can but the the big
gains I think are down in those lower
levels where we create a new model and
expose it up to a whole new class of
languages and application developers so
we should do both is the answer thank
you don't you join us on the boat
absolutely so any further questions find
Doug on the boats let's go sailing thank
you very much thank you so much thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>