<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>To Do or Not To Do: Scheduling to Minimize Energy | Coder Coacher - Coaching Coders</title><meta content="To Do or Not To Do: Scheduling to Minimize Energy - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>To Do or Not To Do: Scheduling to Minimize Energy</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/a40C930X7oU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hello everyone yeah welcome it's a
pleasure to introduce semicolon from
University of Maryland's O'Meara's done
very nice work on basic algorithms and
approximation a little but today he's
going to tell us how they're related to
scheduling to an amazing idea thanks for
it I have a lot of slides today so I
know it's Friday afterwards late so
please feel free to interject ask
questions and also it's fine if I don't
go get through everything and I'll try
to keep it light and proofs so this is
work that I've been involved in for
several years and joint work with
several people who will show you
pictures and give you names at the end
so I'm going to try to cover a bunch of
results and this work was started about
five years ago and the motivation for
our work was trying to understand some
scheduling problems dealing with data
center so as we know data centers are
everywhere these days consumer large
amount of energy so a couple of points I
wanted to notice was that the workload
actually fluctuates over time and so you
might actually begin to ask fundamental
questions like maybe we can save energy
by shutting down a large fraction of the
other machines when they are not in use
and these their servers themselves are
getting more efficient about energy
usage so your cell phone processor for
example can you know go into sleep mode
power save mode running on battery so
energy savings is very very important
more importantly I want to talk about
you know the energy consumption is
obviously very huge and there are
thousands of processes any data center
and each one is consuming significant
amount of energy and we are looking at
sort of scheduling policies up at the
CPU level now more broadly you could say
well I have a large set of disks and
maybe I could shut disks round as well
to save power and then this costly
expenses in bringing them back up but in
CPUs it turns out that that overhead of
turning things back on is much lower you
can put them into different kinds of
sleep states but you can go into sleep
states where you can rapidly recover
from but you're still consuming a lot of
energy if you if you slow down the
processor okay so obviously I'm an
algorithms researcher and you're going
to look at problems which have to do
with scheduling where you have a large
collection of jobs that you want to do
and we sort of want to now examine that
question with this property we have now
have an ability to turn machines on and
off so if you think about sort of
scaling the literature goes back 50 plus
years most standard scheduling problems
are given jobs and you're given some
machines and resources and you have to
figure out ways to run these jobs and
those machines and most of the work
historically is done from the jobs
perspective so we ask the question the
machines are always on we don't really
care about how much energy they are
using but I want to run these jobs for
that this high job satisfaction or the
clients that are issuing these jobs they
are happy with the services so happiness
might be I want fast response time I
want these jobs to complete quickly
nobody really what is about the machines
perspective right but there's a very
interesting trade-off that we trying to
understand that maybe at a slight cost
to happiness of jobs maybe we can cut
the energy use it significantly so
there's clearly a trade-off if you have
unbounded energy then you said you can
maximize job happiness but in general we
want to keep the energy costs into
account so our goal is to sort of look
at both online and offline problems in
the space today's talk is mostly going
to focus on offline algorithms so these
are algorithms where you know the entire
input in advance you trying to compute
some solution advanced clearly in terms
of real-world scheduling that's not the
whole picture you have jobs that you
know about and you find a schedule and
then there'll be jobs that you don't
know about that will arrive at the last
minute and we'll need to be fitted into
a schedule so I have some more recent
work that I won't get time to talk about
our online algorithms as well and then
there are some cases special cases for
which we can derive optimal polynomial
time solutions and then there are other
cases with the problems themselves are
np-hard and then we our goal is to
develop approximation algorithms a lot
of experts here in the in the room the
first stroke who work on this topic
where our goal is to develop algorithms
that run in polynomial time but might
not find optimal solutions will be
trying to prove that these solutions are
close to optimal okay so one other piece
of work from 2010 that motivated us was
an internal internal technical report
which talked about power-aware job
scheduling and the sort of highlighted
the fact that job scheduling hasn't been
studied with this power or energy
constraint in mind and there's a large
bunch of research on this topic called
bad scheduling so what is bad scheduling
bats care
the idea is the following that groups of
jobs in one batch can be scheduled at
the same cost as one job right so you
might have a upper bound on the bad
sides it's sort of like saying what I
want to drive to to downtown Seattle my
car can take you know seven passengers
and whether I take one passenger or
seven passengers the cost the same is
one trip not on Seattle right so that's
what a patch consists off and in a lot
of the literature and bad scheduling
people had mostly focused on finding
feasible schedules by completely
ignoring the energy costs and so one of
the things that we looked at was how do
we optimize for energy and I'll get into
into this in more detail in a few
minutes so let me start off by talking
about one sort of classical problem and
one twist to it that we looked at a few
years ago which was the starting point
of some of this research so there's a
very famous problem studied for about 25
years called unrelated parallel machine
scheduling by I guess starting from the
work violence Rushmore isn't our dish so
here the framework is the following
you're given a collection of jobs j12 JN
which are shown by the red nodes of the
left hand side and then you are given a
collection of machines m12 mm and these
jobs these machines are not identical so
what it means is that some job might run
very quickly on some machine and might
be very slow on another machine or might
even be infeasible to be done running on
another machine but still the goal is to
somehow assign jobs so that we minimize
the maximum load and any machine so you
want to sort of do a load balanced
solution and they give some very nice
algorithms even though the problem is
np-hard okay so the one sort of
generalization of this problem that we
looked at was the following not let's
assume that these machines are sort of
available but you have to purchase them
so each machine has a buying cost so
machine mi has a purchase cost of CI so
it's like saying I go to the marketplace
I have a lots of jobs i want to run lots
of machines are available but i have
some fixed amount of money that I want
to spend and i now want to decide which
machines i want to buy so i have some
budget that i'm given see and i want to
choose a subset of machines that i buy
that fit within our budget and now once
i have chosen the set of machines that I
by now I have a job scheduling problem
because I know what the set of machines
is and I want to schedule all the jobs
of those machines so the goal so
determine this subset of machines with
costs at Massey and then
sign the jobs of machines so that we
minimize the maximum though okay so
that's what we call the machine
activation problem once follow she has
not horrible jaws right the cost is just
to buy the machines in advance right so
uh so I you can see this analogy little
bit later but later on if I have one
machine I could think of this vertical
axis actually as time so i might say
it's the same machine but whenever I
turn it on I'm paying some cause so
there is a relationship between time and
that but even though in this model is
sort of just saying I have a bunch of
jobs a workload that I want to run I can
decide what machines to purchase i have
a budget which was she should i buy
right and of course this problem in this
way actually turns out even it
generalizes the famous set cover problem
right because i can think of these jobs
as elements and these machines their
sets and then I'm choosing certain sets
to buy and then maybe the if I element
belongs to a certain set that job can be
scheduled that machine but its load is
minimal if the job can the element is
not in a certain set in its workload is
very high right so this framework that I
just described actually captures both
the unrelated parallel machine model
when all the CIA are zero and it also
captures the famous set cover problem
for which no approximation algorithm
breath and logon is known right so the
result that we proved is a joint work
with g only inverness ahange the paper
disorder 2010 is sort of we cannot get
the best of both worlds so we showed
that if you fix the budget of c and
there's suppose an optimum solution that
that meets the budget of c and has max
loti then we can find a solution with
cost C times log n with a max load of 2t
so you and and both of these sort of log
in and two are sort of unavoidable
because of the special cases right and
so I won't really go into the algorithm
I just want to say that this was sort of
the motivation for studying this kind of
cost problem and then you can think of
if you think of this sort of as as jobs
and you think of this as time edges
might go to certain machine being on at
a certain time if the job can be
scheduled in that slot right and the
edge is missing if it just can't be
scheduled so now let me move to even a
much more simpler model where
but bad scheduling so this is like a
like a shipping problem so we have a
container that has to leave the port
this is the available time of the
release time of the job and then there's
a dead language the container has to
arrive at a certain location and we have
sort of multiple multiple jobs right so
i have another container that will be
ready at a later time and then is
expected at some some deadlines so we
think of this as simply as jobs having
release times and deadlines and the ship
is sort of like is this batch machine
that whenever i schedule the ship
departing up to a certain number of
packages can be sent on the ship so if i
schedule the ship here then i only do
one job and the ship goes once if I
schedule the ship here I can do both of
these jobs but of course I cannot do an
unbounded number of jobs this the ship
has a capacity so that's what i mean by
the notion of match and so the question
really is what is the smallest number of
trips necessary to schedule all these
jobs so that you don't miss any
deadlines right so that's sort of a very
simple basic model that you can think of
in this batch really well in prior work
looks at this problem but doesn't take
into account the number of trips the
whole goal is just to find a feasible
schedule with an unbounded number of
trips so one of our goals was to was to
find an optimal solution for this
problem so i also wanted to say that is
this problem is a little bit more
nuanced so the shipping model it was not
also known as the tracking model in the
literature where the batch has to be
synchronized right so the jobs are
scheduled exactly the same group you
could think of another pizza oven model
which is slightly different where I put
a pizza in the a1 and then I put a
second pizza in the oven and then when
the first one is finished I can take it
out and put another pizza right maybe
two pizzas can bake simultaneously so
this is like a non synchronous model so
this is a badge but it's not a
synchronized bad it just has the
property that at most two things are
running at any point of time and here we
want to minimize the running costs or
the energy costs which is when the
machine comes on and when the Machine
goes off okay so is the distinction
between these two models clear all right
so in some sense the model on the right
is a bit more constrained because all
the jobs have to be scheduled
simultaneously okay so I'm now going to
get into the into this problem bit more
formally so the problem is described is
the following i have n
ops ii saw is a really similar deadline
in the most general version we can think
of a job as having some length I'm going
to focus for the next ten minutes on
unit length jobs we have a batch machine
that will make life even simpler I
assume time is slotted so I can in ass
time slot tournament on or off and so if
i turn the machine on then we say that
the machine is is active and i can
schedule up to some number of jobs and
the whole goal is to minimize the number
of active / that is very easy easy
problem to think about so you can think
of this simple model as basically
talking about maybe a rack of processors
that i turn on and then bunch of things
can be done at the same time or a
multi-core processor where the processor
is on and a bunch of threads can be
executed simultaneously okay so since I
just want to talk about about unit
length jobs so this example shows you a
simple schedule right so here if you
look carefully notice that the batch
capacity is 3 so I have the schedule
where each a job here could have been
scheduled at any point of time and we
have to align these jobs in a way and
find the schedule so that I'm scheduling
no more than be jobs at any point of
time so that's my batch capacity and I'm
trying to simply minimize the time slots
for which the machine is on in some
sense you could think of that as a
projection on the x axis of wherever you
put these rectangles okay so the
question is now how do we how do you
find an optimal solution for this
problem right now there is a general
problem famously known as capacitors set
cover so what is capacités it cover so
it's generalizes head cover in the
following way so have a collection of
elements and I have a collection of
subsets the subsets have some cost that
you can pay to buy the set but now it
also comes with a capacity so when I
purchase a set I cannot cover all of the
elements in that set but I have a
restriction of how many 11-second cover
and I have some flexibility so if this
set has size 3 and costs two dollars
when I by this set I can cover any two
of the three elements but I can't cover
all three okay now Gabe assets it
covered in wireless the written has
actually many many applications itself
it's an np-hard problem and there is
some very interesting work by by Woolsey
that I'll mention and a second but you
know what is the
relationship with our scheduling problem
so I could think of every time slot as a
potential set right so turning the
machine on is like mind that set and
then all of the jobs that overlap or
intersect the time slot are elements of
that set but of course I the point is if
I turn the machine on you here I cannot
cover all of them right I have some
capacity constraint so you can think of
this problem as a special case of
capacitor said cover but that's not very
useful because kebaya sets cover set
cover is a very hard problem so there's
a famous algorithm I woolsey which gives
a very simple conceptual greedy
approximation but the bound is not so
good its order log and approximation but
of course that maps it to a hard problem
right so but but the problem the
scheduling problem itself is not np-hard
so in 2008 there was a paper that showed
that you can solve the problem optimally
using dynamic programming but the
running time is high complexity is high
okay so it's a polynomial but I so the
next question you can ask is is there a
faster exact algorithm right without
using dynamic programming so that's the
algorithm that I'm going to present next
the algorithm itself is called lazy
activation and this is joint work with
Jessica Chang and Helga bow and so
here's how the algorithm is going to
work so that there's any questions about
the problem itself constable Lego so
hospitals oh just so you don't lose you
on and off there is no cost right so
that's a good point so that is quite
likely the dynamic programming solution
is easier to extend on the general cost
model which is a non-uniform cost so
what matters is maybe it's more
expensive to run a machine during peak
energy times and maybe cheaper to run at
later times and we don't model that
right for us turning the machine on is
so certainly yeah so there a dynamic
programming I'm sure can be extended to
handle cause so let me describe the
algorithm the algorithm is very very
simple so the algorithm is called lazy
activation so the idea behind lazy
activation is that there you don't never
have to be in a rush to do job right if
you do jobs as soon as they are released
then you are not really overlapping them
in the maximal possible way so lazy
activation just says I should basically
do things lazily as late as possible so
you could think of our lazy activation
algorithm it says let's look at the
first job will have the earliest
headline there's no reason to do this
job before this deadline so we'll
schedule it here but once I turn the
machine on to do this job now you have
an option of what other jobs you want to
scare you right there's a whole bunch of
other jobs that you could schedule at
this point of time and so obviously i
have only scheduled one job i have up to
capacity of three so i could schedule
two more jobs and what the lazy
activation algorithm does it says take
the jobs that are earliest in deadlines
in the future and schedule those first
right so that's sort of the optimal we
can prove that that's sort of the right
thing to do right the problem with this
algorithm that I just described you
doesn't quite work and the reason it
doesn't quite work is that you might be
running along and you might get stuck
you might get stuck because you suddenly
come to a point in time when maybe 100
times B jobs have that deadline and so
you now you just don't have enough time
to schedule all the jobs right because
you sort of waited too long and not all
of them can be scheduled here so we're
going to have a pre-processing step in
the pre-processing step we go to the
following so what are on this
pre-processing step before we start lazy
activation now I'm going to look at all
of the jobs and I'm going to scan time
right to left okay so I'm going to look
at a last deadline for example and ask
the question how many jobs have this
common deadline right so if at most be
jobs have this common deadline then we
are not worried in fact that's the
property we want to enforce no to
enforce an every deadline possible value
there are at most be job so that
deadline so now the question is what do
we do if we have more than B jobs to the
state line right so in this example B is
3 but we have five jobs with that
deadline so obviously the optimum
solution cannot schedule all five jobs
that last lot there's an upper bound of
of three so what we are going to do is
we are going to take all the excess jobs
more than be it ever released the early
as possible and adjust their deadlines
by subtracting one ok so now certainly
have the property that at most be jobs
with this deadline and now we will move
to this value and enforce the same
property again so it might have been
true that at this point of time there
were less than B job so the deadline but
now I have more and then we'll apply the
same rule and apply this rule
so that's basically the entire algorithm
so in step one we do this pre-processing
step baby scan the deadlines from right
to left and then in step two now we do
the algorithm left to right doing lazy
activation and then every time I have
some option of what to put I'll pick the
jobs who are not immediately do but that
one have the earliest deadline in the
future all right so a simple example is
suppose to f1 has been run then I pick
the job with the first date and a
scheduler and them all the among
overlapping jobs I pick the remaining
two and a scheduled those with the
earliest deadline I get rid of them and
keep going okay so that's the entire
algorithm so the algorithm is very
simple very easy to implement there's
some interesting properties about the
algorithm that are not completely
obvious so I won't really talk about the
proof of the algorithm the proof is not
very complicated you can work it out but
the not obvious property is the
following so here's an interesting case
here again B is three but we have seven
jobs all that are valid only over two
slots right now there is no way that we
can do seven jobs in these two time
units if I can only scheduled three jobs
at once lot so of course this is input
instance itself is infeasible there is
no way that we can do seven jobs in two
time slots if I have B equals three now
what will happen in step one notice that
i have four jobs with this deadline so
we will subtract one from these two jobs
now I love words of this deadline and
then i will subtract one of them again
and this window will collapse to the
empty set right so this job's window
will collapse so obviously this job has
to be dropped what is interesting that
we can prove that the algorithm actually
scared you is a maximum number of jobs
that could be scheduled in any any
optimum solution moreover the number of
slots or which the algorithm is it turns
the machine on is optimal okay so that's
the part that actually takes a little
bit of work to prove that we are
scheduling the largest number of jobs
that can feasibly scheduled but also at
optimal cost questions
so in the next part of the talk I want
to sort of talk a little bit about our
battalion jobs and there are some
interesting open questions here so in
arbitrary length jobs these are jobs
that are not of unit length so here in
this example have jobs three jobs
release times deadlines in some
processing time and this is the
non-preemptive case on the left right so
this job once I started I have to run
the job all the way to completion so in
this case in this example on the left
the active time of this machine is for
right so the for machine is turned on
for the first three slots then it's
turned off for two slots and then I have
to do the last job but if I am allowed
preemption then we could do interesting
things with this middle job I do two
units of the job I stop it and then I
finish one unit later so I could save a
little bit on the active time by lying
for preemption okay so sort of like
saying that some things in the a1 need
to make for longer but interrupting that
I mean I don't know whether I want to
eat a pizza that got pulled out of the
air one half being baked then I got put
back in right but you could imagine
processing of of jobs it could be
interrupted at for no cost so this
problem itself is np-hard this week is
very easy to prove the non-preemptive
version it turns out on the pre-emptive
case we actually don't know whether it's
NP hard or not ok so is the question
clear so they have jobs with release
Samson red lines are with lens
preemption is for free I can schedule up
to be job simultaneously and I want to
find an optimal scheduler which
minimizes actually we don't know thats
NP hard or not so our focus is on was
for a long time on trying to identify a
polynomial time solution to this problem
because we weren't sure it's NP hard we
could prove that so we were unable to
find out an optimal solution so we
developed an approximation algorithm
which i'll give you the high-level ideas
for but again it's an approximation
algorithm without having a proof that
the problem is np-hard so they might
actually be an optimal solution or it
might be in part we don't know okay so
let me talk a little ears a little bit
and talk about a relationship with
maximum flow which some of you might not
have seen before so the maximum flow
problem is a flow problem directed graph
but I have a source and a sink and if
you want to push the largest amount of
flow from from the social sink
so here we have a source and a sink for
every job we are going to create a node
in this graph and the capacity of the
edge going from the source the job is
simply the processing requirement of the
job right so if this job has lent three
somehow three units of flow will reach
this node and somehow have to be sent
back to the sink and all three units
have rescheduled for every time slot
itself is going to be a note here and
the capacity of the edge going from the
timeslot node to the sink node is simply
the batch capacity be at most be units
if I turn the machine on at most be
units can be scheduled there right now
in standard flow problems the whole
network is no and I just want to push
maximum flow for s to T so here the
problem is slightly different I want to
select some of these time slots to
activate right and once I activate a
certain time slot then I get a capacity
of be going from that node to the sink
node if the time slot is turned off then
this capacity zero I cannot schedule
anything there so the whole goal is now
is to select a subset of these nodes and
then turn them on so that the max flow
has a value which is the sum of the
processing times of all of the jobs so
or everything gets processed okay so
that's exactly the problem that we wish
to solve here okay so once I can decide
a schedule for our subset of to turn on
then it's a flow problem so I can
certainly check feasibility of a
schedule by solving a max flow right if
I decide oh I want slots one three five
and seven on then I can the flow will
tell me what the schedule is in a
pre-emptive a okay and we're going to
turn use this Oracle as a very simple
algorithm now so that leads to the
concept of what I call minimal feasible
solutions so what we're going to do is
going to turn on all of these slots
initially so we obviously know there's a
feasible max flow for feasible schedule
exists then in arbitrary order you pick
the order we just start turning these
off one at a time and if I turn it off
that's a permanent decision all I want
to check is turning it off still leaves
a feasible max low okay so that's the
algorithm right so just finding a
minimal feasible solution so we just
shut down active slots one at a time and
we do not share an active slot down if
is going to lead to infeasibility so
that's the entire algorithm okay so
start from all possible slugs being
active and
as long as a feasible solution is
possible we shut the slaughter okay so
it turns out that this simple algorithm
which is really a dump algorithm in some
ways not being intelligent in what order
you shutting things down if you could
find the right order then you converts
to an optimal solution but being
completely blind we can actually prove
that the cost the solution is no more
than three times opt ok so this simple
algorithm will find you a schedule which
might not be optimal in terms of the
number of active slots but we can prove
that its cost is lower than three times
opt now that bound of three is also in
fact tight so there are examples where
if you are not careful in the order in
which you shut things down you could
actually end up with a solution which is
three times out so i'll show you one
simple example illustrating bound of two
or and then you can generalize this to a
point of three so here's the collection
of jobs with release times deadlines and
then the number on the right shows you
the length of the job or what the
processing needs are right so here's an
optimal solution so this is an optimal
solution where B equals 5 and the
solution sort of has some interesting
properties these jobs notice that around
length 4 and they're all rigid so these
jobs have sort of no option so the
Machine really has some spare capacity
here to do one thing at a time and what
we decided to do was to make progress in
the long job which is the sort of the
right thing to do but if you try turning
the last slot off it turns out that's
feasible right you turn this last load
off then it pushes these unit job
strapping them across like this so all
these jobs get done and then the long
job gets pushed out and that's a
feasible thing to do but shutting that
last slot down is a big mistake right so
that's in this example is going to force
the minimal solution to be factored to
away from the optimal so let me give you
some high-level ideas about about where
this bount of 3 comes from so let me
forget this left shifting part I'm not
going to have time to discuss it is not
really that crucial but once we find a
schedule in the end how are we going to
analyze it there are some time slots in
which I'm actually doing be jobs right
so we found us scheduled by this naive
algorithm sometimes not so which we are
doing be jobs and there sometimes
arts which we call non-full slots we
were doing less than V jobs now this
kind of active time slot is great right
basically i turn the machine on the
machine had a capacity of B and now it's
being hundred percent utilized right
that's great this so we cannot have more
than opt many of such slots the problem
comes and we have many many slots of
this type where the machine had a very
high capacity of B but we were doing
very little work and so we might be
paying sort of a heavy penalty there
right so after the algorithm ends the
main thing is to come up with looking at
a schedule and identify which slots are
full in which slots are non full and
then we sort of have to figure out how
we are going to account for these non
full Schloss it is the large number of
non-food slots I want to sort of prove
to you that they were unavoidable and
the optimum solution also has to have a
very large number of idle slots where
not much was happening so that's the
dichotomy of active slots by full and on
full so here's sort of an example that
gives you some intuition of what is
going on so obviously proof is a bit
more involved but here's a good way to
think about the intuition suppose this
was the entire input right so this job
had three units here this job and one
unit here these are by three units there
and all the slots that you come up with
that non full right and you say well
this is sort of unavoidable what else
could be optimum do there's no way to
sort of piggyback and do multiple things
even though we had a batch capacity of
be so whole goal should be to identify
some subset of jobs jstar to charge
which are which are a disjoint
collection of jobs right if I find a
disjoint collection of jobs all of which
are pretty long then the sum of the
lengths of the jobs is sort of a lower
bound on the number of active slots for
any schedule agreed right so if I tell
you that these jobs are all disjoint in
time then I have to do them the optimist
elution has to pay that cost okay so so
sadly we could not find this absurd
gesture what we were able to find is a
subset jstor of jobs we're at most two
of these overlap at any slot so it's not
purely desires it's a subset of jobs
which have the property that if you look
at any point of time at most two jobs
overlap at that point so now if you
think about the optimal schedule the
best thing that the optimum could do is
have done two jobs right and what we can
prove
is all of our non full slots do not
exceed the sum of the lengths of the
jobs in the subsidy estar right so
that's where sort of the factor 2 comes
from and because we didn't worry about
counting for the full slots we sort of
could be expanding as many as close to
off slots there right so that's sort of
a the part of three comes wrong question
sitting so the atom will have to be
modified right because we have examples
where this minimal solution as
importantly as big as large is getting
close to three be right so the algorithm
is sort of tight right so the algorithm
the proof for the factor of 3 by uh by
it will fit just interpret the algorithm
quite rightly yeah we didn't think about
you already but if you think about this
disjoint argument it is sort of a
combinatorial simple comment oral
argument right close down the slot
instance of these noted does the order
in check we apply I agree overrule in
the order so I do believe still that
they should be a combinatorial two
approximation which intelligent or
dresses with indicated would be the
right way to go but we were not able to
prove it so now the problem does have a
two approximation so I'm not going to
really have time to discuss it but in
the paper that appeared in spa 2014 so
this algorithm is based on LP rounding
so we basically write an integer program
which is very similar to facility
location type problems where you define
a variable Y sub T which models whether
or not that slot is active or not active
and if a slot is active and YT is one
then it gives you a certain processing
capacity of me you want to make sure
that all the jobs get done and you so
this all the jobs are processed and now
you get a fractional solutions have a
picture of a fractional solution I won't
talk about the rounding well that's what
a fractional solution might look like
and then we have to round that to an
integer solution and you can so you can
get it to approximation this way but
this pretty involved so if you could get
a cleaner commented I'll go to my would
be much happier with that
but the algorithm is actually quite slow
so Jessica actually implemented the
algorithm the the flow based one and so
in practice there are a lot of rules
that you can use right so for example
when I showed you an at the example
where I said oh if I shut the slot down
the resulting flow is still feasible so
it's okay to shut it down but you notice
when that long job got ejected out into
the open the cost of that schedule is
very high in terms of activation right
so when we are solving this flow problem
we can actually try to compute the sum
some notion of a cost of this active
schedule and so you can use some
heuristics to sort of guide your search
and choose this lot was that start which
might be in the direction that you were
alluding to so in practice that that
does very well but it's slow because at
every step we need to solve this slow
problem right it's not really
incremental so at every step we are
deciding which slot to shut down so you
might realize studying this slot down is
gonna eating facilities I don't do touch
it I go to another one I tried right so
the algorithm is slow because you have
to solve this flow problems repeatedly
it's not a very fast algorithm and the
same problem happens with wolfie if you
try implementing woolsey it's again not
a very fast algorithm okay so coming
back to this idea of covering and the
slow viewpoint I think this is sort of
an interesting problem right so you have
a flow network this is sort of mortals
the jobs like I described before and
then we have nodes here and there's a
capacity going to the sink and in this
active time skinny ring rather than
competing a flow what we're asking is I
want to select a subset of the nodes on
the right and we pay for how many nodes
we select then every time we select an
orderly get a certain capacity of B and
then we are trying to find a flow that
supports a certain value right so you
can think of that as being another way
to mortal active time scheduling right
now this is a very general framework
right and in fact lots of problems can
be thought of in this framework so if
you go back and look at a lot of work on
on vertex cover so what discovers a
classical problem which is have a graph
and I want to select some vertices to
cover all of the edges it's a covering
problem there's a problem for almost 10
years in fact stuff is here he had a
very nice paper on capacity to vertex
covering
but the idea is that nodes now have
capacity so then you select a node this
node might have you know a hundred edge
is incident on it but if its capacity is
15 you can only cover 15 of those edges
which is exactly a capacity covering
problem so you could think of all of
these capacitors covering problems that
have been studied in the light of this
way in this flow network right so these
nodes here are modeling edges of a graph
and then if this edge is incident to two
vertices it has a possibility of being
covered by that note and they've been
covered with that node and the vertices
of the graph are here and now picking a
vertex cover is just like selecting
nodes of the right with a certain
capacity in the vertex cover case in
fact its capacity is not even uniform
every node has its own capacity and then
we want to route a certain amount of
flow or a sign all of the edges right
and then all of these papers basically
develop constant factor approximation
algorithms for this problem but there's
some special structure so in in the
graph case these nodes have mounted
degrees exactly too we generalize this
to hypergraphs where these nodes have
constant degree and get constant
approximation last year soda there was a
very nice paper by Jim Goodman's and
Wang with the improved about
significantly but all I was trying to
point out was that all of these problems
can be thought about in this general
flow setting and this problem if you
look and Gary and Johnson actually has a
name it's called min edge cost flow min
H cross flow in a way charges for any
nonzero amount of flow going to an edge
you can say well if the flow here is
zero I don't pay for it but it throws
non zero I pay for another some capacity
so there's clearly a close relationship
with all of these problems okay time for
questions before I change gears still
only one of the best yes so for the
pre-emptive case I don't know for proof
that it's NP hard so we have a two
approximation we have the lazy
activation algorithm which works for
unit length jobs so our initial attempts
are all trying to extend the lazy
activation algorithm to deal with non
unit length jobs will it be we couldn't
get an optimal proof any prove
optimality do you guys have any
intuition about where it might go I
thought about it both direction so I
don't know anymore
oh so let me talk a little bit about the
non-creative case we have punted that
case and I want to relate this to a
problem that has been studied
extensively in the literature so I'll
get try to keep this part of the talk
somewhat non-technical so this is the
busy time problem where we again have
jobs the release times deadlines and
some processing times we now we want to
find a non-preemptive schedule that's
the main difference in this part and the
number of batching machines which
initially was assumed to be one in this
line of work is basically assumed as an
unbounded number even though every
machine has a batch capacity and I'll
explain that in a second so let me jump
to an example which will make it clear
so here's an example of busy time right
so here I have jobs with release times
and deadlines and these jobs all have
some length but I want to find a
non-preemptive schedule this time so
this is water the input looks like i
have a jobs with lengths and i want to
find a grouping of the jobs so here is
one possible grouping of the jobs right
so i move the jobs around that's the
only flexibility we really have and then
we have to group the jobs and we group
the jobs in this way where each group or
a batch of jobs has the property that at
most be of them are running at any point
of time so here say B is three so I have
at most three jobs running at any point
of time and so I group them into two
batches and the cost of the first batch
is the duration for which it is on which
is when does the first job start and
when does the last job end and then
there's a cost for the second batch and
the goal is to minimize the total cost
right but what I meant by this
assumption of unbounded machines is that
these two rectangles can actually
overlap and in the differential problem
let of course this get this is a fine
schedule even one machine good run this
kid will come on do these jobs turn off
come on do these jobs turn off but in
the problem definition this is not
constrained so these two rectangles
could overlap so they sort of assumption
is that different virtual machine is
bring to you run every batch and that's
sort of not a desirable part and the
work that I've been doing lately tries
to address that if we have some partial
results in that step okay so is it is a
problem clear is just a non-preemptive
version really with this caveat that
things are unbounded okay what's more
interesting is that a very special case
of this problem for
interval jobs is also hard so what is an
interval job an interval job is like the
job on the top right where there is no
flexibility the length of the job is
exactly the gap between the release time
at the deadline okay so an interval jobs
look like that so you have no choice
about when the job starts or ends the
job turns on at the release time it ends
the deadline so what is hard about it
it's kind of stupid problem what's hard
about it it's just the grouping that's
hard and this problem is np-hard even
for interval jobs okay so there's a
paper by Winkler and Zhang in soda 2003
which proves the NP hardness of this
problem right so all you want to do is
to come up with a grouping nothing else
okay so where are we so the problem was
proven to be np-hard but Winkler and
saying a few months later there was a
paper by Ali cherry in part here which
gave it to approximation and they gave
an example that shows their algorithm
cannot do better than two and then there
was another paper published two years
later which give a slightly different
algorithm which also gave it to
approximation of the same problem and
then a few years later there was a photo
approximation published so you say wait
a second why the bounds getting worse
they should improve these authors are
actually unaware of the previous work
that's all ok now the algorithms all
developed in all three papers are
actually different this algorithm I
would say is the simplest of the lot is
a very very elementary greedy algorithm
so it's very easy to implement and their
algorithm as lower bound is three right
but but because this greedy algorithm
looks so simple I thought it was very
easy to analyze it so coil Jessica and I
spent a long time trying to prove that
this is actually a three approximation
but we failed in doing that so I don't
know where this greedy alkalis after we
fail we actually discovered these two
papers and realize that maybe even
improving it to three he wasn't that
interesting because they were already
algorithms the bounds of two yes is
there any way to say anything about how
these algorithms perform in practice yes
so I have actually a high school student
has been working on implementing them
and the others will do well practice are
sort of i would say small modifications
of these algorithms so you can provide
some intelligence choices to these
greedy algorithms and those actually end
up doing very well in practice but these
purely implemented she never implemented
this once implemented this one
one and most of them this one really
does quite well but with some changes in
the algorithm right doesn't do very well
but if you change it to do something
more intelligently in practice it does
really well worst-case bond that's
correct okay so the story is going to
get a little bit more interesting in a
second right so this this paper is
actually interesting for a bunch of
reasons and I'll try to talk about that
so all of these results are by the way
are only for interval graphs which was
at very silly problem you say well I
have no choice as to when the job starts
so why is this even hearts it's very
frustrating that this problem is
np-complete because it's just the
grouping that makes it hard and then it
can be complete and all these papers
give you know basically two
approximation is the best part now what
do you do when the jobs the more general
case nor interval jobs so there's a
paper by hunt occur at all which gave a
for approximation for this problem and
this is what their algorithm does sorry
interesting it says that just assume for
a minute that your batch capacity is
unbounded okay how difficult is that
problem they prove that that problem is
actually solvable in polynomial time
again the solution is very complicated
uses dynamic programming complicated in
the sense that the complexity is like n
to the power 6 or something like that so
it's not very efficient but you can
solve this problem optimally so what are
you trying to do you can move these jobs
around in time right that's the only
choice you have right now bats give ice
is unbounded as many things can run it
concurrently and you simply try to
minimize the duration for which your
machine is on so you simply trying to
move things around so you minimize the
projection on the x-axis that's all
you're trying to do what they've show is
that you solve this problem for
unbounded be and you get this schedule
and now you treat these jobs as rigid
interval jobs so you basically adjust
the release times and deadlines to snap
around wherever the job got scheduled in
that solution and now you have an
interval job case and then they actually
run the for approximate proxima shin for
the greedy algorithm by the Flamini at
all paper okay
and then they proved that the final
bound is four so this restriction you do
not does not really cost you anything
which is a bit strange okay now we
looked at this and said why they're
using the for approximation now we
discover that there are two
approximations why are we plug in a two
approximation that's a better algorithm
at least in terms of the worst-case path
it turns out that that doesn't quite
work so what happens is when you do this
adjusting of these jobs the optimum
schedule can actually jump quite a bit
right but the greedy algorithm is
oblivious of the optimum schedule it's
just bounds things they based on the sum
of the process in terms of jobs with
never changed all right so there is a
benefit to their analysis of the greedy
algorithm so what we were able to prove
is that even if you plug in the better
two approximations once you've done this
first step the optimal solution might
jump by a factor of two and then when
you apply a two approximation you can
end up with a solution which is four
times the optimum okay so no matter
which route you go you are ending up
with the four approximation you could
solve the problem the way they did by
doing the dynamic programming and then
and then run the greedy algorithm you
get amount of four but you could now
plug in a two approximation and you're
not getting anything better because the
optimum solution jump right and we have
examples where the optimal solution jobs
and you apply that I can they get amount
of four okay so so what do we do so now
our final result is amount of three for
the general problem an hour bound is 34
interval graphs for non interval case
everything is three ok so it is a better
bound so how does that algorithm work
the first step is still the same we
solve the dynamic program for unbounded
be and then get an interval case and now
our greedy algorithm is a little bit
more sophisticated so they're greedy
algorithm does the following is simply
sorts jobs by decreasing order by length
so they want to worry about the long
jobs first and then they're basically
it's like a trivial bin packing
algorithm they start stuffing the jobs
in in batches and then when putting a
job in a batch will exceed the batch
capacity to create a new batch it's a
very simple valuable right so we're
going to do something slightly more
clever so we have a large collection of
interval jobs now
and we want to decide what the first
batch is going to be now remember their
algorithm just orders the jobs in length
and that is one job at a time we are
going to do something slightly more
sophisticated so our goal is to find a
subset of disjoint job so in this case
this red set of jobs is a disjoint
collection of jobs and which subset of
disjoint jobs do we want to find we want
to find a subset of maximum total length
okay so this is if you look at the
Kleinberg Tardos test book in the
dynamic programming chapter in the
greedy items chapter this is known as
the weighted interval scheduling problem
right so there's a very simple dynamic
programming solution for this problem
weighted interval scheduling so the
whole goal is simply to find a
collection of disjoint jobs of maximum
total length and this algorithm we call
greedy tracking and this this joint
collisional jobs will give you the first
track in the batch and then on the
remaining collection of jobs you apply
the same algorithm again all right we
find another disjoint correctional jobs
make that the second track and once I
have filled upbeat tracks that's my
badge and I'm done with the first batch
and then whatever jobs are left i'll
apply the same i'll go through every
time so I'm finding this digit 1
collection of jobs at every time ok now
this algorithm yes and let that you can
do in one shop do a kind of weight but
here the cost functions are different in
the end I'm going to look at the so look
at this example right so if I pick the
first track which are the top two jobs
on here all the jobs of the same length
right so and the example is set up in a
way that there are no three disjoint
jobs so the first track is the first two
jobs then the second track could be
these two jobs and that's my first match
the first two and then track three
creates a new batch is these two jobs
and track 4 goes with track 3 so so
notice that my cost is actually pretty
high because I'm basically turning this
batch on here it goes all the way to the
end that I've done in this batch on here
it goes all dude in now anybody looking
at this picture is going to say whereas
i can track 1 interact 3 should have
been merged together and then track to
interact for stripping merge together
and our algorithm is not taking benefit
of any alignment issues we just Union
the tracks and that's where you sort of
paying a price
it's not exactly answering a question
but I just try to illustrate even our
algorithm we have an upper amount of
three and this is the example that we
have that shows a lower amount of two
right so the right analysis somewhere
between two and three repeating you're
finding color a weighted color every
second every step do for 4 B 4 B steps
do it in one in one shot maybe they
really give up on us but above it or not
great thing about that but you're saying
that somehow that will but how do you
take the overlap cost into account right
we are that's why we think friends FC
right so you're this example we are not
overlapping anything track 1 and track
to get Union into one bundle and track 3
and track for get you into one bundle
and that's not optimal if i put track 1
and 3 together that's a much better
alignment you could you can merge this
you can merge two tracks only they're
identical no an item our team is
oblivious so the first bead tracks going
to the first match the next beat tracks
go to the next patch and so on and even
that works right but it improves amount
of four to three but I think the right
answer is two and it's quite likely that
some clever way of doing track merging
so if you go back how we are finding
these tracks right so safe is right so
we are finding one sort of color class
with maximum weight and then with
whatever the remaining jobs are F we
create the second track what you are
saying that we should have actually
found be of them simultaneously oh wait
that's fine be maximally I don't know
whether it'll help in this example
actually because why would that prevent
track one interactive saw them being
paired together it won't because track
to interact three the only difference is
the overlap issues with track one and
your objective function somehow isn't
modeling that right so that's why I
believe that that's where the
improvement needs to come from so I
don't know whether it'll be be of much
value to solve that problem that's all I
just don't know
so in terms of the of the actual proof
here some of the the key lemma which
underlies all of the analysis and I
won't go through the nest but I've
already seen this lemma so we are able
to prove that at every step when you
find this maximum disjoint collection of
jobs what we're calling the track it has
the property that its span of the total
cost of the track is at least fifty
percent of all of the remaining jobs
right and this lemma is sort of the key
to doing the whole analysis about
comparing to the optimum solution so I'm
not going to spend time going over the
proof the proof at some high-level
follows the greedy proof at this point
in terms of a high level charging scene
of course the proof is actually
different because the algorithm is
different but it's sort of a standard
charging proof I wouldn't say something
very sophisticated let me start by
saying so there's a student who's been
actually implementing some of these
algorithms and trying to compare them we
don't have any any real datasets rights
all of this comparison on synthetic data
sets like I mentioned earlier in this
dynamic programming solution is a lot of
inefficiency in terms of running time in
mapping the general case to the interval
case so again now we have some
algorithms that at least do this
efficiently but at a cost of the optimum
solution um I didn't really get a chance
to talk about this result but that's
fine oh so the simplest online model is
the jobs are available when they are
released and actually we have an
algorithm where if B is infinity so you
have some of this very powerful machine
that you can turn on whenever you want
and whatever is in the system gets run
but it's a very expensive machine to run
so you want to minimize the time for
which you run it and in the online
setting we have a five competitive
algorithm so the algorithm is very
simple if you delay things as much as
you can so you never turn it on and you
are about to be in a situation where if
you don't start the job going to miss
the deadline so you start the job at
that point and let's say this job at
some length 13 then you're going to
commit to turning the machine on for
twice the length of the job right so
everything else that is available as
that machine is running which fits in
that window of time will get executed
and then you turn the machine off unless
some other job is also about to miss its
deadline and you start that but the main
thing we commit to is when we're under
low job we double the committee
to how long we go to run it for and so
at least for that algorithm the upper
bound and lower bound of five or tight
oh we have examples where the algorithm
does five again I don't believe that
that that's that I'd algorithm so I
think they'll be improvements possible
to that algorithm is that like a price
efficiency for a HD entire batch I'm not
sure if you're familiar with ash no I'm
not going it's I didn't send the
question actually okay maybe we can
discuss it offline right so i alluded to
a lot of results in the stock so let me
just quickly go over these slides so the
paper then I spend most of my time
talking about lazy activation actually
only covered one Albie's of the paper
the paper actually has several results
there was an Easter 2012 and then we had
an experimental analysis of of some of
those algorithms of capacity covering
and also of the general length problem
in ln x 2013 and most of the stuff with
busy time and also the pre-emptive case
was published more recent paper is part
when t 14 and I didn't really get a
chance to talk about the work with
Frederick the stuff that I mentioned
very early on was a paper by barn rgr
and myself which was published in solar
2010 with this machine activation
problem and then there are some
generalizations and then these are some
of the papers that I decided in the talk
I also went to show you pictures of my
collaborator soberness aha graduated
with a PhD now she's a faculty were at
UMass Jessica trying finish your PhD a
couple of years ago in our department of
defense jion is now a faculty member
etching why University coil is at Xerox
labs you were asking about Hal game who
earlier he's professor emeritus at
Colorado but he does to read email and
he's been actually pretty hard at work
in his retirement so one things that he
told me is that he had a bunch of
conference papers that he never
published in journals and so he's been
spending a lot of time writing like 40
page long journal papers of stuff he
published in conferences long time ago
so in he said we saw them to read I just
didn't have to read them but very
interesting work and actually I didn't
get a chance to talk about some of the
things he didn't miss a paper which are
very interesting he developed some
combinatorial algorithms for stuff where
we were solving a linear program
initially and Frederick Kohner is
actually an undergrad Princeton I
started working with him when he was in
high school in the in Maryland and so it
continued collaboration so let me let me
take a few more minutes
and talk a little bit about some stuff
that has been the focus of attention of
both myself and the back of the room is
the inventor of skip lists will pew
who's professor emeritus at UMD and form
but starting about a year ago or we've
been hard at work at a building project
so I want to just talk a little bit
about the department or the department
has our over 50 faculty much larger you
consider all the affiliate faculty in
the various affinage schools or
undergraduate enrollment computer
science is booming all over the country
but especially at Maryland we went from
a thousand major to 2,200 undergraduates
just in the last 36 months we have
worked when 250 minors and about 400
computer engineering students so let's
talk a little bit about what looking
ahead so I know hollow lens is a big
exciting thing happening here at
Microsoft one of our alums brendon urie
together with two of his friends from
Maryland Michael and to know and Andrew
Reese what cofounders of a company
called oculus VR which was big time in
the news last year when they got bought
by Facebook for two billion dollars and
so Brendan and Michael have done an
amazing gift to start kick off this
project that bill and I have been
heavily involved in so that's sort of
the picture of the model of the building
that we're in the middle of planning
it's a sixth floor building for computer
science a lot of facilities for lecture
halls collaborative classrooms big open
cafeteria research labs lots of space
for PhD students I guess our undergrads
have been sort of stuff like sardines in
classrooms lately would love those
facilities Ryan sitting at the back
there is an example or and especially
what PhD students the current building
that variant is kind of depressing none
of their offices have have windows and
lights and so this building would be an
amazing facility for sewage to come
together and collaborate and work
together so I just wanted to share with
you sort of at the ground store building
model so even working with the
architecturally six months or so so
that's sort of the extension that was in
the picture before so this part is going
to have both a 100-seat collaborative
classroom as well as a 300-seat
collaborative auditorium the main
building itself that sort of the
footprint so boomerang shaped building
this is the open cafeteria atrium space
and then they're going to be a lots of
research facilities labs
the first two floors are primarily for
undergraduates with classrooms robotics
labs and so on as well as hacker maker
spaces which occupies actually a third
of the fraction of the second floor and
then floor sort of three four five six
or mostly for research and PhD students
so that's been a pretty exciting project
that we are the middle of fundraising
for so you sort of asked me earlier what
brings me to Seattle so we have about
ninety percent of money in place the
state of Maryland is funding about
hundred million dollars grindon michael
together gave 35 million the cost the
project is expected to be about 148
million so we still about 12 to 13
million shorts left for the fundraising
for but the building project is a very
fast track the plan is to do a
groundbreaking next year so i guess the
architects have to finish their plans
this year and opens in two and a half
years so now so come visit it is going
next to the sea sick building so if you
look at this model carefully that's the
csic building and this is a big parking
lot right now so as soon as we enter a
campus drive from rod from campus drive
this is the first big thing you see on
the right hand side actually very
prominent from route 1 yes try the whole
parking lot will go away and this area
will all be landscaped that's my gosh
they rush it okay the second reason why
you see part of it doesn't go all the
way down to the ground yes um 100-year
floodplain so we can't actually build in
that area on the first floor got it okay
and that's an overhang yes that's an
older I also be a coward seating area we
were hoping to do a pure cantilever
turned out that was going to be a little
bit too expensive so don't be some
pillars there of support ok but it'll be
a Plaza that will be covered by the rest
of the building another part is there
but yeah the whole area this will be a
new quad you no good either play frisbee
and hanging out on the fire lots of
outdoor space the cafe and also if you
want to organize conferences here this
was amazing at least for 300 sheep sighs
conference and it's sort of and they'll
be a Luger coming up next somewhere
across the street so that hotel project
started a while ago actually this
already stuff in the ground so they'll
open well before us I think certainly by
January 2017 I think they'll be 0
and that's just like 100 meters away but
yeah it'll be a great place to host
events because this sort of separates
the noise from the rest of building and
so on right you can actually have an
event here and it doesn't disturb the
other occupants the building what else
so in terms of the department these are
sort of some of the areas that we are
planning on growing in the plan is to
hire about a dozen there's a new faculty
the next three years so going to be a
lot of faculty positions um along with
the brand and his mom gives it two
chairs the computer science so going to
be recruited for recruiting for a chair
in the specific area of virtual and
augmented reality there's already a lot
of activity which in cyber security
actually Maryland with the cyber
security center that opened about three
years ago what our plan is to grow that
there's a new quantum computing
Institute Andrew child some University
of Waterloo was recruited about eight
months ago so he joined merrill enemy
looking for a second faculty member in
that area oh and these three areas are
sort of big growth areas i would say in
the next three years well the quantum
computing is really almost everybody in
that space is a theory person at least
the ones that we had to trying to go a
friend rose a theory person for example
andrew child's yeah so he got his PhD in
physics but his most of his publications
are here several papers in theory
conferences for example he was a
professor to waterloo for a number of
years before we recruited him another
exciting thing that's been going on is
CS education with tomorrow this was
actually funded by a gift from from bill
Pugh and this is sort of twofold we're
trying to improve the quality of the
education for our students part of this
involves creating flipped classrooms so
a lot of the faculty have taken this
model on where lectures and video tape
so this is a video lab setup now so if
you're teaching any class in Maryland
you can just come to the video lap
record the lectures in advance so the
students and watch the lectures and then
the classroom can be actually discussion
is opposed to just teaching we just
recruited a a special honors advisor
whose goal is to do CS enrichment for
undergraduates so different from
advisors this was a person which Gerber
who used to be professor environment and
left to go really 15 years ago he just
recruited him back we're also creating
our data science graduate certificate
program which will launch next year this
is a for-credit 12 credit program with
four courses and
we're in the process of creating an
undergraduate assign specialization as
well so this will be similar to the
cybersecurity specialization which is
the only specialization we have right
now okay oh let's say that amount of
time so let me stop</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>