<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Exponential Mechanism for Social Welfare: Private, Truthful, and Nearly Optimal | Coder Coacher - Coaching Coders</title><meta content="The Exponential Mechanism for Social Welfare: Private, Truthful, and Nearly Optimal - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Exponential Mechanism for Social Welfare: Private, Truthful, and Nearly Optimal</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-BmTopi6faY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi it's my pleasure to introduce see
who's from University of Pennsylvania
and he's an intern here it's his second
time interning here and he will tell us
about exponential mechanism for Social
Welfare from a point of view of privacy
efficiency and so yeah thank you and hi
everyone thanks for coming for this talk
what I'm going to talk about today is
how to design mechanism that are both
truthful and differentially private and
this talk is based on a recent paper
with my adviser some puff cannon and
some follow-up discussion with Aaron
Roth so this is what I'm going to talk
about today so first I'll go through
some basic background about mechanism
design in particular designing truthful
mechanisms and as well as differential
privacy and if you're already familiar
with these two areas feel free to take a
nap for the first 15 minutes of the talk
and then after I go through the basics
and make sure we're on the same page
I'll go go on and talk about our main
result which is a general mechanism for
getting both truthfulness and
differential privacy simultaneously and
then at the end of the talk I'll talk
about some discussions which are some
extensions of our results are some
discussion about model and conclude with
a few open problems so that's the plan
of today's talk first of all let me
start with a motivating example which is
this oil fuel allocation scenario
suppose the government now want to
allocate a bunch of our fields to
several companies say BP Shell and so on
now for each of you different companies
may have different values for it for
various reasons for example maybe this
gray of you is some is one on the
Caribbean Sea and bps do troubled by the
US oil spill accident a few years back
so BP has less incentive of getting this
another
of in Caribbean Sea and maybe shell has
done some extensive research on the
field and figure out the amount of oil
store in the field is actually much
larger than everyone else is thinking
so shell has more incentive for getting
the fuel and similarly we can make this
assumption for all of us so basically
for each company and each of you
there could be an arbitrary R value for
the company getting that particular oil
field now what the government wants to
do is to make it allocation between the
oil fields and the companies and just
for the sake of presentation I'll assume
that each company you will get exactly
one oil field we can interpret this as
each company you're having limited
resource and cannot start more than one
knew of you at a time but this
constraint is just for presentation
purposes it's not a restriction for our
result so what the government wants to
do by making such a location since it's
the government who's making the
allocation we will assume the
government's go is not in maximize
revenue because after all the government
can just print some money what
government want to do is maximize the
overall goodness of the society and that
one natural objective is what's social
so called a social welfare which is
defined to be the sum of each company's
value on the oil through that it gets
and it's not difficult to see that once
we make the restriction that each
company get exactly one oil feel the
allocations are simply matchings between
the companies and views and the
objective of maximizing Social Welfare
is simply a max way by pattern matching
problem so there are various ways of
looking at this problem for example we
can think about this as a algorithm
design problem in which case we want to
design the dis wrap box in the middle
and which is kind of a input-output
interface what's the input to this red
box is the private valuations of the
agents and what's the output of these
red box is some outcome from the
feasible range which in this example is
simply the set of all possible by
Jara matchings and the goal is to
maximize or minimize some objective
function in this case is the total
weight of all the edges in the matching
and in particular this not nothing so
special about max matching we have
replaced this problem by any algorithm
design program say Facility Location max
cut or sanitary or any of your favorite
algorithm problem and any other
algorithm design problem can be
interpreted in this framework
so so first of all minimization
maximization are the same up to negation
and also there could be a same in cross
flow or other minimization problem that
also fit into this picture right
for example that's right so so four
minutes minimization problem it could be
that each player on one of the edges in
the graph and then the player has a cost
for the maximum designer of choosing the
edge let's say the maximum designer is
trying to purchase a set of edges that
form a spanning tree that connects a
bunch of cities on the map and it's kind
of a yeah a procurement auction but so
far we haven't get into the auction
setting so so far it's just a
optimization problem and any algorithm
problem can be kind of view in this
picture right
okay so so if we were thinking about the
maxim action program in terms of
algorithm design then we are done
because we all know that this next way
by pattern matching can be solved in
polynomial fit our favorite algorithm
and just run it but the real world
situation is a bit different in the
sense that all this input data are
private information are held by these
companies and throughout this talk we
will refer to this but as agents in the
market and since all these informations
are private to the agents we need to
incentivize the agent to report their
true values in order to pick a
reasonable outcome based on this
underlying data
so suppose we take this into account
what does the picture look like again we
want to design this red box in the
middle and in order to make a different
from the algorithm we will point out
call this red box that mechanism is that
of an algorithm so as the input instead
of the true underlying data what we will
be getting as input is simply the
reported value from each of this
individual agent which may or may not
equal the true underlying data and based
on this reported data we need to pick
some outcome X and along with a price
our payment vector P this payment vector
P R we should interpret them as sum to
that play important role in kind of
incentivizing these agents reporting
their true value and then our goal again
is to maximize or minimize some
objective with respect to the true
underlying data
okay so now what's the goal the goal is
to incentivize the agents to report a
true values so that we can make our
decision and choose a reasonable outcome
to maximize the true underlying
objective but in order to do so we first
need to understand why agents will lie
about a valuations what assumption in
particular we need to make assumption
about how agents will behave in the
market so as a standard assumption in
game theory and I guess in
microeconomics in general we assume that
agents will aim to maximize the expected
utility which it has this quasi linear
form equals the valuation of the outcome
chosen by the a mechanism minus the
payment we charge the agent and the
conceptual so so what's the point the
point is that the agent will lie if
lying can actually increase this quasi
linear utility so we want to prevent
that the conceptual solution for this
concern is to focus on the so-called
truthful mechanisms
what is the truthful mechanism a
truthful mechanism is one where agents
always maximizes this notion of
quasi-linear utility by bidding their
true value no matter what the other
agents do suppose we have this nice
property then we can say with confidence
that there's no reason for the agents to
lie and therefore they should report
that true value and and therefore simply
based on their reported value we will be
able to pick something reasonable and
one of the most famous example of such
truthful mechanism is probably the
second price auction or it's
generalization known as the VCG
mechanism named after with Vickrey Clark
and Grover's which essentially choose a
allocation which maximize the overall
happiness in the society namely the sum
of the valuation of our agents over this
outcome and what they say is that if you
do that then there's some general
methods of deriving the payments that
make it truthful so so far so good we
are
we are able to handle truthfulness at
least for the social welfare
maximization problem and arguably most
if not almost all of the previous work
in economics and as long as well as in
algorithmic in theory has been focused
on this trip on is concerned and in fact
usually for most game theory talk this
slide is the end of the introduction and
we will get into the more precise model
and results part but what I'm going to
do today is slightly different I want to
argue that there's actually something
else we need to worry about if we want
to incentivize our agent to report to
values this other concern is that agent
or people care about privacy so over the
past few years users on users has become
more and more aware of how their private
data or their private information might
be misused on the internet or by some
other third party databases one famous
example is the 2008 paper on the
anonymizing the Netflix database so the
story is that Netflix used to release
these database about which user put what
kind of rate on and each of these movies
of course they also take an eye on
privacy but what they do is simply make
the database anonymous so intuitively
anonymous should implies privacy right
there's nothing you can learn about
about individual agents data once it's
anonymous but what this paper shows is
that by combining this database with
some other information from the internet
they can actually denote the anonymize
this database and learn a lot about
individual agents information from the
database what they do is essentially
using the fact that the agent not only
submit rate to Netflix they also submit
rates through other places like IMDB or
Amazon which is not completely anonymous
and they use that to identify who each
of these individual eight agent
is and then use that fact to be
anonymized the whole database and a more
recent example of how people are we're
becoming aware of privacy is the recent
complaint filed by epic against Facebook
saying that Facebook has misleading use
of agent the use of private information
and also has been sharing more
information about each individual than
they should to third parties and
advertisers and this has result in
facebook kind of has to fix all these
privacy issue and subject to a privacy
audit every other year for something
like twenty years from now and taking a
step back to the toy example that we
consider at the beginning of this talk
in that oil field allocation example all
these valuations of the companies are
sensitive business secret in the
following sense so a company's value on
each individual oil few may comprise
information about a extensive research
of the company has done on the area and
also include information about for
example maybe a company has recent
breakthrough and drilling technology and
stuff like that which the company
consider as competitive edge in the
future business and they do not want to
reveal them to their competitors in the
market and suppose we run the
traditional mechanism say the VCG
mechanism they're almost for sure that
will leak a non-trivial information
about all these private values and
therefore even if the mechanism is
truthful a privacy aware company or
privacy of where agent may still choose
to lie about a value or not participate
in the auction in order to protect their
privacy so the challenge here motivated
by all this example the challenge here
is to how to get to privacy and at the
same time still get nearly optimal
objective and this has been a relatively
new area known as differential privacy
over the past few years
but so far I haven't really defined what
is privacy because privacy is a weight
word for example in the first example we
have seen that making the database
anonymous is not enough to guarantee
privacy so what precisely do we mean by
privacy so ideally what we want is that
by participating in this database and
the third party should not be able to
learn too much about my private
information so more precisely suppose I
fix the participants and values of all
our agents and consider me reporting
truthfully reporting VI and reporting
lie about my variation and by reporting
VI prime this mechanism presumably will
choose outcomes from two different
distributions and what privacy means is
that by simply looking at one or a few
samples from these two distribution the
adversarial should not be able to
distinguish these two cases and more
precisely we will say the the mechanism
has good privacy if the distribution of
these two cases has distance some notion
of distance and most excellent for any
fixed values of our agents and any way
of lying about my to value so what
remains is that you define what notion
of distance between distributions that
we need we choose to define this privacy
and there's a long story here which I'm
not going to get into so how the notion
of distance we will use here is the
infinite divergence between these two
distributions so what does that mean we
will say a mechanism is differentially
private if I fix the value of other
agents and change my value from VI to VI
prime then the probability that any
subset of outcomes being chosen by the
mechanism should not change by more than
a multiplicative e to the absolute
factor
so what does the mean is that suppose I
look at this probability density curve
in the two cases where I lie about my
value and I report truthfully then the
probability dance point wise should be
bounded by this each of the absolute
yeah that's a point I would get you in
the second part so I guess so what the
queue says is essentially that if a
mechanism is differential private then
it also implies its approximately triple
because by lying I cannot change the
outcome distribution by too much and
therefore I cannot gain too much by
lying right but there's a problem with
that because ideally when we talk about
approximate truthfulness what we want is
we can get closer and closer to exact
truthfulness without hurting the
objective that we are trying to maximize
or minimize however by using this
approach if we want to get actually
close to triple Ness then we need to get
arbitrarily close to perfectly private
when we get to perfect privacy then
essentially all that we can do is the
trivial thing of essentially picking a
random outcome from all possible outcome
uniformly at random which is very poor
in in terms of objective so
yes let's say you have right okay by
nine
but this constraint also says nobody can
lose by lying
exactly so yeah that that's another
critic of directly using differential
privacy as a notion of truthfulness but
what we will do what we will be doing is
by imposing payments and with the help
of payment we can actually incentivize
agent to tell the truth even if the
outcome is kind of smooth and does not
change much if no matter what I tell ya
the outcome does not include a payment
at this point no it's actually it should
hold no matter what our agents do so I
should probably put a VI V minus I prime
just to say that fact so yeah we - I
need not be the true value of our agent
so and also for usually we will assume
epsilon is some small constant or even
you know small of one so each of the
epsilon is really one plus Epsilon but
we choose this definition because for
technical reason and it's usually the
standard definition in differential
privacy as well so but we you can
imagine it as 1 plus Epsilon if that
works more towards your intuition so so
I have given the definition of different
differential privacy but let me also
spend two more slides to give some
intuition about what is differential
privacy and what's the general way of
getting differential privacy so first of
all it's not difficult to see that no
non-trivial deterministic mechanism can
be differentially private because any
event that's chosen with probability 0
at one particular input has to remain
zero for all inputs so the best we can
do is simply choose the fixed outcome no
matter what the input is that means we
have to use randomness the problem is
how to use randomness so to get an
intuition I'll briefly go through two
general methods of using randomness to
get differential privacy which is also a
particularly related to our work
they are just input perturbation and
exponential mechanism so let's first
talk about the first matter input
perturbation the idea is to preserve the
market itself so other than the original
agent 1 to N I'll add a bunch of dummy
agents into the market whose value is
drawn uniformly at random from all
possible
valuations and and then what we will do
is to run the optimization problem on
this enlarged market with the original
agent and this dummy agents to choose
the outcome and if necessary project the
outcome back to the original market so
more concretely
about this matching market right I can
add a bunch of dummy companies and then
find a max matching in that case and
then only keep the edges that's adjacent
to the actual company as my outcome and
the hope is that by adding enough
randomness by adding dummy agents with
random variation the whole market looks
random enough and therefore it's
differentially private so what's the pro
of this approach the pro is that it's
extremely simple and it's oblivious for
which algorithm using in the middle and
also oblivious to the structure of the
problem so no matter we can take the
algorithm as a black box and without
knowing anything about the problem we
can still use the dis approach to
enforce differential privacy but of
course that comes with a price since
this method does not use any specific
property and structures of the problem
as you can expect it usually achieved
where we prove objective for most of the
problems in fact it only works for very
restrictive settings where essentially
are the total number of different
valuations for the agents is much
smaller than the number of agents in the
market so what does this tells us is is
that in order to get something more
general and works for more problems we
need to use specific structures of the
problems and arguably the only way we
know of using specific structures of the
problem while still general enough for
all problems is the exponential
mechanism yeah
uniformly random from all possible
variations so we're doing that yeah yeah
we okay yeah we have to have some
knowledge about the problem also we need
some knowledge to project this out come
back to the original market but that's
pretty much it I don't take any to
specific structure of the problems
so the exponential mechanism is
originally proposed by Frank buck sherry
and Kanagawa in 2007 what the
exponential mechanism do is to choose
the outcome X from the feasible range
with probability proportional to the
exponent of the performance of this
now--come scale by the epsilon which is
the privacy parameter and / / - 2 Delta
where Delta is the ellipsis constant of
this F function in terms of V V 1 to V n
but for the purpose of this part we can
always assume that is 1 because I always
scale the function properly so that the
Lipschitz constant is 1 so let's ignore
data for now
exactly but yeah right now I'm assuming
this is a maximization problem if it's a
minimization problem I'll have to take
the negation over here
so there's some nice thing about this
expansion mechanism first of all it's
always absolute differentially private
no matter what the problem is and this
is not difficult to verify and moreover
it could be the right answer for
differential privacy if we ignore
computational efficiency so Aaron Aaron
Roth actually conjectured that this is
the right answer but I'm more
conservative it towards this conjecture
but as a matter of fact it has be proved
to be a synthetically optimal in terms
of a trade-off between the objective
function and privacy for many problems
for example counting queries complete
our public projects K medians and set
cover and many other problems and we do
not know a single count example where
this exponential mechanism is not
asymptotic Li optimal ignoring the
computational efficiency matter so and
actually this is the mechanism
we'll be playing with and get
truthfulness out of it so I think it's
important to you that everyone
understand the definition of this
mechanism and is there any question
about
okay if there's no question about the
settings and about magazine design and
differential privacy I'll move on to the
second part which is how to q1 vers
youtubers with one stone kind of combine
the techniques from mekansm design and
differential privacy to get both
truthfulness and differential privacy at
the same time okay let let me first you
know I hate definition but let me spend
one slide to make sure we're on the same
page we assume there are n agents and
some feasible range of outcome R and
then each agent has some private values
matching from R to zero one use this
interval 0 1 because I want to make sure
the social welfare function has Lipsius
constant 1 R in terms of its each
individual agents input and the
objective is to choose a outcome from
the range to maximize the social welfare
which is defined to be the sum of the
agents value on this outcome chosen and
since we will be considering randomized
mechanism the objective will be
maximizing the expected social welfare
and I cheerfulness and differential
privacy is as we define in the previous
slides I want to make one remark here
about our definition of differential
privacy that is so I'm assuming in this
talk that we only consider the
differential privacy issue of the
allocations but in real world the
payments may leak information as well so
we do handle payments in our paper but
the reason I don't want to talk about
that is the techniques for handling
payment is quite standard and
essentially just add some new partial
noise in the into the payments and I
really feel the differential privacy
concern of the allocation is the more
interesting part so that's what I'm
going to talk about in the talk and
ignore the payments
so given the model what's the general
question we are trying to attack what we
are trying to attack is that can we
design mechanisms can we find a general
ways of designing mechanisms that
simultaneously achieve all four of the
following we want to have differential
privacy we want to have truthfulness we
want to get near optimal social welfare
subject to the privacy constraint and as
computer scientists we want
computational efficiency but of course
I'm being too greedy by saying I want to
achieve all of this because even
ignoring the differential privacy part
getting truthfulness Gustav's welfare
and computational efficiency is the
central problem in algorithmic game
theory for the last 1012 years and we
are still far from being able to
completely understand that so there's no
hope to answer this question in one shot
so what our strategy is is to first
focus on the first three objective
differential privacy truthfulness and
good social welfare and provide
something as general as VCG and then on
a cait problem by problem basis we can
consider the computational efficiency
issue and in this talk I'll only talk
about the interaction of the first three
part which I feel is the more
interesting part
so there are different models of saying
getting truthfulness and privacy at the
same time but I feel I have already
throw into many definitions right now so
is that are distracting you guys with
different definitions I'll simply state
my choice of model and then at the end
of the talk I'll justify why I feel this
is the more appropriate model to use so
what we are going to assume is that
agents will not participate in the
auction unless the mechanism is absolute
differentially private and once the
agent choose to participate then they
will aim to maximize the usual notion of
quasi-linear utility so under this
assumption what we need to do to
incentivize agency reported true value
is to design a mechanism whose
allocation rule is absolutely
differential private and the allocation
work together with the payment satisfies
the new usual notion of truthfulness
okay now suppose we take this assumption
what is known already are in this model
so it turns out that suppose we only
want any two of these three objectives
we already know what to do if we want
welfare and truthfulness we can simply
run the VCG mechanism which get optimal
social welfare and perfectly truthful
suppose we want good social welfare and
differential privacy then arguably we
can use exponential mechanism to achieve
the optimal trade-off and if that's the
case for many problems suppose we only
want truthfulness and privacy then we
can have the trivial solution of always
picking a outcome that's independent of
the input which is perfectly truthful
and perfectly private of course has very
poor social welfare so the real
challenge is getting all three of those
and even this has been considered before
so in the original paper that McSherry
and halat proposed this exponential
mechanism they also point out that
differential privacy actually implies
approximate truthfulness like Nikhil
just point out what that means is that
by definition
lying cannot change the outcome
distribution by too much and therefore I
cannot gain too much as long as the
mechanism is epsilen differentially
private and therefore that implies
approximate truthfulness and Nikki also
has makes some critics about this
approach that although people have less
not not much incentive to lie they don't
have much incentive to tell the truth as
well and also we cannot get opportunity
to chew exact truthfulness without
hurting the objective function so that's
not as a peeling solution concept as we
would like to so in order to handle that
there's some follow-up paper by missing
at all in 2012 and also independently by
hala and Lucia in 2010 they show how to
convert this nearly truthful mechanism
into exactly truthful ones in some
specific settings but first of all this
way of converting it in exactly four
mechanism only work for very restricted
settings and also after this conversion
the mechanism is no longer
differentially private so we are getting
truthfulness but on the same time we're
losing privacy and it's the time as they
attempt to getting truthfulness and
privacy at the same time David shall
study the mechanism design without
payment and propose using input
perturbation as a general method of
doing so so what does that mean improve
perturbation means I'll add a bunch of
random agents into the market as before
and in the middle I'll use a truthful
mechanism in the red box and therefore
this mechanism with respect to the
original agent should still be truthful
but of course as we say that implicit
evasion only works for our very
restrictive settings and this method is
not as general as we want so what we
prove is there's actually a very general
methods of getting differential privacy
and truthfulness at the same time for
very general setting so first recall the
exponential mechanism is to
choose a outcome X proportional to the
exponent of the social welfare scale by
the privacy parameter what we show is
that for any mechanism design problem as
long as the objective is social welfare
the exponential mechanism can be coupled
with some proper payments to make it
truthful exactly truthful so how we
should interpret this in some sense this
is a family of generalization of VCG
mechanism with for which by scaling the
epsilon from positive infinity to zero
we can have a family of differentially
private version of BCG where epsilon
goes to infinity this is the VCG
mechanism because we will always choose
the outcome which maximizes the social
welfare where epsilon goes to 0 we get
this perfectly private but trivial kind
of uniformly at random picking a outcome
uniformly at random from the feasible
range yes so so there are two parts of
outcomes actually so the first outcome
is what I refer to as outcome is the
outcome in the feasible range and that
has something to do with the social
welfare and that part has to be
differential private but also the
payment part has to be differential
private right but as I mentioned there
are very standard trick of payments
since standard tricks for the payment to
make a differential profit as we can add
some zero mean noises to the payments
and since the agent only aim to maximize
the expected utility that doesn't change
really change their utility assuming
risk risk neutral and therefore I will
focus on the differential privacy
concern of the simply the outcome
and also depending on how much you
believe in that conjecture that
exponential mechanism is the right
answer for differential privacy we can
say that for many problems differential
privacy is compatible with truthfulness
at least for this maximum design with
payment setting so before I move on to
the proof is there any question about
the statement okay so there are
different ways of proving this theorem
in fact the original proof we have is a
bit complicated but later we found a
very cool proof by making a connection
to physics so let me first introduce
some background some essentially some
high school or college physics so the
notion I want to talk about is skips
measure so a Boltzmann distribution
sometimes in statistical physics so
consider some particles of a gas in a
container and assume this particle this
gas has RK energy states you want to EK
what the gifts measure or the Boltzmann
distribution says is that suppose I pick
a random particle from the container
then the probability that it has state J
is proportional to the exponent of the
negative energy of the state divided by
the Boltzmann constant and the
temperature okay and this sometimes is
also known in a less precise language as
nature prefers low energy as lower the
energy is the higher this probability is
or higher temperature implies more
chaotic system as T goes infinity you
will have a uniform distribution over
all possible states so then I would like
to make this simple observation that the
exponential mechanism itself is the
Gibbs measure so here's a I want you
verify this observation by this table so
I guess simply by staring at the
probability density function or mass
function we can already seem to see the
similarity between these two guys but
let me make it more precise so in the
Gibbs measure setting nature want to
minimize the energy and in
terms of explanation mechanism they want
to maximize this notion of social
welfare so the social welfare and the
negative energy are playing similar
roles in the probability mass function
and also in both setting we have some
parameter which specify how chaotic the
system is in Gibbs measure we have the
temperature and in exponential mechanism
we have the privacy parameter where the
smaller this privacy parameter is the
more chaotic the system needs to be
because we want more privacy and this
two guy are also playing similar role in
the system so where are we going with
this
the point is there has been a lot of
study for gibbs measure or Boltzmann
distribution by statistical physics and
by making a connection between these two
guys we can borrow some of the theorems
and choose from gibbs measure and use
that to prove our result so more
precisely the notion I want to borrow
from gifts measure is the notion of free
energy so what's a free energy suppose
we have a distribution D the free energy
of this distribution at temperature T is
the expected energy suppose the state is
drawn from this distribution - the
Shannon entropy of the distribution
multiplied by K B times T and it turns
out that this fully characterized the
Gibbs measure as gifts measure is the
distribution that minimizes the free
energy and sometimes this is also known
as nature maximizes the entropy given
the expected energy level and this can
be easily verified or either by taking
the first-order condition of this our
minimization problem or there are very
very but I'm not going to bother you
with the math here so just trust me this
is true for now since we have make a
connection between Gibbs measure and
exponential mechanism we can translate
this fact into the language of
exponential mechanism right so what that
means is that the exponential mechanism
actually is maximizing this guy which
for fun I just called the free social
welfare so the free social welfare is
this social welfare suppose the outcomes
chosen from that distribution plus the
Shannon entropy of the distribution
scale by Q over epsilon this is simply
by replacing the corresponding terms in
the probability mass function and chance
later our previous fact in the language
of explanation exponential mechanism so
if you are familiar with game theory and
mechanism design this actually implies
this exponential mechanism is a maximum
visibility range allocation and
therefore there are standard techniques
to make it truthful and in case you
don't see that I have one slides which
explain why this is the case okay so in
order to see why the exponential
mechanism is truthful imagine the
following imaginary imaginary market
where instead of choosing outcomes we
are choosing a distribution of outcomes
and each of the in agent in the original
market now translates your agent which
maximizes the expected value a ssin with
respect to that distribution but I want
to add an additional agent into the
market whose a pure risk lover whose
utility is simply the Shannon entropy of
the outcome distribution scale by Q over
epsilon now what the VCG mechanism will
do in this imagining market is to
maximize the social welfare with respect
to the original agent plus this
additional risk lover right so it turns
out that the social welfare in this
imaginary market is exactly the free
social welfare which characterized the
exponential mechanism and therefore the
outcome is essentially the same for this
imagine market or with VCG mechanism and
exponential mechanism in the original
market and therefore we can translate
back the payments to the original market
and make it truthful okay and that's the
end of our proof or the main theory
now is there any question about the main
results before we want to further
discussion whore
all right so as I promised I'll talk
about what are the other models for
capturing privacy and truthfulness and
at the same time and why we choose our
model instead of theirs and also talk
about some extensions of our results and
conclude with a few open problems so
what's the other options of modern
privacy the other option similarly a
more natural option is to model privacy
via the utility function in other words
we want to kind of capture how much
information has been leaked by the
mechanism and then define a disutility
of the agent which is monotone in this
privacy laws due to pests participating
in the mechanism and then assume the
agent trying to maximize the usual
notion of utility
- this disutility okay this is seemingly
a more natural option of capturing
privacy or into the framework of
mechanism design and this has been
considered by David Shaw and our
ellington Adel in two papers in 2011
however mr. model actually point out
this assumption is a bit problematic for
the following reason in order to compute
these privacy laws the agent only need
to know his own utility his own
valuation but also need to know what
other agents reports in other words
we're in this dilemma we're assuming
suppose we are in the perfect
information setting where agents know
each other's our values in which case
they have enough information to evaluate
their disutility but since we're in this
perfect information setting there's not
much incentive to taking this privacy
issue into the picture because
everything is public as opposed
agents do not know each other's
valuation then it's funny if you say
that ages actually maximize a utility
which they do not have enough
information to evaluate so we need to be
a bit more careful in terms of choosing
the model
okay so so because the usual notion of
privacy laws we can define is some kind
of distance between the probability
distribution whether I lie or I tell the
truth right and that distribution not
only effect by my behavior but also
depend on what other agents tells the
mechanism and therefore in order to
evaluate how much information is leaked
by the mechanism the agent also need to
know what other agents report and
actually missing madam and getting
gendered out provides some partial
solution and they are quite generic so
what they do is they do not assume any
specific form of this this utility
function and simply assume there's some
disutility which agents have in enough
information to evaluate but this this
utility is upper bounded by this privacy
last epsilon and then they consider
problems where we can design strictly
truthful mechanisms and once we do that
then we can say that as long as the
mechanism is private enough then the
game in privacy for lying is not enough
to compensate the loss in value
valuation by lying because strictly
truthfulness means I will lose some
fixed amount if I lie about my
evaluation and therefore as long as the
mechanism is private enough it will be
truthful even for these privacy aware
agents however since we do not assume
any specific form of the disk utility
function arguably this is the best we
can do this we cannot design we are
specific mechanisms which take the form
of the disability function into the
picture and therefore this approach
again only work for very specific
problems and the line of attack I want
to propose here also as the first open
question is how about Bayesian setting
because in Bayesian setting
people have enough information to
evaluate their privacy laws in
expectation and therefore it seems okay
to assume specific form of this utility
function and therefore this hope to
handle more general settings even by
modern privacy into the utility function
so this is the first kind of open
questions from the talk and next I'll
talk about some extensions of our main
result so first of all notice that the
connection between exponential mechanism
and Gibbs measure and our main theorem
actually does not really use the fact
that we are using social welfare as our
objective function so in general for any
problem the explenation mechanism is
essentially maximizing the expected
performance shifted by the general
entropy of the outcome distribution
scale about u over epsilon right and
this actually gives some intuition why
it works so well for many problems
because in some sense exponential
mechanism is maximizing entropy given
the performance level and privacy in
some sense is trying to maximize
uncertainty in the system and in a hand
waving manner entropy is approximately
uncertain the level of uncertainty in
the system however it seems very tricky
to make this hand waving kind of
intuition more precise because
differential privacy is not defined in a
way using entropy it's defined using
like how much the distance between our
probability distribution condition on
whether this agent lies or not so I
think it's so interesting open question
to our trying to make this connection
more precise given that exponential
mechanism actually achieve optimal
differential privacy for so many
problems there I personally believe
there has to be a more intriguing
connection between these two guys
another extension need to use a
alternative interpretation of our main
theorem so it's well known that
maximizing entropy is the same as
minimizing the KL divergence to uniform
distribution so I can alternatively
write the characterization as
exponential mechanism is actually
maximizing the expected performance
minus the distance KL divergence to
uniform distribution scale by some
program factors the point here is that
there's nothing so special about uniform
right uniform is what uniform do here is
serving as a default distribution over
all possible outcomes and if the problem
has some nice symmetric over different
outcomes in the feasible range maybe
uniform is a reasonable choice however
for some problems maybe some outcome is
obviously worse off compared to other
outcomes and in those cases we should
put less weight on the outcome in the
default distribution even maybe put zero
weight on the default distribution so so
a due to that observation we can derive
a more generalized version for this
characterization so the generalized
exponential mechanism which take a
outcome X proportional to again e to the
actually that that should not be social
out there but arbitrary for performance
of that outcome scale by the privacy
parameter and then also kind of biased
by this prior distribution P of X can
actually be characterized as maximizing
the expected social performance minus
the divergence to this default
distribution P and this generalized
version actually captures most of the
extensions and of the exponential
mechanism in the previous literature for
example sometimes people just pick a
subset of outcomes which form a nice
geometric covering of the outcome space
in terms of this objective function and
then use explanation mechanism only on
that subset of outcomes that can be
captured by choosing P to be uniform
distribute
over that subset of outcomes and what
this means is that all this previous
extension of mechanism are also truthful
if the objective our social welfare so
our technique is actually compatible
with all those hock tricks or extensions
of exponential mechanism sorry yeah so
so first of all choosing a only a subset
of outcomes may improve the
computational efficiency for that the
underlying outcome space could be
exponentially large and the knife way of
implementing our exponential mechanism
is his running time kind of linear in
the size of the outcome space right so
that could potentially improve the
running time and sometimes that could
improve the privacy and objective
trade-off as well
sometimes some of the outcomes are
obviously bad for example in the
matching setting right we can also
include partial matchings into the
picture but that's obviously bad and
therefore I want to eliminate those
matchings into the in outcomes pace
now finally let me conclude with two two
more open problems the first open
problem is has something to do with
having differentially private mechanism
the answers query online so now let's
take a step back and from the this
mechanism design literature and back to
the disk database and answering queries
kind of scenario suppose we have a
database about information say of every
everybody in this building ninety nine
and one I want to answer queries such as
what's the fraction of people in this
building who has blue eyes or who has
brown hairs this is like a typical
database query releasing a scenario and
quite often these queries actually comes
online they are not given up front and
you kind of pick the optimal way of
preserving and using randomness to
answer all of them and ensure
differential privacy and that's exactly
what exponential mechanism do so a
challenging area in differential privacy
is how to answer this query online and
still being able to achieve optimal
trade-off between the error and
differential privacy so what you can
obviously do is independent nodes to all
these queries right but sometimes that's
the optimal because maybe the first
query is what's the fraction who has who
have blue eyes and the other is who do
not have blue eyes then it would be
stupid to use two independent
perturbation for these two queries and
you really want to use one of them and
this gap can be made arbitrarily large
they could be highly correlated and in
which case you want to only add a few
noises into the picture so there has
been some work done in this literature
and this online mechanism actually
performs very well they actually get
error bound close to this offline
exponential mechanism but it's very
mysterious why they are behaving so well
so the next open problem is to us
understanding that so we have
essentially characterized the
exponential mechanism as a the optimal
solution of a convex program and there
are algorithms for solving convex
programs where the constraints comes
online the problem is can we combine
those technique and this
characterization to understand and maybe
even improve this online differentially
private mechanism so now the objective
will be to minimize let's say the our
infinity area of all these are answers
or maybe I'll cue it out.you errors of
all this answer and we have this are we
can view this as a minimization problem
now and the explanation mechanism can be
used to solve that right and also online
some of these online are differentially
private mechanism actually achieve
similar error bound even comparing to
this explanation mechanism which
presumably is optimal if you've given
all these queries up front up to some
smaller factors
so it's very mysterious why they are
able to do so well okay so it depends on
individual settings so for example for
the the kind of counting queries that
I'm talking about say what's the
fraction of people have blue eyes and
stuff like that if the the queries are
actually kind of random enough then it's
no an expansion mechanism is optimal and
even for arbitrary counting query it's
conjecture that that's optimal I mean
it's not probably not really that well
accepted conjecture but we don't have a
counter example where exponential
mechanism it's not optimal with respect
to that kind of query Oh even offline
and suppose we need the conditional
efficiency is suicide then we don't have
a counter example where it is not
optimal it's imperfectly optimal
and the last open question is how about
mechanism design without payments so
what our result essentially says is that
differential privacy and truthfulness
are compatible if we are allowed to use
payments but on the other hand our
approach heavily relies on the use of
payments and in particular today we
shall show that the exponential
mechanism is not truthful without
payments for some problems and therefore
a interesting open problem is to
consider the literature of expansion
mechanism without payments and can we
still get exact truthfulness and
differential privacy at the same time
within this framework and that's all for
the talk and thank you for coming yep
yeah that's a very good question so
actually I our result do not implies
that there's a universal truthful
mechanism and to my knowledge I don't
know any result in that regard but that
might be an interesting thing to explore
as well
so the way you define deflection policy
is that if you defined in terms of some
vector V then it would be convicted a
one it would be straightforward
right right when you define potential
let me see that constraint is not equal
to this rule right
so what they also very mysterious is
that exponential mechanism once we do it
in characterized by this convex program
it's defined on a poem wise manner in
the sense that given any input data we
can compute the differential mechanism
kind of kind of maximizing this convex
program which completely determined by
the input data at that particular point
but differential privacy is defined on a
kind of comparing the distribution of
any two neighboring databases so there's
a mismatch here and it's not here why
with with with this mismatch exponential
mechanism is still being able to do so
well with respect to the definition of
differential privacy
exactly like a transition yeah so that's
like actually turns out to be the right
thing to do for online them I see where
you just maximize whatever whatever
you've seen so far yeah that's right
think of this imaginative update as
coming from yeah
yeah so that that's an interesting
interaction to explore as well so there
even without this result it has been
known that there's a close connection
between learning and differential
privacy and many of the result can be
translated from one literature to the
other so it would be interesting to see
what that means in learning and in some
sense its differential privacy is trying
to prevent someone from learning your
data so they are like dual problems in
some sense I mean of course in a hand
waving sense but we also kind of look
exactly yeah
so yeah it's it's worth looking at this
online learning literature and see what
that means
actually the aaron has a result showing
that any no regret learning algorithm
can be translated into a online query
release mechanism which is
differentially private and different
error bound can be a pain for different
no regret learning algorithm</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>