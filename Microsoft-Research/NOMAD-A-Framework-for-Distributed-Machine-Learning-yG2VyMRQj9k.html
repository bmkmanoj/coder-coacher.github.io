<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NOMAD : A Framework for Distributed Machine Learning | Coder Coacher - Coaching Coders</title><meta content="NOMAD : A Framework for Distributed Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NOMAD : A Framework for Distributed Machine Learning</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yG2VyMRQj9k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so what I want to talk about is no man
it is a play on words so you will see
why it is called nomad as the talk
progresses and I want to think of this
as some kind of a framework for
distributed machine learning okay but
majority of the talk i will be talking
about matrix completion which is a
specific problem and then later towards
the end of the talk i will show you some
ways of why i think it is it is a
framework rather than just a single idea
okay so what you're interested in doing
in machine learning is obviously you
want to build a model and that's what is
the holy grail but and then the way you
measure how well your model does is that
you look at something like a loss
function so they just think of the loss
function as some kind of a a
sophisticated discrepancy score and so
it tells you how what you what target
you expect and what target your model
actually predicts and the mean sort of
in my mind the main difference between
say just optimization versus machine
learning is in the fact that your model
was generalized on scene data right so
this is the whole Holy Grail that you
know you have seen some data you want to
build a model and that model must
predict well on some data that you have
not seen before and one way to achieve
this is by saying that I am going to
avoid overfitting and so I am going to
use some kind of regularization okay so
if you put these ingredients together
there is a there is this thing called
regular wise risk minimization that you
end up often doing and in fact you can
show that a vast majority of machine
learning methods can be viewed as sort
of doing regular risk minimization so
what hear what you are doing here is
that you have some M data points and
some corresponding labels so this is the
loss function so you average the loss
function on your data it's also called
the empirical risk and then you add some
kind of a penalty term which is the
regularizer and lambda is a parameter
that rates off between the regularizer
and the loss and so if you do the right
amount of trade-offs then you will get a
model the model parameters RW ok so this
is the sort of the general 30,000 feet
view of machine learning
and what I want to concentrate today is
is on this framework called Nomad which
is specifically for this kind of a
problem which is collaborative filtering
by the way I must mention that just feel
free to interrupt me ask me questions I
don't have to necessarily go through the
talk I am more interested in having a
discussion ok so just any point feel
free to interrupt me and so this is a so
the problem that I want to consider is
that of collaborative filtering so you
know I spent my summer at amazon and
amazon sells a few million items as you
all know and of course has a few million
users yes again most of you know and so
here are say users and here are items
and you may have some reviews from users
or you may have some purchase history
from users or you may have some
interaction of the users and items right
so you may maybe the user looked at this
item and then decided quickly went back
and so you can infer that the user does
not like this item or maybe he gave it
one star rating for this item and then
you know that he doesn't like this item
or maybe he gave a five star rating and
you know that he or she likes this item
a lot and so on and so forth right so
you have this large amount of
interaction data between users and items
again what you want to ideally predict
and what I amazon would out of predict
is what happens here on on items and
user pairs where you have not observed
in interactions can I predict ahead what
would happen how would these user item
pairs interact and if you are if the
item user / interaction is going to be
positive should i display this to the
user say maybe the next time you log
into amazon it says hey you know this
product i'm sure you will like it and
maybe you you know and if if you really
do show products that are relevant to
the users actually does increase user
satisfaction ok so the way to think
about this is that you have this large
matrix you observe some of the entries
of this matrix and you want to complete
the rest of the entrance okay based on
what you have observed and one way to
post this problem is to say that I'm
going to take this matrix which was
which is partially observed and i am
going to factorize it into two matrices
w NH such that if i take any rows w and
the corresponding column of h if i take
the dot product that gives me a
prediction of that corresponding entry
of a so in other words i want to
factorize a into w rh okay of course i
can set this up as an optimization
problem okay and one way to do this is
to say this is the set of all observed
in trees whatever I have observed what I
am saying what the optimization problem
says is that for all the observed
entries of the matrix if i take the I a
throw of W and the GF column of H if I
take a dot product and look at the
difference between that and what I
observe then the difference must be
small okay so that is the loss and then
of course i want to add an regularizer
in one way to regularize is to just
simply regularize the individual rows of
w alright and so if i add this regular
Eisen and there is a lambda parent so
again this is an example of a regular
fish minimization problem choose no i am
not so i'm just going to assume that you
have you have a god-given k in this case
so but yes the you know there are so so
i am not going to talk as much about the
modeling aspects of the problem as i am
going to concentrate on the optimization
aspects of the problem okay yeah usually
you choose something I mean you know you
choose something reasonably between ten
and hundreds something like that and
what I will shows that the algorithm
that we actually have is fairly
independent of K ok so it doesn't really
matter what k you select as long as the
case reasonable it will give you an
answer within this in similar time
pounds okay so any other questions so
far good ok so now so let's enter into
the more interesting part of the talk so
this is all about background so now one
way I mean of course there are many
different ways in which you could solve
this problem but here is one possible
in which you could solve this problem is
kind of popular is you could use a
stochastic approximation to the
objective function and then try to solve
a you know do a stochastic gradient
descent so the way you do this is that
you would take your objective function
you would approximate it with this with
this one step objective function so here
if you remember there was a summation in
the objective function here this is a
summation over all observed entries and
now you simply say I'm is going to not
do the summation and just going to look
at this one step objective function okay
now i am going to pretend that that is
all that I know about or that is all
that I care about now if you if you
pretend that way then you can easily
compute the gradient of this approximate
objective function so the gradient looks
like this this is high school math you
can do this so this is the gradient with
respect to W and there is a gradient
with respect to H ok and now you can do
a stochastic update ok so this
stochastic update simply says i am going
to update w by using the previous the IH
row of w by using the ayah throw of the
previous value minus ETA times the
gradient ok this looks very a sort of
unassuming and there is nothing fancy
going on here but i want to point you to
one particular very very important
observation in the slide ok so the main
observation here is that when I want to
update the ayaat row of W and I want to
update it with respect to a IJ the igf
observed entry of the matrix then I only
need three pieces of information ok so
what are the three pieces of information
I need to know the previous value of W
so the iro I need WI I need HJ ok so the
corresponding row in H and I need to
know the value of a IJ ok so if I have
these three pieces of information then I
am done I can do this update ok what is
remarkable about this this update is
that this does not depend on anything
else it does not depend on the state of
what is happening to the rest of the w's
it does not happen it does not depend on
what is happening to the rest of the H
as long as I have these three pieces of
information WI a i j and HJ I can do
this update ok so it's sort of very
nicely factorizing in some sense so then
the obvious question is it if you have a
very large problem ok and say if your
Amazon and you have the Amazon scale
problem or your own IMDb which Amazon
does and again you have a few million
users and a few million well not a few
million but a few hundreds of thousands
of movies right and then you want to
factorize the ratings from there then
how do you do this ok here i want to
point out because there is this question
earlier about ok what is big data and
you know why should we care about big
data type question so here really it is
important to use all features it is very
important to use all purchase history
from all the all the users you do see
again that's number one number two it's
not clear in this setting how to do
subsampling and what does it mean to
subsample i have a matrix i have data
which is on a matrix and how do i do any
kind of subsampling on this matrix night
and even if i do subsampling what sort
of information am I am I losing in that
process ok so therefore you do want to
you do want to use the entire data so
all the results that I'm going to
present our own public data sets and
even there the data is very sparse and
in general you would you would expect
that the data is extremely sparse
because you know obviously if if you are
selling two million items you know users
are not going to look at two million
items it will be very small I mean I I I
don't know the I don't even remember the
exact numbers on IMDb but it is fairly
sparse yep
you can enforce other regular risers so
here right now okay that is unless a
good point i didn't i didn't mention
this but what i am going to talk about
allows you to change this loss function
and this regularizer to other regular
risers so that's that's fairly weak but
we are going to do the simplest case
because i just want to convey rate basic
ideas okay other questions okay good so
now if you want to sort of distribute
this computation across multiple
machines then how are you going to do
this here is a is a potential
possibility of distributing these
updates across multiple machines and
this is a paper from kdd 2011 the idea
here by ghulam and the idea here is that
you take your entire matrix okay of
observed ratings and you divide them say
here in this case into four machines
okay so this this basically the one
machine gets the entire red block the
second machine here Center green block
the third one you see blue and then the
gray one and so on okay so each machine
owns this stripe of the data and so on
and the crosses represent nonzero entry
okay so which means that all this part
of the matrix is stored in one machine
this part of the matrix is stored in the
other machine and these are all the
nonzero entries in this part okay and
what you do is you start at your
parameters also distributed in this way
so what you do work and so each machine
here say this gene will get this block
of W parameters okay and it will get the
corresponding block here of the H
parameters okay so this machine has this
w and that edge the third machine has
this the set of w's and that sort of H
and so on and so forth okay now what
this means is that for any entry a IJ
here you have enough information to
perform stochastic gradient updates
remember I told you in the previous line
that to do an update to
WI you only need to know a IJ WI and HJ
ok so which means that if I choose any
nonzero entry here so let us say if I
choose this entry in order to perform an
update to the corresponding WI and
corresponding HJ I have all the
information on this machine ok in the
same thing goes so any nonzero entry in
this part of the matrix I can perform an
update on any nonzero entry on this part
of the matrix in the third machine I can
perform an update and so on and so forth
ok now what the scheme does is initially
does this kind of a decomposition of the
parameters sensitive different machines
perform an update and once the update is
once you're sort of sample sufficiently
in each part you synchronize and you
communicate so what is communication do
you exchange blocks of H ok so if you
pay attention here there was the red
green blue and brown and now they have
exchanged blocks right so now this
machine has brown and this one has red
and this one is green and sorry ok so
what you do is that initially you start
with this assignment of parameters right
so this guy so let's say let's be very
concrete and I says if the first machine
gets the first four columns of H the
second machine gets the next four and so
on and so forth after you have done your
updates you synchronize and you
communicate so how do you communicate
now the first machine will send if the
columns of H to the second guy the
second guy will choose send it to the
third one and so on and so forth ok so
now the second the first machine has all
the information that it needs to perform
update with respect to these set of
nonzero entries and so on and so forth
of course you want to do it see
sequentially so you do not do machine
one sense to machine to machine to sense
to machine three you just do it randomly
so you pick a random machine and then
you send it to sorry
yeah but the communication is not
stochastic right so the communication
you you have a pattern and you you
update yep sorry now we are doing
updating both w's and h's right so they
are happening simultaneously and then
you you know you keep on updating both
W&amp;amp;H so basically the first machine will
update this part of h and this part of w
and then again it will you will
synchronize you will communicate ok now
the problem with this synchronize
communicate synchronous communicate
paradigm which is very popular is that
or maybe I should first talk about why
this is a good paradigm first for this
is a good paradigm because the updates
are decoupled it's sort of you know we
need observation is that you to update
WI and HJ you just need a IJ wih a these
three piece of information or needed so
the updates are decoupled that's a nice
observation so it's very easy to
paralyze conceptual is pretty good and
because of this synchronized communicate
synchronize communicate pattern you can
easily do a MapReduce right so all you
have to do is put it on your favorite
now produced platform Hadoop MPI but
what not and you are done but what is
the bad part about this so in my mind
one of the sort of biggest drawbacks
here is that communication and
computation are interleaved so when you
are doing communication you are not
computing and when you are computing you
are not communicate so if you look at
the network if you look at the load on
the hardware you are having the cpu the
network is idle you are hammering the
network the cpu is essentially i do ok
so the one question is that can we keep
both of them simultaneously busy and
this is one but also there is a bigger
meta question probably we will do some
of this discussion when we have the
panel discussion in the afternoon but I
just want to leave this thought with you
is that in some sense MapReduce is
essentially the wrong paradigm for
machine learning for the following
reason as the data size grows ok you
want to add more and more machines
so that you can solve a larger and
larger problem okay what happens in
MapReduce is that as the number of
machines grows the burden of
synchronization grows okay and
eventually what will happen is that you
will get a point where you will be
spending all your time on
synchronization and communication and
not so much on computation and this is
kind of counterproductive because you
really are doing the wrong thing as as
the data size grows you should be
spending your time on computing not on
communicating now I am talking about
MapReduce so okay what I am talking
about is is this notion that when you
communicate okay and you have to
synchronize ok so I am talking about
synchronous bulk synchronization at
every step that MapReduce requires so I
am talking about MapReduce and of course
the concrete implementation Hadoop of
course suffers from it because
so I mean there are clearly engineering
solutions to this whole to this whole
problem but the the argument that I am
making is that synchronization a kind of
counterproductive as you go to larger
and larger number of machines because
first of all you have to wait for all
the machines to finish their jobs so
there is this there's this thing called
because of the last reducer the second
thing is that you spend a lot of time on
synchronizing and so your network costs
go higher and so it's sort of as the
problem size increases you're spending
all your time on the network which is
really bad thing
yeah but I mean a memory system a memory
bus on your machine is significantly
faster than any interconnect that you
can get even on an infinity band on a
InfiniBand yeah yeah yeah yeah yeah yeah
no I mean I am saying that you can have
the best of both worlds so we will talk
about I'll talk about the framework that
that we are developing and you can
actually get both for I mean we don't
our current implementation does not
support fault tolerance but that is not
because the algorithm cannot be made
fault tolerant it is because we just do
not have the bandwidth to do the photo
I'll show you that actually a CJ limb
has a very nice paper on that and we'll
show that actually we do better than
that okay and again coming back to your
comment about the memory subsystem we do
have to design it carefully right so
that the you know that that you see it's
very important to ensure that you your
your updates are local you are able to
take advantage of memory locality and
cache misses are reduced all that stuff
is important and it does make a
difference and we will talk about some
of that was there a commencement okay
good i am glad that it's it's starting
your discussion okay so we'll have more
in the panel discussion okay so our
answer to this problem is no matter how
does no mind work it has the same kind
of partitioning as before okay so you
you partition your data in exactly the
same way so you stripe your data and
these shots of the data are fed to each
machine and then you shall your
parameters in exactly as before okay and
you start you know you start off with
this set of parameters okay but now
comes the difference okay so each
machine starts so that it says each my
this first machine starts updating all
these parameters the second machine
starts updating these parameters and so
on so everything is as before but now we
make this change at some point randomly
in time okay machine one will roughly
pick a column of H okay and it will
randomly pick a neighbor and it will
decide to transmit this column of h2
this neighbor okay so in this case
machine one will say I am going to pick
the second column and i am going to
transmit to the fourth machine okay
completely asynchronously no
synchronization okay and the fourth
machine when it receives that column now
has enough information such that any
nonzero entry in this part of the matrix
it is able to perform updates with
respect to that okay so now the fourth
machine can perform updates on any of
these parts of the matrix
sorry now there is a queue so you there
is a there is a receiving thread and the
receiving thread keeps on receiving of
parameter updates these parameters are
usually small so if you have a scalable
a locator you can you can handle you
know so we do pre-allocate the blocks
and then we use a scalable allocate to
inside the box yeah so big we can do
that so so one of the things that that
that I'll address that question okay yes
holy horses holy also we are getting
there okay so there is there's more I
mean I am giving you this interest
vanilla scheme and there is more to it
okay other questions so so the is a
basic scheme kind of clear that each
machine is working on a set of party on
a set of parameters and a part of the
matrix you randomly take one of the
parameters or wonder of the parameter
and you pick a random matrix a random
machine and you send the parameter over
to that machine you communicate okay and
on each machine whichever set of
parameters that you own you perform
updates on them okay left sorry yeah so
they so you have threads on each machine
in the threads are working on the
parameters that you own and then they
come here and then you randomly pick a
parameter and you transmit okay now when
I say when I say random okay there are
many things that you can do on top of
this okay so the first thing that you
can do and this is what we do is you can
send a payload along with each message
okay so whenever you receive a column
from some other machine you receive a
payload ok so the payload can for
instance
include things like how busy is this
machine right or how how much you know
say memory capacity it still has or how
much you know how many updates has it
performed on on each set of data that it
has now you can use that information to
tailor your random picking of columns so
for instance a very simple example of
doing it randomly but yet respecting the
load of each machine is to say that if
the number of parameter updates or if
the if the say the capacity of the
machine is low then the probability that
i will select that machine as a random
neighbor to send my columns to is low so
this is how you do dynamic load
balancing so only flying along with the
messages you send a payload the payload
is usually very small you can you can
pack an integer or two along with this
and then the other machine d decodes the
message and then you get to know how
much load there is so what this means is
that if you have machines with different
capacities so you don't have to have a
server rack right of all with identical
machines you can have machines with
different capacities and you can
actually decide on the fly based on
their capacity or their load you can
load balance okay so that is one thing
sorry
ah can you make that more explicit you
can do either right we know pushing just
because the because you are anyway
communicating so you might as well add a
payload to it yeah yeah that's the
motivation that you know I am done with
my parameter now I have to figure out
where to push it onto the network right
so there is a so there is a both models
have their advantages and disadvantages
with the pusher model the disadvantage
is that in some sense you have slightly
older information because you know the
Machine tells you that my current load
is this right and you use that
information to figure out how much load
I mean you don't know exactly what the
load right now is you know probably a
few microseconds before so in the
meanwhile if all the other machines
decide to send data to this guy just
because he is not loaded as much you
will you know you can have some of those
networks okay but it no no because each
see this parameter when when this comes
every time there is a fixed hood so
there is not that is not the payload is
not scaling with the number of machines
so I am just going to push to you the
information about here is a parameter
for you and here is my load so then you
just you all you have to do is you have
to say okay now I know I have the latest
information from this machine I know
what his load is of course there is a
little bit of delay in the load but
that's it okay so there is no there is
it does not not scale with the it scales
pretty well
okay okay okay so so so there is a
there's another okay so there's there's
a lot of things that we do under under
the hood one of the things that we do
under the hood is that you accumulate
messages so you accumulate messages to
get so usually if say okay is 100 n plus
an integer so it's like about like if
you accumulate about 10 to 15 messages
you will have enough on the buffer that
it makes it worthwhile to communicate
yeah but then it will come it will come
back I mean the columns are circulating
so now sort of i want to tell you why
it's called nomad kind of you have been
paying attention it it is obvious
because these parameters are nomadic
they are wandering around in the network
right so the w's are static the H is
nomadic okay so they wander around
nomadic Lee in the network now I am I am
fine i don't i don't have to yeah i will
end whenever I want and whenever
everybody is yeah and when Bruce's
so if a machine falls behind you get
this information in the payload right so
you get this information in the payload
saying that I am overloaded right now
don't send me messages and then the
other machines will respect that so
there are other ways to again we do not
do it but the algorithm does not
preclude you from actually dynamically
load balancing the data as well so you
could at some point of time say that
okay this machine is overloaded so I
didn't talk about the fault tolerant
aspect so there are two parts to that so
the fault tolerance aspect to this is
that you actually can have I can take
this offline we do not have an
implementation yet but it is possible to
do fault tolerance and also it is
possible to load balance on the fly so
there is nothing which prevents you from
load balancing the data but you do not
do you do not want to do it in general
because you want to take the computation
to the data you don't want to take the
data to the computation right the other
way round if you have a terabyte of data
moving it even inside your data center
is hard
now this is just a an array of integers
of the number of machines right if you
do not even have if you don't even have
like an array I mean let's say what's
your number of machines going to grow to
10,000 right i mean if you have a
reasonable data center right so you have
an array of 10,000 integers so that's
exactly why you don't do it you don't do
it by looking at the machine so you
don't deterministically send your
message to the machine which has the
lowest pill which has the highest
capacity you you have a sampling scheme
but the sampling scheme is weighted
towards favoring that machine but that
does not mean that every that is exactly
why you prove you know so there is a
higher probability that if I have
capacity I will receive messages but
does not mean that everybody will jump
on me and give me messages if I am if I
have capacity no I you absolutely do not
want to centralize you absolutely don't
want to centralize you will not see the
reason your own decentralized is you
don't want a single point of failure
okay
no the initialization is here right so
this is the initialization so it's it's
random right so there is a so there is a
the DWS and the h's or are there is you
sample from a random distribution no so
okay there is a so I'm assuming that we
are I mean we are not doing again in our
code we do not do network we do not do
discovery so we assume that you you are
aware so there is you are aware of the
topology of the network right so but
there is nothing which prevents us from
adding a discovery layer on top of it
right i mean you could go through a you
could go to something like zookeeper and
sort of say okay tell me what's
happening sure
so I am after the following I am after
having not not having a single point of
failure I am after not having a
synchronous communication and then all
the risk that I talked about discovery
fault tolerance moving of data all of
this can be added on but we don't do it
because we are not systems people I mean
be me sort of you know I mean this it's
me energy and a couple of our students
right so we only have that much capacity
it's okay if you send it back because
you stick you there is still information
because sorry yeah because you you have
changed the column right so you you're
constantly changing the column so even
if there is a small probability that i
send it back to you you still have new
information I mean there are other
things that you can do again the payload
you can make it longer and say who has
not received this column all those
things are possible right i mean you
know but basically the you know this is
bread and butter you can add everything
on top of it you can make a whole meal
out of it but you know I hope and this
is one of our hopes we have the the code
is on bitbucket will release it you know
will I mean there's a there's a version
that is already released but the code is
on bitbucket you want a hack on it
welcome we'll get you give you a bit
bucket axis and you are happy I mean
you're you're more than welcome to hack
on record
sorry are you talking on the
distributing the parameters already
talking aboot in the data yeah here so
here the rose the RO parameters stay
constant and the column parameters
migrate I in does not matter it's
symmetric it is completely symmetric you
could have you could have migrated the
columns the you could have kept the row
the columns symmetric static and the
rose the reason we do it for matrix
factorization is usually the number of
row parameters is much larger than the
number of column parameters because the
number of movies is in hundreds of
thousands number of users is in millions
so questions okay so where are we with
things okay so now I just want to
quickly also talk a little bit about
other things that people have been doing
was that required was there a question
there okay so i also want to quickly
mention what other possibilities are
other ways in which you could have
solved the same problem of interleaving
community of doing simultaneous
computation and communication so here is
one proposal called DSG d + + this is
also by a gambler's group and this is
very thin paper in ic diem and hear what
you do is that you you instead of
exchanging an entire block you divide
the block into to you are doing updates
on one half of the block while you are
communicating the other half of the
block right and then you do this one of
the process and this is that sort of you
know if you if you are doing real
parallelism you really want to break
down your units of work into the
smallest units of work that you can so
that you can do better tasks feeling and
better sort of scheduling on on your
machines here because your tasks are
sort of still blocked size there are
some issues associated with it I can
talk to you more about this offline on
why this still does not I mean it's
resolve some of the problems but not all
the problems of course it's still
require synchronization of
every block is update okay so this is
the second scheme to solve this this is
over partitioning so this is from CJ
lens group versus rexius best paper
award yeah I have lost track of your
best paper of words by now but I yeah
this is this is the best paper award a
Texas and this is this does over
partitioning so basically you take your
data you put you over partition it and
then there is a thread manager so the
thread manager allocates work to each
thread and then you know what what you
do is that whenever a thread is finished
with a block it goes back to the thread
manager and says give me a different log
because they are more blocks then there
are threads there is always work for
each thread okay so there that is the
way you you do this but it is not
entirely clear I mean maybe maybe you
have ideas on how to extend it to the
multi machine case but obvious is not
immediately obvious how to do this you
know you know multi machine kills sorry
but but there is a I mean I don't know
how to how to not have a central thread
manager right so you need to have yeah
but then you need to the block still
need to be there has to be somebody who
allocates the blocks and there has still
have to be some scheduling okay block I
say so yeah I mean then that so there is
sort of distributed allocation yeah yeah
yeah so there is actually our fault
tolerant scheme is based on the same
idea that there is a owner for each
column and if you do not see the column
back in your network you revive the
column again and then you circulate a
second version of the column so you can
do versioning you can do all kinds of
things in our scheme too okay n.n sort
of this is what Nomad does it sort of
finds the finest granularity of work
that you can find in this case which is
single column and then it circulates
these columns okay and of course I
talked about load balancing and so on
okay so that's a that's a question that
I think we have to address we haven't
started addressing it yet one of the
issues that we have is that in sort of
the decay the first step towards
convergence kinetics is what kind of
assumptions are reasonable to make in
order to come up with a convergence
proof right so should I assume that each
machine will communicate with each other
in a fixed and these are just some ideas
on how to do this we haven't I have some
possible indications on how to do it but
now we don't have a conversation
scare-itage sorry I mean you can only do
expected yes but the a synchronicity is
because the the problem is because you
are selecting a random neighbor right so
so there are issues I mean it's not
straight it's not a straightforward
algorithm to analyze to simply say okay
you know here is here is how the
communication pattern flows and that
that's how exactly where I know exactly
where each piece of information is so
it's much easier to for instance it's
much easier to analyze DHCD as compared
to no matter
actually you can add add users to the
system there is nothing which prevents
you from doing that but then you'll have
to solve this problem of discovering so
you can add a new machine to this system
but then you let inject the information
about the existence of that machine into
other machines so that they know that
this is another candidate for sending
okay so i will show you some experiments
and i am very excited by the experiments
okay so this is just three datasets
publicly available ok so there's Netflix
which is which is been beaten to death
and then there's Yahoo music which has
about 250 2 million non zeros and then
there's huge wiki which has 50 million
rows forty thousand columns and it has
two point seven gigabytes 2.7 Giga non
zeros okay yeah and you know the
versions are available but unfortunately
Netflix withdrew the vid Ruby actual
data set okay so we're using some
illegals data fit but okay I one video
okay so here is what happens on a single
machine so this is FBI CD star star
which is the over partitioning scheme
ccd plus plus is I caught a is a column
coordinate descent solver that was
developed by Inderjit group and then
there is no mad our algorithm and you
can see that you know we start reducing
the test rmse extremely fast even on a
single machine even with over
partitioning and and the other thing i
must admit i must add here is it if f is
it is start strong does a lot of tricks
by using the the floating point
operation so they pack for operations
into one and they use the SSE
instructions set of the of the intel
machines to do their updates much faster
we do not do any of those tricks and
this is plain simple vanilla c++ code
but they are not orthogonal they are
actually parallel right you could there
is nothing which says that you cannot do
the initial clustering and then still do
the communication leeway we are doing it
yes but say no but yeah but your data is
distributed if your data is distributed
your data is not fitting on a single
machine you have no choice no you never
want to move the data if if you can if
you can if you can you never want to
move around it because you always want
to take your communication you know your
computation to the data okay so this is
what happens on these on on netflix on
yahoo and on huge bakim but i want to
show you something that's very
interesting okay so this is sort of to
show that we do reduce the test
enormously reasonably fast but here is
what happens when you look at scaling as
a function of number of course okay so
you keep on increasing the number of
course and you want to see how the
scaling goes and instead of plotting one
of these so you know one of these single
line plots where you show okay when you
have four course what is the speed up
when you have eight course what is the
speed up and so on we decide to give you
the full picture of what is happening so
here you take the seconds into number of
course in other words this is the total
computation that you're expending right
so if you spend one second on for course
and this is like four okay so I and if
you spend four seconds on one
the same equivalent and you can see if
you're you're scaling linearly then this
line should more or less overlap okay of
course which you can see happens here in
netflix now something interesting
happens in the other data sets okay as
the number of course increases you
actually see that you speed up so this
is one for course and this is 32 codes
okay the reason for this is that your
your data is now becoming divided into
finer and finer grains and you're
exchanging information much faster than
before okay so because of that you get
more you you are updating with respect
to fresher information and actually you
speed up okay so this is I call this you
get you get this the super linear
speed-up for the price of a linear
computation okay so as you have for of
course go to 32 course you actually go
do more than eight times faster they
actually converge something like 10-15
times faster okay and this is something
that's a cute feature of this algorithm
of course this doesn't happen so there
is also i mean there's always no free
lunch right so if you keep on deeps
further and further subdividing the data
then the number of units becomes so
small that you don't get any information
from them so that is what you see here
in huge Vicki so starting with four you
go to 28 you're still okay but when you
go to like 30 then you start actually
slowing down a little bit okay this is
what happens on a hpc cluster okay and
again you know we are significantly
faster but i will show you more sort of
interesting results so the same thing on
hpc but ok so this is sort of this
should interest the systems people in
the audience okay so this is AWS amazon
web service sort of their one of the
crappiest machines that you can you can
get okay interconnect is is bad it's
like less than 1 gigabyte per second
throughput is not sustained properly and
anyway you know and therefore course
okay
and the way we run these algorithms is
because no man acquires one core one
thread for receiving and one thread for
sending we only use two threads for
computation okay two threads are
constantly computing two threads are
constantly involved in network all the
other algorithms dsg D&amp;amp;C CD + + all four
cores are computing okay so they haven't
much bigger advantage than us just
because they are doing twice as many
updates as us ok and look at the results
this is crappy commodity hardware ok not
on hpc cluster with a beautiful
interconnect infinity band connect ok so
we just took two threads to coerce
versus every other algorithm using for
course so here is the what I think of
this as an interesting other plot which
is you're always interested in knowing
what happens if suppose as my data set
grows if I keep the amount of
computation per machine fixed but I
increase the number of data sets sorry
the number of machines ok so they come
the amount of work per machine is fixed
but the number of machines grows then
how do i how do i scale and this is this
is scaling behavior this is sort of what
I think is really what you want in you
know when you are when you're deploying
such systems you want to know I have 10
times more data here are 10 times more
machines tell me how do i scale now
right and you see the ok so this is for
four machines the same now that you have
four times more data so now you have 16
machines and then you have eight times
more data and now you're 32 machines and
you see why synchronization is bad ok
give both the HDD and C C++ have to
synchronize we do not have to
synchronize and this is just to show you
that we are family independent of the
regularization parameter and then
there's the other set of experiments to
show you that whatever value of there
was a question before about the value of
K doesn't
really matter in terms of convergence of
our algorithm you still get I mean of
course there is some difference right
between k10 200 but more or less yo you
know your convergence guarantees are not
affected yeah everything is the same
yeah except that you have to remember
that because it is a non non convex
objective function optimizers will go to
different solutions yep yeah yeah yeah
yeah it's a very very so this is where
something that's very important actually
these experiments are very carefully
done extremely carefully done to ensure
that they all start from the same
starting point they all have the same
you know they all have the same sort of
objective function they're scaling I
mean so everything is held constant like
it's yes because it's it's non convex so
the objective function is non convex so
you will end up with different solution
and then I just want to make one more
comment which is that we spent a lot of
time on GraphLab and we I think between
energy its students my students and asks
we spend maybe like a month of our man
month of effort and we couldn't get
graph love to converge on this problem
eventually we did get it to converge but
the solution was really really bad and
the time was so bad that we couldn't
plot it on the same plot we have talked
to the top two we I mean we have sent a
lot of e-mails back and forth I think
exchange 60 emails the graph lab guys
and it's not entirely clear I mean they
are given them access to our clusters
they have looked at it and you know yes
yeah so it's the same algorithm same
data set same hardware
they have a different notion of a
synchronicity they actually acquire
network locks we can talk about this
other line okay and just two minutes can
I ok so i will just show you why i think
of this as a framework rather than just
as a matrix factorization one of things
okay so question is that can you extend
this to a larger class of problems okay
and here is the general matrix at the
regular risk minimization problem and
you're going to do a series of
transformations and i'll show you that
you can rewrite this problem in the same
bilinear form that you solve for matrix
factorization okay so the first thing I
am going to do is introduce a new
parameter you I ok and then now i can
write it this way ok now i take this
constrain into the objective function by
using an alpha I ok which is a dual
variable so I had to do a min with
respect to my primal and Max with
respect to my duel because the problem
is convex usually if they if doing
regular is minimization say logistic
regression SVM type things I can
exchange the max in the main ok so this
is fine now i can push the minimization
with respect to you inside case i get
this now all i have to do is to realize
that this is nothing but the dual
differential dual of l of the laws and i
get this now i get a saddle point
problem and now look at the structure of
this idle point problem and you will
realize that it has it has exactly the
form that you that you need basically it
has the form that each term see this
term depends only on w ok that term
depends only on Alpha and the
interaction term is only a linear
product on Alpha and w ok so which means
that if i look at this stochastic so i
can rewrite it in this way and this
stochastic gradient look like this now
when I need to update a coordinate of W
all I need is a coordinate of my data
and the corresponding alpha ok and I get
exactly the same format and I can do
regular a Swiss minimization and it
turns out that we have some preliminary
it does not converge as fast as a CD or
dual coordinate descent we are working
on it but it does converge and we do get
we do get reasonable performance it is
not very good yet they're still working
on tuning parameters but I just want to
give you a feel and that is why I think
that this is a much more general
framework sort of potentially new way of
doing a lot of machine learning
algorithm so this is logistic regression
using the same setting ok so the code so
no man is available on archive there's a
code the code is available from Young's
home page and the paper and cold far erm
hopefully coming soon these are my
co-authors and see Inderjit verse was
here and then the sky con un is my
student and shank foo and show are in
the deep students &amp;amp; Anchin matsushima is
a assistant professor at the University
of Tokyo who never see you can always
respond to my skype messages any time of
the day or night and if you have any
questions you like to take them c++ MPI
yeah roughly maybe thousand thousand
five hundred it's not a lot
yeah we could I mean we just it just
makes life easier because when you
receive a column then it goes into a
queue and we can manage the queue
ourselves so that's that's all I mean
it's just you could call it laziness in
programming with mpi the problem is that
you you would have to receive a
notification and then a thread has to go
in and put you know pull the d data from
the buffer here you just have a you have
a thread that is dedicated to the
swimming and sending other questions
yeah okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>