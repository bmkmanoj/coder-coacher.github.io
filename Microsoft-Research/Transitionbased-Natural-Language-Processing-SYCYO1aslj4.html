<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Transition-based Natural Language Processing | Coder Coacher - Coaching Coders</title><meta content="Transition-based Natural Language Processing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Transition-based Natural Language Processing</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SYCYO1aslj4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so welcome everyone it's my pleasure to
introduce mega biased arose who's
visiting he's normally a professor in
Barcelona that is currently doing a
one-year stint at Carnegie Mellon where
he's been working with a number of
really interesting people there and
pursuing directions in introducing
structure into recurrent Network neural
networks and crew including like stock
LS TMS has been doing some very
influential and effective research in
parsing and now beyond so with that
Miguel please okay thank you Chris for
the introduction so today I'm going to
talk about the transition base natural
language processing you will understand
what it is in an hour so first you might
be wondering who is Miguel so basically
I did my PhD in computers University of
Madrid Spain then a dude in this period
i also started during our loan research
stay in solid university working under
the supervision of jackie nearing which
i learn all the wonders of transition
base parsing models then i move toward
09 which i have work with people in
generation and also dependency parsing
as well then a i did also research stay
singapore University of Technology a
design in which I also work
infrastructure chief reduce parsing and
also after that I moved to Carnegie
Mellon University in which I have work
is mainly with noy smith and chris dyer
in person and other problems so this
talk is about linguistic structure
prediction English you basically have
sentences like like this one mark Watney
visited Mars and then you want to infer
the linguistic structure the linguistic
structure can be given in many different
ways so one way it could be for instance
named entity recognition in which you
have a sentences like Mark Watney
visited Mars and then you struck that
Mark Watney all all this chunk is a
person and also that Mars is a location
so there are all the things you can do
for linguistic structure prediction for
instance dependency parsing in which you
get a
dependency relations between wars and
these dependencies relations are binary
dependency relations and basically you
get like a some kind of a dependency
base syntactical presentation of
sentences so in this talk I'm talking
about a supervised approach to
linguistic structure prediction yes in
case so my finger in doing unsupervised
I'm talking about supervised approach to
it in which I have a driven with gold
structures I have basically a pairs with
sentence and the gothic structure and
the challenge is basically given a new
sentence we want to infer the structure
of this sentence ok so the I say that
this is a challenge because it is
actually a challenge mainly if you want
to do parsing of a new sentence this
sentence can be a something completely
new for instance or something a center
that has never been written before and
the parcel has to be able to parse this
sentence in the proper way and we expect
to be expected to do that to do it like
this so the general approach basically
the main approach for parsing is what we
call grandma is passing or graphics
approach to linguistic structure
prediction not only parsing because his
talk is not only about parsing so in
this idea you basically build a graph
connecting all wars you score all the
edges with some kind of machine learning
model that says how likely is Diego and
place a to be attached together and then
we search the highest-scoring tree with
some kind of graph is a algorithm and
then we produce the dependency tree or
any other is a basically linguistic
structure for this sentence so the graph
based approaches have some problems so
they they problem mainly the speed and
complexity and they hard infinity I only
things that you have to implement in
order to do them but of course they are
also very powerful in the sense that
they present a global solution to the
problem and of course this makes them
very attractive for basically people
that likes to to incorporate all the
information they can into their modest
the question of this talk is whether we
can model linguistic structure
prediction problems such as NER or
parsing as a sequence of actions and we
can do it at the same level of a curious
yes we can do it for graph ace
approaches okay so what is a transition
based approach for natural language
processing so basically what we have is
a transition system or are an ass track
state machine so we process the sentence
in a sequential way from left to right
and we basically have normally a buffer
that keeps the war that we want to parse
or we want to get the structure front
and we may use different data structures
normally you use a stack and a buffer
like in civ reduce a compilers of
serious passing as you may familiar with
and of course in each step we have an
algorithm that has to select the best
operation or basically the best class or
the best action to do given the parson
estate and we use normally use a
classifier and you can pick the best
classifier you want to do this and
basically people have tried this in many
different ways so they have a lot of
promise this transition based models
because they are facts fast am very
flexible for different kinds of problems
basically they are linear in the length
of the sentence because you take an
action for each word and you end you've
basically finished whenever you you are
done with the sentence and this makes
them very attractive and they are also
flexible in the sense because you can
come up with any kind of set of actions
in order to come up with your problem of
course the present other challenges
mainly a feature engineering is hard
because you need to define the features
over the start of the buffer and you
need to do it in a proper way and you
normally need a linguistic intuition in
order to do that and also the local
Optima problem since you're making the
decisions the parser can come up in some
kind of grown decision at some point and
this can be problem so in this talk I
basically going to try to convince you
that we can actually do feature
engineering much a much better by using
a different thing and also that we can
also fix this local optimum problem by
using a bit better classifier so in this
talk and going to talk about powerful
sequential stack based approaches for
natural language processing or basically
transition based natural language
processing all of them are based on
transition based approaches with a stack
list
this is a cool model will be presented
in a CL 2015 so I'm going to talk about
the transition base dependency parser
I'm going to talk about how we can do
extensions for semantic role labeling so
doing joint syntactic and semantic
parsing I'm going to talk about how we
can do at how we can use also these
powerful syntactic models for language
modeling and generation and I'm going to
talk about the transition base any our
system that was a set to knuckle
yesterday okay so let's be at the
beginning since this is basically how
people started to eat transition based
approaches to NLP i'm going to talk
about parsing okay so what is dependency
parsing so basically dependency parsing
is the syntactic parsing of natural
language in which we have a dependency
base in tactics representations of
language okay this has a long tradition
this cryptic and theoretical linguistics
and is right now very popular I will say
more than increasingly popular very
popular in computational linguistics
because people well we have a lot of
three ones people have half work a lot
in parsing and we have a lot of like a
bass lines to beat in dependency parsing
and of course the main reason of this is
because dependency parsing is very
useful for downstream applications such
as question answering or a match in
translation or many other processing
which you need syntax in order to do a
tube to do these systems better so in a
formal fashion a dependency graph for a
sentence such as this one is basically a
directed graph in which you have a set
of nodes represent the tokens or
basically tokens in there basically wars
in your sentence and then you have a set
of acts that are directed arcs that
represent the dependencies so what
dependency have dependent on ahead and
these dependencies can be labeled with a
dependency type so basically you know
that one is the subject of visitors and
you have it level with this subject
dependency so I'm going to talk about
about the new srk standard rocky need a
circus Thunder transition system in
which we have three different actions so
you get shift which will in this ARCA
standard parser you have
buffer and a stack and then you get
three different actions so the first
action action is to do a shift which is
to take our word from the buffer and put
in the stack and then whenever you have
a couple of words in the stack you can
create an are from left to right or to
create an art from right to left and
this is why you have a couple of actions
that are called left park and right ack
and if you want to pass nombre tea trees
such as this one in which you have
basically closed images and there is no
way to write to draw these three without
crossing edges you can also incorporate
a flop axiom which is which will call
swap in order to do reordering I do by
using that and a stack you can do any
kind of really new one and you can also
pass any non pretty tree for the
sentence and still even if you include
the swap action the parser is still fast
a linear in the length of the sentence
ok so I'm going to give you an example
of how the newest artist and I'll
transition system works for the sentence
that I'm running all over the all over
the presentation so basically the
initialization will be which you have a
buffer full of words basically the words
that we are parsing in this sentence we
have a stack which is empty and the
transition here will be like they like
the initialization so the first thing to
do here since we have an empty stack
will be to click through a sieve action
taking the war a mark and put it into
the stack and now whenever we have only
one word into the stack in the arca
standard parser the only thing we can do
is again to do another ship action so we
could take the war one knee and we will
put it into the stack and whenever we
have a couple of words in the stack we
can decide whether to ship another word
from the buffer to the stack or to
create an arc from mark to one knee or
another from one knee to mark in this
case I guess the best thing to do is to
do basically the left arc between mark
and one knee and whenever we do this we
basically remove this work from the
stack now since we only have one word
into the stack we will get another ship
action and we'll have visited in to
stack and again of course we can see if
Mars or we could create an arc between
these two words so the parser or the
classifier
the size of the best thing to do is to
create this arc between these two words
and then it keeps going shifting the war
and march into the stack and finally it
will create the arc between visited too
much so this is how the transition based
dependency parser works and you can see
you have all the actions here and the
number of operations is actually linear
in the length of the sentence because
you have eight actions and forwards it's
always twice in the arcade sona parser
so the question is given a person estate
the classifier has to select the best
action so how do we do that well we need
features and the features as basically
things that we define over the stack and
the buffer so do my PhD I did this
system mod optimizer so I know very well
about how to do a feature engineering
for transition based parsers so
basically people define features over
the stock on the buffer over a fixed
window features let's say you know ki
take the first couple of words of the
stack the first couple of wars of the
waffle and I define like singleton
features of a part of a speech war
phones etc etc and by using that I can
feel I can fit fit everything all these
features into our classifier and the
classifier will make predictions so this
is how a feature set it looks like
normally like a standard feature set or
the default feature set looks like for a
transition based parser these are like
merged features got a couple of things
and as you can see you only go in
booties will be the offer you only go to
length three or something ladies but now
we can do it better because we have
recurrent neural networks and we can
have like basically ah we can have a
better look ahead of what is coming out
of the affair and what we have in the
stack so we can use recurrent neural
networks to do transition based parsing
so the idea is that a regular neural
networks are good to learn a strap to
start complex patterns from sequences
they are very good for data which is
order and context-sensitive where the
later element of the sequence depend on
previous one and it basically means that
we have a like what we have in
transition based parsing which we have a
stack and a buffer and this basically
are that order unconscious sensitive
they're also Goodrem
information over long distances but a
recurrent neural networks as they are
with the simple implementation they have
a bias toward representing the most
recent information and this is what
people called the bunnies in gradient
program so some people came up with the
idea of doing LSD ms or lonesome term
memory networks which is basically a
body on of these circular neuronal
networks which are designed to cope with
these bunnies in gradient problems so
they basically have an extra memory cell
that keeps gradients and this is going
to keep the gradient onward to not keep
the gradient am i doing that you improve
the results of Iranians in many task
also they are better than Iranians in
several tests and also in parsing and
what we presented in ACL is basically so
we have analyst TM as you know sending a
sequential model encoding of a sequence
so in transition based parsing we need
stacks so we basically change Alice
teams with a stack pointer and to
constant time operations so basically
the concept an operation the bush
operation that we have will be the same
as we have analyst Ian whenever we have
an input with that we have a new input
into Dallas teams and then we also have
a pop operation which basically moves
the stack pointer back and by doing that
we can basically model the behavior of a
stack and we can do parson in a very
efficient way by using astragalus teams
or else ti's so the intuition behind
this title is teams is that the summary
of the stock contents is basically
obtained by accessing the output of this
stack pointer at the location of the
stack pointer so I'm going to give you
an example how it works so this will be
our y tengo to be here so this will be
the stylist TM ok in which we have the
stack pointer pointing to the output of
so you would this will be later output
layer than a hidden layer and the input
layer of the stylist TM so it will be
for this stack in which we only have one
Warmack that the output is going is
pointing to the output layer of this of
this one ok so now if we do a pop
basically we remove it the symbol from
the stack what happens with the startled
st NE
that the output late so sorry late a a
pointer is basically pointing now to the
empty stack so as you can see we can
model any kind of stack and if we make
another input basically shifting
widening to the stack now the output lay
the pointer is basically this one which
is what we need it this is difficult to
understand half another slide which is
the next one which i think is going to
be better for some people some people
like this one so people like the other
one so I decided to keep off so
basically if you are doing push you are
doing the same thing as you do in a in a
in analyst TM and whenever you do pop
you basically move the pointer back to
this and now we will push we basically
great like another branch in a tree for
doing like this yeah what's at the end
you have one stack yes it's compliances
structure yes so at the end for reason
if you are here I do want the output of
your lstm you just run the recorder
neural net for the for whatever from
whenever you get the pointer so you
don't need to remember those others
branches you don't but if you start
removing them the complexity grows and
you so the idea here is that we we just
need to keep whatever we we need for
this and of course all these things you
can get get rid of them but for parsing
yeah any more questions if so this is
the same model that appear in lips what
nips nuclear-armed above Model T so this
early clear that ever I saw the same the
same paper already didn't bother mm is a
different kind of awesome well this is a
cyclist en which was published by me
annual offers in ACL in a Cell 2015
so not if this was use more discreet
okay sorry I get any more questions
about this oh okay so this is basically
what we use so this is our the stylist
themes in which whenever you want to get
you get you get like the encoding of a
stack you can do that so we are
augmenting Alice teens with this stack
pointer and this allows us to to
basically build stacks and do parsing
which we put but we need in transition
based parsing are basically stacks so
the next question of course is how we
can use this to do transition based
dependency parsing with a stack of stems
so we have a buffer as you know because
I saw you example we have a stack and we
also have a list with the history of
action which are basically the things
that the parser has done so far so these
list of actions this buffer and this
stack are basically associated with a
cyclist TM so we have fun encoding of
the components of the stack the buffer
and the list of actions at any time step
the atlas of action is not as tackle
esteem because you are only pushing
things into this you are never popping
things from the list of actions because
you can you cannot change the past but
the buffer and the stack are basically a
cycle stems and we use a rectifier
linear unit to produce the parts of the
state and bedding given the three stuck
in the stems so this thing it basically
looks like this so this will be like the
buffer in which we have this type
pointer pointing to to this element here
we have also the cyclist am here for the
stack and we have the stack Coliseum for
the list of actions with the actions
taken so far by the passion and this of
course is the rectifier linear you need
and the softmax over the actions and the
parcel decides what to do given the
encoding of the three establish teams so
since this is a neural network parser we
have to represent the words in the best
way possible so we have warren bearings
ok so as you can see here in the
previous slide a lot of the things here
basically the awards so we need to
represent them so basically we have a
learn better representations forum for
each war type so these are these
better representations as you see here
so these are taken from the training
culpa so we had we get a little bit
representation and then a representation
for them out for each out of vocal your
vocabulary words we get a fixed
representation like uncle for the words
that are not included in the training
court with basically all v words we also
have the drain water embeddings values
in a new language model be trained more
embeddings while in a tall and Michael
neck next year and these are basically
another were embedding that you can have
here and we also have a land
representation of the part of this tax
so what we do is to concatenate these
three different vectors and for each
word we get a new embedding as far as
this and another thing which is very
very useful in transition based parsers
is the composition functions in
transition based parses we call normally
call these history based features which
are the things that have already been
decided so in mind that you have already
found the subject for a sentence so you
don't want to find another subject for
the same bear right so you get
composition functions are basically the
embedding of a subtree of a dependency
subtree so as you see here you have our
hasta indecision this is a tree and we
want to get the best and very impossible
from it so what we do is basically to
run a recursive neural network or
hyperbolic tangent or the head dependent
and relation plus some kind of bias so
you have the work / has the indecision
you run these companies hyperbolic
tangent of a hasty decision with the
relation and then you get an embedding
from it and this is basically how we
represent a partially with dependency
structures which is a very very useful
source of information in passing yes in
the case where you only have the pedals
on one side it seems like it's an
ambiguous about how to compute this tree
but if your dependence on both sides
there's ambiguity about what do you call
it alone yes there is a mere ed but you
are always you are we starting with head
dependent and then relation so you you
put the cookie say I have let's see the
man let's see
stuff but well even if I just say um
John went home went has two children one
is John yes it doesn't matter in the
order in which you consume the J we it
does akiri doesn't matter so you will
get like a different embedding in which
you one will be a left and the other one
but but you just pick it whenever the
parcel but since is a left-to-right
parcel you normally get the same order
at the same time unless you are doing
shopping and nonreactive parsing which
you can change this is because we are
using a transition base or approach for
parsing in which we basically see if
wars one one by one from the offer to
the stack so he's always had left to
right you can wait which right actually
to do it in the other way any worse
similarly yeah I'm just thinking because
English yeah I default that's right
branching structure yes it's actually
may be easier to go for my so some
people have tried several ways like
doing first left left to right then try
to let fun try to some kind of boating
system between these two and that they
get a little bit improvements that is
not like the most influential
application you can imagine but of
course you you see this is a transition
based parties you have to think about
always let the right person in which you
can also swap words if you want to have
non-productive parsing in Jeremiah check
or these languages this is any point in
time you have perforations from left to
right yes not doing something
bi-directional it yeah but this is what
we are talking with you can do it by
direction on but in this case we are
only hard to think about this like a
transition based approach in which you
take words in a left-to-right fashion
okay sure okay so these are the
experiments that we run first this what
i want so we run for the ACL paper so
these are for the english penn treebank
so we have the stand for dependencies we
use the plate part of his pistols with
the stand for tiger with 97.3 which is
state-of-the-art QC for this tree bank
and then we use a basically the same
settings as done kitchen and chris
manning which is a fit for war
young bass parts that also left to right
neural network parser for which is the
closest model published to our East
oculus TM parser so we compare our
results to the results so we see that
well basically this is a label
attachment score is the percentage of
the scoring tokens with a correct head
and label attachment score is the
percentage of scoring token with a
corrected we also take into account the
label okay so as you can see the results
compared to China money we get a more
than one point here I'm for a late for
label we more or less the same result
like on one poem better so our parser is
basically better in all conditions
compared to China money what I'm going
to show you now is like an ablation
conditions in which we remove the
components are represented before so the
composition functions they were in
various or the part of his pistols and
see how the parser react to that so if
we remove the part of speech tax the
parcel goes down to 92 point fifty seven
level attachment score people normally
take into account this column in order
to compare parsers so 92 point fifty
seven actually a 92.5 six is like the
best one of the best result polished
without part of a space tech so this is
a very very strong result if you remove
the pre-training board embeddings you
see that the parser gets like 92.4 so
you are removing all the semantic
information or the contextual
information you have in the embeddings
and use it as a parser suffers what is
still is very competitive and if you
remove the composition functions or
basically the partially bill dependency
structures and when I say remove it
basically what we keep in the stack is
the head instead of keeping the the
partially the embedding of the passive
III we keep only the embedding of the
head and if you do that the parcel goes
to 92.1 which is of course we go see
lying with previous research in
transition by parsing history-based
features yes no it's not a killer it is
this one when you put the first base
second back in before yes yes yes this
is what they are in the same level but
let's look at it a little bit confused
okay and if we remove the stock Coliseum
we use a stock out in them so instead of
ynez STM's we use our own ends the
parser gets 92.3 which is a still better
than gentleman in which the quit for
Warner network but of course is not
competitive compared to the status games
and then we did experiments with the
Chinese driven so for this sign is 31 we
follow basically Jojen and stiffen plug
which are the people that started win
experiment with this 31 we use gold part
of SP stocks and it is basically again
the same settings are done kitchen
increase money for this paper in the M&amp;amp;P
so here we have more or less the same
picture hey so the stakus teams are well
here better like three points better
than day it's in a money part step aside
state-of-the-art results for these
settings and again if you get go for the
ablated conditions you get you see that
there are differences did you take into
account the part of his Pistons you see
a big drop and this is because we are
using gold photo is vistas in this case
and so here is like the improvement of
part of a spaced exactly is higher okay
and will basically some conclusions of
this part so this is the highest result
ever published for every parcel greatly
left to right transition by dependency
parser is very close to being searched
for complex graphics parser or even I
will say is better than most of them so
this is actually something very
influential and it runs in linear time
we clear decision so you get decisions
and you get a passage which is super
fast and it provides the one of the
pressures on several published ok so now
any questions so far cuz i'm going to
identify suitable the several slides ago
you we supply the part of speech tags as
an embedded but I mean there's only 40
part of speech tags I don't understand
work and repair it off well it's a small
this a small and very no wealth I
mentioned but yeah it's basically I find
one in vain that we learned Julia frame
in some in some languages you have more
than 40 see you want to korean i'm going
to sew example this or results in korean
later you have had
a part of a speech stock that is
basically like 1,000 different packages
is not like the language in which you
have the deep put in two thousand or
something but it's 1000 so you you can
have something okay so let's move to the
character based modeling of words so as
you remember a we're actually going from
the question a we have like an embedding
of part of the speed stacks and then we
also have like a pre-training watermelon
and I'll and representation of the words
from the draining corpus so we can do it
something better we can do character
based modeling of words so in money that
you can you cannot so also ran a
bidirectional STM character by character
of the words view of your input sentence
and then you run it in the other
direction and basically you get you
concatenate these two invariance
together and you can coordinate this
with the reading of the part of his fist
so the intuition behind this is that you
are going to get a lot of suffixes and
prefixes from the from the from the
character resembling and also that you
can do this for every word you don't
mean to care about whether this was an
out of vocabulary war or an e-book have
usually worked you can actually do it
for for everyone in the you're in yours
in your inputs and this was published in
the MLP 2015 i'm going to show you the
results late so basically we model this
character based modeling of words in our
two-dimensional a things with physically
to the embeddings and we run it with the
SME and we got this um as you can see
here we see a cluster of words that are
basically worse in past tense for
English you also see a lot of gerunds
here you see some adverbs over here and
you see some the same word endings of
these awards here or you see the same
word endings of this sword here so you
see a lot of basically suffixes and
prefixes and this information is
something which is can be very useful in
parsing because to pick some purposes
what gives you the morphology and part
of the speech study so by doing this
calculation variants the intuition and
the motivation is that we can improve
out of vocabulary hunting and we can
improve the
the performance for morphological a
bitch languages so we did this
experiment which we have this baseline
model which is what we call words I put
this here because it's in the tables and
we also have this character base model
with we call sharks so this baseline
model is the same one as a presenter
results in Chinese and English before
but removing the pre-training or
embedding so we don't have additional
resources we just have the free training
or embeddings of the treatment okay so
we did experiments for the for some free
banks of the circus on parsing
morphologically with languages these are
basically arabic basque french german
hero hungarian korean police actually's
you can see some of them are
agglutinative languages and you
basically lieutenant morphemes and we
also included turkish because you know
that turkish morphology have this a
agglutination behavior which is going to
be very impressive with character based
modeling i'm in terms of completion will
also run it with english and chinese so
we don't have any explicit morphological
features just to see the effect of
cattle razor presentations and we don't
have additional resources just the
training corpus the parser and no pre
train or Carter base in Venice so we
tried with and without part of a species
as well to see the effect of character
based model words okay so if we try with
analytic languages such as English or
Chinese you see that the parcel with
characterizing balance is better like
one point while pulling this easily
through 0.3 or almost 1.4 Chinese or of
course we didn't expect too much for
English or Chinese because these are
analytic languages in which morphologies
not be playing a big role for syntax but
if we move to a luta native languages
such as fast hungarian korean or Turkish
you see very big improvements so using
how the parser for bus have more than
six points improvements for hungarian
encodes pincode like 84 korean encode
like exactly 10 points actually
important
good mor more than three points so you
see how the parts actually behaves much
better when you sing character based
modeling of words comparing to what how
we behave having a word presentation
like they say sequential thing and if we
tried with a pushin alert empathic
languages we see how the parcel also
improves in all of them so as you can
see these colonies hose is all over it
in bold and as you can see how we also
improve in all languages in polish we
saw a very very big improvement and the
main reason why we saw this improvement
in policies because they ought of
vocabulary rate industry one is super
high so we thought we basically have
seen that the character based modeling
awards are given you something a lot
also when you have out of vocabulary
hide out of vocabulary rates in some
languages you see more or less the same
result such as Arabic or other languages
and when we include this is without part
of a speech that's all this is famous
okay and when we include part of the
speech tags we see that the pictures
changed a little bit so foreign news and
Chinese you almost get the same numbers
or a little bit better for words but you
see that they are the bold things are
always in the glutamate languages so
it's still Turkish Hungarian busque and
Korean which are a beauty native
language is basically when you are going
to need more themes and you get you get
a these kind of things you basically see
that you still get very big improvement
for parsing more than two points is a
lot and in average the parser is better
compared to the one with war
representations as you can see here ok
so the conclusions for this is of course
character-based representations are
useful they all the eight part of the
speech information force all languages
so actually you compare you compare this
column to this column in some cases is a
you go to korean resistance it's
actually even better when you don't have
far to the speech information so you are
getting better information from the
character based modeling and what you
get with part of his fist acts and they
are more useful for algo dynamic
languages in our examples is this for
languages are in
which you can see actually very weak
improvement and it's also a nice
approach for handling the out of
vocabulary program we saw in Polish in
which well when you have an auto
vocabulary a big how to hook everybody
rate the parser gets better so we have
an article submitted to computational
linguistics still waiting for the review
spot somewhere good so with all the
results that I have presented before
okay so well in this this fall I had the
chance to to basically teach a course in
natural language processing sand one of
the things that I taught is about a
parsing so I asked the students to Villa
Park sir so I had this this is student
is working with me and we built this in
sampling system by using different
outputs a different model struggles with
the cyclist en person so basically you
trained several models with the cyclist
TM parser and you build a boat in system
like this one okay we took this like 40
parsers think that day one place will be
attached together and then you run a
graph-based parser actually in most part
set and the result we got is basically
the best result ever reported for
dependency parsing and we are working a
research paper together for this and
also we can't do this to do this
cross-lingual II so Wally he was
interested in this parcel and we were
working basically in this cross-lingual
setup in which we train a language
Universal dependency parcel on a
multilingual collection of three ones so
basically instead of training with a
monolingual 31 we training with several
three banks and we also have
multilingual Warren bearings a
typological information included into
the parser and the nice result about
this this research was that the
multilingual training outperforms
standard supervised training which we
have a moral inventory so this result is
is submitted to tackle we will spread
the review soon and basically this is
the main table this is the monolingual
training in which you basically take a
3-1 or German and you train a model for
Gemma and you get a result the result
for English is only 88 because this is
the universal dependencies you only have
twelve thousand words to heart the size
of the penn treebank and the nice result
is
here basically so you compare these to
these and you see that the multilingual
training is better than the monolingual
trick basically all the columns is the
language universal in which we train
with our languages and you evaluate in
German and here is when you include a
lexical and variance for the target
language language Heidi typological
information I'm fine grain but those
pistols and you see how the parser is
better the implications of these are
very high because if you want to pass
difficult languages in we do not have
any data well you can use this model and
is suspected to be better actually than
the monolingual training and we compare
this with the state of the art with the
same systems so this is when you don't
have any data of the target language and
we are better than the state of your
systems envisage sorry this is you say
you don't have any idea so then so you
basically choose easy sweeties so you
remove that with all the sentences from
your training corporate entities and you
train with all the other languages and
you evaluate yours in your sweetie's
that's it so when you say plus lexical
what is that it means that you use is
explicit embeddings for the target
language that are trained with for this
length so train is also model over there
yes amor link ok I'll label an unlabeled
it which we you could take it for for
any language foremost poets also you can
go perform short answers when you when
you supply the language ID what is that
the language that is the typological
information so linguist would tell you
probably better than me but basically it
gives you like the how the the the part
so how the syntactic information is
behaving in it in its language so you
know how the left branch in white ranch
in wilmore all is it fair to the model
sorry how do you give it is basically
like a signal for the dependence so do
you remember the three cyclist seems so
you get like a signal and I don't know
the embedding or the language city
so you have a set of psychological
features is it is a mess of your SPO or
so on and so yes it is it gluten
institutional and so you have these k
different features and when you use will
include an embedding of this to the
parts and the parts that makes better
prediction as you can see it improves
for all in which a search in solvent
which is you actually see a big
improvement so you go to German and you
include the political information use it
a liberal a fat leaf attachments improve
a lot which is something that I contest
ended the person when we didn't have
this type illogical information was
planning to do a lot of right
attachments we include this and the
parts that started to do it by the story
but i think is a nice person ok so now i
know it is just basically all the
personal stuff so as i promised at the
beginning i'm going to talk about
something different to parsing so the
question is if we can model other
linguistic structure prediction problem
different and parsing as sequences of
actions so I I hopefully convince you
that we can do it for parsing so hope
and we can try to do it for other
province so we the question is whether
we can model then jointly with syntactic
parsing so having syntactic parsing
joint with another task or we can model
a particular problem with the wrong set
of actions i'm going to give you an
example of these two questions so first
talking about joint syntactic and
semantic parsing with cyclist ems i'm
going to briefly explain about this is
another paper submitted to tackle so we
have a joint model for synthesis and
semantic role labeling basically using
cyclist teams so instead of having one
stack we have a couple of stacks one
that keeps the syntactic information
another one that keeps a semantic
information so in one of them we keep
all the partially with dependency
structures that we get from the
syntactic stack and the other one we get
all the semantic roles we can get from
the stack and we got state-of-the-art
parsing and semantic role labeling from
this by using again this tackle seems so
we compared with previous system these
are the system of the con 2009 contest
at eight and this is the closest model
published which is Jane's Henderson's
model which is actually the same
algorithm but without using cyclist en
by using the run
actually so we see how we improve his
results special in the semantic task
okay and now i'm going to talk about one
paper that cutter set yesterday in
knuckle a knuckle 2016 so this is
basically a paper corner on neural
network models for negative recognition
and one of the things in this system is
a transition based on a model with a
cyclist in this are they two men
collaborators of the paper so basically
we have an algorithm that constraints
are labels chunks of input sentences is
a transition based approach again it is
the same way as we do it with the
dependency parser in this case we have
three data structures I'm going to be an
example how this works we have a buffer
we also have a stack and we also have an
output buffer that keeps the thing that
have already been decided and it's a
similar approach to the parser so let me
give you an example of how this work
with the same running example or frame
marwan need which is tyra be Myanmar's
so basically you have a buffered full of
words you have the stack which is empty
and you have your output buffer which is
also NT at the beginning so the system
basically decides to make a shift it
takes a work from the buffer and put it
into the stack now we can decide to say
well marks a foreign mark can be already
an entity and we can go to the output
buffer or you can create and chunk with
another thing so shifting another thing
to the stack and whenever we have this
again you can decide to create that sank
between these two word or shifting
another thing in this case it decides to
create to basically reduce what we call
reduce and is basically taking mark
Watney and creating this and basically
we I'm going to explain you how to put
this information in an embedding here in
a nice way to improve the results of
course when you have a bear-like visited
is not an entity so we can basically get
rid of it and what we do is to put it up
to half an output transition that
basically takes visited and the rate we
froze it to the output buffer and of
course you can also shift marks into the
stack and whenever you have this
since you know there is nothing else you
can decide what is the best entity to
have for this thing and in this case is
location so this will give you like the
result for this sentence for NE a-- okay
so the main motivation of this research
is that most any our systems it uses
they're not resources as you know what
people that have worked with probably
know so they look into gadgeteer system
and databases resources we can do that
of course we can input that into our
model but we don't want to because we
want to do this also for a low resources
scenario so the question is if we can
perform at the same level or even better
without including any of these cisternal
resources such as gotcha tears or
statement databases with information
about entities in the world so we did
experiment with the content 2002 and
2003 datasets for english german dax and
spanish and we only use warm features we
didn't even use part of his pistols to
make this like cross-lingual a
multilingual in which you can actually
try with everything like this so this is
how the system looks like as you can see
similar to a parcel you have the cyclist
TM for the buffer you have the slightest
tiempo de lisa factions you have your
cycle scheme for the output buffer and
you also have a slight listing for the
stack and then you get the softmax of
the previous actions and you basically
decide what to do given the parson
estate or the system state in this case
so in order to come up with a nice way
of representing words but we did here is
to do character wise modeling of words
again so because we thought okay
character-wise mode is going to be very
useful or so for any are because you
have also the capital letters at the
beginning all these things are very very
useful and we also have a pre train
whirring bearings with a lookup table
with basically the same imbalance that
we use for the previous publication in
parsing we use it here in a lookup table
so basically for each word we run a
bidirectional sdn for the war marks for
easton we get like this and then we have
a lookup table for the embedding of
marcin we concatenate these two vectors
and this is our embedding for the system
and again we have also composition
functions so whenever we reduce
whenever we take a chunk from the stack
and put it into this into the output
buffer we have to infer some kind of
information and put it in some kind of
embedding so what we did is basically to
run a bidirectional st n algorithm
variants of these tokens together somali
mark Watney we run a variational seen
over these two tokens together and we
also includes a label in this case like
location or person or whatever and then
we get a presentation which what we call
composition functions of valediction
Alice teams okay so these are the ending
our English results so this is the
result of this title is TM Arthur this
is another system that we presented in
the same paper which is actually a a
little bit better but is say level so
all the systems with a star our
assistance that incorporates a basically
sternal resources or part of the speed
stacks or gadget years linking cetera
setter and our system only use war
phones Athena's and as you can see the
results are actually very very
competitive this is one of the best
results actually published for any are
the same picture is for other languages
which you go to Spanish you see how the
system stack olicity insisting gets like
almost eighty four is the very salable
palace and lstn CRF system which is a
bi-directional LST anything that we have
in the same paper is a so-so a little
bit better and for dutch you so have the
same picture so this is the news a lot
of external resources and you compare
with the rest we have like the best any
our system by only using war phones and
for german we have the same picture of
course we see that the character basin
veins as we saw in parsing are very
extremely useful so you you see that
they have a big impact into the results
especially the cyclist en mode so well
this is the state of the iranian system
is linear in the length of the sentence
again it is fastest we have with the
parser so basically we can run it in a
email in a very fast way and we can
produce a very nice results again
character which the presentation
happened with the dependency parser they
provide very very useful information
rainy are such as say you can have like
out of vocabulary words or other things
and we are only using work phone feature
so we might we are not even including
part of his vistas in the model of
course we could have included but this
this one's not the only when we included
we improve but this was not the task of
the system and we don't have again no
gotcha tears no standard resources so
now I'm going to briefly talk about
another paper that was accepted to
knuckle 2016 so basically is what we
call recurrent neural network grammars
so this is a top-down face structure
parser so it's not dependency part is
restructured partial and we also include
language modeling results and we again
use cyclist seems so the nice thing is
that this is speckle is the end for
syntactic parsing that I saw you before
is a very powerful discriminative model
for language basically so you can
basically infra syntax in a very nice
way but syntax as you know is very
useful for generated language okay so
the idea here that we can use the same
things that we have in this tackle SDM
parser to basically create a generative
model that can be able ready darling
which model so think about it whenever
we see a word from the stuck to the
buffer we can also we can also predict
what worries we can decide what worries
depending on that so we can actually use
it as a language model so we do this in
the context of facial structure parsing
and we'd call it RN in G so basically
the recurrent neural network remarks so
this works more or less like this is a
top-down parser is not like a typical
phrases structure posited basically both
on up so you have the buffer here this
will be like the discriminative version
of it you have the buffer and then you
can also see if terminus into your stack
at the end you are basically building a
facial structure three of your sentence
the nice thing is whenever you saved you
can also predict the word and then you
can evaluate it as a language model and
the language were preceded resulted we
got for the English spend driven so if
this is a sequential sdn basically
understanding which he played the
network even the encoding of the
previous elements we get better results
including the syntactic information of
the cycle STM's for both English and
you can see we have an improvement here
and here and the nice thing is that we
can also evaluate this as a parcel
because it's a person and if you go to
the f1 is course for the english penn
treebank you you got without a thermal
resources 92.4 which is state-of-the-art
one of the best results ever published
for this parser for these three one
sorry I'm for Chinese is close to the
west a little bit worse the best one is
around 83.2 or something like this but
still is a very very competitive result
take into account the game is greedy
left to right and you don't need any
statement resources and this is
something that we are starting with more
ongoing work so I'm we are thinking
about this transition based approach too
much in translation in which a well as
at the beginning I said with a
stack-based parser and a swap operation
you can actually come up with any kind
of reordering so you have a buffer yes
you can do a GG reorder you can do it
you can do any reading we have to talk
about that it I can show you so
basically you have a well it's hard
because we didn't are made of actions
you can grow but you can come up with
most of the readings if you go to an
some languages such as Japanese Japanese
English is going to eat more difficult
parts of restrictions yeah well as this
is a total minor detail please don short
or no product anyway so basically we re
starting on this we are thinking about
this so thinking looking at the results
of our Armenia grammars in which
language modeling results are super nice
and we can also do transition based
parsing and a non-relative passing which
we swap words and we do reordering what
about instead of 16 award from the
source language we see if I were from
the target language so we make a
prediction of the tire planes so
basically we are thinking about this and
help basically supervising the student
on this and basically the final goal is
to do a complete a match in translation
system that runs in linear time this is
a very big project a very big ol but we
are trying to do than I believe is
possible so it might be also interesting
to do it jointly with parsing so
whenever we see for work we also produce
the syntactic parsing so this using this
imbalance can be useful for much in
translation because as you know syntax
is also useful for translation so this
is something we are trying to do
now so in conclusion yes sorry you were
saying what a linear time I mean doesn't
the house or a Montreal let's steal
model also rounded linear time again
it's got a very large constant but just
it's also severe right yes this it is
dependent or in this case you idea would
be to do everything my single step event
really yeah well that's sort of what the
Montreal attached models okay okay okay
so in conclusion for this talk so I
presented powerful sequential models for
natural language processing basically
all of them are based on transition
based approaches with cyclist ians so I
present the state-of-the-art transition
based parser which is getting a lot of
attention from the community it's good
in both in terms of results and speed
because well its runs in linear time
video decoder and produces one of the
best results they were published I also
presented a fresh and new
state-of-the-art a transition base any
our system that produces very high
results without using external resources
can be extended to many languages and
many tasks by using the character
wasting billions of words i presently
how we can do extensions for language
modeling by using a serious approach in
which whenever you see for work you can
also play the war on the water and use
this language model and size so that i
can we can produce state-of-the-art
results also for language modeling
compared with just a powerful STM and
also a briefly i presented sanctions for
semantic role labeling which you can do
joint simple semantic role labeling with
stag las tias sorry i would like to
acknowledge noise meat and grease dire
which I rabbi source on most of these
papers and collaborators in my case of
papers and I would like to thank you all
of you for your attention and you have
any questions
normal question yes there was one of
these many papers where you had an
output the foot and they stack if some
lights if you use the status of each of
them as a yes staff I just curious so
basically so in the parser you mean so
for instance this yes so each of them is
a cyclist in this so basically in the
case of any are these are this is
basically the beginning you do always
push push push for the buffer for
instance you always push all the words
and then you start popping things
whenever you see something from the
offer to the stack just answer is
excessive but in the case of transition
based parsing so if you can call let me
go back so in the case of transition
based parsing so this is the same so we
fill this with all the words at the
beginning and then we basically see if
things when whenever we see with Bob but
if you do non prolific passing with the
swap operation you basically take this
word and you put it back here so you
also do pushes during parts in time so
it is like totally but this list of
actions can be modeled with Alice the
end because it's basically history and
you are always pushing things you never
pop for me but it's all over now
established games
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>