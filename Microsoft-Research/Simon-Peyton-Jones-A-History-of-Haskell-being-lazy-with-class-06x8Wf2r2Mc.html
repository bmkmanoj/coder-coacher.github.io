<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Simon Peyton Jones - A History of Haskell: being lazy with class | Coder Coacher - Coaching Coders</title><meta content="Simon Peyton Jones - A History of Haskell: being lazy with class - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Simon Peyton Jones - A History of Haskell: being lazy with class</b></h2><h5 class="post__date">2016-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/06x8Wf2r2Mc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">languages like ml and particularly ml
Sasol KRC and they used some data types
and pattern matching so ml particularly
had this amazing polymorphic type system
we now referred to as ml style typing
but at that stage it was just ml and it
was completely amazing then on the other
the other side of this slide here there
was a new work on lazy functional
programming that came from several
places including a David Weiss who's
sitting over here and here the idea was
that um you might use call by name
rather than call by value and this was
if you think back to the lambda calculus
this was plainly the way in which God
intended a reduction to take place or at
least church that is Alonzo Church so so
this was very seductive and david turner
and others wrote exciting papers about
about what lazy evaluation could do for
you and infinite data structures and so
forth and we were pretty excited about
that and guy Steele has got as guys
still here and I'm Joey Sussman and
others at MIT were writing papers about
lambda the ultimate go-to lambda the
ultimate imperative lambda the ultimate
declarative this was pretty exciting too
we were too young for the 1960s we were
in short trousers at that time so we had
missed all you need is love but we had
the slightly more geeky slogan all you
need is lambda so at the same time is
all this stuff was happening on
computers on the computer architecture
side this machines were coming out built
by symbolics and LMI and these were
exploiting the language they were
executing with with hardware
acceleration dataflow architectures lots
of work was happening on dataflow
architectures both here at in the states
in MIT particularly but other places as
well Utah and also in Britain at John
GERD and his henchman ancestor built the
Manchester dataflow machine and Ian
Watson is somewhere somewhere here was
Ian in where do you go there's Ian
Watson John Gerdes a hench person so
they were building machines which which
executed functional programs directly
the same time Turner and others were
describing graph reduction mechanisms in
which you could evaluate programs by
transforming graphs inside machine this
was quite similar to dates but a little
bit different I don't have time to say
very much about it but and
in particular work this amazing pair of
papers describing SK Combinator
reduction in which we translated these
lambda terms into this absolute mess of
SK and I and if you look at the SK
denies and here are some they shouting
in machine instructions so so you maybe
think well maybe we should build
machines that directly execute this
stuff so into this highly volatile
mixture of ideas that were intoxicated
us God arrived in the form of John
Backus and he gave visit this is he gave
the Turing Award lecture called under
the title can programming be liberated
from the von Neumann style in which he
not only he made functional programming
respectable by describing FP in some
detail but he also put forward the idea
that we might want to work to liberate
ourselves from the von Neumann
bottleneck of affection execute by by
thinking of executing functional
programs in parallel so this was just
amazing this was like lighting the blue
touch paper on this fire work that have
been building up because it made um it
made us think that functional
programming was respectable and God had
said go forth and build new languages
and better still new computers to
execute them so there is nothing that
sure that young research students like
better than that thinner than a call to
arms like that so off we went and and
it's hard to explain exactly what it
felt I think it's kind of like being on
drugs and boy did we inhale
and the result as you might predict was
complete chaos so the the the 1980s were
it's actually occupied by a lot of
different research groups going off and
doing doing exciting things so many
conferences that the first meeting of
the functional programming and computer
architecture conference happened and
note the computer architecture bit it
was all tied up together the Lisp and
functional programming conference was
born they subsequently merged to form
ICF P many languages appeared many
compilers appeared many computer
architectures appeared including ones
that executed s and K directly Burroughs
even patented the idea
mostly doomed mostly doomed alas so what
several of us were working separately on
this kind of stuff and but it became
clear that because we were each having
had and having our own language our own
computer architecture where our effort
to get at getting an diverted so we met
at almost by chance in 1987 at the
functional programming and computer
architecture conference and we said we
just want to agree a common syntax we
all will essentially have the same
language so we believe that all we
really needed to do was to form a
consensus about the language in order to
reduce unnecessary diversity we had
absolutely no clue what we were taking
on we thought it was going to be done in
a year will be over by Christmas but in
fact it was two and a half years later
before we produced the the first version
of the haskell report and so here is the
timeline so we started in september 1987
and the first exciting thing well I
suppose it was our first meeting our
first main in-person meeting was to
choose the name for the language so we
all were we were led to a whiteboard and
wrote up names we liked I think it was a
black it was pre whiteboard technology
no black board we worked out names we
liked and we crossed out everyone else's
names that we didn't like and in the end
of a very few names left and it was was
mainly curry so he ended up we decided
we couldn't call it I couldn't call it
curry that with too many bad puns so he
called it after by after haskell curries
first name and Paul who Takeshi went to
see and Haskell Co his widow mrs. curry
who very graciously agreed to allow us
to use his name but her parting comment
to Paul as he left was you know Haskell
never really did like that name
but we were stuck with it by then so
then off we went and here is a photo the
only photo target dig out this is not of
the the Haskell committee this is a
meeting of working group 2.8 many of
whom belonged to the Haskell cim
committee here so here they are in
Oxford in June 1992
so this is at this point where a couple
of years in so this gives me a chance to
induce my co-authors so Phil and John
would you like to just stand up here
please so we can we can see you so here
we are over a span of them for 15 years
many things have stayed the same as I
think you will agree with me that we
have retained our youthful good looks
though phil is noticeably less hairy
than he used to be it was extremely
hairy phase in those days other things
have not changed as as you may notice if
you looked at look at it carefully my
wardrobe has not changed
this came as a shock even to me those of
you who know me well well no they don't
take a lot of attention to what I'm
wearing but it was a bit of a shock when
I saw this slide and realized this is
still part of my active wardrobe I
didn't just get it out of the back
closet this is active active wardrobe so
so here you are but now I have but now
I've explained this to strangers because
it's upside down
yeah I could throw it into the audience
you're supposed to smell it
trotty I believe it has been washed in
15
so many things have stayed the same over
this last 15 years but some asked
Haskell exactly occupied a pretty large
fraction of the lives of the people
who've been involved in making it so
just by way of making that concrete I
just show you something else about this
picture this here is my wife Dorothy and
also visible in this picture is my
daughter Sarah who at this stage was in
an embryonic form but 15 years later
here she is considerably bigger so this
puts Haskell in perspective for me
furthermore my children taunt me that
Haskell is really my first and best
loved child and in fact this cat that
Sarah's choking here is a recent family
acquisition and is now called Haskell
they really is I had nothing to do with
it so here we are we've now reached May
1992 and the the third iteration of the
Haskell report is published in sigplan
notices so this is a big step fast
because it's a it's a kind of step
towards respectability I think it was a
really big issue of sigplan notices so
we have a big THANK YOU to sigplan and
to Richard or works of art in particular
for allowing this to happen so we then
then then there follows a period in
which there's quite a lot of technical
developed particular particularly about
monadic IO that I shall describe in a
second and then later about February 99
we we decide to fix a particular version
of Haskell that we called Haskell 98 and
because of the 99 you see and it was it
was aimed to be a stable point at which
which people who wanted to write books
so at this stage it was becoming useful
enough that people might want to write
books about Haskell and and so be useful
for teaching consistent across
implementations or all of that good
stuff while allowing Haskell itself to
progress independently so we gave the
the funny name to the stable version the
the unqualified name could mean really
whatever you pleased now that turned out
to be a really good good choice
so Haskell 98 and came out and there was
then in February 99 but he actually took
another three years to get that
stabilized language design is a slow and
painstaking poster
so what are the stage of which we had
got that stabilized then Cambridge
University Press published it as a book
and that's another big thank you to
Cambridge University Press
because it was a book which they
published with unrestricted copyright so
it was a BSD license book one of the
very few because we couldn't get the
rest of the Haskell community wouldn't
agree to copywriting their report for
for Cambridge University Press so I
thought I was a admirable flexibility
then followed a kind of growth spurt in
the Lark the last five years have been a
bit of a growth spurt for Haskell I want
to say a little bit about that here's
the young the way in which most research
languages go so the horizontal the
horizontal axis is time and the vertical
axis is a logarithmic scale of the
number of users and I liked up to about
a hundred users you're speaking to geeks
and then you start to break into
practitioners so this is what happens to
most research languages were sort of 99%
level now for a number of programming
languages then a successful research
language simply dies more slowly this is
what happens to it and on a sort of five
year time scale now were then there's
mainstream languages that do this they
they don't die at all
in fact they go through this sort of
threshold of immortality at which it
becomes impossible to allow them to dive
is there's just too much code written
using them so they become essentially
immortal forever
so then Haskell has done something a bit
unusual
Haskell went went through the structure
the sort of geeky level and stayed there
for a while but just in the last five
years it's picked up in in what feels to
me like a quality to qualitatively new
way and I've written down here some
comments from blogs that I culled just
to indicate the different kind of person
that started using Haskell and it's it's
they're amusing to read this you can
read them yourself and I like this one
they said learning Haskell is a great
way of training yourself to think
functionally so you are ready to take
advantage of c-sharp 3.0 so that tells
you many things right that tells you
that functional features are making
their way into mainstream languages and
also that it's a good sort of training
ground so I like this this is this
blogger so I thought actually tried to
give you some data for this this
upward-pointing tick it's actually very
difficult to get data on what's happened
to haskell because it's a kind of open
source
public domain kind of thing we don't
sell licenses but he
some so the top one these are all on a
scale which go roughly about 2002 the
presence that the top one is traffic on
the Haskell cafe mailing list which is
completely wild mailing list then it
seems to been rising pretty sharply the
the middle graph is the Haskell IRC
channel which has this is some thousands
of names thousands of users in any
moment there's a couple of hundred
people logged onto this IRC channel and
the bottom one is well this is to do
with my compiler our compiler for
Haskell called GHC since we don't really
know how many people use GHC because
it's free software the only way we have
to track usage is bug reporting so this
is a sort of exponentially increasing
bug reporting uh log for GHz which you
may take two to mean the church is
becoming less reliable but I take to
mean that more people looking in more
places more eyeballs we've definitely
revealed more bugs the things down the
right there's a lot of elaboration of
this in the paper about the stuff that's
going on in the Haskell community at the
moment so backing up then I just want to
say a little bit about what I think's
important about Haskell so when all this
is dust you know and we're looking back
in twenty years time what will be
important about this stuff and I want to
just focus on three things very quickly
can I just find out that time before I
get completely lost where are we how are
we doing along we've got about half an
hour yeah good so purity and laziness
type classes and I wanted a little bit
about processing community there's
actually a lot more stuff in the paper
but I've sort of tried to boil it out
for the purposes of this presentation so
here we are
laziness was the rallying cry that
Jewess that do that particular group of
people together to build haskell that
was the one thing that distinct with the
distinguished that particular group what
we were trying to do is to build a lazy
functional programming language and john
articulated the case for laziness very
very beautifully in his paper wide
functional programming matters which has
become part of Holy Writ since along
with John Backus newspapers it's written
on tablets of stone and and in it he
described how one how lazy evaluation
isn't just a kind of cute trick that
lets you build in from the data
structures that are kind of
entertaining but not useful but it's
actually trick that enables you to build
more modular programs so one that's all
very well but in fact laziness does have
its costs it has some implementation
cost because it makes your
implementations run slow and we can work
hard at improving that but one likely
ever to get back to completely the stage
you are if you're using call by value
but also it's uh it makes it more
difficult to reason about space and time
behavior and I don't I think we
underestimated that problem at the
beginning we didn't really worry so much
about predicting behavior so one but in
Laurens Snyder's talk he was saying how
important it is that predictable space
and time behavior for power-law programs
is no less important for sequential ones
but we take it for granted more on
sequential programs and Haskell really
makes you stop taking it for granted I'm
afraid to say so here it is laziness has
its costs right so if it has its costs
then is it really worth the cost so and
here's the the biggest reason that I
think it is in the end the really
important thing about laziness in in
Haskell so exhibit me speaking
personally now on behalf of my
co-authors is that it keeps you pure
right so every call-by-value language
has succumbed to the the siren call of
side effects of being able to just say
print this or launch the missiles or run
a sign to this global variable but in a
lazy language that really doesn't make
sense in a call-by-value language you
can just about make sense of saying of
functions in quotes that have side
effects in a lazy language you can't
look at this one here that says the list
this is square brackets the list print
yes print no well in a lazy language you
don't know whether the consumer of this
list is going to evaluate the second
element first or the first element first
on either elements or both elements so
this is very bad so in effect then side
effects become insupportable you just
can't mix them with laziness and what
that means it's the matter how much we
must have envied our our brothers and
sisters in the ML community we we envied
them a lot but we could not give in like
that we could not make a deal with the
devil but it was embarrassing whoa but
we had a language that really didn't
initially have very good i/o at all
and again our colleagues when we went to
research conferences they were
incredibly
right about not pointing out that the
emperor lacked IO and I kind of it is a
bit embarrassing to parade down Broadway
matin them lacking important pieces of a
wardrobe but nevertheless so we did and
it's a great incentive to ingenuity when
this happened so we were we were
embarrassed and we young but eventually
one way or another we came up with this
we came up with various forms of IO that
are described in the paper but ended up
fixing on this monadic story that
probably many of you have heard of so
I'm just going to quickly whip through a
little bit of that for the benefit of
those of you who've not who or don't
already eat monads for breakfast so here
it is if you want to run to do let's
let's focus just on input/output in a in
a purely functional setting one way to
do it is to say that a value of type IOT
is a computation which when you perform
it sometime we'll do some i/o and then
deliver a value of type T so now getchar
then is a computation which when
performed will do some i/o in this case
reading a character than the input and
give you a character and put char is a
function that takes a character and
delivers a computation which when you
perform it will print that character so
those are the primitive operations and
then we then we need glue for gluing
together primitive operations into
bigger ones
so this bind operator here the one
that's written an angle angle brackets
equals takes a a computation as its
first argument and a function that
expects the result of the first
computation as its input so then it's an
I can write a program here that says get
char and then binds the result of that
get charter a and then does get chugga
and binds the result of that to B and
then prints B and finally returns the
pair a and B so I'm not gonna pause very
long on this just just to say that this
is a way of allowing you to glue
together big IO things out of little IO
things and these IO things are values
they're first-class values which would
could be passed to functions stored in
data structures and when finally
performed as the the main the main
function of a program will actually
execute and do something to the world
now when all these lambdas make it look
a little bit intimidating particularly
as they black it very heavily to the
right so Mark Jones who
also floating about Mart over here
somewhere invented the do notation which
is just syntactic sugar for all those
binds so the stuff on the right is just
these sugared into the stuff on the left
by the compiler and by design the stuff
on the right looks like C or very nearly
like C so it kind of begins to look as
if you're doing imperative functional
programming and that indeed was the
title of a paper that Phil and I wrote
about exactly this stuff so that started
to enable us to mix purer and purer and
side affecting computation in a single
program once you couldn't get
computations as values then you can
start to do things like can build your
own control structures rather than
having them built in like for loops and
while loops you can write for loops and
while loops as functions and here is one
where p10 is a for loop it takes an
integer and a computation and it repeats
the the argument computation end times
and how does it do that
well if n is zero then it doesn't do
anything at all it just returns unit and
if n is bigger than zero then it
performs a followed by a call to repeat
okay so this is a full loop written
without having a built-in so there we
are so that's that's all good what are
we achieved by this purely functional
and stuff we've achieved the ability to
mix pure and imperative programmed for
pure an imperative code in a single
program without them getting without
them but polluting each other and the
type system keeps them separate so all
the purely functional rules in
particular beta reduction maintain
unconditionally true you don't have to
say you can do inlining or beta
reduction provided it has no side
effects it's still true that's really
good it also gives you a very fine
grained control of effects at the moment
I'm only talked about pure computations
and IO computations but IO is not the
only mouna there are lots of monads one
particularly useful monad is the
transactional memory monad that I
indicate here so a the type system
expresses how much how much effects
happen so reverse as type string to
string it has no side effects at all
the launch the missiles has type string
- IO of what is it string list of string
and it has well very serious side
effects but transfer which transfers
money from one account to
another transfers are n dollars from one
account to another is in this
transactional monad that I shall tell
you any more about you can go and read a
paper about it and all it does the
transactional no dad STM has a very
limited collection of side-effects you
can only read and write transactional
variables you cannot read account from
the terminal or call launch the missiles
but you can of course call the reverse
so these the types give you very
fine-grained control over how much
effects take place and that turns out to
be extremely valuable indeed I think
standing back yet a bit further now this
is perhaps one of the things that
Haskell has to contribute to the world
here's my big picture for what's
happening in the in the world of
programming on the one hand there were
languages that are useful but dangerous
right and they have arbitrary effects so
think see up in the top left hand corner
down in the bottom right hand corner you
have lambda calculus completely safe
right
but useless right because all you're
going to do is reduce lambda telling you
get what another lambda term you know
you can't you can't term it hasn't
written anything to disk or done
database access so so what do we want to
do well the we want to move towards a
place where we have useful languages
that that at least are somewhat safe so
the the Haskell guys are moving from the
no effects the world upwards and
everybody else is moving from the from
the arbitrary effects world towards this
nevada
right and so i think this plays to what
be honest said yesterday about a good
language should change the way in which
people think about software i think
that's cool as a has got and and
language is likely purely functional
programming languages do start to change
the way in which we think about writing
software because they make us think in a
different way about effects so there's
quite a lot of dynamic going on here
these movements are both taking place so
from that from the arbitrary effects end
there's lots of really good interesting
sophisticated work going on in trying to
tame effects in language which otherwise
arbitrary effects involving regions and
ownership types and adding restrictions
in one way and another often through a
type system to m2 languages that
otherwise would support arbitrary
effects and from the other end we are
busy going to in a school towards
selectively offering effects and indeed
that haskell is not the only language
that
in this corner right there's very
well-known language you'd like sequel
and xquery and MDX
which which are all that tend to be one
of the more direct domain-specific
languages that have very limited sort of
effects so and then lots of
cross-fertilization that takes place
between these dynamics as well so in
this direction it is to be frankly
rather usual env right we wish we could
do what they could do and occasionally
we there I think there's some ideas have
gone in the other direction as well the
one that I picked out here is
transactional memory
actually that was a bi-directional idea
transactional memory I learned first
learnt about it when I went to a talk
about team houses talking about
transactional memory in Java we
transliterated it into Haskell and
discovered we try no else and we're now
busy putting those back into the
imperative so there's a very good and
interesting dialogue that takes place
across these hours so I think this is a
good picture to bear in mind so here we
are so this is the end of the what I
wanted to say about laziness just to say
that vindicates um but relentlessly
pursuing a single goal of purity is is
at least something that's worth trying
right and to see where it takes you and
may lead us to interesting things and if
we were to think about what the next
what the next Haskell might be like I'm
actually less personally I'm less
stressed out about laziness than I was
its purity that I really want these days
and I think that the way I put it is the
next ml will be pure and the next the
next Haskell will be lazy if there is
indeed a next on either of these things
and I do think also the imperative
languages are likely to embody more and
more checkable safe subsets of various
kinds okay so much about that now one
little bit about type classes which are
Haskell's the second major innovation
that I want to speak about so type
classes are than something that they're
a unique to haskell this really did come
whether phil Wadler and steven block
invented them at exactly the same time
that Haskell was being born so it was an
amazing serendipitous piece of luck here
in just in two slides is the way that
type classes work for those of you have
not seen them before the idea is that
you may define a a class this is not the
same as a class in an object-oriented
language so you should think of this as
saying a type a lies in class eeeek
it has an equality method whose type is
a to a taboo and then we can say we can
declare that the type int does indeed
certain ally in the class int by
witnessing the fact that there is an
equality method namely the primitive
equality on integers and we can say that
the list type so square brackets in
Haskell means list we say that the list
of a is indeed an equality type if a is
in equality type so that's the this says
if a is in equality type then list a is
in equality type and here is the witness
there is the equality on lists which
uses in its right hand side the equality
on elements as well as we cursive ly
using the equality on lists and then
functions which might want to use
equality a function member which says is
this element a member of this list will
have as part of its type something that
says I the type is for all a that lies
in the class eke a to listed a table and
why does it need to line classic because
it uses the equality operator work down
here in this equals equals bit okay and
so this was introduced initially really
as a way to deal systematically with
things like equalities and civilization
readin show and numerix those are the
main motivations for introducing this
idea now one of the beautiful things
about type classes is the way that they
have a very direct possible
implementation called the dictionary
based implementation so there the idea
is that for every class so this is just
the translation of the previous slide
for a big class I get a data type whoops
there we are a data type so here's a
data type that is the data type of
equality dictionaries so this is a
method suite of the operations of the
Equality type and there's only one in
this case a to a table then an inference
declaration that says equal int lies in
Classique turns into a value of type II
Kent here and the value is well it's
just the equality method for integers
and the witness that doin bar on the
previous slide that we're here we
claimed that lists were an equality type
and that turns into a witness that it
does and the witness is a function from
a witness that a is in equality type to
a witness that equality Cove a are so
that list of a zone
ctype and so it goes and then a function
and like this member which needed its
argument to be an equality type takes an
extra value argument at one time which
is passed at one time which gives it to
the Equality method so you can think of
this as being I passed a member the
Equality function so the way to take
equality on the the elements of the list
so there you are
that's tight classes in two slides now
you know well so Phil and Steve
introduced this and we just adopted it
with wild enthusiasm because it was it
was just so beautiful it's been like
functional programming itself but felt
obviously right and instead of various
ad hoc ways of dealing with
serialization in numerix it was a nice
clean systematic way so we're very
enthusiastic then I started to try to
implement it and at that stage I fell
into a pit of despair because it's
actually quite difficult to implement
type classes well at least when you do
it for the first time so it took several
years of hacking so hack and then we
built a compiler and now we think hey
what's the big deal all right so it's a
it was a it was a distinct cycle I
suspect this cycle may be familiar to
some of you who have implemented
language features as well now type
classes as it turned out approved
extremely convenient in practice so it's
not just equality in numerix but also
monadic operations time varying values
pretty printing collecting reflection
generic programming it's proved to be
much more fertile and we intended and
incidentally yes in Haskell my 17 can
definitely be your 23 but even the
littles are overloaded so in a scale if
you say 17 at type per t1 and 17 at type
t2 are not the same thing at all so we
have dynamic literals even so I thought
I'd just give you one quick example for
what and a very beautiful application of
type classes called quick check so this
is more than just serialization I mean
so quick check is is a program that does
it's a hospital library it's just a
library not really an application at all
it's a library that does software
testing random testing and the way it
works at this you were you have some
function you've written and
like reverse and you wish the tests and
property of reverse so the first thing
you do is you write a Haskell function
called proper F in this case it doesn't
matter what it's called and it takes a
um it takes a list of integers to a
boolean and it returns it returns true
if when you reverse the list of integers
twice you get the same thing as you
started with right that should be a
property that reverse has and you can
write lots of properties here's a
property involving web app involves the
property that involves reversing and
appending so you can write these
properties and then you just call quick
check applied to proper F so prop Rev is
this function so you apply quick check
to the function and quick check out
there's a little pause and it says fine
I passed the hundred tests and I
couldn't make it return thoughts right
this function should always return true
and then I can call quick check on top
web app even though it has a different
type and again it does a hundred tests
and says I couldn't make it return false
now clearly something interesting is
happening because quick check has to
make test cases to work and the test
cases themselves in the case of the
first one it had to make test cases that
were list of integers in the second one
it had to make test cases that were
pairs of lists of integers bizarre two
arguments so here in one slide is how
quick check works so this is is a lot of
accelerated introduction to type classes
so were quick check how they remember
quick check took as its as its argument
the function that you want to test so
it's the type of quick check is it takes
an a to an i/o unit but the ED prints
its results that's the i/o unit part but
the a must lion testable or just here
I'll just write in class test and class
test has a single function in it called
test with a two random taboo why why the
random well we need to feed it random
numbers in order all quick check will
need to feed it random numbers in order
to test it at different values now the
interesting thing is how do you test a
function value because remember the
arguments were going to give it to
functions so we need to make tests we
need to give an instance for test a
Talos well here it is here is the
instance the test the the blob of code
at the bottom here that says test AI OB
is the arrow case the arrow instance for
test so how do you test the function
well you can test a function if you can
test the result of the function that is
if you can test B and if you can
construct arbitrary values of the
argument type that's the RBA what's RB
RB is a class that says if you're in
class RB then there's a function RB
which given a random number will produce
a value of this type so now how do I
test the function how do I now I've got
to provide a witness for test right so
test gets an F a function f that's this
guy and a random number that's R and how
does he do it he recursively calls tests
on the result of applying F to a test
argument what test argument answer r be
applied to a random number oh and I
better split the random number I get
into two different ones so I need a
rounded number supply that I can split
into two in that beautiful you all
understand that but believe me it's
worth going to read the quick check
paper because this is this is this is a
programming pearl like that if there's
this the whole idea of quick check fits
in in just one slide
now goodness for the shift key oh right
yes yes it's very important Catskill is
case-sensitive okay so enough about
enough about a bit particular example so
so one way in which type classes gave
rise to an unexpected fertility was the
in the applications that people found
for them but also they gave rise to
unexpected fertility in the variants and
developments that have arisen since and
I'm you'll be relieved here that I'm not
going to go through any of these except
to say that they're most they're
sketched in the paper with forward
references to to further reading it's
proved to be a much sort of richer scene
than we had initially anticipated and
that was that turned out that's really
very exciting thing to think to happen
to you when it does so here we are
here's a summary of the the Tuck class
story much more far-reaching than we
initially expected and and so somewhat
influential you know has been adopted by
verse other languages but there is that
there is the debt the lurking danger of
heat death right that's that's this
picture really we the moment where it's
still in the stage in which we keep
lobbing more and more elaborations into
type classes in the hope that eventually
will understand some understand some
grand unified theory that will make it
all simple again but the moment who are
still at the stage of making it more
complicated and so it's unclear to me
what
the long-term in 20 years time what will
we say about type glasses I don't know
but it's been a lot of fun meanwhile
okay so here we are but about five or
ten minutes nine good good
nine exactly so I just wanted to say a
little bit to wrap up with about the
process and community of the bunch of
people who've been involved in haskell
so one thing to say is that haskell is a
committee language and it's common to
expect committee languages to be very
burlock and complicated because they're
composed of lots of compromises and in
some ways haskell is but then we
certainly did not have a supreme leader
we did not have a Linna store val-d'or
biana Jewish group to to to to be
clearly in command however much I'm sure
you Fionna you don't feel in command
but-but-but-but-but it but in many other
communities there has been one person
who's clearly been that the lead player
and it really hasn't been a lead player
in the haskell community a single one
and that's that's that's led to a lot
lots of give-and-take however the group
that we that we designed haskell with
did have a kind of fairly clearly
articulated goal and they were fairly
motivated and we're pretty much just at
each other so that was that's a not
necessarily easy to reproduce but that
turned out to work pretty well and we
try to resolve unnecessary conflicts by
always having an editor who was the
current person who would cause some
caused discussions to move forward and
force us to come to a conclusion of some
kind and for syntactic matters we
appointed a person who wasn't
necessarily the same as the editor but
often was called the syntax are and the
syntax are was empowered to make
arbitrary decisions they seldom did but
in principle they could because we all
know the syntax is the the place where
committees really start to lose their
way curiously though people often did
say how much they like Haskell syntax
despite its committee origins another
interesting thing about the committee is
that we disbanded ourselves in 1999
pretty much as soon as Haskell 98 was
out we disbanded ourselves so now there
is no Haskell committee the idea being
we were giving the language back to the
community and waiting to see whether
enough of them will step up to do to do
something further with it so there
really isn't a command and control
economy even of the democratic kind we
had before
I wanted to say a little bit about
language complexity as well so we talked
quite a lot about language complexity at
this meeting and I've written down here
what Nikolas said on one of his slides
yesterday that languages are full of
dispensable complexity and I think one
could criticize Haskell for being full
of dispensable complexity right there's
certainly a lot of superficial
complexity all this type classes stuff
there's lots of redundant mutually
redundant syntax the data types are very
rich as a so it goes on and on that
Haskell report is pretty fat it's not as
thin as many of the languages that we've
talked about today but at the same time
there is it is underpinned by some
fairly deeply held principles that I
want to say and something about in a
moment but not so deeply held that we
actually wrote a formal semantics that
was one of our initial goals and it's a
goal that we object ly failed to meet oh
yeah I don't even know whether I regret
to say that or not it's er I would love
to have a formal semantics but it does
it makes you it also makes you less
nimble so it is what it is we don't have
a formal semantics I'm not proud of it
but I'm not too ashamed of it either
what do I mean by these deeply held
principles I really do think that the
people who built Haskell did have quite
strongly held principles for the way you
should build a programming language but
I then tried to think alright so how
could I articulate that in a concrete
form so this is the best I can do this
is gh C's intermediate language for
Haskell so we translate Haskell into a
small intermediate language that is
essentially the lambda calculus in the
style of system F so it's an explicitly
typed lambda calculus with high order
polymorphism it's it's something very
close to the system F a little bit more
developed and I'm not really hiding
anything from you here I'm showing you
pretty much the whole thing so in this
is the data type of expressions and it
has is it eight constructors and I've
even included a very seldom used one
called note which just adds some 'zv and
of annotation you could drop that
without any loss this is the whole thing
the only thing I'm not showing you here
is the representation for types which
show up here and they're like what how
is a type represented well it's another
half-dozen constructors and that's it so
this is a great sanity check so this is
the way I think of it we start off with
a very large language the Haskell
gorilla and we squeeze in
making him smaller than smaller into
this very easy sort of shrinking The
Incredible Shrinking gorilla until he
gets into these eight constructors and
anything that doesn't fit into that plan
doesn't get in so that's what I mean by
a kind of underlying conceptual
simplicity is that the everything in
Haskell could be explained by this
rather elaborate translation but
nevertheless the translation ultimately
gets you to this incredibly small
language and Duty's survive with this
very small language for 17 years now I
think we're one of the first ones to use
an explicitly tight system f-style
intermediate language and the same one
is still doing the job we have
elaborated it a bit recently and to deal
with type families I want to say just a
little bit about Haskell users so when
the Haskell users are not a very large
group I mean it sort of is now thousands
but not millions of people and they're
incredibly tolerant so the my my
favorite episode was when we
accidentally released a version of the
Glasgow Haskell compiler with the
unfortunate property that under some
admittedly slightly obscure
circumstances if it found a type error
in your program it would report the type
error and delete the source code
I don't want to see that program again
I'm gonna make sure I don't - so you
would think that what would happen is we
would be buried in mail saying you
bastard you've deleted my source code
but what happened we just heard some
mild-mannered er sort of email across
the house got mailing they said by the
way I've made this script that goes CP -
art move my source capture a fresh place
before I compile and then everything's
fine though and you know by the way GHC
developers you might like to know so you
can fix it the next time
now those are the users that I like you
know they're also they're a bit geeky
but then they're very friendly it's it
was some of that something that people
notice on the haskell mailing list this
is they're very friendly and this kind
of user base makes haskell quite nimble
another another property of these users
is that if you implement a new feature
perhaps slant somehow slightly being
breaking backward compatibility do they
bitch about the the lack of backward
compatibility
no no they're like hyenas with red meat
they just fall upon the new feature and
start to use it they complain that it
doesn't work of course but that's a
problem I don't mind so my motto is here
avoid over success at all costs right
you want you don't want to have too many
users but they start to tie you down
however Haskell has perhaps
unfortunately become quite useful over
this time right so and it is it so it's
grown increasingly important libraries
and there's a whole bit about that in
the paper increasingly important foreign
function interface and an increasing
array of features to do with concurrency
and asynchronous exception and
transactional memory and data
parallelism all kinds of things that are
enormous fun but together they make the
language large and complicated so we
undoubtedly are beginning to pay the
price of usefulness and I think it's I
think pretty much any really useful
language begins to pay this price off to
a while so I should wrap up I think I
think Haskell doesn't really meet iana's
criterion that a language to succeed
must be good
must be good enough on all axes Haskell
in effect pursues an alternative plan
that says let's be really good on on one
or two axes let's take a few beautiful
ideas and pursue them relentlessly and
see where they take us not because that
is the only way
to write programs because it's an
interesting way to write programs and
it's a fun way to write programs and so
if you like in the end is not so much
that we want you to write Haskell
programs we want to infect to use of
other invasive analogy infect your
brains rather than your your hard drives
it's the the marketplace of ideas that
the David was describing earlier so I
think that um thinking back I was trying
to think of what you know what does make
languages succeed and I think luck play
does play an important part right so
technical accent in the language design
is clearly young desirable but it is
plainly neither necessary nor sufficient
sadly not necessary and perhaps also
sadly not sufficient to succeed on the
hand luck really is definitely necessary
now I think we had our our slice of luck
this this moment when we all came
together with similar goals and a level
of trust it's not easy to reproduce and
indeed there's a process going on at the
moment to to make a sort of new version
of Haskell 98 4 which is which is which
is working more slowly partly because
the particular combination of
circumstances and cannot be just
conjured up by wishing that it could I
also want to finish just by saying that
Haskell is great fun it's rich enough to
be useful but the thing that I think is
really important about it is that it's a
kind of playful language and it
encourages people who want to play and
so I've given some examples here about
the sort of playing that people do
embedded debate domain-specific language
and cunning type programs but it's a
language in which is fun to write
programming pearls in which people
really do start to do bits of
programming as an art form write papers
about them and even get them published
and I think this is good not just
because it's fun but also because play
leads in the end to to new discoveries
so I think languages that are playful
lead on on to to learning new things so
that's really all I have to say I'm
going to finish this by putting up a
slide with the the names of the Haskell
Committee because I've not mentioned
that by any means all of them by name
but they deserve all of your
appreciation for the hard work that they
put into this language thanks very much
you can answer questions
okay David Unger IBM again then I know
I'm up here a lot but I'm just enjoying
this so much and everything stimulates
thought I've got to go learn Haskell now
for sure so so here's my question as I
understand it there's this triple of
values types that describe values and
perhaps type classes that somehow
describe types and every time I hear a
number like that I wonder you know is
there a fourth is there an infinite
number is that a problem is that cool
should there only be one thing two
things what can you say about this
why Omega goes back to Sharad and others
that in fact has this infinite hierarchy
and it's been around for a while and
people played around going up you never
get higher than three your values types
and cons of people never really need to
go higher than up so it's three because
no use has yet been discovered for
number four right it's also known how to
go up infinitely high if you want to but
people tend not okay okay so well having
a particular class to be able to
abstract over classes but next there's
somewhat geeky application so is that
number for example I'm not sure whether
it is it just seemed relevant thing to
say before thank you so andrew black
portland state university we have you
know an emerald you can have object
constructors and you can have or create
constructors inside object constructors
and you can have objects constructors
inside object constructors as many
levels deep as you like but we never
found a use for more than three in the
comments
no I think that was a very good comment
thank you back you can try to look like
just record construction to me so yeah
in since they are but we've met we the
reason you nest them is to get
parameterization and we had we never
found a need from one three my name is
Paul mcjones currently with Adobe I
worked with John Backus in 1974-75 on
red and I think the monads that you have
are a very good solution or interesting
solution to our the problems we had then
IO or the lack thereof the other thing
was in those days John was extremely I
think this probably persisted against
any kind of variables bound variables
lambda variables and variable free
programming was was a term he often use
I'm wondering if you guys ever got a
chance to you know to talk to him you
know and show him your style of
programming which which uses variables
in a way that that seems pretty
controlled to me I just yes so I'm if I
respond to that briefly so I um I
studied this verbal free style
programming and discovered that I was
getting myself tied in knots doing
indirect plumbing and just saying I want
to name the variable that I'm using seem
to be so much easier so any photo so I
think maybe he wasn't he wasn't so right
about that he was wrong but in education
although it was and and so and we talked
I mean John Williams took that work
quite a lot further subsequently didn't
II do you want to add anything to that
so I had the good fortune to be a
visiting researcher in John Baxter's
group in 87 and we had many very
interesting discussions not so much
about variable freedom more about lazy
evaluation and he was very interested to
know whether or not the next version of
FP would benefit from being lazy but
variable free programming is it's
popular among some haskell users and in
fact there's even a bot on the haskell
IRC channel that will take novice
programmers pieces of code and translate
them into variable free code which
unfortunately nobody that understands
it's certainly a thought exercise we
just still enjoyed okay thanks very much
want just one last observation another
person who is young at the time younger
and influenced by John's paper was Alec
Stepanov who went on eventually to do
imperative functional programming or
something in the STL Ben Zorn Microsoft
Research so one of the inspirations for
you was eliminate or was not back as his
view on eliminating the von Neumann
bottleneck okay so and you talked about
in the beginning of sort of this this
phase there was a lot of enthusiasm
about computer architecture specifically
and and the merging of these concepts
around functional programming and
computer architecture can you tell us a
little bit about how architecture has
evolved and whether you know this your
experience with Haskell it makes you
feel like we've moved further along this
line toward eliminating them but the
bottleneck oh so when I didn't say very
much about this but a bit on the slide
about many architectures I heard mostly
doomed and so I think what we discovered
is that it's it is though it is
attractive to think about making
computers specifically designed to do
and graph reductions say it's a bad
mistake so partly there's no future in
building hardware that interprets stuff
that you could have just compiled in the
first place right anything even do
statically would be good and our other
problem was of course that internal was
making processors faster so quickly that
anything we built was looked slow either
spec even if it was parallel looked slow
by the time you know that a year or two
had gone by so things have actually
changed the bit now so firstly I think
we're now we don't the let's build a
completely different architecture IDE
idea so I think that you know we've got
to start with a conventional von Neumann
architecture just use lots of parallel
machines so it's actually pretty
conventional parallel architectures but
I still think and I didn't say this in
the talk that probably the best hope for
programming large-scale parallel
machines is going to be is going to
involve the very capable control of
effects whether that means a purely
functional base with controlled effects
like Haskell or more mainstream
languages with carefully controlled
effects I don't know but
I'm pretty sure the purity is the way
the future and I think Lawrence night I
had a slide that said something
very like that Kim Bruce Pomona College
I was surprised that you said that no
one's ever written down a formal
semantics particularly given the
composition of the committee and the
purity of the language so I'm wondering
was there something hard about writing
it or just never get around to it
well first of all of course Simon
oversimplified and there were formal
semantics for various fit so for
instance there was a formal semantics at
the core of the original type classes
paper which then got lab rated into
another paper which was here's how you
do type classes less formally and much
closer to what we do in Haskell which
was a great guide to what goes on Simon
talked about the core language which has
an easy formal semantics and so on but
no sign ever got around to doing a
formal specs for all the things he added
it into GHC well even for them there's a
form of semantics for quite a bit of
that too so there's a yin they say in my
paper about tackling the awkward squad
there's a fairly well-developed
operational semantics for IO and foreign
function interface and exception
handling but but what's missing is is
something that puts all of these pieces
together into some giant humongous thing
and in fact I'm not the reason I said
I'm not so ashamed is because putting
the doing the the essentially the the
proof engineering thing and putting it
all together it's not clear what the
payoff for that is big enough to justify
its price but Phil you're absolutely
right I did did oversupply there's lots
of bits are formalized so one place
where for semantics did play a role in
the original design was in the semantics
of pattern matching in equations and we
had many different proposals for what
pattern matching should mean one of them
involving parallel execution and we had
a discussion over whether or not the
that proposal would have pleasing formal
semantics I got the job
one night going home and doing the whole
work of writing one I came at the next
day I said this just impossible I can't
see any way to give this compositional
semantics except by seeing this
in tax in the semantic values and I
remember people around the table say oh
well that's alright then we've got a
formal semantics but I later got the
homework of incorporating Haskell's
guards into that semantics and it just
became so incredibly complex that
everybody decided it must be a bad idea
thank goodness guy Steele Sun
Microsystems a quick comment in the
question my comment is that Common Lisp
and other lisps have an abstraction
feature called back quote I've never
seen it nested deeper than three there
may be there may be some deep principle
lurking here for graduate student look
at the other thing is that the one time
I went in to use Haskell to write a
serious program I was rudely shocked to
discover I really couldn't just drop in
print statements to debug I had to take
unit testing very seriously could you
make just a couple of comments about
debugging and performance profiling in
in Haskell and issues like that okay
well we're testing is concerned then
purity is immensely valuable I'm sure we
agree about that so I don't personally
find testing and debugging Haskell
programs particularly difficult where
performance is concerned they're one of
the big performance problems with
Haskell has been space use it's been
very hard to predict and initially at
least very hard to understand but out of
that came a SKLZ heap profiler which i
think is very good empirical feedback
about space behavior of programs and
that work has enabled Haskell
programmers to understand the space
behavior of their programs rather well
empirically and it's also been copied in
other programming languages so I think
that's perhaps another case where purity
in the end led to a good solution and
also to many interesting research
challenges in some ways
Oh
so I wouldn't have done that except we
are resuming in 17 minutes
have a good quick break</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>