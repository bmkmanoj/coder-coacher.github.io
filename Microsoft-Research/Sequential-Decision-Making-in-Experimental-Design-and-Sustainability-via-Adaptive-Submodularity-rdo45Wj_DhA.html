<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sequential Decision Making in Experimental Design and Sustainability via Adaptive Submodularity | Coder Coacher - Coaching Coders</title><meta content="Sequential Decision Making in Experimental Design and Sustainability via Adaptive Submodularity - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sequential Decision Making in Experimental Design and Sustainability via Adaptive Submodularity</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rdo45Wj_DhA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
start okay so it's a pleasure to have
with us Andres cross from et ha now in
Furyk andreas is sort of well-known in
the machine learning community for his
work on sub modularity and he has given
a set of like his tutorial in was it
didn't I CML 2008 was considered as one
of the best tutorials on this subject in
in the machine learning and allied
communities and so he was at Caltech
after graduating from his PhD and then
moved to ETH Zurich last year and so he
is now a European researcher in some
things and he's coming to terms with the
pros and cons of the European academic
system so he's not going to talk about
that but he's going to talk about his
new work on sequential decision-making
so okay thank you very much push meat
for the kind introduction
it is much for inviting me thank you all
for coming it's a great pleasure to be
here that's just always so much exciting
work happening at Microsoft Research and
today I'd like to tell you a bit about
work that we've been doing over the last
two years or so on sequential
decision-making experimental design and
sustainability via adaptive sub
modularity and this is joint work with a
number of people that are acknowledged
as we go along and particularly I'd like
to just a single or Daniel galavan hope
they do key role in all the projects in
this in this work so this work is
motivated by one what I think is one of
the central challenges in the eye namely
how should we make decisions and complex
highly uncertain large environments
particular environments that are
partially observable then we have the
opportunity to gather some information
in order to make effective decisions and
clearly there's been a lot of very
exciting work in this area but the
general purpose methods that have been
developed often are very difficult to
scale to even medium sized problems so
wide I would like to talk about that
this work is a new new tool that we've
developed that allows to characterize
certain types of sequential decision
problems
very simple methods performed prob'ly
very well and a lot to tackle really
large problems and I'd like to introduce
this tool in particular in the context
of two applications involving sequential
decision making maybe an experimental
design as well as a sequential decision
problem in conservation management so I
get both into the problems as we go
along with the talk before I get to what
a Baptist modularity yes I just like to
briefly bring everyone up to speed about
the classical notion of some marginal
functions that we are all talking about
some topic and I'd like to do that in a
context of an application that I've
spent quite a bit of time working on
maybe the problem of optimal sensor
placement okay so suppose we'd like to
put a bunch of sensors in the building
in order to monitor the temperature to
fix up or figure out of this fire or not
if you would like to optimize such a
placement we first need to somehow be
able to model how useful it is to have
sensors at certain locations we need
some model of the world and in general
the use of probabilistic models that we
model the phenomenon we try to estimate
as random variables for example we may
however an in variable y es that models
the unknown temperature at location s -
we have a bunch of those random
variables we can't observe themselves
but you could put some sensors that
allow us to obtain noisy observations of
what the true temperature is so for
example if you have a sensor at location
s in the environment then it would
provide this amazing measurement excess
of the true temperature YS at that
location okay and then these are random
variables so they're dependent
statistically and the model the
correlation usually was some kind of
stochastic process or probabilistic
graphical model that allows to encode a
certain statistical dependencies about
how temperature different locations say
especially correlated certain aspects
about the signal-to-noise ratio of the
sensors now once we have such a model we
can use it in order to quantify the
benefit of obtaining measurements from a
set of locations and one common tries to
do that is to essentially look at the
expected reduction in uncertainty by
obtaining measurements so we can
quantify the utility of obtaining
measurements at a set a of locations
when we consider placing our sensors as
the difference between the uncertainty
that we have about the temperature what
you try to estimate before having any
observations - the uncertainty that's
left
after we've made our observations and
this is taken with respect to taking an
expectation with respect to the
observations that the senses and I click
on it I'm going to make now this gives
us an objective function that takes a
set of locations and quantifies how
useful that is for the purpose of making
the predictions and that allows us to
compare different placement so for
example if you place the sensors maybe
as illustrated here then we would be
able to get pretty good prediction
accuracy pretty much all over the place
obtain very high utility but if you
place the sensors very close together
then they provide redundant information
and you don't may reduce the uncertainty
as much okay so now there's an objective
function that you can actually try to
optimize for example we can ask well
what's the 10 best locations where I
should place my sensors now
unfortunately that has an empty heart
problem so for very loud large problems
I may not be able to certainly solve it
optimally but at least I can try to
solve it pretty well so I can try to
look at how some heuristics work of
solving this problem and arguably the
most natural heuristic for this problem
is a simple greedy algorithm to just
starts having no sensors and it
typically adds the sensor at the
location that provides as much
information as possible over the sensors
that are you right so I'll place one
sensor at the time until I've placed all
K of those but I never changed my mind I
never made that move any of the sensors
around that I already have have placed
ok so now this is not necessarily going
to give us the optimal solution so this
problem that we can ask at least how
well this is algorithm do and we can do
some experiments but if you take a small
enough problem you can actually find the
optimal solution through exhaustive
search or some more involved
branch-and-bound strategies and turns
out oftentimes the greedy algorithm gets
us very close to your solution ok so
that's not just on this example as a
whole bunch of different problem
instances that we try and so the
question is has any theoretical
justification of any would expect the
greedy algorithm to work well and it
turns out the key insight to analyzing
the performance are two natural
properties so the first property that's
very intuitive it's just a monotonicity
property if we have a placement a
consisting of two two sensors x1 and x2
and we fee add some sensors to it and
compared with placement be
these five sensors right three four and
five got added to the ones we have then
our utility did not decrease right so
formally whenever we have a survey of
sensors we consider adding an element to
the set of sensors our utility cannot
decrease right and so that's just
written in equations here and so it's
useful to think about this difference
here how much is the function increase
by adding in elements as some kind of
discrete derivative if you wish so how
much does the function increase and I
use this notation Delta s given a
because we're going to use a generalize
that notation later on and some others
it just means these marginal gains are
obvious long-
okay so this is very very natural but
adding sensors doesn't hurt but that's a
slightly more supple condition but it's
also very natural
that's a diminishing returns condition
so if you compare those those two
placements a and B in a V you have two
sensors by one and two and again
placement B we have three more sensors
we consider adding one more sensor to it
right maybe the sensor s here then if he
added to the first placement begin a lot
of additional information but if he
plays the same sensor to exactly the
same location of placement B you only
get a little bit of additional
information yeah decreasing marginal
gains and that can be quantified using
the common Atari illusion of super
marginal functions so a set function f
that takes sets as it takes sets of
items so it sets of locations for
example next into the reals is called
some modular if whenever we take is that
a that's contained in some larger set B
and we consider adding an element to
either of those sets then our function
increases more if the add s to the small
set then if he added to the large set
right which in our notation just means
that the marginal gain of adding s to a
is greater than or equal to the marginal
gain of adding s to B I got the notion
of super modular functions and it turns
out that for mutual information at least
if the sensors provide conditionally
independent noise one can actually prove
that information gain is monotonic as a
modular so satisfies those conditions
and why is that useful well it turns out
that it's known that a function
satisfies those conditions then the
greedy algorithm works well so the
seminal result due to an EM house that
I'll who proved that the greedy
algorithm is going to give us in the
optimist
so its value of things at least a
constant fraction of about 63% of the
optimal value okay and moreover for
optimizing mission information this is
actually the best that you can hope to
get unless P equals NP so in some sense
it's the right kind of algorithm for
this problem okay so this is a very very
useful property in many applications
that's outside of computer science we've
been using it in a number of
applications related so optimal
information gathering in placing sensors
and buildings in drinking water
distribution network to detect
contaminations and and other kinds of
problems but all these problems that we
looked at are non adaptive if you wish
so you commit to all the observations
that you're gonna make before you
actually obtain any measurements there's
no feedback
it's an open-loop decision problems if
you wish I think about placing sensors
you commit to all the locations so you
want to place those sensors before you
obtain any measurements
that's what I mean by non adaptive many
applications you actually would like to
take observations into account in order
to make better decisions
one example is medical diagnosis right
supposed to be a veterinarian we would
like to decide how to treat this sick
puppy here right so what you may want to
do is take a test
maybe measure the puppy's heart rate if
the puppy still has a heart rate then
you decide what next test you may want
to run right maybe take an x-ray
depending on how that looks we decide on
the next test that we should write and
so on right so we end up with a set of
tests that we perform but the set
depends on the observations that we've
made along the way okay so sometimes
depending what you see you take a
different path down the stree here that
results in a different set of tests that
you would like to write okay right and
what we like to do is we would like to
come up with a strategy by the policy
for testing in order to decide how to
treat the patient while minimizing your
cost okay now here we are no longer
interested in just coming up with a
single fixed set but rather a decision
tree a policy it's a planning problem
it's a sacral decision problem okay and
so on so modularity is a property of set
functions whether functions
and tell us how useful those sensor the
electron ascenders is there some notion
of a property like super modularity that
allows us to characterize when simple
algorithms work well for such sequential
decision problems such successful
planning problems that's exactly the
question of the asked in this line of
work and I'd like to tell you a bit
about what we have in development in
this context ok so let's look into this
problem a bit more let's well as a
concrete example this diagnosis problem
here and let's model it
probabilistically right so we have a
random variable wider describes the
state of the patient that's what were
you interested in and then we can
perform tests the outcome of which is
again modeled probabilistically as
random variables x1 through xn and for
now for simplicity let's just assume
that the outcome of those tests is
deterministic it's just a fixed function
of the underlying variable Y ok so why
is this useful well let's consider the
states that Y can take what's the set
here right so each of those different
pictures here's one possible deceased at
the top you can have and now suppose we
run a test so you make an observation
for example we observed that the puppy
has fever right in this case we can
eliminate some of the hypotheses that
are inconsistent with that observation
we can cover those hypotheses now again
if you figure out for example that the
puppy does not cup you can eliminate
some of those hypotheses ok so we go
ahead and cover those but the problem is
that if we consider a new test that we
haven't yet executed you don't know yet
what you're gonna see but it could
either come up soon or could up kind of
one right or maybe any other finitely
many different discrete labels items
that comes up so oh maybe we would
eliminate those two hypotheses but in
turn if it came up I came up one we
would eliminate those hypotheses ok and
so what we would like to do is come up
with the sequential testing strategy to
eliminate all but the correct hypothesis
using as few trials as possible ok
and HeLa clearly write to the value of
the test it's depends on everything that
we already have seen in past and you
have to model at all ok so let's start
to formalize the problem with it so you
need to formalize this process for
making observations taking them into
account in order to make better actions
so here's the problem formulation if you
look at so
again are interested in trying to select
out of a set V of possible items and
these items could correspond to medical
tests that we want to perform
experiments that we're going to perform
but also more general actions you can
perform now these items are associated
with some observations that you model as
random variables we have random
variables X 1 through X 3 X n they take
values in some set o of outcomes which
we for now assume it's discrete but
nothing else and then we have some
objective function which both depends on
what we've done so what items we've
picked and what the state of the world
is and the state of the world is fully
characterized by the states that all
these random variables take which is
unbeknown to us okay so with the set up
we can now look at policies and the
value for policy right the value for
decision tree so policy is just what you
think it is it's just a decision tree
that tells us what we should look at and
depending on what oxidation we get which
next item to pick depending over to see
it aside in which next item to pick and
so on so it's a tree that looks like
this one and depending on VC we take a
different path down a tree select the
different set of items and in this
notation here just use this in patient
PI of X V to denote the sets of items
you're gonna pick so these three orange
northerners illustration here if the
world is in state X V so if the world
since Steve XP they're gonna get value F
of that set right and this assignment of
those variables but you don't know this
C of the world so we're gonna average
according to this distribution P over
all states of the world okay
this allows us to quantify the value of
any candidate policy any decision tree
highest policy PI of X which just says
this is the set of all tests I execute
if the worlds in this particular state
okay
so this is the first value of the policy
and we can try to find what's the
optimal policy right that's the problem
I'd like to solve right now I can see
what's the optimal policy pi-star that
maximizes this value subject to some
constraint on the cost may be that no
matter what policy consider I can pick
at most key items it's this problem here
and well even just writing down a policy
of size K is exponential in K right so
this could be potentially very
problematic and it's clearly studying
strictly generalizes this non adaptive
setting because you have these
observations you need to take them into
account so this is clearly how problem
also in general if you don't make any
assumptions it's extremely hard to
approximate so the question is are there
certain settings where we can use simple
algorithms for solving these kind of
problems and again what kind of simple
algorithm could you think about well
let's think of greedy algorithms so
here's the natural adaptive greedy
algorithm for this problem so what does
it do well it works as follows so at any
time it already has made a bunch of
observations so I just picked some tests
a and it has observed the Associated
random variables that's what it knows
about the world okay
now based on those observations we can
quantify what's the expected gained
expected marginal benefits of executing
one more action executing one more test
okay so we can look at the marginal game
using a similar notation as we had
before right marginal gain of adding
observation s given that we've seen XA
when it's defined as follows so we look
at how much does our function f increase
if I add s to set a and the world since
state X V versus what I got before
before I didn't decide some now this
clearly depends on the state of the
world which is unknown so all I can do
is I can take the expectation with
respect to the state of the world but
clearly I would like to take into
account everything I know so it is
conditional on the observations yes
yeah so it means this really algorithm
implicitly defines the policy right so a
policy essentially takes the history
takes a bunch of observations we already
have an app's it to the next action that
we should piss off perform maps it to
the next item that you have to pick okay
let me describe what the policy exactly
looks like okay so this is the policy
that the Queenie policy but just starts
having selected nothing then it
calculates the marginal game this
quantity here with respect to every
single item that I took pick every
single test I could run okay
calculate all those really pick the best
one right and then I pick that item and
get to observe the Associated random
variable got to make the Associated
obviously okay now my observation right
set of observations has increased
recalculate the value of all these items
again greedily picked the best one and
so on
that's how you do it okay it's a natural
adaptive version of this video very kind
of a very natural setting well and we
can ask and then what circumstances this
is all gonna work well yes what what the
function capturing so I'll try describe
it in a lot more detail but in the
context of this diagnosis problem you
can think of this function f here
essentially modeling the mass of the
hypotheses that already have been ruled
out by the observation server you have
made which depends on everything else
okay so that's the greedy policy well so
we know that in a classical notion of
maximizing set functions if the function
is super modular and can just run the
greedy algorithm get a good solution now
we're in the process of actually
planning right selecting a policy the
question is and what circumstances does
this really policy here before
well okay and yeah yeah thanks I can
think of for this week exactly so you
can model the temple dependence you can
be independent according to the order it
doesn't change things but that's exactly
how you can think okay good so that's
the setting so guess right is there some
condition under which this greedy
algorithm works well and in terms of a s
and its natural analog of this notion of
sub modularity to such sequential
decision problems we call it adaptive
submersion arity and we call a problem
instance adaptive submodular if the
marginal games satisfy a similar kind of
diminishing returns condition so if we
fix any item that we consider picking
any medical tests that we may consider
picking and if you look at how much
value to be get given that I've made
observation sexy and compare that with
the gain I would get if I have observed
XP then the marginal gain given X is
must be greater than or equal to the
marginal gain given XP as long as X a is
what's called a sub realization of XP
and that just means that in XP I've seen
everything I see in X a I potentially
made some additional observations okay
that's the condition so whenever this
condition holds
I must have to have this monotonicity
condition with respect to the marginal
game that's adaptive modularity and you
also need an adaptive monotonicity
condition which just says the marginal
gain of any fixed item s given any set
of observations can't be negative
these are strict generalizations of the
classical notion of Manas the city and
submerged value for said functions with
no defined of a value functions in
federal policies turns out these
conditions are enough to lift up there a
lot of the results that are known for
its modulus and options so here's an
example it's the guaranty about the
greedy algorithm and so now if we take a
problem instance that satisfies those
two conditions and if you look at the
value of the greedy policy
then it's competitive it obtains it
comes in fact approximation with respect
to the optimal policy right
but in this sequential setting right so
it competes with the optimal policy
which could be this exponentially
complicated a large object that I can't
even write down okay yes next hop out of
the start
get back to this question means that a
is a subset of B and it means I've
observed the random variables indexed by
said a right in this case and I've
observed identical outcomes of those
variables realizations and maybe I've
observed some additional random
variables right and the values of those
observations can be arbitrary okay yes
so what I'm just so I describe the
greedy policy which is one candidate
policy one out of a huge number of
policies and this theorem is a statement
about the value of that policy so that's
why this notion of marginal gain is
taken with respects to expectations I
take the expectation about everything I
don't know conditional on everything yes
so you need to know a distribution or a
try distribution of all those random
variables so trying distribution of all
those random variables right maybe I
know a prior distribution of a deceased
is that the public can have okay their
sequence you can make even if you don't
have a prior you can talk about that
separately but for now let's assume you
have the system
okay that's the setting this is this is
one with salt but it turns out there's
actually a number of other results that
can be lifted from this classical
setting to this adaptive setting it's a
question of assumption props okay and
I'd like to show those in the context of
two applications and it's the first one
let's talk more about this for this
optimal diagnosis problem and optimal
experimental design business machinery
and so here's again just as a reminder
to setup that you're talking about right
so we would like to figure out some of
what is the state of the puppy right
what's the disease you'd like to figure
out on the state of variable Y you can
take those tests the outcome of those
tests is a deterministic function given
Y right so whenever have some
observations by eliminates certain
candidates values of this variable Y
okay now in this case it's very natural
to define the value of a set of an
observation as the mass of the
hypotheses that are ruled out in which
case the marginal gain of performing an
additional test conditional on some
observations it's just the expected mass
of the hypotheses ruled out if you were
to run test t and we already have made
observation sexy okay so we take the
expectation with respect to the outcome
of the test or just conditional on
everything that you see right and you
look at what's the expected mass of the
inconsistent hypotheses that are you
gonna get rid of that's a very natural
criterion that we want to optimize turns
out it's one that's known it's called
generalized binary search it turns out
in this problem is actually equivalent
to maximizing information gain
maximizing mutual information with
respect to Y okay what so it turns out
that this criterion satisfies this these
two conditions one tenacity and
similarity here's the proof on one slide
for the sake of time not gonna step you
through it the most important takeaway
message from the slightest but it's fits
on one slide all right if I wouldn't
take me three minutes to explain it I
want to save those to get to some other
results now this is nicer so now we know
it satisfies those conditions
that means that we immediately know that
the greedy algorithm works well for this
problem it turns out unfortunately
this application that's not in your salt
actually known that the greedy arrogant
works well for this problem the problem
is known as the optimal decision tree
problem in computer science and there's
actually quite a long history of work in
this the fact that the greedy argonauts
was when we discovered a number of times
the best we sold so far is that greedy
algorithm gives you a logarithmic
approximation in terms of cost right so
at most for our log one of our human
approximation is the best known result
so far turns out with this new analysis
you actually improve this slightly so as
you can prove it's a log whenever for
men you get rid of the factor four but
that alone is not so exciting but I
think it's much more interesting if you
get this result for free from this one
slide proof on the previous slide ok so
it's a very very simple consequence of
this characterization yeah this is a
very different setting we can talk about
that offline ok ok but in general
compressive sensing is not adapt
different so on so it's a very different
kind of set ok ok so this is one - all
so this is one right so you get this for
free from this one slight proof but what
what's much more exciting than that is
even that since we now have this general
perspective looking at those problems
you can try to tackle much more
complicated settings right in particular
all the results so far were predicated
on the assumption that the outcome of
the tests is a deterministic function of
Y ok so there's no noise but in practice
there's always noise these tests are
going to have false positives false
negatives and so on you have to be able
to deal with that ok so let's try to
formalize the case where we have noisy
observations but you deal with noisy
data and that actually makes the problem
a lot harder and one of the reasons is
that if you have noise then our
observations don't rule all type of
these to just make them less like ok now
what does this mean well and it means
that it's not even clear how I should
formalize the optimization problem right
because you
if I've seen everything and they still
have some uncertainty about the state of
the public saris let's resolve that and
let's take a decision theoretic
perspective so what we're really trying
to do is trying to gather information in
order to make optimal decisions okay
let's take a decision theory at exact
point just want to get enough
information to make the right to school
so how long would that home lies that
well from a decision theory perspective
would essentially say I have this
unknown variable Y right and I have some
actions so they can perform maybe in the
setting right so I can decide to either
and I treat the patient or not perform
surgery or not right and I have a
utility function which may be obsessed
that if I decide to not treat the
patient but the patient is sick then
that could have severe consequences
right but you can have some of false
positives or negatives or a different
cost and so on right and so if you knew
the state of the patient you knew what
is the optimal action but you don't know
what the state of the patient is so the
best we can really do let's maximize the
expected utility right we can try to
figure out what's the optimal action I
should take with respect to mine sir
right and also think about this from the
perspective of classification right I'd
like to figure out some ibadis to
hypothesis why what's the label right
there's some classification tasks the X
1 through X and other features that I
can observe right and all I care about
is making an optimal classification
decision my utility is just the zero one
box so I kind of said okay that's the
problem what to solve so in this
business why do we put ourselves in a
situation that we observed everything
run every single test still have some
uncertainty but there's some optimal
action that has minimum cost that's the
action I'm interested in so I would like
to come up with the testing strategy
that selectively gathers information
some point decides to stop and whenever
it stops it's guaranteed to take you out
to watch this gives you a very
well-defined optimization problem and
you can compare different policies with
each other and see how well they do in
terms of cost okay now this is clearly a
very important problem rights that's
received a lot of attention there's lots
of different ways of how one could solve
this problem right so for example and
could just try to use the results that
are known to work well for this
noise free case right but also there's
other lots lots of heuristics that have
been developed for this right for
example if you ask a statistician they
would tell you to do Bayesian D optimal
design so maximizing as a maximizing
mutual information with respect to Y
that's a very natural criterion if you
ask your decision theorist they may tell
you so maximize value of information
which is the expected improvement in the
ability to make the optimal decision
this is all standard criteria that a
user needed to be a day to day basis and
many applications turns out none of
those are adaptive Sumatra unfortunately
but alone may not be the problem right
it occurs just mean you're not studying
the right condition problem is this one
the problem is that you can come up with
very simple examples of this
optimization problem essentially all
those criteria can potentially perform a
beat really bad right so they can of
course it's exponentially larger than
the optimal policy so when you construct
the problem since in certain be right
and so we can talk about the details
later on it's a fairly right of course
somewhat synthetic construction in worst
case example and practice often they
work pretty well right but let's get
back to that issue right so have some
okay good so this is one so but now
since they can form our view badly let's
see if we can maybe come up with the
better day that actually is competitive
with the optimal solution and here's our
proposal to do that and the idea is to
take the noisy problem and reduce it to
the noise free problem that we know how
to help
now how me one wish to do that well
let's look at this setting so these are
the four diseases in some sense right
that we'd like to distinguish each of
them corresponds to a different action I
should take how to treat the patient and
now let's make noise part of the problem
let's create different manifestations of
those noisy diseases right that just
defer in terms of what I offer what
observations I would get if I were to
run certain tests okay so for example
there are two manifestations of this
orange disease here right in the simple
cartoon example the second test always
comes up zero
the test always comes up one but
sometimes the first up the first test
comes up sir or sometimes it comes up
one right but both of those are just
manifestations in some sense of the same
to see so I'd like to take the same
optimal action with respect to both of
those okay that is the setting and now I
can ask well how should I test in order
to figure out which action I should take
notice that since we now annotated all
these hypotheses with the outcome of the
tests you somewhere back to the noise
free case right we know exactly if we
run a test x1 which of those
manifestations we're gonna get rid of
right
okay yeah so let's get back to those
issues later okay
the right notice is purely a cartoon
picture okay okay this is any so so now
let's do the following so what you're
gonna do is we are going to group those
manifestations of those diseases
according so how we should treat right
so we have those groups of more of these
manifestations here right that belong to
the same optimal action okay good so
what you can do is you would like to
distinguish them so let's think of those
here as nodes in a graph and we have an
edge between any of those noisy
hypotheses right manifestations of those
diseases if and only if they lead to
different optimal actions that we should
perform right so you have this graph
here and the weight of the edge just
corresponds to the product the
probabilities of these individual
hypotheses and now in this situation if
I run a test so if I run X 1 for example
test 1 and I figure out X 1 equals 1 I
can get rid of all the notes that are
inconsistent with this observation now
we have to address that somewhat angle
around right so let's get a read of
these as well and notice that you're
done you've identified the optimal
action if and only if all the edges are
got right so that's the sufficient and
necessary condition to have identified
what's the optimal so it's very natural
as an objective function to consider the
total mass of the edges that we cut if
you make a certain cell phone
so instead of quantifying some of the
measure at the mass of the hypotheses
that we cut away some more quantify the
mass of the edges don't you come to me
okay
it's the idea turns out this criterion
is in fact adapt to some mantra and we
know that the greedy algorithm on the
problems so in fact we get the policy
that's that is competitive with the
optimal policy okay
it's the idea and I think it's a nice
tool this video some of the first
approximation algorithm for solving a
value of information in a fairly large
class of probabilistic models okay it is
defined in terms of the mass of the
links in this graph the result is
defined with respect to the optimization
problem I had several slides ago which
says I would like to test such that if I
stop testing I know for sure that what
optimal action I should take that's it
but the cost is not with the respect to
students actions
the cost is monetary cost of performing
tests right so every every medical costs
maybe every medical test maybe has some
cost
I'd like to minimize the expected cost
so this is really the quantity I care
about right so it's kind of an
interesting result right so so this
criterion is is some really weird
criterion really pretty crazy right why
should I care about this criterion right
so but it's the signed enemy but it has
these nice computational property so
it's adaptive semaj net and means it has
the property that if you optimize it and
make progress with respect to the
problem that you actually trying to
solve that's the end ok good so this is
theory how does it actually work in
practice and so we use this in the
context of sequential B's and
experimental design problems in B in
behavioral economics and this is trying
to work with : camera whose would
behavioral economists at Caltech and his
graduate students
and the idea is the following so one
premise of behavioral economics is that
people tend to not act rationally in
some sense at least they don't
necessarily obvious maximized expected
utility you've given choice between
actions with uncertain outcomes right if
you think about putting a subject into
the situation where they can choose
between true Gamble's and a gamble is
just sometimes the distribution of our
payoffs right maybe gamble a you would
win $10 with 70% chance lose $10 for 30%
chance in gamble be you win $10 with 30%
chance winner lose nothing with 70%
chance right if you maximise expected
utility you'd like to pick gamble a
because you get an expected $4 of gain
right but oftentimes in experimental
settings you see that that subjects are
risk averse they would pick a gamble vs1
right so because you know you would
never do thing but that's where is
different theories that have been
proposed that explain how people really
act and uncertainty including a constant
relative risk aversion portfolio
optimization normalized prospect theory
and so on and these are difference
sometimes descriptive theory is that I
hypothesize how people may behave but it
was never a comprehensive study really
tried to compare and contrast which of
those theories actually best describe
what people do in certain circumstances
and one of those challenges is that all
these theories have parameters and if
you're not very careful of how you
choose those parameters oftentimes they
would give you very similar prediction
so it's actually a very challenging
experimental design problem and so we
can now ask well actually design
experiments to best tease apart those
theories that's exactly what you do so
we formulate this as a base in X mental
design problem then we have a variable Y
that describes that indexes the theory
that we'd like to identify and the
variable theta that describes in terms
of the parameters of those theories
they're correlated wet and then what are
the tests while the tests are
experiments we can perform is
essentially presenting in the subjects
with a pair of Gamble's this
distribution so payoffs and ask them to
choose one of those right and then you
have to model likely
how and usually what when you do is you
some kind of logistic likelihood
functions so each of those theories from
you tries by Y and parameter settings
give a different value in some sense for
value difference for the pairs of
Gamble's and you would say if this value
difference is very strong very large I'm
pretty sure right I would like to have a
distribution like the value of
distribution of x1 that's close to
deterministic but if some of the
theories are ambivalent with respect to
both choices maybe I'm also not so very
sure about what answer this object is
going to give okay let's model in this
in the setting right so that's a very
natural way of formalizing this lesson
external from the sine setting and now
we can ask how do different theories
perform in this problem right and so one
issue is we need ground truth right so
at least something that we can do is you
can take this model simulate from it so
we pick the theory variable we pick the
parameters conditional on the theory and
then using those settings we essentially
simulate responses that subjects me
provides and we can use different
criteria for choosing those tests choose
the number of tests then we try to get
our best guess the net estimate for this
theory and see how high is the chance
that we got it right right how does the
accuracy increase in terms of the number
of observations we're going to get and
one natural baseline is just picking
tests uniformly at random this is this
yellow line here right and one would
expect it one should be able to do much
better than random it turns out it's
also pretty easy to do worse than random
so here's generalized binary search
which is this criterion which is
probably near optimal in noise free case
forms really poorly in the setting so X
X is the number of tests that are being
performed and the y axis is the
percentage of times that the map
estimates the maximum a posteriori
estimate with respect to the model
agrees with the ground
so this is purely in simulation
according to the model so I'll tell you
about the lab experimental next slide
but this is purely in simulation see how
well you can actually recover the ground
to thin according to this model and so
general spinal research is pretty poorly
this is uncertainty sampling which is a
very common active learning strategy you
would just pick the test that you are
least true about that you most most
uncertain about a performs really badly
so this is the standard active learning
method of you for example for active
learning in support vector machines but
what actually was very surprising
yes value of information which in some
sense is the right criteria that would
be the optimal criterion if you could to
look ahead but if you really optimize it
you perform very poorly okay now
fortunately you can also better than
random so the standards technique in
Bayesian exponent to the sign would be
maximize mutual information with respect
to Y and that does better than random
but still despite a bit worse than this
this new adaptive smart design criteria
ok all right so in this case I think
this is for to distinguish okay good so
that's the setting so this is what is it
in simulations right but actually gets a
significant improvement over these
standard criteria that I used ok so now
let me actually talk about the lab
experiments so our collaborators were
very excited about this and they're
running studies so we brought in 57
naive subjects who they ran through this
testing procedure so they asked these
questions selected by these tests chosen
out of a set of about 30,000 possible
designs or pairs of Gamble's that they
considering to select from and then
random are through a sequence of 30
tests then performed the maximum
posterior classification and this is the
classification that they got in yet ok
and so here each of those bars
corresponds to a different theory right
that's what you would like to
distinguish this prospect theory the two
left ones are two different versions of
that
mean value skewness two different
versions expected right Maximus expected
utility and constant relative risk
aversion and there's actually some
interesting results so one first of all
is that well expected maximize the
expected value got a fair amount of of
experimental support actually right but
also there's quite a bit of
heterogeneity in terms of the types that
subjects were classified into and it may
be the most interesting expanding was
that was no support at all for a
constant relative risk aversion which is
one of the actually most commonly used
hypotheses that describe how people make
decisions and answer so undergrads at
UCLA some of them so yeah maybe less
support for expected value okay good so
that's sunny but there's actually
something else you can take away from
this so when we first I tried to do this
they talk is about forty seconds just to
choose the next test so why is that well
you have to actually search through this
live space of possible designs right
what's the next section I want to run
and for all of those have to try what
happens if I ever get to see pick a
gamble a a versus program will be how
would it affect my posterior you have to
score right and so it's very expensive
okay so this cost here is independent of
the design cracking right so usually
Phoenicians takes this long value
information takes this long and so on
but it turns out that this criterion if
you're optimizing since it's modular we
can use some standard techniques that
are known for classical sub modular
functions so called lazy evaluations and
accelerated version of the greedy
algorithm which allows us to drastically
improve the performance of the sound so
I essentially picked the next test in a
couple of seconds instead of minutes and
this can really make a difference in
terms of running this in a in a actual
lab world okay so this is one one study
they actually running another study at
this moment that's so so they are very
interested in this
now let me move on to a different
application now for something completely
different and it's a sequential decision
problem as well but in a very different
area and it's a collaboration that we've
been working on over the last two years
or so with ecologists at the Unites
United States Geological Survey in
particular sarah converse and Beth
Gardner and Steve Morey who is from the
US Fish and Wildlife Services and it's
about resource allocation problem how
should the government invests our scarce
resources in order to met who conserve
right recommend which land should be
part in order to ensure survival of rare
and endangered species okay it's not a
theoretical problem is really very
concrete case study here's a species
that they're looking at as the Muslim
pocket gopher district or enlarged and a
tailored checkered spot it's a case
study in Washington State and it's
actually quite a complex problem so
first of all it's quite large so the
goals to select out of several thousands
of parcels that have quite some
heterogeneity in terms of soil types
vegetation slope conservation cost but
of course clearly effects there the
model there's all sorts of important
geographical aspects of example if
you're a pocket gopher then an
eight-lane highly may be pretty
insurmountable obstacle and there's
other important aspects if you really
want to maximize long term persistence
you have to look at species population
dynamics of example their flu a model a
reproduction colonization predation
disease famine harsh weather and so on
right and all of those of course are
important considerations modeling the
struggle of the survival of the species
ok so what we do in order to model loss
dynamics we use a common model in
ecology called patch dynamics model so
essentially what you would like to do is
you would like to say if the animals
live at certain parcels of land at a
certain point of time right where are
they going to the next time step right
and in the patch dynamics model you
would only model whether it
subpopulations present in one of those
patches of lands or not okay
and to model this one the dynamics as a
dynamic Bayesian network it's a lot of
parameters associated with those right
today we have random variables that
describe whether a population is present
at a particular location clean it's
unobserved right suits model publicly
then they are environmental conditions
and those model all sorts of unknown
parameters right survival probability
depending on soil type but also
spatially correlated events of example
presence of harsh winters and industrial
accidents and so on that affects the
survival and especially correlated
fashion and clearly does also what we
can control right what can you control
us which patches of land that you
actually wants or recommend for
conservation right and of course all of
those affect the distribution of where
the species are going to be at the next
time step ok so that is the set up well
that of the predictors come from so that
is where our collaborators really did an
amazing works they went through the
literature and assited all those
parameters from previous studies for the
parameters that were not available
before they essentially held expert
panels got the experts in and asked them
to draw curves right so likely do you
think that the species is going to
remain alive as a function of the patch
size that you have clearly there's
disagreement so you have uncertainty in
those parameters you want to model the
effect of that uncertainty on your
predictions that's that's that's all
done in the Indus model and then that
gives you something that you can really
reason about one point about to clarify
so the map consists of these parcels
these are small atomic union's units of
land that can potentially be bought and
some of those are too small so sustained
sub population so what you do is we look
at larger groups of those populations
called parts are called patches and what
we what we do is we do not model
colonization across those patches of
land but we model all the dynamics
within those patches of land and then
the problem becomes
choosing those patches of land in a
sense to hedge against the uncertainty
about the model parameter and maximize
resistance so that's the objective
function when you take our model and
then we look at which candidate set of
patches we may pick right and estimate
what's the probability of the species
still being alive after say 50 years of
time according to this model right that
gives us an objective function that you
can actually try to optimize now infants
in smallest hearts and practice what we
do as a sample based approximation it's
actually sample the dynamics from there
and essentially try to cover those those
samples that we get and status the
objective function that we have now we
can try to choose those patches in order
to maximize persistence right so you can
given some budget that you can spend and
given a cost of all these different
patches that you can consider we can ask
well that's the best investment but how
can you maximize their persistence
subject to this constraint in the cast
which is np-hard but turns out to be
some modular okay so we can use all the
machinery that I talked about before so
turns out here is a setting that you
really want to use non greedy algorithms
but it can still exploit some modular
you know that could get good solutions
here's just one candidate solution
that's you would that you get for
realistic problem parameters that they
describe and so one takeaway message
here is that you really get this fairly
heterogenous set of patches heterogenous
both in terms of soil properties and so
on all the parameters to really hedge
against a certainty so spatially
endogenous to avoid spatially correlated
events what you again want to hedge
against those and also none of those
small crosses major obstacle into
highways and rivers and so on okay but
so this is the if you use the kind of
static version to the problem so here
are just some narrow numeric results
this problem you can't really solve it I
solve it exactly so what the araÃ±as
essentially do some optimization
algorithms against some simple baselines
just to see how much do you actually
gain by optimization so you compare
against random but you also compare
against some simple heuristics for
example just really maximizing the
amount of habit or every area for those
species and you can do a lot better by
by optimization that's one aspect but
this is really a very simple version of
those problems when we showed this
to our collaborators the they told us
that swell problems actually a lot
harder than that so in particular what
you have to do is we only have a certain
amount of money every time period we
some will have to make optimal use use
of that right but what also does change
is both know the amount of budget that
the decision makers have to invest but
also the actions that they have right to
the set of lighter of patches of land
that they can actually purchase changes
over time right so in some sense the set
of possible parcels VT that's available
at every time step are changes over time
okay you'd like to come up with a
strategy of allocating those resources
over time in order to maximize
persistence another important aspect is
that actually as you make decisions
you'll learn something so for example
you'll learn if certain parcels of land
are really successful in sustaining the
species and you'd like to take that
information into account in order to
make your future investments yes so the
result I'm going to tell you makes no
assumptions about how the parcels are
going to change some principle they
could depend on what you do but he also
did take advantage right if you somehow
were to model the effect of your actions
on the available view availability of
future actions you potentially may be
able to the better okay good so so does
the setting and well it turns out that's
the objective is adaptive so model in a
setting it's being used to machinery
that he talked about in this in this
talk and that's exactly what we do and
translated to this application means
that essentially about this recommended
is doing some sort of optimistic
allocation that's what every time step
you do the best you can with the budget
that you have right so just optimally
investors using whatever non greedy
algorithm you have in order to maximize
the persistence and then whenever times
we commit to this decision right then
time passes budget changes the sort of
hustle changes and so on can make the
best decision you can and so on right
and if you look at the value of the
final Reserve right so the total
objective that you came out in the end
then it's going to be competitive even
against start of the best clairvoyant
algorithm even an algorithm that knows
exactly how much budget is going to be
available then right and which patches
are going to be available right and
that's actually a quite general result
this is not just for this particular
application but holds in general for
these adaptors of module type of actions
okay good so are something with some
quantitative results so yeah what can
you do but as the first of all one
question is how much do you actually
gain by being adaptive right so what you
could ask is you could come up with a
simple solution that you first in some
sense rank the different choices that
you have according to the utility right
then somehow figure out how good are
they right and just pick them as soon as
they become available right and that's
what we mean by a a priori optimization
and what this plot shows essentially as
time passes and you pick more and more
of those patches according to the
recommendations that the algorithm gave
you how does the objective function
increase and it turns out that even if
you is very very simple heuristics for
choosing the patches at each time step
just picking them at random you already
do better than this a priori decision
right by just adapting to this
availability you can actually do a lot
better right if this is if you do small
optimized adaptively by a based on area
you do better than random but if you
actually optimize the objective function
which you care about do a lot better
than those heuristics okay so there's a
strong benefit from doing it up this
optimization and there's actually value
optimization right so you can actually
gain something in comparison to simple
heuristics yes yeah how are you ruling
out you know you pick you pick the magic
patch the first time and I make
everything available to you the rest of
time if you don't choose that magic
nothing will ever be available to you
ever again so it seems like you should
have able to get more than like zero yes
so it's really a competitiveness so you
only compare against that seemed set of
rights essentially the sequence of then
certain parcels become available to you
know it's competitive in the no regret
sense you compare against exactly that
same set of of actions that become
available to you you can talk about more
in what formal details later if you like
right but you also don't really try to
take advantage that if you make certain
actions then how would that affect you
yes so that you don't try to okay yeah
okay so so this is right sort of
scenario fools but actually if you want
to use that then you actually have to be
able to you serve in some interaction in
fact the fashion so currently you have
this decision support tool that you're
building that collaborative I can use in
order to essentially experiment with
with the system this for example see if
there works or pick certain patches how
would that affect the recommendations
for other patches that you should buy
right if you can exclude certain patches
that you should never buy how would that
affect your future choices and and so on
right and given that the problem is so
much letting you use these highly
efficient algorithms all the
recomputation is essentially done in
real time I can show you later on the
laptop if you're interested ok so that's
it so that was summary of to two
projects there's a lot more going on in
a general area of a sabbatical
optimization machine learning and the I
a lot more that can be done I want to
point out for those of you don't know
about it I maintain is MATLAB toolbox or
sub-module optimization that implements
a lot of the algorithms for us of
marshal optimization you can download it
from the web page and also want to
highlight to be running the series of
nips workshops on the screed
optimization in machine learning and the
videos are now online on video lectures
okay so that's a um introduce this
notion of Leptis of modularity which i
think it's a useful set of abstractions
of sequential decision problems very
simple algorithms
pretty well right and I mean you know
that for problems that satisfy these
adaptive emotionality conditions and of
the results that are known for the
classical setting of optimizing some
modulus that functions can be lifted so
much more complicated as sequential
decision problems and all the results
still go through I told you about some
of those but there's a lot of other
properties that's the adaptive submerge
medicine problems have and also showed
you some applications that others that I
didn't get to talk about today is a
really interesting adaptive influence
maximization problem can tell I talk to
you offline but I think there's also
much more to be said about problems that
satisfy this property and I think one
one interesting takeaway messages so in
general right greedy algorithms are very
useful because highly efficient but it's
very important to understand then they
actually work or not right and if you
really try to carefully characterize
then these algorithms work then care
that can lead to new insights about
designing your objective functions which
need to high performance okay that's it
thank you very much we have time for a
very few questions just choosing new
tests performs better then supposedly
intelligent policies we are optimizing
in information gain etcetera so
information gain actually did better
than random application that one okay
but a lot of the other ones actually did
quite a bit worse than random that's
actually not a new phenomenon you
actually see this in a number of active
learning problems that have a similar
flavor because if you're not very
careful about how you select examples
you can essentially create bias rights
and that can be highly problematic in
the ability to generalize
okay so we are running like we are
already running late so let's thank and
just again and he'll be available till
the evening so if you want to talk just
like contact me okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>