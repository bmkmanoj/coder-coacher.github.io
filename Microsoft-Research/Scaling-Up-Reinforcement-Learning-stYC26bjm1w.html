<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scaling Up Reinforcement Learning | Coder Coacher - Coaching Coders</title><meta content="Scaling Up Reinforcement Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scaling Up Reinforcement Learning</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/stYC26bjm1w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so continuing literally where we left
off before tea break and so what are the
reasons that a lot of revived excitement
is there in the reinforcement learning
community and also in other communities
who have never taken brain first winning
very seriously before is this big
success in game playing in arcade arcade
games right so I'm pretty sure people
have seen many of these Atari games
space invaders pong breakout seaquest
yes yeah right so so this company called
deep mind set up this q-learning agent
ok it did nothing more clever than that
set up a queue learning agent that took
the video screen as an input you know
the pixels on the screen as an input and
as output produced command joystick
commands go left go right up down and
also press the button joystiq button
right it produced those commands with
rustic wedding say simply lake fire
right and learn to play games from
scratch right this is the thing stop and
think about it is amazing no visual
processing went in right they just
learnt from raw pixels fed as input
right and it learnt to give actual
control signals as output and it learn
to play games does not told what the
objective of the game was in the
beginning just this gets a reward based
on based on the game state you know and
they used a very complex neural network
like they use the convolutional network
and then they trained it using
q-learning they did none of the more
fancy stuff that the deep learning
people do they trained it using SQ
learning and used gradient descent for
training it right and it is in fact
according to some accounts considered
one of the hardest AI problems not just
learning problems one of the hardest a
problems that people have solved so far
but more importantly for the research
community
right so we looked at certain large
examples earlier we looked at helicopter
control we looked at backgammon we
looked at go other things which looked
at very large state spaces and solve the
problems but they are one off right
there is no no team other than those
derived from and drawings group which
can make a helicopter fly this requires
significant amount of infrastructure to
come to the point where you can do that
right and backgammon the hundred ninety
odd features the Jerry tesoro used or
enough proprietary it so you cannot
reproduce those results really right and
so what is nice about this thing is that
deep mind which subsequently was bought
by google and actually released the code
that they use for playing this game
learning to play this game and you can
reproduce a result okay so that is what
is amazing about it so here are some
videos of this learning to play the game
right this is wrong so when it starts
it's pretty bad right and as time
progresses it starts learning slightly
better all right so whenever it misses a
ball it gets a minus one right and so it
starts playing slightly better here
as to how to forward these things
can't see anything
so now it's really learnt well in fact
it tries more it more often than not
wins without considering a single point
or maybe one point or something like
that it becomes really good and it's
learning from scratch now they took the
same network right they took the same
network right of course it's untrained
okay from scratch again they made it
play breakout so same you're supposed to
know breakout is supposed to bounce a
ball and so like this they have trained
it on something like right now I think
they can play something like 42 out of
the 50 Atari games that are available in
the simulator of course they have to
retrain everything is not it is not like
one network that we just take it and put
it in another game that we play that
game so they have to train from scratch
but the thing is they don't have to tune
parameters they are not changing any of
the number of neurons in the layers and
stuff like it's the same architecture in
your network architecture that can learn
all of these games okay yeah
yeah so we are doing some work on that
this is really nothing much out there in
the community so for example one of the
things which we found out was when you
play retrain the network on break out
right so in the convolutional networks
the general wisdom is that they learning
filters to process the input right so
you take the filters you learn in
conclusion let in the breakout and I
rotate it by 90 degrees and you use it
in pong it seems to work right it works
better than learning palm from scratch
so there's some useful information that
is being transferred but then it's these
are anecdotal right I can say so pong
and breakout look similar to me and
therefore so this is a more systematic
way that you can define of what are
similar games when would transfer
between games work and so on so forth
that's open question so it's still a lot
of work that needs to be done in that so
but so if you are interested in
transferring the general deep learning
setting there is some quite a bit of
work there but in the deep learning and
deep RL kind of a setting resolution has
very little out there yeah but it is
here is another game which is
essentially the sea quest and should
probably can't see anything on my screen
somewhere about here so it is learnt
really well now and the amazing thing is
the same network okay and what is really
nice about these videos is it these are
not produced by deep mine okay this
comes from a blog some independent set
of researchers have actually reproduce
the results from deep mind using the
code that was released by them and the
same set of parameters that they
released they have trained it so this is
not just the one of arbitrary success by
deep mind so me define also has released
some of those videos which is a really
cool but this is from a block from an
independent party and I can also vouch
that even my students have managed to
reproduce the similar kind of results
using the code released by Google so now
the Grand Challenges how do you make
this work for other problems right which
hasn't had the benefit of significant
amount of optimization done on it right
so that something's left us here are
some references for some of the stuff I
spoke about and so this is the standard
the RL textbook reinforcement learning
an introduction and it is also available
freely online so rich in nandi are
working on a second edition and the
current work in progress whatever is the
current state of the second edition that
is also available online so so you can
go to rich certain home page and you can
pick it up from there so these are more
standard textbooks AI and machine
learning text books which have small
sections on reinforce maligning if
anyone wants to read up and this is more
a technical introduction to
reinforcement learning so neuro dynamic
programming which verse equation set
sickness and if you are interested in
some of the neuroscience aspects of it
so Dan and Abbott is a very good it's a
very huge stone on theoretical
neuroscience but it has a whole chapter
on reinforcement learning models in
neuroscience if you're interested in
that aspect of it so these are good
books true okay okay any questions on
so now I am
okay so this is what I should have
started at four o'clock so scaling up so
there are many approaches to scaling
that people have been looking at in the
literature I mean that is the main theme
of research in RL for the last decade or
so okay so it's just so much work out
that I really cannot hope to do justice
if I try to cover those so what I am
going to do is essentially talk a little
bit about anything parameterize
representation okay and talk a little
bit more about using hierarchies because
that's something that is close to my
heart and it's what I do a lot of and
then yeah questions later alright so the
tomb I would classify approaches to
scaling into two main subdivisions one
is using parameterised representations
we have been talking about many
functions that we have to maintain right
so we have to maintain a value function
right we have to maintain a
representation for a policy quite often
we do not maintain an explicit
representation for the policy we just
take the value function when you act
greedily with respect to the value
function we get our policy right but
other other instances we might want to
represent the policy explicitly because
it's easier than trying to be greedy
with respect to a arbitrarily defined
value function every time right and
therefore you might want to directly
work with policies so in which case so
all of these require some kind of
representation so far we have been
talking about this being represented as
look-up tables you know for every state
I have a value right for every state i
have an action given by the policy but
you could also want to represent these
by some kind of parametric functions ok
that loves you now we are not limited by
the memory size or anything you can
actually look at very very large state
spaces and additionally you get another
advantage right so you get your
generalization right so that is the
whole thing with all the supervised
learning parameterised function
representation that you have been
talking about so far right so you get
generalization so you also can know
something about states you have never
visited just because they happen to have
the same parameterization as states that
you have already seen and assuming that
you have a good parameterization then
this allows you to have very nice gentle
ization
given to unseen parts of this takes
place okay so so that is that is one
thing which people do another one which
I am going to call as hierarchical
decomposition is essentially to try and
take a very complex policy
representation or a very complex problem
definition and break it up into smaller
parts and then you solve these smaller
parts individually right and then you'll
learn to try and put them back together
so i will expand on this little bit more
and subsequent slides but this allows
you to at every point of time solve the
small problem you take a very complex
problem and then you break it out into
smaller parts so each part you solved
separately and then when you put them
together you don't have to worry about
the solution details of the smaller
parts you just have to stitch them
together at the higher level and there
again you can be paying attention to a
much smaller power so every every stage
you can look at one problems and we can
solve it so one way of thinking about it
is that you are kind of layering your
learning so I mentioned this in the robo
soccer example earlier so at each level
you learn to solve different problems
and then you can scale to larger
problems right so in value function
approximation so I am going to say that
since you have all been looking up
looking at regression and other things
in gory detail so I thought I can
compress these slides a little bit right
sign right all of you know what linear
regression great so what we do in value
function approximation so if you
remember I said you can approximate
value functions or you can parameterize
policies okay i am not going to talk too
much about models i just have a very
brief note on that but you could do for
Ali functions or policy so essentially
what you do is that you use some kind of
linear parameterization so I am going to
say we hat is the parameterised
representation for the approximation for
the policy right and w is the set of
weights that i have to learn and fee of
s is some function of the state that
gives me a set of features right that i
am going to use so in backgammon that
fee of s could return me that hundred
ninety two component vector that i was
talking about right or in in the Atari
games TFS could be the output of the
relational neural network could be very
complex I didn't say feed has to be
simple okay i am saying what you do
after you get free is simple its linear
Sophie can be pretty complex right so
you can put in a lot of things into Fifi
could be as something as simple as the x
and y coordinates of your grid world
right but typically if you are looking
to solve large problems you end up
putting in a lot of complexity into the
fee right so now the question natural
question is where does he come from and
that is the problem of AI that's
something that's a question that is
being the fundamental question that
artificial intelligence researchers have
been trying to answer from the fifties
so don't expect me to give an easy
answer but given all the hoopla right
now a fee comes from your deep neural
network ok he should come to lips you
and it makes last a half of nips was
doing deep RL is this how it looked like
so anyway so it's really a lot of hoopla
but deeper and I am not kidding so if
you are looking to get employed are
looking for good PHT admissions
elsewhere right note anyway so this is
essentially so this is my linear least
squares right so I am updating my weight
looking at the error in the prediction
right so v pi of s is what I am so
whenever I write V we are more or less
in the in the prediction domain right
when I one looking at control i will
write Q but I am looking at prediction i
will be using v right so so v pi is what
i am trying to approximate right and we
had this my current approximation so
that is basically my error right and I
minimizing the squared error and block
so I get it right what is the main
problem here I do not know my target
right so the whole thing is
reinforcement learning is nobody sitting
there and giving you the targets right I
do not know what beep I is if I know
what we pi is then my problem is well
solved right oh you can come an Argus I
no no so I'll for some select states I
will tell you what we pay
right and now using those the knowledge
of v pi in those select states now
generalize to the rest of the world rest
of the state space that is the normal
regression problem i can give you the
target values but again you cannot
compute the target values independently
for some arbitrary we are some arbitrary
states yes right so you really have to
solve for the entire problem domain for
you to find out what is an estimate of
good estimate of EP is so however you
are going to get the targets from right
so there are several answers to this so
if you think about it what is v pi v pi
is the expected return starting from
state s and following policy PI right so
what I could potentially do is pick out
certain states okay just one trajectory
from those states using my pie and
assuming it will end at some time T
right then SNL these trajectories n I
can take the entire return I accumulate
over the trajectory I do not have to
estimate any values along the way I just
take the full return take an average of
those returns and say this is my target
v pi there's a different way of
estimating what my veep I should be so
for individual states along the way I
can just estimate what the value
function would be and then for those
states i can plug it in here and then
assume that i will get the
generalization to the rest of the state
space so this will actually work because
the targets that i am giving right our
unbiased estimates of what we pi should
be all right so v pi is actually a
random variable some sense right so but
i am giving you unbiased estimator for
that random variable is and so we can
use that as the target in this and it
will work but then it requires a lot of
lot of work I mean you have to generate
a lot of samples from each of the states
each of those I mean if you want a large
enough sample you have to do this and
suppose you are changing your policy
again everything goes for a toss so what
people have done is instead of trying to
find any kind of unbiased estimate for v
pi they just use the tdt
so what is the TD target is RT plus 1
plus ouch should be a gamma there I
apologize RT plus 1 plus gamma times we
had st plus 1 that is the TD target if
you remember that is what we were using
as our predict for finding the
prediction error when we did the TD 0
update right so essentially they take
the same thing and plug that in as the
target for as an estimate for what beep
I should be now you can see where the
problem is going to come in right
because i am using we hat as my target
for estimating we had right so it is
essentially whenever I change my w maybe
had also changes right so the next time
I come to the same state s and try to
make an update I will have a different
target so a problem has become a
non-stationary problem by policy pi is
fixed the dynamics of my world is fixed
right my transition matrix p is not
changing my expectation is not changing
the rewards are not changing my policy
is not changing but i still have a
non-stationary problem so will it work
surprisingly enough it turns out it
works okay and so here is a is a whole
set of equations that you have to use so
your delta T is your TD error if you
remember so this is my TD target and
that is my current estimate so the
difference is the TD error so on the
gradient of this expression that we had
basically reduces to delta T times C of
s T right and then my weight update
becomes this weight so it's fairly
simple thing so this is linear td0
algorithm and not getting into the the
proof of this but the thing is so there
are a couple of conditions and also
under which this will converge to a
value function that is close to the true
rms see minimizer of the value function
so if i had v pi here if i have the two
v pi here right and i minimize the mean
squared error and given my
parameterization i can approach that
true v pi to some distance right so i'll
have some error given the trooper given
the parameterization i can get some
distance Delta close to the v pi Delta
could be pretty large depending on how
bad my parameterization is so what we
can show is that linear td0 will
converge to a point that is close to the
two maybe the minimizer to the of the
true value function even though i'm
actually using this moving estimate of
what the value function should be so so
that is a nice thing to know so the
couple of conditions for that so one
thing is I should visit enough number of
states right so that I get a good
estimate that we always always have a
problem with that and the second thing
is that my fee is t right well the
initial results required that VST should
be linearly independent the feature said
that he get should be linearly
pendant but now people have relax that
assumption as well the other thing is
alpha should decay over time right so
the alpha T should decay over time
otherwise I do not get convergence I
will always be changing my parameters so
that is the thing and so it is a pretty
strong result for linear function
approximation but no such strong results
exist for other complex representations
right so even you know well other well
understood function approximate us we do
not have any results about convergence
strong results of wood convergence but
there are many many successful examples
we already saw to itd gammon uses a
neural network right and the atari game
that deep kunis cuz they call it DQ n
the deep q network okay so DQ and this
is a much more complex neural network
than Jerry tesoro date and all of this
seemed to work right if you go by theory
okay none of these are supposed to
converge so obviously there's some more
work that needs to be done to understand
what is going on but it's interesting so
I'm just going to talked about linear t0
suddenly I am not really getting into
the details of the algorithm because you
guys have done enough of regression
already so only the only thing is you
have to be aware of what is what is a
target that you are trying to regress to
so the other approach people take to
scaling which is not really new has been
around for at least 20 years now our
policy search methods right so why did
people start looking at poly success
methods quite often it turns out that
policies have a much simpler description
than value function so one classical
example people give is in the problem of
inventory control I so inventory control
the problem so I have many many
different items in my in my warehouse I
have to decide when to place an order
for new items and off which item should
i place an order it turns out that the
policy itself is is a very simple
threshold policy
if item X falls below threshold T sub
theta then place an order so now all I
need to do is figure out what is the
theta right but then you look at the
value function it looks like a very
complex surface depending on the cross
product space of how many items are left
of each share type right so the present
value function becomes a harder problem
and representing the policy directly is
much easier so people looked at direct
policy search methods because of that
the second thing is so value function
approximation methods like I discussed
just now do not have good convergence
properties except under very simple
cases even though they work well in
practice but convergence results are
hard to show but then policy
parameterization methods actually have
strong convergence results they are
known to converge to local Optima in the
search space and so on so forth and the
third thing which shiny touch upon so
far is the problem of partial
observability so far we have been
assuming that you know what the status
right I say you look at the environment
you find the state and you act according
to that in many real problems you are
not able to find the state it for
example we have been saying that in
backgammon or tic-tac-toe or something
the state is fully observable to you
just need to look at the board that's
not really true you need to look into
the mind of your opponent right and this
is where so far we have been implicitly
assuming that the opponent is a fixed
opponent who is playing the standard
strategy and you do not have to really
worry about the mental state of the
opponent but in reality when you are
playing a game we have to worry about
the metal sheet of top and you can't
observe there or we'll talk about real
robots so obviously the robot is not
able to sense the entire world around it
it only has a few sensors so nars and so
on suppose they still see about the
world and therefore the world is
typically partially observable they
don't have full information about it and
empirically it has been observed that
policy approximation methods perform
better when there is this kind of
partial observability marginally better
than value function based methods right
so there's some of the reasons why
people look at topology search methods
and there are two classes of approaches
here so one is a direct policy search at
a very popular way of doing it is using
genetic algorithms so you have some kind
of a representation of the policy and
then you mutate that you cross over that
with another version of the policy and
then you evaluate that and that is your
fitness function and then you all these
policies over multiple generations blah
blah blah right and a more systematic
way of doing it is something called the
policy gradient methods where you have
this parameterization of the policy you
find out the gradient of the performance
with respect to these parameters and
then move in the direction of the
gradient not opposite and you do
gradient as it so so how is this going
to work on so I'm the policy depends on
some kind of parameters theta so theta
could be action preferences right it
could be weights of a neural network
could be could be ready of different
things and so you are going to modify
this policy parameters instead of
estimating action values hit is a
simpler version of what a performance
measure would look like let's assume
that for the time being that we are in a
simple problem setting where there are
no States so you have a single state and
we only have to pick out what is the
best action to take in that state okay
so we will extend it in a minute right
so the the estimate of your performance
would be okay Q star is the best
possible action that you can take and
given that you're selecting actions
according to pi which is parameters P
theta so overall performance would be Q
star of a into the probability of taking
a right and I can compute the gradient
of this with respect to theta and so Q
star does not depend on that so
basically have to take the gradient of
that right so now I am going to do a
little bit of a trick right so that I
can produce a quantity which is an
expectation according to my current
policy pie so if you look at this
quantity so I basically i made the
gradient as the expectation of this
quantity right according to pi
that makes sense right what i have
written out here is essentially there is
the expression for an expectation it is
expectation of this quantity according
to the distribution pipe right now now I
can estimate this expectation by drawing
samples because I have access to this
policy pie so I can behave according to
this policy pie draw samples okay and
then from this expectation and that
gives me the gradient right so that's
basically what my it's just there is a
couple of typos here I am missing in ETA
here that is a gradient of ETA right and
as I can form an estimate and that's
basically it all right this is sometimes
called the likelihood ratio so you can
think of this as the derivative of the
logarithm of pie right so now we can
have complex pie which involves a lot of
products and other things then take the
logarithm right then becomes simple much
simpler expression then you can take the
derivative so i am not going to work out
any examples here but people are
interested i can give you some examples
of complex policy parameterization which
become which shone into easy update
rules right so in 92 this person
williams proposed this algorithm called
reinforce which is one of the earliest
neural network-based RL algorithms which
essentially use the same kind of
derivation that i wrote down for coming
up with the with an update rule so this
is this version instead of doing the
summation over n samples i am doing it
in a fully incremental fashion so each
for every sample I every time I take an
action I go and update my theta so that
is essentially what i am doing here and
yeah like I said you can treat it as the
derivative of the logarithm of the
policy right so the full reinforce
update according to Williams is given by
this expression where he had an
arbitrary reintroduced quantity called
baseline which which tells you what is
the reward that you are going to get
if you are behaving according to the
policy pipe and then essentially it
allows you to figure out whether your
current action is better than average or
worse than average so then she just
introduced the baseline is basically
plunked it in arbitrarily and then
showed that it does not affect the
convergence behavior of the algorithm
and the motivation for that was to be
able to figure out if the rewards that
you are receiving is it better than
average or worse than average because
you do not know what a good reward is
right in fact we looked at examples of
problem where all the rewards are minus
ones and then there is one reward of
minus 100 right so this so then now as
soon as you say there is a minus hundred
then the minus one starts looking better
way so you do not know which is good
just by looking at absolute value so you
basically have to have a baseline to
compare it again so that is the
motivation for adding this and then this
term which is the derivative of flan pie
we call the characteristic eligibility
and thus reinforce is actually seeing a
big revival now because a lot of the
deep learning community not the RL
community people RL community guys are
happy to do q-learning with deep
networks in the deep learning community
people are happy to do reinforce with
deep networks so there are two groups of
people doing deep are so 11 group which
does reinforce another group is just
doing the Q learning right so so how do
you extend this to the full
reinforcement learning problem so here
your samples are I take an action I get
a reward okay and then I go and update
my parameters in stuff there if you want
to look at the full reinforcement
learning problem instead of taking an
action I am going to say you have to
take a whole trajectory so your samples
are no longer actions your samples are
entire project Ares in the state space
right so what if you do not have
episodic tasks then you just pick an
arbitrary state you are saying then say
that your trajectories are defined as
returns to that state you start from the
state if you keep behaving and when you
come back to the state you say okay that
is the trajectory I cut it that so that
you get basically you get samples which
are whole trajectories
right and then your criterion instead of
the reward you had earlier now I am
going to use the return along this
trajectory I just add up all the rewards
I get along this trajectory I use that
as my evaluation measure right and the
gradient of this thing here which is
essentially the probability of seeing
this return when you start from status
not it so earlier when I said when we
had the pie there at the resistant see
the probability of picking action a
right when a pic action e then I am
going to get Q sorry so here I am going
to see the province of picking action i
am going to see what's the probability
of picking that entire trajectory so
that is the generalization from the
one-step case that we saw to the fulfill
problem and it turns out that the little
bit of algebra you can show that this
expression reduces to the summation over
the individual ratio likelihood ratios
of the actions along the trajectory it's
it's it's fairly straightforward all
right the main problem here though is
this whole estimate depends on the
starting state s not right so this is
the fee remember this is this is the
performance measure for the policy as a
whole but it depends on where you start
in the state right so one way of getting
around that starting state assumption is
to assume that we have a fixed initial
state if you're playing games that is
not a problem you always have a fixed
initial C but in other cases it may be a
problem there are ways of getting around
that and one popular technique which I
am NOT going to talk about unfortunately
this is to use what is known as average
reward which is the paratime step reward
and you can show that for finite mdps
and under certain conditions that this
is actually independent of very starter
right but we're not getting there right
so here is a simple way of doing this so
this is a eligibility trace
where I am going to essentially keep
track of this summation right in an
incremental fashion and then Here I am
keeping track of the rewards again I am
doing this in incremental fashion I can
put all of this together and define a
whole algorithm okay which is
essentially a policy gradient algorithm
right this allows me to solve problems
without actually referring keeping a
value function estimate right and use a
parameter is representation for the
policy and this can scale well provided
you come up with a mechanism for
generating good samples so what is the
problem here see the the the the
estimate of the gradient that the policy
gradient method is using is a completely
unbiased estimate and if the horizon of
your episode this very large the length
of the episodes are very very large okay
the variance in the estimate can become
very very large as a bias is 0 right the
variance can become very very large okay
and so this can lead to very slow
convergence so if you can come up with
mechanisms that either cut down on the
recurrence time or introduce a bias into
the estimate of the gradient right so
that loves you to be more efficient then
you can come up with very efficient
algorithms overall that allows converge
faster and can work on very large state
spaces right so what are the kinds of
things that people do so one thing which
is kind of a heuristic is to truncate
the eligibility traces so if you
remember the summation of zi t so i will
just stop it at some point or I kill DK
the summation of zi t so that the
eligibilities do not run for too long so
that will automatically reduce the
variance that I am having and the other
class of methods is to use which makes
the gradient biased is to look at this
actor critic methods in actor critic
methods you have both the value function
estimate and the policy parameterisation
that is explicitly maintained
right so in fact a lot of the the
neuroscience community people prefer
this kind of a mechanism where there is
an explicit value function and policy
parameterization separately maintained
because it apparently is more
biologically plausible than talking
about q-learning and saying that there
is a max network that runs over all the
actions and then tries to pick the max
and essentially what we do here is that
when you are trying to estimate the
return that you are going to get under a
certain policy a kind of bootstrap that
using the Q values that you are
maintaining you do not wait for the
entire sample right so you do not just
sample the return along the trajectory
but you use the Q values that you are
estimating to kind of give you a
shortcut to getting those samples what
do you mean by convergence of samples
yes so that's always true read any of
these learning algorithms all of these
are data-driven so the the how well you
are going to perform depends on the
quality of the samples that you are
getting right so now we have moved to
some kind of regression domain all of
these so here now we're convergence is
going to depend very highly on the
quality of the samples that you have a
test the variance is very high right if
the variance of each of those updates
that you are making is going to be very
high then convergence could naturally be
very slow sometimes it might not happen
at all so those in fact if the if the
horizon time in the episodes right or
the recurrence time is very large
convergence won't happen it will diverge
anyway so in actor critic methods
essentially use the value function and
because the value function is an average
estimate over many many runs okay so the
variance would be low so if you are just
add instead of taking individual samples
you can take the this these are
different methods for looking at
variance reduction and again lot of work
is going on and to confound us all so
people who work
neural networks get these things to work
regardless of what we say about variants
and lack of convergence and etcetera
somehow these things work so we need to
figure out how to get this to work right
and the other way of using
parameterization is to look at model
parameterization and here we typically
learn with the offline data or batch
data I generate a bunch of trajectories
and then I do a lot of computation with
that batch of trajectories and then I go
back and it could possibly generate
another batch or if it is offline I can
just stay with that batch and do what is
the best i can write and so there are
different methods some of them
explicitly construct a model and work
with it right some of them implicitly
construct a model in terms of the
updates at they are doing right and so
linear denies method that falls under
the explicit construction method right
and least-squares TD or fitted curation
or methods that actually construct the
model implicitly right but then in fact
they have pretty good performance in
practice and I in my group we have found
that fitted q iteration works really
well when you are actually doing some
kind of batch mode oral and and you can
also alternate actual experience and
learning with models you know you can do
the normal q-learning updates and other
things and then interleave that with the
model-based updates model-based updates
could be something like dynamic
programming so you could do something
like dynamic programming and then
interleave that with going and learning
on a sample based method and those
things work well as well all right so
I'm going to kind of leave the
parameterised representations at this
right and then move on to learning with
hierarchies so here is a very popular
toy domain that people use in
reinforcements n is called the taxi
domain right so there is apparently
there should be a taxi somewhere here
missing which is going around picking up
passengers from different pick-up and
drop-off points
whenever there is a demand let us say
the thing the way it operates is say
somebody appears at our and says hey I
want to be picked up the taxi goes the
picks them up and then the person says I
want to be dropped at B then the taxi
navigates to be dropped them off and
then I will ready for another pickup
right so whenever the taxi tries to pick
up a passenger from any of the locations
other than RGB and why it gets a
negative reward whenever the taxi tries
to drop off a passenger at a location
other than where he or she wants to go
it gets a negative reward right and for
every step it moves along the way it
gets a negative reward so essentially
basically has to do everything if I
shortest time possible so that it gets
the maximum reward right so you can
think of this as a single mdp that you
are trying to solve alternatively you
can also think of this as a bunch of sub
problems that are arranged in some kind
of a hierarchy so for example so here is
a root problem okay solve the world
right and then here to there just
essentially it is a repetition of these
two tasks again and again I have to get
a passenger I had to put put the
passengers where they want to go so get
the passenger put the passengers of that
so now i can think of ok what does get
the passenger mean so you get the
passenger means i will navigate to where
the passenger is ok and pick up the
passenger and what does put the
passenger me well navigate to where the
passenger wants to go right and then put
down the passenger let so this
essentially now I am breaking it up the
simpler simpler subtasks right what does
navigate mean well I wherever I am right
use these for actions north south east
west and go to the destination that is
given to you so now why is this a
simpler problem when I am solving this
problem I do not have to worry about
whether the passenger is in my car or
passenger is outside the car and I do
not have to worry about where the
passenger wants to go right all I need
to do is ok I have the XY coordinates of
the car right now and i have the XY
coordinates where i want to go just go
there i hadn't had to worry about the
whole bunch of other things so it
basically just reduces to just
navigating in the grid world the
original problem that we saw earlier
right
a very simple problem it is reduced to
that so this is now become a much
simpler problem to solve and if you
think about it once I solve this problem
right figuring out when to do the pick
up and went to do the put on becomes lot
easier right at that point I do not have
to worry about any navigation problem I
just need to do a match between my XY
coordinates and where the passenger is
my XY coordinates and where the
passenger wants to go so it basically
just have to match those and if they
match then I can execute the reaction so
now the problem has become much simpler
so not only have a broken down the the
temporal aspects of the problem what
action i have to do after what i have
also broken down the spatial aspects of
the problem right and i am doing
navigate i only need the x and y
coordinate right so when i do pick up I
just need to know where the passenger is
I do not need to know where the
passenger wants to go and it will drop
off I only need to know where the
passenger wants to go because there
should be in the car right so the faster
is not in the car then there is no point
I shouldn't be even trying to do the
put-down task so these are the things
I'm also simplifying the representation
so this whole idea behind hierarchical
reinforcement learning so you not only
break down your control problem into
simpler problems but we also break down
the presentation so there are many many
ways in which many many frameworks as
people are proposed so the popular ones
are options max Q and there is a very
simple one that came little earlier
called airports so essentially all of
these frameworks let the agent learn
some skills right like navigate is a
skill all right so pick up and put down
as a skill so it learns allows agent to
learn some skills and reuse them
repeatedly to solve problems that is
basic idea behind all of these right so
let's quickly look at the options
framework all right options framework is
in some sense fundamentally just a
generalization of an action so if we
think about it an action can be started
in some state in the MDP right then it
us it executes some small piece of a
policy is essentially do that action let
us say go not right so I can start it
somewhere and they can just execute go
not and then i will stop so instead of
that think of an action that says hey do
not go not go five steps north right so
i will start it from somewhere and then
i will just execute the policy this
resistance lee 5 North actions and then
I'll stop right now I can start making
this more complex I can say instead of
go five steps north I can say hey go
pick up a passenger that is my action or
leave this room that's my action or go
to the airport right so now this is a
really complex action right and well
given its bangalore that certainly takes
a lot of time to complete right so so
these kinds of flexibility is
essentially what the options for America
loves us so the option technically
consists of a triple okay so I is the
set of states in which the option may be
started right in the actions you really
had never thought about it but an
options will have to think about it
right so when you say put down a
passenger this option then it can be
only started in states where the
passenger is in the car right and pie is
the policy that has to be followed
during the options execution and beta is
the probability of terminating in each
state it could you can make a
deterministic you can say if beta is one
then you stop beta 0 you do not stop so
it can make it like an indicator
function so that then it will become the
set of states in which the option can
terminate but sometimes it is convenient
to have the termination condition as
being stochastic so the definition
allows for that right so for example in
this case you can think of multiple
options being defined and can say okay
all the from all the states in this room
i define one option that will take me to
this doorway and this is useful thing to
know right so because i can start
defining navigation here as go from room
12 room to it go from room to room 3 and
then you can find your destination in
route 3 suppose i am looking for some
say a telephone you can say from here go
to that room
and then there will be a telephone in
that room you can call right so this is
how you think how this have you break
down complex policies so you do not tell
me that okay climb up the heat shown off
this step push the door open right so
you just say go to the other room and
then there will be this so likewise you
can start thinking of breaking down this
along maybe along the structure of the
problem domain right here well I mean it
makes sense if I say leave this room
from room 1 go to room 2 it makes sense
to assume that you can start in room 1
right outside of room man listen make
sure yeah it could I mean it's up to you
to define things however whichever way
it is useful to you right and just
telling you what the framework allows
you to do in case of leave a room option
it will be useful to define it as any
box in the room so the because now I
have introduced actions that actually
have duration to complete and earlier we
are not worrying about the duration if
you remember some time back looks like
ages ago I said that your time steps 1 2
3 do not correspond to real time steps
so one could be three seconds two could
be five milliseconds and so on so forth
right so but we never worried about that
but now I am explicitly introducing
actions that take different amounts of
time right and therefore the appropriate
model for this is something known as
semi Markov processes semi Markov
addition process not not the md piece
this is generalization of MVPs where the
actions are derated right and there are
generalizations of q-learning TD
learning and stars are never that other
things all all that you have looked at
so far which work with semi Markov
process so you can use those variants to
learn with options so that is nice thing
about options most of the other learning
algorithms actually have their own sorry
most of the other hierarchical
architectures come come with their own
learning algorithms it's a hams have ham
q-learning and max has max QQ learning
and things like that and but then
options is fairly straightforward right
but then there is a big question where
do these options come from so what are
useful options
and there are many ways in which people
have defined I mean options so more
often than not it turns out that you try
to find some notion of what is a
bottleneck right doorways are
bottlenecks so why is that sorry
doorways are bottlenecks because when I
want to move from here to anywhere here
I have to pass through this one state
you know so these are like access states
when I when you move through the state I
have access to a lot of other states in
the world right so geometrically it
looks like a bottleneck right so this is
this kinds of states are called
bottleneck states they did not
necessarily be John's mirik like
physical bottlenecks like a doorway so
it could be that I have to pick up an
object I have to pick up a key so that I
can open this door and leaned right so
that's picking up the key could be a
axis access tape not necessarily the
passing through the door so people have
looked at different kinds of different
ways of figuring out what the
bottlenecks are so there is this work
mcgovern bartow and couple of other
extensions where people just looks at
look at trajectories through the state
space right and basically count the
number of times each state is hit in the
trajectory and states that are hit on
many many different trajectories and on
successful trajectories or identified as
bottlenecks right and then there is this
other work using graph partitions where
you assume that you have access to the
transition model and then treat that as
a connectivity adjacency graph between
states that if you can if you have an
action that takes you from one state to
another then assume that it is connected
in a graph right and then try to do
partitions on that and if you think of
the room world example you are going to
get the well-connected grid components
like four components that are like grid
like and then there will be one or two
states one or two nodes that are
connected between the grids so when you
try to do cuts on that graph we can
naturally see that those doorways would
be the one set or cut right so you could
think of doing something like that and
then
notion of between nests from social
networks they look at shortest paths
between nodes and if lot of shortest
paths pass through a node then you say
that the no one has a high between this
if you think of again let's go back to
the rooms well so I go from room 12 room
to if I take any source node in room 1
and a destination node in room to the
shortest path between them has to pass
through the doorway right so that then
that doorway becomes state of high
between us and then you can cut it there
right so all the states with high
between this would be places where you
want your options to take you likewise
there are many ways in which you can
look at it and so I will try to talk
about metastability in the next five
minutes I don't know if I can get to
this is something which we have been
working with and it gives you very
robust options and we are done some cool
stuff with it and let us see how much i
get through if you start really late
because my timer says have been talking
only for 44 minutes yeah so basic
motivation is going to find regions of
the state space that are well connected
right and then try to abstract them away
as a single state right so essentially
now what happens is I have this four
rooms that are well connected I am going
to say a each room is a separate state I
have room one room to room three room
for and then there is connectivity
between them so that is essentially what
i am going to try and find and so we are
and then when we want to define options
they are basically transitions between
these abstract states right so we are
going to use this notion from physics
right from a field of physics called
conformal dynamics where they talk about
metastability right metal better
stability is a state of the system in
which the system can stay for an
indefinite amount of time as long as
there is no external input but with
probability 1 it will eventually leave
that state so it is not a truly stable
state it truly stable state is it will
stay there for four
unless there is a external disturbance
or external energy input right but the
metastable state is something it will
leave after a while but it could stay
there for a long time right now if you
think of walking around randomly in a
room right just walking around randomly
in the row with very high probability
you will only stay in the row you really
have to stumble on a doorway and when
you are next to the doorway you have to
pick the right action to exit the door
through the door for you to leave the
room so so if I am just walking around
randomly I will be staying in the room
for a very very long time so you can
think of these kinds of things as
metastable regions and so essentially we
are going to try and find these
metastable regions right I am going to
skip this little suffice it to say that
we have a clustering algorithm okay that
not only gives us clusters in a very
deterministic fashion like unlike
k-means right so whenever I run this
algorithm will get the same clusters not
only gives them clusters in a
deterministic fashion it also allows me
a very systematic way of assigning
memberships right so this whole thing I
mean I can talk about the clustering
algorithm for an hour so let me skip
that so it also gives me aware of a nice
way of determining membership to these
clusters so what we are doing is
assuming that is the clustering
algorithm which I am skipping right so
now i can run the clustering algorithm
this is a kind of a room a room world
domain gone slightly crazy so there are
14 rooms and then they are connected by
some maze of doorways and when i run our
algorithm on it it finds most of the
rooms some of the long rooms get split
into two but it gets its most of those
right and some of the other spectral
clustering methods that we tried they
fail miserably on this right so here is
what is happening so remember I told you
that we have a membership function the
membership function is going to start
looking like this so here is a world
which is a room within a room right so i
am going to for every cluster that i am
going to find i will have a membership
function so how much does the state
belong to that cluster and it turns out
that for the outer room i am going to
get a membership function that looks
like this and for the inner room i get a
membership function that looks
that So You very neatly separates the
outer room within a room right now I can
say all the states for which the inner
room has a higher membership go into one
abstract region all the states where the
outer room membership is high go into
another region so not only this right so
you can see that there is actually dip
in these functions that tell me where
the doorways right so if i want to go
from the inner room to the outer room
all i need to do is follow the gradient
of this membership function and i can
just say i want to go to the outer room
i start from here and then walk in the
direction of decreasing membership or
walk in the direction of increasing
membership to the outer room and i will
be after all right so it not only tells
me where are the the connecting points
it tells me how to get there also
basically gives me the option it gives
me the i right so I is given by the
membership function it gives me the pipe
is given by the gradient of the
membership function it gives me the beta
beta is essentially all those points
where the the transitions happen right
so i can use these these points to
actually define beta so it gives all of
this thing in one go basically a run
clustering i get i pie in beta so none
of the other methods i spoke about have
this kind of a facility so you actually
have once you find where the option is
supposed to take you so you have to run
another routine to figure out what I pie
in beta should so like I was mentioning
earlier so it does not have to be like
physical doorways or anything it could
be more behavioral bottlenecks so here
is a region where there is no obstacles
or anything you just have to pick up an
object right and when we run this
clustering algorithm it finds out that
holding object is one abstract region
not holding the object is another
abstract region and this peak here in
this this dip here in this membership
essentially tells you the transition so
this is actually at four come a 15 in
the inner in this space so a Chelsea how
do you go and pick it up right and as
already told spoke about this right and
so we can do lots of interesting stuff
here so
so so far we have been assuming that you
have the full transition matrix
available to you so that you can do the
segmentation right that is what we
needed to run any kind of spectral
clustering algorithm so you need the I
said you can take the transition matrix
convert that to a graph and then run
segmentation on the graph so what if I
didn't have the transition matrix so
what we did was we again did some kind
of sampling right so we start solving
the problem assuming that you do not
know anything about the world so
initially you're behaving blindly
basically randomly so in some sense you
are doing a random walk because they
have learnt nothing to begin with so you
have no information about the world so
essentially you are doing some kind of a
random walk right so what happens is you
gather all the transitions that you are
seeing during the random walk you make
an estimate of the model of the world
right you make an estimate of the model
of the world and then run segmentation
on that estimated model how well this is
work well we do not have any any results
on how well this is going to work but
empirically we found that can actually
solve really complex problems and so
well we did some simple examples so here
is a two-room grid world so it is a very
very thin room it's more like a corridor
and here is a larger room and then the
goal was placed randomly all over the
rooms and then so you can see the red
line is basically our learning algorithm
so it figured out just two options one
was go from this room to this room other
one was go from this room to that room
and you can see that it is optimal
almost from the beginning so as soon as
options were discovered here right the
first few episodes we managed to
discover the options and after that it's
behaving much better than the other
benchmarks we consider so essentially
what did we the baselines we considered
or a bias to move in a particular
direction so I was telling you that you
could define options that say go north
five steps right or go east four steps
so like that so we had options that
instead of taking you one step at a time
it could take you very far so
allows you to explore the state space
quickly so that is the reason behind
that and we also used a graphic aid
based approach which is kind of at that
point when we run the experiments it was
the best option discovery thing there
was empirically and we could be elk and
elk it actually does do well at the end
but it finds a lot more options are
necessary it ends up finding likely
ricola yeah i am sorry i didn't get what
you said
so we we have done some experiments
where we we did this in a time equalized
fashion right and so we are certainly
better than doing some things like you
learning I plane when you like you
without without doing any option
discovery right because even while we
are learning those initial trajectories
right when we are doing the initial
trajectories we are learning it's not
like we are behaving randomly completely
so it's essentially q-learning at the
beginning and then as and when you start
discovering useful options you get
better and better at it right so in that
sense it's it's fine it works well but I
can talk to you about some of the other
time equalised experiments that we have
been airing there are some really
interesting results that we have with
randomly generated options okay which
which actually do well I'm not talking
about that here because it's kind of
self-defeating in this case but so it I
can talk to you about that offline right
and here in the taxi domain we run this
and again so we found about 20 options
and what was interesting about these
options that we found we didn't give it
that initial tree structure i was using
for illustrating hierarchies right so it
just you're just learning from scratch
but then it learned many of these
options it learned we're very similar to
pick up alright so it actually learnt
pick up at our pickup at why right so
each one of those destinations so it
learned for pick up options it learnt
for put down options for each one of
those locations and not only that it the
remaining 12 options where some kind of
pairwise navigate options you know so
navigate from close to are too close to
be navigate from close to why too close
to our so those kinds of options
actually is very nice the way the
options are discovered by both all the
put out / women put down and pick up
options and as well as a subset of the
navigate options it learnt that and of
course we can do some more fine
parameter tuning to figure out the right
set of options but we didn't do any
careful tuning of parameters here so we
just use the same heuristic for picking
the number of options or int across all
the domains
which is essentially Houston eigen gap
heuristic so it was interesting and we
then did it on the Mario domain how
people have played this Mario thing you
have run across the screen pick up coins
kill monsters and so on so forth and
that's possibly the largest domain that
has been used so far for option
discovery in fact taxi is its largest
than any other paper as reported so far
but some of the reviewers didn't think
so so we actually run it on the Mario
domain technically there are 25 raised
to the power of 352 states in the domain
but only about 20,000 states are ever
visited during gameplay so but you have
to account for the fact that your state
space is that last so some kind of
sparse representation is needed so we
used different kinds of parameterised
value functions so one this a kind of
aggregation state aggregation based
measure called CMAC and we also use deep
water encoders for representing the
value function and this whole transition
matrix the samples for trajectories and
other things that we are doing but only
in that fee of yes space we did look at
the yes space and once you use
parameterizations we get this fee s
factor right so the transitions were
defined only on the fears not on the s
itself and then we did segmentation on
this right and try to learn options and
surprisingly I did well right and so it
gets rewards for achieving side goals
such as gathering coins or killing
monsters and other things so so this is
the the cumulative number of sight goals
that the agent achieves as the learning
progresses so you can see that the Q
learning agent Rock you learning agent
learns to play the game it learns
successfully to run from left to right
right but it doesn't know how to kill
monsters or gather coins or anything it
just learns to survive and reach the
right edge of the screen while the gen
that is learning options learns very
quickly to policies to complete the side
goals and does really well and you can
see some of the options it discovers or
whenever it sees coins on the screen it
jumps up gathers those coins and in fact
it repeatedly jumps up sometimes it
misses the coin so it can go up up and
down
try to gather the coin all the time and
then it speeds up to jump over a pit so
it learns it as a single action so as
soon as it sees a pizza where it starts
running faster and then jumps over that
is a single action does not have to make
decisions in between right and other
interesting thing it learnt is there are
different kinds of monsters in Mario's
people have played it and that is a
turtle okay so when you kill the turtle
it is not dead right in the jump on the
turtle the shell is still alive so you
have to jump and then immediately kick
the shell away otherwise the shell can
kill you right so it learns that as a
single option jumps on the shell and
kicks it away because otherwise quite
often the episode ends there right so
these are nice things it learned
somewhere pretty nextly inexplicable I
have some videos but i'm not going to
show this to you with because whenever
there is a pillar on the left side of
the screen and coins on the right side
of the screen it just goes crazy you
just keep jumping up and down in the
middle of the screen forever and until a
random exploration pulls it away so so
this is some kind of inexplicable
options it learns pretty sure it's
because of the parameterization that we
are using but then it does really well
right so feature we are trying to do
some experiments with Atari games now
trying to find options without an Atari
games we want to look at extending to
state and action spaces and we are
looking at you know many of these like
the rooms world domain and things like
that they are very symmetric you know I
can rotate the room slightly and then
you can still recover the policy that
you want to use and so on so forth
trying to see how we can use symmetries
and a bunch of other things which I
didn't talk much about so I can't really
explain here so here is a conclusion for
the whole thing not just for this talk
for both the talks a lot of excitement
in the community now right and
especially scaling and it's lots of
interesting experiments that are coming
so there are a lot of work on learning
from demonstrations how we can learn
from a human so you can cut down on the
initial random exploration that I was
telling you you can cut the cut down on
that by asking a human to do something
and then trying to learn by imitation of
that right and so there are many many
advances in model-based reinforcement
learning which I didn't talk about today
and of course deep RL is
being pursued very actively by different
groups of people from different
disciplines right but I did say that
deep q network on the Atari games is
reproducible but it is reproducible on
the Atari games right so Phoenix we need
more work in to get off the shelves RL
algorithms like you have off the shelf
SPM solvers and things like that so you
need two more work to get off the shelf
all but the sense is that we are getting
closer than before in fact there was a
period of time when at least personally
I felt that we are not making much
progress towards that but I think in the
last few years certainly lots of
interesting activities so start working
in order yeah thanks any questions ok so
if not let's thank the ravindran again
for giving a couple of very eliminating
lectures and reinforcement learning</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>