<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tree Structured Gaussian Process Approximations. | Coder Coacher - Coaching Coders</title><meta content="Tree Structured Gaussian Process Approximations. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tree Structured Gaussian Process Approximations.</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SBgAr9tAYgE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
great I'm a bit nervous about talking
here not not at least because Christians
talk was so amazing but last time this
time last year I spoke with in a few
hours of baby daughter had arrived and
I'd been stuck in the the MSR
communications black hole so I guess
these things don't happen every time you
come down into the Amazon lecture
theatre then so I'm going to talk about
some work I did with my great PhD
student Tang his widow said today on
speeding up Gaussian process methods I'm
particularly interested in Gaussian
process methods for time series and very
interested in audio applications so when
I started working in this general area I
met up with david mackay and David
McLeod said it'd be great if you could
train your gaussian process time series
models on zubin jaramana speech and then
have it synthesized realistic Zubin
garavani speak out of the out of the
model unfortunately that turned out to
be a bridge too far but what we did come
up with was a method that was great at
learning the statistics of time series
such as audio textures which sounds like
running water crackling fire howling
rain and wind and then generating
synthetic examples that you really can't
tell the difference between those and
the original although they'll have a
very different pattern of falling
raindrops or different pattern of
crackles and things like that so let me
play some examples just to motivate why
we're interest in these models so is
going to be a whole batch of textures
these are all synthetic sounds from a
model involving Gaussian processes ah
and there's no sound
what
okay so each one of those little chunks
was a little synthetic texture that
we've trained this model which has
Gaussian process inside it on and those
samples can be really short there can be
two or three seconds and then you can
ask them all to go ahead and generate an
infinite amounts of new data a bit like
Christians text examples let me just
play that again so these are water
sounds to start with two different
running water sounds this is the a
campfire and then a bigger fire which
follows these are wind sounds which are
a bit ropey I think wind has frequency
modulation it which we find hard to
match rain were really good at this is
snapping twigs it's a bit different from
the original but I think plausible and
at the end that's synthetic applause
coming from the model okay so that's all
well and good but what we want to do is
we want to you know in order to do this
under the hood we're using Gaussian
processes and they're using very very
long time series models so time time
series which are several seconds in
length with a relatively high high
sample rate so the question is how we
able to do that and this talk is going
to be about an approximation method that
we can we can use in these models in
order to make learning and an inference
if we wanted to do inference in these
models to remove these textures safe and
speech really really fast and at the
start I want to point out a difference
between the approach we're taking in
this talk and the previous couple of
talks before Christians that also talked
about Gaussian processes in a time
series setting so in order to point out
the difference I I want to go back to
this model so here's a very very basic
model for a time series xote that says
exit time t is equal to lambda times x
of t minus 1 you just wait the previous
x value and then add some gaussian noise
on it and in the previous three talks we
talked about one particular way of
generalizing that first order auto
arrested process it was to replace this
linear dependence on
of t minus one with some general
function f and then put a prior on that
function that was a gaussian process
because that's what those those two
talks did and and you'll notice a couple
of consequences from that immediately
first off the Joint Distribution of all
the X's 12 t in your big time series is
now non-gaussian because of the
nonlinear function f and it's still a
discrete-time model so we're still
sampling this with some regular sample
rate what I'm going to talk about in
this talk because it works a bit better
for audio is a different generalization
of this process so we could say that
this autoregressive process puts a joint
gaussian distribution on all of the x's
from time t through to time big t and
instead of parameterizing the Gaussian
distribution in this way we're going to
say that the whole time series X of T is
is drawn from a run a Gaussian process
with a particular mean function and the
covariance function so now any marginal
of this time series is going to be
Gaussian distributed unlike before and
we've now got a fully continuous-time
process that would help if you wanted to
say change the sample rates of your of
your observations so some nice
properties coming from that the reason
why this is perhaps better for audio
applications is because you need really
long temporal dependencies in audio and
if you did that using this sort of model
you need a very very high dimensional x
space and that can be can be tricky to
handle so so this is the route you've
taken and as I said we're using this for
modeling time series like speech
possibly mixture with with these textual
sounds which are easily you know 10 to
the five to ten to seven data points and
you probably saw in the previous slides
that people presented that this has a
cubic cost in T so for these very long
long time series that really is
prohibitive and we're using several
Gaussian processes in this model for
these these textual sounds so multiply
this number by by 10 20 or 50 to get the
full size of this the latent space which
is in those months so how are we able to
do this well what I'm going to first
explain is if you use an off-the-shelf
method for approximating
for approximating GPS in this context
it's not going to work very well it's
going to fall apart catastrophic Lee and
we're going to investigate the method
the UTN used in his talk in in the
nonlinear Gaussian process state space
models now in this different time series
setting and just sort of show you how
well it performs so here's some time
series data here X is at time index in
this example and we've got some noisy
noisy observations of it and what we can
do we're imagining our goal is to fit a
GP in this example to it so you can see
if you squint your eyes there's a mean
function now going through all those
points interpolating between them and
some error bars around them and we're
going to we're going to compare this
full true if you like GP fitting to
approximate ones and in order to do that
we're going to measure how good the
approximate ones are by their mean
squared error in this in this task
predicting the underlying noisy function
values and if you run the full full GP
in this example it takes a long time and
several tens of seconds but it gives you
a very low and standardized mean squared
error ok so the approximation method
that you TN pointed out essentially
works as follows it introduces a pseudo
data set a smaller data set than the
original whose purpose is to summarize
the full Gaussian process posterior and
it Wiggles around where it puts these
data points in order to come up with in
some sense the best matching approximate
posterior to the to the true underlying
posterior so if you use a very small
number of pseudo points so here I've
chosen for and this is showing the input
locations the locations are which it
decides to put those pseudo observations
then the approximation to the true
posterior is quite bad it gives back
this very smooth red line here it picks
up on on the fact that this sort of
sequence of observations here seems to
be a bit lower than is typically the
case and this one here is a bit higher
but other than that hasn't got any of
these fine fine Wiggles now of course
what we're going to do in a minute is
increase the number of pseudo points
here and we'll see progressively better
better correspondence between the
approximate solution and the cheapest
true solution the error in this case is
rather large so there's the mean squared
error as you'd expect is is rather high
now as we increase the number of pseudo
data points in a little summarizing data
set you'll see that the approximating
function gets bigger as gets better and
better matched to the original so we've
now got lots of pseudo inputs and the
standardized mean squared error is
coming down and down and down until at
some point were you know a reasonably
well approximating the true posterior we
could have continued adding more and
more data points in and it would have
indeed converged on on this point down
here okay so now here's the rub this
this sounds fine why can't we just jack
up the number of pseudo points in our
data set for these texture like stimuli
and eventually we'll know will be
approximating the true posterior that
the the original GP would have and
aren't we home and dry well here's the
rub the cost of this algorithm as
pointed out by you CN is of order T M
Squared we're zooming in here sorry
sensitive clicker yep so the the cost is
order T M squared so it's growing with
the number of pseudo inputs in the
squared way as you'd expect now let's go
and do a thought experiment how many
pseudo inputs would can we get away with
well the time scale of variation in
original time series I'm going to call
that towel and it's quite quickly
varying so tells quite short in this
example compared to big t which is the
duration the overall duration of the
time series and the effect of each one
of these little pseudo inputs that you
add in to approximate your posterior is
always local and it has roughly an
effect width of order towel okay it
roughly influences the function
depending on how you set the hyper
parameters in a little region which has
length town and so that means in order
to well approximate the whole thing we
need to make sure we have enough pseudo
point spaced roughly every towel seconds
in order to get accurate approximation
and that
means em needs to scale implicitly to
retain the same accuracy with big t here
and so when you put this observation
together with the scaling back here that
em down now depends on T you get
something which is order T cubed again
for the same computational accuracy and
we really haven't gained anything by
these approximation methods apart from
the constant which sits outside of this
order so in this time series settings
that means that you really can't use
these conventional conventional pseudo
data point approximation methods because
you keep on having to add in more pseudo
data points and at some point is just
going to break and you're going to lose
lose accuracy so I'm going to tell you
now very quickly about the intuitions
behind our method which is going to be
faster than this version of doing things
by considerable margin and essentially
gives up almost no error as compared to
the four GP so we're going to design an
approximation method which is bespoke
for time series it work great for time
cereals or work poorly for other
situations and you'll get the full
benefits of using a full GP in a
fraction of the time on orders of
magnitude quicker now in order to do
that I need to tell you about prior work
which this is going to lean quite
heavily on and the prior prior work it
was by Ed snelson and zubin and it's
called the fully independent training
conditional approximation and we're
going to base a new method largely on on
this work ok you can think of it working
it in a very very similar way to the
method I showed you on the previous
slides the previous slide showed generic
behavior of a whole class of methods
switching include the fitzy
approximation so how did zoom in and Ed
come up with a way of approximating GPS
and well they started with their GP to
start with shown here by a factor graph
of an aficionados and they started by
augmenting that model with a bunch of
pseudo inputs so these are our pseudo
data points we're just going to add to
our GP and they're going to be smaller
than the full number of data points
we're going to handle in
eventually give us some computational
advantage and so they have inputs here
which I've called X those are the things
that were the bottom of the previous
diagram that we're optimizing and
outputs you so here we go we've
augmented it and we've placed a Gaussian
a joint Gaussian process prior now over
the use and the f's and everything is
pairwise connected so here's the the
joint gaussian process prior now in
order to make computation more simple
we're going to remove some of the
dependencies in this full GP and that
will lead us to efficient algorithms so
in particular we're going to move all
the direct dependencies between the
function values all of these direct
dependencies through one another all of
the dependencies between the function
values are going to be mediated by the
pseudo data points you at the top okay
so just go in and remove them and now
we're going to calibrate this new model
so that it's as close as possible to the
original GP that we had ok and the way
we're going to do that is we're going to
carry out a minimization of the KL
divergence between our new model here
sorry add new model here and our old
model over here so we can do KL old
model two new model and that lets us
figure out what these new distributions
here / over our pseudo inputs and the
relationship between the function values
F and our pseudo inputs down here ok and
when you do that it's a very
unsurprising result what you what comes
out you find out the optimal
distribution over the pseudo input prior
here in this new model is equal to the
original prior under the GP of the use
and the conditional distribution of the
f's given the pseudo inputs is equal to
the conditional distribution in the true
model okay so you just retain the
original distributions and ditch the
direct dependencies between the f's ok
so that's all well and good doesn't work
very well for time series for all of the
the reasons that we discussed before so
how are we going to extend that for time
series and the idea is to introduce some
additional links that we're going to
delete from this graphical model in
order to make things
chain structured okay so we start in
exactly the same way we start with our
original f's function values we augment
with pseudo inputs again in exactly the
same way and now we're going to remove
some of the direct dependencies between
the function values so we're going to
remove the direct dependencies in this
example just like this in the same same
way as we did before and we're also
additionally critically going to assume
that they use at the top here and now
chain structured okay so we're going to
remove some of the connections between
our pseudo data points that we had we
added in in this particular way and
similarly we're going to then remove the
dependencies between use and F's which
were not in the same block okay so by
deleting links in this careful careful
way we've converted our original graph
that was a joint Gaussian process over
the pseudo inputs and the actual
variables we observe into something
which is is chain structured that seems
like it's going to be a disaster but
inside each one of these use and F's in
here will actually imagine having a
whole number of variables so it's not
like there's a single you variable up
here which is acting as a bottleneck in
a time series it's going to be a whole
stack of states now we just go ahead and
calibrate the model in exactly the same
way as we did before and so we taped the
original model that we started off with
and then minimize a KL between that and
the new approximate model which takes
this nice chain structured form and that
gives us the optional way or an optional
way of setting the conditional
distributions on the use how you at time
T depends on you at time t minus 1 and
the relationship between these function
values and the underlying pseudo data
inputs okay so this this method for
constructing a new model so the goal
here has been to construct a new model
parameterised by pseudo inputs which
retains as much of the structure as the
original model but now is going to be
amenable to computation and the reason
why it's going to be an amenable to
efficient computation is that we've now
got a chain structured graph
so we can do efficient inference using
common filtering and common smoothing
rather than having to invert huge
matrices as we would in a normal normal
GP and what's more in this particular
setting you can now have as many pseudo
inputs as data points is if you want
because you're going to get
computational gains by the fact that you
just have to run common filters to do
your matrix inversion rather than having
to invert it in the first place so we
can introduce more and more and more
pseudo inputs as the length of our time
series grows and grows and grows and so
this is just summarizing exactly as we
had before these these methods for
approximating Gaussian processes are a
bit odd in a way because we're perhaps
normally used to thinking about
inference as going in and approximating
posterior distributions in some way and
really what we've done now is we've
written down an entirely new model which
is calibrated to the original model but
in which learning and inference will
have many of the properties and
hallmarks of of the original model and
in this case the model is a linear
dynamical system but the
parameterization is very very strange
right because in each one of these
relationships here between you at time T
and you at time t minus 1 and the
relationships between you and F they're
all parameterised in terms of the
original parameters of our GP the hyper
parameters the length scales the noise
values and things like that so it's a
very weird non linear dynamical system
with parameters which themselves depend
on the hyper parameters of the the GP ok
so the complexity as we said is linear
in time and it's d squared costs where d
is the average number of observations
per block but the critical thing really
is this this linear scaling on time now
and many of the same things would have
gone through if it's not obvious if you
had multi-dimensional time series so
I've only talked about uni dimensional
time series but for instance in the
texture work we have lots of Gaussian
process at each time point exactly the
same things apply and similarly you
don't necessarily have to use a chain
you could use a tree structure over your
inducing inputs as well if you want to
use
patient later or or perhaps other data
from so geostatistics okay so finally
I'm going to show some results I'm going
to use an audio date or do missing data
imputation example these are examples
from the textual type models inferring
here these are missing sections of
speech and this is a single model that's
been trained on lots of speech and we've
asked it to impute these missing
sections of about 15 milliseconds each
and you can see that these textural
models do a decent job of fairly long
range interpolation through these little
regions of a speech sound and now I'm
going to compare what happens in a
missing data imputation case when you
use the Train structured approximation
as versus these normal of the shelf
methods so at the top here is the
original fitzy method this is the same
plots you seen before with training time
along the the x-axis here and mean
squared error what's the right you can
see this is doing rather poorly even we
even in this case where we use 1500
pseudo inputs this is for a time series
which is 40,000 and samples in length
and these are some other methods that I
want to go into one of which is the the
variational free energy method that you
TN use that we show to start with very
similar to fit see as I claimed earlier
and this is a sparse spectrum method I
sorry this is the variational now this
is this is the sparse spectral method
apologies this approximates the spectrum
of the process seems like it might be a
good idea for audio does about the same
as these other conventional methods and
here's so it's something sensible to
compared to where we blocked up the time
series in two independent blocks and
then we're going to use the fit see both
method to approximate each one of those
blocks and that actually does a bit
better on these examples but it would
have horrible blocky artifacts because
you've thrown out your time series
structure and you said each block is
independent and this is perhaps the what
people thought would have been the most
natural thing to do it's a method based
on a connection to stochastic
differential equations in our hands we
could only get it to run on to 20,000
data points because it took too long and
it really is it is very very slow and if
you compare that so this
the frontier traced out by these current
methods and you compare that to what we
get from running this new train
structured method you can see that we're
cutting down for the same approximation
accuracy for the same mean squared
accuracy we're cutting down the training
time so at least a factor of 10 in this
example here and tracing out this this
frontier these numbers that I've shown
here are the numbers of observations
associated with each block in the
approximate method and the size a number
of pseudo inputs in each one of these
blocks in me now use and there are sort
of sensible heuristics you can use in
order to figure out how to set these
things automatically it's not like you
need to run things multiple times to
figure out these numbers need to be ok
so summary is you can use GPS even for
enormously long time series to do funky
stuff like generating synthetic textures
and moving them from speech but you need
to use approximation methods and if if
you are in a time series setting these
types of methods have outlined here
these chain structured and tree
structured Gaussian process
approximation methods will get you all
the benefits of a normal GP but for a
fraction of the computational cost
because it's ignoring very long
dependencies in your GP which basically
are extremely extremely weak ok great
I actually have one so did you take a
look at the latent representations and
whether they are semantically meaningful
or have some interpretation for the
texture stuff you mean yeah that's
really hard so the the latent space is
at least 100 dimensional that's just the
ace of the latent variables if you're
talking about the parameters and other
things that really govern the statistics
there are thousands and thousands of
parameters and these things so that
their GPS on each frequency band and
then the spectra temporal modulation
which is induced by another layer of GPS
so it's hard to do we haven't found a
good way of compressing that high
dimensional space in a meaningful way to
discover what textures are near each
other and things like that but that
would be a fun definitely a fun thing to
do
change in the kind of and the texture
that you have in some business starting
stopping the applause or changing
between two different types of speech
sound yeah so this model is gives out
temper temporally homogeneous textures
so it can do sort of one type of you
should think of it in some context is a
non stationarity so that might be the
claps in the applause example or the the
pitter-patter of rain drops you can
learn those temporal statistics you'd
need to put another layer on the
hierarchy to do things like merge to
switch off one texture and bring on
another but you know it could it could
be done and we have examples where we
played with pairs of textures in that
way yeah but you still imagine it's the
bottom layer of your big hierarchical
model that you could you could go alone
if you're doing that so one more
question up there yeah yeah I can do I i
think the method knows clearly what
frequencies to show in that so how do
you deal with this spectral components
of the observations do you you tell the
algorithm through the prior maybe that
the process is band limited or
everything is learned from India
training yes this is inferred as you go
along so this is one underlying model
and and you can see that the predictions
it makes really does vary on what the
previous context was of the little gap
it's looking at so in this one it had
lots of high frequency Whipple's and
it's learned that in this context it's
good to predicts predict with high
frequency wibbles on your your
underlying smooth underlying smooth
variables and then this one things are
rather more slow it so it figures out
what the local spectra temporal
statistics are in some sense and then
interpolates using those local spectra
temple statistics so it's automatic okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>