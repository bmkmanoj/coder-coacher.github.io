<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Computer Vision and Bioacoustics | Coder Coacher - Coaching Coders</title><meta content="Computer Vision and Bioacoustics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Computer Vision and Bioacoustics</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vHpw5u7e2XQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay well welcome to the computer vision
and bioacoustics session Ricardo Torres
is our first speaker and I'll just turn
it over to you listen good afternoon
everybody my name is eco-tourism from
University of campinas and I'm going to
present here ongoing work regarding the
use of machine learning approaches for I
did fine in detecting remote for
knowledge patterns so like to
acknowledge no funding agency also
collaborates in the student first I'd
like to provide a kind of context of
this work no this work has been
developed in the context of deaf and
knowledge project which is a joint
collaboration between us for Institute
of competing unicamp at unica and but
the finale's laboratory whose head is
Professor Patricia morellato from NASA
so basically in this project where we
are interested in obtaining and
performing analysis on phonology data
phonology is can be seen as a kind of
scientific studies of very any kind of
periodical by biological phenomena that
can be somehow correlated with climate
conditions in general the the typical
approach is based on combining on the
field observations with different kinds
of measurements for example based on
remote sensing images and in order to
come up with models that somehow explain
how the knowledge evolves
over time and what you plan to do what
we're doing in this project is to to
include an ill new set of technology to
support data acquisition by using
digital cameras so basically we are
proposing middle scale approach which is
concerned with dub and we are interested
in orbit in a kind of photos basis of
cameras the idea is to somehow
understand Finola logical phenomena by
taking to account features extracted
from those images and one of the targets
ads also provide new approach for data
acquisition so that technology experts
do not need to go to the field and make
their observations the main objectives
of this project are therefore to propose
a new set of technology for supporting
hamad phenology monitoring and I also
come up with different strategy models
methods and algorithms to process the
raw data betanian with these new
technologies for further information
please check our website in this
particular talk we are interested in
addressing this identification problem
would like to to guide the knowledge
experts on finding individuals in the in
the target region which might be of
interest in terms of observing servation
from the field we also like to support
different my knee perspectives in the
sense that you might we may come up with
new unexpected correlations among among
different species and of course since
different you know plants are correlated
with different kinds of animals such as
insect by providing this kind of
identification approach would also to
support ecosystem understanding okay
regarding dataset creation we have
installed a camera on top of a tower and
we have been obtaining this kind of
image and the novelty here is okay there
are some other initiatives but most of
them are related or have been proposed
in the context of temperate regions
which usually have a film fewer number
of species in comparison with tropical
species so to the best of our knowledge
diseases on one of the first initiatives
to fill this gap we in concerning our
study area we have being work on on a
region it is cerrado region which is a
very typical vegetation in Brazil the
results that we're going to present here
concerned the use of machine learning to
support identification process is based
on a or not on a data set or betanian
from august through october which is a
which refers to the kind of the main
living season in the sense that you know
from the image perspective we expect
that to have a kind of greening
phenomena for most of the species and we
with the help of the knowledge expert we
define it some target regions that could
be used to guide our machine learning
approaches usually this initial step is
based on the definition of specific mask
is each one for each target species
and by extracting the average color
information grain red and blue
information which is the commonest
approach for encoding phonological
behavior over time we have been
obtaining this kind of time series as
you can see the painting of the region
different patterns are have been found
for example in the case of the first two
species here you have a kind of greening
patterning the sense that you have
higher green values at the end of of the
time series you know in the specific
period that I have mentioned before for
the species the behavior is different
you have a kind of stable you know you
have a kind of stable behavior of the
time city which is expected given field
observation so we our first steps in
this project was to correlate with the
features that we have instructed for
image with data orbiting it from field
observation and as you can see for
evergreen species and or for deciduous
species which you have the greening a
well-defined it greenie season we were
able to more or less correlate our image
based data and okay machine learning so
how do you model our identification
problem using machine learning computer
vision techniques so basically given the
set of images we extracted the time
series based on in in this particular
example color information and each
time series or set of time series are
related to specific regions in the image
and we model the our feature space for
image classification as a problem of
finding appropriate cluster of time
series using the time series values to
guide the learning process well the main
challenge here is concerned with the
fact that to have a small sample size
the small sample size problem which is
relate to the fact that we need you know
clever machine learning approach to to
learn what is going on based on on on
small samples and also we face some
problems regarding their symmetry in
training in the sense that you have much
more examples of a known or a target
that are not of interest then samples
from the target species and as you can
imagine it will also face a lot of Huard
challenges you know depending on the
season you can have as you can see in
this picture some gaps in our yard data
because of you know in this case our are
broke and we miss a lot of data so we
need to do with that in our machine
learning approach another issue you're
going to illustrate using this
particular video here we have some
problems concerning shadows of the tower
let me repeat again okay and also there
are some distortion caused by the
changes in the arm so we need to come up
with an old hedge stration and image
alignment procedures to to correct
the image okay we we developed a kind of
approach based on three steps the first
one is based on perform a kind of mood
scale segmentation and the second step
we is we we perform a kind of feature
extraction based on different regions
defined for each scale and the third in
the third step we perform training using
a kind of mood scale you approach okay
regarding segmentation there is a kind
of segments that we obtain here a kind
of zoom for a specific region in the
image for each region for each scale we
tracked appropriate features in this
particular case here we use the average
going red and blue information so we're
focusing on call information in this
result are going to present here and for
a mood scale training we are using mood
scale classifier based on at the boost
which basically tries to combine to
combine the opinion of different weak
classifiers in terms of classification
we combine five scales three bands 13
time stamps which represent a daughter
of 195 classifiers and to illustrate our
results we we selected two important
species a speedo Spearman makhanya
defined appropriate masks for performing
training and part of the mask were used
for training part of the mass / where is
it for testing and we obtaining this
kind of result it would like to do
emphasize here that another
important aspect of this project is is
concerned with the fact that you have be
retaining at different images for
different time stamps along the day so
have been obtaining 5 image per hour
from 6am to 6pm so this is the reason
that the output of these different
classifiers are quite different of
course depending on on different
lighting and color conditions and in
this particular work we are also
interested in determine you know the
most appropriate time for observing a
given species in for performing
classification and he garden
classification accuracy we observed that
yes different species present different
patterns in terms of power information
and another interesting result is
concerned with the fact that early in
late hours are very useful for
identifying correctly those species so
just to illustrate how this approach
would be used in a real world scenario
we have a knowledge expert would for
example select a mask as you can see
here and by using our approach we could
generate other possible regions in which
they would observe the same species and
so to sum up oh
to summarize what we have been
discussing so far we proposed that kind
of mood scale classification approach
for performing the identification of
target species in a kind of analogical
study environment and we found out that
yes we can use this approach to
identifying this kind of image and also
we found out that some specific
character issues that were not expected
before for example that extreme hours
are useful for this kind of analysis
what to plan to do is to perform
correlation with climate variables with
also to to consider all kinds of
individuals and of a single species in
order to perform more experiments with
our approach and also we have been
proposing a set of new representations
for improving the the classification
results in particular we started working
on textual information and the results
are are very good as well and we
submitted it to a journal we also
developed new approaches to encode and
to compress a little bit the
representation the idea here is instead
of using the whole set of images we
select just those pixels which are
related to the target species and you
generate what we named a visual rhythm
which is a kind of very nice
representation of each individual and
that that those results here concerning
the use of this
the presentation will be presented in
September in the ICP conference we also
we are trying to evaluate other
approaches to characterize time series
in this particular work we investigated
different shape based methods the idea
here for example is to consider a time
series as a kind of open contour and
take advantage of existing shape based
representation methods for you know deal
with possible noise found in our data
and we're going to present those results
in in July in I guards conference this
is a kind of example of results that we
have obtained it and as you can see
depending on the time of the day you
have a different behavior in terms of
time series representation using shape
information and again extreme hours seem
seem to be a more promising in terms of
classification in retrieval future
development we have been stalling now
cameras on different sides with
different kinds of vegetation so we for
sure face new problems regarding data
acquisition and also new charge new
challenges in terms of phenological
pattern recognition and we also planned
to to go to to another a scale when and
also use our machine learning methods to
identify similar regions you know from
images of attaining by drones or similar
or using similar technology and we
believe that by using that
we're going to have a more powerful
framework for the knowledge to support
the knowledge it studs okay and I'd like
to invite you all for you know discuss a
little bit more on some of achievements
of the context of this project in the
demo fast session we're going to present
a desktop tool for managing the time
series and the betanian images and also
some prototypes on on developed for
portable devices to support data
acquisition the field so be glad to
present some other results during the
demo fashion thank you very much thank
you very much to have any questions to
suck easier database available the data
set you mean not yet to it currently
there is another branch of this project
with concern with the development of
information system we are defining the
databases defining you know the the most
typical queries based on the knowledge
expert needs and sometime maybe this
year we're gonna you know make part of
this data available in the website of
the project please keep in touch so that
you can let you know when you know the
hour to is available online okay so
eventually then yes any other questions
okay I have a quick question that seemed
at the end you are indicating that you
will be adding additional sensors that
have a wider scale of you do you plan to
use the sort of the local phenology
sensors local cameras to validate the
larger ones as you work in yes this one
of our particular goals is to understand
the correlation
you know among the data between up from
different scales and maybe to understand
a little bit more about what kind of
noise is associated with each data or
betanians on different scales and how to
deal with this noise more properly in
infinite logical models yeah okay very
good thank you again okay thank you okay
our next speaker is Paul row from the
Queensland University I'm sorry yeah
queensland university of technology
Nelson yeah
okay thank you so my name is Paul row
I'm from Queensland University of
Technology what I want to do to talk a
bit about some of the bio acoustic
sensing work which we've been doing I'm
going to give a bit of an overview of
some of the analysis techniques that
we've been using and i'll also give a
bit of a sort of background about the
the technique and why why we think it's
a good technique first of all let me
just point out where I'm from so this is
Brisbane in Australia it's a great place
on invite you all to sort of visit the
future and visiting please please please
get in touch ok so the background
motivation for this work is sort of as
follows the it's interesting I work with
a lot of ecologists and I'm a computer
scientist and when I talk to the
ecologists they sort of often use
similar language that I'm used to using
I talk about systems and services but
they're talking about the earth in terms
of systems and services they talk about
the the sort of services that the earth
provides to us and the impacts that we
have on the earth so there's a sort of a
sort of feedback loop here and of course
it's important to sort of understand
what's what's going on so in order to
better understand the system and to be a
better manage the system you need to
have a sort of monitor what's going on
and this is you know original a sort of
Galileo's um you know make you know if
you make things countable so that you
can better better suffer measure so what
we want to do is to get better
information about the the health if you
like that I understand that's a bit of a
controversial term to ecologist about
the sort of the health of the
environment so we can better understand
what's happening to the to the world so
Australia is is classed as mega diverse
just as Brazil is so it means that
there's a very large and intriguing
biota in Australia we've got lots of
weird and wonderful animals just as you
you have here
Australia also is I guess we sort of
suffered from a lot of problems of rare
species becoming kind of extinct over
the years Australian ecology is quite
fragile is very dry continent Australia
like Brazil were an exporter we export a
lot of sort of food so the the natural
environment is very sort of important to
to Australia so it's something that
needs to be sort of preserved for for
many reasons the sort of traditional way
that monitoring is done for fauna is
sort of like this essentially it's very
sort of manual it involves a lot of in
the field observation it's possible to
monitor floral biodiversity using remote
sensing by using kind of you can use
UAVs or sort of satellite imagery and
things like that new colleges will often
talk about those things being kind of
proxies for the faunal biodiversity so
they'll measure sort of floor and use
that as a proxy for the faunal diversity
but when they actually do want to
measure a formal diversity the way that
they do that is actually by going into
the fields it actually involves lots of
people going to the field and taking
observations and obviously that sort of
a very error-prone difficult and
expensive sort of processed because of
the people are involved you might also
deuce if you look at these um slides
it's also sort of prerequisite to be an
old guy with a beard and a hat that's
not necessarily the case though so what
we want to do is we want to find a tool
for measuring funnel biodiversity so the
idea that we have is to use acoustics
for this is this tool so we want to use
an acoustics by essentially sort of
listening to the environment I used to
sort of talk about putting tape
recorders out into the environment but
then I realized that most of my students
didn't understand what tape recorders
are and later on I'll talk about CDs now
I realize that now a lot of students
don't even know what CDs are some
running out of what ways explaining this
but essentially what we want to do is to
put out some sort of recording devices
into the environment to sort of give the
the scientists if you like lots of sort
of our ears so we can give them sort of
extended ears many years so they can
listen to the sort of the health of the
environment listen to all the species
are there can get an idea about what's
going on so let me just play some sound
it was work it's not working is it
okay so that it's meant to be some sound
flying they will continue I guess let me
just check my volumes up here
so the the issue is how to analyze this
sound we're going to record all of the
sound out in the environment we're going
to put lots of sensors out there have
all the sound coming in so how we're
going to analyze it it actually goes a
bit more than that so it's more than
just scaling observations so it's more
than just getting lots more observations
that we could make if we were only using
people it's also about this idea of
having what we call an acoustic
observatory so the idea of the
observatory is that we end up with this
sort of permanent record of the
environment and this is really really
important because there aren't good ways
to get direct and permanent sort of
objective records of environmental
health particularly if you want to do a
forms of environmental accounting if you
want to start doing things like
environmental auditing if you want to
you know work out if there's some
environmental offsetting occurring and
you actually want to get a sort of
measure that over time to see how the
environments changing or to check
whether indeed some offset is working
where the summit has been some
development and some developers offset
the development somewhere else in
exchange for rehabilitating some land to
make sure that the the environment of
that is meant to be that the health of
that piece of environments meant to be
improving you need some sort of record
of this the great thing about acoustics
as you get this sort of record you get
this sort of permanent record that you
can go back to so it's not just about
scaling the observation it's about
having this permanent record that you
can go back to and it also means that
you can go back to this record to to
reanalyze it so there's all sort of
saying about one person's one person
sort of a noise being another person's
signal also holds here so if someone's
been doing some sort of monitoring just
for one particular species other people
who are going to go back to but go back
to the the data and re interrogate it so
just as with an astronomical observatory
you can go back and re interrogate the
data you can do the same thing here okay
so that's a very very brief overview of
the motivation for why you might want to
use sound as a way to not just sort of
scale observation but also to have this
sort of permanent record with the
environmental health
so that you can kind of require it and
so you can also have this sort of
permanent record about the environment
about what's been sort of occurring
there so what I want to do now is just
to give a bit of an overview of how you
can kind of actually achieve that that
analysis so I'm not going to drill into
a lot of detail i'm going to keep it
sort of fairly fairly high level but i'm
happy to sort of talk about some of the
details if you wish or will point you
towards papers okay so let's try to try
some more sound see if this will work so
this is a this is a spectrum whoops
except i wanted this is a spectrogram
which is a visual representation of the
sound we have any sound
ok
that wasn't the sound
don't think I'd say that that's meant to
be a frog anyways you know well we'll
move on so that essentially what we what
we want to do is we've got sound which
looks like this and I'll come back to
how it looks because that's kind of
important too so this is a visual
representation of the sound this is a
sort of a spectrogram and we want to
sort of annotate it to understand what's
actually going on so we want to annotate
so we can kind of see if you like within
the sound what some what the particular
kind of calls are so here we've got
these are these are all Australian birds
so you won't know them but we've got a
bunch of different um the birds are
calling this is this is sort of a bit of
a pond here because some if you actually
look we've actually got two birds
calling at the same time so this is
analogous I've called this a cockatoo
problem it's analogous to what's called
the cocktail problem in cognition and in
Nam speech recognition where you get
your trying to understand what time
people are saying when you actually got
the site you know many people speaking
at this at the same time so this is one
of the issues that we've got with
dealing with the sound the the other
issue with sound and processing sound as
we end up with a lot of it so we've got
about our we haven't got petabytes a day
we've got about 100 terabytes of data
and we've accumulated so far so it's a
bit like nature's iTunes ok so we've got
all this sound it's being kind of
recorded 24 by 7 we're keeping all of
the sound because we want to have this
this kind of acoustic Observatory ride
and then we need to sort of somehow
analyze all of this data so all the
sounds pouring in from all of our senses
you want to analyze it somehow and come
out with some sort of graphs about
what's happening to the environment so
essentially what we've done is rather
than having to have sort of armies of
people go out into the environment to
make observations we've translated that
problem into a data analysis
problem person now instead of needing
armies of people to to to go out and
make the observations you know in effect
when the armies of people to to listen
to all of the sound if we were going to
listen to it so the this is how he
started thinking about the problem this
is how I started thinking about the
problem this some this doesn't work so
the kind of classic approach the
approach that a computer scientist like
me would take it to say okay so what
we've got is we've got the sort of a
sound at the top here and somehow we
need to understand you know every let's
be careful not to swear here every sort
of click whistle and a noise in the in
the sound okay so that's a sort of a
classic reductionist approach it's
saying we've got the sound we want to
understand everything in the sound I
want a magic black box that's going to
understand everything and then I'll have
this sort of fully annotated data which
I can then feed into Excel and do love
my analyses on to understand how the
environment is sort of changing and
that's what if you know if you use
something like in a sound town to work
out what a tune is or Syria or something
like that that's kind of what you'd
expect but it doesn't really work the
issue is is that developing personal
developing kind of recognizes that are
expensive you know people like Microsoft
have invested I don't know how much but
I'm guessing it must be millions if not
tens of millions of dollars into doing
kind of things like speech recognition
and it only works very well if you've
got a very clean channel and if it's
English and all these other things we've
got a lot of variation in sound so
species communicate in many different
ways and we've got this very low
signal-to-noise ratio we've got lots and
lots of noise and we've got classic
things like this cocktail problem we
know they're really hard in which we
can't even solve with millions of
dollars of research into human speech
processing so really trying to sort of
it this way in terms of having this
magic black box just doesn't doesn't
quite work on the other the other thing
is that the scientists won't trust the
results either so even if you do this
you think
got this magic black box the ecologists
are not going to believe that you know
if you just give them this list of
species that you've sort of also
magically produced from the sound father
the ecologists aren't going to believe
it anyway so the the classic gum and I
guess the classics of problem solving
approach is to completely skirt around
the problem and to solve a different
problem the different problem is this is
to say that the problem is not to
analyze all this sound completely
automatically the problem is we've got a
small amount of the users time we've got
maybe an hour of the users time so as a
scientist the scientists may be prepared
to spend half an hour an hour analyzing
the data but in that hour of time that
they use for analyzing the data we
actually want to analyze say a week or a
month worth of sound okay so we're not
something on a magic box what we're
saying is that we want to be able to
just use the the users time very
efficiently the other thing is that we
so so so what that really means is we're
taking a holistic view and we're
thinking about the whole system
including the users so if you actually
look at the the ways that you can
actually analyze sound there and you
actually sort of bring the users into
the picture you see that there's quite a
few different ways we could take a
manual approach we could have a semi
automated approach where the users are
actually interacting with the system to
do some interactive data analysis or if
we do have in an automated approach
we're going to need the user to get
involved in some amount of training and
avicennia verification of the results so
all of the forms of analyses which we
now look at incorporate users they
always have done it's just that before
we were rather blinkered and what we
were doing and if we bring users into
the picture we can do a lot more
interesting things so what have we
actually done well the first thing is
that if you're going to do these sorts
of tests and experiments you need some
existing data set so we've actually gone
ahead and we've actually done a complete
manual analysis of three days of data
we've had employed a bunch of
birdwatchers to completely analyze this
33 days of data that we've got
some sort of ground truth we've got some
a test data set which we can use and
people are doing speech analysis and
things like this is one of the first
things you do and other speakers have
mentioned this you need this existing
data set to actually sort of test what
you're doing out on this is so we've got
over ten we've got tens of thousands of
these annotations in this data and I
guess the important thing about this
graph is there's a long tail so there
are a few but in this case these are
birds it's a few birds in our survey
that cool very often and a lot that
don't call very often so this is what we
call our weapons of mass detection so
this is a set each of these is the work
of a research student and these are the
techniques that we've used for doing
some Sam analysis so the first trick I
guess is not to listen to all the data
you record all of the data but you just
do some sampling so you do some classic
time based sampling and you can kind of
work out that even by just doing a small
amount of sampling if you do it at the
right time so dawn and dusk typically
for birds or in Australia if you want
the amphibians and things you do it just
after it's been raining then you can
actually get a lot of high percentage of
the species so just by doing some sort
of randomized sampling around dawn and
does another proach which originally I
was going to pull the radio times
approach but you need to be English to
understand that is the sort of guided
listening approach and what we do here
is we are extracting a bunch of
different indices which give a different
metrics for giving a holistic overview
of the sound so we look at measures of
sort of entropy acoustic complexity
variation and intensity and things like
this we're now looking at kind of doing
false color imagery a bit like the false
color satellite imagery and you can use
that to kind of guide users to wear from
a I guess from an information-theoretic
point of view there's some sort of
interesting things in the signal
another approach is where rather than
sort of have this problem of a
recognizer which is expensive develop is
you have some more general recognizes
and you let the users sort of build up
these recognizers so we employ here a
sort of a syntactic pattern recognition
approach where we have these pattern
recognizers for recognizing different
bits of the signal corresponding to sort
of we know wideband noise clicks and
whistles and things like that and then
we can kind of combine those in
different ways just as in an editor you
might kind of combine different graphic
elements and things like that another
approach is to say we don't listen to
the sound at all because humans are very
visual and we can actually scan through
the sound visually so for some
particular calls we can kind of scan
through the sound for example these
particles here and they're fairly easy
to recognize and so just by sort of
fast-forwarding through those
spectrograms we can get at least an
order of magnitude improvement over
actually listening to the sound final
approach is one of the final approaches
that we're looking at is some is
birdwatcher stretching leveraging
birdwatchers and unlike kind of more
tasking based citizen science approaches
or crowdsourcing based approaches we're
sort of trying to take this sort of army
of of dedicated birdwatchers and to sort
of translate their real-world pastime
into a virtual pastime and that's quite
new no one's done that kind of thing
before to take a real pastime actually
translate it into a virtual space so
we're actually trying to translate the
birdwatchers pastime of going out into
the field listening to birds and trying
to do that in a virtual space where they
actually listen to sound and trying to
effectively sort of emulate the some of
the sort of social things that go on
during a bird walk
this is just some of our sound recording
hardware sand the devices and things
that we use we've got a number of
project this is only a small smattering
of the project but where we've been
working with them for example this
gastric mouth brooding frog which is
considered to be sort of extinct where
we're trying to find if there really is
still are some of them around working
with koalas koalas make a interesting
bellowing sound to try and locate those
and many other things working with
cryptic bird species in the life so
sound to conclude sound off as a
promising approach to monitoring the
environment it produces a lot of data
which is sort of interesting if you
introduce a big data and things like
that it's it's difficult to analyze
sound but if you sort of bring the users
in and consider the users as being part
of your analysis than that she sort of
simplifies the problem and gives you a
lot more other interesting opportunities
in terms of analyzing the sound so in
general I think there are a lot of
interesting science research problems in
south and sound bioacoustics and it's
particularly missing once you bring the
users in because you have a lot of
interesting of HCI issues and things
that creep in I'd like to acknowledge my
team and research students and Microsoft
researchers that have supported this
work and SP not sure if I pronounced
that right and much less research for
inviting me
okay well I have like hundreds of
questions that came up in my mind but
none I'll start with you part of the
data that you are dealing with is based
on crowdsourcing what kind of
technologies people were using in order
to make that data available for dating
session so yeah so so the crowdsourcing
I was referring to there was the
crowd-sourced analysis of the data so
this is where we've already recorded the
data but one that we do have also
projects where we've actually got users
going out and recording sounds with with
phones and the like so I've just got a
student who started looking at that now
so they're actually sort of recording
you know animal sounds using using a
mobile device and the ideas that's
thinking every sort of uploaded and
analyzed on a server but
about 20 20 million in so yeah I said so
the question was how many people would
have the expertise to do the sound
labeling so several answers that first
or that there aren't there are a lot of
bird watcher so in the US that's about
20 million bird watchers about 20,000 in
Australia we're also looking at how you
can actually sort of train people that
are interested in doing that kind of
analysis so there are the real quite a
lot and yeah we'll see i guess and yet
oh i have as your sort of talking about
taking the signal without doing the
species identification and calculating
snr and so forth and your ultimate goal
is like a logical health is there how is
your thinking in regard to using species
identification as an intermediate step
versus just skipping over that and
looking at a ecological health as a
function of the spectrum so yeah that's
a great question so that that's
something we've looked at or are looking
at I don't think we've got a definitive
answer to that so that the analogy would
be something like a library where you
might say okay so without actually being
understand what people are saying and
what a library is if you have some sort
of data about a library and the sounds
and the library you might say well if
it's completely quiet that's probably
not healthy if there's a lot of noise or
some party going on that's probably not
healthy but if you hear you know if
there's just sort of shuffling of chairs
and things without actually be able to
analyze it and work out that's shuffling
of chairs just from a signature you
might know that's a healthier
environment for a library so yeah that
it's a great research question I'm where
we started off looking at that I think
now we're moving more towards this idea
of just using it as a guide to guide
users to sort of search through the date
/ yeah it's fascinating question because
it is signal that that's the thing it is
signal it's not the species of making
these sounds to communicate and it's
deliberate so its signal all right any
other questions before we move on yes
okay we thought about mimicking the
sound to trigger the animals to make
more sense I yes but in some ways we're
trying to get away from that so one of
the projects one of the ways that some
the on earth Allah gist's monitor the
cryptic species is by doing core
playback and that's not sort of good you
need to get ethical clearance and things
for that and you're actually disturbing
the environment and potentially
attracting more birds in by doing that
playback so by doing this sort of
passive monitoring you actually get a
better idea about the species that are
there because you're you're disturbing
the environment less alright well thank
you very much Paul and move on our next
speaker is Alan hanbury vienna
university of technology I'll give a
minute to
so good afternoon now i'll be talking
about a cloud-based evaluation
infrastructure for medical image
analysis and search now as we've seen
today there are many tasks that people
wish to automate by computer so we take
two at random it could be finding
relevant documents in a large document
collection or it could be segmenting the
organs in radiology images now if you
have such tasks there are many people
who will write algorithms to solve these
tasks so you end up with many algorithms
which solve the same tasks and you don't
know which is the best one so basically
what you want to do is line all the
algorithms up give them a task let them
loose on the algorithms see how they
perform and see the reasons why they
perform as they do so this is where
evaluation comes in so there are many
evaluation activities that are organized
so they could be called for example
evaluation campaigns challengers
benchmarks competitions and basically
this is where the organizers make
available data and tasks for
participants to try out their algorithms
on and such evaluation activities
actually make economic sense so one of
the activities that has been going for
the longest is organized by the NIST of
the National Institute of Standards and
Technology in the US and this is the
trek campaign and after almost 20 years
of running this they asked an
independent organization if running this
evaluation campaign makes economic sense
and this organization came up with the
result that for every one dollar that
NIST invests in the track evaluation
campaign there are between three and
five dollars in bed
if it's accrued to the information
retrieval researchers why is this
because an information retrieval
researchers don't have to let's say
waste their time putting together the
data to try the algorithms out on or
creating ground truth whatever there are
people who do this and they get
objective results to compare their
algorithms on of course this is also
useful for companies because they get to
see which our Griffin's work best for
certain tasks and of course this also
has scientific impact for the
researchers they publish papers on their
evaluation results and these papers gets
itís so traditionally evaluation
campaigns are done as follows you have
at the top the organizer and the
organizer has to first of all come up
with some tasks that he or she wants the
participants to take part in has to find
the data on which these tasks can be
carried out and also ensure that there
is enough ground truth available to be
able to evaluate the algorithms now the
ground truth could actually be quite
tricky because well firstly you might
just have it but if you have enough
ground truth to run a successful
evaluation campaign then maybe you have
to think that the task is maybe not so
interesting because you already have the
ground truth what's the use of
automating it so in practice what you
need is a really good way of generating
a really good approximation to the
ground truth on a large amount of data
and the various tricks let's say that
you could use to do this so let's say
the organizer has all of this it's up at
the top there and then the evaluation
campaign gets launched so as you've your
participants in the evaluation campaign
start connecting to the organizer they
download the tasks in the data and they
have a certain amount of time in which
they can carry the tasks out on the data
when they're finished with this they
then send the results back yogam
in hopefully the format that was defined
by the organizers and the organizer will
compare these results to the ground
truth and come up with an evaluation of
how the algorithm is performed and then
usually all of the participants in the
organizers get together and they discuss
the results so that's how it's
traditionally done now we get to the
problem what if we want to do an
evaluation campaign on many terabytes of
data so let's say we have something like
five terabytes of data so we run into
the problem can we just ask the
participants to download five terabytes
of data and run their algorithms on this
do they have sufficient computing power
to do this will they run into other
problems well firstly if they have a
slow internet connection it could take a
long time for them to download the data
so they have been very solutions to this
the common solution is we send the data
on hard disks through the post this has
been done in the trek campaign for
example there's a lot of overhead
involved with this and yeah so it's not
the most efficient way of doing it so
what we've been looking at is actually
running these evaluation campaigns in
the cloud and there's a project called
the visceral project which is working on
this so the idea with the visceral
project is that we want to bring the
algorithms to the data instead of
bringing the data to the algorithms
usually the algorithms are much smaller
so you could just load them to where the
data is and run them there so the way
we're doing it is we're putting the data
on the cloud and the participants in the
evaluation campaign get computing
instances on the cloud and get to run
their algorithms on the data in the
cloud so in visceral will be running two
benchmarks so the first benchmark
imaginatively called benchmark one has
to do with whole body labeling in 3d
imaging data so you see an example over
there on the right of a
three-dimensional ready
image with some ground truth drawn in
for the organs that need to be segmented
so basically for the data we are working
together with some hospitals and they
are providing to us for use in the
project images that have been acquired
during daily routine clinical work so
basically there is the PAC system in the
hospital where all the radiology images
are stored and we're downloading those
images and we'll be using them in this
benchmark the sort of images that we're
getting so we're getting a whole-body
MRI and CT scans like the one that you
see on the right again with some manual
ground truth of the organs that should
be found drawn in we're getting some
imaging of the abdomen various other
modalities like contrast-enhanced CT and
which is used for oncological staging so
the participants will be provided by
with all of these modalities and this is
this multiple modality approach is
useful because there's some organs that
can be more easily found in some
modalities than others so why is this
interesting why do we want participants
to get these radiology images and
segments all of the organs automatically
well in hospitals there are very many
radiology images taken routinely so many
patients will come every day and get MRI
or CT scans done and usually what
happens to this data is it put on the
packs saved in the hospital archived and
never looked at again so there's all
this information there potentially it
would be possible to find useful things
out from it but it's just there nobody
looks at it again what's getting worse
is the amount of imaging that's captured
so the device is getting better and
better the resolutions are getting
higher so this means that you're getting
more and more images which
just getting saved taking up more
storage space and not being used so if
it could automatically as a first step
locate all of the organs in these images
this is then a first step towards
converting this unstructured data into
structured data and we could do some
data mining on this to find out many
useful insights now there are very many
medical research groups working on this
problem of locating organs in 3d images
segmenting them and so on they have the
problem is very difficult to get the
data so often due to ethical
considerations and so on hospitals are
not so happy with giving this data away
for researchers to use but because we
are doing this at a larger scale we've
had some good cooperation from hospitals
so we actually have the data now the
next problem is if you're doing this
alone the manual rotation is expensive
so common approaches if you want to do
manual annotation for such an evaluation
campaign is you put it out on let's say
the mechanical turk and you ask people
to do the annotation here we have
medical images you need experts to do a
reasonable or good segmentation of such
images so radiologists in this case and
radiologists time is expensive so now
I'm going to tell you the technical side
of how we're going to run the evaluation
campaign and after the technical side
are tell you more about the tasks that
we're going to set so this whole
benchmark is going to be run in the
Microsoft Azure cloud and we have inside
the cloud a registration system so the
participants will connect to the
registration system sign up and as soon
as they've signed up they will get
access to computing instances in the
cloud which are linked to the training
data
in this case they will have four months
to carry out the tasks that we have set
on this data so that by the end of these
four months what they have to ensure is
that the programs which are so the
executables which carry out the tasks
that are set are installed on these
computing instances at the same time the
organizers are going to be writing an
analysis system and on the submission
deadline the participants will lose
access to their computing instances
these will be transferred to the control
of the organizers and the analysis
system will run the algorithms on some
test data that the participants have
never seen before now I mentioned that
getting the ground truth is complex so
in addition to this we also have what we
call the annotation management system
and what this does is it sends images to
a team of radiologists that we're paying
with the instructions to annotate these
images now the nice thing about creating
ground truth in this way is that we can
actually look at information that is
generated during the benchmark and use
this to decide on which are the best
images to get the radiologists to
annotate so work out how to make the
best use of this quite expensive time
that the the radiologists are giving us
so they will generate the the gold
standard and then we will of course
compare this to what the automated
systems do so that's how the task is
going to be run from a technical side
I'll now give you a bit of information
on how it's going to run the task
themselves so in fact we're going to
have two tasks
the first one we've called the
multi-layered task so basically the
participants will be getting hold of
some training data so the CT MRI and so
on images with some manual annotations
they then have the four months to write
or to use this data to train their
algorithms so that if they're a give are
given images which are not annotated it
these algorithms should produce these
annotations at the end of the four
months these trained algorithms should
be implemented in their computing
instances then we move on to the
evaluation phase so of course the
participants lose control of their
computing instances and they are linked
to the gold corpus evaluation so now the
organizers will run these algorithms and
we'll look at the results that are
produced by them and compare them to the
ground truth now we know that not every
research group can do everything so
there are some groups that specialize on
locating sort of roughly where the
organs are the other groups that
specialize on actually finding the
boundaries of the organ so doing the
detailed segmentation of each of the
organs so if we just give images with no
further information there might be some
groups who say well we can't do
everything so we've thought of that and
if groups asked for it we will also
provide additional information so an
example of additional information we
could provide you know even during the
training stage could be some sort of
landmarks which give the locations of
the organs so four groups that can't do
the location of the organs automatically
they sort of know where the organs are
and can use that to see the the
segmentation
so that's the first task the second task
we've called the surprise organ task and
basically the idea here is we want to
give groups that are not specialized in
medical imaging but that are specialized
let's say in machine learning the
possibility to try out the machine
learning algorithms in this benchmark so
here again we will be giving the group's
the training data with the manual
annotations and their task will be to
write an algorithm which when given the
many manual annotations for a single
organ will learn from those how to
segment that organ so the result at the
end of the training phase should be an
algorithm that does things in two steps
firstly it learns and then it applies
that on data which is not labeled so
then in the evaluation phase in the
first step we will give some example
images with some organ that we've chosen
and haven't told before which one it is
and then the algorithm will run through
its learning stage and come up with a
classifier and then we give this
classifier the unlabeled data and it
should find and segment the organ which
is then again evaluated
so this is the preliminary timeline so
we plan to open the benchmark on the
first of August so from then it will be
possible to register to take part and to
get access to the data and the computing
instances on the 26 of September there's
a workshop at the Makai on medical
computer vision so there there was be a
session to discuss experiences with this
benchmark on the thirtieth of September
we plan to have what we call initial
verification runs so we're going to ask
participants just to provide some
results on small amount of data so that
we can just check that everything's
running correctly and the final closure
of the benchmark when the computing
instances were passed the organizers is
on the thirtieth of November so that's
what we're planning to do in the next
month's we've also been thinking beyond
that so some of the problems that we
have come up with but haven't solved yet
or firstly what we're dealing with
private data so the moment we have the
permission from the ethical Committees
of the hospitals to use the anonymized
data to run these benchmarks on but of
course if you do the anonymization you
could potentially lose some information
that could be quite interesting to find
out useful things so does it make sense
seeing that it's not necessary for the
participants to ever see the test data
that we put anonymized data as the test
data we can run the algorithms on this
that the participants never see the data
can the participants learn something
from results from data that they've
never seen this is something we learn to
think about we're also thinking of
moving it towards the signs so putting
in some data identifiers so it's
possible to cite the data also thinking
is it possible to come up with algorithm
identify us so this is the algorithm
that produced these results at the
moment we're running this in that we
have some deadlines so the algorithms
should be submitted by a certain
deadline
but of course it would be possible to
run this continuously so whenever
somebody wants to try out some
algorithms on the data they could submit
it and within a reasonable amount of
time get the results back so this is
something that we're looking into
implementing and finally another problem
that I've mentioned is that there could
be some groups that right algorithms for
locating organs there could be other
groups that write algorithms for
segmenting organs cause it makes sense
to put these algorithms together so you
first run the location algorithm then
the segmentation algorithm you get of
course here lots of there lots of
combination possibilities what's the
best combination of organs of algorithms
from different groups can we test this
out and some even longer term challenges
so at the moment we're running the
benchmark on the Microsoft Azure cloud
so we're getting support from Microsoft
for this but if we take it further we
want to do this in the long term who
would actually pay for using the cloud
so would participants the research
groups let's say be prepared to pay for
the cloud time to take part this is
something need to to think about also
the sharing of components so if some
groups for example provide the locate
location components others provide
segmentation how can we share so that
these algorithms can be used together
and finally if we have solutions that
work how can we transfer them to
industry in an easy way so the visual
project is supported by the European
Commission in the seventh framework
programme and also Microsoft Research
and remember to look at the website you
can join the mailing list to keep up to
date on what's happening because the
first benchmark is opening on the first
of August thank
33 excellent talks we have any questions
here I see one of the back
hello that work yesterday thank you very
much for a great talk I have a couple of
questions to ask the first is do you
have any intention to extend this belong
beyond images image data into a
multimodal data sets that kind of thing
yes so the benchmark to which I haven't
spoken about yet the plan is that this
will deal with let's say more
information retrieval problem so this
would be relevant case retrieval so the
use case there is that a radiologist has
an image there is something in the image
which he or she has never seen before
and then he or she would want to launch
a search into the hospital archives to
find images with similar structures and
the most of the radiology reports that
are found are attached to these images
and of course once we find the radiology
reports we can bring in text search so
we can analyze these find out words that
occur in many radiology reports then use
those for text queries and so on and
another question actually you you
mention it as an open question on your
last slide but I'm trying to ask you
anyway when you're talking about
transfer to industry do you have any
thoughts about how that might be
accomplished yeah I do have some
thoughts but there's still in a very
embryonic stage but I can imagine I mean
had some discussions with some companies
who would be quite prepared to actually
use the cloud if the algorithms are
there and they could get access to them
we would need to work out some sort of
way of that the companies could then pay
let's edit licenses to the researchers
for the use of these algorithms so I
imagine they would need to be some sort
of control or something on the cloud for
how much is used but there are many
ideas for how this could work
all right any other questions ok well
let's thank our speaker again and all
three of our speakers and thank you for
attending I think we now go out into the
lobby for some happy hour and demo fest
so I hope we will all be able to join us
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>