<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>BDA - Mergable Summaries | Coder Coacher - Coaching Coders</title><meta content="BDA - Mergable Summaries - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>BDA - Mergable Summaries</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tSqbx0e5YJk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so the last talk of the bookshop is
going to be a summary talk of course be
talking about summary that can be merged
and incidentally our paper is also many
reserves merge together from six authors
from five different places so this book
shot we have been talking about
summaries a lot and as we know big
data's big but the value of more
precisely the value density is low so we
can often shrink the data size a lot
without losing too much information many
people have talked about that so in
particular many of these summaries
provide a trade-off between Aaron and
Aaron size so if you want the you want
you you want approximation error to be
smaller than you the sizes larger but if
you can tolerate power solid big errors
and then you can kind of work with very
small sub arrays which saves your space
time and communication so there are many
sub rights that have been proposed in
the literature also start with the basic
room sampling which is a very basic
summary that can be that it's very
generic and can be used for a verb for
for many purposes of domain may not be
the best and then there are many
specialized some rates that are designed
to preserve a special kind of
information empirical in particular the
sketches this is a large class of beta
summaries and then we have frequent
items assembly that only preserves the
most important namely the most frequent
items in a very set and then they're
quant housing histogram which
approximately preserve the data
distribution and then finally 4g
immaculately how there are all kinds of
geometric sum raised in particular
corsets which preserve certain geometric
properties of the data set so here we
are interested in the murder ability of
the summaries so in children speaking
more amaura were motability simply says
that summers can be merged just as if
they were simple algebraic operators
like some or
max they are associative and commutative
so here so that means when you have some
rays that can emerge then you can you
can divide your data set it many pieces
and build a summary for every piece and
then more and then more time together in
arbitrary fashion following an arbitrary
merging tree so know that we want the
summer is the quality of summer is to be
preserved no matter how many more days
you will have been that that hot that
hat have been done and in particular
with even in the end you have the
summary for the whole data set it and it
potentially can be peepin can be even
much further with other data sets in the
future so this is actually very similar
to the modern model that was proposed a
few years back although they are the
only consider computing single value
single-valued function so here we are
considering more general purpose
summaries and also in addition to
preserve inequality what we also want
the summary size to be bounded of course
we because we want to rule out the
trivial solution where when you want to
do a merge you simply put some together
without reducing the size so this
trivial solution of course will give a
brief period of the quality but the size
of summary will grow linearly as the as
number of murders which we definitely
don't we definitely want a boy so in so
ideally we want a summary size to be
independent for the base pay that size
but and all at least would be sub linear
in the data set so know that this model
generalized streaming right because in
streaming what you are doing is simply
every merge is merging a summary and a
single item and still so here we allow
the merge the merge between two
summaries so there's the generalize the
stream model so that being said that
means we for the inland and this
motability model we cannot hope to do
anything better than the streaming model
so streaming lower bound also becomes a
lower bound for the motability model
okay there are a lot of applications
that one can think of for for this
vulnerability especially in large-scale
distributed computation
and in particularly in many of systems
program had no control on how things are
merged for example Matt Matt and Matt
reviews everybody in every every video
so essentially there's a lot of murders
merging messages from different mappers
and even before the shut there's trouble
faces there is a combiner which does
some local merging and programmer have
no control of how things are merged and
in the general system which is can be
seen as a you know a lightweight version
of MapReduce we are is only one reducer
here is we also have a merging tree and
which can be deep free for you consider
the local executing trees together so
and again programmers have no control of
how the merging trees are shaped in the
progress which is designed for rack
processing here every in so this is in
so here cut the conversation / CC number
in number of rounds in every round every
node sends a message to a neighbor and
note also receives the message from
Austin a burrs and optionally the
programmer can can space about a
combiner function and then this combiner
program will be used by the system to
combine messages that are designated for
the same node and this is actually from
the original paper which says you know
the messages are combined and the
groupings are presents the combiner
there's no guarantees about the message
about the which message are combined and
groupings presented to combine all the
order of combining so combiner should be
only be enabled for commutative and
associative operations so that means
essentially we want more ability of the
summaries okay okay and finally in
social networks of course every session
of like some data sets and they we
definitely do not want to community
oddly the back to the base station which
will confuse a lot of energy so they
will compute a summary for its local
data and then they will transmit the
data submarines back and all of the way
the summaries can be merged again
because sensor nets they we have no
control at all of how the merging tree
looks likely my
very unbalanced so we so we won the
merging absolutely work regardless of
the shape and the size of the merging
tree so in this talk America presenter
few simple examples to show up to show
how murder Billy can be achieved and so
we're going to start some regards star
some with some very simple examples to
give your idea of how things can be
merged suppose we want more to random
samples so for example this we have a
sample we have a sample of size 5 from
data set and here is also a cent of size
5 from another data set we want to merge
these two stems together so that we
still have a sample of size five from
the combined data set so if we only have
a simple themselves they cannot be
merged but if we also maintain the size
of the underlying data sets then things
then the samples can be more easily the
idea is to simply simulate the sampling
process without ripped or a replacement
so so as so and it in the in the in the
so we first with probably one over N 1
plus n 2 we take sample from from the
length from left side okay so that's we
take this into the combined sample and
then we also decrement the and one by
one and then we probably into over in 1
plus and two we take a sample from from
the right side and then we do this one
by one until we have a set of size five
and one can easily argue that this still
gives you a random sample of size five
from the combined data set and finally
we also we also add up the we also
updated the size of the combined data
sets to be associated with this new
sample so this new sample can be again
merged in the future so that is the
example and uh and also other does all
the leaners gadgets that suit afore
mentioned they are favorably workable
because they are linear projections and
they include actually a quite large
class family of sketches so here example
of coming sketch and also are the other
summaries other sketches that are
for into this category can be easily
merged chemical emerged in addition this
summer this linear scan can even can
even be subtracted so that's a given
even better another class of summaries
that can be easily emerged is those
based on the max value or the top K so
for fog than the mean hash summary here
this is very useful for estimating the
similarity between sets so what this is
summarized doing is the apply hash
function H to every atom in the data set
and then we don't we only retain only
retain the case smallest K elements with
smallest hash values so now you have k
smallest letters from mr. sets and case
most angle from the other data sets then
you have 2k and what are you what you
want you want you can do is just to keep
the pic at the case knowledge from 2k
elements so this can also be trivially
merged okay so these are some very easy
examples now we're going to see some a
couple examples that are now so trivial
a bus but I was like as you will see
still we're very simple but the analysis
requires a little bit of work okay so
heavy hitters is one of the earliest
examples problems that have been studied
in a streaming literature taste back to
82 so this is the famous mg algorithm
which maintains the K items that occur
the most frequently and they also the
summary can also exit they are estimate
their frequencies with editing error and
over k plus 1 so the K lager the summary
summary such a larger but the error is
also smaller so many of you have they
have known this album it's actually very
simple so what this summary maintains is
it maintains k up to k different items
and their count their and their cans and
when a new item is to be added to the
summary if they're this item is already
currently in a summary we just increment
is counter increment is counter if this
summer if a new item comes in and this
new arm is not currently
maintained by the summary then if we
current summary currently has less than
K items recently edited summary if the
summary is already full what we do is we
were going to decrement the counters of
all items by one and that's it so that's
the algorithm is very simple so standard
analysis shows that it has error error
band of an over k plus 1 namely in the
end when all this on all items have
arrived then then all the items
currently maintained by the summary they
are cons differs by the real account by
at most n over k plus 1 and any items an
ottoman tent have accounts at most an
over k plus 1 so you can essentially
estimate all the other items by count of
0 so there's a recent result a couple
years ago which gives a stronger bond
which is f1 residual k which means that
we only counted items the total account
except the k most the decay largest
items so this gives you gives you a
stronger bond when the data diffusion is
highly skilled so however so neither of
this tool is good enough for proving
murder ability so the first one is
actually too weak but this one is like
too strong big and then it's just a
strong bond is also not good for me to
believe because we are able to show that
after merging the same band still hose
yes yes for this one so this is the
total account except the K largest items
yeah so that's the rigid bridge account
so what we sort of cheap motability we
have this made this simple observation
which is okay the act the error in smes
em economy is actually a mice em over k
plus 1 where m is simply the sum of a
counter is currently in the summary so
to cease to see this the analysis is
quite actually quite straightforward
because every time we do decrements
we decrement are the counts by 14 k plus
1 different items so every item is
deleted and I am most mice n minus M
over k plus 1 times so that's why every
item has error so much okay so this is
actually a very Equestria forward
analysis but this observation has has
led us us to a very simple merging
Allison and Allison is actually very
intuitive very natural so suppose we
have two mg summaries so the blue one
and the red and the yellow one so first
of course we do the obvious English is
we add up the corresponding counters so
now after this we have may have between
K and 2k counters okay and then next
we're gonna we're going to take the k
plus 1 largest counter and you know
by-by ck k plus 1 in this example case 5
so we are looking at the sixth largest
count so that means 1 2 3 4 5 6 this one
so then we will decrement all counters
by ck + 1 mean this is in the in this
case too okay so that's all we do so it
is clear that out of this procedure the
summary size is K same as before so now
we only need to argue that the same same
error error guarantee which is n plus n
minus M over k plus 1 still hosts for
the merge summary so this can be
analyzed and follows so the reservation
is that the merge subtracts at least k
plus 1 c k plus 1 from counter sams so
this is because we have you know we have
K items remaining so these k items each
have been documented by c k plus 1 and
and also of course the be the k plus
ones not seek a plasma itself is also
deleted okay so we have this inequality
which is the k plus 1 MC capo is smaller
than or equal to the total number of
counters before the merge minus the
total number
total counter the total count in the new
summary and then by interaction the
error in the merge summary is the prior
error which is by our induction
hypothesis and plus the new era which is
which is essentially CK plus 1 and then
if you work have a mess and magically it
gives you the same error guarantee okay
so it's a the algorithm is quite simple
and the analysis is not difficult either
but it's just tough and so that
everything works out okay ah and in fact
this problem has been previously studied
I'm and in sigma two thousand five
minutes I CD and they gave a different
merging option which actually very
complex and there's no guarantee on the
size and also an error increase a fish
merge so if you know in advance how many
more that you're going to have then you
can do provisioning but but but anyway
in our working out zoom in our ability
model we we do not allow the Axum to
know in advance how many more geez there
are and we actually do some experimental
comparison comparing the two algorithms
even giving the knowledge of n to the
previous album with them and we see that
for the achievement same arrow we can
have a reduction in terms of summary
size by a factor of almost 10 and we
also have a control example in which the
previous murdering algorithm completely
failed we have some racism that's very
big well our summaries has the Bunge the
size of k which is guaranteed by the
merging algorithm alright so that's the
the heavy-hitter problem okay and and by
the way we have also seen shown that
space-saving which is another popular
heavy hitter summary this is also
workable and we actually we show that
this space-saving is actually the
isomorphic to the mg algorithm although
it was seen to be quite different on on
the surface okay alright so next to get
a move on to the actual approximation
problem which is also known as a
quantile i'll actually had hit a
he swam in one dimension so here we are
we are so here we are still talking
about a sample from the data set except
that a actual approximation is is in
some sense a more uniform sample in this
example we have we pick essentially
every three points from the data set and
in this example things we simply take
the points of randomly okay um so how do
we so what we mean by more uniform so we
use this following criteria namely we
look at any range are in one dimension
or render easton holy or interval we
could look at any interval and we look
at the fraction of points in the
interval and we also look at the
fraction points in the sample that is in
the interval so ideally we want to
fraction to be close so if the two
factions are only differ only by by some
small epsilon then we say that this
sample is an action approximation okay
so a uniform central only needs went
over aksam central points by just
picking equally spaced points from the
dataset whereas if you do it randomly
then classical result shows that you
need one over epsilon square procedure
points so this is both upper and lower
bound and with this will give you a
absolute approximation with constant
probability so it shows that if we do
more careful sampling we can
significantly reduce the simple sighs
okay um so this is closely related to
the quantile problem where we have a
distribution and the bb5 quantel is
simply the item that is ranked at
position phi where phi some is some
fraction between 0 and 1 so note that
for action approximation this solves the
deal problem where you if you're given a
you for you are you are given the item
you want to estimate estimate its
position in the in the in the data set
so all we have to do is to do
take the interval to be a prefix and
then we can simply count the number of
points in the sample and they use that
as we can play we can simply compute the
fraction of simple points in the range
and use that to approximate Phi and that
will give you a additive error f epsilon
okay once we have that we can do binary
search to find the Quan Tao so these two
prompts are erratic equivalent okay once
we have the contest we can be with a
hike we hide his gun which approximated
distribution well so compared with agree
with hit a histogram which used fixed
binning this histogram is more adaptable
to distribution whereas although fixed
bending is trivially Murderball but what
happens if all the other odd points for
you to say being then this histogram
tells you nothing right where the phone
tells adapt well to the skewness of the
distribution alright so the quantou
problem again as a very classical
streaming problem has been heavily
studied but at least ten papers on the
the same problem and this problem and
the best deterministic algorithm is by
greenwood and connor which is a keynote
speaker and the best randomized
algorithm is a three pack is one way of
showing lock acute 1107 which is
independent of n on a multiple case if
the data set if data is drawn from fixed
universe of such you let's say we are
integers from 1 to 2 power 2 32 then
there is a murder both summary of size 1
or X from love you okay but it requires
the fixed universe so it doesn't work
for Overland serial numbers or
user-defined hugs okay um and there's
there is a comparison based murdering
algorithm for for this problem that
they're the error increases after each
merge so this is not truly murder / by
our definition so our new result is this
thing which is one waxham block 1 point
5 1 1 epsilon this
is independent m and this is also works
in a comparison model so our album is
actually very very simple and this is
also not entirely our invention this it
tasted back to the first streaming paper
the more than pipes and paper so there
they also consider the problem computing
excellent proxim a shin from from from
the in one dimension ok so there the
idea is to first be divided dataset into
pieces of size k and then we do a bottom
but we do a bottom-up binary merging
okay so the base case is every piece is
Sasuke so the that's the same as some in
size so that on the base case or under
leaves under under under under leave lab
of the summaries the base the dataset
itself so now whenever we do a merge we
actually we are merging two summaries of
equal weight namely they represent data
sets of six eyes right so that's what i
call this equal weight merges so the
merging husband is very simple we just
we just combine the two summaries and
then we saw them and then we take every
other element okay so that's the idea so
that's the algorithm so that's near
algorithm but then they were so that so
in their analysis the arab roast
proportionally to the height of the
merging tree so this is not really
murder by our definition so what we did
is very simple we just add a random a
simple randomized first instead of
always taking instead of taking every
other element dieter mystically we with
we flip a coin which probably half we
take all the odd position elements which
probably have we take all the even
position admins and that's it so that's
our algorithm so now now let's see how
why this algorithm preserves error name
the error doesn't grow as the number of
asthma merges that are being that having
that happen done so to show this let me
rewrite this this requirement a little
bit so this requirement can be easy
prevent to the falling to the falling to
the following Emily we want to simply
where we won't estimate the number of
data points in a range okay and we do
that by just stupidly counting the
number of simple points in a range and
this go up and the scaling factor is
simply the end / the simple size so
that's the scaling factor okay and we
want this to be a additive error XOM
okay now if we consider the base case
rarely real namely the base case is
where we work to leaf nodes in a base
case that fits gating factor is too
because it because data set is 2k we
reduce that to k by taking half of the
elements on so let's let's consider any
interval I so so if this interval
contains a even number of points then
our estimator which is two times the
number of silver coins in the range this
has no air at all actually this is
accurate the other case is if the num
points in the range is odd then our
estimator is is actually always wrong
but is in expectations right is correct
it's more precisely is probably have to
be able to have error of plus one which
probably have it it has error nice one
so it's always an unbiased estimator
with error at least error at most one
okay so that's the base case now we look
at the mouth how how things are
happening in the mount in multiple in
this module levels of merging tree so as
we go up to the as we go up the levels
the error the scaling factor also
doubles so that means the error also
doubles but then the number of murders
shrinks by half every time we go up one
level so if you look at the the max the
worst-case errors this grows miss levels
because total air right the same for
every level however because it's we are
doing randomized merging with even even
if it Ruby if we look at the variance
total the total variance the total
variance actually doubles every time we
go up go away
because number of merges decreased by if
I go to but the error increased by error
for every merge increased by a factor
too so if you take the square of the
error then and instead of up so the
variance is actually dominated by the
height by the highest level so it
doesn't matter how many levels are there
down there it only depends on the error
of the top of the highest level then you
go through the computation which I will
now do that we we can figure out that if
you want probably well over dirt I went
over der tattoo to have extra in error
for any given range you'll want your
summary size to be 1 or X alone times
log square root of lock when / third a 1
over Delta and then if you want this
epsilon approximately correct for all
ranges by the way they're only 10 acts
of different range to consider because
if to image are very close if one is
correct the other one must be correct so
there's only one web saloon sufficiently
different renders to consider they apply
another Union bound you get this bound
ok so that's the summary size so that we
have with custom probability we get all
ranges correct within absent an error ok
so this is the equal weight merge
analysis to make it to work for summer
promoting summaries of deep of different
weight we use a standard logarithmic
trick that is we maintain a number of
some race each of each of a different
weight the weights are geometrically
increasing 1 2 4 8 16 32 and then you
will have two different summaries of
different ways what you do is to simply
do this merging by the binary addition
ok and so that every time you do merge
you are still merging two summaries of
equal weight although the auditorium
room they may have different weight ok
ah and this as you give you and this
gives us an extra lock and factor ok and
then there are some then we were able to
get rid of that end login factor oh we
are not where we can rip
login factor by a log 10 acts alone so
the intuition here is that okay if you
if it is large you don't eat we don't
really need to keep all the login levels
so they'd be the smaller weight
summaries smaller weight levels they
don't contribute much to the to the
total error so what we can do is to
replace those lower levels with a buffer
for which we simply use random sampling
and we know that random sampling the
size actually that doesn't depend on em
so that's where we can get rid of the
dependency m so the the the the the the
algorithm itself is actually quite
technical so I will omit the details
alright so the this extra boxes ocean
also naturally extend to higher
dimensions so we here when we what I
just talked about is the 1d case where
we for any interval we want to make sure
that the the fraction of simple points
approximate the real fraction reasonably
well so in higher dimensions we can
simply replace the intervals by any
range that we want a for example we can
consider we can we can look at all the
estate the circles namely we one sample
to be uniform in the sense that for any
circle the fractional points in the
circle approximates well the real
fracturing points in the circle so on
and then this whole theory about we see
about wrench the basis of pounding VC
dimension and we were able to show that
for any range space with publicity
mention the then the we have a more
durable epsilon approximation that head
size has this size epsilon to the power
of minus 2 D over D plus 1 so when these
1 this is 1 epsilon we are all making
some log factors when SD goes higher
this this this goes based approaches to
ok all right and also we cuz there's
some other geometric summaries so this
is the apps on Colonel I from Colonel is
a geometric corset that approximates the
convex hull of the data says reasonably
well and
we also we also have some preliminary
results on multiple apps and kernels
although we the size here has a pretty
large dependency on and login which is
not so nice because in the streaming
case we can have a summary size that is
independent n where in the murder book
case we cannot okay okay so this is a
summary of the summaries that we can
merge so the heavy hitter is very nice
so everything is the same for actual
approximations what the terms take
streaming is a locking factor bigger
than a staircase which we still is still
a big open problem whether we can get
rid of that and the conductor is no we
cannot but it's no still there's no
definite answer for the murder case if
our our Alvin is randomized and there's
the previous fixed universe algorithm
which has a factor of love you and open
question is can we can we replace that
log you by login namely can we make the
the G case I'm very portable and that's
not known for the randomized case is a
best streaming also and this estate best
murder both are the same and if action
colonel if you have log factor there's a
big difference there okay um so some
open problems are other than those that
i mentioned earlier on a bourbon side so
we're also interested or actually
integrated the racket problem is is
there any separation from stream model
and the multiple model because murder
boy is a is more general generalized the
stream model so potentially there could
be problems that are can be done in
streaming model but cannot be been in
the multiple model but we haven't found
any we don't have any proof we don't
have any lower bounds that separates
these two models so far and of course
for such a for any streaming problem you
can add same question can can can this
album work in the vertical model okay
alright so that's a happy</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>