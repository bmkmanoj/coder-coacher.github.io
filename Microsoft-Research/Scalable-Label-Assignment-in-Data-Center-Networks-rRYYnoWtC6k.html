<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scalable Label Assignment in Data Center Networks | Coder Coacher - Coaching Coders</title><meta content="Scalable Label Assignment in Data Center Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scalable Label Assignment in Data Center Networks</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rRYYnoWtC6k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so I'm to let it introduced meg
well-read Sullivan who is a PhD
candidate at UCSD to join the advised by
min vedad and Keith marzullo and despite
the fact that she's not finished her PhD
and she actually already has quite a
history with Microsoft between her
master's and PhD she worked for a year
and the windows fundamentals group
working on app compat and she's also had
two postdocs with Doug Terry at MSR SVC
excuse me internships internships excuse
me very very short postdocs really
summer postdocs redox thank you in fact
we have at least one other person in the
audience who is also an intern has been
an intern of Doug Terry and for the next
two days she'll be interviewing with us
for a postdoc got to write that time
position with the district systems or
operating systems research group take
your way thanks Judy so today I'm going
to be talking to you about label
assignment in data centers and this is
joint work with my colleagues radicand
arange in mysore and malvika tiwari from
UCSD in jean from erickson research and
of course my advisors all right so what
I really want to tell you about today is
the problem of labeling in a distributed
network so anytime we have a group of
entities that want to communicate with
each other they're going to need a way
to refer to one another and we could
call this a name and address an ID a
label I'm going to take the least
overloaded of these terms and say a
label okay so to give you an idea of
what I mean by a label historically
we've seen labels all over the place
your phone number is your label within
the phone system your physical address
in terms of snail mail or for internet
type things my laptop has a mac address
in an IP address right now so those are
labels okay so the problem labeling in
the data center is actually a unique
problem because of some special
properties of data centers so when I
talk about a data center network what I
mean is an inter connective switches
connecting hosts together so that they
can cooperate on shared tasks and these
things as I'm sure you know are
absolutely massive we're talking tens of
thousands of switches connecting
hundreds of thousands of servers
potentially millions of virtual machines
and just to drive this point home I've
got the canonical yes data centers or
big picture here I'm sure you're aware
all right so another property that's
interesting about data structures is
that we tend to design them with this
nice regular symmetric structure so we
often see multi rooted trees in an
example of a multi rooted tree as a
factory which I've drawn on this slide
okay but the problem is even the
best-laid plans reality doesn't always
match the blueprint okay so as we grow
the data center we're going to be adding
and removing pieces we're going to have
links switches hosts all sorts of things
failing recovering we could have cables
that have been connected incorrectly
from the beginning or maybe somebody's
driving a cart down between racks and
knocks a bunch of stuff out and puts it
back in correctly basically we don't get
to take advantage of this nice regular
structure all the time or at least we
can't expect it to always be perfect so
just to give you some context what might
we label in a data center network so
ultimately we have end hosts trying to
cooperate with each other so host puts a
packet on the wire and it needs a way to
express where this packet is going right
needs a way to say what it's trying to
do so some things that we might label
for instance are switches and their
ports host Nick's virtual machines this
sort of thing just to give you some
concept of what we're trying to label
here so for now let's talk about the
options that we have today on one end of
the scale we have flat addresses in the
canonical example of this is mac
addresses assigned at layer 2 now these
things are beautiful in terms of
automation right they're assigned right
out of the box they're guaranteed to be
unique for the most part so we don't
have to do any work and assigning them
and that's fantastic on the other hand
we run into a bit of a scalability issue
with forwarding state switches have a
limited number of forwarding entries
that they can store in their forwarding
tables right so this means that they
have a limit in terms of the number of
labels that they can know for other
nodes in the network and we're talking
about flat addresses we run into this
problem where each switch is going to
need a forwarding entry for every node
in the network and at the scale of the
data center this is
more than we can fit in our switches
today now you could argue that we should
just buy bigger switches but remember
we're buying tens of thousands of them
so we're probably going to try to stick
to as cheap switches as we can so from
there we might consider a more
structured type address of address and
usually we see something like
hierarchical and the canonical example
of this is IP addresses assigned with
DHCP now here we solve the issue of the
scalability in the forwarding state this
is because what we do is we have groups
of hosts sharing IP address prefixes
okay and so a group of hosts can
actually take their prefix and have this
be corresponding to one forwarding entry
and a switch farther away in the network
ok so we allow sharing and this can
packs are forwarding tables on the other
hand if we're looking at something like
IP someone's got to sit around and
figure out how to make these prefixes
all work someone's got to partition the
address space and spread it across the
network appropriately and configure
subnet masks and DHCP servers make sure
all the DHCP servers are in sync with
themselves and sync with the switches
and this is really unrealistic to expect
anyone to do this is a significant pain
point at scale so more recently there
have been several efforts to combine the
benefits that we see at layer 2 or layer
3 and try to address these things and
I'm only going to talk about to today
these are the two that are most related
to the work that I'm going to tell you
about these two are Portland's location
discovery protocol which was done by my
colleagues at UCSD and then msrs DAC
data center address configuration now
one point I want to make about both of
these is that they both somewhat rely on
an ocean of manual configuration via
their leverage blueprints so there's
some notion of intent of what we wanted
the topology to look like but more
importantly both of these systems both
rely on centralized control now I could
make the usual comments about
centralized control there's a bottleneck
there's a single point of failure that
sort of thing but I think these things
have largely been addressed what I
really want to talk to you about is this
idea of how we get the centralized
controller connected to all 100,000
nodes okay this is a problem they can't
all be directly connected to the
centralized controller otherwise we have
a hundred thousand port switch and
that's pretty cool but we don't have
that okay so in order to get all these
opponents to be able to locate and
communicate with our centralized
controller we're going to need some sort
of separate out-of-band control network
or worse we're going to have to flood
okay so you could say well an
out-of-band control networks not so bad
it's going to be much smaller than our
current network but relatively smaller
than absolutely massive is still pretty
big right so someone's going to need to
deploy all the gear for this right and
it's going to need to be fault-tolerant
redundant and somebody's going to need
to maintain it so this is again
something that we we run into a problem
that we were trying to avoid before
right we don't want to have to
administer and maintain all this stuff
so let's try to tease out the trade-off
that we're really looking at here what
we have is some sort of trade off
between the size of the network that we
can handle and the management overhead
that we have associated with assigning
labels and this trade-off looks like
this as the network size grows it's more
management overhead this isn't meant
this is just a concept graph so it's not
meant to have any particular slope maybe
it's not even a line but the trend is up
into the right okay the interesting
thing about this concept graph is that
someplace along the lines of networking
size of network size we have some sort
of hardware limit okay and to the left
of this limit we're talking about
networks that are small enough that we
can afford a forwarding entry in the
forwarding tables per node in the
network and so to the left of this limit
we're free to use flat addresses like
mac addresses however to the right of
this limit this is where we run out of
space in the forwarding tables and we
can't fit an entry per node in the
network so we're going to have to
embrace some structure in our dresses
some sort of hierarchical label okay so
just to give you a frame of reference we
talked about Ethernet and ethernet sits
to the bottom left of this graph almost
no management overhead but small and
networks IP on the other hand is going
to sit towards the top right we can have
very large networks but we have to deal
with the management overhead so like ldp
and DAC our goal here is to try to move
down vertically from IP to get to this
target location where we get to support
really large networks with less
management overhead okay and the way I'm
going to show you how to do that today
is with some automation now of course we
all know there's no free lunch
there's a cost to everything right so
what I want to point out is if we're
going to embrace the idea of automation
then the network is going to do things
for us on its own time now we can set
policies for how it's going to do things
what it's going to do but ultimately
it's going to react to changes for us
okay additionally if we're going to do
something with structured labels that
means our labels are going to encode the
topology and that means when the
topology changes those labels are going
to need to change and since the network
is taking care of things automatically
it's going to react and change those
labels okay so this is this is the
concept that we're going to have to
embrace the concept of relabeling where
when the topology changes the network is
going to change the labels for us all
right so now that you have a rough idea
of what we're trying to do what I'm
going to present to you today is alias
it's a topology discovery and label
assignment protocol for hierarchical
networks and our approach with alias is
to assign hierarchical labels so we get
this benefit of the scalable forwarding
state that we see with structured
addresses to assign them in an automatic
way so we don't have to deal with
management overhead of 100,000 nodes and
to use a decentralized approach to do
this so we don't need some sort of
separate out-of-band control network now
the way that I like to do systems
research is a little bit different I
like to look at things from this kind of
implementation deployment measurement
side of things as well as from a more
formal side from a proof and formal
verification side okay and so today what
I'm going to be talking to you about is
actually two complementary pieces of
work one that was an implementation and
deployment type thing that was in
symposium on cloud computing last year
and one that's a more theoretical piece
of work from distributed computing last
year and these two things actually
combined together to form alias this
topology discovery and label assignment
protocol that I've just introduced to
you so just to formalize the space that
we're looking looking at right now what
we often see in data center networks is
multi rooted trees okay and what I mean
by a multi rooted tree is a multi-stage
switch fabric connecting hosts together
in an indirect hierarchy now in indirect
hierarchy is a hierarchy where we see
servers or hosts at the bottom at the
use of the tree of switches rather than
connected to arbitrary switches we also
often see Pierre links and by appear
link what I mean is a link that I've
drawn horizontally on this picture so a
link connecting switches at the same
level now one thing I want to call your
attention to about this graph and this
is just an example of a multi rooted
tree is that we have high path
multiplicity between servers so what
this means is if I take two servers and
look at them there are probably many
paths by which they can each reach each
other maybe some link or node disjoint
paths okay and our labels are ultimately
going to be used for communication so it
would be nice if at the very least our
labels didn't hide this nice path
multiplicity right if they allowed us to
be able to use that when we communicate
over top of them okay so I want to give
you a very brief overview of what alias
labels look like just to give you the
concept okay in alias switches and hosts
have labels and labels encode the
shortest physical path from the root of
the hierarchy down to a switch or a host
okay and so there might be multiple
paths from the root of the hierarchy
down to a switch and so that's which may
have multiple labels so to give you an
example that teal switch labeled G
actually has four paths from the root of
the hierarchy okay so it's got four
labels and similarly its neighboring
host H in Orange has four labels as well
so as you can see we've encoded not only
location information in these labels but
ways to reach the nodes now a few slides
ago I made a comment about having too
many labels and now I've just introduced
the concept of having multiple labels
per node right so it would seem I've
just made the problem worse instead of
better right but in a few slides what
I'm going to do is show you how we
leverage the hierarchy and the topology
to compact these things okay to make
some shared state so that we don't have
so many labels now almost any kind of
communication scheme would work over
alias labels obviously something that
leverages the path encoding in these
labels in the hierarchical structure
would be the most clever we implemented
something that actually does leverage
this but I won't have much time to talk
about that today so I just want to give
you some concept in terms of some
context in terms of what the forwarding
might look like so
you can think of for the purpose of
today's talk is some sort of
hierarchical forwarding where we
actually pass packets up to the root of
the network and then have the downward
path be spelled out by the destination
label okay so if someone wants to get a
packet to host H it just needs to get
that packet up to a B or C and then let
the downward path be spelled out by the
destination label of H all right so what
do these labels really look like and how
do we assign them what does this
protocol look like well alias works
based on immediate neighbors exchanging
state at some tunable frequency okay
what I mean by immediate neighbors is we
never we never gossip anything past
anyone directly connected to us okay and
we have four steps in the protocol that
operate continuously now when I say
continuously what I mean is they operate
as necessary if something changes then
state begins getting exchanged again if
someone has nothing new to say then the
state exchange just reduces to a
heartbeat okay and these four steps are
going to be the following first we
overlay hierarchy on the network fabric
now remember when a switch comes up it
has no idea what the topology looks like
has no idea where it is in the topology
what level it's at and so on so it needs
to figure this out next we're going to
group sets of related switches into what
we're going to call hyper nodes after
that we're going to assign coordinates
to switches now a coordinate is just a
value from some predetermined domain so
for the purpose of this talk we're going
to use the English alphabet as the
domain and letters as coordinates but
it's just any value from a domain okay
and lastly we're going to combine these
coordinates to form labels so I'm going
to tell you about each of these steps in
detail so that they make a little bit
more sense okay so first thing we have
to do is overlay hierarchy so in alias
what we say is that switches are at
level 1 through n where level 1 is at
the bottom of the tree and level n is at
the top and we bootstrap this process by
defining hosts to be at level 0 so way
our protocol works is when to switch
notices that it's directly connected to
a host it says hey I must be at level 1
okay and then during this periodic state
exchange with its neighbors its
neighbors say hey my neighbor labeled it
self as level one I must be level 2 and
so on for level 3 and this can work for
any size hierarchy right this could
continue up any size tree and the beauty
of this is that only one host needs to
be up and running for this process to
begin okay so now that we've overlaid
hierarchy on the fabric our next order
of business is to group sets of related
switches into what we're going to call
hyper nodes swish doesn't have any
himself if anything it's high linearity
yes yes yes so that's which will be
pretty much useless at the top of the
hierarchy so it will be have some paths
potentially that it can relay things but
it probably won't have been meant to be
at the root of the hierarchy so it
probably won't have a ton of connections
so it'll just sit at the top of the
hierarchy until its host connects it and
pulls it downward okay so what are these
high per node things and what are we
trying to do with them remember labels
and code paths from the root of the
hierarchy down to a host and so multiple
paths are going to lead to multiple
labels and we said we needed a way to
aggregate these two compact these right
we're going to do that with hyper nodes
so what we're going to do is we're going
to locate sets of switches that all
ultimately reach the same hosts on the
downward path so sets of switches that
are basically interchangeable with
respect to reach ability moving
downwards just because I'm going to use
this picture for several slides I just
want to point out we've got a four level
tree of switches here and we've got the
level 0 hosts at the bottom which are
invisible because of space constraints
so in this particular picture as you can
see we've got to level two switches that
are highlighted in orange and these to
level two switches both reach all three
level 1 switches and therefore they both
reach all of the hosts connected to
these level 1 switches and so with
respect to reach ability on the downward
path these two switches are the same
they're interchangeable so let's
formalize this a hyper node is the
maximal set of switches at the same
level that all connect to the same hyper
nodes below and when I say connect to a
hyper node below I mean via any switch
in that hyper node may be multiple
switches in that hyper
and of course this is a recursive
definition so we need a base case and
our base case is that every level once
which is in its own hyper node now I
think this is actually a tricky concept
so I'm going to go through the example
in some detail so we have three level 1
switches each in their own hyper node
then at level 2 we have to hyper nodes
as you can see the switch to the left in
the light blue or teal only connects to
two of the level 1 hyper notes whereas
the two switches on the right as we saw
in the previous slide connect to all
three and that's why we have the level 2
switches group this way if you look at
level 3 we've got the orange switch all
the way over to the right and that only
connects to the dark blue level too
hyper node so it's by itself on the
other hand we've got the two yellow
switches those both connect to both
level to hyper nodes and so they're
grouped together now notice that those
two yellow switches actually connect to
the dark blue hyper node via different
members but they both connect to the
same too hyper nodes ok so to reiterate
why we're doing this remember that hyper
node members are indistinguishable on
the downward path from the root their
sets of switches that are ultimately
able to get to the same sets of hosts ok
yes yes just for reach ability yep ok so
now that we've grouped into hyper nodes
our next task is to assign coordinates
to switches now remember a coordinate is
just a value picked from a domain for
this talk it's going to be letters
picked out of the English alphabet so
let's think about what these coordinates
might look like and how they're going to
enable communication remember we're
ultimately going to use them to form
labels and those labels are ultimately
going to be used to route downwards in
the tree so for example if we have a
packet at the root of this tree and it
needs to get to a host reachable by one
of the switches in the yellow hyper node
it may as well go through either right
they both reach the same set of things
we can forward through either for the
purpose of reach ability and we'll still
get to our destination on the other hand
if we have a packet at one of the yellow
switches and it's destined for the
bottom right of the tree then it needs
to go through the dark blue hyper note
it can't go through the one on the left
that teal one right so what this tells
us is
we don't need to distinguish between
hyper node members switches in a hyper
node can share a coordinate okay and
then ultimately share the labels made
out of this coordinate on the other hand
if we have two hyper nodes at the same
level that have a parent in common
they're going to need different
coordinates because their parent is
going to need to be able to distinguish
between them for sending packets
downward since they reach different sets
of hosts so this seems like a somewhat
complicated problem to assign these
things so the question is can we make it
any simpler and what we're going to do
is we're going to focus on a subset of
this graph and a subset of the problem
so we're going to look at the level 2
hyper nodes and think about how they
might assign themselves coordinates so
these level 2 hyper nodes are choosing
coordinates right so let's call them
choosers now they're choosing these
coordinates with the help of their
parent switches right because their
parents witches are going to say you
can't have this coordinate because my
other child has it so let's call their
parents deciders so at this point what
we've done is we've pulled out a little
bit of an abstraction that shows a set
of chooser nodes connected to a set of
decider nodes in some sort of bipartite
graph now this may not be a full
bipartite graph but it's a bipartite
graph so what we're going to do is we're
going to formally define this
abstraction this chooser decide our
business then we're going to write a
solution for this abstraction and then
finally we're going to map this solution
back to our multi rooted tree so to
formalize what we have is what we
officially call the label selection
problem or LSP and the label selection
problem is formally defined as sets of
chooser processes which I've shown in
green here connected the sets of decider
processes which I've shown in red in a
bipartite graph okay now with an eye
towards mapping back to our multi rooted
tree these choosers are going to
correspond to hyper nodes so remember
hyper nodes may have multiple switches
but for now we're just collapsing that
into one chooser node and then the
deciders are going to correspond to
these hyper nodes parents witches okay
now the goal a solution to the label
selection problem is that all choosers
eventually select coordinates that we
make some progress and also that
choosers that share a decider have
distinct coordinates now with an eye
back towards mapping back towards our
tree this is so that hyper nodes
ultimately have distinct coordinates
when they need to so an example of the
coordinates that we might have in this
particular graph are the following so
choose our c1 c2 and c3 share deciders
right so they all need different
coordinates and that's shown in this
example here on the other hand for
instance c 1 and c 6 don't share any
decider so they're free to have the same
coordinate if they want to now formally
we define a single instance of LSP as
the set of deciders that all share the
same set of choosers okay so this is
basically if you want to look at it this
way the set of maximal are full
bipartite graphs that are embedded in
this one graph so here we have actually
three instances of LSP in this graph now
note that a chooser can end up in
multiple instances of LSP for instance
c4 and c5 are both in two instances okay
and this begs the question what do we do
with choosers that are in multiple
instances now on one hand what we could
do is we could assign them one
coordinate that works across all
instances okay now this means they only
have to keep track of one coordinate
which is nice but on the other hand if
they have to have a coordinate that
works across all instances they may be
competing with a few choosers from each
instance and this may give them more
trouble in terms of finding coordinates
that don't conflict with someone else
god
coordinates for example use a 160-bit
quit or
so there could be if we could use if we
could use the mac address as a
coordinate that'd be nice however we're
going to end up tacking coordinates
together to form labels and we would
like to try to limit the size of the
labels okay well we might want to use
them we want to use them for forwarding
we might use them for rewriting in a
packet different things so we want to
try to keep them to a reasonable size so
on the other hand if we didn't assign a
coordinate that worked across multiple
instances what we could do is assign a
coordinate per instance okay this means
that c4 and c5 would each have two
coordinates now this gives us the
trouble of having to keep track of
multiple coordinates but on the other
hand we're keeping the sets of switches
that might conflict with each other
smaller okay and ultimately keeping the
coordinate domain smaller which will
give a smaller labels so it turns out
that either of these specifications are
just fine we've actually implemented
solutions with both we've actually
looked through both and they're fine but
there's a nice optimization as we map
back to alias if we do the second did
you have a question so if you introduce
a single link into the system
you break the uniqueness next slide so
could you do the next slide I will in
one second okay so we decided to go with
the one coordinate per instance just
because it gives us a nice property when
we map back to alias so just to show you
how that works what that means is c4 and
c5 reach going to need two coordinates
one for each instance that they're in
and note that there's no constraints
about whether we have the same
coordinate for both instances whether we
happen to pick different ones etc it's
just per instance and if they happen to
be the same no worries all right so
here's your slide so at first blush this
seems like a pretty simple problem to
solve in fact our first question was can
we do this with a state machine with
Paxos right the difficulty here is now
remember we're formally stating the
problem we're not actually solving it
here but one of the constraints of this
problem is that connections can change
and when a connection changes it's
actually going to change the instances
that we have going on and this doesn't
map nicely to paxos okay so for instance
no pun intended if I add this link
between chooser c3 and decide or d3 what
this does is it actually pulls chooser
c3 into the blue instance and then c3
needs to find a coordinate for that
instance ok so the difficulty here one
of our constraints of this problem is
that the instances can change and in
fact we expect them to change so I'd
like to reiterate that any solution that
implements the label selection problem
and it's invariance is a perfectly fine
solution ok and there are many ways we
could do this we designed a protocol
that we call the decider chooser
protocol just based on what we were
looking for in terms of mapping back to
alias ok so the decider chooser protocol
is a distributed algorithm that
implements LSP it's a las vegas-style
randomized algorithm in that it's only
probabilistically fast but when it
finishes its guaranteed to be correct so
this is in contrast with a Monte Carlo
algorithm where we finish quickly but
we're not guaranteed to be absolutely
perfect when we're finished now we
design the decider chooser protocol with
an eye towards practicality because
we're going back into the data center we
want something very practical something
that converges quickly doesn't use a lot
of message overhead we also want
something that reacts
quickly and locally to topology dynamics
ok so this issue of connection is
changing we want to make sure that we
react quickly to them and we want to
make sure that a link added or changed
over here in the network is not going to
affect labels over here right we
definitely don't want that so to give
you an idea of how the decider chooser
protocol works our algorithm is as
follows we have choose or select
coordinates opportunistically from their
domain and send them to all neighboring
deciders so let's suppose chooser c1 and
c2 select X and Y respectively they're
going to send these to all their
neighboring deciders now when a decider
receives a request for a coordinate if
it hasn't already promised that
coordinate to someone else it's a sure
you can have that ok and in this case
neither of the deciders have promised
anything to anyone so they both send
yeses back to these choosers and then of
course they store what they've promised
now if a decider has already promised a
coordinate to another chooser then it
says no you can't have that coordinate
for now I've promised it to someone else
and here's a list of hints of things
that you might want to avoid on your
next choice ok if a chooser gets one no
from any decider it just selects again
from its coordinate domain and tries
again now it does this avoiding things
that it's that have been mentioned to it
in the hints from its other deciders ok
once a chooser gets all yeses it's
finished it knows what its coordinate is
so in this case both choosers got yeses
and they're finished now of course I've
just shown you the very simplest case
here there are all sorts of interesting
interleaving zand race conditions and we
have coordinates are taken up while
they're in flight or while they're
promised by a decider that ultimately
won't work out this sort of thing there
are all sorts of complicated issues here
I've just shown you the simple case so
you can see what the protocol looks like
all right let's talk about mapping this
back into our multi rooted tree so what
we have is at all levels in parallel we
have as many as needed instances of the
decider choose your protocol running on
a small portion of the tree ok so we
have several instances at each level on
smaller chunks of the tree happening all
in parallel so each switch is really
going to function as two different
things it's going to participate in a
chooser for a typer node and it's also
going to be helping nodes below it
switch their coordinates and it's going
to be functioning as a decide
okay so to look at this in more detail
at level 1 in our example tree we have
three hyper nodes so we have three
choosers and their parents witches the
three deciders help them choose their
coordinates now things get a little bit
trickier as we move up the tree okay so
remember that all the switches in a
hyper node are going to share their
coordinate right so they all need to
cooperate to figure out what that cord
it can be the reason for this is that
each switch in the coordinate may have a
different set of parents and the parents
are going to impose restrictions about
what the coordinate can be based on
their other children and what
coordinates already taken so we need
every single switch in the hyper node to
participate in deciding what the
coordinate can be or rather what it
cannot be okay so we need input from
every switch in the hyper node now our
difficulty here is that the switch is in
the hyper node are not necessarily
directly connected to one another right
unless we expect some sort of full mesh
of pure links at every level we really
can't expect them to be connected to
each other so what we do is we leverage
the definition of a hyper node to fix
this so remember a hyper node is the set
of switches that all ultimately reach
the same set of hosts okay and so if
they reach the same set of host that
means that there is one level one switch
that everyone in the hyper node reaches
okay and so we select one such level one
switch for each hyper node via some
deterministic function so in this case
I've gone with the deterministic
function of I drew it farther to the
left on the graph okay so what we do is
we use that shared level one switch as a
relay between hyper node members and
this allows them to communicate and
share the restrictions on their
coordinate from their parents okay and
because this is more theoretical work
and we've changed the protocol a little
bit we need to give it a new name so it
is surprised the distributed chooser
decider chooser protocol because we've
distributed the chooser across the level
one switch and I / note okay so coming
back to our overview we've overlaid
hierarchy we've grouped into hyper nodes
and we've assigned coordinates that are
shared among hyper node members our next
task is to combine these coordinates
into labels this is actually quite
simple the way that we do this we simply
concatenate cord
it's from the root downward to make
these labels so for instance this maroon
switch here has three paths through
different hyper nodes and so it has
three labels now to make this kind of
more clear if we didn't have hyper nodes
it would have six labels okay because
there would be six paths so what does
this give us really what we get is that
hyper nodes create share clusters of
hosts that share prefixes in terms of
their labels so that when we have a
switch here in the network it can
actually refer to a whole group of hosts
over here by just one prefix okay so
this is compacting our forwarding tables
in the same way that you might expect
with something like IP except that we
didn't pay for the manual configuration
here we did this automatically now I'd
like to bring your attention briefly
back to relabeling remember that was our
are no free lunch thing so one we have
topology encoded in the labels when we
have pads encoded in the labels if the
pads change we're going to have labels
changing we call this relabeling so an
example of this is if I fail this link
here shown in the dotted red it's
actually going to split that hyper node
into to hyper nodes okay and this is
going to affect the labels nearby
because remember labels are built based
on on hyper nodes okay so in our
evaluation we show that not only does
this converge quickly but also that the
effects are local just to the nodes
right around this failure here now I
know I said I wouldn't talk about
communication much but I just want oh
gosh an empirically improve that there's
actually found on
we we showed it with a model checker so
we verified that it happens locally we
also did some analysis to convince
ourselves so the punchline you will they
can get our gold labels but without
manual assignment yes so let me propose
a straw man I'll turn the system and
maybe you can compare or what you
subscribe to the best wrong one so I'm
going to run DHCP and I take an
open-source TV server sometimes and do a
little hacking on it and configure it
essentially higher on the plane so that
each dhcp server doesn't just give out
addresses but it also claims portions of
the address space with PHP server but no
hierarchy so top level switches but they
have you know entire class a but some
giant stuff that and they give out
smaller and small big small subnets for
these concerns below and so so other
personally until finally the lowest
layer you have to HP servers without
being individual addresses to host
rather than dividing up sudden it's more
kindly I think you'd end up with
something similar he did up with still
hierarchical labels completely automated
you could also have the advantage of
also being a real IP address can use
that existing router works and you'd
also have custom pointing addresses as
opposed to variable addresses that's
your scheme
so what are the I don't think in a spot
a little way no problem what would be
the difference between what I just
described away from this is something
I'd actually like to think about further
I have given some thought to what we
could do if we distributed some sort of
controller or something among nodes and
this kind of looks like that sort of
thing my first concern would be getting
all the DHCP servers in sync with each
other and agreeing with each other and
who tells them which portion of the
address space they can have or if they
decide amongst themselves how do they
sync up with each other this is
definitely something that that we've
been we've been looking at as current
and future work there are a lot of
different different ways that we can
kind of distribute something centralized
but logically distributed across certain
portions of the graph ok ok so you
should an example a local change when a
limb fail but is it the case that the
introductions or the other failures and
links can change the level in the tree
which seems like IV of a switch it seems
like it'd be very disruptive is that not
is that still local it's a good question
um so first of all in terms of just in
terms of failing a link vs recovering a
link it turns out there's actually not
much difference when you fail or recover
a link what really matters in terms of
the locality of the reaction is what
happens to the hyper node on top of that
link whether that hyper node ends up
joining with an existing one splitting
apart that sort of thing now of course
if you change certain links you can
change the level right if you if you
break your last link to a host then
you're going to move up in the tree from
level 1 right all right then that's
going to pull that route switch down
from the tree it does it does not
globally usually um unless you do
something if it's soap
depends where the failure actually isn't
it depends on how many nodes are below
the hyper node so with your example if
you hang a host at the root of the tree
this is actually going to pull this
route down right and this could have
some some bad effects in terms of
generating peelings where there used to
be up down paths right so one of the
things that alias gives you that I think
is really nice is this notion of
topology discovery so we have several
types of flags and alerts that you can
set so if you don't expect if you didn't
build a network with many peer links and
you start to see many peer links
something's wrong this isn't what you
intended and we send an alert and so
lots of these things that are going to
cause disruptions about like this are
going to be able to be found immediately
and detected would in fact have dramatic
effects yet you to that assertion
failure rather than those it's not that
all changes cause local perturbations
it's that changes either cause local
perturbations or their indication that
you did something horribly wrong it is
an additional things that and also some
of them that don't cause only local
perturbations are not going to matter so
if you pull if you pull a root switch
down then you're definitely losing paths
but because of this nice path
multiplicity you're not losing
connectivity okay because there's still
several routes which is it'll provide
that connectivity okay sure okay so just
to touch on the communication that we
would run over these labels I know I
promised I wouldn't talk about it but I
want to give you some context for how we
might use them as it I'm just think
about where's my book
network it before with a reed switch on
there if I just go and plug my laptop
into the really switch because I'm the
buggy the network right now community
well do that happens to tell that is
well obviously it's like the worst case
right but it but it's something that
could clearly happen I mean it great but
of course we can set up our protocol
right so that your laptop doesn't act
like a server they live somebody pushes
a cart yeah knocks out cables back in
the wrong place well that's that's an
infant but then you said something
interesting which is that it makes paths
go away it seems like a strange property
that plugging a laptop into an empty
corner routes which will eliminate may I
haven't removed and legs my same network
except that I used up one more poor well
so it doesn't make physical pads go away
but based on the structure of the
communication that we're going to talk
about and that we used it will make some
logical pads go away but we pause it
that there are still plenty of pads not
actually it's a weird property to have
that you're that you're losing your
losing the ability to route across
certain links because you because you
added a host that's not doing anything
right reading diagnostics so ultimately
ultimately we are encoding topology and
the labels and we are restricting how we
were out across those labels and so we
need some sort of well-defined way of
structuring these labels and and this is
a cost that comes with that yeah the
fact that very early we're talking to
use lists to identify the trees yep yep
now if you are willing to admit some
sort of scheme where you labeled nodes
as particular levels then we could make
this go away right and and that's not
that unreasonable of a thing to request
right we could say switches within Iraq
or right right
so we want to draw for something
automatic but of course you could make
this go away if you if you were willing
to label okay so just so look at our
communication scheme what we do is
actually something very similar if
you're familiar with the Portland work
to what they did for communication so as
a host sends a packet at the ingress
switch at the bottom of the network we
actually intercept that packet we
perform a proxy art mechanism that we've
that we've implemented to resolve
destination MAC address to alias label
okay to one of the alias labels for the
destination then we actually rewrite the
mac address in the packet with the alias
label now as i'm sure you're thinking
right now this means that we have a
limited size for the labels in this
particular communication scheme um there
are also other schemes ok so then what
we do is we forward the packet upwards
and then across and then downwards in
the network if we choose to use any
across peer links now this is based on
the upstart down star forwarding that
was introduced by autonet ok and then
when the packet reaches the egress
switch at the end of its path the egress
which goes ahead and rewrites the alias
label back for the swaps it back out for
the destination MAC address so then host
don't know that anything happened now if
you weren't really willing to rewrite
MAC addresses and packets or if you
didn't want to have fixed length labels
then you could use something
encapsulation tunneling some other way
to get these packets to the network this
is just to give you an idea of how you
might implement this ok so now I'd like
to tell you about how we evaluated this
so again we kind of approached it from
two sides from the implementation
deployment evaluation side as well as
the prove it and verify its side but
ultimately our goal really was to verify
that this thing is correct that it's
feasible and then it's scalable that's
really what we care about so I'm going
to talk about correctness first because
if it isn't correct then it really
doesn't matter if it's scalable right so
our questions that we wanted to answer
as far as correctness for first of all
is a leah is doing what we said it does
and does it enable communication ok so
to figure this out we implemented alias
in mace which is a language for
distributed systems development ok so
you basically specify things like a
state machine so if I receive this
message I send the following messages
out if
time or fires I do this now mace is a
fantastic language for doing this kind
of development because it comes with a
model checker if anyone isn't familiar
with Mason wants to be please come talk
to me because I would love to tell you
about it okay and what we did with the
mace model checker is we verified first
of all that the protocol was doing what
I said it would that I didn't write
tremendously buggy code okay then we
verified that the overlying
communication works on alias labels that
two nodes that are physically connected
are in fact logically connected and can
communicate this is actually a really
great use of the model checker because
it turns out that there are some strange
graphs that we found with the model
checker where communication didn't work
and it was based on an assumption that
we had made incorrectly early on in the
protocol when we were designing it so
again model checker is great because you
find these sorts of things the last
thing that we verified was the locality
of failure reactions so making sure that
this invariant of if I fail a link here
no one over here changes making sure
that holds then because mace is a
simulated environment and we never trust
these things we ported it to our testbed
at UCSD now we use the testbed that was
set up for Portland so it already had
this up star down star forwarding setup
and so all we had to do was make sure
our labels worked with an existing
communication scheme okay and they did
work and this gave us a way to sanity
check our our may simulations the other
things we wanted to look at in terms of
correctness were first of all does the
decider choose your protocol really
implement LSP remember I said anything
that solves it is fine so did we come up
with something that actually solves it
so we verified this in two ways first we
wrote a proof and then second what we
did was we implemented all of the
different flavors of the decider chooser
protocol in mace so having one
coordinate / / multiple sets of
instances having multiple coordinates
distributing the chooser etc and we made
sure that the invariance that LSP has
were held so progress everybody
eventually gets a coordinate and
distinction this notion of if you share
a parent you can't have the same
coordinate okay so we made sure that
these things held the next thing we
wanted to check was did we really pull
out a reasonable abstraction with LSP or
did we just look at something random
right was this was this a good place to
start
so what we did was a formal protocol
derivation from the very basic decider
chooser protocol all the way to alias
what I mean by this is we started with
the very very basic decider chooser
protocol ok it's not that many lines of
code and then we wrote some invariants
about what has to be true about this
system then we did a series of small
mechanical transformations to the code
we're at each step we prove that the
invariant still held ok and ultimately
we did the right series of mechanical
transformations such that we ended up
with the full distributed version of
this operating in parallel at every
level ok and so this convinced us that
this is actually a reasonable
abstraction to have pulled out so this
is an analysis process so we took it for
instance to go from the just the non
distributed chooser to the distributed
chooser we we formally said where we
would host each bit of code we we
defined cues that would actually later
be based on message passing etc so it's
actually like a grep and replace where
we made strategic choices about what we
would but we would replace and made sure
we didn't break any of our own variants
ok so the next thing we wanted to check
was feasibility because if we
implemented something that's never going
to run on real switches than it really
again doesn't make sense to to use it
doesn't matter if it's scalable so first
we looked at overhead in terms of
storage and control now storage I don't
mean forwarding tables I just mean the
actual memory on the switch to run the
protocol we just wanted to make sure we
weren't going to overwhelm these
switches ok and we looked at our memory
requirements and they were quite
reasonable for large networks we looked
at this both analytically and on our
testbed the next thing we looked at was
control over head and with control over
head we have this trade-off between
overhead and agility so this is based on
both the size of the switches and the
frequency with which we exchange state
with our neighbors ok so if we have
nodes exchanging state very frequently
then of course when something changes
we're going to converge more quickly on
the other hand we're going to pay the
cost and having more things exchanged
more frequently more control over head
so what I gave with here with some
representative topologies ok so
different sizes and what the control
over head might look like for these now
the reason I say three plus in terms
depth and the 65 k plus and so on in
terms of Hosts is because this number
doesn't depend on the depth of the tree
so this would work for any any size tree
with these this size switches okay and
this column here that I've got this is
worst-case births exchange this is the
absolute worst case that we could ever
expect to see this is if we have one
level 1 switch acting as the the shared
relay for every single hyper note in the
topology and managing everything and if
we have every single switch come up at
once then this is what it's going to
have to send once for this topology so
this is this is a pretty pretty
worst-case behavior okay and then I have
a few different cycle times listed and
just showing you what what the control
overhead actually would be corresponding
to these cycles and we actually thinks
this is quite reasonable given it's a
worst-case and it's in terms in terms of
how much of a 10g link it would take up
ok the next thing we looked at was
convergence time is this protocol really
practical ok now what I want you to do
for a second is suspend disbelief about
the decider chooser protocol let's just
say everybody magically pics coordinates
that work and that there are no
conflicts ok and let's look at what the
base case convergence fraley asst would
be in this case so on one hand to
measure convergence time we just
measured it on our topology right we've
perturbed something we start it up and
then we looked at the clock ok we found
that our convergence time was what we
expected from our analytical results
which I've shown here now in this case
our analytical results only depend on
the depth of the tree not the size of
the switches ok and essentially our
convergence time is going to be based on
two trips up and down the tree ok this
is one to get the levels set up and one
to get all the hyper nodes and
coordinates set up now of course we can
probably expect it to be less than this
because there's going to be some
interleaving of these lot of these
cycles that happens but just to give you
an idea here are some example based
cases for some topologies and I've shown
you some cycle times that are
corresponding to nice control overhead
properties from the previous slide ok so
now you can unsuspend your disbelief
about the collisions and the decider
choose our protocol and let's talk about
how bad that is ok
so remember I said the decider chooser
protocol was probabilistic ly fast what
does that mean is that really reasonable
well the way that we decided to look at
this we looked at it analytically it
turns out that there's a very
complicated relationship about the
coordinate domain the thickness of the
graph of the bipartite graph and so on
to do this analytically so we moved away
from that since we have numbers for that
but they don't really convince it
convinces of anything and we move to an
implementation using the mace simulator
now the mace simulator is a lot like the
model checker but it actually runs
executions in different orders and it
picks a little bit differently about how
it represents non determinism okay and
the beauty of the simulators it actually
allows you to log things so you can say
X cycles have passed and this property
is turned to true okay so what we did
was we built three types of graphs the
first was based on a fat tree so we took
two levels out of a fat tree to
represent the decider chooser problem
okay the second was a random bipartite
graph and the third was a complete
bipartite graph so as you can imagine
the complete bipartite graph is the
absolute worst case scenario right
because we have many more choosers
competing with each other than we would
expect and are competing across many
deciders right so each chooser is going
to collide with every other chooser and
is going to be told so by every other
decider the next thing that we varied
was the coordinate domains so we tried
things where the coordinate domain was
exactly the minimum number that it could
possibly be for that graph we tried
things where it was a little bit bigger
than it could be by 1.5 times the number
or two or two times the number of
choosers okay so these are still pretty
reasonable coordinate domains because
remember in practice we expect instances
of the decider chooser protocol to be
these small chunks of the graph okay and
what we found was that for reasonable
cases this thing converges quite quickly
two or three cycles on average and this
is including the cycles that I mentioned
in our base case for just establishing
connectivity between the two levels now
the worst case was the complete
bipartite graph with the smallest
coordinate domain possible okay and the
reason for this was the simulator
decided to generate some interesting
behavior for me much to my dismay so at
one point we had one shoe
for which every single response from
every single decider was lost 89 times
so it took eighty nine cycles to
converge because this chooser couldn't
get any information about what it's
Gordon it could be but when it finally
heard back from all the deciders and
heard you can't have any number but X
took X and moved on so of course this is
probably not a realistic scenario but
this is this is where we see the
stragglers when when the may stimulator
decided to give us some some sort of
crazy behavior James the failure rates
for components in your base in our life
what you think implications are for
particle I would love to know um
unfortunately I don't have a data center
to look at but i'm told that actually
link failure is a pretty big problem
it's pretty common links flap they come
back they fail i'm told that it's a
pretty significant problem so that's why
we designed this protocol both with one
of our big constraints in LSP being that
we need to be able to deal with the
instance is changing and one of our big
constraints overall saying that we need
to react quickly and locally so we
definitely think it's a real problem you
know not something that's going to be
intermittent ok now the scalability
assuming that people would be laying out
their networks in
shree like apologies complexion was fair
to good question so alias will work over
any typology of course it's going to be
ridiculous if you if you try to overlay
a tree on a ring right so it really does
work best over something that can be
hierarchical now we do see multi rooted
trees pretty often in the data center I
know that's not always the case
especially in this room but we do tend
to see them we are told by network
operators that that's often the
structure so we designed a protocol that
would work for them so if you have a
data center that structured in a
different way probably alias isn't the
right choice all right so scalability
does this thing really scale to these
giant giant data centers that I
mentioned okay well our first blush at
making sure it was scalable was just a
model check large topologies the reason
for this is because sometimes as i'm
sure you all know when we scale out a
protocol weird things happen that we
didn't expect so we just use the model
checker to make sure nothing changed at
scale and nothing went weird okay then
because the model checker only scales to
so many nodes we wrote some simulation
code to analyze network behavior for
absolutely enormous networks networks
that are more realistic in scale okay
and so what the simulation code did was
it laid out random topologies then it
set up the hyper nodes figured out what
the coordinates could be assigned
forwarding state and then looked at the
forwarding state in each router okay the
model checker only up to a couple
hundred nodes maybe 200 not not enough
to convince us at the single scalable
the the simulations that we did up to
tens of thousands sometimes hundreds of
thousands but I don't display the
numbers for hundreds of thousands
because I didn't run enough trials to be
sure because they took hours and hours
and hours to run okay so here's the
results of what we simulated so let me
explain this chart on the Left we have
the number of levels in the tree and the
number of ports per switch okay and so
what we did is we first started with a
perfect fat tree of this size then in
this percent fully provisioned column
what I did was I failed a portion of
this Factory now you may wonder why I
failed the fat tree the reason for this
is in a perfect fat tree the hyper nodes
are going to be perfect right everything
is going
be aggregated really nicely what we
wanted to test was as we get away from
this nice perfect apology and as we
start admitting failures are the hyper
nodes still really doing their job are
they aggregating are we really still
going to get this compact for named
state that we wanted okay now the next
column shows the number of servers that
each topology supports and what this
gives us is a very worst case comparison
to using layer 2 addresses so this is
not a fair comparison but it just gives
us a base case the very worst case
scenario so if we had layer 2 addresses
we need this many entries in our
forwarding table and of course the last
column gives us the number of forwarding
entries on average in each switch Italia
in alias okay so as you can see orders
of magnitude lower than the worst case
at layer 2 now of course we could expect
to see similar numbers for something
like I take IP if we cleverly portion
the space and everything but of course
we'd have to pay the manual overhead in
this case the manual configuration cost
and in here we didn't have to ok so what
we see as we get forwarding state that's
like IP without paying the costs
associated with IP ok so to conclude
understands here is yes we did um we did
it both in mace and we did it using our
testbed that we had already set up for
Portland so Portland assigns addresses
that look a lot like this but in a
centralized way so we actually already
had that forward or set up unfortunately
the performance is pretty hard to
measure because we did it with open flow
and updating flow tables is pretty slow
so we were limited in our evaluation by
the speed of updating flow tables which
ended up being a limiting factor oh
right I guess I want it ready back for
me too especially asked for a really
long way since you introduced a lot of
complexity around finding
plotting addresses or labels i guess i
was probably more convinced that you can
just use big rounded labels if you'd
actually show that there was something
that broke but it sounds like if you're
if you actually built a motor what would
happen with the quad rash the call over
to use you know 25 labels or something
great you just use random Renaissance
sure another idea would just be to carry
MAC addresses down the tree and maybe
have someone in the hyper node be the
leader whose mac address got to
represent that hyper node so this is
this is definitely something that we
could consider a lot less complexity the
problem is that we do run into these
long addresses and what this does is it
makes our entries in our tea cams for
forwarding longer right makes them take
out more space and I think well if we're
scaling out to the size of the data
center we may have a lot you know we may
be able to fit we may not so so it's
already a little bit of a known issue
right that we're running out of tea camp
space and deacons are expensive right
especially in commodity switches now if
we're talking about using bigger
switches and this is a non-issue and I
would definitely go with something like
what you're saying but we're trying to
buy these cheap commodity switches right
and we can't expect them to have a ton
of space for forwarding state and so if
we have this kind of prefix matching
business going on and we have really
long labels we're actually going to be
taking up multiple lines in the tcam
with each entry okay so if we can afford
the forwarding state then I'm all for it
but I think that for the purpose of this
work that we're looking at smaller
forwarding state then we can afford that
okay I'm sorry
routing algorithm this right right but
then we do have things like open flow
that give us the ability to modify the
routing software on an on switch it over
before just thinking it actually they
have every single packets now on the
slope out how so release every flow is
always filled out every flow isn't this
little yes and there have been there has
been work to address this and to make
things like open flow faster but if
we're going to what we can upgrade the
firmware we can use something like open
flow so so we feel it's okay to modify
the switch software probably not the
hardware so you have a graph showing
that Jeremy's scheme only if you use
this number of servers and yours and
cheese I don't have a sense of how many
more times machine I don't that's
something that I really should do for
future work and that's definitely
something that we should talk about
later if we again okay so to conclude
hopefully I've convinced you that the
scale and complexity of data centers is
what makes them interesting and that the
problem of labeling in them is
interesting because of these properties
I've shown you the alias protocol today
which does topology discovery and label
assignment and it does it using a
decentralized approach so we avoid this
out of band control network it leverages
this nice hierarchy in the topology even
though we have switches and links
failing we still have this nice
hierarchy so it leverages this to form
topologically significant labels and
then of course it eliminates the manual
configuration that we would have with
something like IP still giving a
scalable forwarding state okay so I'd be
happy to take more questions very
interesting
how can this work I mean can you
incorporate work like low balance that's
a very good question because what we're
doing is we're really restricting with a
label to a set of paths right and it's
difficult to do something like global
load balancing on top of this this is
one of the topics that we're actively
looking at right now is how do we get
this to play nicely with load balancing
so along those lines one thing that
we're looking at so I think it's
important to understand that there's
actually two levels of kind of multipath
with an alias the first is picking a
label if you have multiple labels than
each label corresponds to a set of paths
ok and then within a label we still have
this ability for multi path and so this
relates to load balancing right because
we have a set of pads that we can
possibly load balance between and so
this is something that we're actively
looking at can we select a best label
can we take a best label select it and
then map other labels to alias no pun
intended to this label so that we can
use all paths for this label this is
definitely something that we're looking
at questions that when you need to be
offered load balancing into the
including basic in recent memory
on the switches meaning I mean basically
you need to record multiple entries on
to a destination is there you can decide
which entry to choose
so I'm not sure I see what the question
isn't that the question is um basically
in this framework to destinations
potentially you have you may need have
basically there's multiple positive
right in your tea can basically entries
do you need to record multiple
concentric swords or it's a match i
think it basically just one line without
load-balancing it's just want mine
because what we need to know is for
local things we need to know the address
that goes through this particular switch
to get to them and then for far away
things we just need to know their top
level hyper node and know how we can
route up in the tree to get to something
that reaches at top level hyper node so
of course with load balancing then the
story could change
me when you can you had nice pictures
but a few things and three or four
labels but when you've got a hundred
thousand machines how many labels can
point
not too many not too many so um for
example you can see here that we end up
with each switch actually only needing
to know about are you talking about how
many labels end up for switcher goes ok
so this depends how how broken the tree
is ok so as you start to break up hyper
nodes as you start to fail more of the
tree then you're breaking into more
labels on the other hand as you break
the tree further you're actually
breaking pads and then the number of
labels go down because there are fewer
ways to reach things so the base case
for this is if we have a fat tree you're
going to have one label / host we did
lots of different topologies not all of
them are realistic because the model
checker decides to fail what the model
checker decides to fail but we didn't
see too many labels / hosts in our
topologies on the model checker which
was smaller a couple hundred nodes we
saw four or five labels for host in the
worst case and these were really
fragmented parts of the tree i mean the
model checker will generate something
that looks like a number of just strings
coming down that have no cross connects
wonder how you imagined integrating this
into the operating system
it recently every place where direct
like IP addresses for exposed like this
is the guy because in your packet you
need some different so is your face now
that's passively labels up to that my
bachelor correctly so the end you mean
the operating system on the endo
straight the end host doesn't know
anything happens the end host because we
have an in our communication strategy
anyway because we have this nice kind of
proxy arp thing set up all we need is a
unique ID for node and we resolve that
to an alias label forward through the
network and then rewrite back into what
that unique ID was so you can imagine a
scheme where we did this on IP addresses
although then you have this overhead
with assigning them right but so of my
example was mac addresses you could do
this with anything in fact i think the
interesting thing here is that you could
do some sort of any cast type thing if
you were willing to modify the end host
right so you could send a packet to
printer and then your ingress which
could go ahead and resolve printer to a
number of alias labels may be
corresponding to different nodes and
then pick one of those and send towards
it based on whatever constraints
so where's the translation where's the
transition the translation from Ivo mac
address at the bottom level switch where
we're connected to an end toast that
switch how does it keeps all of the
limits no we did we did a proxy art
mechanism so what we do is because we
have to pass things up in the tree
anyway in terms of setting up hyper
nodes and so on we actually pass
mappings between alias labels and
whatever our you IDs are going to be in
this case MAC address is up the tree and
so the roots of the tree all know the
mappings and so what we do is we have a
level one switch ask the roots of the
tree so we have it send something up
words like an ARP request it actually
intercepts real or / quests from the
host sends these upwards and get some
answers back down so there are a number
of ways we could we could write this
right it's just some sort of replacement
for our infosys label as my 8pcs address
I'm sorry good good man you fellows are
making sense could you turn the label
for any post translated in some way
direct names were my favorite
like of these accessories
I would think so this isn't something
i've considered but i think that as long
as it fit in the space that we were
trying to use and as long as it didn't
have any sort of conflicts right if this
were visible externally then this would
be a problem I don't see why not that
being said I reserve the right to to see
a reason not to in the future</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>