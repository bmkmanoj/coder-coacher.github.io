<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Algorithms Among Us - Panel &quot;Human-level AI – if, how, and when?&quot; | Coder Coacher - Coaching Coders</title><meta content="Symposium: Algorithms Among Us - Panel &quot;Human-level AI – if, how, and when?&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Symposium: Algorithms Among Us - Panel &quot;Human-level AI – if, how, and when?&quot;</b></h2><h5 class="post__date">2016-06-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PokeSrTknaM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay welcome back everybody so this
session is about human level artificial
intelligence and we have a very
distinguished panel to discuss to
discuss these issues so going from right
to left we have Gary Marcus Tom Dietrich
yan lacuna Andrew Inge and Shane leg and
my name is Maura Shanahan and I'm going
to be moderating the panel now we've
solicited questions because it's such a
huge audience we as you heard earlier on
with solicited questions online but I
gather some people having difficulty
submitting them online so they can also
if they want to come to the front and
submit questions to Victoria Krakov now
who is right here if they would like to
submit questions during the during the
panel okay so okay so I've put a slide
up here which suggests some of the
topics that week I think we could we
could discuss today and and the first
one that I wanted to do to tackle is the
very notion of human level artificial
intelligence and do we think that this
concept of human level artificial
intelligence and obviously here we're
looking somewhat further ahead in time
than then we were looking in the
previous panel in the previous session
so we're being a lot more speculative
about where artificial intelligence
might go in the more distant future
decades in maybe a few decades so is
this concept of human level artificial
intelligence useful one do we want to
try and make a more nuanced version of
it maybe do we instead maybe you want to
think about this whole question of
generality is that really what we're
interested in here is it how we build a
more general kind of intelligence and
many people these days use the term
artificial general intelligence so is
that perhaps a more useful concept so
I'm hoping the panel aren't going to
undermine the notion of human level or
artificial intelligence altogether
because in that case we have
a lot to discuss but i'll start off by
by asking whether they think this is
useful idea so maybe you start with gary
and they need it badly because i'm sick
okay great so just on the question of
whether a ji itself as a meaningful
concept i would say obviously not
obviously it's a crude approximation but
that doesn't mean we need to go home so
i think it obviously should remind
everybody of the famous saying about
pornography I know it when I see it it's
probably going to be hard to define they
probably aren't sharp boundaries for it
it probably entails a lot of things
they're probably a lot of domains in
which we are already exceeded human
level AI right so the planning that we
do in a chess computer already exceeds
human level III but I think there are a
lot of things we haven't made much
progress on yet so a good example of
that would be comprehending open text we
just don't have machines that are
anywhere near as good as say a 12 year
old at reading an arbitrary story and
being able to say who did what to whom
and so I think even if we can't strictly
define a GI it's clear that there are
challenges in front of us and they're
going to revolutionize the world when we
get there and so it's certainly worth
talking about okay just who wants to
pick up on that yeah sure so first of
all I don't need to remind this audience
that we are still very far from anything
that could be qualified as human level
AI but perhaps for people who watch the
video later who are not as good into the
techniques that were using about this
we're very far away from it it's only
mildly useful as gary said there are
certain areas you can you can go down to
the toy store or supermarket down the
streets and buy a gadget for thirty
bucks that will beat you a chess in some
respect that machine is super you know
as human level intelligence or better
and and we're going to see more and more
of machines of this type that are expert
in particular areas we have lots of them
including in things like image
recognition nowadays which wouldnt think
were were attainable in a short time
just five years ago so we're going to
see more and more of those things coming
up and the question is
in the end where we have a collection of
all those all of those things that
together cover all of what we call human
intelligence I think that's kind of
qualitatively different a little bit
than then solving individual problems so
do you think that the essence of what's
qualitatively different there is is
captured by this notion of generality so
is it and is there something you know
sort of deep an important that is
difficult to capture that before I
learned anything about deep mind I wrote
a piece for The New Yorker called can
super mario bday I and it was about a
kind of satirical project where somebody
built I think it was an old Nintendo
thing where they had access to the
entire data set all the pixels in the
image but all the memory registers and
were able to beat a whole bunch of
different games just by sheer brute
force and my piece was kind of satirical
as well as this piece that I was writing
about but the idea is yeah you need to
do generality deep mind is taking a step
towards that one could argue about
whether it's a step far enough but
generality is certainly part of what's
been missing from AI so as the the
person who proposed the term out of
field general intelligence I guess I
should try to do defend it a little bit
so I think it is a useful notion I think
it's useful to distinguish between
systems which can specialize and all
sorts of different things such as these
chess-playing algorithms versus systems
which are not really designed for
anything in particular but they have a
range of capacities which allow them to
spend an extended space of of problems
so I think that was a that's useful
terminology to sort of help clarify the
importance of that and I think this is
if you if you read some of my thesis and
so on this is a fairly common
perspective on the notion of what
intelligences that the emphasis on
generality in the fact that humans can
you know learn to play chess and go and
poker and space invaders and speak
Japanese and and and and do algebra and
so on and so on this huge range of
things that people can do and how this
is very very different to what we
typically see in
in artificial systems in terms of the
importance of human level artificial
intelligence I think it's you know it's
it's it's true that you know obviously
computers will be far superior already
are far superior to humans and many
things and in vastly inferior but I
think it's also an interesting a natural
point of reference when computers reach
a level of capability where over all the
sorts of mental tasks that people can do
they are by and large as good if not
better because this is that you know
because this is a level of intelligence
and generality that's sort of very very
natural for us to think in terms of so I
don't know you know in the in a
theoretical sense I don't know if
there's anything particularly special
about that but I think it is a very
interesting point for us to think about
in terms of the evolution of you know
and the development of systems and their
capability and so on so for that reason
I think it's it's it's a useful one to
two to have in mind so I guess I sort of
see two different reasons or maybe two
different things we might be talking
about when we talk about artificial
general intelligence on the one hand you
know one of the big motivations for the
entire field of artificial intelligence
was to understand what is intelligence
and and and as people are sort of our
best example of it until recently
perhaps understanding just basic science
how is it that people managed to you
know flexibly move across scales of time
and space and their reasoning how do
they rapidly learn the n plus first task
after having learned and previous tasks
how do they allocate their time and
their resources very well so all these
notions of you know bounded rationality
and so on and and our our understanding
of that is you know we've been making
progress over time but
it's been slow i would say and but so
that's one reason the other reason is
you know would it be useful economically
you know for business or for medicine or
whatever for something society cared
about their i don't know the story is
quite so obvious right because right now
we found very high value in these narrow
systems that that that and and those
systems it seems our easier to build and
easier to understand easier to field and
and more or less easier to maintain and
it's not it i would say we don't have
any really convincing demonstration that
a general-purpose system would have a
better value proposition so i think we
should let Shane come back on that one
yeah so I I disagree with that and I
think that we're seeing evidence of the
importance of increasing levels of
generality and learning sweeping through
many areas of artificial intelligence
machine letting the moment so the way in
which handcrafted speech recognition
systems have been superseded by learning
systems now the way in which handcrafted
image recognition systems have been
superseded by learning systems and I
would say the way in which handcrafted
continuous control algorithms are
starting to be challenged by learning
systems and I expect that they will go
beyond as well I don't think you will
ever get anything is sophisticated human
level motor control over such a vast
range of different situations unless you
have algorithms that are really learning
rather than trying to build all sorts of
very clever things into them just what
Andrew you you know I want to make an
analogy so first those you that know me
I love a I'm machine there anything is
having a vast impact helping a lot of
people i'm super excited for AI but i
think they're working on artificial
general intelligence today work on the
AGI today is a little bit like working
on colonizing Alpha Centauri right it's
a great mission I really hope that
humanity will get there someday to
colonize other star systems Alpha
Centauri is four light-years away with a
lot of problems to solve I picture that
there's probably
meeting somewhere of the Society for
colonizing Alpha Centauri I'm picturing
your mainly a bunch of like bearded guys
and maybe your men and women debating if
you want to comments Alpha Centauri
you've gotta solve hydroponics or some
DNA thing or whatever just like I think
in AI where people that get him say on
South Asia you've got to have caused the
reason you've got to have any logical
reason you've got to have this innocent
distances you can make of all stories
like that all this can make a great
research I definitely support these
things but I think it is so far away we
should work on it and when we work on it
I'm sure we'll get lucky and some of the
things we do will be useful in have
short-term impact i will give you one
data point I live in Silicon Valley I
have for a long time I live and breathe
and eat makes a lot of people in Silicon
Valley and in Beijing that are the ones
on the front lines of AI building code
shipping product you know really making
a difference to hundreds of millions of
people's lives today almost all the
people I know that are doing that
important work transforming people's
lives today almost none of us use the
word API in our vocabulary we just don't
think about it in the same way that I
have the greatest admiration for the
people building space rockerz China
sense allies the law of opens or
whatever I suspect that most of them do
not spend their day to day talking about
how to colonize Alpha Centauri okay
that's great so so I think we've moved
very nicely into the question of when if
and when this you know this is gonna
this is going to happen so I'm sure that
some members the panel I've got
something just disable that maybe Gary
and then well maybe everybody I don't
you show not sure probably i'll do but
i'll just start with something that
andrew just said which you think very
much bears on when we get there which is
what strategy we take so i think there's
a lot of low-hanging commercial fruit
that makes sense for people to pursue
right now but doesn't necessarily take
this closer to AGI and so to the extent
that we think getting to AGI faster is a
good idea which we can itself argue over
we might want to think about different
strategies so most of the research is
being driven by short-term commercial
interest and I think that leads to
solutions that are good statistical
approximations of things where there are
underlying causal models for example and
not necessarily to us capturing
example the causal models and so if you
can get like eighty percent of a problem
done very quickly and faster than your
competitor that might make more sense
than building the a GI system that might
take a lot longer to pay its rewards I
agree it will take longer and it's
usually easier in a particular domain to
you know craft something more for that
domain so i don't i don't disagree with
that but if there are fewer efforts that
are looking at building really really
general intelligence well maybe we have
less competition well no I don't want to
ensure the timing question but what are
the obstacles to more intelligent
machines whether we call it a GI your
human level intelligence I think it's
pretty clear to me it wasn't clear to me
only five years ago what was the path to
to definitely making machines more
intelligent but we're starting to you
know it was kind of like driving in a
fog we didn't know where the obstacles
were and we didn't know what we what we
brick wall we're going to encounter I
think now we see a mountain somewhere on
the horizon that we need to to get
through and that and that mountain
there's two mountains really or two
peaks that we have to go through one is
the sort of melding of representation
learning or deep learning as is practice
today with reasoning and memory in fact
there is an entire workshop on this
topic saturday by a bunch of people from
facebook not surprisingly and there is
another area which i think is probably
even more important and it's
unsupervised running so most of the
learning that animals and humans do is
unsupervised running and it's a you know
rather than say supervised running or
reinforcement running so i have this
this joke that i say when i'm surrounded
by friends from d pine which is if
intelligence is a cake the cake itself
is unsupervised running the icing on the
cake is supervised running and the
cherry on the cake is reinforcement
running and you know defined being so
interested in reinforcement learning you
know that kind of starts an interesting
discussion
but I think really what so here is the
problem we know how to do supervisor on
in quite well and the demonstrations are
the last few years of deep running and
commercial nets and things like this are
really really impressive more impressive
than I thought they would be like if you
had asked me five years ago do you think
you should use a commercial net to the
face recognition I would have told you
know you know they're good for like
generic categories but four phases no
you know we use a more specific method
for this they actually work really well
and there's a lot of you know other
domains that we've seen like this where
what those things work amazingly well
much better than we thought so we know
how to do supervised running we have a
good handle on some areas of
reinforcement running of course there's
a lot of interesting work going on there
that we need to do the problem is that
we have no idea how to do unsupervised
running properly there are situations
where in supervised learning works like
for example learning language models
predicting the next word in a you know
in a text it works okay okay we can
learn representation of the text like
this it's relatively simple but if I ask
you to build a system that will watch a
movie look at the video and predict what
the next frame in the movie is going to
be or what the frame you know a number
of different choices for what a frame is
second from now it's going to be or a
minute from now an hour from now we have
no technique that can do this properly
and and it means that there is an
essential underlying principle that we
haven't figured out and this is the
principle that machines and animals do
to learn so why is it an supervising
it's so critical it is that it's an
argument that Jeff in turn has been
making for many years decades it's the
fact that if you have a learning system
with you know a few billion parameters a
few billion synapses let's say if it's a
brain or tubulin parameters if it's a
official neural net the only way you can
you can have enough information you can
give enough information to the system to
constrain that many weights is by asking
you to discover the structure of the
world you're not going to get enough
information out of a piece of
reinforcement once in a while or you
know a bunch of labels by by showing
pictures right so the the problem we're
facing now is that we don't know how to
exploit how to have machines discovered
the structure of the world by just
observing it and that's the problem we
have to solve that's a big mountain we
have to solve the smaller mountain is
the reason you
memory etc I created everything except
the size so I agree with everything
there except i think maybe i'd like to
correct a misperception that we mostly
do reinforcement learning a deep mine
that's not true at all we're most famous
for doing reinforcement learning and
atari playing and all that sort of thing
because that's what you see in the media
a lot but if you actually look through
our publications we we do a lot of
generative models and unsupervised
learning and it's actually one of the
biggest topics that we do and you'll see
a lot more papers in the coming year
including things like predicting what's
coming next in video frames and so on
that yarn was talking about I think I I
think we we won't run run the risk right
now in our field that because we've had
this huge success in deep learning over
the last five years that we see the
whole world from the perspective of that
but I think we have to remember that if
we look in the AI literature a lot of
the people in literature are living
where we are in the 11 / squared n world
right whereas we as our datasets get
bigger and bigger we rapidly improve our
performance there's a whole bunch of
other people who live in the B to the D
world the branching factor raised to the
depth of the search tree for them even a
you know an order of magnitude speed up
is just one level shallower and or one
level deeper in the search tree and so
there's a the problems that they face
their I think are quite different from
the ones that we face in our 1 over root
n domain you know regime and and and
they've been making progress too but it
I think we need to be humble about how
far 1 over root n is going to carry us
and we are going to need a lot of
additional ideas beyond what we have
right now so yeah and you want to come
back on that as well and yeah I got a
direct answer to this so the interesting
thing that's happened over the last few
months or a year at least is the fact
that one of those problems which is
classically been a you know are to the D
D to the ND to the B whatever the tree
exploration
is now are now being sold with
supervised machine learning so a good
example is the game of Go both people i
define and the Facebook have been
working on this and there are systems
now there are basically congressional
Nets trained to look at the go board and
predict where the next best move is
going to be and you combine this with
search techniques and you get pretty
good with the good players and so the
things going to reduce the amount of
search you have to do by essentially you
know using a supervisor running or
mutation running in that case okay so
actually that brings us to the question
of what are the possibly actually before
what get onto that i want i'd like to
ask you y am wet i'm coming back on what
you say in response to what Andrew said
whether using Andrews analogy that
you're perhaps a bit less skeptical than
Andrew about the prospect of human level
AI you see CST to use his metaphor that
we're more you know in the position that
they were beginning of the Apollo
program that it may be is an engineering
task that it's maybe a decade's worth or
something or a couple of decades but you
do actually see it as it's not Alpha
Centauri right it's the moon it's not
our first ontario's not the moon is you
know I think I think we could build
considerably more intelligent machines
without having to break any major a lot
of physics that we know about or as good
on colonizing Alpha Centauri that might
actually require a major revision of
what we know about physics you know
unless you let people you know we're
producing space and travel for 40,000
years but or robots so so I yeah I'm
less pessimistic than then Andrew on
this one I think I think there are clear
obstacles that we might that might take
a few years to clear or might take a few
decades to clear and frankly I don't
know and there's probably other
obstacles behind that we haven't
realized this is a it's a very long
history in AI of you know people having
a set of techniques and thinking now is
you know I just have to drive on this
highway okay there is fog but i'm quite
sure there is no wall and then you you
hit a wall after five years and there is
kind of a a I winter again so you don't
know what what's the next obstacle we're
going to to encounter once we clear
unsupervised running reasoning memory
etc but but I I'm
no I I think I wouldn't be as as
pessimistic as how long it's going to
take 44 really sort of your making
significant progress I don't want to
talk about human level intelligence
because I don't know what that means
really but but make significant progress
at least you know typically I'm very
optimistic about short-term progress
even prior to the next five years or
something but there's a big gap between
that I hope will colonize Mars or
something but there is a big gap between
progress in the next several years I'm
very optimistic about versus the more
distant future which i think is harder
to be certain about I'm curious about
something how many of you in this room
are students your grad students are
undergrads okay call all of you I want
to share of you something about strategy
and this relates to to to how we might
make progress or not make progress
towards a I when I was a student myself
I had not yet learned the skill of
seeing a little bit further into the
future and so I could see maybe one year
ahead or maybe half of your head and
when people talked about seeing beyond
that you know frankly I thought it was
 because I just didn't see it
but as I've gotten older one of the
things I've struggled to learn is how to
see further into the future so as to
better take actions today that allow us
to have the best impact on the world not
just six months from now but maybe one
or two or three or maybe five years from
now so um there was a book written by
Philip tetlock called super forecasting
other studies has really influenced me
that they've really studied how far can
humans see into the future and what he
showed was that looking at the broad
swath for the population as well as you
know CIA analyst with access the latest
top-secret data of Allah no one not even
the best you know intelligence analyst
could see more than five years in the
future this study was done for
geopolitical events Oh questions like
you know what's the chance that the
number of refugees in Syria with more
than X million in six months and then
question questions like that what's the
chance that this country that country no
one could see more than basically more
than about five years in the future I
don't know if this conclusion applies to
technology when I look back in history I
feel like none of us could see more
10 years a hidden technology I mean none
of us could forecast the rise of deep
learning 10 years ago none of us had the
iphone or the cell phone the smartphone
is let in 10 years old none of us saw
that coming you know and I don't know if
any of us can see more than five years
or ten years but I think there is a big
difference in skill between people they
can only see one year hit vs people they
can see three or four maybe five years
ahead because you can do a much better
job relate to your actions today to how
to have an impact on maybe the three to
five maybe ten but that's a real real
real / time scale um so my opinion is
that there's a lot of progress in AI in
the one to three-year time square where
I think I can see quite clearly and I
should really on I think unsupervised
learning is one of the next great ways i
think the recent rise of machine
learning has most even supervisor
earning deep learning supervised
learning i think that as we run out of
labeled data for a lot of todd's getting
so hard to get more label data might
think that on supervised learning could
be named expect wave having said that I
think that there is a big gap between
those of us they try really hard to peer
several years ahead versus those that
think they can see 20 years ahead and
frankly people that think then C times
did I call I don't think any of
us can see 20 years ahead in this face
of Technology um and you know as John
said we've had multiple AI winters we
tend to be like that I was a student in
carnegie mellon many years ago and i
still remember was it the 80s or early
90s when Alvin right CMU's self-driving
car trained using neural networks is
driving a few rows on Pittsburgh and
there were the breathless media reports
about how your neural networks for
self-driving causes in the early 90s
late 80s and early 90s how self-driving
cars based on work of CMU when I take
over the world and none of them none of
us at that time could see far enough
ahead I think we are actually reaching a
point to take that as an example where
we can see a clear path from where we
are today too realistic self-driving
cars and and I don't like to talk about
things and hype about things where we
can you know kind of see
opinion about what the future might look
like I do a lot of work on self-driving
cars if I do right and and the reason we
talk about it is because I think we see
a path to actually make it happen and
and and I I think it's more responsible
as scientists to speak carefully and I
do support working toward colonizing
Alpha Centauri just be clear right I
think that's a great thing to work on I
don't think everyone should work on it
and I think you're very careful of how
we of the claims we make when when we're
working on that problem yeah so it's
particular answers on self-driving car
when I saw the Alvin paper it all myself
these guys should use a commercial net
not not a fully connected network that's
obvious and you know and then we could
build chips in fact you know I was at
bell labs at the time and and we were
building ships for conventional nets and
it was pretty clear that if you wanted
to build start driving cars you know
with the progress of Technology you will
have to have a system of this types that
we put you know behind a single camera
or maybe two cameras and and that that
was the way the way to go so this was
the early 90s I thought you know if the
lads I continued on this path perhaps we
would have had this kind of technology
much earlier but it didn't happen this
way first of all neural net fell out of
favor second of all Bell Labs kind of
needlework itself into two and so this
kind of stuff wasn't possible at AT&amp;amp;T
anymore and we're working on other
things but it was pretty clear to me
that you know the surviving cars of the
future is we're going to be using
complex and in fact they do if you buy
test by today it has a little vision
system that keeps you in lane and it
actually is accomplished on that that
looks at the other road you know I had a
paper about 12 years ago that exactly
used that and demonstrated a little
robot driving around so you could see
this you know it took longer than I
thought it would take all right the
self-driving cars are actually
interesting in terms of like the gap
between being eighty percent of the way
there being a hundred percent of the way
there so it's very interesting in
particular because if it works
ninety-seven percent of the time then
the driver is going to fall asleep and
maybe not be there for the other three
percent of the time so I think the Tesla
stuff if i understand it
hands pretty heavily on the lane
markings if they disappear then the the
driving system doesn't work very well
I've heard other systems have trouble
and fog and I assume where you and I
live in New York City with a lot of
pedestrians there's going to be some
issues and so forth it's a question
about even there though I don't think
it's at all the poster child for AGI but
even there there's a question about to
get that last couple percent to make it
truly reliable and safe do you need like
richer real world models or is the
convolutional that stuff going to be
enough I don't think anybody knows the
answer there actually I you know since
summer solid red stuff in class let me
describe to you how I think we will get
there and I think this is a realistic
plan you know so far a lot of debate has
been about whether you try to do
everything at the same time let's build
a car that does everything on the all
worlds also concerns aura that's really
hot versus the car makers is you can
increment to approach where you make the
cause more and more autonomous I think
there's a different approach that we
should consider which is to build a self
fully autonomous fully autonomous car
that does everything but only on one
route right so if you want to build a
bus will have a bus just drive in the
same circle over and over and over again
we have long around your work and I
think that if you do that then you can
work with local government local police
and make sure that there's no
construction there that surprises you
and you can fit your models just you
know obstacle detection on this one road
that's all you need to do and make that
save and you can do that I think that
the relatively near future not driving
everywhere in the city but just on one
route in the city we can put calls in
the road in a relatively short term
small very small numbers of years after
you built this one row driving around
around the circle's you add a second row
round around the circle's at the third
row until slowly you can build it up to
take over or have you car able to take
over more and more roads until hopefully
you can then evolve toward being able to
drive everywhere I think that so far
there everyone's been assuming that the
dimension I think most progress is
incremental a lot of progress is
incremental
right a lot of progress seems like you
came out of nowhere but to those of us
on the inside working it was actually
more incremental than looks like from
the outside a lot of car makers think
that the dimension of incrementation is
to build a car that drives everywhere
and to make a car that drives everywhere
more and more autonomous there's another
dimension that very few groups have
considered that I think is actually more
promising strategy this is what we're
doing it by do which is to drive a car
that is fully autonomous but the
dimension of incrementation is to drive
on one more and more ribs over time so I
actually find that promising approach
isn't Google doing that in Palo Alto oh
I think that maybe yeah I i shall honest
don't know I google I yeah I honest you
don't honestly on there I don't know
what Google is doing this there's
someone else you wanna say luckily Shane
is here too I don't work on self-driving
car so I'm not going to try and comment
on that with respect to having a GI as a
goal it's you know it's I don't want to
claim that we know when we'll do it you
know maybe there are five breakthroughs
maybe there are 50 breakthroughs nobody
nobody really knows until we get there
you know it's and it's going to take a
long time it's going to take decades or
something nope nobody really knows so
it's not about knowing when exactly
various things or will happen and be
able to forecast that very accurately
but i think it's it's it's useful in the
sense that it allows you to conceive of
a problem and start thinking about all
the different aspects of that problem it
gives you a framework for thinking about
things and the types of our solutions
and the different part different things
you need to solve on the road to get to
to where you want to get to whereas if
you are trying to optimize something
much more locally you might find that
you can get better better solutions and
and so on and beat the current state of
the art but it may also be clear that in
the long run this isn't isn't what you
need to do to get to for example AGI so
i think it's useful in terms of creating
a framework for you to think about your
priorities and the problems that you
need to work on rather than a claim
about being able to predict exactly when
we'll get there or when little soul
any of these particular problems and now
I would like to sort of speaking of this
sort of notion of getting to AGI or
human level AI I just don't think it's
an event so we see sometimes in the
press things like achieving human level
AI will be the greatest event in human
history okay I don't think it's an event
it's a very slow evolution and it was
kind of just what Garrett was saying
also it's we will add capabilities and
and I think there's their good argument
from Stuart Russell that that the the
hardware matters in the sense that human
you know constraints and biology lead to
certain sets of resource effectiveness
trade-offs and a lot of our meta
reasoning is precisely about out
resource allocation to achieve to
optimize that trade-off and evolution
has worked on that too when we build
artificial intelligence on various
different kinds of harder they will each
of those different hardware
infrastructures will probably lead to
different trade-offs and so we probably
won't see you know the systems that have
exactly the same strengths and
weaknesses as humans they'll have
different strengths and weaknesses so
there won't be a moment an instant an
event it's it's more we'll just see
gradually improving technology and and
just like Moore's Law is the sum of 12
or 13 major advances along the way and
technical tricks the same is true for
for the kinds of things that will lead
us eventually to to something that we
would say was a very general reusable
redeploy about kind of AI which is the
time technology we'd like to see I agree
with you I don't think there'll be a
single moment but when people write the
history maybe 50 or 100 or 200 years
from now I think one thing that they'll
pay attention to will be when computers
were able to read open and read or
listen to open edit domain general text
I think that may itself be something
that takes five years or something like
that but I think that will be one of the
turning points I don't think many people
think that it's going to be some sort of
sudden of being too well defined point
in time but nevertheless I think it's
there is something there is something
fundamentally something fundamental is
really
happened when the when machines are
smart enough to rival us over most
cognitive capacities and you know you
can sit down and have a really
sophisticated philosophical debate about
something with a machine and it's I
guess it's it oh that's that's way that
would happen after the moment I was
pointing out actually I'd like to just
come back and ask Andrew something so
you you know you talked about your
responsible science and so on and you
grant that we can't see into the future
right so surely that means to say that
we can't be very confident that you know
that we won't achieve human-level
artificial intelligence perhaps in 20
years right you say we can't see into 20
years ahead certainly can't see 50 years
so so doesn't that mean that to say that
scientific responsibility should mean
that we should at least think about the
possibility even if we don't know the
possibility that we might create human
AI and what that might mean you know I
think those those someone I once said
doesn't close to the bunch in the media
when I think this is one you know
there's been a lot of hype about a I
recently about evil AI evil super
intelligence evil killer robots I think
dennis when you guys start to God
acquired by google you a condition on
that was a set of a committee to ensure
ethical AI and I think the peak of the
hype was when Elon Musk said that AI is
more dangerous than the nuclear weapon
something like that right I hope that
was a peak anyway who knows um and one
thing I I said in code a lot is that I
think you know worrying about AI evil
killer robots is like worrying about
overpopulation on the planet Mars right
I I hope that you know we will get to a
point someday we call eyes the planet
Mars and it will be overpopulated at
some point and it will be polluted
because you know and then they'll be all
these children on Mars sadly dying of
pollution we need to worry about it then
but I just don't know how to work on
that problem now productively because we
have even set foot on the planet yet so
I don't think there's necessary a bad
thing to you know form a committee as I
guess domest it to ensure ethical a i i
i think is great if people want to form
committees to study it is at great
length i personally am choosing not to
spend my time on those committees but i
have no objection really i think is a
fine thing you know as a society we need
to work on different things right it
really many different people need to
work on many different things and it's
good if i work on something yon works on
something tom works on something else
everyone's email chain works or
something else I think the best word I
can do for Humanity right now is
building and shipping deep learning
algorithms working on self-driving cars
work on speech recognition we're on
computer vision that's what excites me
more but I'm actually glad that there
are other parts of society you know
studying ethical AI I think it is I
think it's a great thing to do and so
with regard to the committee I think
it's a little bit overblown you know we
have a number of people who get together
and occasionally and talk about
questions about a I safety and
responsibility and so on I think that's
a good discussion to have from time to
time of course the media likes to blow
that into some sort of horror story that
we're all freaking out and and very very
worried and so we must be meeting to
talk about these topics no I think it's
just responsible discussion to have from
from time to time in terms of a I safety
and all that I think it's both over
blowin and under emphasized in some ways
so the a lot of the the media discussion
gives you the impression that you know
these super powerful AIS are turning out
any day now and the huge threats and we
should be very afraid of it and all this
sort of thing I don't think that's the
case at all it's you know it's going to
we're going to be working on a I for a
long time and there's lots and lots of
problems we have to solve and there are
a lot of uncertainties around and so on
and so on that's it many people believe
in there are surveys that show this
people who are working in AI and machine
learning that advanced artificial
intelligence I went user to human level
but at least fairly advanced artificial
and
Hodgins will be is a realistic
possibility within you know the
lifetimes of some of the people here
today and if that is the case then I
think you know it many people would also
agree that if this were to happen the
impact on our society and all sorts of
ways they could be negative they could
be they could be very positive would be
profound and so if we are approaching
even if it's you know decades and
decades away if we are approaching a
transition of this magnitude I think
it's only responsible that we start to
consider to whatever extent we can in
advance the technical aspects of this
societal aspects of those legal aspects
of this and whatever else because you
know it's some being being prepared
ahead of time as was better than trying
to be prepared after after you you
really need some good answers so Gary
and then yeah I agree with what Jane
said and I like the formulation of the
argument that you made Murray the one
thing that I would add is in terms of
being prepared we don't just need to
prepare for AGI we just need to prepare
for better AI that's going to have lots
of implications so there's this kind of
notion of a magical moment which I think
none of us on the stage actually believe
in and in the public eye it's all about
what happens at that magical moment but
you can think just internet of things
and a whole lot more deep learning on
the ground doing a whole lot more things
and already I think issues of safety and
security and risk already become
important so I think it's important to
study these issues regardless of you
know whether the progress is linear or
exponential or whatever towards this
mythical AGI thing there there is
progress it's substantial it is going to
change things and I think it makes sense
to study them yeah so I think some of
the fears are
of you know human level AI are related
to the fact that people think artificial
intelligent machines will have similar
intelligence to human so that relates to
one of the questions that's on board
right now you know will AI look like
like human intelligence and I I think
not not at all we we like to think of
ourselves we like to think of our mind
as being sort of general intelligent
machines but our brains are very very
very far from being general we're driven
by you know basic instincts that are
built they were built into us by
evolution for survival or our brains are
very limited in there no type of
connection and signals they can process
and the kind of functions they can
compute efficiently and we you know we
were very slow at adding numbers
together which lower than computers are
so there are certain things that you
know it's very difficult for us to
imagine a different type of intelligence
that human intelligence because that's
the only example we have in front of us
but in fact machines will have very very
different types of intelligence so in
particular they will not have all the
drives that make humans do bad things to
each other you know we we generally do
bad things to each other when we feel
threatened and we want access to
resources we want to you know not starve
and you know etc you know preservation
instinct pays it basically makes us do
bad things to each other there's no
reason to build this into machines and
so there's no reason for machines to
have curiosity unless we build that
explicitly into them and so you know so
so a lot of dangers that we imagine of
machines you know like in movies that
you see you know coming out of Hollywood
where you know machine all of a sudden
becomes intelligent and all of a sudden
figures out that you know there's a
world out there and becomes curious to
see it there's no reason for machines to
be curious unless we build that the
property into them I just like to point
out ex machina didn't come up Hollywood
but out of England
you know so I should agree very that the
rise of AI and younger the rise of AI
poses challenges i think is less of the
rest of AI turning unethical kind of a
scientist I think maybe I think that the
biggest challenge is the challenge to
employment so I see everything printemps
in here Eric and Andrew McAfee have been
really thought leaders yet but they're
dare work has been since my thinking a
lot we typically haven't of the impact
of intelligence of itai an IT more
broadly on employment I think that's
actually the biggest challenge we will
face in the next several years and I
sometimes worry about the hype about you
know AI evil intelligences being a
distraction from the much more serious
challenge we face which is that of a
dependent the potential for massive
unemployment or massive downward
pressure on wages and I would much
rather have serious leaders in
government and corporations in academia
have a serious conversation about that
challenge which I think will affect
hundreds of millions of people even more
so than having done worry about you know
AI turning evil and that's how all true
that's all true even without a GI so I
mean presuming that my kind of gambit
about hei and automatic cars is wrong
automatic cars won't require at least
much in the way of AGI maybe they
require a little bit but they're already
going to significantly change the labor
landscape and we're going to see a lot
of that possibly long before AGI happens
Tom do you want to say you've been quiet
for a little bit out the variable okay
well maybe if so if this is a little
pause maybe we could move on to some of
the questions or do talent see some of
the questions that that we've received
from from from the audience by email so
thank you very much for the questions
that have been sent in on a little
online form so so quite a few of them
have been sort of covered already so I'm
just picking out a couple that
definitely happened and the actually the
first one that I would like to pick out
relates to another of the questions on
the board which I'd like to maybe
rephrase a little bit which is let's put
aside the whole business about AGI and
human level AI now let's suppose that
you wanted to kind of go for the
moonshot as it were towards and make a
you know make a kind of big leap towards
the next level of sophistication of a i
right then what would be the kinds of
technologies that you would go for what
would be what would you see as the major
research themes to go for and and so
maybe you could all kind of address that
one but in there's a particular question
here which is that so one person asked
is reinforcement learning an acceptable
model for general AI and if not why not
and I see rich Sutton in the audience
right at the front here so maybe maybe
he wants to chip in there I don't know
we could give you the Mike actually my
bitch
well of course it is yeah I want to say
one thing further about that which is
that the unsupervised learning is on
there's actually a terminal
terminological issue about whether
unsupervised learning and reinforcement
learning it I mean all these things all
these things are are these terms they
evolved over over years as people do
different things and people people who
were doing one thing to another thing
and they included in what the name of
what they're doing these are all these
things of all over top over time in
particular unsupervised learning is
really all over time and probably will
evolve quite a bit further and I'm
betting that in the long run which zoo
on supervisors owner will do a lot of
things in prediction and will be using a
lot of the same prediction learning
methods that we use in reinforcement
learning so I think there's a lot of
room to grow there this one thank you
I've always wanted to jump off at the
stage into an audience and now I tell
you okay fulfilled a kind of lifetime
dream there and so maybe I hand over to
the 22 Andrew you look dying to say
something so do you yan so so you know
in terms of what areas of AI where areas
of research to invest in I think um you
know deep learning is really rising now
one thing that many of you might not
know is that in the early days of deep
learning lay around 2007 oh and it was
just getting is the start of his v
surgeons I guess a lot of us are around
in those early days actually had a
strong focus on unsupervised learning
and the reason is there's an argument
raised by geoff hinton the number of
bits of information from unlabeled data
is huge and and maybe in comparison to
reinforcement learning it reinforcement
learning if all you get is a sporadic
reward signal you know in real life
easter was a so infrequent and there's
so few bits of information it seems
difficult to build the entire system
based only on reinforcement learning
to get really technical I think there is
a difference between a cost
differentiable cost function which can
differentiate and learn from having a
cost function is different than having a
small ratting point why there was such a
lot more information in the cost
function so I think there is a
difference there but back in maybe 2007
a lot of a lot of you know my students
DC is back then and some of the yawns
were some of your shoes were some of
Hinton's were was was on sun supervised
learning and then i think what happened
was as we scaled up our computers you
know supervisor and he became so
valuable we created so much value for so
many people that almost all of us
shifted you know almost wholesale into
supervised learning and i think we
probably maybe shift a little bit too
far i well actually love to ever i do
we're investing other groups are also
doing more work on supervised learning i
think the first wave has been in my
opinion the first wave of the rise of
deep learning was because of scale
bigger computers and more data and more
data is primarily labeled data and
rather than going from you know image to
label we can now have complex inputs to
complex outputs like we were I think we
were the first to do image to sentence
infinite image I'll put a caption a lot
of people did at about the same time you
know there's a sequence of sequence
learning which came out of the team my
OT my google + and so on your facebook
is doing right the input a question open
an answer a lot of input input
supervised learning and we're now seeing
richer and richer forms of that and I
think the combination of supervised
learning together with lots of data and
giant machines to scale is in my opinion
what's really driven a lot of the
progress in machine learning and deep
learning over the last few years I am I
think there is a point where we're
starting to run out of labeled data in
speech recognition you know we train our
algorithms on like a decade of audio
right there's about a decade is about
nine thousand ninety thousand hours and
for a lot of problems just running on
the label data but our ability to access
unlabeled data is still nearly unlimited
so I'm very excited by worth their kind
of relate to the what rich said one
thing I think as people as humans we
have these buttons in the head they're
supervised learning otherwise learning
and this reinforcement learning or maybe
maybe a few other packets
and we tend to take new things and and
kind of do k-means clustering or
something and assign them to one of
these buckets I think that a lot of
algorithms that you know we could
categorize into these buckets but I'm
really the answer lies learning word
embeddings or slow feature analysis a
lot of algorithms that doesn't fit
neatly into one of these buckets and i
think is law of those ideas better use
of unlabeled danger especially that that
if I were first year PhD student today
looking you know to do work on the
five-year time scale not the two not the
one year time scale but maybe like a
five year time scale I think doing work
on unsupervised learning is something
that I would seriously consider but this
was the understanding that unsupervised
learning is this they bucket is not one
clean idea it's just as very convenient
k-means clustering to give an opus over
the simplified description of a big set
of ideas so yeah I have a little
defensive reinforcement learning and the
question that was raised about whether
it's appropriate so I think we're
thinking about reinforcement learning
it's useful to distinguish reinforcement
learning as sort of a framework versus
reinforcement learning as as algorithms
for it there for adapting behavior as an
algorithm for deputy behavior yes you're
getting some sort of sparse reward
signal and so on and that you can only
drive so much learning with with these
Philly and frequent scalar values as a
framework I i think i think it's it's
it's very important because what you
want what you wanted me situations is
you want agents that act in the world
and solve some sort of task or some sort
of problem and that's what reinforcement
learning is about as a framework and
within an agent within that framework
part of the agents algorithms maybe
reinforcement learning other parts may
involve unsupervised learning and
representation learning in various
different ways and and all sorts of
other things so that the the algorithm
may only be one part of an agent that
employs many learning paradigms but as a
framework which i had i think this
question here is about i think it's very
important because generally we want age
in many cases that go out and achieve
goals in the world they had some sort of
gold function or something like this and
that's what reinforcement learning as a
framework is about these agents that go
out and achieve some tasks actually can
I just make my one little comment of my
own and then over over TN so your
comment about the sparsity of the
reinforcement of the reward signaling
reinforcement learning of course in a
social context you know society is rich
with reinforcements elves and there when
a human infant infant is rare rare than
it's a constantly getting reinforcements
in reward signal so so I'm not security
in a realistic context that it's quite
so it's fast anyway young well it's
still you know one scalar value once in
a while and there is no way this can be
enough to train the 10 to the 14
synapses we have you know brain so you
know that relates to something I said
earlier that which builds on a you know
something Jeff in turn is set for a long
time that to have enough information to
constrain a very large neural net to
learn the structure by the world you you
need to essentially use on supervised
running whatever however you define it
which means perhaps predicting the
future representing the world so you can
do a good job a prediction to some
extent the essence of intelligence is
the ability to predict the future and
taking into account your own actions
into it and so there are models you know
they've been proposed in the in the past
based on own prediction if you can
predict what the world is going to look
like when you move your head 10
centimeter to the left you probably have
a good idea that the world is
three-dimensional and you weren't born
with it you learn that you weren't born
with the fact that when an object moves
you know it's it's not in the place that
it was previously the objects cannot be
two places at the same time your own
spontaneously that when an object is
occluded by another the other one behind
it is still there you run that after a
few months of life actually you're not
born with that either so there's a lot
of things you learn about the world by
just observing it and you're you know
babies are surprised when you show them
things that kind of don't fit with their
their model of the world and you refine
your model of the world as as you grow
so what methods what principles what
algorithm what metrics do we use to
build systems are able to do this in
further the world is two-dimensional
spontaneously infer that you know object
permanence in you know infer things like
this we don't even have the basic
principles for it and even if we had
good algorithms that could work we don't
even have good ways to measure how well
they work for example you try to do a
video prediction right so you have saved
you know billiard balls and and you know
they are being very simple physics and
you can try to train a machine to
predict whether balls are going to be a
second from now the thing is there is a
lot of uncertainty about how the world
works they could be things that happen
to the balls you know maybe air current
or things like this so you don't have a
perfect measurement of how the ball are
being hit hit and therefore there's an
uncertainty about how the world is going
to be a second from now and best the
best predicting machine can do that you
train to with regression let's say with
a you know square error or something
like this is predict an average of all
the possible things that can happen an
average of all the possible scenarios in
the future and what you get is a blurry
image of you know all the balls at all
possible positions that doesn't work we
we don't know how to solve that problem
we don't even know how to measure the
performance of it if we had a method
that would do it we don't know how to
represent another way of saying this as
more formal is we don't know how to
represent probability distributions in
high-dimensional continuous basis we
don't know how to build a machine that
will tell you here is a picture that
I've taken tell me if this is an actual
picture of the earth is's garbage some
you know artificial thing or noise or
whatever we cannot we're not able to
build the black box that will turn on
the light when you show it real picture
and not turn out no Tony down when you
when you should it's a fake picture you
know something that is not real or
doesn't exist so that means we don't
have good ways of representing the world
yet that's the major conceptual problem
we have to solve today that's what I
would recommend
you know young students young young
ambitious students starting in this
field to try to solve yeah I want to
make one that we're going to go to two
minutes left by the way so i think we'll
give the last the last couple of lot
final comments is our own so let me
first defend reinforcement learning to
say that the basic perceptual stream
that reinforcement when it gets is the
next state of the world and the reward
signal and the next state is an
incredibly high dimensional object and
encompasses everything you've been
talking about in unsupervised learning
so this what is this scalar award thing
that's nonsense that's just defining a
specific task you're trying to do right
now right and of course if you look at
any real application problems it's
actually a multi-criteria reward problem
and so there are all kinds of trade-offs
and there's robustness and they're all
there lots of issues there I so I mean I
think the you know the trouble with
calling everything I supervised learning
is that it's just not it's just like
talking about nonlinear right it's the
residual class so it includes everything
what's the main challenge we have it's
partly representation it's partly that
the only unsupervised learning methods
we have that really work are ones where
we do a lot of modeling and impose a
strong model structure on the world then
we can learn without without a reward
signal or a supervisory signal the
trouble is we don't we there's a chicken
and egg problem there we don't want to
do all that modeling and you know i
don't know maybe maybe zubin automated
statistician will be able to to point
the way to something like that no I'm
serious in terms of having very flexible
non parametric models that can
incremental e impose that so I would if
I were placing bets I would be looking
at something like that Gary Gary gets
lost would as the last word there's a
word that didn't come up I think at all
or at least very much which is
neuroscience and and here's my challenge
for students to think about which is why
is there so much complexity and
diversity in the brain and how does that
relate with the simplicity of the models
that we have so there are hundreds of
kinds of neurons in the brain there are
hundreds of proteins floating around
every individual synapse half the genome
is there to specify the exact circuitry
of the brain that doesn't seem to have a
reflection
the models that we're building is there
anything we can get from the brain and
why it's so complex that would help us
with our models that's my closing
question excellent so all that remains
to do is to thank the panelists thank
you very much indeed
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>