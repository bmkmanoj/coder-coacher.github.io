<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Starfish: A MADDER and Self-tuning System for Big Data Analytics | Coder Coacher - Coaching Coders</title><meta content="Starfish: A MADDER and Self-tuning System for Big Data Analytics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Starfish: A MADDER and Self-tuning System for Big Data Analytics</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KAyUiFm7uDI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi everyone it's my pleasure to
introduce hero rotas hero do too who's a
PhD student who is wrapping up his PhD
at Duke University hero has done some
really good work in the area of query
optimization his in sort of tuning
sequel queries and now more recently on
tuning MapReduce programs so today he's
going to talk about his work on the self
tuning system for big data analytics all
right thank you and thank you everyone
for coming so today I'll be talking
about starfish that's a project that we
started at Duke I guess I started about
two years ago ever since now he has
grown I'm now leading a team of four or
five students they're actually working
on enhancing starfish in different
directions and volar even started
exploring some opportunities and some
collaborations with some other
universities so clearly is a very big
project but for today's talk I'm only
going to focus on the the work that I
have done related to starfish all right
this is actually it's a very good area
it's a very good time to be working in
the area of data analytics I've been to
a lot of talks over the last six months
or so where's all people from
architecture systems machine learning
their motivating their work on how the
the data sizes just keep growing and
growing exponentially and based on some
IDC reports some very recent report the
amount of data that's going to get
managed in the various data warehouses
over the next decade is going to grow 50
times right and the the amount of
hardware itself the cluster sizes will
also grow ten times right so data sizes
are really really growing however the
the number of people the IT staff the
administers are actually managing and
tuning these particular clusters it's
only going to slightly grow by 1.5 x so
clearly you're the same amount of people
that we have today are going to be asked
to
managed in tune much much larger
clusters with a lot a lot more data than
they do today so it's very timely that
we are considering ways of automating
the process of managing and tuning the
classroom making it very easy for the
users are to actually tune the clusters
right then this is exactly where
starfish comes into play the idea behind
starfish is to provide this ease of
management and good performance
automatic good performance out of a
system for big data analytics now when I
start working on starfish a couple years
back I wanted to figure out and really
think about what are the kind of
features that data practitioners would
like or require even from a system on
big data analytics write sisters the
users would like a system to be magnetic
that is they would like to be able to
just simply load their data in whatever
format structure semi-structured or
unstructured data and the system should
be able to handle that and you should be
able to analyze any kinds of of data of
course the system would also should be
agile right data changes the type of
analysis that users want to run on top
of those data also changes so the system
should be able to handle changes a very
easily the type of analysis that people
like to do can be very complex could be
very simple to very very complex might
want to do some machine learning some
statistical analysis some text analytics
etc and of course different people for
different analysis would like to use
diff akhand of interfaces and of course
those kind of interface is usually make
more sense you're going to do some
statistical analysis you might want to
use something like r which is a
statistical software or if you're going
to do some business analytics then
you're more users are more comfortable
with something like sequel right so be
nice for system can actually support all
these different types of interfaces data
life cycle awareness is also another
very important aspect today people are
starting putting together
this big systems that consist of various
components you have the user facing
systems they're serving the users then
you have some loga gregation systems and
then finally some analysis system so the
data will flow through the whole
pipeline the system should be aware of
that pipeline to make sure that
everything goes smoothly and the system
behaves with good performance elasticity
of course nowadays is also very
important especially with how cloud is
becoming very popular people could now
have run a system on the cloud and they
will like the system to be elastic in
the sense that we should be able to add
more resources or take out resources
from the system itself and then the
robustness of course you know it's
almost a given the system should be
robust it should degrade gracefully in
case any types of different failures
would happen so this six main features
in some ways for what we call the modern
principles right so these are the
principles that govern data analytics
systems today now if we take a look into
this matter principles in some ways
they're really targeted targeted in
making the systems easier to use right
users just be able to just load the data
run my analytics I don't need to do any
complicated details or any crazy
transformations to get things to work so
it's very easy to use however you know
if you have a system that's really
really easy to use it's very hard to get
good performance out of it automatically
right and that in some ways is a
consequences of the principles
themselves if you have a system that can
I be used to analyze structure or
unstructured data that means that now it
creates challenges like for example a
lot of the the users are now have to
rely on high level using higher level
in order to do all the complicated
analysis that they would like to do over
these semi-structured or unstructured
data they want to write their udfs to
custom lee parse the data and then
custom we analyze the data at sutter now
like I said sit keep saying this you
know semi structure and structure data
that means that the system doesn't
really know what this data is right
we're moving away from the good old
relational world where the system knows
ok this is the schema these are the
statistics not anymore the system
doesn't know this information right so
it's very hard for the system to figure
out the best thing to do in order to get
off good performance and then finally if
we look at some of the these properties
like elasticity right if you take a
system like Hadoop I'm going to talk
about a lot of kind of in a little bit
it is elastic you could add five notes
for the system and the system will pick
it up and things will work that's
wonderful however how do you decide
whether you should add five notes or not
maybe you should have ten maybe you
shouldn't have any all right how do you
come up with the right kinds of policies
in order to automate this process and
make sure you're doing the right thing
right so a lot of challenges that are
involved here so our goal for starfish
from the get-go was try and get a good
balance between ease of use and getting
good performance automatically and this
is something that's going to come out in
the targus I as I go on all right we're
not interested in getting the absolutely
best performance out of the system it's
all about getting good performance in an
automated way right so currently we have
actually two years ago we've selected to
build the starfish system on top of
Hadoop mainly because it already
satisfies a lot of those modern
principles it can handle unstructured
data it can run all sorts of different
types of analysis its elastic etc so we
decided to build starfish
on top of Hadoop now analyzing data in
Hadoop usually involves loading your
data a lot of times as simple files into
the distributed file system and then
running parallel MapReduce computations
on top of that to interpret and process
the data itself now over the years
there's a lot of systems that have been
built on top and around hop Hadoop
creating an entire ecosystem mainly to
satisfy different users needs and
preferences right so there's a lot of
systems on the side there's flume list
it's a log aggregation system there's
HBase so I get a key value store on top
of Hadoop on the top there's a lot of
systems like pig and hive that offer
more declarative languages more
higher-level languages that users can
use for expressing the computations that
they would like to do for the type of
analysis they would like to do amazon is
offering Hadoop now as a service on on
their cloud so customers could say hey I
would like to get a 20 note how to
cluster to do something all right now if
we look at this entire ecosystem there's
actually a lot of what I'm going to call
a tuning problems or tuning challenges
that we can address ranging all the way
down from you know the cluster itself
how do we decide what kind of machines
do we want how many nodes do we want to
put together to put this cluster
together then going up right we have a
distributed file system going to lay
down the data how do we decide how to
lay out the data both the original data
and they derive data that's going to be
generated from the different
computations we're doing then moving up
we're going to be running MapReduce jobs
right my bridges jobs their execution is
influenced by a lot of different
configuration parameters so how do we
tune all those parameters to get good
performance out of it now moving on into
some of the
the high assistant higher level systems
like pig or hive that in some ways that
generate workflows of my produce jobs
now you don't have individual MapReduce
jobs anymore you have graphs of
MapReduce jobs that are running together
to achieve a particular graph and then
finally you know you have overall
workloads you're running on the cloud so
you have a new set of challenges that
you need to to address all right so in
this talk first i want to i'm going to
talk about what starfish can actually do
today what are the kind of features and
what are the capabilities of the
starfish system so these are going to be
the high level the what starfish can do
and then i'm going to go into more
details on the how starfish can actually
do all the things that it can do today
and I'm for the interest of time I'm
only going to focus on tuning
individuals individual MapReduce jobs
but of course more challenges about
workloads and workflows and cluster
sizing problems who will come through
and I can talk more about it if you guys
like later on and then I'm going to
finish off with some experimental
evaluation and some brief mentions of
some related work and so let me start
about what starfish can do today so what
I want to present next is set of
experiments in some ways that we've run
that in some ways will showcase how
starfish can be used to manage MapReduce
workloads on the Amazon ec2 cloud now
there's nothing specific about Nina
amazon ec2 here you know everything
could work on in-house clusters as well
amazon ec2 was simply a convenience for
us to create different kinds of
scenarios inviting the cluster that
we're using now for people who are not
very familiar with amazon ec2 the air
what amazon has is set of different
machines at different instance types the
users can rants but I think 11 and I
a table outlining some of the more
popular ones so the users could ran
machines of any type to put together a
cluster now if we look at this table
different machine types of course have
different machine specifications if i
were to focus on the c1 that medium a
machine of the c1 the medium type has
five is it two units whatever that means
has 1.7 gigs of memory it can offer
moderate I of performance and it will
cost seventeen cents per hour to rent
one of these machines yes now they do
not quantify what it means and also they
don't really quantify the ECT units
which in some ways complicates things
for users because now you know I'm a
user I know my workload but if I don't
know exactly what the machines are how
do I decide which machine should i be
using right again stressing out the need
for some tool or starfish in this sense
actually tell us hey this is what you
should do right and then apart from
clusters we also vary the workload now I
try to select MapReduce programs that
come from all sorts of different domains
ranging from business analytics to graph
processing to information retrieval and
then the data sets that I used we're
both real and synthetic with sizes
ranging from tens of gigabytes up two
terabytes and of course adding to this
table are more traditional benchmarks
out there like the t PCH benchmark via
the yahoos picmix benchmark there's a
high performance benchmarks that I've
played with those as well night so
starting from the basics right you're
running you want to run a single
MapReduce job on the cluster on your
Hadoop cluster
if you take that job you take Hydra
buses and that job asses and you're
going to run it you're going to get some
performance now like I said earlier
there's a lot of different configuration
parameters that you can use that will
actually affect the performance of that
particular job so what this graph shows
is actually the performance when running
these different MapReduce jobs that have
shown the table before outlined on the
on the x axis of the graph as we run
them using three different configuration
settings now the blue bar shows the
speed up the blue bar represents the
jobs running using the default MapReduce
settings and the graph is normalized on
actual the default settings so of course
everything here is one now Hadoop
experts and very expert users mainly
from yahoo or Cloudera who have a lot of
expertise into using Hadoop have
published a lot of rules of thumb what
we call rules of thumb thing about this
as these are best practices on how to
actually set different configuration
parameters in order to get good
performance out of the system now I went
through all that all those blogs and the
different tutorials etc figure out the
different route which different rules I
should apply to different jobs
essentially manually tune every single
job right and the red bars here shows
the speed up that I was able to achieve
after following all these different
rules of thumb compared to the default
settings now as you can see we can
clearly get really really good
performance out of it we can get speed
ups up to you know 30 20 to 30 x speed
ups by following these rules which is
wonderful it's great however the one
thing that
is not really shown in this graph is all
the the effort and the time that I put
into figuring out which rules should be
used for different programs a lot of
those rules are also kind of quality
things like okay if your MapReduce
program is not memory intensive and
could do partial aggregation then you
should set some memory buffers to be
high all right that's rule of thumb
right as you see that you know it really
depends on the program that means I need
to understand what the program is doing
it I also need to understand what the
different settings mean in order to be
able to set them etc etc right so it was
a very labor-intensive time intensive
exercise to figuring out how to set
these rules of thumb I in order to get
these good speed ups the final bar the
green bar here shows the speed up that
we're able to achieve after using
starfish till automatically tell us what
configuration settings to use and as you
can see in all the cases starfish was
able to automatically give us settings
that would match all the rules of thumb
and in some cases even surpass the
performance that we're able to get with
all these rules of thumb and again I
didn't have to spend any time in
figuring out what rules to use or how to
use them right now of course all of
times people don't just submit
individual MapReduce jobs will submit
entire workflow of MapReduce jobs either
themselves or they're going to be
generated by higher level systems like
pig or hive at this point starfish is
also able to support tuning p cladding
scripts which is the other scripts use
by the peak system so what you see here
is two graphs on the x-axis there are
different queries from two different
benchmarks the t PCH and the hive
performance benchmark and this graphs
show now the speed up with regards to
rules of thumb
I stopped using the default settings
clearly you know they're performing
pretty poorly in some cases you can't
even use them anymore because some of
the programs will not even run using the
default settings and again in all cases
as you can see from the green bar the
starfish optimizer was able to give
settings that will match or surpass the
manually tuned settings that were able
to achieve for the different MapReduce
jobs here now another very interesting
scenario that arises in a lot of
companies is the presence of two types
of clusters usually as you know
companies will have development clusters
and they'll have production clusters you
know all the mission critical workloads
will run on the production cluster
usually well isolate it and then there
are a bunch of development clusters
where the developers can actually test
there are the different MapReduce jobs
they're developing figure out how they
work figure out that they work correctly
and then they want to move them on to
the production clusters when the jobs
are ready now there are two questions
that arise naturally in this setting
first is okay I have seen how my jobs
behave on the development cluster so now
how will this job behave once I stage
them over on the production cluster and
the second even more critical question
is what settings should i be using for
running these jobs on the production
cluster now one of the capabilities the
starfish has today is after observing
how the jobs behave on the development
cluster starfish can actually answer
both of these questions without ever
running the jobs on the production
clusters themselves alright so this
graph shows the running time of again
our different MapReduce programs the
blue bar shows the actual running time
this is how much time it took to run
these various programs on the production
cluster itself and then the red bar
shows the predicted running time this is
how much time starfish predicted that
these jobs will take when we run them on
the production cluster now a few things
to note here is that starfish was able
to make these predictions without ever
running any of these jobs from the
production cluster it observed how the
jobs behave on the development cluster
which in this particular case consisted
of ten large nodes and the the jobs were
processing about 60 gigabytes of input
each whereas the production cluster was
actually a much bigger much beefier
cluster consisting of 30 extra large
nodes and the jobs were processing now
much larger data so starfish after
observing how jobs behave on a small
cluster on small amounts of data was
actually able to predict how the jaw the
same jobs would behave on a much bigger
cluster processing a lot more data now
the other thing to notice here one of
the important things that I want to
bring out here is the accuracy of these
predictions you know they're some of
them pretty close some of them not so
much but the important thing is that we
actually we're able to get the different
trends together and we're able to do
that then we can actually optimize the
individual MapReduce jobs later on yes
run times on the production cluster
relates to the run times that you saw on
the
morning to me basically I'm asking about
correlation it's just a question of
finding us so the simplest model could
be okay I take a simple linear pattern
scale everything yes bump it so the
answer is I see what the the question is
actually let me simplify that question
so let's say we had we run two jobs on
the development cluster and we saw job
one was running much faster than job too
would that still be the case in the
production cluster would I just say okay
this is a three times bigger cluster
multiply the running time by three would
that still be the case the answer is no
there's a lot more things in play here
yeah you have more notes to run your
processing little more data it's just
things are just too complicated to be
able to do that linear scale up and
there are even cases that I've seen
where one job might be running a little
faster on the development cluster but
then for one reason or the other it
would run worse on the production
cluster etc right and then the final
scenario that really arises with the
cloud is the following suppose we have a
user that wants to execute a map reduced
workload and of course there are some
performance requirements that come along
with it for example they might want to
you know do some report generation and
they want to finish within two hours
before you know the morning or something
and a lot of times they may have also
some actual dollar amount because you
know we're using the cloud we're
actually paying money to use the cloud
so there might be some monetary goal
there as well right so now in this
settings there is arranged a different
range of decisions that the user has to
make ranging all the way down from the
cluster itself right the user now needs
to figure out okay how many notes should
i use what kind of type of notes which i
used to put my class were together etc
and then once I put the cluster together
on a run Hadoop there's a bunch of
different settings that I need to figure
out
you know how many slots should I use you
know what different memory settings
should i use for Hadoop etc excuse me
and then finally you know these
workloads are going to run we need to
figure out okay how many map introduced
us should we run again there are
different memory related parameters
should I use compression all sorts of
different parameters that we need to set
so as we see now the range of decisions
grows even larger it's a lot more
decisions to make against starfish can
actually help into answering making
these kind of decisions for the users
themselves right so to summarize there
are tuning levels at all sorts of
different levels job level workload
lover cluster level etc so these are the
different kind of tuning challenges and
tuning problems that starfish is able to
resolve today now a lot of this you know
tuning challenges for tuning problems if
you like you know they have different
flavors in some ways but when we started
putting a starfish together we really
wanted to figure out some sort of a
universal approach that would allow us
to actually solve all these problems in
a nice unified way now the way I was
actually able to achieve that was to
take you know it's a unified approach
into turning and this unified approach
will be summarized in this particular
slide right so the first but we wanted
to do was to observe how the job behaves
and learn from it right not the
responsibility of a component that we
call the profiler so the profile the
profiler is responsible for collecting
concise summaries of how different
MapReduce jobs are executed on different
clusters and learn from it right then
this information will be used from a
different component which we call the
water engine that can actually estimate
the impact
that different hypothetical changes can
have our to the job execution itself so
we can now phrase questions to the
system of the four okay I have observed
how this job behaves running 10 my 10
reduce tasks on a 20 node cluster right
how will it change how will the
performance change if I were to run 20
reduce tasks in 10 instead of 10 how
will it change if I were to add three
more nodes into my cluster like what if
I now have a bigger data set how will
that affect the performance yet so the
what-if engine is responsible for
answering these kind of questions and
then finally for the different problems
you can think about it you know there's
a lot of different decisions that we
have we can make all right so there's a
search space to search through and for
that we have a set of optimizers the
optimizers are responsible for
enumerated and searching through the
decision space to figure out what's the
best decision to make to get good
performance and to satisfy the users
performance requirements so now I'm
going to go a lot into into lot more
details regarding the profiler the word
if engine and the optimizer from the
aspect of tuning individual MapReduce
jobs right now the way that we view a
MapReduce job is the following we view
it as a quadruplet consisting of the
actual MapReduce program that's going to
run on the cluster the data properties
of the data that's going to get
processed by this program the particular
cluster resources that we have and then
finally the different configuration
settings now running a MapReduce job on
Hadoop cluster essentially involves
running individual map and reduce tasks
the map tasks are going to process
partitions of the data which are called
splits to produce some intermediate
output that's going to get
transferred or shuffled into the reduced
tasks the news tasks are then going to
process the intermediate data to produce
the final output and depending on the
resources and how big the cluster is
this map or reduced Haskell actually run
in multiple waves now what affects the
execution of each map and reduced task
is a set of configuration parameters
these configurations determine how many
map and reduce task i'm running
determines how do i want to partition
the data how do i set different memory
related parameters to buffer you know
map output or the reduced input should I
use compression of this intermediate
data or the output data should i use the
combiner that's a local pre aggregation
function that can be applied at the map
output etc etc so we have a fairly large
space of different configuration
settings that can actually affect the
performance of MapReduce jobs now at
this point the you know a typical
question that might arise is you know
how much do these different
configuration settings actually affect
the performance of individual MapReduce
jobs now the answer is well the transfer
is it it really depends on the program
itself and the cluster and the data but
they can actually have a significant
impact on performance in different ways
for different programs so now what you
see in this graph is on the the X and
the y axis we see two particular
configuration parameters that's the I
audits order and B and the iota sir the
record 0 % these are two parameters that
affect the the memory buffer then used
for the map output so on the z axis we
see the actual running time of a
particular program it's a in particular
as a word co-occurrence MapReduce
program that was run on a 16 node
cluster on ec2
so we see how this performance varies as
we vary these two configuration
parameters okay so we see there are some
areas this is the blue areas where the
job performs very well we get really
good performance out of it then we see
some other areas the red areas where we
actually get some really bad performance
out of the job itself right so it is our
job to figure out how do we get settings
that are in the blue region and
definitely far far away for all the red
region itself right so this is just a
two-dimensional projection of the
surface back when I actually generated
this graph I generated a lot of this
graph with different configuration
parameter settings got all sorts of
crazy different shapes and forms but all
these surfaces in some ways can be
mathematically can be represented as a
function and it is a function of the
program the data the resources and the
configurations right so all these four
things together are affecting the
performance of the MapReduce job well
then you can use essential pretty much
any performance metrics that you would
like from resource consumption to total
execution time etc for simplicity let's
now think about performance as the total
running of the job right so when we're
asked to optimize that is to find the
optimal settings that will minimize the
MapReduce jobs essentially involves into
figuring out okay which settings should
i use to minimize this function f like
the you know in theory all is well
however the main challenge here is that
this MapReduce jobs in many cases are
actually expressed as my produced
programs retaining high-level languages
like Java or Python or generate it by
some systems like peak or high right so
how do we capture the
tuition of the job how do we represent
these different programs written in
these arbitrary languages and the answer
to that is profiles job profiles so a
job profile is a the abstraction that
I'm using to represent the execution of
MapReduce job right so think about this
at the end of the day this is a vector
of features that characterize how
individual map and reduce task executed
on the cluster itself and it records
very detailed information at the level
of task faces and when I say task faces
I mean really subtasks subtask faces
right like I said earlier MapReduce job
will run as map tasks and reduce tasks
now within every map or reduce task we
can actually break down the execution
into various smaller steps for example
for a map tasks first we need to read
the input split from the distributed
file system then we need to execute the
user defined map function to produce
some output that output is going to get
serialize and partition into memory
buffer the memory buffer gets fully
fueled up then we actually need to spill
that to disk before spilling we're going
to sort it there's a combiner we're
going to use it in this compression if
compression is enabled again we're also
going to use it so that might actually
to the creation of multiple spills and
then finally we will need to merge it to
produce the final output that's going to
get shuffled to the reduced tasks and
there's a similar thing that goes on for
the reduced task right now by using this
information in the job profiles we can
actually analyze the execution of
MapReduce jobs and understand how it
actually behaved so what you see in this
in the screenshot here is a screenshot
from the surface visualizer a graphical
user interface that we have that the
user can use
to understand how the jobs behave so
what you see here is for every different
note you can see that the black box is
representing individual map tasks that
run and the purple boxes actually
represent reduce tasks so we can quickly
visualize all the map and reduce waves
and how they execute it on the cluster
now all that information is available to
us from the job profile itself there are
two main dimensions in the job profile
there is one set of data flow fields in
this one set of cost fields the data
flow fields represents the flow of data
through the different map and reduced
task phases alright so these are things
like how much records and how many
records and how much bytes did they go
into the map tasks and came out of the
map tasks how many spills do we do how
many merge runs do we do etc for the
people who are familiar to Hadoop this
is a superset of the Hadoop counters
right so it's a lot of information that
represents how the data went from the
different sub task faces and that
information and actually prefer i go
there and then the second dimension is
the costs this represents for the most
part timings of how much time will span
executing the different task phases
within individual map and reduce tasks
like how much time did we spent
performing I all versus how much time do
we spend doing cpu processing of the
user-defined map function now all this
information we collect this information
from all the map and reduce tasks we
don't just have you know single numbers
here we actually have entire
distribution of these values and then we
can use those distributions to actually
build more meaningful and more
insightful
representations like histograms for
example by looking at the output
produced by the different MapReduce task
we can build a histogram that will tell
us you know how much data were output by
the different MapReduce task so we can
see if there was any skew involved there
were any bottlenecks that would
potentially create any bottlenecks in
the execution etc and then with the
costing information we can actually
break down the execution of individual
map and reduce tasks so we can see how
much time was spent in the different
phases and again perhaps identify that
okay so in this particular case the
spill phase that's the phase where we
actually you know sort and spilled to
disk took a significantly long a chunk
of this mob execution so perhaps there
is something we could do there maybe
there's some configuration settings we
can change there in order to figure out
how to decrease the execution time of
the map task now generating these
profiles comes in actually two ways one
is we can actually measure how the week
actually measure these fields and this
is done by the profiler itself and this
is how we can generate the job profile
or we can estimate a job profile and
this is actually done by the water
frenchie and I'm going to go into both
approaches in a little bit now when it
comes to the measure measurement based
profile right this is where the
MapReduce job is actually running on the
cluster and now we want to observe what
happened right when I started putting
together the other profiler you know I
had some goals in mind first of all I
should be able to turn profiling on and
off dynamically all right so when I turn
it off there should be zero overhead on
the on the cluster itself which is
something that essentially required
especially if we're running on that on
the production cluster and then on
demand can turn it on and observe and
learn from how the job behaved with some
low overhead the second requirement well
second and third in some ways had to do
with how usable starfish is going to be
or what's going to be the first one is
we didn't want to modify Hadoop in any
way and we also did not want to require
the users to modify their MapReduce
programs in anyways all right so in some
ways we wanted to increase the the
chances for starfish to be adopted and
that's definitely something that was
proven to be very wise I guess at this
point because people have actually
started looking into starfish ants are
using starfish right now the way I was
able to achieve all these goals in terms
of profiling is by using a technique
called dynamic instrumentation so down
instrumentation is a technique that has
become very popular in programming
languages and then the compiler world to
monitor and a dbag in some ways complex
systems so we took this approach here as
well now individual map and reduce tasks
are running into Java Virtual Machines
jvms right now what dynamic
instrumentation allows us to do once we
enable it in some ways is to actually
cut in into these java virtual machines
in dynamically inject bytecode based on
some event condition action rules that
is if anywhere happens say method gets
called right and a condition is met then
a particular action needs to be taken
now that actually in most cases has to
do with you know recording you know the
content of some variable or doing some
fine grain timings to collect all that
information so when we enable profiling
we dynamically instrument Hadoop Java
classes to collect these raw monitoring
data now draw monitoring data
will then get collected post processed
in some ways to build this individual
map and reduce profiles that are then
I'm going to get combined together to
build job profiles which is what we're
going to use later on for analyzing and
optimizing the execution of MapReduce
jobs now of course when you're running
my produce in large scale you're running
them up with this job that could
actually have hundreds or even thousands
of map and reduce tasks there's really
no need to profile every single one of
them so therefore we have enables some
sampling features into starfish the
first one is to use something to profile
fewer tasks rather hand at 1000 map
tasks running who could profile ten
percent of those map task and we're
going to collect enough information to
build a very accurate job profiles and
the reason of course behind sampling is
to decrease the amount of overhead on
the job execution even further now if I
only want to run this job to collect
some profiling information very quickly
because let's say I wanna run to this
large ad-hoc job and I just want to get
some information very quickly then we
don't even really need to run all those
you know 1,000 tasks we can actually
execute fewer tasks so we can get aprox
built approximately like job profiles
very very quickly that we can then use
to optimize the execution of the job
right so what these abstraction what
this job profiles enables us to achieve
is in some ways we're able to represent
any arbitrary MapReduce jobs by using
this you know set of fields this data
flow and cost fields and in all reality
we use these data flow and cost fields
to actually compute a set of data flow
statistic and a set of costly sticks
which are in some ways independent of
the actual execution it's
alright so data flow statistics are
things represent statistics over the
data like what was the selectivity of
the user defined map function in terms
of records what was the compression
ratio that we're able to achieve and
then cost statistics are statistics
things like what was the average I or
cost from reading from local disk or
what was the average CPU cost for
processing the user defined map function
now by using this data flow when cost
statistics we can actually predict how
my produce jobs are going to behave in
different settings and like I said the
ice is done by the water function so the
water friend gene can actually predict
you know the hypothetical changes on
data cluster resources and configuration
settings on the the programs on the
program execution so the job profile can
be given to the word if engine to
estimate how the job will behave and
that's all it needs in terms of the
program itself right so we're able to
completely abstract away the the
user-defined map and reduce program into
this job profile now when we want to ask
a hypothetical question to the orphanage
in there are three more inputs that are
required right so we need to specify you
know the full essentially scenario I
want to run this particular job
represented by this job profile over a
particular over a new input data set on
perhaps different cluster resources and
use perhaps different configuration
settings now any of these three could
actually be hypothetical so these four
inputs will go through the water engine
and out will come the properties of the
hypothetical job now the what-if engine
works in two steps the first step is
taken by the virtual profile estimator
now this component is
munnsville for estimating what we call
the virtual job profile this is the
profile that we would have observed if
we were to run this job on this
particular settings right then this
virtual job profile is going to be used
by by our task scheduler simulator that
will simulate the scheduling and
execution of individual map and reduce
tasks on the cluster in order to get an
understanding of the full execution of
the MapReduce job and when I say the
full execution I really mean a
description of how each and every map
introduce task is going to behave and
then we can actually use that
information to visualize any of the
screenshots that I showed earlier about
the histograms the timeline of execution
etc we can actually visualize all of
that by the output of the what-if engine
now of course the most critical
component of all here is this virtual
profile estimator right so the goal of
the virtual profile estimator is the
following give it a profile that we have
observed for a particular program on
some input data some classical resources
and some configuration settings we want
to estimate what the virtual profile
will be for the hypothetical job in
different data probabilities different
cluster resources and different
configuration settings now what you're
going to see in this one slide here it's
actually about one year worth of work
give or take that I try to summarize
inner in a single slide so estimating
you know making this estimation
essentially involves estimating the
individual categories that comprise you
know these MapReduce job profiles who
want to estimate the data flow
statistics the cost statistics data flow
and cost information for these virtual
profiles now these categories represents
you know of course very
different information and therefore I
decided to use different modeling
techniques to model these different
categories for estimating data flow
statistics for the virtual profile at
all we need to do is we look at the data
flow statistics that we have observed
and combine it with the current input
data that we're asking the hypothetical
option off and then we use some
traditional carnality models good old
you know database style carnality models
to estimate the data flow statistics of
the virtual profile now when it comes to
cost statistics right this are like I
said earlier this is information that's
very well connected to the action of the
machine specification and we're talking
about I all costs or CPU costs so if the
cluster resource says the hypothetical
or the new cluster resources are
different now we need to translate these
cost statistics that we have observed on
you know one cluster into the cost
statistics that we expect to observe on
the new cluster so for this purpose I
use a set of relative black box model
this is machine learning models
essentially to actually make this
translation and then when it comes to
data flowing costs these are specific in
some ways on how Hadoop behaves so for
this I I came up with a set of white box
models that is these are analytical
models literally a set of mathematical
equations that explain how individual
map and reduce task will behave in terms
of data flow and costs this information
here the the analytical box models it's
about 16 pages of math that are
available in a technical report and all
of the other things are described in
this paper that I present and in soccer
leer so there's a lot of details hidden
behind this slide and of course I
definitely don't have any time to go
into any of this so I've decided to only
go a little bit into these relative
black box small
this machine learning models that we use
to make estimations across different
clusters right so imagine that we have
you know a blue cluster and a red
cluster that they have perhaps different
types of nodes different number of nodes
you know thing about this could be the
production cluster and the develop the
development cluster and the red would be
the production plaster for example right
so we we run in profile the job on the
blue cluster we collect the job
profiling information and then we get
the cluster istics of this particular
job running on the blue classroom right
so we have this you know set of numbers
essentially to represent the cost
statistics now our goal is to figure out
what the custody stakes will be for this
job once we run it on the red cluster
now in order to be able to make this
translation if you like or this
prediction we essentially need to
collect information about how would a
job behave in these two different
clusters right so in any typical you
know in many typical machine learning
algorithms we need some training data
and these training data will come after
running a set of training jobs very
carefully chosen set custom me custom ly
implemented set of training jobs that
will run in both of these clusters to
collect essentially cost statistics
right different jobs will behave
differently have cpu-intensive jobs are
your intensive jobs etc we run them on
both clusters so we collect this cost
statistics for different scenarios and
then we're going to use those
informations to build a machine learning
model here pretty much any supervised
learning algorithm would work after
playing with a few I decided to use
something called an m5 model tree thing
about this is a decision tree where at
the
the leaves you have small regression
models to capture their the trends so
these models can then be used to
translate cost statistics from the blue
cluster into cost statistics in the red
cluster alright and then the final
component the job optimizer can be used
to figure out what the best
configuration settings to use for
running a particular MapReduce job over
perhaps different input data and
different cluster resources again the
job optimizer follows two steps the
first is to enumerate the entire search
space and try to figure out if there are
any independent subspaces that we can
cut our space into and then for each one
of those independent subspaces now we
need to search through this space now
this space essentially represented by
that f function that I showed very very
early on and implemented by the wharf
engine so for this of course you know
it's a sir it's a the function itself
it's it's black box is non model I
doesn't behave in any well-known way so
we ended up I ended up implementing this
method called recursive random search
that's responsible for searching through
the space and as it searches through the
space from the different points in the
space it will make appropriate what if
calls to figure out what the cost will
be for executed for executing their job
in the cluster at that particular point
yes jobs right job is Hector sponsor
control hmm in order to look release
program p you need to train also on
executions biggest execution the program
dealer put your trade or compute event
in other words you support ad hoc you're
right right right that's that's very
good question and that is actually a lot
of details are going to how do we come
up with these training jobs one thing
you could do is say okay so you know
what your workload looks like so we
could take a sample of that workload or
even the entire workload that you know
and then run it on both clusters observe
and then build that information that
that would definitely work the problem
with that is now if you have a new job
that come in that behaves differently if
you don't have the training data we
haven't observed the training data then
the machine learning model is not going
to work very well right so for that
reason we did not follow that approach
right so if we take a step back and
figure a thing about what do we need
here we need to predict how cost
statistics will behave right so how
these numbers relate going from the blue
cluster to the red cluster so the goal
of the these training jobs is in some
ways to generate data that will cover
this prediction space that consists of
all the different range of values that
the different cost statistics can take
so for this purpose I actually
implemented a set of custom MapReduce
jobs that they don't do anything useful
to the user but they exercise they
actually run mob and reduce tasks that
behave very differently in order to
capture as much of that prediction space
as possible so that any job that will
come in existing job or new job will be
very likely to have cost statistics that
will fall into the training data that we
have collected right all right
now you said until 11 40 10 minutes okay
that's plenty of time for that okay all
right a sense of only left now with
experimental evaluation is the main
section so for the experimental
evaluation like I said it's essentially
evaluating the different functionalities
of starfish that I have presented so far
so let me start from the the profiling
itself like I said earlier if we turn it
off we have zero overhead but if we turn
it on we do get some overhead and the
the tool that I ended up using turn out
to have a little more overhead that I
would like it I guess so I didn't
mention this earlier so I use this tool
called be traced to actually implement
the net profiler into so what you see in
this this two graphs is I wanted to show
this trip cost benefit these trade-offs
between the overhead and the benefit we
can get from the profiling so the first
graph shows the percent overhead that is
added to the job as we profile more and
more tasks as you see we profile every
single MapReduce task and in this case
by running these work recurrence over 30
gigabytes of data we have a few hundred
map tasks that are running we can get up
to almost thirty percent overhead which
is pretty significant but by only pro
but when we start profiling Lee only
five ten twenty percent then the
overhead that we observe is much much
lower definitely more appropriate and
then if we look at the graph on the
right that shows what is the speed that
we're able to achieve by using job
profiles that were approximately job
profiles that will build from you know
the different percentage of the task
profile we see that profiling up to ten
percent of you know tasks it will give
us enough information to build a profile
that's
that's pretty much the same profile as
if we were to profile one hundred
percent of the tasks now the next brief
evaluation of the water engine of course
it did a lot more to evaluate the water
engine but visually visually the easiest
way to observe is actually to compare
essentially the different surfaces that
we can get so on on the left here you
see the actual surface that you saw
earlier that shows the running time of
this work occurrence program acid was
run in different settings for this to
configuration parameters so I literally
run this job it was five times for
twenty different times to get this
particular graph now I used one of those
settings I profile in one of those
settings and then use that jab profile
to ask to make twenty what if calls to
the word if Angie and that generated the
estimated surface on the right so as you
can see you know the word if I'm not
going to claim that the two surfaces are
identical of course but what I will
point out is how the water fine gene is
able to clearly capture all the
execution trends that results from
varying these two configuration
parameters we can clearly identify the
blue regions and the red region is here
and the different trends and all these
based on a single execution job on a
single settings then we have let's see
this this graph shows the the running
time of running these individual
MapReduce jobs on the production cluster
using settings though three different
settings one was using the rules of
thumb settings the second one the the
red bar was using the settings suggested
by the starfish optimizer based on the
job profiles that we have observed on
their development cluster right before
Ronnie
on the production cluster at all so as
you can see in all cases were able to
match or improve upon the execution
running time of the rules of thumb
settings now to see how much better we
could have gone if instead of profiling
on the development cluster we were to
profile on the production cluster I put
their these green bar so for the green
bar we actually profiled on the
production cluster and then use that job
profile to get goo get the configuration
settings to use on the production
cluster now as you can see in almost you
know every single case it didn't really
matter whether the profile came from the
development cluster of the production
cluster we're still able to get very
very good performance in both cases and
then finally these / traits to the last
scenario where we can actually make
predictions and recommendations with
regards to the number of nodes to use or
the type of nodes to use so what you see
in this two graphs is the running time
on the top and the monetary costs on the
bottom of running our MapReduce work
consists of all those MapReduce jobs all
right so the blue bars shows the actual
running time of the workload run on
let's say this word 20 node clusters of
different node types now the red bar
shows the prediction running time of the
entire workload again on the different
node types and to get these predictions
we use the job profiles that were
collected on a 10 node cluster of m1 dot
large instances and again as you can see
we're still fairly accurate in
predicting the running time as well as
the monetary cost and again the
important thing here was to actually
capture you know the trends and not the
absolute predictions
so finally clearly this this a lot of
related work around with starfish I just
wanted to I guess briefly mentioned some
of related work there's definitely a lot
of work being done on self-tuning of
database systems with project like Leo
from IBM or auto admin here at Microsoft
Research so we'll work on adaptive query
processing where you change your
execution plan planet drying time now in
terms of optimizations at the MapReduce
systems this load rule based techniques
that I've been implemented especially in
the higher level systems like hive or
pig and I or how to select good data
layout so columnist storage papers are
coming out etc and then finally when
with regards to performance modeling
that's definitely a huge variety of
papers for black box models and illegal
models simulations in the MapReduce
systems and in different systems etc
right so to summarize starfish is a
system that is able to achieve good
balance between ease of use and getting
good performance automatically out of a
system actually a whole ecosystem of
system like Hadoop and all the different
systems around it and it focuses on
tuning at all sorts of different levels
from job level to work flow level and to
cluster level we've also released part
of the the starfish code so you can
actually go on the website you could
download starfish and can play with it
you can use it to visualize your the
execution of MapReduce jobs you can ask
what if questions and you can optimize
my produce jobs and then in the next
version we're now adding some actually
already added support for pig and we're
in the process of adding support for
other higher level systems like hive and
cascading alright and I guess at this
point I'll take any questions for the
time we have left
because this this might be a question
that takes a little longer but the one
interesting thing is your choice between
using what sort of using machine
learning for the cost models and then
using white box models for other parts
orite motivated that the I could
summarize that in one line that I guess
no size fits all would be the answer
here there's there so diff there the
different things that I wanted to
estimate we're so different than using
let's see where was that hey using the
the same model for everything just
didn't seem to be right right when we're
trying to make predictions with regards
to memory specification to hardware
specification like we have different
memory different CPU architectures you
know I representing that using white box
and illegal models we just almost be
impossible right or you would not be
able to get good predictions out of it
right now the other way around I if I
were to use I could have used machine
learning models to capture some of the
to predict data flow and cost
information for example however these
are far more complicated there's a lot
of interactions between them so i would
have to figure out what all the
different interactions and their
prediction space is actually much larger
and then at the same time there was no
reason to do that because we actually
understand how hadoo behaves we can see
how it behaves we know exactly what's
going on we know the different rules
that they follow for spilling and for
sorting so we can actually model all
those things analytically and if we can
model things analytically then we can
actually get much better predictions out
of it right so the clear motivation was
figure out what's the bed
model that will give us the best
predictions yeah so use of Internet
earlier on your topic
people are actually using start thinking
about is can you say more about who is
using it and what is need to pay back
yes oh so within academia itself
starfish is actually inspired inspired a
couple of new projects it's a project
that i know of waterloo and another one
in hong kong university that they're
starting to look at different ways to
extend and use starfish now in terms of
the industry itself i have been
contacted there a couple of local
companies at the Research Triangle area
and then I guess our biggest connect
connection right now is Zarah set it's a
company that commercializes Hadoop and
offer support for Hadoop to competitor
of Cloudera that they've started to look
into how to use starfish and maybe we're
actually looking to some collaborations
possible collaborations on how they can
actually use starfish in their product
and how we can actually run starfish on
some real production clusters and
production real real world workloads and
data right it will of course yahuwah as
well now that we have support for pake
the next thing to do is actually give
them softer so we can see how it behaves
when it analyzes again real workloads
express in the pig latin query there's
one more that i'm missing oh and
Hortonworks of course yeah came out of
yahoo right yes
say adaptive query processing or just
other techniques where how much does
this dress right uh so as for us
configuration settings is concerned I
really don't think there is much speed
up left but this is only one little
slice in some ways of the optimization
space now like I said that the i guess
the very first slide was i wanted to
present the work that i did by myself
right so these are all the work that I
have done now aduke we're actually
looking into a lot you know much larger
picture of starfish in particular right
now we're looking into how to optimize
ations at the different levels of
workloads right so we have a workload
that's expressed at saying hi or pig
latin right now there's a lot of
different optimizations we could do
there we actually broke it down into
four different categories right so we
could do logical optimizations you know
traditional join rearrangements push
down selection those kind of
optimizations of course that we could do
them in a cost-based fashion by using
some of the starfish capabilities here
the the second layer is physical
operator selection now we have you can
implement the same logical operation in
different ways how do you select which
one is better right should i use my
typical reduced site join versus a
fragment replicate join etc the third
layer is how do we park now these
logical and physical operations into
individual MapReduce jobs right and
there there's a lot of possibilities for
merging jobs together sharing scans
sharing data transfers figuring out how
to set up data layouts or partition the
data in particular ways such that you
know jobs that come later on to read the
data actually benefit from there so
there's a lot of work there actually
this is work I'm working on right now
with another PhD student back at Duke
and then the fourth level is the you
know the parameter space level which is
what this presentation was focused on by
though if you look at the entire stock
there's a lot of optimization
opportunist and a lot of speed up that
you can get</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>