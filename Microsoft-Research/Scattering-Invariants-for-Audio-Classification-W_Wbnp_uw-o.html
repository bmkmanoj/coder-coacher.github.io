<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scattering Invariants for Audio Classification | Coder Coacher - Coaching Coders</title><meta content="Scattering Invariants for Audio Classification - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scattering Invariants for Audio Classification</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/W_Wbnp_uw-o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay welcome everybody it is my great
pleasure to welcome a joke a man didn't
hear from the Ecole Polytechnique in
Paris we're sort of got lucky he was in
town visiting some folks at u-dub and
had some free time were able to snatch
him and come and give us a talk on his
very interesting work on a scattering
transform which is sort of having lots
of interesting applications and audio
and image processing and without further
ado I'll I'm gonna take away and we
should all say congratulations i believe
he has defended his thesis recently and
in two days will start a postdoc at
princeton so he's the only has two more
days of freedom left for a good job
thank you yes so so my name is Joachim I
met some of you already and I'm going to
talk about some of the work that I've
been doing to get us to found my advisor
during the during my PhD thesis these
last few years and so my thesis has been
mainly concerned with classification of
audio signals and one of the problems in
in classification is we want to model
our different classes of signals very
well in order to be able to identify in
what class to put an unknown signal
problem is we have a lot of different
types of variability and audio signals
that's not necessarily very important we
can have we can have shifting in time we
can have stretching in time we can have
trans position in frequency and so on
that's not necessarily that important
but nonetheless takes up a lot of
training data if we want to construct a
good model of it so for example we have
these three sounds which are the same
word pronounced differently and as you
can hear as
it's pretty obvious that they're the
same word but we see on the spectrogram
that we have very different we have
different very different properties in
terms of pitch duration and position
relative to the center and so we have
all this data we want to be able to
model it well one Mabel to model all
these variability well but it's not very
useful for determining the class of the
the signal and so this is this is one of
the one of the problems and what's
usually done to solve this is instead of
trying to model the waveforms directly
we have an intermediate representation
that reduces the variability of the data
in order to allow us to construct models
without having as much as much training
data and so we don't want to reduce too
much information because we want to be
able to keep what's important for for
classification but this is not
necessarily what very obvious we don't
necessarily know what type of
information is going to be important for
the classification task so what we're
going to do is we're going to take a
very conservative approach and we're
going to say we know what we don't need
in our representation we know what's not
going to be useful for the
classification task and this is going to
be transformations of the signal that
don't affect its class and we want these
transformations not to affect our
representations either and so this can
be time shifting this can be frequency
transposition and so on but on the other
hand we want to keep as much
discriminability as possible so just a
quick overview of my talk I'm first
going to talk about time shift in
variance and how it applies to audio
representations specifically the
scattering transform and then look at
how the scattering transform is able to
capture a certain discriminative
properties of audio signals after that
we're going to look at transposition
invariance how we can extend this
representation to get something that's
that is invariant to shifts in pitch
which gives us another type of
scattering transform and then finally
apply these to two different audio
classification problems and so that way
we formalize this mathematically as we
just take our signal we shifted
in in time by a constant and that's our
time shifting and what we say is that a
problem is invariant of time shifting if
a given signal is still in the same
class after we shifted in time and then
we require a representation also to be
invariant but we also have a more
general class of transformations of the
signal which are time warping and so
this corresponds to taking a signal and
then deforming it slightly in each
position so it doesn't have to be a
constant deformation which would
correspond to a translation but it can
vary with position and this means that
it also includes dilations you have
locally a time warping like this is
going to be a deformation and the larger
the deformation generally the more
different the sound will be it will
induce changes in pitch changes in
duration and so what we want to say is
we don't want to be invariant to time
workings instead we want to be stable in
the sense that small time warping which
have little effect on how we perceive
sound how a little effect on its class
we want that to have a little effect on
the representation but for larger time
warping where the sound is going to
sound very different we want the
representation to change a lot so we
have what's called the Lipschitz
continuity condition or a stability
condition to time warping that we're
also going to require this is in fact a
much stronger condition we have lots of
representations that will satisfy time
shift and variance but not necessarily
time warping invariance an example of
this is the spectrogram which is
commonly used but not often for
classification directly and the reason
for this is while it is time shift in
variance in the sense that if we do a
small time shift with respect to the
window size we don't have stability to
time warping specifically if we take a
pure dilation where we have a linear
time warping we see that for a sample
signal like this where we have a
harmonic stack in the low frequencies we
overlap significantly in the original
signal and in the word signal but in the
high frequencies we're not going to have
the same thing because the shift is
going to be proportional
to the center frequency a solution to
this is to average the spectrogram in
frequency which gives us the male
frequency spectrogram and the important
thing is when we average it we wanted to
be constant Q at high frequencies
because this constant Q filter Bank is
going to ensure that we compensate for
the increased movement in the high
frequency components and the result is
then after averaging we have something
that overlaps just as much in the high
frequencies as in the low frequencies
and we can see that for this example the
difference between the male frequency
spectrograms is going to be on the order
of epsilon and what's interesting is
that this representation has been
motivated mainly from psycho acoustic
and biological directions by saying that
this is something that we see generally
in the in the cochlea but it turns out
that there's also a mathematical
justification in terms of time warping
stability why we want to have this type
of this type of averaging and so as a
summary whenever we have a signal and we
want to create a representation of it if
we use a spectrogram we're going to have
problems we're going to have invariance
to time shifting but we're going to have
instability to time warping but a
constant Q average will stabilize this
problem and so in order to look at this
a little deeper the problem with the
mill frequency spectrogram is that it's
not very useful to characterize larger
scale structures by itself and to see
why this is we're going to slightly
reformulate the definition the way we
calculate ml frequency spectrogram is we
take a spectrogram and we average it in
frequency like we saw just before
another way to get something that's
that's very similar it's not exactly the
same but contains a similar type of
information is to take these filters
that we use to average and instead
convolve the original signal with them
which gives us this time frequency
representation that is very high
frequency that is very high resolution
in high frequency and much more regular
in low frequency and then we average
this in time to get a regular time
frequent
sigrid like in the case of mal frequency
spectrograph there is a way to show this
mathematically that we have equivalence
between the averaged filter filter
response amplitude and the male
frequency spectrogram but they're not
going to go into that right now and a
way to view this is to say that we have
a wavelet transform now wavelet
transform is sometimes thought of is
this sort of dyadic decomposition where
you have one octave that's taken up by
the wavelet coefficients and then you
decompose and take the lower low pass
filter and then take the high pass
octave of that and so on but there's
actually a lot more flexibility that we
can put into it for example we can have
a much more narrow response and
frequency which gives us this constant Q
wavelet filter Bank and so it's going to
be constant Q and high frequencies and
linear in the low frequencies but it
turns out this is not necessarily that
big of a problem and we're still going
to call them wavelets even though
they're not exactly wavelets but they
have a lot of the nice properties that
we need from them and what's also
interesting is this is going to give us
basically a male frequency scale as well
just by by having this this type of
constant Q filter Bank so using this
week we can define a wavelet transform
which is a low-pass filter which
captures the low frequencies that's not
in the in the bandpass filters together
with the the bandpass filter system and
if we make sure that we cover the whole
frequency axis we get an invertible
transform and so we can now say that the
male frequency spectrogram can be
thought of as a time-averaged wavelet
coefficient amplitude now if we look at
the wavelet the wavelet decomposition
the modulus of these we see that when we
average them in time we lose a lot of
information in terms of temporal
structure and this is one of the reasons
that male frequency spectrograms are not
very good if we want to characterize
completely a large-scale structure
because if we increase the averaging the
averaging scale we're going to get
something that's more invariant but
we're not going to get something that
that captures the temporal dynamics of
the signal and so we have a loss of
temporal structure we need to capture
this fine structure and the question is
is how a one approach that's been that's
been used as modulation spectrograms but
the problem is if we take a modulation
spectrogram which is going to consist of
taking a spectrogram of each of these
separate channels we're going to run
into problems again because we're going
to have the instability time warping and
so we need to look at some at a
different method for capturing this high
frequency information yeah very clear
the pictures on the right is normal no
the left is a series of serve actual
bandpass filters yeah executive rust and
everything system is exactly than their
average to get a Mel frequency
spectrogram it's not technically the
same and so just to digress a little bit
the the information is contained if we
reduced the window size we do retain
more information in the mail frequency
spectrogram in the sense that we do have
the sequence of vectors but the problem
is we're then going to need a classifier
a model on the back end to learn which
specific sequences correspond to a
certain class and so on and this is what
the hmm does and what we want is we want
to relieve the model of the
responsibility of learning the temporal
dynamics and so we want a representation
that covers a large a large duration of
time but that captures as much
information as possible and we see that
if we try to do that with the mel
frequency spectrogram we're going to run
into trouble because we're just going to
lose more and more information
Metro morning yes to indicate that that
kind of just for me it's not going to
destroy the information but it's going
to capture more information that we
would like it's going to it's going to
capture a lot of details it's not
necessarily important and this is kind
of the idea of stability to time warping
because if we're if we're unstable with
respect to time warping that means that
we sense changes in the signal that's
not necessarily that important that
corresponds to these high frequencies
that can move very quickly even though
there's not a big change going on and so
the modulation spectrogram because it's
it takes a spectrogram of these signals
and the spectrogram has this instability
property means that we're going to run
into problems now what you can do is you
can take the modulation spectrogram you
can average it along modulation
frequency using a constant Q filter bank
and then you get some people have been
doing this so it's like a constant Q
modulation spectrogram and then you do
have a stable stable representation and
that's going to be similar to to what
we're going to end up doing actually yes
that's Leslie yeah I mean he's he's done
both I think I think he calls it
modulation scale what he does the
constant Q averaging yeah it's it that's
better yeah that's much better because
then you you lose the sensitivity that
you don't really want and so that's what
similar to what we're going to end up
doing and so the issue is that when we
take when we take this wavelet modulus
and we average it we lose information so
we're going to see how we can how we can
recover that information and so just as
a small sidebar these filters that were
considering their analytic filters so
they have no negative frequencies and
this means that when we take the modulus
we're actually going to get a nice
somewhat nice envelope estimation of the
signal because it's going to correspond
to taking a real wavelet coefficient and
then computing a Hilbert transform of it
so when we take the modulus we're going
to get the whole grid envelope of each
sub-band and then what we do to get
what's equivalent to Sarge you get a
question no all right what we do to get
our mail frequency spectrogram is we
average this in time and so we basically
just recover the low frequencies of this
of this envelope now the rest of the
information is then going to be
contained in the the high frequencies
and we see that sibs the wavelet
transform is invertible we can think of
this low-pass part as just the the the
Lopez part of the of a wavelet transform
and so we know that the rest of the
information is contained in the high in
the high frequency coefficients of a
second wavelet transform them that we
apply to the the envelope the problem is
that these coefficients are not don't
have this invariance and stability
conditions that we want and so we take
the modulus again and we average and so
we get this these are called and
second-order scattering coefficients
these we call first sort of scattering
coefficients and second order scattering
coefficients and so we have as many
first order coefficients as we have
first order frequencies and these
correspondent to acoustic frequencies
corresponds to a certain sub band that
we're looking at and then we have this
second decomposition which is then
corresponds to more something like
modulation frequency we look at each
sub-band and we decompose it and we look
at the different types of oscillations
present in there another way to
formulate this is to look at a very
similar transform which is a nonlinear
transform which is the wavelet modulus
transform we take the wavelet transform
we apply the modulus to the wavelet
coefficients and this gives us a basic
building block for block for
constructing these types of coefficients
and we essentially get a cascade that we
apply to original signal X and out of
that we get an average which is just the
average of the signal which an audio
we're not going to care about most of
the time it's going to be zero and then
we get these first-order wavelet modulus
coefficients is what we call it is then
we reapply the operator to these
coefficients and we get what's called
the first-order scattering coefficient
and then second order wavelet modulus
coefficients then we reapply gets second
order scattering coefficients and so on
so technically this can go on to add
affinity but we're going to see that
this is not going to be necessary
thankfully so the structure of this is
basically a convolutional network we
have convolutions followed by
nonlinearities followed by convolutions
one of the differences is that here we
capture information at each layer as
opposed to just at the end and also the
filters aren't learned instead they're
fixed to be wavelets and this is not
this is not to replace any sort of deep
neural network methodology rather it's
it's a complementary approach the reason
that we don't learn here is because we
know that we need the invariance
condition we need the stability
condition and then once we have a
representation that satisfies this the
idea is that we can then plug it into a
deep neural network that will then have
less that will then have less problems
learning these invariants and stability
properties because we've already encoded
them in the representation it's the
modulus simple modules so it's that's
all it does so on but its necessary
condition do you get a variance if you I
mean if we remove the if we move the
modulus our invariants are going to be
all zero because we're going to have all
sorts of linear operators and the only
the only linear the only linear a
variant you can get from a signal is its
average and so we will get zero so the
non-linearity allows us to create more
and more invariants from from our signal
linearity not eand is desirable oh do
you
maybe some other kind of love you yeah I
mean you could we haven't done some we
haven't done experience in audio we have
done a little bit of experiments in
audio and it turns out that the choice
of non-linearity is not all that
important like you could you could
replace it with like a local Max or the
they've been doing other stuff in images
and it turns out that it's not that
important the difference that the
modulus is nice from a mud mathematical
perspective because it turns out that if
you want this stability condition and if
you want your your transform to be
contractive it turns out that you're
restricted to have a point wise a point
wise unitary operator and so it's going
to be the the modulus may be multiplied
by some complex numbers so there's ways
of showing that but in experimentally it
doesn't seem to matter all that much us
how can you make sure you still reserve
information for discrimination yeah
that's I'm going to get to that because
it's not entirely obvious that you're
discriminated from this and so the idea
is that whenever we lose information
from averaging we recover some of it
with the high frequencies of the wavelet
decomposition but then we lose
information again and so we recover it
again so the idea is that whenever we
lose information we recover some of the
lost information and so that's how we
remain discriminative but it's my
understanding what is the meaning of zen
master
cool always fine yeah yeah so that's an
averaging in time he's just averaging
yeah all of these convolutions are in
time so here we capture just the average
of the signal here we take the signal
weak take a certain band pass filter we
take the modulus if we compute the
envelope and then we average it in time
to get something that's invariant and
stable and then since we've lost
information here in the average we
recover it in the high frequencies here
in the second wavelet decomposition take
the modulus an average once again and so
it's to get these invariants that we
need to average because otherwise we're
very sensitive to shifting and warping
we're just rummaging in cruise the
filter back no mail yeah yeah these are
both these are the first one is a male
scale filter Bank sort of it has about
eight wavelets proactive it turns out
that we don't need to use the same
wavelets in the second decomposition is
the first decomposition so here it's
more like a traditional wavelet filter
Bank you have one wave liquor octave or
two is what we found works works best
but these are the basic parameters of
the of the representation how many
wavelets proactive and how large you
want to make the the averaging yeah are
you going to our league with the
delipcious the dilation that comes later
yeah a little bit yeah yeah I think it's
the next time yeah hold on yes
so with the exception of the modulus
this looks like a diamond target filter
Bank it's not like completely idiotic
because we have what depends on what you
mean again I mean if you were to choose
awaited such that and under direct
scaling the skating functionaries
orthogonal to its previous skating yeah
and you don't you no longer have that
orthogonality just does that matter in
this place from look quite sure actually
talking
yeah no I mean it's the these wavelets
are not necessarily the same wavelets
that you would have in like an authority
orthogonal wavelet decomposition in fact
these wavelets are very redundant and we
would like them to be if we had wavelets
that were earth organelle we would run
into problems it turns out and so these
are actually very redundant wavelets
they they overlap significantly in
frequency and they have they have a
nonzero scalar product and so it's it's
not necessarily the same framework that
would you would have in a dyadic
orthogonal orthogonal wavelet
decomposition so it's it's me an icy
mountain mountain doesn't see and hear
you're trying some things that you want
to be more redundant because if you're
more redundant you're more flexible to
these types of changes like a time
warping won't suddenly shift one
coefficient into another bin and then
you're completely lost and so you want
you want to have a certain type of
redundancy it also makes sure that when
you take the modulus you don't lose
quite as much information so it has all
sorts of nice properties the fact that
we are redundant so these are wavelets
but they're wavelets in a very sort of
general sense they're wavelets in the
sense that they're dilations of a mother
wavelet period and they're barely even
that in this case and so the important
thing is that their constant Q that's
that's what we care about so it's it's
less related to like orthogonal
decompositions and so on so we can
continue this cascade and then get for a
general for a given em we get an M
thought of scattering coefficient we can
put all these together into one big
vector and we get what's called the
scattering transformer scattering
representation we define a norm on that
and with this norm we get this theorem
which proves the Lipschitz stability
condition that we were talking about
earlier and the reason that this holds
through is basically if you have a
wavelet and you apply it to a deformed
signal you can do a change of variables
so that it's equivalent to the original
signal on a deformed wavelet
and the this is not going to be
drastically different from the original
signal because a deformed wavelet and
the original wavelet are going to be
relatively close in terms of Euclidean
norm this will not be true in a Fourier
decomposition for example a sinusoid the
deformed sinusoid are going to diverge
significantly at a certain point that's
kind of the heart of the argument but
it's it's like a proof that runs for 10
pages so don't ask me to go into detail
but that's that's the basis of why why
it works so we have this nice stability
condition for this this representation
we have we also have lots of other nice
properties like conservation of energy
if if we have a wavelet transform that
that conserves energy we also have as a
consequence of this that the amount of
energy in each order decays and so it
turns out for most applications we can
just look at first and second order
coefficients we also have that when we
demodulate we get something that's
essentially low frequency and so we
don't need to keep the same sampling as
we had in the original signal so we get
like this graph I was showing you
earlier and the high frequencies we're
very irregular in the low frequencies
we're very regular so it lets a save in
computation in space it also means that
once we decompose this using the
second-order wavelet decomposition we
don't need to take here we need to take
a lot of high frequencies but here we
can get away with with just looking at
the low frequencies and we see this for
example looking at the second-order
decomposition here where we plotted the
first-order frequency the acoustic
frequency and then the second order
frequency the modulation frequency we
see that a lot of this is going to be is
going to be 0 and so we could neglect to
compute them and the result is we get an
algorithm that runs in and about n log n
I had to calculate these coefficients
now the question is what sort of
information do we have in the scattering
transform we've seen that we are
discriminative in the sense that we
capture a lot of the high frequencies a
lot of the the temporal structure that
we've lost but what exactly does this
constitute and so
we're going to look in the case of audio
of a simple model which consists of an
excitation filtered by an impulse
response and then modulated in in time
by an amplitude a and the question then
becomes what sort of information can we
can we see is recovered by the first and
second order coefficients in this type
of model which is a relatively
constrained model but it still allows us
to look at some simple phenomena so for
example here we have three sounds with
the same excitation harmonic excitation
with different different amplitude
envelopes first is a smooth envelope
then we have a sharp envelope with a
with a sharp attack and then we have a
sinusoidal amplitude modulation of
tremolo we can listen to them so we hear
they they sound quite different and we
see here in the scalar graham in the in
the wavelet in the wavelet modules
decomposition that they look quite quite
different but when we calculate the
scattering coefficients what we're going
to do is for each of these channels
we're going to isolate them and we're
going to average them in time and so
we're going to lose the distinct
characteristics of each of these
envelopes and they're going to turn out
to look more or less the same if the
window size is large enough if we if we
need a certain amount of invariance
we're not going to be able to capture
this information directly and this is
essentially what we would see in the
mail frequency spectrogram with this
window size however if we capture then
the second order coefficients which is
going to be these this signal but
decompose using a second wavelet
decomposition then the modulus and then
average in time we do capture the
differences in the envelope in the sense
that here we have mainly low frequencies
and that's represented here in the in
the display in the second order
coefficients here we have transients so
we have high frequencies and here we
have a sinusoidal modulation so that's
represented as a maximum around that
frequency
so we see that we capture this these
different properties of the envelope in
the second order coefficients whereas
the first order will mainly give us
information about the type of excitation
and the filter the spectral envelope the
filter H we can replace this by a
stochastic excitation the white noise
excitation for e and we're going to to
end up with similar results and we can
hear that these are also relatively
different sounding signals but yet the
first order does not necessarily see any
difference between these three signals
whereas in the second order we're able
to differentiate between the three the
difference between this and the harmonic
case is that here we do have a low level
noise floor that's due to the stochastic
excitation that also has seen in the
second order so we do have a bit of a
bleed over there but we do capture the
information on the envelope as well use
two orders yeah for right now we've only
looked at two orders and it's in
classification results it's usually
enough to get to get good results but
I'm going to get to that in a little bit
see
on the bottom figure in the previous lie
and what kind of value was it evaluated
for lambda 1 it was full that under way
yes that's that's important so these are
I fixed one lambda 1 for this for this
particular display and so it's at this
frequency at 2,400 hurts but in reality
you have a whole sequence of these
displays but you would need some sort of
3d representation that it's a bit
difficult to to do but yeah so this is
for one fixed frequency channel that
I've showed this for but you're going to
see something similar in all of them
because the envelope is the same at all
frequencies in this in this model so
you're going to see that so how many
dimensions we have soda not number one
it depends I mean in the tests that
we've looked at it could be around 40 50
up to 70 and in the second order it
could be in in 100 200 yeah this is a
calculated this for a very dense
sampling so this is not representative
for each values Union first of all do
you have mommy oh no no I mean for each
for each time frame for each point in
time I have about 50 50 first order
coefficients and 200 second order
coefficients just about it depends and
the parameters but in on that order yeah
so we can also look at frequency
modulation as you heard just now we have
sinusoidal demodulated pitch and f
constant pitch and an amplitude
sinusoidal amplitude modulation and what
we have is this is also represented in
the second order in terms of this
harmonic structure here which is
different from the amplitude modulation
in when we just have a sinusoid the
problem is we can also create an
amplitude modulation here which sounds
different from the frequency modulation
but gives us very similar second ordered
coefficients and so this shows us that
there is a certain drawback to the to
this type of scattering representation
in that it doesn't really show that the
difference between am and FM but it will
detect the presence of either one those
would be the same in every lambda 1 no
they would not exactly so this is this
is for this fixed one it would be the
same but it's not possible to make them
exactly the same for most of them but
they're difficult enough to tell apart
that I think it will cause a problem in
in modeling them you don't have a very
clear distinction between the two but
yeah this is just for a fixed lambda 1
and finally the final example we're
going to look at is looking at frequency
components interfering so if we have two
notes that are played together as
opposed to played separately in the
first case they were interfere and
you'll have a beating phenomenon in the
envelope that's going to give us a
maximum in the in the second order
coefficients so we can listen to and so
in a sense the dissonance that you here
between the two notes is is captured
here in the second order coefficients a
certain type of roughness and another
way to think about this is to say that
since we have a certain bandwidth of the
first order frequency filter bank we
lose information about absolute position
but the second order is going to give us
relative position information about
different frequency components in each
frequency band so to summarize in terms
of in terms of this model what we can
see is that first order coefficients
tend to capture a very short time very
short time structure like looking at
excitation looking at the filter which
is going to be in these cases around up
to 10 milliseconds of duration whereas
this the second order coefficients is
going to characterize larger scale
structures like amplitude modulation
frequency modulation and interference so
we have this kind of separation of
scales between the first and the second
order coefficients in what they
characterize another way to understand
what type of information is captured in
this representation is to look at
reconstruction from scattering
coefficients and what we have is we have
a result that says that for certain
wavelets we can in fact inverts the
wavelet modulus operator and since the
scattering transform is a cascade of
this operator we can cascade this to get
an inverse of the of the scattering
transform the problem is to calculate
this inverse is a non convex
optimization problem it can be relaxed
somewhat but it's still intractable for
the types of audio signals that we're
looking at so what we're doing is
actually we're using an approximate
of a heuristic approach to to to recover
this which is a basic alternating
projection algorithm like Griffin and
Lynn sort of approach and so to get the
original signal we can cascade this
inverse wavelet modulus operator the
problem being at the end we won't have
the necessary with modulus coefficients
and so we have to use a deconvolution to
get our first order estimates so there's
errors involved in this process
obviously and since this is approximated
we get errors and so this is not a
perfect reconstruction I would not
recommend using this and some sort of
fancy audio codec or anything but what's
nice is it gives us an idea of what type
of information is captured in the first
as opposed to the second order and so we
can see this for a speech signal where
we have the original signal vital
questions would be quickly we have the
reconstruction from just the first order
where you see that certain parts have
been smoothed out you lose you lose a
lot of transients that we have here for
example but if we add the second-order
coefficients to the first-order
coefficients and reconstruct from that a
lot of questions will be quickly we do
have a large amount of artifacts but we
hear that the the attacks and the
transients have been restored to a
certain extent in the wreck instruction
from the second order so this kind of
confirms what we saw with the models in
that we we do capture that type of
temporal dynamics in the second order so
now you see you mean if you have an
equal infinity then you can recover the
original signal oh yes if you had a good
yes alias you answer yet equally feeling
yeah but but in reality no because what
happens is each time you take the
modulus you're pushing your energy
you're pushing your your information to
the low frequencies and so at some point
most of your information will be
captured by the low pass filter and so
you won't have to continue to decompose
anymore so at that point your
deconvolution is going to be trivial
you're going to be able to recover your
your
on your way with modulus coefficients
fine because you're averaging won't be
what would have done anything and so
then you can theoretically recover the
problem is we're using another very good
algorithm here and so once we cascade it
far enough we're going to get all sorts
of errors that will accumulate and so
actually adding the third order to this
is not going to improve but in in if we
had a good algorithm to invert then yes
it should yes you have any prove that as
the term gets higher higher the errands
where you getting smaller my phone is in
theory yes I don't it should be true yes
but I don't think I don't think we've
proved or looked into the details of it
but intuitively it should yes but yeah
so we don't only have we have lots of
types of variabilities and audio signals
we're only going to touch upon a few of
them here and so except for a Time Shift
invariance we also might want to have
frequency transposition invariance and
so here for example we have two words
pronounced by to be different speakers
and we have a difference in pitch but
also different in informant frequencies
and so we can listen to them
encyclopedias encyclopedias and so we
hear that they're the same word but we
don't we don't have the same pitch
information then we see here in the
Foreman's we don't have the same format
information and what's especially
interesting is we don't have a pure
scaling of the frequencies we don't have
a shift in pitch or a shift informants
that's that's constant over the whole
spectrum instead we have a different
shift at different parts of it so the
distances between the foreman peaks is
going to vary and so this corresponds
more to a frequency warping then if your
frequency transposition and so this
tells us that we might not just want to
be invariant d frequency transposition
but also stable to frequency warping in
the same way that we saw with
translations in time
because if you want that a little bit
different you might change from one
start another son so how do you build
that kind of accumulation back to over
here so the idea is rather than they
don't compute sorry the idea is that we
don't were not completely invariant to
these types of frequency walking's we're
stable to them which means that a small
frequency warping is not going to change
your representation very much so a small
change in the formant frequencies is not
going to change it very much a large
change will again is going to change it
more so your representation will be able
to hopefully separate between the
important changes in formant frequencies
and the less important ones but if you
have if you guys probably know this
better than I do but if you have a
smaller format figures that shift
slightly if that's going to change your
your the ID of your vowel then you're
going to run into trouble with with this
type of thing yes right you're walking
is enjoyed that these special cases when
provision will curve the answer is not
explicitly but it turns what's what's
nice with this continuity condition is
it tells us that we have a continuity
with respect to these these time warping
and what that means is that a warping is
going to be realized locally as a linear
transformation in the representation
space and so what's nice then is if we
have a linear discriminative classifiers
and it's going to be able to project
against certain directions and then that
way create a real invariance to certain
time warping while maybe increasing
sensitivity to other time warping so
since we linearize time warping zor
frequency warping xin this case this
classifier should be able to decide
which one's is important given enough
training data of course but that's kind
of a nice property that comes out of it
but since we don't have if we had pure
invariance than yeah that would become a
problem because then you might run into
these types of issues maybe remote
question
although I can California that's right
have you have our name is actually in
burien Malcolm's lady who you may know
is asking the question and i quote i
believe the hilbert transform only makes
sense as an envelope follower if there
is only a single component in the sub
minute a single frequency firm so if
does the theories that make sense from
the hell brie I guess questions is it
steerage go make sense when the number
transform doesn't actually give you the
truth I mean because the the idea here
is that it's nice to be able to
formalize it as looking at the envelope
of a specific frequency component but
it's not necessarily crucial to have it
to have it be specifically an envelope
and we saw for example in the case where
we do have two frequency components in
the same filter bank in the same filter
response what we do see is we don't get
the envelopes of each one separately
instead we get the beating phenomenon
between them which in our case is nice
because it gives us information about
the internal frequency structure in the
filter bank which we wouldn't
necessarily have if we just looked at
the individual envelopes present in
there so it's not necessarily dependent
on having an exact estimate of the
envelope in this case what's important
is having a non-linearity that brings us
to the low frequencies it just happens
that in some cases we can interpret it
as an envelope when we just have one
frequency component so I hope that
answers your questions okay so yeah so
there are some approaches that have been
done to to get this type of
transposition invariance one first first
perspective is to say that transposition
frequency which is a scaling of
frequency is going to be a translation
in log frequency and so log frequency
and mail frequency in in high
frequencies mail frequency corresponds
to log frequency and so at these
frequencies we can simply average the
mail frequency spectrum and we get
something that has this transposition
invariance and this is basically what
the mail frequency kept strim does in
taking the lo q Francie
officiants in the DCT which amounts to
an averaging along the mail frequency
scale the problem is once we start
averaging along frequency we lose
information and so we can we can gain
some invariance by doing this but if we
try to gain more and more invariants to
capture larger and larger scales we're
going to lose more and more information
so we're kind of limited in that sense
another approach is because apply
familiar with is vocal tract length
normalization where you try to realign
the spectrum of at a given point with
that of a reference speaker using a sort
of scaling coefficient an advantage of
this is that we don't lose information
because we're just simply rescaling the
whole spectrum but it's sometimes
problematic to estimate the scaling
coefficient and if we don't estimate it
exactly we don't have the desired and
variance property another problem is
that we don't have the stability to
frequency warping unless we start
considering nonlinear warping of the of
the spectrum but that then you run into
problems so if you have a large search
space and so on and so these approaches
have their advantages but in this case
we lose information and in this case we
have a certain instability so the
approach we're going to take is that
instead of averaging a log-log frequency
we're going to take the scattering
transform along the log frequency scale
we fix up at a time point t and we go
along log frequency and we compute
another scattering transform and so on
this gives us the separable time and
frequency scattering transform or just a
separable scattering transform and it's
nice in the fact that it has the
invariance that we want to time shifting
and frequency transposition and that of
course depends on the scale that we
consider here with the with the
scattering the transform we could decide
to have a small scale or a large scale
but it captures more information that if
we've just done a simple averaging and
in experiments we usually can restrict
ourselves to the the first order
coefficient so it's usually enough for
the skills that we consider
the problem is that this approach and
actually the approach of the original
scattering transform loses a lot of
important information because we average
we decompose each frequency band
separately and we average each frequency
band separately and so for example these
two signals if we look at the regular
scattering transform with the window
size that covers the whole signal it's
going to think that they're that they're
going to give the same exact
representation because we have this this
same frequency band here as here but
shifted in time and so since we don't
capture any information on the
correlation between frequency bands is
not going to be able to tell the
difference so we lose this joint
time-frequency structure and so a way to
solve this is instead of decomposing
each frequency band separately we would
like to do as in and she up shamos
cortical representations where we
decompose this time frequency scalar
gram in two dimensions in time and
frequency simultaneously instead of
doing in this as in the separable case
where we decompose just once along the
temporal domain and then once a look and
then average and then once along the
frequency domain and so here we get then
a two-dimensional wavelet that we use to
decompose the scalar gram and we can
then form like we did before a wavelet
modulus transform and cascade this onto
the scale a gram and this gives us
what's called the joint time frequency
scattering transform with a joint
scattering transform and so this has the
advantage of having the same invariance
and stability as the separable
scattering transform but it captures
more information in that it's able to
distinguish between these cases where
we've we've shifted individual frequency
components and so it captures
correlations between frequency bands and
we could also create a representation
that is sensitive to a frequency
transposition by emitting the averaging
along frequency in just averaging a long
time and this gives us something that
has the same invariance
as the regular temporal scattering
transform but that captures more
information so finally we're going to
look at some classification results to
try to validate some of these claims and
the first is to look yeah so some quest
the net
difference kids are with this time
frequency scanner / trance / and using
the victim or effective it is we were
using Kapoor filters yes it's a good
poker bar filter bank you I mean this
has been done i think by it was the
second is it justified so here the first
order is going to be X here is the I
didn't I had it in the previous slide X
is the scalar gram the wavelet modulus
coefficients the amplitudes of the
whaler coefficients from the first order
so the first order stays the first order
wavelet decomposition stays the same and
then on that this the second order
decomposition which which includes this
low pass filter and then this this
wavelet decomposition with the modulus
and another low pass filter forms the
second order decomposition and so but
it's it's going to affect the first
order for first order as well but the
decomposition is regular wavelet
transform then two-dimensional wavelet
transform and then you reapply the
two-dimensional one and so on to get the
higher order but we're just going to
concern herself of the second order for
now so does that answer your question
there all right we can come back to ya
so so to try this out we look at the
timid task of phone segment
classification and excuse me what we do
is we try to classify a segment where
we're given the beginning and the end so
it's technically cheating but it makes
for an easy easy problem to code and to
test so that's that's what we did well
it does give us interesting results on
the different types of representations
and so what we do is we take the
scattering transform over about 160
milliseconds and we compute it using 32
millisecond frames and we put this all
into one big vector that we then put
into an SVM and we train a classifier
with that and then with the goal is then
to try to classify the phone that's in
the center
this segment and the results that we
have first is that we see that we
perform similarly to the Delta n FCC's
with the first-order coefficients and so
here l equals 1 means that we have first
order l equals 2 means we have first and
second order and then first second and
third order them for l equals 3 and so
the first order coefficients perform
similar to the MF ccs which is what we
expect since they carry similar type of
information in the second order we do
see a certain improvement of almost two
percent absolute which shows us that the
temporal dynamics that we capture within
each of these frames is actually useful
for for this type of classification the
third order we actually do worse and the
reason for this is that when we look at
small time scales as i said earlier when
we apply the modulus we're going to
force energy towards the lower
frequencies and that means that once we
get to the third order we're already
very regular we're already very low
frequency and so it's not going to be
very important to capture the high
frequencies in the wavelets and so in
the third order we're going to have a
lot of noise we're not going to have
very much information and so it's not
going to help us classify in this case
yeah yeah in the same way as so we take
delta msecs over 32 milliseconds and
then we concatenate them over this whole
big segment and then we feed them
exactly as in the in the scattering case
to make it comparable you know doing
such a we just said won't be so low so
that we also include the log duration of
the second all that night that night
that that makes about 2% absolute
difference so that might be why you stay
here the state of the art is I think Jim
glass so it's like a committee based
classifier with different segment
lengths and stuff and so they get 16.7
percent
Delta FCC's in first order it's around
50 I think second orders like I said
around again around 200 third order I
think it's like maybe 500 but I'm not
sure about the third order I know it's
about 200 for the second order what
features yeah yeah yeah but but I mean
we're also using an SVM so it's not
necessarily the end of the world to have
if we're using GM's when we started this
I was using GM's because I figured
that's what everybody does and then they
found out the results for not very good
but yeah so it's if you're going to have
trouble if you try to choose some sort
of generative model like that but the
SVM does handle it pretty nicely
features yeah I mean I did some
experiments where I did like huge
vectors of 10,000 you know it was able
to do it yes this is whatever stuff or
not so if we look at if you want
transposition invariants what we can do
which is quite nice is we can skip the
frequency averaging and since we're
using an SVM the SVM is going to
optimize a locally linear a
discriminative plane in each at each
point and so if we have enough data it
should be able to learn how much it
wants to average in for each class pair
and so that makes it so that we don't
have to decide how much transposition
invariance to enforce on our
representation but instead we can depend
on the SVM to learn it if we have enough
data and so we tried fixing the the
averaging scale and then not averaging
in time and not averaging in frequency
and letting the SVM try to learn it and
it turned out that that was much better
so that's what these results show and so
to compare for the for the second-order
scattering coefficient we have seventeen
percent if we add a scattering transform
on top of that along frequency we get
sixteen point five percent it's almost
at one percent absolute improvement and
then for the joint case where we then
capture more time frequency structure
instead of just temporal structure we do
improve somewhat
again over the disciple case so we also
looked at musical genre classification
because that's something that everybody
does so it's nice to be able to compare
and so this basically consists of having
30 seconds of music trying to classify
into one of ten John rus' and then what
you what we do is we do for each seven
hundred milliseconds we classify that
and then we vote over the whole track to
to try to get a result and so again we
get results similar for the first order
and the Delta msecs once we add the
second order we get a big improvement of
about forty percent relative and the
difference between this and the previous
cases here we're looking at much larger
time scales so the difference between
what the first order captures and what
the second order captures is much
greater we have a lot more temporal
dynamics that we need to recover in the
second order and so we do have a
significant improvement here for the
second order and so the third order does
a little bit better but we're still that
in the regime where we expect the third
order to carry a lot a lot of energy and
a lot of information yes the last year
is from Josh
state-of-the-art I can't remember it's a
paper from 2009 where they use
modulation spectra and then they do all
sorts of like lDA's and they some along
certain dimensions and stuff and stuff
and they get like a 9.4 error rate I
can't remember I can send you the paper
here and so this is just using an SVM
just like kind of feeding it in there
one of one of our colleagues who is at
Princeton previously who is now at with
what's the fans group managed to get
eight point eight percent using a sparse
representation classifier on this using
the second-order coefficient so by being
a little bit more clever you can you can
squeeze some more out of it and for the
transposition invariant case we see that
we have a certain improvement with the
separable scattering transform in the
case of the joint scattering transform
we actually don't do better we do we do
about the same and this can be
attributed to the fact that we don't
hear the issue is not necessarily that
we want to be able to characterize very
fine join time for using the structure
here the issue is that we have data
points that are very very far apart and
we want to be able to create good and
variance between them and so adding more
specific information about the joint
time frequency is not the sir going to
help us one what we want is more
invariance not more discriminability in
this case and yeah is it what is there
is with the table
then you expect where the third order
but the third order you would expect the
third order to play a larger role when
you're looking at larger time scales and
so here we're looking at less than one
second and you can see by looking at how
much energy is contain a third order
that is not very important once you
start looking at larger time scales and
that you expect it to do better the
reason that we didn't look at larger
time scales in this case was because we
didn't we didn't even if we added the
third order going beyond seven hundred
milliseconds resulted in integrated
performance and I think that has more to
do with the nature of the problem once
we go beyond that we start if we have
something that's about 500 milliseconds
we have a greater chance of getting a
segment where we have maybe one
instrument and so on and so it's easier
to characterize as opposed to if you get
a larger segment where you have to
characterize mixtures of instruments and
so on and so it becomes something that's
that's more complicated for the
classifier to deal with and so if we had
a problem where we had a very large
scale structure like on the order of
maybe six seconds they wanted to capture
then yeah the third order would would be
expected to do better but we haven't we
haven't seen any task where it actually
does a lot better yet that's that's kind
of the thought but yes so in conclusion
just looking at invariance and the
stability conditions for representations
is is a very powerful framework for
trying to analyze different properties
that they have with their classification
performance and the scattering transform
provides such a representation that has
these nice properties and it turns out
that it actually captures important
audio phenomena like amplitude
modulation frequency modulation and so
on we can extend the scattering
transform to get the joint scattering
transform which captures join time
frequency structure and also gives us
transposition invariance in a
discriminative way and then finally we
see that once we test this in numerical
experiments we do get state-of-the-art
results in the two tasks that we looked
at and I should add that
off of this we've also looked at
classification of medical signals in
terms of heart rate variability for for
fetuses and we've also had some nice
results in that area but they're not in
this presentation unfortunately so
that's it thank you very much for
listening Lizzie speech on league speech
no that's kind of one of the next things
to look at to see how we how we perform
in terms of Tucker separation and in
terms of presence of noise and so on
it's not something I mean timid is very
very very nice it's very clean and
there's there's no noise so that's not
something that we've looked at but it's
something that that would be interesting
to look at in the future but also
looking at other types of distortion
like clipping or masking and so on and
see how robust it is to that type of
change and how we can make it more
robust to that yeah this afternoon did
not get in there i would like to let me
know
alright thanks again thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>