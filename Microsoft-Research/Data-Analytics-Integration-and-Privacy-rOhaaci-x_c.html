<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Analytics: Integration and Privacy | Coder Coacher - Coaching Coders</title><meta content="Data Analytics: Integration and Privacy - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Analytics: Integration and Privacy</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rOhaaci-x_c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
hello it's my pleasure to welcome Steven
back from Stanford University his thesis
is on entity resolution data analytics
more broadly and he's going to be
talking to us about it right thanks
prograf for the introduction so I'm
Steven Huang aim and I'm a PhD student
at Stanford University today I'll be
talking about my thesis work called data
analytics integration and privacy so
nowadays the amount of data in the world
is exploding and companies are just
scrambling to make sense out of all this
information now there are lots of recent
articles talking about analyzing large
amounts of data and here is one cover
article from The Economist cover page
you can see that there are lots of zeros
and ones raining from the sky and
there's this gentleman in the middle
trying to gather some of the rain and
he's watering this flower which
signifies useful information that's
being extracted from all this
information so within data analytics
I've been working on two problems data
integration and data privacy now even
before you start analyzing data it's
very important to combine data from
different sources and so data
integration plays an extremely important
role in our data analytics for example a
few years ago we had this devastating
earthquake in Haiti where a lot of
people died and people around the world
came as volunteers to help out the
Haitians the following figure shows
different types of data that was
integrated and analyzed in order to help
out our Haitian friends so first there
were SMS broadcasts and then people were
texting each other and then the local
media was generating a lot of data about
what was happening after the earthquake
and finally there weren't any official
maps in Haiti for navigation so people
geotag their own location and
crowd-sourced the maps and these were
used for volunteers to drive around
Haiti so you can see that data
integration played a very important role
in helping out the Haitians now MSR has
been a pioneer in data integration so I
really don't need to and motivate this
problem for this audience so the second
topic I worked on is data privacy
so the following image is from a recent
wall street journal article and it's
talk is depicted depicting how insurance
companies nowadays are sifting through
the web and collecting personal
information and analyzing it in order to
predict the life spans of their
customers so let's say that we have
Sarah on top in green color and let's
say that she has a lot of bad health
indicators for example actually has she
has good health indicators for instance
she only has a mile one mile of commute
distance and let's say she does a lot of
exercise and then she reads a lot of
travel magazines and she does not watch
much television on the other hand let's
say that we have Beth on the bottom in
red color and this time let's say she
has a lot of bad health indicators so
for example here she has a long commute
distance of 45 miles she buys a lot of
fast food and she has bad foreclosure
activities and she watches a lot of TV
all this type of information can be
found from our social networks blogs and
home pages and in this example the
insurance company may conclude that Beth
is likely to be is probably less
healthier than Sarah and therefore is
more likely to have a shorter life span
than Sarah now although this kind of
analytics may be very useful for the
insurance companies it's very disturbing
for the customer side and there's
actually a study that shows that there's
a high correlation between the analysis
of this data with the actual medical
tests that the insurance companies
perform on their patients so this is a
so I've been studying the problem of
data analytics in the data privacy point
a point of view as well and there is an
interesting connection between data
integration and data privacy so the
better you are integrating data the more
likely someone's information is leaked
to the public so you have worse privacy
and vice versa the better privacy
measures your take the harder it is to
integrate up information so it's no
coincidence that I've been studying
these two problems so the following
slide summarizes my PhD work first I
written I've written a number of papers
on a topic called entity
ocean which is a data integration
problem second I've recently written a
number of papers on data privacy and
finally I've published a paper called
indexing boolean expressions which was
done while I was interning at Yahoo
research and the techniques here were
used in the display advertisement
prototype of the Yahoo web page so for
this talk I'm going to follow focus on
the following works during part 1 I will
elaborate on work called evolving rules
and during part 2 which is going to be
shorter than part 1 I will mainly talk
about work called managing information
leakage and briefly talk about a new
newer work called disinformation
techniques for entity resolution so
entity resolution is a problem where you
want to identify which records in the
database refer to the same real world
entity for example you might have list
of customers where some of these records
refer to the same person and you might
want to remove the duplicates
efficiently now entity resolution can be
a very challenging problem so in general
you're not simply given IDs like Social
Security numbers that you can just
compare instead many records contain
different types of information and other
records may have incorrect information
like typos which makes entity resolution
extremely challenging problem there are
many applications for entity resolution
so in our haiti example you can think of
the scenario where there are lots of
people sick in bed so the hospital's
might have these lists of patients at
the same time other people may be
posting photos of loved ones that are
missing so now you have an entity
resolution problem where you want to
identify which patients in these
hospital lists refer to the same pre
people as those in these are photos of
missing ones ones in addition there are
many other applications for entity
resolution and this makes of NC
resolution extremely important problem
as well so to summarize my research
research focus on I'm not trying to
solve the entire data analytics problem
this is a huge problem that requires a
lot of research to completely solve
instead I'm focusing on a sub problem
called data integration within that I'm
solving an entity resolution problem and
for this presentation I will focus on
scalability so my work on evolving rules
was motivated through my interactions
with the company called spocom so this
is a people search engine that collects
information from various social networks
like facebook and myspace blogs and
wikipedia and the goal is to resolve
hundreds of millions of records to
create one profile per person now
whenever I was going to Spock I was
hearing all these discussions of how to
improve the logic comparison rule for
comparing people records at one point
they were talking about comparing the
names addresses and zip codes of people
but when I made another visit they
realize that the zip code attribute is
not a good attribute for comparing
people so they then decided to compare
the names addresses and phone numbers of
people so every time I made a visit they
were always making these incremental
changes of their comparison rule and
that motivated that made me think think
of how to produce an updated year result
when it up if you if you change your
comparison rule so let's say that
starting from an input set of records I
we use an old a comparison rule to run
enced resolution and produce a result
set of records now if you change your
comparison rule then the naive approach
is to simply start from scratch and
produce a new result using the new
comparison rule now the naive approach
obviously produces a correct result but
it's not the not necessarily the most
efficient result and if you're a company
like spa com that's resolving hundreds
of millions of records then you really
don't want to start from scratch every
time you make a small increment
increment elaine's on your comparison
rule so in our paper we propose a
technique incremental technique that can
basically convert an old ER result into
a new result and we call this process
rule evolution and hopefully rule
evolution is much more efficient than
simply starting from scratch so for
example let's say that we are want to
resolve for records yes please
trying to reduce the right so I'm going
to illustrate your that problem well i'm
going to show our solutions are using
this simple example so bear with me so
for example let's say that we're trying
to resolve r1 through our 4 and each
record contains the attributes name zip
code and phone number now of course in
the real world there may be many more
attributes and many more records to
resolve but I'm only showing you a small
example for illustration purposes now
let's say that we use a simple er
algorithm where we first performed
pairwise comparisons of all the records
and later on cluster the records that
match with each other together so
initially let's say that our comparison
real comparison names and zip codes of
people and if two records have the same
name in zip code the comparison rule
returns true and false otherwise so in
this example the only matching pair of
records is our r1 and r2 because they
have the same name John in the same zip
code five four three two one and there
are no other matching pairs of records
so after clustering the matching records
we get our first er result where r 1 and
r 2 are in the same cluster but are 3
and r 4 are in separate clusters and the
total amount of work we've performed
here is 6 pairwise comparisons because
we were comparing our for workers in a
pairwise fashion now notice that record
r 4 has a null value for zip code so
this may be an indicator that the zip
code attribute is not that not an
appropriate attribute for comparison
because it seems to be very sparse so
let's say that we now change our
comparison rule and compare the names
and phone numbers of people instead of
the names and zip codes so using the
naive approach again we're going to
compare all the records in a pairwise
fashion and in this case the only
matching pair of Records is r2 and r3
because they have the same name John and
the same phone number 9 8 7 so after
clustering the records that match with
each other we get our new year results
where r 2 and r 3 are in the same
cluster and r 1 and r
or in separate clusters and in the total
amount of work we've done here is again
six pairwise record comparisons because
we've started from scratch yes please
will the persons with their local
persons be a function of the key that
you're using for everything wouldn't the
number of comparisons you are doing for
each case be a function of which fields
you're using of which rule you really
you're saying you're always doing
paralyzed but it seems like you know
what the rules are so the number of
comparisons would be a function of what
the rule measures oh right so here I'm
using a simple er algorithm that just
blindly does pairwise comparisons but
you can think of more advanced er
algorithms that kind of reduce the
number of the record comparison as well
so so again I'm illustrating using a
simple algorithm and it always performs
pairwise comparisons among all the
records that's why I'm just saying it's
six comparisons um if you use a
different year algorithm then you we can
you can get a case where it does fewer
than six comparisons but just bear with
me because this is you know isn't I'm
not saying that this is the state of the
art or algorithm at all so now while six
comparisons here may not seem like a big
deal if you're resolving hundreds of
millions of Records then this is a big
deal so we would like to reduce this
number as much as possible so before I
give you the answer of how to
efficiently produce the name and phone
number results let me show you go
through an easier example where I would
like to perform rule evolution from name
and zip code to name and zip code and
phone number so in this case there is an
interesting relationship between the two
comparison rules the second comparison
rule seems to be stricter than the first
comparison rule and you can see that up
to a records that do not match according
to name and phone number will not match
according to name and phone number and
zip code as well so therefore notice
that we don't have to starting from the
first result we don't have to compare
the records across different clusters
because we know they will never match
according to the new comparison rule
instead we only need to compare the
records within the same cluster so in
this case we'll only compare r1 and r2
and it turns out that are 1 and R 2 have
different phone numbers so they end up
in different clusters so just by
performing one record comparison we've
produced our yu-er result for name and
zip code and phone number which is much
more efficient than the six record
comparisons that would have been
produced using the naive approach yes
right so I'm assuming that the
comparison rule is in a conjunctive
normal form of arbitrary predicates in
general
right so later on i also have rule of
these rule evolution techniques for
distance based clustering algorithms
where your co clustering records based
on their relative distances so I'll talk
about that later on so going back to our
original problem notice that you can't
use the exact same technique use here to
produce the name and phone number result
because there's no special strictness
relationship between name and zip code
and name and phone number so for example
although r2 and r3 are different
clusters in the first result they might
have the same name and if they also have
the same phone number then they may
result in the same club end up in the
same cluster in the name and phone
number result so we can't use this
technique and our solution for this
problem is to use what we call
materialization so in addition to
producing the ER result for name in zip
code the idea is to produce an ER result
for the name comparison rule and another
result for the zip code comparison rule
as well and starting from this
materialization notice that we can you
to exploit the name result to
efficiently produce the name and phone
number result using the exact same
technique used in the previous slide so
again there is this interesting
strictness relationship where name and
phone number is stricter than name so
we're only going to compare the records
that are in within the same cluster in
the name result and only just by doing
three pairwise record comparisons we can
now produce the name and phone number
result which is much more efficient than
the six record comparisons that would
have been produced by the naive approach
so while so here rule evolution seems to
be very efficient but an immediate
question you can ask is what are the
costs we pay for doing this using this
technique so there are time and space
overheads for creating this
materializations in this example it
seems like we're running at the
resolution three times which basically
defeats the purpose of reducing run time
so in our paper for the time overhead we
have various amortization techniques
where the idea is to share
offensive operations as much as possible
when we're producing multiple er results
for example when you're initializing the
records you have to read the records
from the disc and you have to parse them
and do some more processing and this
only needs to be done once even if
you're producing ER results multiple
times we also have ear algorithms
specific amortization techniques so one
of the ER algorithms we are implemented
in our work always sorted the records
before doing any resolution and the
sorting here was the bottleneck of the
entire year operation so in this case we
only need to sort the records once even
when we're producing ER or results
multiple times so all of these
amortizations techniques combined it
turns out that the time overhead is
reasonable in practice and I'll show you
some experimental results later on now
for the space overhead notice that we
need to store more partitions of the
records but the space complexity of
storing partitions is linear to the
number of records and in addition we can
use compression techniques where we only
can simply store the record IDs in
partitions instead of copying the entire
contents of the records over and over
again so again in our paper we show that
the space overhead of materialization is
reasonable as well so we've considered
many different obvious
oh yes so in general we review the
comparison rule as a conjunctive normal
form and the strategy is to materialize
on each of the con junks now this is the
most general case because we don't know
how the new comparison rule is going to
change but you can actually improve this
approach but if you know some more
information for example if some of the
conjuncts always go go together then you
don't have to materialize on each of the
context but in this the most general
approach we arbitrarily is on one
conjunct at a time yes not specifically
additionally very possibility
yes have you come how does your
technique compared against as people are
done
there's a great deal of work on
materialized views this is in the same
spirit as materialized views but this is
mainly on work I'm working on clusters
of Records so this is more of this is an
ER algorithm the ER specific a technique
so I'll just keep on explaining and
hopefully this will be clear so I've
consider many different year algorithms
in the literature and I we've
categorized them according to desirable
properties that they satisfy so if a
year algorithm satisfies one or more of
these properties then we have efficient
rule evolution techniques so the first
property we identified is called rule
monotonicity and here you can see a
number of ER algorithms in the
literature that satisfy this property in
the red circle the second property is
called context free and you can see
again a number of Eire algorithms in the
literature that satisfy this property in
the blue circle and in addition we are
used to other properties in the
literature and further categorized the
ear algorithms according to the
properties they satisfy and depending on
where you are in this Venn diagram we
have different rule evolution techniques
that you can use in this talk I will
only focus on the purple area which is
the intersection of rule monotonicity
and context free so I'll define these
two properties and then explain
illustrate our rule evolution technique
but before that let me define two
preliminary notations yes please so a
lot of the real systems they will be
couple they are process centered ages is
just walking pairwise comparison mustard
does this characterization then spread
to the individual components of this
pipeline or it only applies to systems
which you described as a single stage
yeah here I'm assuming all the steps to
be a one-year algorithm so we assume
that an ear algorithm basically receives
a set of Records and partly returns a
partition of Records so this can be this
can include blocking and resolution and
post-processing so everything is
included in this black box er algorithm
just individuals they just given the
systems tend to be more piecemeal where
they want to plug in some lovers then
they want to play with some vultures
yeah certainly so you can view blocking
to be a separate process from NC
resolution so it's kind of a orthogonal
issue so in order to scale into
resolution you can use blocking
separately but for each when you're
resolving each of the blocks you can use
these the rule evolution techniques here
yeah ok so a comparison will be one is
said to be stricter than v2 if for all
the pairs of records that match your
according to be one they also matched
according to be too for example name and
zip code is stricter than name because
any two records that have the same name
and zip code will also have the same
name as well now the second preliminary
notation is domination so this so an ER
result is dominated by another result if
for all the records that are in the same
cluster in the first result they are
also in the same cluster in the second
result as well so in this example if you
look at the first er result the only
records that are in the same cluster are
r1 and r2 and since these two records
are in the same cluster in the second er
result as well the first er result is
dominated by the second year result but
this third year result is not dominated
by the second result because although r3
and r4 are in the same cluster they're
not in the same cluster in the second
result so this is not dominated so using
the strictness and domination I can now
define the rule monotonicity property
which is defined as follows is saying
that if a comparison will be one is
stricter than be too
then the ER result produced by be one
must always be dominated by the ER
result produced by be too so for example
let's say that we're producing an ER
result using the name and zip code
comparison rule and we produce another
ER result using the name comparison rule
then if the ER algorithm satisfies rule
monotonicity it must be the case that
the first er result is dominated by the
second er result so the second property
is called context free and the formal
definition is shown is shown on the
screen intuitively is saying that if we
can divide all the records into two sets
such that we know that none of the
records in these two different sets will
ever end up in the same cluster then we
can resolve each data set independently
so for example let's say that we have
four records r1 through our four and
let's say that r1 and r2 refer to
children while r3 and r4 we refer to
adults so from the start we know that
none of the children will ever end up in
the same cluster as with the adults so
we can divide the records into the
following two sets so if the ER
algorithm satisfies context-free then
we're allowed to do the following where
we resolve of each set independently so
we can first resolve our r1 and r2
together without carrying whatever
happens to our three and r4 and produce
clusters then we can resolve our three
and r4 without carrying whatever happens
to r1 and r2 and produce another set of
clusters and the property guarantees
that by simply collecting the resulting
clusters we're guaranteed to have a
correct er result now notice that i have
this a cute diagram of a thick black
wall in the middle which is basically
stopping information from flowing to the
left from left to right or from right to
left um yes who's
B is the rule yes yeah right so as an
example where context-free is violated
let's say that our three is the father
of r1 and r4 is the father of our two
let's say that the ER algorithm first
result of compares r1 and r2 and
clusters them together and compares r3
and r4 and puts them in separate
clusters now the fact that r1 and r2 are
the same child may be strong evidence
that are three and r4 actually the same
father so the ER algorithm may change
its mind and end up merging our three
with our four so that's an example where
some information is flowing from left to
right so that's a violation of this
context free property so using a rule
monotonicity in context free I'll
illustrate a rule evolution algorithm
that exploits these properties to do
efficient evolution so let's say that we
are evolving from the comparison real
name address and zip code to name
address and phone number now first of
all I'm going to materialize on all the
condoms so for so in this example I will
produce an ER result for the name
comparison rule and another result for
the address result and another one for
the zip code result but I don't show the
zip code result due to space constraints
so during step one the algorithm is
going to first identify the common
conjunct of the old and new comparison
rules in this example the common
conjuncts are the name and address
comparison rules and it's going to
perform what we call a meet operation
between among the er er results of the
common conjuncts in this example we're
going to perform a meet operation
between the name results and the address
result intuitively we're taking the
intersecting clusters so in them in this
meet result you can see that r2 and r3
are in the same cluster and the reason
is that these two records are in the
same cluster in the name result and
they're also in the same cluster in the
second result as well but r1 and r2 are
not in the same cluster in the meat
result because although they are in the
same cluster in the name result they are
not in the same cluster in the address
result now using rule monotonicity we
can prove that
this meat result always dominates the
final er result which means that none of
the records in different clusters are
ever going to end up in the same cluster
in the end result so during step to the
algorithm exploits the context-free
property and you know since it knows
that none of the records can match
across different clusters is going to
resolve each cluster independently so
for the first cluster it only contains
our one so we're going to return our one
as a singleton cluster for the second
cluster we're going to compare r2 and r3
and let's say that it turns out r2 and
r3 are different refer to different
entities so they are placed in separate
clusters finally for the third cluster
it only contains our four so we return
our four as a singleton cluster and
simply by collecting the final clusters
were guaranteed to have arrived at a
correct ER result and the total amount
of work we've done here was only one
record comparison which is much better
than the six record comparisons that
would have been performed by the naive
approach so in summary I've defined the
rule monotonicity and context-free
properties and I've Illustrated an
efficient rule evolution algorithm that
works well for this purple area again
depending on where you are in this event
diagram we have different rule
evolutions that work for those air
regions as well yes the idea dimension
the example is actually relying on the
fact that the quality of name
so suppose my projects one of the form
that name is in that
I did it's possible that we are actually
about it because the constants the
thresholds of each of them
columns actually get changed right in
which case the realization becomes much
more yeah so so I'm assuming that the
conjunction rule is a conjunction
conjunctive normal form of arbitrary
predicate so within a predicate you can
use whatever similarity function you
like um here I was just showing you an
easy example where we only perform
equality constraints so correctness is
obvious in kind of me like 30 gates
right like when you have such concerts
and searchers it's not like you have the
middle I spend more per decade site so
if you have a another point nine
we can go to make it like so in this
model if you have if you're doing a
similary comparison between names and
you're comparing with you know high
threshold and you have another predicate
that uses a lower threshold we consider
these two predicates to be totally
different so that's but you can do more
optimizations by taking into account
that these are two predicates are
actually doing the same string
comparison compared with the title with
a threshold yes yeah so it depends on
the ear algorithm if you are using for
example the single link hierarchical
clustering and you kind of you relax you
relax the threshold which means I'm so
you reduce the threshold then all you
have to do is build on top of the
previous er result now if you go the
other way around then this this approach
does not work yeah yes vivek so you
don't go changing the roof whatever the
data changes
yeah so that's a very important problem
so you this work is a mainly focused on
when the comparison rule itself changes
so it's an orthogonal problem and in the
future I like to use a do research on in
both directions but this work does not
consider the case where you're having a
new stream of Records which you'd like
to inquire to your er result okay so so
far I've been talking about rule
evolution techniques for what we call
matched based clustering algorithms so
in our paper we also have rule evolution
techniques for what we call distance
based clustering algorithms so here
records are a cluster based on the
relative distances so I won't have
enough time to go through all the
details but just to give you the
high-level idea let's say that we have
four records are stn you and let's say
that the record s is close to our
because s is within that dotted circle
around our but the records t and you are
far away from our because they're
outside that dotted circle now whenever
the distance function changes all these
pairwise distances are going to change
as well and although we don't know how
exactly those distances will change
let's say we at least know the lower and
upper bounds of those distances as
denoted in the as the red brackets so
for example the record s may suddenly
become closer to our or may end up
further away from our but we still know
that s will still be inside that dotted
circle so we know that s is still close
to will still be close to our and for
record you although the distance R from
our is going to change we still know
that you is going to be outside that
dotted circle now the interesting case
is record T so T may end up with in that
dotted circle or outside the circle and
in that case we're going to be
optimistic and cluster r and t together
but later on revisit this cluster and
check if aren t are indeed close with
each other if not we're going to split
tea out of ours cluster so I'm glossing
over a lot of details but that's the
high high level idea for a rule
evolution for distance based clustering
algorithms
so the following slide shows the all the
data sets i've been using for all my
yard works the first data set is a shop
contain shopping items provided by yahoo
shopping where items were collected from
different online shopping malls and each
item represents a record represents an
item and contains attributes including
the title price and category now since
these records were collected from
different data sources there are
naturally many duplicates that need to
be resolved the second data set is a
hotel data provided by Yahoo travel and
again records were collected from
different travel sites like orbitz and
Expedia and again different records may
refer to the same hotel and while there
aren't too many records here there are
there are many attributes per record so
the data is very rich here finally we
have a person they a data set provided
by a spa com where people records are
collected from social networks like
Facebook in myspace and blogs and
Wikipedia and we have a lot of Records
here and and each record contains the
name gender state school and job
information about a person so among
these three datasets we are for my
evolving rules work I only performed
experiments on the shopping and hotel
data sets and in this talk I'll only
show you experimental results using the
shopping data set yes Ravi the data to
pieces
hope holy expensive is it to rent the
name
all the extremely expensive yeah you
don't really you don't want to run the
naive approach oh the night well ok so
the naive approach is defined to be
starting from scratch right ok so your
question is what else so how fast does
how long does it take to resolve
hundreds of millions of Records yeah so
I was interacting with Spock da comment
I think it takes in word of hours or
maybe days but I'm not sure about the
exact number because it's kind of a
corporate is kind of a secret that they
have so they weren't revealing all the
numbers to me not that I've no not not
Spock calm so this company recently got
acquired by a company called in telecom
so you know since then they might have
changed their strategy but when I was
interacting with Spock come they were
just using a regular day a DBMS like
like my sequel yeah yes please racially
you have good questions for the hotel
data set we had a gold standard which
was provided by a Yahoo employee that we
knew but for the other two data sets we
don't have a gold standard is very hard
to generate a gold standard for this
data on this sets so i'm going to show
you two representative experimental
results of my work and both of them are
going to be runtime experiments the
reason i'm not going to show you any
accuracy experiments is that our
techniques are guaranteed to return one
hundred percent accurate ER results all
the time so we're not trading off
accuracy with scalability we're only
improving the scalability of rule
evolution so that's a feature of our
work so these are this plot shows
results of valuing our techniques on
three thousand shopping records and the
x axis shows the strictness of the
common comparison rule so we tested on
many different comparison rules and in
some of these experiments we evolved
from title and category to title and
price now in that example the common
comparison rule is the title comparison
rule and for this code for this rule we
were extracting the title from two
different records and we were computing
the string similarity of the two titles
and we then compared the this value with
the threshold
now if we increase the threshold then
the title comparison rule becomes
stricter in a sense that fewer records
match according to their titles so in
that case we move to the left side of
the x axis on the other hand if we
decrease the threshold then the title
comparison rule becomes less strict in a
sense that fewer records up more records
match with each other so in that case we
move to the right hand side of the x
axis so just think about the exit access
to be to represent different application
semantics now the y axis shows the
runtime improvement of rule evolution
vs. the knife approach but here I did
not include the time overhead for
materialization but in the next plot I
will compare the total runtime of rule
evolution including the time overhead
with that of the naive approach so we
implemented for different er algorithms
in the literature and you can see that
in the best case we get over three
orders of magnitude of improvement while
even in the worst case we still improve
the runtime of the improve the naive
approach yes please the Ninth approach
is to simply run er from scratch without
you without you up exploiting the
materialization so um so given a set of
Records simply run your own year
algorithm using the new comparison rule
oh here so these are four different
algorithms on the red plot is the sordid
neighbor technique where we sort records
and then we slide up we use a sliding
window and and compare the records
within that the same window these are
all different techniques hdb means that
we're merging records that match with
each other in a greedy fashion and a CBR
is similar but we have some properties
desirable properties that guarantee that
the ER result is going to be unique and
finally hdds is a distance based
clustering algorithm it's simply it's
the single link hierarchical clustering
algorithm
right so k-means does not did not
satisfy any of the property so it was
not like a good year algorithm to
demonstrate our our techniques yeah so
when you're comparing the total run
times here's what you should expect so
let's say that for rule evolution you're
performing rule materialization once and
then you perform rule evolution one or
more times so the x-axis here shows the
number of evolutions and the y-axis
shows the total run time initially if we
for rule evolution we're paying an
overhead for the materialization so rule
evolution is slower than the naive
approach but as we perform more rule
evolutions the incremental costs paid by
rule evolution is smaller than the cost
for the naive approach where we simply
run er from scratch so at some point you
can imagine rule evolution is going to
outperform the naive approach now the
following slide shows a result of one
scenario we've considered we are the
x-axis shows a number of shopping
records that were resolved and the
y-axis shows the total none run time in
hours on a log scale I'm only going to
show you explain the top two plots that
are colored so the red plot shows the
total run time of the naive approach
while the blue plot shows the total run
time of when we're using rule evolution
using the exact same er algorithm and
here we performed materialization once
followed by one rule evolution and you
can see that the blue plot still
improves the red plot which means that
in this case the rule evolution was
saving enough time to compensate for the
time overhead paid by a time overhead
for materialization and although this
gap seems very small the unit of the
y-axis is in hours so the gap is
actually the runtimes improvement is
actually significant now in yes it's
also a log scale as well so that makes
it a improvement even more significant
now the important thing to understand
here is that this result shows is only
for one scenario we've considered you
can think of many other scenarios where
you can either get results that are much
better than these results are much worse
for example if you're comparing multiple
evolutions instead of just one then
you're probably going to have much
better runtime improvements and those
shown in the and in this plot on the
other hand you can think of pathological
cases as well pathetic cases so for
example if the old and new comparison
rules are totally different then there's
no point in performing rule evolution
because there is nothing to exploit so
in this case just simply running the
naive approach is the most efficient
technique and if you run rule evolution
here you're you're probably slower than
the naive approach because you still
have to pay the time overhead for
materialization so the takeaway of this
plot is that we now have this general
framework for rule evolution that you
can use to evaluate if rule evolution
makes sense to your application or not
so in conclusion we propose a real
evolution framework that exploits er
properties in materialization and we've
shown that a rule evolution can
significantly enhance the runtime of the
naive approach so that was the first
part of my talk and the second part will
be much shorter um yes but rules that
are linear combinations of red there are
weighted a bigger combination so I think
that's the case of distance based
clustering so an example for a distance
function is up you you add the titles of
the name similarity with the address
similarity and you can do a weighted sum
so the at the end you get some distance
value so our distance based rule of rule
evolution techniques for distance based
clustering algorithms work work work for
your case they did another case of the
case where the is a nonlinear cell or if
the
as long as you return a distance it's
fine so so you don't have to satisfy any
triangular property or anything but the
only assumption we make is that you have
to have an idea of you know how much
this distance can change in the next
evolution so you we have to have some
information that for example this each
distance is only going to change and
most by ten percent or by some constant
amount like five so as long as we have
that information you can use our
techniques right so in this work are we
assume conjunct a conjunctive normal
form so if you only have disjunction
everything is going to be considered as
one predicate okay so we assume that you
can convert this into a conjunctive
normal form and then we are the ideas to
materialize on each of the condoms yes
please yeah so I mentioned that briefly
so it's a so we're only saving multiple
partitions of records now instead of now
so the space complexity is linear to the
number of records but notice that you
can use a lot of compression techniques
so you can save the reg a partition of
record ideas instead of a partition of
the actual records so this saves a lot
of space and it turns out that the space
overhead is reasonable and practice it
doesn't yeah it's not more than ten
percent basically
to lose your advantage in normalizing
very far in analyzing essentially the
mighty close yeah so um so that's that's
yeah that's an issue that's a I guess it
that's a concern that we have almost
rules already
well so so we're
so at least for the experiments we
performed a lot of these comparison
rules were just conjunctions of of
predicates so we we think we believe
that this is a reasonable something is
reasonable to assume that you have a
conjunction of predicates yeah but there
is definitely I explain the complexity
for converting a dnf expression to a CNF
expression is exponential but this is
but once you do that then you can use
our rule evolution techniques yes please
absolutely your speed up is a function
that has a sequence puncture the
sequence of rules that you have and
currently what are you putting fine
you
resolution yeah so my understanding is
that I don't have the the well again
I've interacted with Spock calm but they
didn't give me all the details they have
but my impression is that some of these
predicates are always used so you always
compare the names and addresses of
people and afterwards you may compare
you know the zip codes or phone numbers
or some other attribute like well gender
for example so so in that case you can
actually do something better than you
can improve my of this this approach
where you can materialize on the name
and address combination and and if you
have a zip code then you may or may not
materialize on the zip code so this is a
very like application-specific
optimization you have to see which car
conjuncts are used together and which
conjuncts will likely to change or not
and my understanding is that you know
certain conjuncts never really change
it's only kind of like the tail that's
changing but I can't like I'm confirm
that how many
to resolve her
so when I was visiting spocom every time
I made a visit they were discussing how
to improve their comparison rule so this
always happens I can't give a definite
number but it's always a continuous
improvement so it's very rare that you
have complete information about your
understanding about your data and
application so in the real world you're
always you know getting more on while
you're constructing your system you're
getting a deeper understanding of your
application and you make improvements to
your your match function and comparison
rule and then it's a back-and-forth
thing so then you evaluate this rule
then you realize you've missed something
and then you improve your comparison
rule and this kind of repeats so it's
pretty frequent yes please some
questions about the skilled radio
tonight in some sense they don't do that
for example if you look at many of the
year techniques you can say this two
part is impaired wise matching detecting
things that are likely to match and it
does not the question sir but both of
these about a second can be very nicely
the ruins
to help you in your information review
some compelling it's a well-designed
here with a baseline or you know what if
you have
what what does use what techniques are
used to permit so with respect to such a
system then how much right so for my
work I only considered up the ER
algorithms in this Venn diagram so I
know these are very cryptic but this is
sort of the sort of neighbor technique
and this is the hcs are all hierarchical
clustering the emmy is the technique
proposed by mon janelle kind where you
have a queue and you want to may not
only compare each new record with
clusters in the queue so regardless
mentioning k-means but we didn't
consider k-means and we don't we didn't
consider your case where you have this
very scalable way of decoupling them you
know the the identification of
candidates followed by a clustering so
these are the algorithms we use in our
paper but it'd be very interesting to
actually see how your techniques if your
algorithm actually satisfies any of
these properties okay yes it was a just
hand pixel I was just considering a 100
so a comparison rule that I used
contained I'm to two predicates and then
I was changing one of them so it was I
wasn't considering like I was in doing
entire survey of doing rule evolution so
but this was just trying to demonstrate
that how rule evolution saves time and
compensates for the materialization
overhead but I I agree about there's a
lot of like future work that can be done
from here basically there are lots of
unanswered questions ok so I'll move on
to the second part of my talk will where
I will mainly talk about a work called
managing information leakage and then
briefly I mentioned a more recent work
called disinformation techniques for
yard so at the beginning of my talk I
motivated you of about the data privacy
problem by explaining how insurance
companies are sifting through the web
and collecting and analyzing your
personal information to predict your
life spans um here's another interesting
example for for my work so Korra calm is
Q&amp;amp;A site where people can ask questions
and other people can answer those
questions and not too long ago there was
this person who posted a question
basically challenging anyone out there
to find all his information on the web
and if you look at this question you can
see there's no indicator of the identity
there's no clue about the identity of
the person who asked the question you
can't find his name there's no ID of the
person who posted this question so just
by looking at this text you can't get
any any clue now it turns out that once
you post a question on core com you
become the only follower of your own
question so there was this anonymous
user that came along and he was lucky
enough to see that used on to discover
like the only follower of this question
and so he clicked on the profile got the
name and started searching through
various social networks blogs and home
pages and to extract all the information
about this guy and so the anonymous user
posted 26 bullet points of personal
information about this person one bullet
point says that you're 24 and you will
be turning 25 very soon another bullet
point says that your mother's maiden
name starts with the letter P another
one says that you can program in C++
another one says that you're related to
some incident in LA back in 2009 and
finally another one says that you are a
Democrat ok so Joseph who was very
impressed it turns out to be the person
who posted this question was impressed
and actually categorized the 26 bullet
points as follows so some of them were
categorized as correct and downright
scary so apparently Joseph did not
anticipate that someone would figure out
his exact age and if someone figures out
my mother's maiden name I be freaked out
as well now some of the bullet points
were categorized as deliberately public
so the claim that Joseph can program in
C++ was probably information that could
be found in his online resume
interestingly there were six incorrect
bullet points so the claim that Joseph
is somehow related to this incident back
in LA in 2009 turns out to be false
information so Joseph had this facebook
profile image that was related to
incident but it turns out this image was
a deliberate obfuscation that Joseph
made in order to confuse the adversary
any adversary that wanted to find his
information also the claim that Joseph
is the Democrat turns out to be wrong as
well so it turned in this case of the
anonymous user confused this Joseph to
be the same person as some other Joseph
with the with the same name and since
the this other Joseph was clearly a
Democrat the anonymous user thought that
the first Joseph was a Democrat as well
but here Joseph clarifies that no party
can handle his I do sing chrissy's so
after this experiment joseph concludes
as follows in bold font he's saying that
it seems to be far more effective to
allow incorrect information to propagate
than to try and stem the tide so my
research and data privacy isn't is
twofold first I like to quantify just
how much of your personal information
speaked being leaked into into the
public and second I like to propose
management techniques for information
leakage now the interesting connection
between this work and my previous works
on NC resolution is that I'm assuming at
the adversary is performing entity
resolution to piece together information
in this example the anonymous user was
piecing together Joseph's information to
nor more about him so I'm basically
using the ER models I developed in my
previous works to simulate the adversary
to then quantify information leakage so
that's the interesting connection
between this work at my previous er
works so to formalize this problem a bit
further I'm going to assume that all the
pieces of information on the web are
simply records in a database as shown in
as the blue boxes and I'm going to
assume that there is a notion of full
information of Joseph as shown on the
top of the screen again the adversary is
going to perform entity resolution and
piece together the relevant records are
referred to Joseph and intuitively the
information leakage is defined as the
similarity of this blob of information
with that complete information so i
won't have enough time to go through all
the details of my measure but here are
the key features so first we incorporate
entity resolution
second we don't assume that privacy is
an all-or-nothing concept so if you look
at many previous privacy works many of
them assume that especially in data
publishing the assumption is that you
somehow have complete control of your
data before you release it so once you
make it perfectly private then you're
allowed to you can just give it to
someone else for example a hotel oh
sorry oh um a hospital may may be trying
to release a set of medical records of
its patients to the public for research
reasons now there are lots of
anonymization techniques you can use to
make it very hard to figure out which
patient has which disease so once you've
made the data entirely anonymous then
you're allowed to publish this
information to the public in comparison
our works make a fundamentally different
assumption where we we assume that some
of our information is already in the
public so whenever you want to interact
with your friends through Facebook you
have to give out some of your personal
information like your birthday if you
want to buy a product from amazon com
you have to give out your credit card
information so in order to do our
everyday lives we're continuously
exposing yourself to the public so our
view of privacy is that there's no such
thing as perfect privacy and that
privacy should be within a continuum
from 0 to 1 and we're just trying to be
as private as possible in addition our
information leakage measure incorporates
the the uncertainty that the adversary
has on his data and the correctness of
the information data itself so once you
release the information at your
information to the public we assume that
it's very hard to delete that
information so you know even if you
attempt to remove some information some
other people may have made copies of
that information and companies may have
backup files of that information so if
you delete a photo from facebook who
knows that if that photo is still
floating around the web so in that
setting the only way to reduce
information leakage is to add what we
call disinformation records as shown in
red where the idea is to a dilute
information by producing records that
are realistic
incorrect now this information is not a
technique that we're proposing as a new
idea this strategy has been used since
the dawn of mankind so during World War
two the Allied forces generated a great
deal of disinformation to trick the
Germans into thinking that they would
land on clay but they actually landed on
Normandy and this was a main turning
point of the war so we're adapting this
on ancient strategy to the realm of
information management so in our paper
we propose two techniques two types of
disapprobation records the first type is
called self disinformation where the
disinformation record snaps onto one of
the correct records and lowers the
information leakage by itself the second
type of disinformation is called linkage
disinformation and here the
disinformation record is connecting an
existing record that's not correct to
one of the correct records and here the
contents of the existing record yyy is
being used to lower the information
leakage these two strategies turned out
to be very effective in practice and we
always saw examples for both of these
strategies in our coral example recall
that the anonymous user mistakenly
thought that Joseph was related to some
incident back in LA in 2009 and it turns
out to be that turned out to be
incorrect because the facebook profile
image of of Joseph was kind of a
deliberate obfuscation that's an example
for selfness information the anonymous
user also mistakenly thought that some
other Joseph was the same person as this
Joseph and mistakenly thought that you
know this the original Joseph was a
Democrat that's an example of linkage
disinformation and in our paper we
discussed these two strategies in more
detail now in a more recent paper I have
which I submitted to the LDV I focus on
the linkage disinformation problem in
more depth so here I assume that the
adversary is our running entity
resolution to cluster records that refer
to the same entity and once we have that
I assume that for each pair of the
clusters there is a way to generate this
information that will trick the
adversary into thinking that these two
clusters refer to the same entity and
therefore should be merged with each
other so I call that cost the generate
the cost for generating that district
room
is what we call the merge cost and the
cost Sardino now written and in black
black numbers for each pair of clusters
now for each cluster that we
successfully merge with our target
cluster which is Joseph cluster in this
example we assume that there is a
benefit we obtain because we're sort of
weird i looting the information of
joseph which is a good thing so for each
of the clusters i indicate the benefit
in using rent numbers so given this
setting we can now formulate an
optimization problem where the purpose
is to the goal is to emerge clusters in
a pairwise fashion starting from
Joseph's cluster so that we obtain the
maximum benefit while paying a cost
total cost that's within a budget be in
our example in this example if if the
budget B is 3 then the optimal
disinformation plan turns out to be that
green tree where we first merge the left
cluster with the top cluster and
simultaneously merge the left cluster
with the bottom cluster and in that case
the total merge cost is 1 plus 2 which
is exactly of three here and the total
benefit we obtain is 1 plus 1 which is
equal to 2 now I won't have enough time
to go into all the details of our
algorithms but it turns out this problem
is strongly np-hard which means that you
can't come up with a pseudo-polynomial
algorithm and there are no approximation
algorithms as well so in this in the
most general case we only have
heuristics however if we restrict the
height of the plans to be at most one
where we can only dredge clusters to the
target record directly then this problem
becomes weakly np-hard and we do have a
exact pseudo-polynomial algorithm and a
to approximate algorithm and i'll be
happy to talk about more details if
you're interested yes please
can you up like a anonymity
alright so here I'm talking about the
disinformation problem I think your
question refers to the measure of
information leakage so for KN and then
compared to K anonymity the K anonymity
measure is a zero or one measure where
you're either completely safe or you're
not safe at all if you give you up if
there are you know so so your data is
considered safer not safe it's a black
and white thing in comparison for
information leakage measure we can
quantify more fine-grain notions of
privacy so that was about the measure
but this this slide is more about how to
generate disinformation records that can
maximize the benefit of target record oh
this is up this is the budget we can you
can use to for generating the
disinformation record so there there is
a cost for creating a brand new record
right and you know you can't simply
generate whatever you want so this is a
way to kind of limit the total amount of
information you can kind of
disinformation records you can you can
produce using that that will confuse the
adversary part of it so I I have more
problems with the benefit can you
quantify the benefit yeah
so so this this is one one way to do it
there are many ways to model this
problem both right so what 101 approach
is to simply define the benefit as the
number of records within this cluster
but that's not like the only way you can
do it you can for example well compute
the benefit by by using an arbitrary
function that works on like the so after
you Club immersive clusters you can
apply a general benefit function on all
the records that have been mistakenly
emerged but in the in this problem well
we assume that you can somehow add these
are benefits in a spy of summing them
together and this makes the problem easy
easy to solve analytically so it's kind
of so it's not we're not claiming that
this is the only way you can define
benefits but that's a this is actually
an open problem that we haven't know
where that has to be studied more yeah
risk of someone might take this use it
in a different way just smear you
yeah so this thesis information
techniques can actually be very harmful
for companies that are doing data
analytics on big data so in that case of
you if you're trying to extract profiles
from you know social network social
media data then Obama really wasn't born
why okay okay yeah yes so if people
manage to so that's kind of
misinformation here I'm focusing on the
generating data that can confuse the
adversary into thinking that you know
different records refer to the same
person or object so it sits in the same
line as producing this information for a
president obama yes
in general I don't know like what pieces
are we talking about me outline where
without knowing you don't have it is not
a feature how do i implement
yeah this is a very case by casing so in
general you can assume that you have
knowledge of all your information out in
the public but there are some cases some
application certain applications where
you do know your where your information
is so a good example I always use is
let's say you're the camera company
Nikon and you want to you kind of have
this your latest and greatest product
and you don't want to leak the specs of
your camera now there is a site called
night of Nikon rumors calm which is
dedicated to spreading rumors about the
next best camera produced by Nikon and
people actually post in rumors
information that they think is correct
and so for example you might guess that
the next nikon product is going to have
40 million pixels and you know some some
answer your speed of some few seconds at
both some have certainty shutter speed
and so on now you can act you can also
add a confidence value which reflects
how confident you are in this
information so there aren't too many
rumor sites so in this case nikon can
sort of figure out where all its camera
information is where all the rumors of
his camera are coming from and you plus
you might only be interested in these
rumor sites as well so so there are so
the bottom line is no I'm not claiming
that you have perfect information about
all the data but in in real world
applications well in for certain
applications you your it's enough to
know it you do have information about
you know where your information is you
do have a good idea of where your
information is located I hope that
answered your question okay alright so
this strategy when you get right down to
it the strategy is probably really for
situations where other people are
descending
the kinds of information you can
disseminate about yourself on places
like
these kinds of things it seems to me
like if you're putting it out there in
the first place the assumption is you
want people to know so
you say that's correct characterize that
sort of the utility of this is really
about about spending people who are
trying to disseminate information about
you don't want to see
yes so I think you're getting at the
issue of reputation so you don't want to
post information that gives gives a bad
name to you so this is a simple
people like that that's the camera
example right I mean you as a company
you know what you don't want someone
else to to put the specs for your latest
camera out there so go on a
disinformation campaign to spend spend
what people think they know exactly yeah
that probably doesn't apply so much to
the things that you put out there
yourself because the assumptions we put
it out there and put it out there
this is a distinction to make right so
so here you're the the problem is
focused on trying to trying to hide a
certain information and lowering
information leakage it's not about
trying to reveal information now there
there are there's an interesting startup
called reputation calm which actually
tries to solve the reverse problem there
they want to promote some of your
positive information to the public so
they have these interesting new web spam
techniques and where where you can kind
of make sure that some of your
information appears in the top of top
search results of being yeah yeah I'll
just move on um but these are all
interesting questions and this is a kind
of open problem so in summary I've
proposed a new measure for quantifying
information leakage and I proposed this
information techniques that can be used
to reduce information location and thus
manage it so that was the second part of
my talk and for the related work so
there's been a lot of previous works on
NC resolution and privacy and instead of
just listing all the works on one screen
I thought it would be a better idea to
just talk about the high level ideas so
there's been a lot of works on ants
resolution that focused on accuracy and
relatively fewer works that focus on
scalability MSR has been a pioneer in
resolution result and I'm well aware of
the data cleaning project by the DMX
group and I regularly sites papers from
surjit arvin and Rogoff my work has been
mostly focused on the scalability aspect
of ER and if also proposed many new
functionality for and ER so for example
my rule evolution work up solves a new
problem for yard where I want to update
an ER result incrementally when the
comparison rule changes frequently for
privacy there's been a lot of data
publishing works in the past and again
most of these works assume that you
somehow have complete control of your
data and their their anonymization
techniques you can use to make this data
set private before you release it to the
public
in comparison we assume that there's no
such thing as perfect privacy and we're
just trying to be as private as possible
there's been a lot of measures on
privacy as well again just to address
Ravi's comment most of the measures
assume that privacy is zero or one
concept and in comparison our techniques
assume that privacy can be anywhere
between zero and one so this kind of
flexibility and it enables us to measure
in a more find more fine-grained notions
of privacy and I am aware that ms are
also has produced the state-of-the-art
privacy measure called differential
privacy from the Silicon Valley research
lab so for future work i'm interested in
many ideas a direct extension of my work
is to do a study data analytics in a
more distributed setting so you might
end up performing data and licks on many
machines either because you have simply
have too much data to run on a single
machine or you might have privacy
constraints where companies are not
willing to share their information so
you're forced to run analytics from
separate notes the issue here is up to
max up exploit parallelism as much as
possible and also perform analytics in
an accurate fashion I'm also interested
in social networks so nowadays you find
a lot of graphs about people and it's
very important to analyze this
information and identify interesting
trend trends among among people so a few
months ago I got this email from
LinkedIn which is a professional social
network where they were all telling me
that there's solving this problem where
they would like two identical user
profiles add user skills and companies
so that they can connect users up with
certain skills to companies that want
people with those skills so in order to
do this mapping correctly you really
need to resolve the people and skill
sets and the company's properly and so
you can immediately see that there are
entity resolution problems here
currently I'm working on
fascinating topic called crowdsourcing
where the idea is to use the wisdom of
the crowd to solve problems that are
hard to resolve tarps to solve using
computer algorithms only for example you
might want to resolve a set of photos
where the goal is to figure out which
photos refer to the same person now if I
show you two photos of surjit then you
probably can mediately see that these
two photos refer to the same person but
if you try to do this using a computer
algorithm this is going to be very
challenging because this involves
sophisticated image analysis and even if
you you know use those algorithms here
the computer is not probably not going
to do a good job so so the issue so the
challenge is to ask humans just the
right questions and use those answers to
do the right clustering now humans can
be very slow expensive and error-prone
and so they make mistakes both
intentionally and unintentionally so a
significant challenge is to incorporate
this this erroneous behavior of humans
when doing crowdsourcing so in
conclusion data alex is a critical
problem I've solved two problems within
analytics data integration and data
privacy and I've mentioned that there is
an interesting connection between these
two topics the better you are
integration the worst you are in privacy
and vice versa so thanks for listening
to my talk and I'll open up the
questions
yes yes that's fine thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>