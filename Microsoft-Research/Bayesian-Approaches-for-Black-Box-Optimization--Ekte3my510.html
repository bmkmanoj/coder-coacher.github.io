<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bayesian Approaches for Black Box Optimization | Coder Coacher - Coaching Coders</title><meta content="Bayesian Approaches for Black Box Optimization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bayesian Approaches for Black Box Optimization</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-Ekte3my510" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
um so I am going to talk about Bayesian
approaches to blackbox optimization and
sort of different from some of the other
talks we had today I'm basically going
to give sort of a broad overview and
then tell you about so I'll try and
motivate why you should be interested
and then I'll give you some problems
with Bayesian optimization that either
you can solve you can help me solve
someone will solve these problems okay
and I should also say that after I tried
to make this as general as possible and
get rid of equations and I'm not sure it
even has to be Bayesian anymore so if
you're not interested in Bayesian
methods then you can still listen okay
so what do I mean by blackbox
optimization so by the name first I mean
it's just optimization so I've got some
function that I'm interested in solving
f I want to find the maximum of that but
F is a black box so really the only
thing that I can do is I can query its
value and I can't get any gradient or
anything I can just say F what is the
value at X and it will give me back some
some Y value and that Y value may even
be corrupted by noise so really what I
have is at the end of the day I have
some box that I'm gonna give it X's I'm
going to get back Y's and the function
that I'm interested in optimizing is
then going to be the expectation of
those Y's that sort of churn through the
black box and come out and in particular
I'm going to do this sequentially so
I'll have some sequence of X's that I'm
gonna specify in some manner in order to
try and find the best value of F okay so
before I sort of dive in to this a few
examples so one of the the big ones that
been sort of making waves recently due
to some work by Jasper snook and
colleagues is in hyper parameter
optimization so F is in some sense the
training loss for some machine learning
algorithm and we have some hard to set
hyper parameters so I may have some
other hyper parameters of the model that
I can optimize say the weights of a
neural network or something like that
but say the learning rate or some other
parameters that are difficult to set I
can't get gradients from them I can only
sort of churn them through my model and
then get back out the training set error
those are the things that I'm interested
in so that that that lost and then those
hyper parameters that's FNX so in the
setting of reinforcement learning or
learning to control like Carl talked
about earlier today so F might be I set
some of the parameters and I run my
simulator or even I run it in the real
world and I get back whether or not the
robot fell over or something like that
that's another example
there's dealing with the state spaces
there in a way that Carl's been doing is
maybe a little bit more complicated
though but in principle I could just
treat that as some function f that I
want to optimize or recommending ads is
is another example I want to really look
at someone and say okay what is the
expected number of times they click on
some ad when I present it to them or
let's say I present them a sequence of
ads so I really could keep going because
this really is any function that I'm
trying to optimize so any optimization
problem and it really works when those
functions that I'm trying to optimize
are noisy or when it's costly to perform
this evaluation and this last example
may seem a bit familiar if anyone is
familiar with with sort of the classical
bandit setting and in this setting
what we have is we have K arms so and by
arms I'm really talking about these sort
of slot machines here that I'm
interested in pulling one of them at
each iteration and I get back some
reward and eventually I'd like to sort
of settle upon the single slot machine
that's going to give me back the the
best reward when I pull it so each one
of these arms has some Associated
distribution with it and so at every
round I'm going to select one of them
pull that arm and observe the reward I
get back well in this setting each of
these arms just really corresponds to
one of the inputs of my black box
function so if we talk about the F
function that I defined in the previous
slide
that's just F of 1 or F of K so I just
have discrete inputs but it's really the
same problem so the sort of black box or
Bayesian optimization problem that I'm
going to describe here is the same sort
of setting as the bandit setting okay so
now what I'm going to talk about is sort
of this general optimization strategy
and what we can really do is if we talk
about sort of the bandit strategy that I
introduced in the previous slide sort of
what I can look at is at every round I
just have some function some acquisition
function alpha that is dependent on my
data the data that I've previously
collected by by interacting with the
actual function f or my noisy
observations of the function f and alpha
is some function of that previous data
and my input space so what I'm going to
do is I'm going to look at this alpha
function which I'm going to define it in
a second here but it should be something
that is easy to evaluate as opposed to
my hard to evaluate function f and I'm
going to maximize that once I maximize
that then I just sample that point I
record my data and I
churn through this
over and over okay so really what I've
done is I've rewritten this in terms of
some new acquisition function some new
strategy that I can talk about
optimizing and really what this function
here should be is it should be high in
areas that we expect the maximum to be
in but it should also force us to
explore the space so we don't sort of
inadvertently miss out on looking in
areas where the where the actual
function could be hi I just decided not
to look there and as should be evident
from the the sort of title of this slide
I'm going to use a a Bayesian model to
inform what this acquisition strategy
should be but really I could use
anything right well in principle I could
use anything right there but whether or
not it actually performs well will
really depend on what I define this
acquisition function to be so here's an
example and here's what why I was saying
I'm not really going to talk about this
in terms of a Bayesian model and I'm
sort of going to just say well I've got
some model and it happens to be a
Bayesian posterior and it happens to be
a Gaussian process but it could really
be any sort of model that quantifies my
my uncertainty in space so here what I'm
using is a Gaussian process posterior
I've sampled the function at this single
point right here that I'm noting this
this blue cross and I have this
acquisition function that's defined like
this and this in particular happens to
be the expected improvement acquisition
function okay so just as a better
example of this I can actually run this
so this is just the same sort of model
and this is running right now so my nose
is relatively small so hopefully there
won't be any problems here but so I've
sampled at this thing
this point was chosen randomly and then
this is my acquisition function down
below and it is peaked at this point
right here so that Red Cross is the next
point that I'm going to evaluate and and
so I'll evaluate that function update my
posterior and and then recompute my
acquisition function and then I'll
continue this process and I'll be able
to look at where it's high and just
continue this process so what we see is
that after a couple of iterations sort
of this point right here is going to
turn out to be where the maximum is but
all have tried that a couple of times
but I'm still eventually going to try
other points in the space in order to
make sure that I'm not missing out on
some big peak that's over here that I
just haven't seen yet but once I try
this a few times I'll sort of go back to
that location right there to sort of
zoom zoom in on where I think the actual
optimum is and so however I did point
out that what a good acquisition
function is is sort of up in the air the
expected improvement acquisition
function it actually works reasonably
well but you'll see here one of the sort
of deficiencies of it it tends to get
very very peaked with plateaus that are
pretty much a zero throughout the space
once you've sort of done enough
exploration but if it's a high
dimensional space there can be large
areas with with sort of plateaus at zero
and then peaked regions around either
areas that are where you think the
optimum are or areas that you haven't
explored so the the acquisition function
itself is sometimes difficult to
optimize but so well we can keep doing
this a couple times and eventually I'll
have tried it enough right here that I'm
very confident the optimum is right here
so maybe I'll go back and start
exploring areas where I'm less where I
have less confidence in them okay
so so that was just one particular
acquisition function there are a few
other interesting acquisition functions
I wanted to point out one of them is
sort of been popularized by Phil Hennig
and some colleagues recently and that
entropy search and basically the idea
behind that is that we can define a
distribution over where we believe the
maximum to be and then we can look at
the change in entropy of that particular
distribution based on the points that we
we sample so one of the difficulties
with this is that it's very very hard to
compute this so there are a number of
different approximations one has to make
for this but it has this interesting
property where we can compare functions
even for things that were not interested
in so for example this has been used
recently in the setting of of hyper
parameter optimization where we might
have by also by Jasper where we have a
setting where maybe we can evaluate the
function on let's say 200 points but
we're really interested in the setting
where we want to test on let's say a
million points or some very large data
set so what we can really do is look at
hyper parameters hyper parameters that
were tuning for some machine learning
algorithm and test them on scenarios
with low amounts of data a couple of
times to sort of get information and we
can really zoom in on things with large
amounts of data and the algorithm can
trade-off between these two types of
scenarios so another interesting
approach is Thompson sampling which
basically takes that same distribution I
talked about it's a distribution over
where the algorithm believes the Maxima
is and it's a posterior distribution
given our current data and it takes a
single sample from this posterior
maximizes that single sample and then
returns the Arg max of that so it's
actually quite related to entropy search
one of the interesting things about this
is it's often quite easy to take a
sample from that posterior distribution
and and in fact so one of the
interesting things about this particular
algorithm is that it is that we just
have to take that sample so it actually
if we want to look at more complicated
posterior models it may fit up quite
nicely with some of the things that
people have been talking about and
probabilistic programming languages
today where if we just have a language
where we can specify models and then
take samples from them that is the only
thing that Thompson sampling needs and
it actually performs quite well one of
the troubles is when the core if we have
some rich covariance structure often if
this covariance is sort of informative
enough many of these different
approaches have sort of similar
empirical performance and it's sometimes
difficult to choose between them which
method will perform best and it often
shifts with different problem instances
one strategy that we've taken is sort of
a as someone said earlier today a
kitchen sinks type approach is just take
a bunch of these different acquisition
strategies and throw them all at the
problem now we can only evaluate one
particular acquisition strategy at a
time and so what we do in what we did in
this scenario was so here we have our
posterior model and there the different
acquisition strategies we select one of
them based on its previous performance
and where its previous performance we
would hope is a predictor of how well it
will perform on on the function we're
interested in and and then we use that
to select a particular acquisition
function and actually evaluate the point
that's recommended by that one of the
interesting properties we found though
is that this will actually mix over
different acquisition strategies and it
doesn't necessarily even settle on a
particular one so in fact here I'm
showing for one single
run this mix of portfolios over
different acquisition strategies
generally the red one and the yellow one
tend it and this is the the basically
the probability at the width gives the
probability at each iteration that we
choose a particular strategy the red one
and the yellow one tended to perform
better on their own because they they
sort of did more exploration whereas the
green one was sort of more exploitative
however when we mixed over these
different strategies we tended to see
that the green one
still got picked with with a high
probability because now the entire
algorithm itself was mixing over this
exploration and exploitation and the
fact that the green one tended to
exploit more sort of wasn't to the
portfolio's detriment so now I wanted to
highlight a few sort of problems with
Bayesian optimization so in the model I
talked about earlier I I vaguely
mentioned that I was using a Gaussian
process prior over functions in order to
to select my points X that was my prior
over functions okay
so often however the posterior models
here have hyper parameters and whereas
with Gaussian process regression we can
often just fit these using the marginal
likelihood however that's really a bad
idea here simply because there's not
enough data so in the the example I had
right here if I just cancel that and run
it again at the beginning I really just
have one data point so trying to fit the
marginal likelihood as you can well
imagine with one data point is a
horrible idea that you shouldn't do so
one thing we can do is is put a hyper
prior over these prior parameters and
and sample them so when we sample the
hyper parameters of the model that we're
using one of the things is one of the
problems with that is that this can also
be costly so for GPS we
have to evaluate the marginal likelihood
and as we get more and more data points
that gets more and more costly so
another thing we can do is we can and
this is sort of one of the the problems
I wanted to highlight is that maybe we
can make some sort of approximations to
this so sort of with the the Fourier
features that were brought up earlier or
with the sparse GP approaches like the
fitzy approach that Snelson and and and
Zubin worked on or the variational
approaches that Lawrence and other have
done that might be a good way to do this
sort of approximation when we're
sampling bandit methods have also
suggested that maybe we not necessarily
that we want to overestimate our
uncertainty but that we want to make
sure that we don't underestimate it so
that we always get a over approximation
of this and that what whether that's the
right thing to do as well is another
open question so I alluded to this in
the previous slide and one of the things
I would really like to do is to be able
to take this algorithm and run it for a
million iterations but in in the
background here I have a Gaussian
process that I'm using to fit my model
and so this grows as we add more data so
getting a Gaussian process with a
million data points really is is not
feasible so these can obviously be
updated incrementally as we get one data
point at a time but if we sample the
hyper parameters that really gets thrown
out at every iteration so again the
question is in this sort of setting what
are the the sort of smart approximation
methods we can do to and how should
these feed into the the model that we're
working with here in order to get sort
of approximations to the sort of
expected improvement or something like
that and I'll skip over this but there
are some things we can say about the
convergence of these methods
you can definitely say that eventually a
Bayesian optimization method will
converge to the truth but that's really
not an interesting result because you
know it's purely asymptotic bandit
algorithms have sort of similar sort of
things that they can say and there are
some things that you can say about sort
of Bayesian bandits and Bayesian
optimization methods but definitely
there's some more work that needs to be
done there so I think I'm pretty much
out of time so I'll just leave this up
here that's sort of just a summary of
the interesting subproblems that I would
really hope someone solves and questions
if you have any</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>