<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Practical Statically-checked Deterministic Parallelism | Coder Coacher - Coaching Coders</title><meta content="Practical Statically-checked Deterministic Parallelism - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Practical Statically-checked Deterministic Parallelism</b></h2><h5 class="post__date">2016-08-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3dBcxLo6fKo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">she's fine you Oh guest / and um vines
to tellers a son one of our interests
some are worthless it was introduced a
few days ago so when you enjoyed your
sister can't be this weekend but not
tomorrow because you can't leave you to
that yes I'll be here in the morning
don't wanna catch this morning and the
rest of this afternoon wait okay no
holds bar office questions during the
talk yes please yes please so this is a
talk about the work we've been doing
over the last few years on statically
check to deterministic parallel
programming so the motivation for me
starts with the broader topic of
reproducibility this is why I care about
determinism so reproducibility is
important in many aspects of computing I
found that if you search github for
can't reproduce there are at least 40
3000 hits which are presumably from bug
reports that people cannot reproduce so
reproducibility in software engineering
is important it's also this is also
reflected in the fact that the Debian
project has invested some significant
effort in getting bitwise reproducible
builds but unfortunately this is a sort
of package by package trench warfare and
they've in spite of investing effort
only gotten to about eighty six percent
of packages that are a bit wise
reproducible so it seems a bit
unfortunate that there isn't some
underlying abstraction that ensures that
this number is always a hundred percent
rather than a package by package slog
one final example of where
reproducibility is important is of
course in science itself in the
computational sciences so in the context
context of computational science you
would expect that a scientist would be
able to in some day in the future may be
able to come across a plot either
produced by some cloud software they're
using or on somebody else's blog and
perhaps with one click be able to
download the program that corresponds to
that plot and reproduce it locally so
that's the world I would like to live in
and of course this means that the the
cloud in this case has to be able to
cough up the program P which actually
produces that plot given some input data
so for these kinds of batch processing
jobs the input data in the sciences
would typically be some scientific data
set that would be archived in some
long-term storage but there's still the
matter of controlling the execution of P
so that we get exactly the same pixels
in the plot that comes out in the end
and then the scientists can have
controlled reproduction of this
computational experiment so that they
didn't can modify it to experiment
further now controlling the execution of
this P really has two pieces to it we
have to control the environment and we
have to control the execution and that
execution can be controlled using
dynamic means or using static
programming language features so first
about the environment that our tools for
this have recently gotten quite a lot
better so they're not perfect but we can
achieve something like a deterministic
base image using docker or Nick's OS to
say here's the system image that I
expect this program to run in and
describe it in a declarative way and a
precise way but what about the execution
so the dynamic approach is to put p in a
sandbox and there are a number of
packages out there that so-called
deterministic multi-threading systems
like kendo or D threads which will
control the execution of these threads
at runtime so peek in this case be an
arbitrary x86 program let's say and
kendo or D threads can control the
interleaving of all the racy activities
on these threads so that gives you a
form of determinism there's this field
is kind of nascent and there are not
very good top to bottom software
solutions for this right now there are
some modified operating systems that
support this but it's it's still in its
beginnings and furthermore even the best
results in this area require a two to
four X slowdown in the execution of the
program in order to achieve determinism
but there's aside from performance
there's actually another problem with
dynamic enforcement that I want to point
to as a motivation for static
enforcement so if you think about it
because the deterministic threading
packages enforce deterministic thread
scheduling the thread the number of
threads becomes an input to the
semantics of effort so this program P is
now a function from inputs and number of
threads to outputs so you can reproduce
it on your machine as long as you run it
with the same number of threads that I
used on my machine if you're it is but
it would be nice if we could change
certain aspects of the environment like
how many threads we run on so we can run
it on a 72 core server or four core
laptop portably and get the same answer
in both cases so I would say that this
is a detriment to portability
that's we're really just an efficiency
thing I suppose you could fix pick some
maximum number of threads but then your
laptop might run out of memory trying to
run this on 128 threads for instance and
you know this of course other
architectural features slip in as well
you might have the GPU model the
processor model the OS version there are
lots of things that can slip in here as
as non-portable inputs to your program
and in the most extreme case of course
the scheduler trace itself this is what
we would call non determinism when you
depend on the exact schedule the OS uses
for scheduling threads but even then you
can still achieve reproducibility if
reproducibility is your only goal you
can use a tool like Mozilla RR or pin
play from Intel to record the exact
series of system calls and then replay
that exact execution but a trace of
system calls is not exactly a very humid
accessible form of a program for you to
download and use okay so the hope would
be to instead use static checking and
this is not a new idea of course fully
45 years ago there was this elegant
quote about this very topic it is
therefore very important that a high
level language designed for parallel
programming should provide complete
security against time-dependent errors
by means of a compile-time check so I
love this because it provides motivation
for our current work and I think this is
a great dream and this was the quote I
was wondering if you'd want to disavow
now but it seems that we haven't
achieved this goal yet we've done a
shabby job of implementing this dream
and why is that well the reason I would
point to is that of course the lion's
share of languages that we use have
shared mutable state as part of their as
part of their programming model and
while we do have an intellectual
foundation for achieving determinism in
spite of this we can use ownership types
permission types fractional permissions
we've got a lot of work over the last 15
years on separation logic so we
understand how to use one of these
framing rules to build the type system
of a programming language such as
deterministic parallel Java which was a
project from Illinois also concurrent
revisions at Microsoft Redmond
deterministic kerala java has a type
system that directly incorporates a
notion of region types and a disjoint
ennis relation between regions and in
the type system it can conclude that
writes the one region of the heap are
non-interfering
with rights to another region of the
heap if the two regions are disjoint so
this is all great but this idea doesn't
seem to have achieved great uptake yet
either and part of the problem may be
that this has a rather high annotation
burden not to mention the fact that it
requires strapping a totally new type
system on top of the existing Java
language which has its own adoption
hurdles so the main subject of the talk
today is to take a different approach so
not to start with a language with shared
mutable state but instead start with a
language where there is no shared
mutable state and try to add back some
of these structured safe side effects
that we can use for high performance
parallel programming but which we can
design by construction so as to retain
determinism and that's the main idea of
this talk so what do we pick as a
starting point for a language with
shared with no shared mutable state well
they're really two ideas that that I'm
familiar with in this area and one of
them is to use a data flow or message
passing type language and the other is
to start with a functional language or
functional subset of an existing
language so I've worked on both of these
ideas I did my thesis on the first one
working on a stream processing language
that was called wave scope and wave
scope it was a it was a standalone a new
domain specific language and so work on
it sort of span the stack from runtime
to applications and we did a lot of work
on the compiler which was a two-stage
heterogeneous metaprogramming compiler
and also on the runtime which executed
on embedded devices in the network such
as this custom platform that we built
for acoustic localization of animals
some of these platforms had no operating
system and so there was lots of fun with
compiling for embedded platforms there
so we also did a number of applications
with this kind of stream processing
language with no shared state between
the actors we did pothole detection in a
fleet of taxicabs around the Boston area
we also did some work on background
subtraction and images with tracking it
again with tracking animals in the wild
that's a marmot by the way in colorado
that's what we tracked with our acoustic
platform all right so I also heard from
Suresh Jonathan that they had run wave
script on
on a test fleet they have it Purdue of
wind turbines okay so I believe strongly
believe that this idea works works
rather well that we should have more
support for data flow languages and of
course data flow graphs of data
transforming operations with no shared
state is also the basis of a number of
very popular data parallel frameworks
recently whether it's fine grain for
vectorize programming coarse grain for
data center level parallelism like flume
java or something in between like the
new tensorflow system also these ideas
are very applicable to GPU programming
so graphs data flow graphs of data
transformations and we ourselves have
done some work in this area which I'm
only going to briefly site now because I
want to move on the main system we've
worked on in the GPU space is the
accelerate language which is an embedded
domain-specific language inside Haskell
and it's getting some usage out in the
world there's a startup company called
flow box that uses accelerate as its
main means of programming GPUs okay so
with that out of the way I wanted to
spend most of the talk talking about
this second idea so data flow and
message passing are great but there are
some reasons that we have to going
towards more general purpose programs
programs with arbitrary functions or
task parallelism so if we start with
functional parallelism as our baseline
and we want to add a structured set of
effects how can we do it well first of
all what do we mean by function
parallelism well this of course refers
to just the fact that in a functional
program any two sub expressions can be
executed in parallel so you can
visualize the evolving functional
program as a tree of potentially
parallel function calls now of course as
with most modern parallel languages we
use compiler hints to say where the
parallelism should go in the Haskell
implementation but this is no different
really from other parallel languages
such as silk plus which have a mechanism
for parallel function holes but in the
context of a purely functional language
there's a significant performance
opportunity cost when it comes to
writing this kind of parallel program
that is to say that you can't have any
crosstalk between threads so any
performance opportunity that depends on
modification to shared memory you can't
leverage that opportunity you likewise
lose the ability to do in place updated
data structure between threads within a
single thread functional programmers
have known how to do this Simon and
others we're doing
this at least as early as the early 90s
there's something called the st monad
which enables you to do in place single
threaded confrontation so you could do
for example a sequential in place sort
using the st monad but you can't expand
the st monad to cross parallel leaves of
this computation because that would
introduce immediately introduced
databases and non determinism and you
wouldn't be able to use this in the
functional portion of your program so
that's disallowed so our basic idea in
the series of work that I'm going to
tell you about today is that we
reintroduce a new monadic lease here
which we call the power monad so one of
these power sessions is much like an st
session it's a dischargeable effect in
the same sense that s T is a
dischargeable effect except it also has
a built-in notion of forking parallel
computations now what can these parallel
computations do to communicate with one
another well they can communicate
through channels we know that that
retains determinism from Khan's 1974
proof that con process networks are
deterministic we could also communicate
to ivars or single assignment variables
there's also a pretty long tradition of
single assignment languages going I
think back to Tesla in 1958 we've also
you can read about our extension to
program with eyebrows in Haskell Simon
had a paper with us in 2011 and Simon
Marlowe has described this in his book
on parallel programming in Haskell in
the last few years we've taken this a
little bit further and we've introduced
a new kind of shared variable that we
call L VARs for lattice variables and
the general idea of an l VAR is that it
captures some monotonically changing
shared state so this would be things
like sets that grow in size but don't
shrink with insert but not delete
counters that only grow but never
decrease anything that can be defined as
occupying some state space that forms a
lattice semi join lattice specifically
so we typically implement these kinds of
data structures especially sets and maps
using lock frickin current data
structures because of course they are
shared between threads that we need to
operate would perform insert operations
on different threads accessing the same
data so these locks freakin current data
structures evolve with commuting rights
during one of these parallel regions but
then at the end of the parallel region
there's an implicit barrier and after
that point these data structures are
frozen and appear
are just like any other purely
functional data structure to the
functional program that's using the par
mon adam alright so we've currently
implemented this in haskell you could
just as well start with any other purely
functional language or compiler enforce
pure subset for example the version of c
sharp and midori it's pretty close to
what you need for this here's what it
looks like in the haskell library you
can do a monadic computation which
creates a new l VAR of some type like a
set and then you can fork child
computations within which you can read
and mutate that variable concurrently
with the parent thread which is also
reading a mutating the same variable and
the essential game that we play is to
design the set of operations over these
variables so that this can never result
in non determinism and thus when we exit
this run par session when the monad
returns and we return a value whatever
value we achieve is as though it were
computed by a purely functional
computation so we've got kind of purely
functional semantics but uh but some
imperative mechanisms okay so that's
what we referred to is the power session
which matches the little shape in our
visualization to take a more concrete
example here's a shopping cart so if we
create a new map Alvar we could
asynchronously use one thread to insert
that there's one pair of shoes and on
another thread insert that there are two
books in the car and of course these two
inserts commute so the end result of
this computation is deterministic and in
fact if we adds the last thing in this
computation get the number of shoes we
don't even need to wait for the books to
be written to return this this
computation can return as soon as the
shoes are available because you know
that that part of the state doesn't
depend on the book part of the state so
what is the actual lattice that goes
with it this it looks like the following
so the least upper bound operation in
this lattice is the merger of two finite
maps and when we always start start off
at the bottom state that's the initial
state for our variable the top state by
convention needs error and we use it as
a runtime exception now depending on
which we evaluate first if we insert the
shoes first we're going to go to that
state before moving up to our final
state there but we could have very well
followed the other paths so the book
might have gotten inserted first and
then the
shoes which would have left us there so
clearly we end up at the same state at
the end of the computation and how do we
define this operation that reads out the
number of books without worrying about
the number of shoes well the semantics
of this is a little bit interesting
because you need to define what we call
a threshold set this bit that's in the
dashed box and this threshold set is any
subset of elements in the lattice that
are pairwise and compatible so their
least upper bound the least upper bound
of everything in that set must be top so
once you've established that you know
that you can return from this get
operation whenever you are at any state
in that threshold set or any state above
it because if you're in a state above it
such as the yellow arrow here you can
basically figure out which one of the
threshold set the threshold elements
you're above and that tells you what you
should return so this is how our
blocking reads work now there are a few
problems with this basic form of the
mechanism so you can't see the exact
contents of the cart which can be quite
frustrating you can't iterate over the
items in the cart you can't determine if
an item is not in the car so that is
asking a negative question and you can't
react to rights you weren't expecting
this simple insert and get interface is
very limited so those are all issues
that we addressed in some of our recent
work extending this basic model where we
add a notion of handlers quiescence and
freezing that i'll tell you about in the
next few slides so freezing is just what
people would normally call a read on a
mutable variable it's an exact
non-blocking read so it gives you the
current state of the variable at that
exact point in time which under normal
conditions is a non-deterministic
operation because the thing may be still
in flux and change it but the key here
is that freezing is a destructive read
where you actually mark the state of the
LVR as frozen and any subsequent
attempts to write to a frozen L bar
cause a right after freeze exception so
basically when you freeze you get the
exact value and issue the value was too
early there was still a right coming
then you throw an exception so a whole
program that's written with this
mechanism with this fine grained
freezing mechanism that allows you to
freeze individual alvars it basically
has two possible outcomes when you
run that program you either get a unique
final final answer or you get an
exception so this is a property that we
call Kwazii determinism because you can
never get the wrong answer you either
get the unique answer or you get an
exception little lung bun power thing so
you're going to intern the value even
before all its feds are finished it
means maybe some of them are later going
to light into the other would you wait
till all done you typically need to wait
till they're all done okay you neither
one flush out could leak atherton be use
right now I you can go inside these
power Mel things crackles launch the
missiles uh well our goal is not to only
treat deterministic program but just to
track the determinism level so you can
launch the missiles but then you can't
run power in pure code you have to use
run part I 0 which you can only run in
IO code a quazy determinism you have to
run it in Iowa Saudi exception that
would be bad mmm cool woman I yes that
would be sort of the full
non-deterministic mode where you launch
missiles in this quasi deterministic
mode you can use freeze but not launch
missiles okay and you still are forced
to Santa IO monad because this is a form
of non-determinism this quality to
Taylor's so the run part that gives you
back just the value that can be used in
cure code doesn't let use this phrase oh
but I argued that this is still an
interesting point in the space because
there are certain ways that quazy
determine is a little bit different than
determinism in particular it can support
recovery strategies so you can basically
have ways of retrying and controlling
the schedule to try again if you get a
freeze after put exception alright put
after freeze exception on me oh you can
be run the whole computation if you like
because the computation is deterministic
and you know what the starting point was
so yes that particular l bar on that
execution is frozen and we we have an
implement a fine-grain way to roll back
but but you can retry the whole thing
alright so anyway this is just an
interesting point in the space between
determinism and non determinism so we've
got determinism we've got Kwazii
determinism and then we've got data race
freedom which is weaker and then fully
unsafe code and our goal is to provide a
suite of mechanisms and a type system
that allows you to track exactly where
you are in the space and limits what you
do
accordingly ok so we've seen some
examples of these least upper bound
rights that move you up in the lattice
we've seen block threshold reads like
getting the number of shoes we haven't
yet seen an example of the handlers
quiescence and quiescent said he
mentioned although we saw an example of
fries so here's a here's an example for
handlers and quiescence what if we want
to do a breadth-first search on a graph
and we want to accumulate the nodes that
we've seen in a set l VAR so from some
starting point to do it we can insert
everything that we come to into our set
l bar and then recursively insert all
our neighbors in parallel so that works
fine and the set l VAR could accumulate
some of these inserts will be redundant
because you'll insert neighbors that are
already present in the set so one issue
here is how do you implement this when
you can't look at the contents of the
set l VAR other concurrent data
structures might allow you to check to
see what's in the set a check to see if
the neighbor is already in the set the
scene set but with elbows you can't do
that because that primitive mechanism of
checking whether something's absent
enables non determinism so how do you
write this in a statically deterministic
way well the way that we do it is we
enable a new notion of an event is an
update which actually changes the state
of an l VAR as distinct from an
introductory there which does not cause
an event and we say that event handlers
listen for these events and launch
callbacks when they occur so the
interesting thing is that this still
retains determinism because you don't
know who will insert seven and you don't
win they'll insert seven but and you
don't know if you're the first to the
second person to insert seven but as
long as you call the handler at least
once on seven you can keep the whole
result deterministic and then finally we
need an eight a way to tell when all of
this is done so what does it look like
in code well we create a scene set to
track the nodes that we visited we
attach a handler to the scene set and we
say for every new node for every new
element that appears in the set simply
take its neighbors and add them back
into the set by calling the insert
operation here right here so then we
kick it all off by just inserting the
start node and inserting that one node
will cause this handler to be called
again and again
it reaches a fixed point and we detect
that by acquiesce operation which
basically says when no more copies of
this handler running return and after
that point it's safe to call freeze so
this pattern of quiescent freezing is
safe this is a quazy deterministic
program i showed you because it uses the
freeze operation but we could also
achieve the same effect by just wrapping
it in a run par and depending on that
final global barrier to be the sort of
freeze of last resort if you will oh not
at all because each time you try the
only way can go around the loop is by
inserting a node that's already on the
set which does not cause the callback to
fire Oh No ah well yes events are
updates to change the elgar state
inserting something that's already there
does not change the of our state
therefore no handlers fired so that's
the that's the idea and so this
particular suite of mechanisms allows
you to stay deterministic handlers and
this definite events and we depend on
quite heavily for all of our data
structures is El Valle set them as a DP
to the elbows allows containing sense of
other things well it's also deeply built
it with some magical callbacks and
quiescence images well every data
structure to be implement has its own
implementation of callbacks so there's a
lot of when we ever win a particular
concurrent data structure like when we
have a seat rye or concurrent skip list
we've got some very data structure
specific mechanisms for implementing
these this API so that is certainly true
and in terms of it being magic or not
well the implementation of data
structures are trusted I'll get back to
this in a little while ok so the
implementations no there's not a no
guarantees about determinism or
quiescence or anything crap has to be
you know you just have to know what
you're doing for the log three data
structures yes we do a way to implement
a special class of l boards which of
these purell bars that are simply a box
containing a pure data structure and we
can give quite a few more assurances
there but when you drop down to the
level of implementing a lock free data
structure like a concurrent skip list
well verifying that is a much bigger
inter press and I'll get back to that
invariant ok so now I have told you a
little bit about the LVR abstraction and
in the second half of the talk I want to
zoom in
little bit more on the performance the
applications it's some more extensions
to that basic abstraction I've already
shown you and then some implementation
techniques so how do we implement these
data structures efficiently I mentioned
lock free implementations and also how
do we operate in a parallel contacts
we're also computing communicating over
the network and needing to serialize
these L bars ok so first of all what
kind of code actually gets faster when
we use alvars the kinds of things is in
targeting are purely functional programs
that have performance problems that are
keeping them from scale from parallel
scaling so we took a que si fa
implementation or control flow analysis
from Matthew money and the core of the
algorithm is essentially this this
search over this graph of abstract
States so at each point in time what we
do is we take the current set of
abstract States we've explored and we
expand each one to include the neighbors
and then we set Union all of those back
together so the original functional
program that we started with spends
quite a lot of its time constructing all
these data structures only to giving
them back together now unfortunately
none of the fusion frameworks I'm
familiar with of which the functional
programming community has developed many
are very useful for this kind of
scenario where you're doing operations
on nested collections so fold of a union
over a set of sets for example and I'd
love to hear otherwise if that's the
case so we couldn't think of a good way
to paralyze this or to speed it up using
the purely functional program but if
you're willing to have some effects you
can use the elbow formulation in which
you just have a much more imperative
program and you DeForest all those
intermediate states instead you just
create a single accumulator l VAR and
you do a parallel for loop which is
implemented those handlers by the way
you do a parallel for loop iterating
over there all the states and then
iterating over all the neighbors of all
the states and insert each one of those
into the final accumulator so you've
effectively deforested all these
intermediate sets that you were creating
only to Union back together so that
gives us a big speed up just from the
deforestation and then it also gets the
parallelism the limiting factors that
we're keeping us from achieving
effective perils and out of the way
namely the program on the top was
spinning all of its time futzing with
these data structures in a way
it was fine grained and not very
parallelizable the program on the bottom
gets a reasonable 8x parallel speed up
on a small typical workstation machine
alright so that's um that's sort of a
combined benefit of a wrap of 200 x uh
oh gosh this is a couple years ago and
I'm trying to remember so this was not
close to linear speed-up I think it was
a 12 or 16 core machine and we're only
getting a tech speedo something this is
not a tragedy closure business you go
I'm is merging over the state which
states right so this would be one step
this will be one step of the algorithm
oh so this doesn't have the automatic
and then will you add them you know that
you call a call a quiescent answers
right this is there's no part of that
right that's not going on here this is
just a reduce of a map yeah if you no
way to fuse that I'd love to see it like
full build or what have you or stream
fusion yeah but in case this is one way
to use it or to DeForest it okay so one
more example application from the
programming languages domain type check
it so all of these languages have the
bottom one is racket have recently
acquired quite a number of static type
checkers or gradually typed type systems
and there's a common feature of all the
languages that are on the slide which is
that all of them have a slowdown in type
checking because they have very large
union types combined with subtyping and
so racket for example the type of the
plus operation lowly plus operation
actually has hundreds of different cases
in a sum so as soon as you ask a
question of is a a subtype of B you can
have to check up to a hundred thousand
combinations of two of these types and
as you can imagine this makes things
quite slow there are files in the racket
repository that take five minutes to
type check and this is a significant
source of frustration so we don't make
this asymptotically faster but what we
do is ask if we can improve the constant
factors by paralyzing and deforesting
where possible and our recent paper from
this year does exactly that we show how
you can take the original type checker
in
type drackett and implement it so that
the streams of possible solutions are
deforested and instead are using a sort
of CPS style with these callbacks to
push possible solutions down to chain
without any allocation we also
introduced this concept of saturating L
bars so under certain circumstances we
can use L VARs to represent a type
variable under unification which after
all occupies a lattice and if we're
exploring multiple possible
substitutions some of them won't work so
we define a simple convention that
doesn't modify the theory where if you
have a notion that a l VAR can achieve
too much information and be broken where
it's basically a combination that won't
work then you can immediately free it
and reclaim memory and this is what we
call a saturating l bar we say that it
moves to a saturating state if it's
achieved too much information too many
constraints and has a contradiction in
it and so in that we manage to speed up
the type checking algorithm in type
bracket by a reasonable amount and we
also achieve that deforestation benefit
again although this in this case not for
the map data structure but instead for
these streams of solutions okay so um
one thing that we have to think
carefully about when we move into this
space of exploring search spaces using l
VAR s is that there are really quite a
few different relationships that the
parent and the child computation can
have the one that i've showed you so far
is the scenario where we have pure code
purely functional code that issues a run
par to run a monadic effect and then
discharge that effect so that's a
scenario where the parent is basically
the pure computation the child is this
monadic power complication but of course
also wherever we invoke a parallel call
within an already parallel region with
the async Combinator that i showed you
this also has a parent-child
relationship and it can be a quite an
interesting parent-child relationship in
this scenario where we're searching for
a valid basically accommodation to type
checks out of these tens of thousands of
possibilities we might want to have the
notion of a cancellable computation so
this is a parallel computation which you
might at some point decide that you
don't need for example if you're
computing a parallel and over two
computations as soon as one of them
returns false you can cancel the other
one so what are the semantic
requirements on what you can do within
that block
if you want to have the ability to
cancel it so this is exactly the kind of
thing we reflected our type system so
that we can establish a set of ground
rules about what affects you can be
where if you want to have to give
incapability like canceling a future or
like doing deadlock detection in a
future as Simon mentioned before with
with cycles in a graph there are some
scenarios where the easiest way to write
a graph algorithm is simply to run a
child computation to deadlocks under
certain conditions and then simply
detect that deadlock and we have a monad
transformer that does that in fact we
wrote a paper about how these different
effects combined together the memory
effects on the one hand and the control
effects on the other hand and what we do
is for the memory effects they give you
a coarse-grained approximation of what
kinds of things yorba natick computation
will do we introduce this e / ammeter to
Ramon add so we have power EA where a is
the return value and E is a little type
level product of different bullion's
that tell you that implement these
switches there they've got five switches
in the current implementation that say
basically do you do any least upper
bound write operations if you do that
switch is set on do you do any block and
get operations if you do that switch is
set on and normally most code you write
is polymorphic over these switches it
doesn't care but under certain
situations you need to restrict them so
in terms of these determinism levels
that i mentioned deterministic
non-deterministic Kwazii deterministic
if you tell me that you want to do
cancellation you have to add a monad
transformer this is one of our control
effects over on the right you add the
Monad transformer but you also tell me
that you want it to be deterministic if
you want to do cancellation but you
don't care about non determinism fine
you can have that choice but if you want
determinism then there's a particular
set of constraints on these switches in
particular it has to be a read-only
computation so the right switch is
turned off and the bump switch is turned
off bump is a special form right that's
non idempotent least upper bound
increasing a counter is the canonical
example okay so that's one example of an
interaction between these control
effects and the memory effects one of
the things that's fun that was fun about
this work was just identifying
interesting combinations of control
effects and memory effects especially
because these aren't the kind of usual
effects that we think about all the time
like exceptions in state but they're
rather more esoteric
specific to parallel programming so let
me let me show you some of those
combinations that I think are
interesting so here's our parent and our
child computation once again the parents
the caller spawning a parallel child
computation so let's say to be a
cancellable future so we better be
running in that cancel t-mo dad
transformer but we also have some extra
observations that we can make here I
mentioned memoization on the right here
is one of the control effects so
memoization means creating tables which
are simply memo tables for pure
functions now one of the interesting
things about memo tables is that they're
stateful in the sense that they
accumulate results which can speed up
future computations but that's that
State fulness is effectively invisible
to all clients right if you isolate
exceptions so you can have a cancellable
computation which is read-only but which
actually updates a memo table and this
is a trick that we use in the type
checking scenario because you can
actually do useful unification work and
it can be stored in the memo table even
though you're cancellable and even
though you want to retain determinism so
that was a fun combo that we found also
there's saturating l VAR as i mentioned
those are the ones that can get too much
information and break well there's a
funny property that if the if the type
of your sub computation enforces that
the only affect your child computation
has is to write to a saturated Alvar
well then if the elbow in fact saturates
you can have no future useful effect
that's visible in the outside world
because the one thing you were
outputting to has now gotten into a
broken state and so then you can be
cancelled immediately so this is another
one of those combos if you write to only
a single variable variable saturates you
can be cancelled deadlock is a little
bit of an interesting one because it's
one of the few scenarios where we're not
allowed to do read operations from the
child computation if your goal is to
detect deadlock operations then doing a
read within the child on any
pre-existing state from the parent it
would be hard impossible to distinguish
temporarily blocking on some parent
state via blocking read verses true
deadlock so you can't do reads but you
can't do rights it's ok for the deadlock
the deadlock detection computation will
run and we'll deterministically deadlock
or deterministic
complete and whatever rights it does
will be a deterministic function of his
execution so it's okay for the deadlock
detection scenario to allow the child to
write to state owned by the parent so
these are sort of fun interactions that
sit at the boundary of our different
mode ads for control effects okay I want
to switch gears and show you a little
more on the application front so see for
quite a lot of combinations as a
programmer might be a bit daunted by
yeah so kind of asheboro he wants some
sort of simple story that kind of always
works is that there's may be interesting
for some implementation point you've got
the programming story you expecting to
expose yes so all all of these are
basically uh each is Combinator with a
with a hairy type so when you want to
use one of these you have to sort of
select exactly that you have to yeah
yeah and the only thing I'll say about
that is that the types do grow quite
hairy but inference is not a good job so
far and so we can have reasonable terms
but pretty ugly types so for example if
you want to use this one you call a
Combinator it gives you a little escape
hatch there's a lot of for all esas hang
out here we're effectively doing region
typing using the st trick and so the
parent region is a different region in
the child region and what you get is you
get a little lift function that's passed
to your child and it's your escape hatch
so if you want to modify things in the
parent you call the lift function to
compute to sort of cast your right
effect to the parent so yes this is
quite manual compared to if you build
these features directly into a language
I'm sure you could go quite a bit
further and making it economic verb
anomic the child quite restrict which of
these so if the child uses something
then it puts you in private places short
sure yeah if the child has a right in it
then it's a perimeter will be fixed so
that it has right it won't be
polymorphic in that switch and and then
if you try to use it under a cancellable
you'll get a type error and then you
have the choice of either doing
something else or gear shifting down to
non determinism but at least it lets you
know that this combination of things
you've tried to do is not deterministic
so if you want to do it you have to own
up to it and move it to the IO monad
that's the idea all right so I want to
tell you a little story about an
application that is implementing this
work with my wife is a biologist so this
is a application for phylogenetic and
the it does a few different things it's
called five Ben it's for exploring
different phylogenetic trees because
sometimes you end up with quite a few of
these existing software will produce
like a thousand different trees which is
hard to deal with so what it does the
main thing it does is it creates a
distance matrix telling you what the
edit distance between all of the
thousand trees you just got are so this
is just an example of an application
that turns out to be monotonic and a
little bit of an anecdote to support the
claim that monotonic applications
actually are hide the wild perhaps more
than you might expect so what is that is
in this application do it computes tree
edit distance between pairs of trees
triet this interesting fact can be
computed by treating every interior node
in the tree as a by partition of the
leaf nodes and then simply counting how
many by partitions are differ between
one tree and another you have a set of
by partitions I have a set of pi
partitions the set difference between
those by perdition's is our tree at a
distance that's kind of fun and so how
do you compute the tree at a distance
well the hash RF algorithm is to take a
set of trees traverse over all the trees
and then for each tree traverse all of
its by partitions that is traversal of
its intermediate nodes and then
basically count up which tree ids
exhibit which by partitions so this is
something I'd like to call attention to
because not only do we have a regular
data structures like sets and maps but
we have unusual key and value types so
this is a this is a map whose key type
is AB I partition of tree of IDs and
whose value type is a set of integers so
that's kind of interesting and then well
this is a two-phase algorithm you have
to have a barrier after the first phase
and you iterate over every bucket in the
thing on the left and you ask ok this by
partition was in tree too and also entry
3 so there's no additional distance
between tree 2 and 3 but there is a
digital distance between trees one and
two and one and three so you do this
little N squared operation and you
increment the distance matrix so this
distance matrix is built
x + 1 operations one at a time in
parallel so naturally this is a kind of
fetch and add operation and this is a
two dimensional array of counter alvars
so we've got a map elva on the left
we've got a counter elva on the right
with non item potent bump operations
we've got a barrier in between the two
because we need to use a freeze
operation or a run part than freeze to
be able to exactly read the contents of
the thing on the left and build up the
thing on the right so that's how this
application works that's how 5in works
it's quite fast compared to most of the
things biologists we're actually using
for this dendro by phi lip and so we've
got a few biologists that are that are
using it now and I think I have some
maintenance work to go do on it as we
speak but that's one example of
monotonicity in the wild now so far
we've been talking about these alvars
which accumulate information
monotonically but there's there's also
of course scenarios where you want to do
non-monotonic rights to different parts
of the heap that are disjoint and
therefore the rights are not interfering
so DPJ of course and systems like it
specialize in making this safe whereas l
VAR specialize in things like reductions
where you intentionally want
interference between computations but
there's no reason that we can't combine
both of these flavors and in elvish we
do we provide a separate mechanism that
you can use to do disjoint rights to
different regions of the heat so for
example if you want to do a parallel
merge sort well what is the fastest
possible or that you could have written
in haskell before this work well merge
sort of course is to divide and conquer
algorithm and each time you do a
recursion you're getting sub arrays back
if you write the entire algorithm out of
place this is going to be a pretty slow
sort contrary to some claims now Haskell
has long had the ability to move
subcomittee when you bottom out to
sequential typical parallel sort will
always eventually bottom out to a
sequential sort and when you bottom out
to will sort you can do it in place just
like you would do it and see or any
imperative language so that's great but
there's actually an ugly thing about
this picture which is that you still
have some number of phases where you're
doing it out of place and those phases
where you're doing it out of place are
at the top of the tree when the thing is
biggest and it's bigger than l2 cache
size so this creates a significant
slowdown the sequential version in place
but the parallel version out of place is
not good enough and you can see that in
some so for a benchmark suite of l VAR
applications all of which have no shared
mutable state so they're just using
alvars the one that stood out is slow
was sorting because sorting couldn't
make any good use of in place update to
state across threads so it didn't have
very good scaling on this 12 core
machine so how do we fix that well I
want to do is we somehow want to create
a parallel session that encompasses all
of these parallel subroutine calls and
makes it safe to to sort different
pieces of the same right so I still want
to do su zesty the st monad to do the
heavy lifting in the end where I
actually want to sort the sequential
array but I want to make it safe for
those sequential array slices that I
sort to be physically located within a
single global array that I started with
so I don't want to do copying operations
that's fine but then avoid data races
and how do we avoid non-determinism AKA
how do we ensure this region disjoint
this property that is disjoint from the
region on the right so there's been
quite a lot of work on program analyses
and on type systems that enable you to
determine this non-interference property
but here we actually use a much simpler
strategy so rather than in the alias
freedom of a program we simply preserve
the alias freedom so we only give you
tools that preserve alias freedom when
you're working with mutable state so
this is something like one of these
boxes where you're not allowed to ever
directly touch the mutable state
yourself you're always operating indoor
indirectly through some safe mechanism
and so that means that we need to define
a grammar of safe actions that you can
execute on mutable state of you embed
these two SD guys home safely in the
inside the power session that's right
so okay so these of these SP guys you
can run them in a relation using all
become higher up in this thing very easy
Riley so I made me skip to step a little
bit so if it you've got this vector you
want to run an SD company to the left
half of the right half those are both
pieces of the same object so that's ok
we have slicing operations to do
constant time slicing of an array in
data dot vector for instance in Haskell
oh but how do you know that those things
don't aliens I see so so so the other
part of the same step yeah well they're
part of the same physical heap object
never you never have the same location
accessible between two different st
connotations so so so much my point was
either so it's a it's a case where you
could not play high rank R honesty no
it's not good enough because you need
some way to subdivide the other way and
incur lesson yeah but the trick that we
use is not that complicated so basically
what we say is you're in a state monad
where your state is a mutable entity and
there are no aliases that mutable
entities so you're it's sort of an ISO
isolated reference you basically got the
only copy of that and then here are some
things you can do to that mutable entity
they're safe you can create a new one
it's always safe to create an array it
doesn't have a lease with anything you
can duplicate an array I that's all we
say if it makes a copy of the memory so
it doesn't introduce aliasing and you
can also split the array so split in the
middle split into even and odd elements
you can define whatever grammar safe
splits you want the trusted library
writer in this case defines what is safe
and hands a set of safe combinators to
the user really physically just allows
you to have you know it's 01 yeah well
okay at least if you're splitting at the
left or right if you split it even and
odd elements it's a little messy but but
yeah for a typical split where you split
at a position it's just doing an 01
operation to create a new point to the
left half point of the right half and
the way we do arrays that's all they're
always balance checked so you can't use
your point of the left half to sneak
over and access the right half Booker's
with vans jacket all right so that's
basically it for these safe grammar of
alias freedom
preserving actions the the only wrinkle
here is you do need to be able to handle
for example tuples of arrays we need two
arrays to do merge sort so this could
get if you had a lot of different
mutable data types in your algorithm
this could get very clunky because you
need to sort of treat it as one mutable
object and issue these commands against
it like duplicate and split but what we
find is that if you already can offload
most of your state to either immutable
state which can be read anywhere or
monotonic state which can be read by L
VARs so your reductions or whatnot could
happen in alvars the state that you
actually need to update destructively is
often very simple for an algorithm
there's often only one data structure
that needs to be updated destructively
so that's why it I think it's fairly
ergonomic to use this kind of statement
add approach and that's what we use to
fix our performance to bend our curve
back up for a parallel merge sort and
here's our parallel merge sort compared
against the same algorithm implemented
in silk and then also implemented in
deterministic parallel Java so we do
okay there alright any questions on that
before I do the last leg here oh sorry
these are all normalized against silk so
they start lower than silk yes yeah so
silk starts at what everybody else
starts at some fraction of silk so worse
than self better than DPJ is the short
story yeah and this is on a normal
hotspot JVM dpj doesn't have any runtime
overhead it's just a type system okay
last segment unless there are any other
questions this is the implementation bit
where we get a little beneath the hood
so there are two implementation
techniques I want to briefly tell you
about before we wrap up one of them is
about these concurrent data structures
themselves so I mentioned using walk
free data structures that now there's a
reason that we haven't simply replaced
all of our sequential data structures
even though dotnet and the JVM have
great libraries of concurrent data
structures we didn't simply throw out
all the other ones and that's because
there are some disadvantages to
concurrent data structures I think of
them something like this they
they are sort of slow to get going
they're often more expensive to allocate
if you have a lot of small data
structures you typically don't want
luxury concurrent data structures but
once you really get going once you have
a lot of contended access and a big data
structure then you're going to win with
a lock free data structure so here's
just a little evidence to support that
so for example concurrent skip list map
inside the JVM it's slower to allocate
than then a pure data structure there's
a pure data structures for Java as well
everything I'm showing you in this
series of slides is implemented in
Haskell and Java just for an extra point
of comparison it's a pure data structure
in the context of something like L VARs
it really means if you have an only a
single mutable location because l VAR is
still mutable but you can always use
this age-old trick of having a single
reference mutable reference pointing to
a pure data structure which is quite a
handy thing to have you can operate on
the entire data structure atomically it
has a number of great advantages but it
necessarily is unscalable because
there's only a single mutable memory
location the different threads are going
to have to contend over so this is the
data structure that I want if you've
just allocated it if it's uncontained
'add if you want to take a snapshot of
it there are many reasons i want this
data structure on the left but there are
also many reasons that i want one of
these full-sized fully featured
concurrent skip skip list type data
structures on the on the right and how
do we know which one to pick well one
option is certainly to pass the decision
on to the user we can implement data lvr
skip list and data elvir map and we do
but that is a little bit unsatisfactory
because we'd love to just expose a
single default map data structure that
does the right thing in most scenarios
and furthermore sometimes you can't
statically decide because you have code
locations you might have for example a
nested collection of collections some of
which are hotly accessed some of which
are cold in which case there is no
static decision as to which of these two
data structures you should use so the
simple thing that we've been exploring
is how do you take this a and B data
structure and adapt between them
dynamically so this was the top
of Ric one of our ICP papers last year
we design an algorithm for detecting
contention and transitioning from an a
state to a transitional state gradually
copying over in the background the old
data structure to the new one and then
eventually committing the transition
from the a/b state to the B state and
garbage collecting everything on the
left so the result is a data structure
that performs mostly like the a data
structure on the single threaded use
case or uncontained add use case and
mostly like the concurrent data
structure on the multi-threaded
contended use case we also proved some
guarantees about this the resulting
hybrid algorithm in spite of doing these
internal transformations retains lock
freedom and it's important for that
proof that the starting data structure
actually be a pure immutable data
structure likewise we proved linearize
ability and correct set semantics for
the for the hybrid data structure
assuming those things already hold of
the underlying a and b data structures
so we compared the hybrid versus the
non-hybrid in both java and in haskell
and the upshot is basically that the
hybrid data structure can be twice as
fast as the concurrent one when you're
on one thread but it can be way faster
than the non-concurrent one when you go
to 16 threads do you do you guarantee
that it is contended it will eventually
get to be a B case or is it just that
you will keep backing out if you fail
yeah that's actually interesting so the
way we formalize it we guarantee that
some method will complete in bounded
time and we actually have to guarantee
that it will it will transition because
we formulate transition is one of our
methods that has to complete within
bounded time so the scheduler chose only
to execute the transition method that
would be the only method that we
complete so the short answer is yes sir
ah yes um convention sorry the hybrid is
twice as fast as the concurrent data
structure on one thread because it stays
in the a state it never goes to the B
state if it's one thread yeah yes the
non scalable data structure does
terribly if you're hammering it from 16
threads I see to the high boots I is
better than the bad case on one phone
and is better than the bad case on exact
many things exactly it's a compromise um
all right oh gosh I can switch another
slide deck with that but I don't have
the I don't have the slide for that
right now it's on the order of ten or
twenty percent i think all right second
implementation technique and then we'll
wrap up once you've run one of these
parallel sessions you get back a regular
immutable data structure you get back a
frozen Alvar and when we want to take
this to use it at a distributed
programming context it's important to
send these data structures over the
network but as I said we're focusing on
irregular data structures like sets and
maps so they tend to consist of many
heap objects a graph of heap objects
rather than just a big unboxed array for
example so what we would really like to
do is arrange it so that all those heap
objects can be allocated together in one
region of the heap so that we can send
them over the network together and this
is our other ICP paper from last year
where we worked on something we call
compact normal forms which are a form of
region-based allocation and basically we
fact that the Glasgow Haskell compiler
is already using a block structured heap
so it's pretty easy for us to construct
a new kind of heat block that model is
one of these compact regions and
furthermore we leverage immutability
because we know that if we pack all of
these red objects into that compact
region and we know that the transitive
closure of all pointers will stay in
that compact region that invariant will
never be broken in the future because
there's no mutation that will come along
and point to outside of the region so we
leverage those advantages in this
scenario to expose a very simple program
or API for working with compact regions
so there's NC region types exposed in
the type system this is just
purely a runtime technique so the nice
thing about this is that lets you send
these data structures over the network
much more quickly than if you have to
serialize in deserialize them and that's
actually a little bit surprising because
it's in spite of the fact that you use
many more bites you use up to twice two
to four times as many bytes that you're
actually sending over the network but
the network is very fast such as we ran
it on a quad band infiniband network
then it still comes out much faster to
skip the serialization and send the in
heap representation directly over the
network likewise if you're loading large
data sets from disk for example a month
of Twitter data was one benchmark we did
in this paper this is going to take
several gigabytes loading it from disk
would be 21 x faster compared to the
best serialization libraries we have
available this would binary i believe
it's 21 x faster to just in map the
compact normal form than it is to
deserialize from disk so what we do is
we build a transparent caching layer
where for any file on disk that you
parse you can just transparently cash it
on some temp file system in this
internal representation and then in map
that in especially if you're doing
random access into the middle of this
Twitter data set rather than consuming
the whole thing then it can be 100x
faster for for random access than a
serialization approach all right that's
the LA show you wrap bud here I wanted
to turn that the main thrust that I'm
are the main thrust of this argument is
the determinism should be a safety
property and should be treated like type
safety and in fact in the context of
these Haskell libraries it is a matter
of time safety so there are different
ways that we can ensure this safety
property we can rip by construction as
we've done with our basic design of
threading and synchronization and
barriers likewise with our Combinator's
for retaining alias freedom for for our
st beautiful state between threads so we
can ensure this property by construction
in some cases we can trust but we don't
want to trust the end user we can still
try to minimize the trusted code base
for our library writer we've got this
very important distinction between
different parties here because we want
to be able to run untrusted code
potentially that we've gotten over the
network
and know that the type that our type
checker signs it and the determine is a
level therein will hold so right now
we're trusting quite a lot because we're
trusting RL of our implementations and
there's sort of no bound to how many of
these you need because you need to port
as many data structures as you need
unfortunately it's a rather long term
project to get any full verification of
those concurrent data structures
especially against weak memory models so
that's smart people are working on that
and it's a it's an ongoing effort
eventually maybe we'll get a verified
see try or verified concurrent skip list
in there finally there are a couple
little tidbits which I haven't
emphasized in this presentation which
are actually holes in our argument so
there are ways that you can take our
existing elvish library and get it to
behaved on deterministically because
there are little tiny bits of the users
code that we unfortunately trust and
these are things like associativity the
associativity of operators that you use
in a reduction or a critical one is when
we're doing all of those inserts to
shared sets the key types that you use
for inserting into a shared map or set
they need to have a total or ordering
function they need have a true total
order and likewise their equality needs
to be commutative if you have a broken
EQ or or instance and this is a haskell
specific type class then you can leak
non determinism because even though the
application should be deterministic it
depends on these commuting inserts and
the specific set of comparisons on keys
from run to run is going to be
nondeterministic so right now that's a
hole and we're trying to to work on
patching it we've currently got a
project to use the proof extensions of
liquid haskell to try to prove these
properties and basically squeeze out our
last little bit of trusted code from the
user so the at all alright so that's it
today and I'm happy to take any further
questions you have
you go straight down to raw relax memory
code some of these algorithms look
beautifully white free and yeah that's
right code turn out to be rather be at
arias in relax memory i yes we do um and
we mostly trust other people to get the
algorithms right for particular data
structures for hat for high-level
haskell code we've got another project
that's looking at making sequential
consistency the memory model for Haskell
but that's for i/o computation and
really everything here we talked about
is outside of the IO monad so we want to
have it we want the memory model to not
appear to not because of Lotus but yes
our implementations do have to deal with
relax memory so that would be down
inside the implementations of all these
data structures mainly yeah now
interestingly i find that um you know
most concurrent blog for data structures
say they go you don't ever use rights
they only use compare and swaps and
reads so in some sense especially on x86
TSO which is what we've been targeting
so far the scenario is somewhat simpler
we've outcomes with no rights in
compared to one has a sense of course
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>