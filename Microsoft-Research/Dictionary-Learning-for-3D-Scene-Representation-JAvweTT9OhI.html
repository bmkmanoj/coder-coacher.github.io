<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Dictionary Learning for 3D Scene Representation | Coder Coacher - Coaching Coders</title><meta content="Dictionary Learning for 3D Scene Representation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Dictionary Learning for 3D Scene Representation</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JAvweTT9OhI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright welcome everyone uh thanks for
coming today our ego and I have the
distinct pleasure of hosting Ivana
tajuk's from Rico innovations in menlo
park california Ivana got her PhD I
think around 2009 in from EPFL spent a
few years after that at Berkeley at the
Redwood Center for theoretical
neuroscience and then after leaving
their went to Rico so we're very excited
to hear what she's got to tell us Thanks
thank you for thank you for the
introduction and for the invitation to
come here it's a great place pleasure
first time for me at Microsoft Research
here so I am currently with Rico
innovations but all that I'm going to
talk about today I've done during my
postdoc at the Redwood center so I'm
going to talk about dictionary learning
for 3d scene representation and I will
explain during the talk what dictionary
learning is so basically the whole idea
and the whole go of my of this
particular working of my research is
what are the best representations for 3d
scenes and how to acquire them and how
for from that images and information
that we acquired how we build those
representations so this is a joint work
with Brunel's housing he was my postdoc
advisor at UC Berkeley and Jack
Culpepper who was a PhD student at UC
Berkeley and now he is with IQ engines
also in berkeley and sarah drew is who
is who was the postdoc at berkeley in
the math department and now she's with
deutsche telekom in germany so before
going into 3d representations I want to
give you a little bit of motivation and
go back to the effect to see how we
actually capture the information about
3d scenes so on there are many ways to
capture and
here I show two most used ways to
capture one is if we have a network of
cameras that is capturing a 3d scene and
we can have multiple views of that scene
from different angles another way is to
use hybrid image and depth sensors so
this is a time of white cum camera from
PMD German company and it captures depth
information and i will usually show
depth images depth map in a color map in
color where red represents closer and
further is blue and gets also here
intensity map or reflectance map of the
object or Microsoft Kinect where you can
have the depth map and you have the
color image the Turk or registered so in
both ways of capturing we have rich 3d
visual information and it's has many
applications in 3d television in
surveillance robotics exploration and
just to name a few but what is worth of
pointer pointing out is that depth or
disparity is central to both these
approaches of capturing 3d scenes so
here the depth is contained implicitly
in just by the parallax between
different views and here we have
explicit measurements of depth like
distance from the sensor and we need
that depth information too if you want
to interpolate in between the distant
views so of course we cannot have view
of the currently with the current
technologies we cannot have views from
every single point in space so you can
assemble that space of camera positions
and then interpolate in between and
extrapolate but there are problems with
that and many problem is its
measurements are unreliable
so we get usually very noisy data and
here I'm showing you an example of from
a time of light camera this is the
intensity the reflectance image and this
is the depth image and if you zoom in
you can see that there are a lot of kind
of salt and pepper type of noise this is
much different than the noise type of
noise we see in regular images or we can
have from laser range scanners this is a
tree in front of a pillar and you can
see that there is errors in acquisition
around the boundaries of the object
objects and this is from structured
light sensors where you have missing
pixels where you have occlusions between
two views inna in a scene so the bottom
line is that for different type of depth
sensors we need algorithms to do
denoising remove the noise or in
painting too in paint where there is
missing information and of course these
are both ill-posed inverse problems and
solve them we need some prior
information about the data that we are
reconstructing so for depth and for this
to solve it we actually need good
representations and the idea of using
representations solve inverse problems
is not new it's been used for decades in
image processing and fourier bases have
been used for representing images
wavelets and I in the last decade we
have seen a lot of people using over
complete reflection Aires where you have
a much larger number of elements in your
basis then you have even is the
dimensionality of your signal and here
I'm just just shown an example of a
dictionary that's been learned from
a database of images so this elements of
the dictionary that we usually call
atoms if you actor eyes them and put
them in a large matrix you have a big
fat metrics and to reconstruct the
signal here I'm showing an image patch
back from Lana you need to multiply it
with the long with a tall vector and if
you want to do this really efficiently
you can say well I want this factor to
be sparse to have some small number of
nonzero components and if we want now
let's go back to our problem of noisy
images so what we will observe is not
exactly this factor of the signal but
something with the added noise so then
the node denoising problem in if we are
using representations becomes estimating
this vector a vector of coefficients to
reconstruct our original signal and in
the noising people for images usually
assume either stationary Gaussian noise
or poisson distribution of the of the
noise but it's usually for the Gaussian
it's usually considered to be the same
distribution of the same variance of the
Gaussian through the whole image and for
Poisson it's also linked to the to the
intensity of the image so pretty well
known models for images but what about
depth so in depth we have seen that the
noise is usually spatially varying so
let's all call non-stationary but it's
not in time it's non stationary
throughout the whole image and we can
also add the time dimension but for this
dark i'm just going to keep on still
image is still depth maps but also a
bigger difference is that the statistics
of depth maps differ from the statistics
of images so just by looking at that
North you can see well there is
less texture than we have in images and
we have much more sharp and oriented
edges so let's say let's see if we want
to represent with two de-noising with
whele trash holding simplest
representation we get a lot of ringing
artifacts around the edges all i can say
fine that's that's great we know that
because wavelets are dogging our way
boats are not really optimal we can use
curve lets the better represent
boundaries so we already get some better
results or we can just say well we're
not going to use representations we are
just going to use some work some local
filtering or not local and this is where
the non-local means de-noising and this
is yes this is all done on that only on
depth so you can see well this is a
great result but if you want to zoom in
into bit of details you can see that you
lost some details here this is a part of
my hair and smooth it out a lot of the
information in the depth maps so we have
looked at this problem and said well
let's see if we can learn sparse
representations of that so not just
start from representations that exist
for images like wavelets or curve let's
but let's learn them from the data so in
the rest of the talk i will first go
briefly through sparse representations
just the background and please interrupt
me at any time if you want
clarifications or anything i suppose
that people have already heard has
anyone heard about sparse representation
is there someone who is not familiar
good and we can go quickly through that
and then the two parts of the talk will
be learning space representations for
depth only and then the second is in
your work learning representations
jointly on images and depth so in
intensity in depth and for both of these
I will show you how we model the data
how we learn the dictionaries and what
do we get results on inverse problems
okay so um I so sparse representation is
are also leaner representations like the
you know transform coding of wavelets
and curve lights but the difference is
now that we have this big dictionary
which is over complete has a much larger
number of elements of basis functions or
atoms than the dimension of the signal
so our coefficient is a long vector and
to reconstruct it is then becomes a
combinatorial problem so it's not easy
to find these coefficients and we have
an infinite solution set so what people
have proposed is to look for a sparse
solution and there have been some sub
sub optimal algorithms like the matching
procedure or the basis pursuit
de-noising and I will just quickly
explain the basis procedure noising in
the next slide because that's the type
of algorithms that we use and then
another bigger problem is well great we
can get the coefficients but how do we
find this big dictionary like nobody's
going to give that to us so for that
people have developed dictionary
learning or they're called also sparse
coding methods just to from given data
to learn what is the best dictionary for
that date okay
so this is just one slide to go over how
we find this bar solutions once we are
given a dictionary and then we'll go
into the dictionary learning problem so
if we want to pose our sparse as we want
to find this partial solution to our
construction problem basically we want
to minimize the LZ rope to the norm of
our coefficient vector so you have the
smallest number of nonzero entries and
this is np-hard under this is under a
quadratic constraint that the
approximation is bounded by some error
so what people then propose is that
let's do let's relax and l1 norm to an
l0 norm to an l1 norm and if you have
that is our objective function and keep
our quadratic constraint which is a
convex and it can also be formulated as
unconstrained and the advantage of
course it this is convex and easy to
optimize so that is the basis proceed
they're noisy and introduced by Jen down
on saunders in 99 so great so once we
have the dictionary we know how to find
a vector of coefficients it's not
necessarily the same solution as the l0
known but its most of the cases it's a
pretty good approximation then of course
you wonder why why would we use sparsity
and it turns out that this is a really
good prior in solving inverse problems
such as de-noising and in painting
because it gives a good generative model
of the images and of i'm just showing
here an example again on the line I on
the images but I will show you that from
apps in just a second this is the noise
using sparse representation in
translation invariant while with frames
so just by adding over completeness to
our representation we can get better
results than for example just using
plain wavelet threshold so just a simple
example showing that
over completeness already gives us
something and sparsity as well but what
we can also do it's really important
that we can adapt this dictionary to
signal statistics so we don't have to
use this translation invariant wavelet
reims we can learn our own and this is
sugar you see that you're going in the
noise is this incidence
so at this point I'm just saying telling
you the background which is learning the
signal the statistics of the signal
let's say you have no noise in your
signal or the noise is actually subsumed
in the approximation error so he said
I'm going to find a signal model up to
this approximation error that Valerie
presents missing but for depth i will
show you just a sec so this is so people
have previously proposed dictionary
learning or sparse coding and it was
first proposed by a thousand and feel in
97 this is the maximum likelihood
learning where they said well we'll have
a linear image model and for the
learning we're going to maximize the log
likelihood that natural images arise
from the model above given that's
dictionary so they posed as this
optimization problem to maximize the log
likelihood over the set of dictionaries
and they have used images for training
from this van haren image database of
natural images and it's supposed and
then okay
we have a dictionary after but before I
show you what they obtained as a
dictionary I'll show you how they solved
actually this problem because now they
had a model of the signal to put into
the model so they uncoupled the
conditional probability for all the
images given the dictionary and the
coefficients times the prior on the
coefficients so this is important to put
the prior on the coefficients because
that's where we put the sparsity
assumption on the coefficients so if you
put this prior to be a core taluk
distribution which is tightly picked at
0 and have it tailed and this one is
just a Gaussian distribution if the
noise is assumed to be Gaussian it turns
out that this our objective function
that this optimization problem can be
cast as equivalently as minimizing the
energy where energy has this form we
have the quadratic approximation error
plus the l1 norm lambda times the l1
norm of the coefficients so you can see
that this is exactly the same objective
that we that is for basis precision
noising except except here we have
unknowns which are both the dictionary
and the coefficients so to solve it they
proposed a two-step alternating
optimization so they have a big set of
natural image patches and there they
initialize the dictionary randomly and
in the first step they fixed that
dictionary and minimize the objective
over the sparse coefficients so basis
preceding noising again then they fix in
the next step in the learning step they
fix the sparse coefficients and minimize
the energy over the dictionary so that's
the learning step basically it's just
taking you can do it do simple gradients
and so you take gradient steps in the
coefficients and then the new dictionary
and coefficient the dictionary until you
converge so and starting from a
completely random dictionary they
learned a dictionary that has oriented
bandpass filters that are Gabor like so
they look a lot like Garbo Gobber
wavelets and this was just learned from
the statistics of the date this was an
important result actually in
computational neuroscience because they
showed that this sparse coding can be
also principle for encoding information
in the brain because this emerged
dictionary looked a lot like the
receptive fields of neurons in the
primary visual cortex everyone not just
you know the fact that they are been
passed and gabor like but also
distribution in orientation and in
frequency okay so that's for the
background is do you have any questions
so far because from now on we're just
going to build upon okay so then we
looked into how to learn these
dictionaries for depth and we had a
challenge so how can we learn it such
that we are robustly spatially varying
noise so we have to learn from noisy
data and the noise is not anymore
stationary Gaussian noise that was
assumed previously in the dictionary
learning so what we propose to do is to
add another so we had the linear model
dictionary times the coefficients and we
have the approximation error and then we
can put that to be stationary but we
have another type of error here which is
just a jeweled to the acquisition device
and we model it as a multivariate
Gaussian so we wanted to be general here
and say well let's assume that each
pixel has a Gaussian noise but the
variance of that noise can vary along
them
some pixels can be more noisy some
pixels can be less noisy and we are
going to infer those statistics are with
also inferring inferring the sparse
coefficients so we assumed a
multivariate Gaussian noise and this is
its covariance matrix and we assume that
these are the noise is not correlated
between the pixels so that's one of the
limitations of the mom so what's how
does this change our graphical model for
the images so we still have the sparse
coefficients we have the dictionary and
then we have different noise variables
added to the each pixel in the depth map
so this is just to highlight the
differences with respect to the beggar
to the basic dictionary learning sparse
coding method so these are the new
variables the new noise that will do we
have added just to be able to deal with
different noise in depth maps ok
how does this change our learning
objective where you can still use the
maximum likelihood and now when we break
down our likelihood we have three terms
this is this term is the Gaussian
distribution of our approximation noise
so this is fine and then we have the
prior on our coefficients we can put
that as a laplacian and a prior on our
covariance matrix so I'm prior on our
noise so then for the prior on the noise
we decided to do a non informative
Jeffrey's prior just to not put any
structure on the noise to stay as
general as we can and so now with what
is the difference it is with respect to
the regular objective in dictionary
learning is that we have another set of
variables that we need to infer and then
we have one other distribution and one
other parameters here okay and then the
energy function is also slightly
modified so here we have the sparsity
term but which remains the same but in
this term we have the log of the
variance which is due to our Jeffrey's
prior and then the error at each pixel
is divided by the value two times the
variance out of the noisy that pixel so
these are also unknowns that we have to
find so it's pretty a simple formulation
and it can be also modified if you know
some specifics of your noise variance
how it's related to the signal you can
put here as well mm-hmm seems there is
there a couple common models I've seen
there are variants in depth images one
is that it varies square of the depth
the other is that it it very fitted for
a long lines of a plus on
the variance is the is
brightness yes so but this doesn't seem
to be motivated no really by either of
those no so hmm we wanted to keep it
more general as general as we can so we
can adapt it to not to take that more
specific relations into account but we
just wanted to keep it as broad as
possible so it's not always related just
to depth that you can use it in other
scenarios to but if you if you have that
kind of deterministic relation you can
easily put it and it's going to simplify
you know finding your variance if it's
related and you don't have to infer it
anymore you can just relate to the thing
so it's safe proportional to
yeah I guess this is the death you can
still use yes you can still use the same
objective is just that this I then
become harder to optimize yeah yeah
and you can also change the prior this
turns out we wanted also to infer this
so that we know the reliability how
reliable is it that's estimates so
basically we wanted to see for each
measurement of the depth how reliable is
and how well it fits to the signal model
so it not necessarily always linked to
the acquisition device but you know
occlusions or some some other noise
lambda multiplied are there is some sort
of like Rajic yeah can you be can it be
spatially varying as well let's say I
have my depth map and I know exactly
where the bad measurements are mm-hmm so
can that can that be taking to account
well well well actually basically a lot
especially pairing of all does it make
does it make the solving the the complex
protein powder would so what you can do
is you know use these solvers that
actually find your lambda while it's
finding your solution for each batch so
it finds the best lambda for each batch
so it's already you know you can already
use a solver that deals with that
problem or you if you know how to
estimate one double yourself then you
can put it there and especially girlie
does not work
oh you would well you cannot bear it
here for each pixel you varied for each
patch in the image yes yes it might be
possible to do it per pixel I think
there is some work on that but I can't
really remember right now but yes you
can oh you can vary it / coefficient you
yes the work that I've seen I can point
you to the reference is change you can
change different lambda for four
different coefficients they call it kind
of like the scale model but that's kind
of different with your thing is is that
your model assumes that voice
kiss vision
soon sofas in pixel the scene changes
voice cursing switch huge
um so right now I'm just doing static
depth maps not it's not a video depth
map all just a dick here just still that
maps if you do it image bring a frame
per frame you can just find different
variants for each of the pixels in
different frames or you can constrain it
to smoothly change from one frame to
another but it's different the point is
that it's different per pixel
so now how do we solve this how do we
learn the dictionary so we have two
parts again we have inference first so
we need to find all the coefficients and
we need to find the variance for each
noise brit pixel and we solve that in a
iterative manner so we first initialize
our Sigma's to be equal and very large
value so we start from hypothesis that
all the pixels are unreliable and then
we find we infer the coefficients then
we fix the coefficients and solve for
the variance and this has a closed-form
solution so usually it takes couple just
couple of iterations between these two
to find the solution to between for the
aids and the Sigma's and then once we
have that we solve for the dictionary
and this is just to show that it's
averaged over all the dictionary and all
the examples in our image based data set
so it's the same similar principle we
have info inference in learning we
iterate is just here in the inference
that we also find the variance of noise
per pixel
okay Sigma I square that equation goes
to zero please fi-
yes
the time with that
Oh
no because we have this Sigma 0 term
which is added to it it will if this
also the Sigma
as a sigma is at bottoms or this 0
yes some good point so basically this to
this Sigma I we add I put it here to be
more just a simpler the Sigma I has the
plus the Sigma 0 squared which is this
approximation so yeah basically sigma i
is the sum of the Sigma X plus this
sorry yes Sigma 0 is constant good
that's a good just for like either first
presentation basically this Sigma hat is
Sigma squared plus the Sigma 0 squared
that's how we limit it that's why it's
here we don't let it go- because if it
goes into negative a program 0
so when I minus that is big then you are
sorry
when was when the when the difference
between F I and I had is
basically them
to denominate large same as
yes so chewy just ignore it exactly so
if the if the sig if the measurement
doesn't really fit the model and just
isolate it it doesn't fit then it
reduces that term and says well this is
not reliable measurement because it
doesn't really fit while our model
okay suyo we have feature in here inside
nobody so here what is happily ever see
me when I thought of featuring yourself
also is a variable yes yes so excellent
that's the lorry yes so here we fix
dictionary we start from a random and
then here we learn it we optimize over
the dictionary so there's a lot of birds
here thankfully just have some boys and
see it so please be equity position
working at kennesaw conditions
dictionary and awesome awesome or seems
like it's video determining fault yes
and that's exactly just invitations
always converge I because of the combats
so each step so because the objective is
convex each step is convex but it's not
necessarily the whole objective because
it has no sorry I never gotten an on
conversion dictionary unless you don't
unless you have really small number of
trains are you running one dictionary
per image no a one dictionary for a
database of images and I thought you
were doing this for y is 0 because as
you assume that i'll ask me what is
before is the most 10 per pixel it's in
what you meant by c ya ya know it
changes for image to image so if you use
different seem different so then they
seem different it was one with different
you won't need one same person so that
means you have to just keep it sing
about four different scenes yes that's
why we infer Sigma for each pixel for
you seem pretty much yes per pixel for
pixel image yes please per pixel per
image
so yeah and here's a wee Everidge for
the diction so this is done for each
batch and then because it's independent
you can just too sparse calling for it
bad but then here for learning step we
averaged over all the patches over all
images position of the patches is just
regular it's random randomly chosen
problem yes that take a lot of images
and then you randomly chose patch so I
can show you this is from the middlebury
benchmark we took all the day of the
depth maps and I think we had something
like 30 depth maps and we extracted 16
by 16 patches of 16 by 16 pixels these
are some examples and so here most of
other the irregularities are the
occlusions or the seclusion so these are
either missing pixels and we don't mark
them as missing pixels we were just let
our algorithm see if it can figure out
that is non reliable date so you should
have learned in the dictionary and i
will show you here in grayscale because
it's I think it's easier to see and what
we saw is that we have a lot of oriented
eight address they are a bit different
than the for the images they're kind of
more sharper edges and we got some
couple of slants like here it's you we
think that this kind are kind of used
for the the ground because usually in
this depth maps the ground is kind of
tilted it looks because the sensor is
looking that way and this can be x
positive or negative coefficients so it
doesn't really matter if it's white in
the front or black in the back but the
biggest conclusion is that it is
different than the dictionaries people
use for images so
we have tried to see well how better do
these dictionaries do and we have done
that on the non add annoying dusk so
this is just a mathematical formulation
of a denoising tasks so India noising we
now have our learn to dictionary and we
have our coefficient oh and we have our
noisy our noise images are here so we
want to reconstruct like that the noise
the image F hat so what we want need to
do is to infer both the coefficients and
the covariance matrix of the noise for
that patch so here dictionary is fixed
so it's just our inference step is the
same and so I'll show you here example
for these offers real depth maps but
synthetic noise just to be able to see
to have the ground truth and to compare
against it so this is an original depth
map this is obtained with a laser range
scanner this is from the young and pervs
database approves database and we have
added non stationary Gaussian noise we
have corrupted 1% of the pixels in depth
maps and the variance was randomly
chosen for each of the pixels and we
have done total variation denoising and
got you know result that smooth it out
some part but still left some some of
the noise on the noise the curve light
trash holding gave also some pretty bad
results non-local means give nice smooth
results but was thought of the details
this is a trees forest in the back and
this is the result we obtained with our
non-stationary a sparse coding and
Sebastian compare different algorithms
like these of course
I test ratios so we try to be fair to
Justin yes I tuned the like for total
variation the regularizer the lambda i
chose to chose the best one for
non-local means and for Carla yes so I'm
trying to understand what exactly going
on here and I feel that exactly you must
be trying to learn the dictionary and
the condition that
so in the literature of parameter
estimation there's a lot of work out
destination
where you can do all kind of things too
and from the way your threshold it seems
that what you're doing is basically just
take away our liars in the past
well we also estimate something about
those out outliers not just you're not
just trash holding them and saying these
aren't useless you are also well I have
reliability measures doing that because
when you when it's greater than Sigma 0
your bicycle racer
because you are here
seconds from knowing its smaller unix
smaller than single is that right
us there
minus Sigma 0
and the top was also
date has been thrown away
if it's below Sigma 0 yes yes if it's
absolute in the approximation error will
unclear about it we're all Sigma 0 then
the you are using they
it's about seconds there
just
no it's just appropriately weighted you
waiting it's stressful did when he
previously it's right here right so yeah
if it's on the top condition then it's
zero which is Sigma Z also if you're
waiting and we'll pick so in the same
way 0 plus additional one the second
time when you put it there because the
problem is Sigma I square plus 60 so
you're not with this
one-half fi- at my head square so when
the top is also if I Madison
waste a lot of constants Illinois
oh I oh I see what you've been um no
because when you are because it further
in the first step here you are inferring
a and this sigmas are from the previous
iteration so you're never dividing with
the same this is you say you solve it
after and then you go back
in the robust estimation you can say I
threshold i get rid of ten percent
Alaska private jet we infer the outliers
and I search
seems to me very
well I will show you on an example so we
get per pixel some estimates of the
variance and I will show you how they
look like so they are not trash holding
there is some variability in them so
this is just similar showing the results
averaged over different images in the
same perv database and this is the
non-stationary is sparse coding result
that performs much better than the other
ones and this is just we corrupt it with
a different number of bad pixels and so
I think maybe here you can see what we
get from the data so this is from laser
range data the one that I showed before
and this is the original when we put
into the non-stationary sparse coding we
get an estimate and then we get the
reconstruction and then we also get per
pixel the inferred variance so you can
see that it's it has different values
for different pixel and then you can use
also this data to if you want to do
further processing of this you can use
this data to kind of put a real ability
measure on each pixel and this is what
you get from non-local means so you also
remove some of the noise but you smooth
out the some small details like here
here and this noise which is basically
correlated noise we were not able to
denoise because it doesn't fall under
the assumptions of the model that the
noise is not correlate so this is the
limitation of this algorithm but the I
think the best is that the best
properties that it gives us both the
noise the image and the noise that map
and inferred variance
seems to be mostly known roses yeah so
we can get some of the correlated so
these are correlated along the edges so
they are not just single pixel outliers
but when it have has a lot of
correlation then it has problems and we
have we have done that also on the time
of flight data so this is the original
depth and this is the noise then I can
just hope it doesn't zoom in basically
we removed all these pixels and then we
kept some of the fine details of the
depth map and we can also do in painting
just an example from connect depth maps
again we can in paint well around the
edges here but when we have big pieces
of the missing information which is
bigger than our patch size we cannot
fill it in 16 by 16
you can learn it larger but then you
need bigger machines
we learned it was just complete you can
also make it more over complete and it
will be better so it was 256 elements
can you somehow be done hierarchically
where you start with smaller batch sizes
and then where you get it right let's
get them for the bigger one you just get
the area where it's bad I think yes and
I think some people have done in a kind
of like a multiresolution or dictionary
learning yes there is there's also work
on that yeah that's a good point ok so
on I will go now through the second part
and it will be a bit quicker but stopped
me please if you need some more
information so here we decided to go
step further and see if we can learn
multi-model representations of intensity
plus depth so it's pretty easy to see
that there is a lot of correlation
between the intensity reflectance in
decision time of I cameras and depth or
the intensity and disparity in
structured light so here we need to
model representations basically we need
our dictionary atoms to have two parts
their intensity part and their depth
part and I let me quickly tell you why
some very simple models that you can
think of will not work on this too
simple example so this is just synthetic
just illustration if you have a like a
3d edge this of the observer the hybrid
sensor have an image piecewise
continuity and depth again or another
example if you have a slanted surface
with some texture on it let's say a sign
rating if you look at it limit will be a
chirp and then the Deaf will be just a
gradient and what's you you can say well
let me just put on top and we put
intensity on top of the depth and put
each atom to have its intensity and
death part just put one vector on top of
another and multiply with the same
coefficient so in the example 1 you will
have you can do that but what will
happen is that because you have the same
coefficients proof for both they may
intensity and depth you will have to put
the variability in terms of the contrast
within the atoms which we will lead to a
combinatorial explosion of the
dictionary so we cannot do this text
model what people then have proposed is
the what they have a common dictionary
model people refer to it as joint
sparsity well you have the same
dictionary and two different
coefficients and here that would be
great we just put like a hard wavelet or
step function and multiply with
different coefficients and that will
account for the contrast difference
however in the second example we won't
be able to use this model because we
cannot have one atom that can represent
both a slant in a chart so the
conclusion is that we need a model that
has different atoms and different
coefficients but still model the
correlation between two modalities you
can see getting cold in the invite
missing the value to the context
in the best
who's there
this is also done
it is you can do that but then you have
a problem of phase warp or wrapping
right
it has been I can point you to work has
been done for videos not for that but
it's like optical flow type what we
proposed is to have this like a sparse
model for image space model for depth
and then just have coupling variables
that multiply with the magnitudes to
give the coefficients so this ideally
binary but we can relax to be between 0
and 1 and they just say well if it's
zero we are turning off both intensity
and depth if it's one we are turning
them on so at the end we want to have
sparsity on this coupling variables and
you can put that into a system model so
you have atoms for intensity atoms for
depth these are the magnitudes of the
coefficients for a for intensity and dep
and these are the coupling variables and
then you can have different noise in the
intensity and depth so then becomes a
problem how do we then do inference we
can so if you look at it put it as an
optimization problem we want to minimize
the l1 norm of our capital a coupling
variables so since there are positive
dishes of the Sun and then we have a set
of constraints this is the quadratic
constraint representation of the
intensity and here I'm just doing for
the stationary type of Gaussian noise
it's another step further to do it for
the non stationary and the same for
depth and let's just say that our
magnitudes are bounded by certain values
they can be big whatever so this problem
is by linear so we cannot optimize it in
a straightforward way but just doing a
simple change of variables where we put
that a is M stems X and B similar we can
get a convex and it's a second-order
cone program and we can use all
existing solvers to find your
coefficient so this optimization problem
gives us the values for X and for the
eighth for the a and B and equivalently
for the magnitude so we named it joint
basis for suit because it jointly finds
the intensity and depth so for each
patch for each pair of intensity and
depth we get different X variables so X
is and ms are all hidden variables they
are estimated for each image and only
the dictionary these are the parameters
that are estimated for the whole
database so then again in a similar way
we can have a two-stepping iterative
process to learn the dictionary we you
know initialize it randomly and then do
the joint basis proceed to find the
sparse coefficient vectors and X as well
and then use that in the in the learning
just to learn the dictionaries and
iterate between these two steps so very
similar and we have learned it again on
the Middlebury benchmark database we
used now we used both intensity and
depth and we have learned the twice-over
complete dictionary and here I show in
each of these little panels an atom that
has its image intensity part and
disparity part or the depth part and
just by looking at it we can see that
there is coinciding edges which we would
expect because an edge in 3d would
induce an edge both in intensity and
depth and there is a texture we saw that
there is sometimes we have a slant in
depth and a texture in other image so
this type of atoms which are a bit more
rare than the
but they still exist and we can also
plot a scatter plot where on this axis
I've evaluated a gradient angle of a
depth Adam so basically this is the
gradient angle the normal if you would
see it as a slant it will be like on
normal on the surface and it's a texture
part which I just fit it which I to my
Gabor and found its orientation so if
you do a scatter plot and here it point
corresponds to an atom in the dictionary
you can see that there is a big diagonal
structure a lot of points around the
diagonal basically there is a 90-degree
angle between the gradient and the Gabor
orientation and just for comparison I
didn't show it here we did a similar
experience with experiment with the
group lasso algorithm and it gave us
completely you only formally distributed
it isn't that all give us the same arm
the same tendency of the correlation
between these two so this was this was
an interesting result because all of
people have looked into the statistics
of intensity and depth and they found
out closer objects and to be more have
to have higher illuminance and higher
intensity values but this was to the
best of my knowledge the first time that
we have shown that there is except exist
some type of correlation between the
gradient angle of the depth and the
orientation of the texture and of course
we have done some in painting
experiments just to show that if we have
dictionaries that are learned both on
intensity and depth we can get slightly
better in painting results than just
adapting dictionary here we removed
ninety percent of the pixels from the
depth map and here remove 96% just see
how far we can go and just from four
percent of the pixels
the intensity we can reconstruct this
depth map which is still blurry and it's
not perfect but it's really from four
percent of the data it's just me yes
that's really missing pixels per million
both Italian and you know just the depth
basically we are relying on intensity to
give us it would be a good experiment to
try you know removing intensity and
keeping depth and removing both at the
same or removing both a different
relationship it's a prior so try if you
see a gradient or oriented texture in
the intensity it will try to fit a slant
or if it finds an edge and soph I'll try
to fit an edge in the depth saying why
that's blurry
that's this is for missing I mean if it
weren't missing it yeah it'll be blurry
so I think that's blurry because these
models are essentially a leaner so
they're kind of you know you're
completely combining this atom so if
you're you know you're bad at estimating
one you're just removing some of them
it's not like a layered model in a scene
for depth where you would nicely
represent edges
are you is lambda turned up so high Oh
sparser um so here because we have used
this up this solver we don't even put
the lambda in so it's found it finds the
best lambda to solve it for the group
last so I this comparison with the group
class so I just ran it for a bunch of
lamb doesn't it took the best one if you
have a you know if you turn up lambda it
will be smoother for the group last but
here you don't have that option to turn
it assumes that energy because your past
to do some evolution yes oh let's do
let's do it's overlapping patches but
it's it's a linear model yeah so your
parish so 16 by 16
these are 12 by 12 yeah we had to reduce
them because now it becomes the
dimensionality of the problem increases
just the entire dictionary yes yeah
so 288
how many images do
this was like all the middlebury that I
could download so i think it's around 30
intensity depth pairs depth images but
within each image you take random
patches which are 12 by 12 so you have
you have a lot of data I'll be nose yeah
I've been um this is kind of the data
said that I found they can do this type
of training gone I'm not aware of any
other database that I can can use maybe
just maybe from the from the MPEG yes it
was your next slide here in front of
relationship we have the angles of death
and
you think that same relationship a hold
of you just looked at raw hatches from
me from the depth and intensity rather
than
that's a good question i don't know
let's add something worth checking yeah
I suppose you know people would have
already you know looked at something
like that but yeah I know that we tried
it I can show you I have one slide I
think it's this one so this is a plot
with the group lasso and it's completely
different if you do dictionary learning
with reply so you don't get that I can
maybe also show you this so I said that
some people have already looked into the
relation between luminance and how close
the objects are and I have you know
looked at the same thing for my atoms
and it turns out that you know in Adams
also we get in a bit closer if you look
at it like closer and further then you
can you also get a closer objects appear
brighter so it's pretty dominance over
here is the bright surface is closer and
histogram in the red one is when the
darker surface is closed so we got the
same tendency that people have already
observed
yes clear if you if you didn't have even
any dead pixels in etobicoke region you
couldn't contain fat for there's no
propagation from neighboring cells you
would have to learn larger dictionaries
if you have a whole yeah if you don't
have any data you don't have a system
Laura's in bigger patches or doctors in
a bigger number of passion bigger
patches so it so if you if you don't
have any data your f your observations
are empty set so you don't have anything
to be blogging order that so what we do
is usually average like we're shifting
patches and then averaging so if you
don't do that there you can have the
blocking art yes I so I think on this on
this one's I did I moved every two
pixels and then average not every single
pixel
so yes so just I liked always to
conclude with these representations of
day of David Marr just not to show you
where where I think we are with the
sparse representations right now so his
he put these representations going in
from the pixel representations to kind
of edges and blobs in images to two and
a half D representations when you take
into account the local surface of in
orientation and discontinuities tabs etc
and these are all kind of viewers
entered representations and these are
mostly what we have worked on so far so
what I presented today is mostly in this
block and to go to through 3d
representations that are objects entered
its it's still part of future work for
sparse representation so i think that's
that's it thank you yeah so please if
you have any further questions this work
I say I I can I have prior knowledge of
my more
for example most of my patches are like
locally planner or know exactly I most
of my study and have exciting news I
have low very very kind of surfaces with
that adding adding my an extra terms
with the minimization problem that
basically forces that somehow that help
or is it not necessary all my is my
training set just I don't need up so so
this is a completely like nonparametric
learning so we are learning pixel per
pixel for each image path so if we have
twelve by twelve image patch we have 144
parameters per pet if you know that you
have surfaces and you say well i'm just
going to learn a normal to the surface
that will completely define my surface
and that will have three parameters
right so you can reduce the number of
parameters I don't know how your
objective function will look like it
will be comfortable convex or not but
you definitely are you are definitely
reducing the number of parameters
these prayers should be scaling
significant you get closer to subject or
further away
the dig
hi so there are people who have you know
try to do like a multiscale do like
larger objects of smaller objects I have
not seen much difference in the way the
dictionaries look like so there was a
the toe by 12 could be scaled any size
oh yes tires were hits and goods like if
you had a patch that cut No tobeco patch
that had no depth data you can apply
your entire dictionary but scaled out so
that you're basically reusing the same
cars but on a larger scale
well you can just you know you're
filtering basically the higher frequency
from your data right if you're just
upscaling
you won't be able to reconstruct higher
frequency information from sure but you
will still be able to in pain things
that had in the ashes that were
completely devoid sakali i look at the
region with her depth data you could
have had that using the same picture
it's basically Tanana to shrinking the
original image applying your algorithm
and it's going back there so then you
could
virgins
yes it's
good things to try good baby okay thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>