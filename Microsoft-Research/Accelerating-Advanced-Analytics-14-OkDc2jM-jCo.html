<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Accelerating Advanced Analytics [1/4] | Coder Coacher - Coaching Coders</title><meta content="Accelerating Advanced Analytics [1/4] - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Accelerating Advanced Analytics [1/4]</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OkDc2jM-jCo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay thank you everyone for coming it is
my great pleasure to introduce arun
kumar from the university of wisconsin
where he is co advised by jeff norton
and jignesh patel arun works on the
intersection of data management and
machine learning with the stress on data
management and the research he has
produced should be interesting for
various reasons I for one it's highly
decorated we were talking about a sigma
best paper award in 14 it's also been
highly controversial so so his paper
submissions to database conferences have
caused all sorts of very strong
reactions from the reviewers and it's
all too practical so various pieces of
code that are based on his ideas are
actually now shipping with number of
different products and make sure to ask
him about all of this but the most
important tidbit that I want to leave
you with is what few people know is that
Arun is actually a member of this
American Screen Actors Guild and has
modeled and starred in commercials and
again feel like one commercial I'm
exaggerating slightly but he is he has a
man of various talents so if this
database stuff doesn't work out
Hollywood is opportunity with that I'll
hand it over to our rooms oniisan and
Amazon car commercial okay great thanks
Christian and thanks to everyone for
coming to the talk and all the people
who are viewing it remotely and as you
mentioned my name is Arun Kumar and I'm
from dynasty of Wisconsin today i'll
talk to you about my research on
accelerating advanced analytics i can
talk to you offline about the commercial
so advanced analytics is becoming
ubiquitous and for our purposes i define
advanced analytics as the analysis of
potentially large and complex data sets
using statistical machine learning or ml
techniques so basically it's the coming
together of these worlds of data
management and machine learning and I
approach this intersection from a data
management standpoint we all see
advanced analytics in action practically
every
day and I don't need to preach to the
choir here so everyone here already
knows about this every time you get
ranked results from Google's of search
engine the output is because of advanced
analytics every time Netflix recommends
a movie to you that is advanced
analytics in action and the whole world
saw IBM's Watson system defeat human
champions in the jeopardy question
answering contest Watson is powered by
advanced analytics the high-profile
success highly visible success of these
applications of advanced analytics has
led to an enormous demand in the
enterprise domains and again most of you
are probably already familiar with this
health care retail insurance finance
even the academic domains the sciences
and the humanities for products that
make it easier for these users to
integrate advanced analytics into their
applications market research firms
estimate that the market size for
advanced analytics products is set to
grow to anywhere between six to twenty
nine billion dollars per year over the
next few years and no wonder then that
practically all major data management
and analysis companies want a slice of
the spy there are also a slew of
startups in the space and open source
tool kits to integrate advanced
analytics into their applications in
spite of all this as I'm going to
explain today they still remain numerous
bottlenecks in the end-to-end process of
building and deploying advanced
analytics applications and at the end of
the day my research is about
abstractions algorithms and systems that
mitigate these bottlenecks in the
entrant process from a data management
standpoint thus accelerating advanced
analytics and the acceleration I mean
both system efficiency the running time
of the systems and the algorithms
involved and human efficiency the
productivity of the data scientists and
other humans that are involved in the
process I'll start with a high-level
overview of my research which is largely
been in the context of two groups of
humans in the advanced analytics
lifecycle data scientists who are tasked
with building and deploying machine
learning models that analyze data and
behind the scenes we have the software
engineers at companies such as Google
Microsoft IBM Oracle who are tasked with
building implementations of these ml
techniques on top of data processing
systems such as relational database
Systems spark Hadoop and so on from
conversations with data scientists
engineers and others at various settings
we found that there are numerous
bottlenecks in their work in throughout
the lifecycle of advanced analytics we
wrote about some of these bottlenecks in
an ACM Q magazine article that was
invited to the communications of the ACM
in 2013 and my work has largely focused
on three classes of bottle next the
first one arises when data scientists
have to build ml models and that is the
process of feature engineering
transforming the raw data into an ml
ready feature vector there's a lot of
work that goes into that space and some
of my work has explored different
bottlenecks in the future engineering
space the second class of bottlenecks
arise when a male models that have been
built have to be integrated into
production software and production
systems and the third class of
bottlenecks that I've that I've looked
at are from the perspective of software
engineers who have to build ml toolkits
that scale these algorithms larger
amounts of data the sheer diversity of
these ml techniques makes it a tedious
and daunting process for them to build
these two kids and there are some
bottlenecks that I've addressed in that
space to be more precise in terms of the
publications that focus on these
bottlenecks in the context of feature
engineering I have worked on two main
bottlenecks the first one is when
features arrive from multiple tables
which requires joins of tables before ml
can be applied applied this has been in
the context of projects Orion which
appeared in Sigma 15 and project Hamlet
which is said to be appear in sigmoid
later this year the other bottleneck
that I looked at was the process of
exploratory subset selection where data
scientists are often involved in the
loop in picking which features they want
to use for the emir task often they use
environment such as are for this process
and we focused on elevating this process
to a declarative level by introducing a
domain-specific language for feature
selection in the our environment and
applying database style and ml cell
optimizations to improve the runtime
performance of this process and increase
the productivity of the data scientist
that is project columbus that appeared
in sigma 2014
in the context of integrating ml models
into production I have looked at
probabilistic graphical models for
optical character recognition or OCR
data the data scientists want to query
in conjunction with structured data in
our DBMS s that is Project staccato that
appear in vldb 2012 and finally in the
context of helping software engineers
build scalable ml implementations it
took a step towards devising a unified
software abstraction and architecture
for in our DBMS implementations of ml
techniques that is project bismarck that
appeared in sigma 2012 to speak of the
impact of some of my research as
christian already mentioned briefly
project bismarck decode and all the
ideas have been shaped by numerous
companies including greenplum which is
now emc oracle and cloud era and we also
contributed the code often bismarck
project to the madlib open-source
analytics library staccato is being used
by projects and digital humanities and
Sciences columbus won the best paper
award at sigma as he mentioned and ryan
and hamlet i'm currently speaking to a
product group within microsoft the
online security they keep changing their
names they will call the universal
safety platform but Robert McCann is the
person and integrating some of these
ideas into the scope ml environment on
top of Cosmos and they are exploring
integrating that with their ml pipelines
and also with logic blocks which is a
database company that wants to deploy
some of these ideas on top of their
production analytics platforms for
today's stock I'm going to focus only on
this first bottleneck of applying ml /
drawings of multi table data and I will
dive deeper into projects Ryan and
hamlet I picked this particular
bottleneck primarily because of my how
it because it I think it best
illustrates my earlier point about the
coming together of the worlds of data
management and machine learning how it
gives rise to new problems and
opportunities and also the sheer number
of open research questions that this
work has led to yes so you keep saying I
here at least three or four projects
right so is this in context of one
system is there a reason why
everything's it
project well the Orion and Hamlet has
been in the context of one system
environment the Bismarck staff has been
in the context of an under system
environment but in terms of data
platforms a lot of this has been a
prototype on top of say postgres equal
or hive in Hadoop so from a system
perspective a lot of these ideas are on
top of the same data platforms but I
call I consider them system because I
look at like end-to-end stack like from
the direction all the way to the
execution perspective okay so here's the
outline for the rest of the top today
I'll dive into ml / joints and then I'll
talk about my future research ideas a
bit of a deeper outline for ml / joints
I'll motivate the problem and set up an
example that I'll use as a running
example for the rest of the talk and
dive into projects Orien and Hamlet so
let's start with a little bit of
background normalized data all of us
here probably know normalized data but
just to set up the example and introduce
the terms many structured data sets in
the real world or multi table for
example consider a data scientist that's
dealing with data at an insurance
company like American family they have
data what customers which is a table
that contains attributes about customers
the gender age income employer and so on
they also have data about employers
companies universities and other
organizations containing data about
companies like state where's the
headquarters what's the revenue and so
on notice that the employer ID of the
employer is an attribute in the customer
table that connects these two tables in
database para lines it's also called a
foreign key and the employer ID is what
is called a primary key also called just
the key in the employers table if we
want to get some information about the
employer of a customer we need to do
this fundamental relational operation
known as a join that basically stitches
these two records together based on what
the employer ID is data of this form and
key foreign key joints are not specific
to insurance they are ubiquitous to
arise and practically all domains
whether structured data they arise in
recommendation systems where you have
data about ratings being joined with
users and movies they arise in online
security they arise in hospitality
industry they arise even in
bioinformatics so now that you know
everything about databases let
introduce some example and set up the
terms for machine learning here's the
same customers data set a very common
machine learning tasks that are you this
used in the enterprise setting is
customer churn prediction basically data
scientists want to use machine learning
to model the customers to help prevent
customers from moving to a competitor
and they start with these attributes
about customers which which become the
features for your machine learning
algorithm the age the employer and
income and so on now they start with the
training data set to train the ML
algorithm and they have to predict the
churn attribute which is based on past
customers that have turned or not and
that is also known as the target or the
class label and the job of the data
scientist is to build a machine learning
model using this training data set sail
adjusted regression model or an SVM or a
neural network and so on now here's the
twist that causes trouble in this
paradise the employer ID is a foreign
key that refers to a separate table
about employers ml meets the world of
normalized data given a setting of this
kind data scientists view these
attributes about the employers as
potentially more features for their
algorithm and the intuition could be
that maybe if your customers employed by
a rich corporation based in Washington
they're less likely to churn and so they
basically want to get all these features
which basically forces them to do key
foreign key joints of these two tables
in order to gather all these features
for the ML algorithm unfortunately
almost all major ml tickets today expect
single table inputs for their training
process this forces these data
scientists to do what I call ml after
joints so what is this ml after joins
and what is the problem here's an
illustrative example for the customers
and employers table that some
representative number of customers and
employers hundred million customers
hundred thousand employers 10 features
for the features the feature vectors of
the employers are shown with different
patterns over here and you have this key
foreign key join the input could be say
hundreds of gigabytes but after the join
the input blows up to several terabytes
and this is because the join has
introduced redundancy in the data notice
that these vectors about Microsoft are
repeated for every customer that is
employed by Microsoft
in many cases this blow-up and storage
is actually a major issue in fact in one
example with one of the product groups
within Microsoft they actually
encountered this problem when they were
doing our web security related
applications it even occurred with a
customer of logic blocks they mentioned
that they are joining five tables and
their data blew up over an order of
magnitude so storage is one of the key
bottlenecks that ml after joints
encounters you have to have extra
storage space because of these ml
tickets there is also redundancy in the
data which causes redundancy in
analytics computations or it that way
stunts I'm in the real world data is
rarely static as you get more examples
more features the data scientist now has
this additional headache of maintaining
the output of the joint and keeping the
models fresh which impedes the
productivity and ultimately all of this
impedes the exploratory analysis process
that is often required for building
machine learning models extra storage
and wasted runtime or what I call system
efficiency issues maintenance headaches
and impeding the exploratory analysis
process or what I call human efficiency
issues because the data scientist is
spending time on grunt work but undoes
the productivity to stop and nowadays a
lot of these analytics computations are
moving to the cloud they're every bit of
extra storage and extra computation
could incur extra monetary costs to
summarize I introduce the problem of Mo
after joins this is a fundamental
bottleneck in the end-to-end process of
building Emma models data in the real
world is often multi-table connected by
key foreign key relationships but since
most ml toolkits expect single table
inputs the data scientists are forced to
materialize the output of the joy that
concatenates feature vectors from all
these tables which leads to human and
system efficiency issues yes Ravi when
you build an ml more review route on the
real trade data center on a training set
and are in training sets much smaller
our vision and second question the
patterns you explain right redundancy
they seem to be very amenable to
compression techniques like run-length
encoding right so why don't I take a
denormalized relation and compress it
right so great questions coming to the
first question is the training sample
does it have to be smaller in the actual
data set
yes in many applications sampling could
actually work but for many of the other
ml tasks they actually want to get say
that effects that are less likely to
occur in a sample and therefore they
want to use the whole data set so there
is a spectrum of applications over there
even inside a sample if you have a
sample example set they might be
redundancy in the data and as I'm going
to explain there will still be a
trade-off space for some of the
techniques I'm going to talk about where
it could still benefit the performance
runtime performance coming to the second
question with respect to compression yes
many of these cases these data sets that
have redundancy are amenable to columnar
compression however that introduces an
orthogonal trade-off space where you
have to know trade-off computational
efficiency for decompression or printing
directly on compressed data and one of
the techniques that I'm going to talk
about today is sort of a stepping to
that stepping stone to that looking it
directly at operating on compressed data
how do you integrate a similar technique
so that's a nice idea for future work
and some of the ideas that I'm going to
explain today are amenable to tates okay
oh yes internet screams find that one
again because it seems like the
maintenance is just like a view on the
end though that's all you need is that
is there something more than that well
yes yes and no it depends on the
application and the environment in some
of these cases they actually produce
copies of the data and they move to
different tool kits so there the
infrastructure that we have built in and
say the database environment for view
maintenance that gets lost and therefore
they have to manually keep track of pair
they have the data and how to update the
data and so on in the database context
we can construct a view of these joints
and do the joints virtually on the
flight I'm going to talk about that as
well turns out that there is still
computational redundancy if you do the
view approach and there's a trade-off
space that I'll talk about okay to give
an overview of my work in my thesis work
I propose this paradigm of ml / joints
to combat this problem of ml after
joints and then project Orion which
appeared in signal 2015 we show how we
can avoid joins physically essentially
we do not need to construct the output
of the join we do not need the joint
stem cells you can operate on data
exactly as it resides in the multi table
format we show how this can help improve
runtime performance because the data in
the input of the joint could be much
smaller but it produces exactly the same
model and thus gives the Sigma accuracy
this could potentially improve
human and system efficiency going
further in project Hamlet Sigma 2016 we
show that in some cases you can actually
avoid joins logically as well what do I
mean by that you can actually get rid of
entire input tables without even looking
at them this obviously runs faster but
I'm going to explain why it can give
very similar accuracy this could
potentially improve human and system
efficiency even further moving on I'll
dive into project Orion in project Orion
or idea to combat the problem of ml
after joins is this technique of
factorize learning and the intuition is
simple we factorize or decompose these
ml computations and push them down
through the joints to the base tables
much like how database systems which
selects selections projections and
aggregates down through joints the
benefits we avoid redundancy in i/o
because we're not constructing this
output dataset we avoid redundancy in
computations we don't need extra storage
runtime would improve and also it could
be potentially more data scientist
friendly because they operate on data
exactly as it resides rather than on a
materialized intermediate data set the
question is is factorized learning even
possible and in this paper we show that
yes it is possible at least for a large
class of ml models known as generalized
linear models also known as gia lamps
that are solved using batch gradient
methods the challenge is how do we
factorize this year lamps without
sacrificing the ml accuracy the
scalability of these implementations and
the ease of development of these ideas
on top of existing data management
platforms I will start with some brief
background about these glm so those of
you are not familiar with it recall the
classification example that I'm using is
running example classify customers as to
whether they're likely to turn or not in
ml we start by mapping each customer to
a d-dimensional numeric feature vector
in this example here you have two
dimensions so basically customers are
just points and now GL ends classify
these customers based on a hyperplane w
over there on the one side are points
that basically customers that churn on
the other side or those that do not the
question is how do we choose a good
hyperplane W in GL m's recall that we
start with a training data set that is
the input we have a set of labeled
customers those
label points labeled s yes or no plus or
minus and the idea to compute a
hyperplane is to minimize a certain
score of the hyperplane on the training
data set say the Miss classification
rate that is essentially a sum of n
terms where n is the number of labeled
customers and a function of the inner
product W transpose X I that's the
distance of the point to the hyperplane
and the class label why I there excise
the feature vector of the earth customer
and why is the class label of the earth
customer it turns out that the Miss
classification Raiders how to optimize
and thus GL M's instead use a smooth
function of the inner products w
transpose x I and depending on what this
function is we get a different in a
model it's the squared distance it's the
popular least squares linear regression
model that use for trend analysis if
it's the log loss then it's the popular
logistic regression classifier fits
hinge loss its linear SVM and so on so
basically the tick away here is that all
of these ml models have the same common
mathematical structure which is this
minimization problem with the sum of n
terms and a function of the inner
product of U transpose X hi oh yes so
there are spm with other kernel
functions right then that will work yeah
that's a good question so it depends on
the structure of the colonel if the
Colonel's operate on the inner products
then some of these techniques that I
talked about can actually put apply
there as well but in this particular
work we focused on GL ends and therefore
we only deal with linear SVS just for
generalities thank you ok so how do we
do this minimization so in this
technique called gradient descent that
I'll give some background about we need
to get to the optimal hyperplane w that
minimizes this function which is also
known as the loss function lfw and it
turns out that the last function for glm
is a ball shaped curve formerly known as
a convex function and the optimal is
essentially the bottom of the ball that
is some separating hyperplane in your
point spaced it in general the loss
function is not closed form and thus
people use iterative solution known as
gradient descent where the idea is
simple you start with some value W
compute the gradient at w and the
gradient is essentially the
generalization of the slope to multiple
dimensions and you move in the opposite
direction of the gradient the descent
and alpha is
step size parameter that controls the
success how much you move in the
opposite direction to give an
illustration we start with some w0 that
is some separating hyperplane over there
we do one pass over the data set
basically look at all the points and
compute the gradient take a step in the
opposite direction we get to a new model
w1 do one pass or the data compute the
gradient who in the opposite direction
and keep doing this iteratively until
you get closer and closer to the optimal
so this is basically the bgd technique
how what does it do today in the context
of yes single pass over the internet yes
and how many purses typically does it
take to college it depends it depends on
the data set and the accuracy desired by
the data scientist in practice I've seen
they run like 20 to 40 50 Eurasians cost
of just joining the key foreign keys is
that the body making the larger process
if you want to do 40 pulses I'll explain
some of the experimental results a
breakdown of what is the runtime of the
join what's it turns out that the
redundancy introduced by the joint is
often the dominating factor further okay
okay so recall that we simply run bgd on
the concatenated feature vector if you
want to do FML after joins we have
features from customers stack next two
features from employers this is the
expression for the loss take the first
derivative that's the expression for the
gradient it is also a sum of n terms
with a scalar function G and you scale
the feature vectors X hi notice that the
gradient nebula L the model or the
hyperplane w and the feature vector X I
are all D dimensional vectors that D is
the total number of features so how does
this work we start with the input of the
join to the join get the output of the
joint don't forget about the input start
with the initial model w0 one pass over
the data you get the first gradient
update the model next pass over the data
get the next gradient update the model
and proceed iteratively so basically bgd
after joins you physically write the
output of the join and then you do one
scan of the output table or iteration
what does factor is learning do recall
that our goal was to push computations
to the
stables and our intuition a simple you
split up the computations on the two
feature vectors from the two tables XC +
x e this is the expression for the
gradient the inner product can be
rewritten as a sum of inner products
over the customer and employee of
features and the inside is that as some
of sums is still a sum thus we can do
one pass over the employers table to
pre-compute the partial inner products
or the employer features use that when
we do a pass over the customers table to
reconstruct the fool in the products
however we run into this challenge of
how do we scale the feature vector of
the entire a feature vector for every
customer turns out that one pass over
each base table may not be enough we
need to save some statistics and
exchange them across these tables and
reconstructive or gradient so how does
this work so here's the output of the
join get rid of the output table get rid
of the joints you start with the initial
model you chop up the coefficients /
coefficients of a customer features and
coefficients / employer features one
pass over the employers table you get
some statistics and if you're curious
these are basically the partial in the
products per employee ID use those to
just takes to do one pass over the
customers table and we get a portion of
the gradient we also get some statistics
over the customers table and if you're
curious these are basically the group by
sums off the scalar function g /
employer ID use those statistics do a
second pass over the employers table and
we get the remainder of the gradient
stitch these two vectors together that's
the food gradient proceed and update the
model and go ahead to the next iteration
so basically factorized learning
requires one pass of customers and two
passes over employers per iteration and
it completely avoids the need to join
these two base tables now yes Rick can
you have sort of design features then
requires combining some columns from the
fact and the dimension yes I like a more
complex or do that in that case does it
still yeah that's a great question so
you basically talking about feature
feature interactions now there are
methods called second-order methods like
Newton's the set where you actually
construct the Hessian matrix that has
pairwise interactions among all features
turns out that this factor is learning
technique that I talked about here for
linear linearity extends to that
technique as well however there is a
trade-off in terms
of the runtime performance that
trade-off space looks a little bit
different than what it looks like for
here so in terms of feasibility yes it's
possible but in terms of efficiency the
trade-off space is a bit different okay
so now I talked about the algorithm when
we wanted to work in practice there are
some challenges that arise when you want
to implement it on rail system what is
these statistics that I talked about / /
employer ID do not fit in them
aggregation context memory and how do we
implement this on top of existing data
processing systems without having to
change the internals for ease of
deployability for the first one we go
into the details of a partitioning based
approach in the paper where we staged
the computations of different portions
of these statistics and stitch them
together to reconstruct the full
gradient and for the second one we use
the abstraction of user-defined
functions specifically user-defined
aggregate functions that are available
in practically all major database
systems as well as on hyphen hundo so
how does this work in practice in terms
of efficiency oh yes tom oki foreign key
choice right that's all I need to join
between the two tables so the two
aspects to it in this particular paper
we focused only on key foreign key
joints because they are very common and
we thought that would be nice and in
terms of applicability of this technique
to general joints where you can have the
full cross product appearing it is
technically feasible but you need an
identifier a dummy identifier that needs
to be added to the statistics because
the attribute joining attribute is no
longer the primary key in one of the
tables so we haven't looked at that in
this paper but conceptually there is
nothing that prevents us from extending
it to it okay so what about system
efficiency is it actually faster we've
done an extensive empirical analysis in
the paper and I'm going to give a
snapshot of one experimental result here
we implemented all these techniques as
user defined functions written in C on
top of the post curse equal open source
database system and this our experiments
were single node with 24 gig ram we
synthesized data sets that resembles
some of the number of tuples and number
of features we saw in practice and here
we have customers with 150 million
couples employers 10 million tuples we
ran logistic regression with batch
gradient descent for 20 iterations I'm
going to show two things the storage
space consumed throughout the process of
learning and the run time on the y-axis
the
put data set the customers and employees
together is about fifty two gigabytes
BGT after join is over there that you
require about 120 gigabytes of storage
space factorized learning in contrast
does not require any extra storage space
it basically there's a gap of two
seventeen percent in terms of runtime in
terms of storage space and in terms of
runtime the gap is three hundred and
thirty-four percent and to break down
the runtimes as Harvey asked earlier bgd
after drawing spends about nineteen
percent of its runtime in constructing
the output of the joint and 13 minutes
per iteration of PGD in contrast
factorized learning you avoid the
joining time and the runtime per
iteration is about five minutes because
it operates on the smaller data that's
the input of the join now of course all
of these relative runtime numbers depend
on now several factors and these are the
number of tuples number of features how
much memory the system has the number of
iterations and the degree of parallelism
and so on and we have done an extensive
sensitivity analysis to all these
factors in the paper overall it turns
out that factorized learning is usually
the fastest approach but not always
there are some cases where the bgd after
drawings approach could be slightly
faster and we devise database style cost
models for the aisle cause in the CPU
cost of this technique that
automatically picks the fastest approach
on a given instance of the input there
are more details in the paper and i'll
be happy to chat with you are offline
about it we give a proof for why the
accuracy of the bgd technique is not
affected and why we get exactly the same
model attack Christ learning there are
other approaches to scale to larger than
memory data that we discuss and the
trade-off space there and the question
about the views are turns out that I Oh
redundancy can be eliminated if you use
use but computational redundancy still
exists and thus it could still be slower
than factorized learning in many cases
and we also extend all of our techniques
to the parallel chad nothing parallel
distributed environment and prototype
some of your ideas on hive in Hadoop as
well as on spark and we've also extended
this to multi table joint specifically
star joins but the extension is trivial
to snowflake joins as well yes
um great wine all right so subsequently
they also extended our ideas to GL M's
solved using STD stochastic gradient
descent as well as coordinate descent
methods and we've also extended it to
probabilistic classifiers like naive
Bayes and decision trees and also to
clustering techniques such as k-means
and a lot of this has worked done with
masters and undergrad students basically
I could offload some of these technical
work to them and focus on newer ideas
because I didn't want to focus on
extensions of this directly myself and I
focused on the newer idea project hamlet
that our talk about next I'm also
working on extending the idea factory is
learning to linear algebra so that we
can generalize it to any ml computation
that's expressible in linear algebra and
the input of the join will be
automatically basically given a linear
algebra script that operates on a matrix
that's the output of the join will
produce a system that will automatically
rewrite it to a script that operates on
the input of the joint yes so you see it
depends it depends on the techniques it
depends on the data parameters it turns
out that for stochastic gradient descent
there is no computational redundancy in
the in-memory setting because the model
changes after every example and for
coordinate descent it could be that
certain overheads impose are imposed by
the columnar access patterns and we
looked at it in the context of column
stores and the trade-off space look a
little bit different for each of these
techniques some of the probability class
pressure and clustering it's very
similar to be GD yes that's GD because
it seems like you don't get the
computational game and then if you're
doing it on a few you want to have I ok
neither so I was curious did you find a
game yes so it turns out that for STD
since you don't have computational
redundancy the only thing that matters
is the data redundancy the i/o
redundancy and so in order to avoid
construct now join me say can do a hash
table and then you can do a view
basically an index has join and the
challenge there is what if the hash
table doesn't fit in the memory then you
need to partition the data but that
scrambles the ordering that you need for
SGD which is very sensitive to the order
in which you access the data and now it
introduces a new runtime accuracy
trade-off and it video looking at some
of the issues in this trade-off things
like what is the foreign keys highly
correlated the class label and those
sorts of issues so
can expect a paper about that pretty
soon but i'm happy to chat with you
offline about more details if you're
interested okay in short a sum of sums
is the same some factorizing avoids
repeating some solo and after joins no
more to learn / join Co for that is
project Orion in one stanza moving on
I'll dive into project Hamlet recall the
same running example customers referring
to employers here is an observation
given the employer ID all the features
of the employers are fixed so do we
really need the state and revenue
features if you already know the
employer ready that's already present in
the customers table there's this notion
of feature redundancy in the information
to the literature and we can show
formally that given the employer ID all
the employer features are redundant this
motivates us to basically consider the
idea of omitting all those features that
is avoiding the join logically in short
avoiding the joint the use of foreign
key employer ID as a representative of
all the employer features thus we get
rid of that table however there's also
this notion of feature relevancy certain
features the employers could be highly
predictive of the target in this case
the churn in which case you might
actually want to bring it back and let
the algorithm decide and so oops we need
the joint they might be wondering what's
the big deal why do we need to think of
avoiding the joint why not just use a
feature selection technique these have
been studied for decades that manage
this redundancy relevancy trade-off and
we can just apply that in this context
well in one word the answer is run time
but here's a brief background about what
is feature selection to those of you not
familiar with it it is essentially a
method to search the space of subsets of
features and update as an obtain a
subset that is probably more accurate or
more interpretable and there are various
algorithms for feature selection
wrappers filters embedded methods one of
the most popular ones is forward
selection where you start with a single
feature in your set and you compute the
prediction of the generalization error
the test error and then you keep adding
one feature at a time depending on
whether the test error goes down or not
backward selection is the reverse you
start with your entire set of features
and you keep dropping one at a time
based on whether your test error goes to
not and then there are filter techniques
that rank features there are also
embedded methods like regularization l1
or l2 norms coming back to the question
about why bother avoiding the joint well
if you're wide the join you reduce the
number of features that you give us
input to these algorithms which reduces
their search space potentially improving
their runtime basically we are
short-circuiting the feature selection
process using database schema
information the key foreign key
relationship million dollar question
what happens to accuracy to understand
the effects of avoiding the joint and
accuracy we need to understand the
bias-variance trade-off I'm going to
give a little bit of background here
learning theory tells us that the test
error of a classifier can be decomposed
to three components the bias the
variance and the noise the bias also
known as the approximation error is
again this or informal explanation is
sort of a quantification of the
complexity of the classifier
supportassist base models that are more
complex tend to have lower bias the
variance is a characterization of the
instability of a classifier to a
particular training data set models that
are more complex tend to have higher
variance fixing the training data set
and fixing the model if you give smoke
fewer and fewer training examples the
variance tends to go up the noise is a
component that no model can mitigate
because of an absorbable data and
traditionally the bias-variance Trade
Office illustrated as follows in ml as
the model complexity keeps going up for
a fixed training data set the training
error keeps going down but the test
error goes up beyond the point on the
left is a situation of high bias and low
variance due to low model complexity on
the right is a situation of low bias
high variance because of high model
complexity situations with high variants
are also colloquially called overfitting
and so the key question for our work now
becomes what is the effect of avoiding
the joint that is omitting the feature
vector about the employers on the bias
and the variance we did an applied
learning theoretic analysis of the
effects of avoiding joins and bias and
variance and I'm going to give a brief
summary here the effect on bias so
here's the full feature vector we rename
the impiety as FK f to be generic and
without loss of generality assume for
now that the customer feature set is
empty and so we have this reduced
feature vector
the classifier that we learn is
essentially a prediction function it's a
function of the feature vector and HX is
the hypothesis space of all possible
classifiers that you can build based on
the feature vector X be sure that this
hypothesis space does not change if you
omit the employer features in this case
basically HX equal to H FK in a sense
this learning theoretic result is
equivalent to the earlier information
theoretic result that I mentioned about
the employer features being redundant
and what it basically leads to is that
if you avoid the joint oh and if you
actually avoid the foreign key and use
the employer features instead turns out
that the hypothesis space can actually
shrink so basically what this leads to
is if you avoid the joints the bias is
unlikely to grow up if it if it's not
likely to go up then we don't need to
worry about the bias so however it
actually turns out that for some popular
classifiers like logistic regression and
I you base the bias can actually go down
so what happens to variance so if the
hypothesis space is unlikely to unlike
li to change does that mean the variance
is unlikely to change the short answer
is no and the key insight here is that
feature selection may omit certain
features in your vector to understand
this here is an example here's your full
feature vector and again without loss of
generality assume the customer feature
set is empty we have this reduced
feature vector and suppose I give the
following true concept that generates
the data that is sort of the worst case
scenario for avoiding the joint suppose
the true concept is as follows if the
state of the employer is Wisconsin then
the customer is unlikely to churn
otherwise they likely to churn if I
generate thousands of examples based on
this true concept and give that as input
to a feature selection method what is
the likely output feature subset it's
highly likely to be state and the key
insight here is that in general the
domain of the foreign key could be far
larger in the domain of the features it
refers to their only 50 states they
could be millions of employers to
understand this here's an illustration
this is the true concept and the
hypothesis space of classifiers built
using the state features say encompasses
the true concept but if you use the
foreign key the hypothesis space is far
larger and notice that we already showed
that hf Kriegel HX thus if you use a
foreign key you could end up with higher
where
compared to the state however avoiding
the joint has forced us to use the
foreign key as a representative of the
state feature which leads to the result
that avoiding joints could increase the
variance in short we ask the question
what happens to the bias and variance of
your way to join another learning to
read analysis suggests that the wiring
the joints does not increase Tobias but
it could increase the variance this is a
new runtime performance accuracy
trade-off in the ML setting using
database schema information and we asked
how can we help data scientists exploit
our theory of this trade-off in practice
an hour idea is to devise practical
heuristics that help the data scientists
bound the potential increase in variance
and they can now apply a threshold and
then see if it is a ball of certain
threshold it's too unsafe I'm not going
to avoid to join I call this process
avoiding the joint safety the challenge
is how do you even obtain a bount or
white joint safely there is no precedent
for this well it turns out there are
certain bounds in learning theory that
we can use and these bounds are bounced
on the absolute variance based on the
web neck Sherman tank is or the VC
dimension of a classifier and
essentially you can think of it as a
quantification of the complexity of a
classifier hypothesis space models that
have higher VC dimension tend to have
higher variance fixing the training data
set and there are several bounds and
learning theory using the VC dimension
on the variance we apply a standard
bound from Cheyenne choice popular
learning theory textbook combined with
sour slim about the growth function the
expected tres terror and the expected
trainer the difference is bounded by
this function of the VC dimension we the
number of training examples N and the
failure probability Delta and this is
for training data sets of size n which
are examples are iid distributor
independent and identically distributed
so now here's the catch these bounds are
in bounds an absolute billions what we
need is a bound on the increase in
variance and this leads us to this
heuristic the ror rule where our
intuition is that the increase in bound
costs by the increase in VC dimension is
an indicator of the risk of avoiding the
joint and we define this quantity risk
of representation ror which compares the
hypothetical best feature selection
output that you can get after avoiding
the joint versus what you can get with
avoiding the join and that is this
quantity over here there are three
terms vs is the VC dimension of the best
classifier that you can get after
waiting to join vinos the other one and
delta bias and codes the difference in
the bites but now here's the problem vs
vino and delta buyers are impossible to
know a priori in general because they
require prior knowledge of the true
concept if you already know the true
concept you probably don't even need
machine learning so how do we overcome
this conundrum our idea is to upper
bound this quantity ror to eliminate
these terms that require prior knowledge
of the true concept and by upper
bounding it we make it more conservative
in the sense that it's unlikely to avoid
joints where if you avoid them the error
shoots up but it is likely to miss
certain opportunities to avoid joints so
we are upper bound the ror the details
are available in the paper essentially
for visa dimensions that are linear in
the number of features and this includes
popular classifiers like value based on
logistic regression the ror can be upper
bounded as follows we have this two new
terms over here dfk is the domain size
of the foreign key the number of
employers and QA star is the minimum
domain size of the features in the
employment table and the ror rule now
essentially says if the RHS over here is
less than some threshold epsilon it is
safe to avoid the join now we looked at
this and then we thought it oh yes
that's the single feature with minimum
size exacts rate the domains I got the
smallest or smallest the main feature
okay so we looked at this and we thought
it still looks so complex can we
simplify it further to help the
scientist and that's where we came up
with the temple ratio rule or the TR
rule where the idea is to eliminate the
need to even look at the employee
features basically we want to get rid of
that second term there define the toefl
ratio as the ratio of the number of
examples and to the domain size of the
foreign key dfk that's that quantity
over there intuitively it's just the
number of training examples the number
of tuples in the customers table to the
number of employers that's the domains
as the foreign key that is why i call it
the topple ratio now if the domains i
just the foreign key feature is much
larger than the minimum domain size of
the feature in the employers table which
is almost always the case in practice
then for reasonably large tuple ratios
it turns out
this bound becomes linear in 1 over
square root of double ratio flip it over
take the square upper bound becomes the
lower bound and we end up with the
double ratio rule which essentially says
if the topple ratio is greater than some
threshold tau it is safe to avoid the
joint notice that this is even more
conservative than the other one rule so
after this long journey to the wonderful
world of learning theory we come back to
this stunningly simple double ratio rule
that only uses the number of tuples in
these two features to help us avoid join
safely in this I now context so even if
you have a million features about the
employers it is possible to safely
discard them without even looking at so
does this work in practice we r an
extensive experiments with real datasets
I'm going to give a snapshot here we
tune these thresholds for these rules
using simulation study and it turns out
that the top o ratio of 20 is good
enough to help decide whether a joint
can be wider or not notice that this
simulation has to be done once per VC
dimension expression not once per data
set as you need to do for hyper
parameters in machine learning and the
error tolerance that we set for the test
error was 0 point 0 0 1 we use real
datasets there are three here there's
more in the paper Lisa from cargo all of
these are classification tasks in the
Walmart data set we want to predict the
level of sales per department in a store
by joining data about sales with stores
and whether indicators in the up data
set we want to predict ratings of
businesses by users by joining data
about ratings with users and businesses
flights is a binary classification task
where we have data about flight routes
and we want to predict if its code
shared or not we're joining data about
other routes with Airlines and airport
information we applied a standard
off-the-shelf classifier on iu base
which is very popular and combined it
with popular feature selection methods
like forward backward filters and so on
and they used to standard holdout
validation methodology to measure
accuracy fifty percent of the train
label data set was set aside for
training twenty-five percent for
validation during feature selection and
twenty-five percent was set aside as the
final holdout test error which is the
final indicator of accuracy yes RMG in
your training set that's a good question
so in this particular example the domain
of the foreign
is defined by these tables over here
each individual example foreign key
instance may or may not occur in a
particular training sample so we use
smoothing to basically figure out what a
value should be assigned to the keys
that do not arise in the training
example so the domain is known most of
the examples of the foreign key values
can occur in a training example but for
those that do not arise we use laplacian
smoothing so what are we comparing input
all the features to the feature
selection method versus avoid those
features that the TR rule says are safe
to wipe so does the TR rule predict
correctly so that's what we want to
check notice that this is orthogonal to
the earlier work or project Orion very
worried about physical drawings here all
the data shown a priori we just worry
about which features are eliminated so
the results are the paper but here is a
snapshot about the results for backward
selection with naya base and the topple
ratio rule is applied on a per join
basis so you join us decide we decide
independently if you use all the
features as input on the walmart data
set the error for the predicting the
department weight sales is point eight
nine six one and this is the root mean
square error with one to seven levels
thus the topple ratio rule avoid the
join with stores and with whether it
says that both rule both joins a safe to
white what is the error if you avoid the
stores what is the error of your white
weather turns out that the error is
point eight nine 10 notice that lower
error is better but here in this case
it's not significantly higher therefore
the TR will got it right on both these
cases it is safe to avoid and that's the
overall error by avoiding both those
both of these two tables is point eight
nine 10 however on the yelp data set the
error that all features as input the
rmse four levels 125 is 1.1 330 two
tables the TR rule says neither of the
message to avoid if you avoid the users
the error shoots up to 1.2 one if your
why the businesses the error shoots up
to 1.2 4 and therefore the TR will got
it right in both cases they are not
saved why and the final error is of
course the same as the original error
because all features are being used on
the flight status set the errors
discrete 0 1 loss because it's a binary
classification its point 13 90 3 tables
TR rule says airline
save to white but not the airport tables
but if you avoid each of these tables
turns out the error doesn't shoot up so
basically the tier will got it right on
airlines but miss the opportunity on
airport tables the overall error is
basically the error that you get by just
avoiding the airlines table notice that
these are the examples of the missed
opportunities that I mentioned because
these rules are conservative and that's
their scope for devising less
conservative roots here what happens to
the run time on the up data said there's
obviously no speed up because no joints
are avoided on the flight data set by
avoiding this particular tables features
the speed up was a modest two times but
on the Walmart data set the speed up was
82 times sorry as compared to Ryan or
two so this is orthogonal to Orion so
this is like using all pictures so
everything is physically joined but we
now give only the features from the
sales tables and omit the features from
these two tables so there's a difference
if you apply Ryan ah the physical
drawing it would be that it would be
faster yes so is this the 82 / or I or /
ml laughter joy it's ml after joint so
you can check the full table and omit
the features that the TR rule says can
save to omit but yes integrating those
two could give more speed ups as well
yeah okay so overall we had seven data
sets that we measured in the paper of 14
joints turns out that the TR rule
correctly avoided seven out of 11 joints
that are safe to avoid but miss the
opportunity on four joints and it
correctly did not avoid three joint that
are not safe to avoid and the speed apps
range from no speed up because no joints
were avoided all the way to 186 times
yes I can say if you will don't use the
one simplification that actually leads
to the simple TR rule as to have the
more complex formula quick point next
slide so turns out that the results are
the same for logistic regression with l1
and l2 as well but it's also the same
with the ror rule that you asked
basically it turns out that the way in
which we simplified the Ottawa rule is
in a way that Bradley is reflected in
the real world data sets so TR and
Ottawa somehow give the same results
even though they are vastly different in
terms of how conservative there
so it turns out that the accuracy can
actually increase by avoiding the joint
in some cases and dropping the foreign
key causes the accuracy to drop
significantly in many cases because the
bias shoots up and they also show the
details of the simulation study that
goes into how we tune these thresholds
and finally we also handle cases where
there are skews in the foreign key
distribution foreign key skills or a
classical problem in the database world
for parallel hash coins and so on and we
present the first results of the
implications of foreign key skills for
ML accuracy in short to join or not to
join that is the question topol ratio we
did coin use at your discretion that is
project Hamlet in short to summarize I
present it to you the problem of ml
after drawings where data scientists are
often forced to construct the output of
the joint because data in the real world
is often multi-table connected by key
foreign key relationships but most ml 2
goods expects into table inputs
materializing the output of the joint
leads to human and system efficiency
issues in project Orion I showed how we
can avoid joins physically no need to
get the output of the joint no need for
the joints ml can operate directly on
data as exactly as it resides makes it
run faster in many cases and use exactly
the same accuracy improves human and
system efficiency in project hamlet we
went one step further to show how we can
also avoid joints logically in some
cases basically get rid of entire input
tables without even looking at them
which makes these techniques run faster
but I explained why they would yield
similar accuracy potentially improving
human and system efficiency even further
moving on I'm going to comment briefly
about my future research ideas to put
some to bring some perspective into my
current work and future work and how
they're related talked about learning
over joints with project Orion and in
the context of feature selection or
joins this project Hamlet several models
are being learned over joint it turns
out that that's part of this larger
exploratory feature selection process
with data scientists compare multiple
subsets we talked about that in logic
Columbus that is part of this larger
process of feature engineering that I
mentioned earlier where the deep excuse
me where the data scientist converts the
raw data into the feature vector that we
need for ml and that is part of an even
larger process and I promise
to stop with that is the exploratory
model selection process where basically
the task is to obtain a precise
prediction function based on the data
set the raw data set for my short term
future work for the next couple of years
basically want to focus on new
extensions and generalizations both on
the system side and on the applied
learning theory side for Hamlet Orion
and Columbus and for the longer term
future work I want to focus on helping
improve these processes of feature
engineering and model selection more
generally I'll speak briefly about my
thoughts on this model selection process
from conversations with data scientists
at various settings observe that model
selection is often an iterative
exploratory process data scientists
explore a combination of three things
which I call the model selection triple
what is what are the features that's
involves feature engineering what is the
algorithm is it I you based logistic
regression and so on and the hyper
parameters for that algorithm the data
scientist starts this process by
steering and deciding which combination
they want to try out first and then they
evaluate the accuracy of this
combination they executed on a system on
the data they get the results consume
these results manually often basically
figuring out maybe I need to change the
features or maybe I need to change the
parameters and they proceed with the
execution and then consume the results
and do this iteratively the bottleneck
that I've observed is that most systems
today force these data scientists to be
on one of two extremes either the force
temp to do only one combination per
iteration that is one MSD per iteration
or they tend to cut the data scientist
out of the loop and automate the whole
process doing one MST per iteration
turns out to be too tedious and painful
for these data scientists automation
works in several applications but in
many other applications the data
scientists do not want to be cut out of
the loop they want their expertise to be
used and they do not want that to be
ignored and so the question I ask is
there a more flexible data scientist
friendly middle way possible between
these two extremes and we wrote up our
thoughts on some of this idea which is
which appeared in a vision paper at ACM
sigmod record just a few weeks ago and
this is work in collaboration with
Robert McCann who's part of the security
team that I mentioned within Microsoft
and our idea is as follows enable these
data scientist to be several logically
related MSTS together using say a
declarative interface
a higher level interface that does not
need them to enumerate these msts they
could be several logically related
subsets together or parameters together
and under the covers we have a system
that generates code and evaluates
multiple models and they could now apply
database style optimization ideas that
eliminate redundancy in computations
materialized intermediate data and so on
and let's bring together techniques from
both the ml world and the data
management world to speed up this
process and ultimately since the system
is now more aware of what combinations
are being explored and how they are
related they could apply ideas from the
provenance management literature in the
database world to help these data
scientists debug these models and
process this information about this
process even more effectively helping
them steer the next iteration better and
so overall we explain how we can combine
ideas from the data management
literature and mal literature and also
human computing interaction to improve
the system efficiency of this process
and also the productivity of these data
scientists and that's a segue to my
concluding remarks about the importance
of intersectionality I'm a data
management person and that is my
background but over the last several
years I worked closely with people in
the machine learning world and
interacted with a lot with a lot of ML
ideas and as I see it advanced analytics
is really the coming together of these
worlds of data management of machine
learning moving forward I'd like to
continue working in the space of
advanced analytics building bridges
between these two communities because of
the sheer number of open research
problems and new opportunities that
arise when you take a joint view of
these two worlds and I'd also like to
explore the interactions with human
computer interaction angles because in
many of these tasks the data scientist
is often in the loop and also like to
look more closely at application domains
where these advanced analytics
techniques are used I've worked closely
with enterprise data scientists also web
data scientists I'd like to work with
other application domains as well to
look at what the impact of these
advanced analytics techniques are on
their applications that brings me to the
end of my talk I'd like to thank my
advisors Jeff not in jignesh patel and
other mentors chris ray and david do it
at the gray lab as well as my co-authors
and collaborators all these systems and
all the techniques that are talked about
code for all my projects are available
as open source tool cuts on my webpage
even the data sets are available feel
free to shoot me an email if you'd like
to talk to me about it offline and I'll
stop now and I'll be happy to take more
questions but here and from the remote
audience thank you yes don't the model
selection reminds me a little bit of ml
base didn't mention that so one question
I have is and once you so first of all I
the optimizations you talk about are
those similar to the ones that ml base
talks about the multi query optimization
running that's one question the other
question is once you're at that level my
dad kind of is that where the huge cost
is and then maybe some of the things
that you were talking about in Orion and
Hamlet become less relevant or kind of
are they just as relevant in the bigger
con context and they are on smaller
college right so to answer the second
question first Orion and Hamlet and even
Columbus the way think about them are
building blocks for this larger vision
and so all of these techniques will
contribute to these optimizations
framework that I talked about and when
you take joint view of these model
selection triples new opportunities
arise like can we reuse results for
across models can be materialized what
sort of intermediate data sets to me to
materialize and also interactions
between provenance and optimization
become important can we basically use
malls across iterations can we provide
computations if they want to do what if
debugging those sorts of questions so
the way I look at it as all of these are
building blocks in the context of the
model selection optimization now coming
to your first question about ml base we
actually talked about relationship with
MLB's in the paper as well the way of
view it is ml basis one way to specify
an interface they do not go into the
feature engineering details like the
joints for example they talk about
automating algorithm selection and hyper
parameter tuning which is nice and so
it's closer to one end of the spectrum
where you want full automation whereas
here we talked about this entire
spectrum full automation all the way to
individual iterations and bridging the
gap between these two extremes and
coming up with frameworks maybe not one
frame at what multiple frames ml base
could be one Columbus is another there
are automated hyper parameter tuning
libraries that can be viewed as
basically new interfaces for the small
selection
management process and the term we use
in the paper is this could be a narrow
waist or new class of systems that
optimize the model selection process and
thinking about it in this way enables
new interfaces like combining multiple
feature sub sets with parameter
combinations which ml base doesn't do
for instance so some of them could be
similar some of them could be different
like for example in the automated model
search stuff they dart what they do
batching they also do computation
sharing so some of that could be similar
but some of them could be different like
when we look at the feature engineering
contexts the joint stuff for example
they do not do anything like that so
looking at the end-to-end process of
model selection some of the
optimizations that we talking about in
this context some of the ml base ideas
could be relevant here but then it also
opens up a space of whole new space of
new optimization ideas and there's also
like we go to the details in the paper
we categorize it into several categories
of optimization ideas we could also look
at introducing new interfaces for
accuracy runtime trade-offs like the
hamlet stuff or some of the other one
starting and model caching and reusing
in those sorts of optimizations okay any
questions any other questions anyone
questions from the remote audience
surprise me those sort of to see
provenance man right there because
ultimately what what we're doing and
this is this may be just just like the
traditional I part right ultimately what
we're doing is we're we're selecting
Vogel's right before combinations of
models features and type of parameters
night can you give some additional
provenance management plays in Japan's
yes suddenly so in the context of the
database world for sequel query
processing people looked at their
provenance how provenance why provenance
all sorts of these models to debug why
certain things are in the output or why
certain things are not in the output in
the ML contacts the debugging we're
talking about is why certain features
matter or why certain subsets of
examples matter so these sorts of
debugging questions as to where they
need to invest their effort for the next
iteration they need to get more examples
or do they need to get more features of
certain kinds providing system support
for these sorts of questions could be
helpful currently often what they do is
just pure intuition or through manual
nodes they track the changes and they
try to figure it out so there's a
low-hanging fruit thereof just basically
finding provenance for machine learning
and then providing these querying
support for the process information
stack in next stages things like what if
debugging things like basically
recommendations for features like can
you use some passed information to make
recommendations based on the way behaved
in the past those sorts of questions you
say this you want to have the same sort
of Providence management that have a
database in context machine learning
what you're not saying is that the
database problem its management
techniques give you a leg up oh yeah
it's the former it's like applying the
philosophy of the provenance management
work that you've seen in the database
community bring that to between nany
same technique some of them might work
some of the techniques can be reused
some of the techniques need to be device
from scratch so looking at this
connection looking at this context I
think is a very interesting here yeah
okay great yes enjoy uh-huh so in many
cases if you want to build a model you
may not build it on the entire fact data
if I even want to subset is safer
customers in Northwest right right now
firstly so for that purpose I mean you
could join anyhow laughter because the
conditions are under the dimension
difference right so here in our example
customers referring to employers you are
saying employers based in Washington
something like that okay huh yes do you
still have tried to join it at least
some of the dimensions sure do you still
see a benefit in performance in those
cases right so is it for Ryan or Hamlet
both okay so there are two aspects to
this oneness even if we have a
completely de na mele stable where
everything has been physically written
out as a single table what we found and
this is in the context of the santoku
system that I demonstrated at vldb last
year it might be worth normalizing the
table back to the multi table form under
the covers without the data scientist
having to know and then apply the
factorized learning technique because
the per iteration runtime could actually
be significantly lower if you do
factorize learning and therefore this re
normalization can actually turn out to
be beneficial so that's what we found in
the context santoku and that's true both
in memory out of memory whatever but it
depends on the model it
on the data dimensions and all of those
things and so the same cost model and
the trade-offs matter what is the other
question for the hamlet stuff even if
the data is denormalized the runtime
comes from the feature selection
computations that you're reducing so it
doesn't matter whether the data is
physically joined or not Hamlet will
still be able to give you speed up
because you're avoiding the computations
but if you have data access times that
matter then the physical join
optimization might also help okay
nothing online right I don't see any no
questions yet great thank you for coming
everyone thanks Kristi</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>