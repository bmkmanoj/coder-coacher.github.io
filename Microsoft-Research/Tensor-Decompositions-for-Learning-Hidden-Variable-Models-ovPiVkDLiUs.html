<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tensor Decompositions for Learning Hidden Variable Models | Coder Coacher - Coaching Coders</title><meta content="Tensor Decompositions for Learning Hidden Variable Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Tensor Decompositions for Learning Hidden Variable Models</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ovPiVkDLiUs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay I think we should start so it's my
pleasure to welcome Sean Cody this
morning for an Amazon New England lab so
Sean public does a really neat
introduction and people here probably
talking already sure has done a lot of
work in computational learning theory
and and also other works and today he's
gonna teach us how to use tensor
decomposition for learning hidden
variables models okay yeah it's fun to
be here I'll be around for the week so
if you want to chat we can continue
discussions so how do we learn models
with hidden structure this is really one
of the questions were facing in a lot of
practical applications even from simple
settings to really complicated settings
in same machine translation so just
start with two basic examples for
mixture models which is mixture of
gaussians which I think we're familiar
with so here we're gonna see data like a
bunch of point clouds and we'd like to
figure out the means of these point
clouds and another standard example are
these topic models so there we can think
of having a collection of documents and
each document is about one or more
topics and we think of the document as
being like a collection or a bag of
words okay
and you can think of these two is like
canonical mixture models and how do we
learn them learning is obviously easy if
someone gave us the labels you know
someone told us which documents are
about which topics it's easy but if you
don't have these labels how do we learn
okay so let's start by just looking at
what's used and what's known so
oftentimes is using practices e/m it's a
very natural algorithm what k-means and
what do we do we guess the the cluster
assignments are the parameters assign
the points to various clusters and
iterate and then there's various
sampling based approaches in MCMC
approaches and a lot of the the
practical algorithms essentially do
inference and learning so as they're
learning they try to figure out which
points are assigned to which cluster and
at some level this seems
intuitive but this might also be part of
the difficulty in in learning because
we're trying to figure out this
inference problem and influence is hard
in some of these models like in the LDA
model like if you have multiple topics
per document just solving the inference
problem is hard okay but that's practice
what about theory what's the upper and
lower limits we know about learning and
there's actually been some really nice
work recently by Adam kalaiy on Kimora
and Greg valiant so the first thing
these guys showed is they just make the
problem simple what do we know about a
mixture of two gaussians this is about
as simple as it gets it turns out even
that wasn't known and what they showed
is HUD alone a mixture of two gaussians
in poly time when the gaussians could
overlap because once things overlap
that's when things start becoming
difficult actually even when they don't
overlap but are close it becomes
difficult in high dimensions but in just
two dimensions they gave a poly time
algorithm which was efficient so at
least that case we know we can solve but
K was 2 so you can easily be exponential
in K where K is the number of gaussians
ok so there's kind of a search based
procedure on a line okay so subsequent
to that work there's some really nice
follow-up work by Unger and Gregg which
actually in a sense was a negative
result where they showed it seems like
you actually need exponential in in the
number of gaussian samples to learn a
mixture of k gaussians and this is an
information theoretic lower bound and
this seems bad and the point is if you
have K gaussians which overlap but not
like we're on top of each other but by a
reasonable amount he actually showed you
could potentially need many many samples
to learn this thing from an information
theoretic point of view which basically
says computationally your hosts to so
this looks bad right now and is a really
nice construction by unka and Gregg and
to some degree this talk is going to
contradict that
because I'm gonna argue that for kind of
a natural case of mixture of gaussians
and these topic models we can come up
with a closed form and efficient
estimation procedure and there's gonna
be interesting for a number of reasons
which people are getting a little
surprised by because this is a non
convex problem and the solution were
coming up with is non convex yet it's
closed for them it's a pretty simple
approach because it's based on linear
algebra techniques we aren't solving
inference in the learning process and
this is handy when we start looking at
these topic models with multiple topics
in them because sometimes inference is
hard but somehow we can do things
greedily and still figure out what the
topics are with this kind of closed form
estimation procedure well come up to
this question of how do we avoid this
lower bound because I'm saying we could
do things in closed form and nicely but
I just gave you this lower behind and in
extends to a number of other settings
like this led a model which is a very
natural notion when you can have
multiple topics for document okay and we
can get a closed form estimation
procedure for that closed form
estimation procedure for hidden Markov
models and I'll discuss some
generalizations of these ideas to
structure learning like in these models
used in linguistics and bayesian
networks but most of the talk is really
going to focus on these two simple
models and understanding how we learn
them and basically the proof is very
simple it's geometric and I think we can
really understand how to learn these
things so let's just go slowly and
definitely no no global optimum
efficient in closed form so we'll see
what that means precisely but there is
no local optimization here this is
global Optima for estimating the
parameters okay but again we'll see what
we mean here because we have sample data
and this questions about you know the
efficient statistical rate and so on but
nonetheless I'm going to stand by the
claim that it's a closed form no
initially we're not using M so we'll see
and that's the point
we aren't doing inference it's more like
a greedy approach but let's see how we
do that okay so let's just start with
some definitions but definitely ask
questions to the talk cuz i think the
these two example is a simple enough
so we should be able to understand the
proofs and everything okay and I want to
do these in parallel because they share
a lot of similarities and you know this
is the one side of notation I think this
this should all be reasonably clear but
definitely ask questions so for the
topic model we're just going to consider
the single topic case but let's go
through the case a mixture of gaussians
and the single topic model in the
mixture of gaussians we think of having
case centers mu 1 mu k these are points
in a vector space and in the topic model
let's think about having K topics okay
and I'm going to use the same notation
so the topic case each of these new eyes
or a distribution of awards okay so you
good with that okay and then now we're
going to think about how we generate a
point so the nature of Gaussian case we
first sample some cluster with
probability WI so that's my parameters
and the topic case we're going to decide
on a topic with probability WI so that's
why they're kind of analogous and the
mixture of Gaussian caste what do we
observe we observe the mean corrupted
with spherical noise because that's the
case I'm going to consider I'm going to
consider the case with sphere-cone oiz
and this really is the underlying
probably stick model of k-means right
and so k-means what do you do you assign
things to the closest point so this we
can think it was the the probabilistic
model for camions okay so we just see a
point corrupted with noise and we're
going to see many such points okay so in
the case of topic models we're going to
see a document and documents going to
consist of M words which are sampled
independently from this topic okay so
it's an exchangeable model so in the
document just consists of M words and
they're all sample at iid from UI so
that so we so at this level we kind of
see some distinctions between these two
models and the mixture of gaussian case
we're adding noise to one of the means
and the topic model case we get many
words drawn independently from the same
probability distribution of the same
hidden topic
it's a multinomial model yeah that's
right it's a multinomial model here and
independently so they're exchangeable
and this is kind of why it's a bag of
words
it doesn't matter the order in which is
he the words we see this collection of
samples you put single in parentheses in
there indicate that this is not the
topic models that are commonly used in
practice where you have a mixture of
topics so we're gonna come back to the
case of multiple the LDA model I mean
this is one of the standard models used
but we often like richer ones like le da
but will will be able to handle that as
well but in terms of understanding
what's going on I think it's actually
helpful to consider these two in
parallel because these are really one
hidden mixture component and the
learning question is we're just going to
see a bunch of samples and what we'd
like to recover are the means or the
topics the mixing weights of these are
just real numbers and Sigma for the case
of the mixture of gas so there are
questions here about this and I like
this because we kind of see why how
these models are similar and how they're
different and the main difference is the
topic model we get multiple samples from
the same hidden state and here we kind
of get this a different notion of noise
so we good okay and let go
you know so we see multiple documents so
this is it's right so so this is one so
in the mixture of gaussians 1x is a
point okay and the topic model you see
multiple documents so think of the
analog of X here to be M points here and
we see multiple documents yes each point
here corresponds to one documenting the
document is this collection of words and
that's analogous to one of the exits
let's go back to the dimension come on
Ivan right now I just think of these as
discrete but will formalize that later
how do you think think about it
correctly but we're good with that model
okay good so how do we learn these
things okay and learning is good these
sample points how do we figure things
out and there's a lot of work that's
been done on this I'm not going to go
through the details of all of this work
but there's really a ton of work from
the theory and m/l community trying to
figure out how to learn mixture of
gaussians back from a really nice paper
by Shawn Joey over a decade ago and you
know a lot of the work was really on a
case where the gaussians are very well
separated they don't overlap at all and
then you know what kind of approaches
can we use to figure this case out but
they have relied on a ridiculous amount
of separation and then there's some work
where they can actually overlap or where
they're much closer together and most of
these are basically more like infamous
information theoretic results and
they're all typically exponential in K
where you're searching all over the
place well these are all okay right so
all the ones I've listed up here are
mostly Theory results where you're
trying to find the global Optima or it
may be I think it's something provable
rather you care about computation which
is why they're all either exponential in
the number of topics or the clusters are
very well separated and you can in even
their it's tricky for what to do you
know the procedure for the ones with a
lot of separation are but they're more
based on distance based clustering where
you kind of some of them may introduce
the right notion of using linear algebra
approaches but all of these are quite
different because no one has a very good
understanding of the e/m type approaches
and how to but these are the lot of its
theory and some of them give interesting
algorithms they write in insights for
topic models there's also been a lot of
work you know so Christos and
and other people had one of the early
papers on how to view topic models as a
matrix factorization approach this been
some really nice work by Joseph Chang
which kind of heavily influenced our
work more from phylogeny trees and even
recently there's been some nice work by
Sanjeev Arora on the multiple topic case
looking at this as an nmf problem and
how do we kind of do non-negative matrix
factorization again they can prove it
with only separation conditions so this
is basically there's a lot of work you
can ask me later on about details of it
and to some degree we're going to take a
pretty different approach from a lot of
these a lot of that work so let's just
back up and forget about sample sampling
issues and just start thinking about
identifiability and let's go back to
this old idea before maximum likelihood
by Pearson so Pearson was the old
statisticians is like how do we estimate
models and his idea was something called
the method of moments which is we see
averages and data how do we figure out
the parameters which give rise to these
averages and you know going back to this
multiple topic case we started
addressing this by identifiability
questions so what's the question here
let's look at our moments for the
mixture of Gaussian casts the first
moment is the mean the second moment is
the expected value of X X transpose the
third moment is this tensor and they
just keep getting bigger for topics we
can think of the moments in a very
natural sense we can think of the first
moment is corresponding to have
documents with one word in them we think
of the second moment as documents would
to suppose we had an infinite collection
of documents with two words in them what
do we know what we can figure out the
Joint Distribution of two words in a
document we think of the third moment is
the Joint Distribution of three words in
the document and now forget about you
know sampling issues we can just ask
identifiability question which is how
many words
how long do documents need to be before
the parameters are well specified
because if every every document contain
one word in them and I had an infinite
collection of documents I would know
this exactly but it's not identifiable
obvious
city right you can't figure out the
topics if every document has one word in
them okay so we can ask this even more
fundamental question which is suppose we
had exact moments when other model is
well specified in what order of the
moment suffices to nail it down and this
is an interesting question when you have
multiple topics per document because
another question is how long do
documents need to be right say if every
document had five words say five topics
in it
I'd have say a hundred possible topics
how long do documents need to be before
the model is identifiable there's an
information today is independent of
sampling issues and and for the most
part let's just proceed for now given
that we have exact moments and we want
to address the identifiability question
and then we want to see can we invert
these moments efficiently can we take
these these moments and figure out the
parameters and as you note that these we
can think of as all as are the matrices
or tensors okay because this is a 2-byte
this is like the bigram matrix and this
would be like a trigram tensor okay so
we get with that so this is an even more
basic question and this is what I mean
by closed form solutions I'm going to
show you can easily figure it answer
this question okay good Oh merci
okay so now let's just look at why I'm
comparing these two and just to use some
vector notation because it keeps us on
the same footing so the mixture of
gaussians we've got K clusters
we're in D dimensions in the mixture of
gaussian case or we could have D words
typically we think of D being bigger
than K okay we have more words than
topics more dimension in the clusters
because the mixture of gaussians case
what's the expected value of x given
we're from cluster of I it's just the
mean the definition right for the vector
notation it's helpful to think of words
like this is the first word as these hot
encoding is where this says the second
word is on okay why is this handy
because if we use this encoding we can
think of these news as probability
vectors if use a D length vectors which
sum to one and then we can think of the
probability of any given word given that
we're from topic I is just being mu I
okay so this is just the expected value
of a word given topic and this is just
new I is why vector notation is handy
because it kind of keeps things on the
same footing if the noise model here is
obviously different the noise model here
is spherical the noise model here is
multinomial and it depends on the
particular would we're getting maybe
that's just notation are we good with
that okay so now I just start looking at
these moments and trying to figure
things out okay what is the first moment
look like okay when the mixture of
Gaussian model it's just the average of
them UI's it's a data center and the
topic model is just the average of the
topic topic probability vectors and
obviously the model is not identifiable
from this because you know this is
always specious arguments but parameter
counting suggests it's not enough and
there are definitely problems with
parameter counting arguments but
nonetheless we know you know the mean
isn't enough okay we good with that
forward hill
all right so that's look at the second
moment okay so let's look at the mixture
of gaussians model we're looking at I'm
going to use this kind of an outer
product notation instead of X X
transpose so what is the second moment
of mixture of Gaussian
it's the expected value of X X transpose
each X is a mean plus noise right so
what you get for the mixture of
gaussians model is basically a
contribution of how the means vary right
because you've got this picture where
you've got these memes which lie on sub
subspace actually I'm gonna put the X's
it means and then we've got kind of
points lying around the means okay and
the contribution for the second moment
is kind of how the means are configured
with respect to each other plus the
Sigma squared times identity matrix due
to the the way that the due to the the
variance and the points okay now
something really nice happens for topic
models a sort of discussion zapala to
when you have kind of cross correlations
and matrix so if you look at the the
Joint Distribution between two different
words these words are sample an
independently conditioned on the topic
so the noise is independent so the Joint
Distribution of two words if you viewed
as a matrix is just the weighted sum of
the means because it's just you know if
you condition on the topic the noise is
independent so it's just the expected
value this is just the expected of x1 x2
the condition on the topic two
independent and the expected value of x
given the topic is just mu I so the
Joint Distribution of the bigram matrix
is just the average of the mean mean
transpose yep
words in each document the dictionary is
of size man yeah you know the dictionary
size D and you get em samples from the
document so so literally the document is
just take the first two words so in this
one suppose the document just has two
words in it this I'm looking at the
joint dish what a nice working on
different words in the vocabulary you
know X 1 is X 1 is the first word X 2 is
the second one okay so we're looking at
the Joint Distribution of the first word
in the second word and those two are
independently drawn given the topic so
the average value of X 1 X 2 transpose
is just mean and mean is you know think
of a mixture of gaussians case if we
could get two different samples from the
same Gaussian then these noises would be
independent and that would go away from
the mean so what I mean by identifiable
is there are two different models which
could give rise to the same means so if
you only knew the means could you figure
out the parameters in your case more oh
yeah two clusters differ only by their
mean so if I know the mean I can
yeah so if I just if in the mixture of
gaussians model if you just knew the
global average of the of the means all
you see is in this case the global
average would lie somewhere here if you
just see this point you can't figure out
those X's the means yeah that's exactly
so I you just know a of X and now we're
going to look at a of X X transpose and
what we get is this and we get in the
Gaussian kiss so the topic model kiss is
this and now we see the first
distinction because this thing is a
lower rank matrix what's the rank of the
bigram matrix okay it's the number of
topics where as this thing is for rank
it's dimension D okay this is the first
distinction and this is handy
exchangeability is really a wonderful
property being able to get many samples
that are independent condition on the
same state because we get the Salone
rank matrix it's still not identifiable
because basically all we learn is
intuitively it's basically you can think
of this geometrically is what we learned
is an ellipse for where the means lie
it's kind of what a second moment matrix
tells us we basically say you know we
basically just figure out an ellipse
which is like the covariance matrix and
and we don't really know anything past
that this kind of a rotational problem
but we're getting close to the right
number of parameters but you can
actually show it's not identifiable and
it's not identifiable in the worst sense
it's not like it's basically every
single model is not identifiable it's
not just a few points get confused
should I expect like wingtai squared is
less than B
okay in the movie topic phase I know
actually
well beginning there in a second
parameter County suggests okay well how
big is this this is size D squared and
how big is three D cubed what D cubed
has a lot of parameters in it so but
this is definitely fishy because the
some models were are just fundamentally
not identifiable
even when parameter counting arguments
wears less than D the oh I think brand
matrix still be loved right so think
well this is always the low rank say say
say say this is only low rank when D is
bigger than K so it's not can't squared
versus Z its D versus K right in the
single topic place but then the motion
topic hey I know this one's gonna be
surprising so this one will break your
intuition that this way the
identifiability question was really nice
to us because we need multiple topics
like how long do documents need to be
before you can identify it and forget
about computation because we didn't
expect an efficient algorithm I just
wanted to know the exact answer that's
question and it had to have an exact
answer but we couldn't for the life
because some of the difficult to
identify ability some is very hard to
make identifiability arguments that are
non constructive so there's a really
beautiful theorems from the 70s by Chris
Cole and how to do this but they're very
difficult because did you know their
existence proof so so the kind of very
interesting questions but we'll
definitely get to that moment method is
not as good as making life
let's get back to that sir so there's
kind of a long debate between
statisticians between these two and
Fisher basically argued the moment
methods so Fisher basically argued
maximum likelihood was more efficient
yeah it basically is criminal so he uses
the sample is better but the point is we
kind of new moment methods were easier
to use like even in simple examples like
you look at dude on how it the first
addition I think it says you know for
some of these problems you want to start
with a moment method and then use Newton
and then kind of the modern versions of
statistics by modern and more like
theory statisticians they said do method
of moments because maximum likelihood
isn't consistent and then do a step of
Newton and then they argued that was as
efficient as maximum likelihood because
maximum likelihood does stupid things
with infinities so the best thing to do
for getting the constants is they're
basically consistent with each others so
do the method of moments then do a step
of Newton because it's a little local
optimization and that's as good as
maximum likelihood we're trying to do
terms of moment we'll get to that in a
second but we're trying to solve it and
forget about samples right now let's
figure out how to solve the moments and
for now let's drop the mixture of
gaussians case okay well come back to it
because it's not low rank let's just
proceed with the topic model case and
then we'll come back to the mixture of
gaussians okay okay so again now we
should kind of we see what's going on so
if we have this trigram matrix what's it
going to look like well the noise is
independent we're just going to get mew
mew mew right so what we get with so now
let's suppose documents have three words
where do we get well we know I'm just
gonna define em to to be the by ground
matrix empty a is going to be the
trigram probabilities and it just looks
like the sum of W is Miu Miu Miu
so it's this d cube size beast okay and
so the same argument the noise is
independent okay so now how do we figure
out the muse from this okay now let's
just work in a slightly better
coordinate system but basically this is
the problem right now how do we solve
the
structure we see a matrix that looks
like this we don't know the muse and we
see this D cube sized object that looks
like we know it's guaranteed to look
like this but we don't know what these
Musa how do we figure it out
now geometrically let's just think in a
nice coordinate system I was you know
whenever I see a matrix I like to think
about isotropic coordinates so let's
just think about transforming the data
so that this bigram matrix is the
identity and it's lower rank so that
means when we transform it we're going
to be working in K dimensions now so
just take you know the bigram matrix
make it look like a sphere that means
we're going to project things to K
dimensions and we have K by K matrices
and we do that we're going to do the
same linear transformation of the tensor
which means the equivalent way to look
at this problem is we know the second
moment is isotropic it's a sphere we
have a third moment that looks like this
where now we're in K dimensions and what
this transformation to a sphere does is
it makes these means orthogonal okay so
basically you know in this problem we're
now working in a coordinate system where
these X's are orthogonal and this is in
we someone says look here's a K cubed
object which is pretty small it's
guaranteed to have this decomposition
you know they're orthogonal what are the
means no this is a this is a this is a
three-dimensional decomposition because
this is each of these or D cubes so we
know such a decomposition exists for
this cancer it's three dimensional how
do we find it okay so that's no you're
not gonna I'm not gonna do that but it's
there's many different ways to do it but
that's the question so we really just
reduced this question of how do we do
this decomposition okay so just back up
a second and look at
things as linear algebra operators so
let's go back to matrices remember this
notation where we have a matrix M two we
can hit it by a vector on the left and
right which is you know we can look at a
transpose M to be a notation I'm gonna
write that as hitting M 2 with a and B
this way and just usual matrix
multiplication means we just sum out the
matrix over those coordinates and for
tensors we can also check think of these
things as try linear operators we can
take a tensor and you can hit it with
three vectors and just in the same way
you do matrix multiplication you just
take this tensor which is has three
coordinates and sum them out over these
three vectors this is exactly so it's
kind of linear in each of these
operators right it's you know multi
linear hit okay
what's an eigen vector of the matrix
looking at in this form well if it's you
hit it with a vector you get back the
same vector scalar now there's a
generalization of eigen vectors for
tensors whereas just hit the the kind of
the cube now rather than a square hit it
twice with a vector you're going to get
back a vector because right you're
sending over three things so if you hit
it twice with the vector you would get
back a vector and we're gonna call it an
eigen vector if that Direction is
proportional to the direction we hit it
with so this is actually pretty well
studied in various areas of mathematics
and other areas
the problem is tensor is horrible in a
lot of ways in the most general case
because they don't inherit a lot of the
nice structures of matrices but we can
still define them and there's a kind of
active area of study as to what these
things look like but that's a definition
it turns out for our case they're well
behaved okay so what's our case our case
is we had this problem we know this cube
looks like a sum of orthogonal vectors
okay any guesses
well let's what are the eigenvectors of
this whitened tensor okay so let's just
hit this m3 with two vectors V what does
it end up looking like well it ends up
looking like you know this beast was w
mu mu mu we hit it v on the muse twice
it looks like w v dot mute y squared
times mu I and if we want to find an
eigenvector this had better be equal to
lambda V okay so let's suppose view with
mu1 they're all orthogonal so I put mu1
twice in that expression where do I get
on the right hand side with the music
with agonal so what do you get
yeah you just did w1 times new I know aw
Twitter one from the transformation so
the only so and and this is it that that
basically all of the tensor eigenvectors
all the topics exactly they're projected
and they're scale and so it's easy to
unprojected oh the whitening matrix we
just stopped it back in d dimensions and
then we just we can figure out the scale
because we just make it something to
watch their probability vectors okay and
what was the assumption we needed to get
this to work out well what did i do well
i made things white when does that work
I need the topics to be linearly
independent that's the only assumption
is obviously if one topic is in the
convex hull of the others this might be
problematic because in its use even
identifiability questions but as long as
things are well conditioned which
basically always occurs in general
position this is a minor assumption so
as long as the topics are linearly
independent which is minor all of the
tensor eigenvectors are the topics very
clean statement and the reason tensors
are nice to work with is because for
this particular case we have this nice
tensor structure and general attempters
are amiss but this is about the nicest
possible case ya know in third moment
suffice sister identify it and this is
the decomposition you need and this kind
of know multiplicity issues either it
turns out you know as eigenvector is if
you had the same eigenvalue or linear
combinations were also i can visit
there's no issues here because even if
all of these w's were the same it
doesn't cause problems because of the
way tensors worked because it is cube
you can't get linear combinations being
eigenvectors we drop the gaussian case
for now this is for the the topic model
case with one topic per document the
cornellà can be arbitrary yeah so so so
now this statement is you just e
documents of length three any number of
topics as long as they're linearly
independent
and this is kind of the closed-form
solution which we can think of as an
eigenvector problem you know the topics
are going to come from the rank of the
biker and tricks' basically you just
look at the rank of that matrix and
that's the number of colors
and the Americas of this four emoji
we're gonna get back to that so now the
question is what's the what happens in
mixture of gaussians what happens in LD
a and kind of richer models but this is
a clean way of understand what about an
algorithm for snout too because can we
solve this thing you know we know I did
this for I get four matrices general
ones are hard but it's basically known
you know we did analysis now just
basically analog to the power iteration
you just hit it twice repeat because you
want to find a fixed point this is
exactly how matrices work and then
deflate and this thing converges
insanely fast because it's even faster
than the power iteration for matrices
which is like log 1 over epsilon log
wanna website basically this powering
means you converge extremely quickly
okay so it's a very fast algorithm
different ways to view this which I find
kind of interesting from a geometric
perspective is we're basically
maximizing skewness you can view that
eigenvector condition as saying it's
almost like a subgradient condition
which says maximize m3 hitting it with
v3 times okay which is like kind of
maximizing the variance in the third
moment okay and it's kind of like
finding the spiky directions and that's
kind of a another way to view it and
this is why it's greedy because it says
all local optimizers of this third
moment other solutions we want which is
why you can kind of rip them off one at
a time because kind of every spiky
direction is the one you want and so you
know there's a lot of different
algorithms which are efficient because
they're greedy these are very similar
decompositions from those studied in ICA
because somehow the same tension
structure arises in that setting so we
understand it and we also understand
some of the statistical questions
because if we don't have exact moments
you just use plug in estimates but it's
just a perturbation argument and the
stability it kind of just depends on the
you know how overlapping the clusters
are but that's real
but now let's so we're good with that
this is a minor I mean the real thing is
understanding the closed form structure
and the rest is prohibition but now
let's go to the if we're done with that
I we should look at the mixture of
gaussian case
and the multiple topic is so we good
with this and this is pretty cool pretty
clean I think we hopefully understood
the proof and everything so now what
about the mixture of gaussians case ok
let's go back Sue's this pesky Sigma
squared I and if you look at the third
moments we're gonna again get problems
okay but what Sigma squared can we just
figure it out it turns out yeah it's
supposed e was bigger than K strictly
bigger than K so the previous were
algorithms worked if D equal K with the
topic ones but for now I just suppose D
is like K plus one or bigger which is
always the case right like a picture
like this with Sigma squared is just the
variance off of this subspace ok so
right because basically in the subspace
if I look at the direction of variance I
get a contribution for how the means
vary and the way the points work out but
if I look at the variance kind of a
direction orthogonal to the subspace the
only contribution from the variance that
direction is Sigma squared okay so that
actually what that means mathematically
is the minimal eigen value of this
matrix is Sigma squared because these
things lie in a K dimension of space
just to look at any direction orthogonal
to that which will happen if you look at
the minimal eigen value or just the K
plus-1 Tigan value that's Sigma squared
so we know a sigma squared it's just the
minimal eigen value of that matrix which
we know okay and and we need a dimension
to be one bigger for that argument
so it's estimable so we can just
subtract it out so we can basically
figure this beast out and the way we do
that is we look at the second moment
matrix we figure out the noise get rid
of the noise yep why is it going to be
the smallest part of WI
they see the point is you're gonna use
an orthogonal direction because this is
a PSD matrix so look at hit this thing
with any direction V right you're gonna
get V on this guy but if we can
guarantees the the views to be
orthogonal to this we can get it to be
zero so the minimum C right all I'm
saying is geometrically is you can pick
out the variance off the subspace
without just by minimizing the variance
so yeah if it's a greater and it turns
out there's a cheap trick that even if D
equalled K which was the case we want to
be linear you can still do it because
it's a very natural point that actually
but let me just go back here I don't
want to show you the math the mess but
basically if you just look at the
covariance matrix so when you subtract
off the mean Sigma squared is the
minimum eigen value of the covariance
matrix so even if D equalled K you can
still figure out Sigma squared just the
minimal so forget about that that case
it's not Sigma squared is now okay so we
can get rid of that now what about for
the third moment because if we look at
the third moment it's not just me mu mu
you're gonna get extra junk okay but
let's just look at what that is so let's
just go to one dimension suppose we're
in one dimension and we have a random
variable which is a mean plus Gaussian
noise where n is zero Sigma squared
what's the expected value of x cubed
well we get one term which is like new
cubed right and with the other term
it is squared expected value data squid
is Sigma squared times nu and there is
accounting three expected value is this
so basically you know the topic mod we
want nu mu nu I'm just doing it one
dimension we get some extra junk
yeah three Sigma squared meal but we
know mu because that's the first moment
that's a of X and we know Sigma squared
regretted this is one day so now we just
got to write this in kind of a tensor
way so basically the way you think about
you know three think of three Sigma
squared mu as equal to three is equal to
Sigma squared times one times one times
nu plus one times nu times mu plus mu
times one times one kind of stupid way
of writing it but the only reason I say
that is because so m2 what do we do is
to check out Sigma squared times
identity has been estimated and then we
just do the tensor version of three
Sigma squared to get rid of it which is
basically and these e eyes are the basis
vectors so it's this is really just you
know mean one one one mean one one one
meeting okay minus Sigma sway and and
these two exactly have the structure we
want this means for the probabilistic
model underlying k-means weight and we
basically can construct this form the
eigenvectors are the projected means up
to scale so how do we on scale it turns
out you can get the scaling from the
eigenvalues themselves in this case but
this is the main point then the main
point is algebraically the structure we
have if we just rip out the noise is
this just pretty neat and and this is
why we see the difference in this topic
model is because we didn't have
exchangeability the noise correlates so
we've got a futz around with the noise
because we can't get these low rank
matrices but doesn't really matter we
just get this and then you know we get a
closed form solution with the exact
moments now let's go back to Greg and
hunkers result they gave
expansion lower bound but the way they
did it is they put key gaussians on a
line in a configuration of a very
particular configuration and what that
effectively does is you know the
assumption we needed to solve this
problem is they had to be in general
position and K gaussians on a line or
not in general position and so what's
kind of nice that we actually know
there's some gap in between that if
they're not in general position it could
be bad and if they are in general
position it turns out we only have a
polynomial dependence on the separation
condition so by that I mean you know in
general position they still could be
close together we could look at the
minimal singular value of the matrix of
the means and it's only a mall
dependence there where somehow once they
become on a line you could be
exponentially bad in the separation or
is it their general position it's
somehow polynomial be bad and you can
kind of see why because if they're on a
line the third moment is still a number
so it's not identifiable from the third
moment if it's a number you have to go
to very high moment and estimating very
high moments is unstable now that's only
one algorithm and they prove it
information theoretically so it's for
any algorithms but it's the intuition is
nice so that's how he got around the
lower body we don't know how to solve
the elliptic case and and there I don't
I suspected I don't even know how to
prove hardness results there cases which
I think statistically I'll find the
estimate but computation we have no idea
and I don't think there's any language
for how to even understand hardness in
these average case type scenarios so
many wait for this very natural case
it's just basically I've seen the poo
this is it okay
it's pretty simple and the very last
thing I want to do is look at this case
of multiple topics oh yeah so that's
basically a like a basically the analog
of a matrix prohibition argument that if
I'm willing they say Polly it'll appear
in the paper for now but basically you
know for SVD is there's like weddings
theorem and these kind of theorems of
how accurate is an SVD if our matrices
are perturbed and this is just the
analog how accurate are these
eigenvectors if the tensors are
perturbed and that's what I'm what I was
referring to here what it depends on
basically are how collinear the means
are but in a nice way and that's kind of
real because as the topics start
becoming collinear you're gonna expect
to need more samples and that's because
the stability of an SVD even for the
matrix case the stability of SVD depends
on the minimal eigenvalue but it behaves
kind of nicely in the same way our
matrix perturbation theory is nice and
the only time it starts becoming bad as
if it becomes actually degenerate we
don't know how to solve it but it
basically it's all like 1 over root n
and nice dependencies but I'm not gonna
explicitly give those theorems here but
they will appear in the paper and and
they're kind of dependencies one would
expect and I think even statistically
they're real they're kind of information
theoretic
but the point is they're mild not like
these exponential
whiskas Thanks so I want to look at for
a finite sample first and she would use
like the actual minimum eigenvalue for
Sigma squared or do you like correct it
so what I would do is I would i would
first projected in the k dimensions and
use the minimum in the k space there's
actually a trick i would do is i would
actually try to use a slightly different
model for the mixture of gaussians we
can talk about that later I I would look
at it more like a topic model case which
I'll get to because this is more like a
spherical case and I don't know how to
handle the none there's another case of
mixture of gaussians I can solve but
maybe let's come back to that ended it
in the discussion section let's go to
the multiple topic case unless they're
no no it's the eigenvector so I can yeah
just do the eigenvectors of the tensor
the many different ways to solve that
problem
I mean in a sense this is the moments
I've set it up for you and just go to
town the point is we know there are
algorithms that solve this they come
from ICA you can actually do this with
two SPD's as well so the earlier work we
were using kind of bad algorithms to do
it because you can kind of project this
down to matrices and kind of you know
there's this term simultaneous
diagonalization that's another way to do
it but geometrically once we instead
this it's like this is the structure we
know how to solve it you can think of it
as an eigen alized eigenvector problem
which is a really clean the eigenvectors
are telling us the means yes up to scale
but we can find this scale and then we
can find the W we got Sigma square
because that's where how he got these
formulas it's the minimum the covariance
so let's go topics a topic model case if
there's multiple topics for a document
so now you can have a document like 30%
cute animals 70 percent YouTube or
something like that yeah how do we
figure this out well what's the
identifiability issue you know if every
ducky about five words now do we need
three times five or how long do they
need to be and it would be bad if the
moment had to increase because in a
sense estimating higher moments becomes
exponentially more difficult in the
order of the moment okay but parameter
accounting suggests third moment is
enough still even if you have a mixture
of multiple moments you've got D cubed
parameters in third moment and for a
long time we thought you you know the
LDA problem had to be exponential and
this is borne out because Sanjeev and
some other theory ditions basically gave
very strong assumption solve it you know
people who did not think you could do
this in closed form but it turns out led
eight all you need is three words per
document and it's the same idea of just
maximizing some third moment so Lda
basically you know you have to specify a
distribution over distributions because
now every document is about a few
different topics so it means you know
every document you know you have to
specify this distribution over
distributions and these Lda
distributions are these kind of nice
pictures we have level sets here so
that's what this prior is here so this
pi is rather than this document being
about topic one that this document is
going to be about thirty percent
Carano most 70 percent YouTube that's
specified there and that's this kind of
distribution over the triangle which has
a particular form in a sense it's like
the nicest possible form you could
distribution you could put down for a
triangle and it captures paucity because
you could have pictures where these
level sets kind of bow out okay and the
point is that you know you just got to
write down what these expectations look
like and you know
one physicist you're working with years
ago that's not so bad he just integrates
in gamma functions but the point is you
just look at the structure of these
things and the same trick like for the
mixture of gaussians because now if you
look at these correlations you've got
kind of it extra terms in a similar kind
of way and you just subtract them out
the stuff you don't want and this kind
of has nice limiting behavior
what were the kind of coefficients of
this subtraction basically depend on
like a sparsity level of the the model
which is often set in practice so so the
only parameter we need now is the kind
of the some of these alphas which is
often said in practice and it's like an
average level it kind of determines the
sparsity level so if you know that you
can kind of determine what to subtract
and it kind of blends between two
regimes so when alpha goes to zero you
kind of go back to the symbol single
topic case where these go away and
there's more stuff here because it looks
more messy you know you get all the
symmetries ations and stuff like that
but it's still easy to write and it also
becomes very large with these moments
actually end up looking essential
moments when it alpha is large what this
thing ends up looking like it looks like
ye of EXO and minus the mean times x2
minus the mean and same thing here and
that sort of should be the case because
it's this triangle starts looking like a
product distribution so these two
regimes are kind of nice and I like this
kind of maximizing perspective rather
than the tensor augen vector because
basically you know you want to find
these pointy directions
so this skewing is almost like a
geometric problem how do I find the
corners of a convex polytope from its
moments it isn't you know always
different perspectives are interesting
and that's basically what's going on
here I'm just looking at this diversity
distribution skewing things a little
based on the way the measure is put on
the simplex so the you know the
maximizers point to the corners that's
it so Lda has this closed form solution
you can kind of tweak these moments with
lower order things which you know they
have the same structure the eigenvectors
are the topics
solution for a different objective right
if the moments are exact it's just a
closed-form solution that's what I mean
okay so if it's not a closed-form
solution it's an inverse moment solution
but as I said before at least in some
cases in Costco stats we know if you did
the moment estimator and then did a step
of newton that is about as good as
Emily's and for these problems it might
actually be better adjust you'd there's
often like infinity problems with MLS so
in practice you know it's actually kind
of nice to use moment estimators and
then local search on top of that so a
variational basis
yeah I guess inference is hard so yeah
you'd have to do like sampling or a
variational after that okay so that's LD
a and again with you raise the point
this is nice because influence is a
headache in these models and it's hard
and that's it was very important for
this you know the single topic models
mixture of gaussians it's easy to do
inference these problems it's hard
because even just writing down the
posterior is not close to home yet we
can still solve it in this greedy way so
somehow we've gotten rid of inference in
these moment approaches same format the
moments just figure it out any way you
want okay I can recommend it to the echo
vector so but you have to cover that I
do venture into the parameters with the
eigenvectors are the distribution so you
just unprojected and normalize it no
choir underwear there's a prior on the
topics distribution is because the word
distributions we think of miss
parameters those are the topics there's
no prior there
so that you'd go up to the moment I mean
in a sense no think about more this
identifiability viewpoint rather than
global optimum if you know these moments
uniquely specify the parameters and they
have a closed-form way to get it and
then it's a kind of a crude addition
argument you need you a second a second
or that's gonna be close to maximum
likelihood because you're you're
basically chasing the maximum likelihood
cost function at that point but a step
of mutant is pretty darn good so that
you will get you this kind of bull not
local that'll be actually it's not gonna
be low it's going to be close to the
maximum likelihood solutions can be
close to the global maximum likelihood
submission because the parameters are
close you'd have to do some like
convexity argument yes this is this
wacky okay so now we're getting close to
discussion section so I'm just answer
this so this is where it's funny with
these average case hardness results
because we know a maximum likelihood is
np-hard in general yet what this is
saying I'm gonna argue that this will
give us something close to the maximum
likelihood solution because we're
getting the true parameters and I can
give you a poly sample size so even
without doing this step of Newton I'm
getting close to the true parameters and
the maximum likelihood solution the
global one is also close to the true
parameters so what's the contradiction
here well the contradiction is this is
like an average case result it's saying
if the points come from the distribution
then we can find it and there are cases
where we have average case results
knowing the distribution disparity
between milliliters is an organ
that's the city and the hardest results
are showing that there exists if if we
could solve this problem for all point
configurations then P equals NP this is
not saying that this is saying with high
probability for configurations that tend
to look like the model is correct those
are the ones we can solve which is why
from a complexity point of view it's
very hard to start understanding average
case hardness when the model is correct
because I think there's cases for these
models where statistically like a
mixture of gaussians case I'm sure there
are cases where you know when it's not
spherical statistically it's fine and we
know those cases yet solving them we
don't know how to do and how you improve
a lower bound we don't have it's like
only the Pullman or something is kind of
interesting cases we know how do I give
a lot of bones for it but anyway maybe
get back to discussion so one more slide
I just want to say the idea generalizes
to other models so hidden Markov models
length three chains you can estimate it
with the same idea you can kind of pick
up tensors that look like that some
other recent work on how to questions
for like structural earning so these is
modeling linguistics called publizity
context-free grammars and you see the
sentence like the man saw the dog with
the telescope is the man seeing a dog
holding a telescope or is he you know
holding the telescope and how you
figured out it involves parsing problems
there's a wonderful set of questions
here for how you learn these models
turns out we made some recent progress
in showing the even if you had infinite
statistics the general phrasing of these
problems are not identifiable if you
only see sentences which is pretty
frustrating with some work with percy
under some restricted assumptions we can
make progress on these models but again
there's an interesting in between for
the restrictions we make and the non
identifiability of these models which
would we like to make progress on some
also some recent work on learning the
structure of bayesian networks with
these models so suppose you see some dag
you observe these nodes here you would
like to figure out the structure you
don't even know the number of nodes or
the edges how do you even know this
network is identifiable we can look at
moments and try to figure out what's
going on
and here we actually need some new
techniques even to just figure out you
know identifiability because for example
suppose this network these are the
observed went up this way get more and
more nodes as you go up you wouldn't
hope this thing to be identifiable so
even just characterizing identifiability
turned out to involve some graph
expansion properties and then combined
with some moment ideas I was thinking
this would be interesting for hmm but I
was talking to Jason Iseman this past
summer and he pointed out that to say I
only look at adjacent trigram statistics
okay how do I get identifiability on
chains like a BBB star a or CBB B star C
say I have those two possible sequences
coming up for my model where the a is at
the beginning and the end but there's a
sequence of B's that's too long in the
middle cincin so these are models where
the assumptions could be wrong or you
have to use I mean there's kind of these
cryptographic hardness results that you
can make long chains and kind of hide
combination locks in them and to some
degree the hope is these are should be
divorced in practice because if I if I
see like some garbage fall and bang
something happens we really fall in the
future krypter krypter graphically this
is hard linguistic phenomena right like
I have agreement phenomena that can be
arbitrarily long distance now maybe I
should try to use tree structured models
that's right so I would argue that
hopefully for linguistics the regimes
were in are not the cryptographic
hardness regimes but the delicate C is
how do you kind of phrase the model or
to avoid that I'm not convinced that
trigrams just has to suffice to capture
some of these
Dependencies either so in practice they
seem to work reason they were for
initialization so Jeff Gordon's been
doing a lot of work even linguistics
micro Collins has been actually playing
around not with these eigenvector
methods but with kind of earlier
operator representations he's been
getting some reasonable results but I
would argue these are modeling questions
rather than fitting questions which is
why these models are very interesting
but now how do you fit them so it's this
is a wonderful questions across the
board here you the chance can be
arbitrarily long but you only need to
look at the statistics of three things
in a row to figure things out because
it's the same kind of idea you don't
need to look at very long chains it's
just the correlations and three normally
have enough parameters but you can
actually weekly my third moment you can
kind of construct these sensors and
figure out you know if you have any
higher moment you can also solve it
because it gives you the information
about the third what basically have the
same structures you can use higher
moments as well where the thing is if
you have longer chains you can estimate
the three better so it's even an LD if I
have longer documents I would just use
the longer documents to get better
estimates of my trigram statistics I
would rarely go beyond third or fourth
third is good if you have kind of
asymmetric things if you things are more
like katatak you want for me it's good
question soon so in a lot of settings
understanding the tensor structure does
provide us some interesting solutions
for these problems and kind of
surprisingly they're very simple
solutions and people in many different
areas and I'm actually studying various
algebraic properties of these tensors so
the paper for this stuff is going to be
forthcoming there's somebody agree it's
all in previous papers we just didn't
really understand the the tensor
structure we were solving it with this
basically simultaneous diagonalization
where we took the third moment projected
it to a second moment and
we're fussing the honor that way they
weren't really the best algorithms but
now we're realizing this is the
structure we have and there's actually
algorithm some ICA like they've
considered power methods even Tong Jiang
has some paper with gulab like you know
15 years ago with an advisor on tensor
decompositions for how to do this this
is joint work with a number of
colleagues over the years to particular
people are Daniel Serena NEMA Alan from
our Daniel has been working on me since
the beginning hmmm stuff he was an
intern with me at TTI he's a postdoc at
Microsoft I named Bo's faculty at Irvine
and she's visiting you know both those
two are fantastic they're terrific to
work with and just did so well right so
this is in a sense is what we're trying
to avoid when you learn these things
because somehow coupling inference with
learning mix makes it difficult when we
think about learning bayesian networks I
sort of don't want to think about these
explaining away things in the same way I
don't want to think about which
documents are about which multiple
topics in the learning process I want to
at least one way this it's not the only
way other methods might be good but it's
a different way of thinking about it
what can we recover from just the
average correlations but for the
learning the Bayesian networks actually
that paper just appeared on archive
maybe yesterday or today or something
but there we actually need more
techniques because somehow the problem
with Bayesian networks is you know this
Lda motto is almost like a one-level a
kind of model where these are which
topics appear and these are the words
which appear and we have an explicit
model of the correlations between these
topics in the other day model the
problem with these Bayesian networks is
we have no idea what the correlation
model is here at this level because we
don't know what's above it so what we're
actually using there is some ideas of
looking at a sparsity constraint which
is kind of what the expander condition
is doing and there's a paper from Colt
this year
how do you take a matrix and decompose
it into basically some kind of you know
weight matrix and then some uh but but
it's sparse times some other matrix
there's a nice paper by Dan Spielman and
we're really utilizing those techniques
along with some of these moment ideas
but even I mean the right way I could
think about is like what do these things
mean to even be identifiable and if you
could make a formal argument for that in
many cases I think that does reveal
something about the structure of the
problem because if it's not even
identifiable that's giving you some
intuition as to what the hard cases are
and these kind of hidden variable models
identifiability it's fundamental because
we don't even know the structure of
these things and it's clear you can put
in other nodes sometimes to give rise to
the same structure so it's just
different techniques though other
questions so I'll be around for the week
and I'd be it'd be great to chat with
people because we are playing around
with these as algorithms people
understand these algorithms and other
settings and they're very natural
particular feature is it just find some
pointy directions in your data right
it's like this led a problem the problem
is if you try to do this in your data if
it's noisy all the words alive really
far away from the corners but somehow by
averaging it is what allows you to say
I'm finding these pointy directions so
somehow you can't just find the pointy
directions like in the raw data because
words don't lie anywhere near the you
know where they should be and even
documents of length three it'd be you
know if I say this stuff this documents
about sports history literature and dogs
and there's only three woods in it you'd
be like you're crazy this is only three
words yet the so somehow you really do
need to do these averaging techniques
and there are an interesting class of
algorithms for initialization so we have
been toying around with that and it's
just another bag of tools we have but if
you're interested thing around or that
we find a chat and there's more broadly
this set of techniques because you know
I don't think they solve everything but
thinking about any problem differently
always helps us and this is a new set of
tools that I think
should be complementary to other
approaches</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>