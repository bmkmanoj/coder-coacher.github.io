<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scalable Language Specification 2013 | Coder Coacher - Coaching Coders</title><meta content="Scalable Language Specification 2013 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scalable Language Specification 2013</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QQjlpqARz7w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you
this talk is meant to be
tutorial and I thank you for sticking
around this morning if you have some
mild interest in understanding an
approach to property extraction from
programs or language definitions then
I'll try to make it worthwhile for you
to understand how the abstract
interpretation methodology might be just
what you're looking for and so as a
tutorial I am going to make my way
through what was stated in the abstract
published in this proceedings and so I'm
going to begin with some basic examples
some definitions and the important stuff
comes in the first ten minutes then
we'll cruise through the middle part
looking at how you apply these things to
different definition formats and how
it's used in practice and the last part
of the talk I'm going to return to
issues regarding properties as
propositions there are logics that you
get when you define a form of static
analysis and we'll see how these things
might scale upwards which is really a
tough topic as of course all of you know
scalability never comes easy let's begin
with the background there's going to be
definitions and details on these slides
more than I can read or maybe more than
you can rebut I wanted these slides to
be self-contained you can download them
from my webpage later and read the
definitions if you care but let's focus
on the pictures so in the upper right
part of the screen what I tried to show
is um the integers let's say we have a
program that takes an integer input and
produces an integer output and we want
to test it but there are too many tests
there might be just a subset we want to
test like maybe just two four six eight
but there's still infinitely many tests
how can we cover all of these tests well
this kind of question can be answered by
saying maybe I can designate some tokens
or properties that represent collections
of the integer chess and medium I can
run the program test the program on the
property tokens instead of the concrete
values so I've used say neg zero and
pause the sign of the integer might be
significant to what I want to test you
can see I've arranged them though as a
complete lattice because what happens if
in the middle of the analysis I can't
remember any more of the integer in the
middle of the program is a negative or
zero I lost track so you have to use a
token any it's the join or the the
weakening of information and then of
course dooley there's a bottom element
none means no data at all or dead code
now you see to the left of those
properties I've written out all these
integer test sets i've written a
function gamma gamma is the
interpretation function it says what
those tokens stand for and so the token
neg for example stands for the set minus
1 minus 2 minus 3 mumble and what we'll
see is that we're going to require the
gamma does something special it
preserves loudest theoretic meet as
intersection in the world of test data
sets that will allow us to define a kind
of weak in verse in just a moment but
for now think of gamma is telling you
what the tokens stand for so here's our
program in the upper left corner it's
reading an integer and we have these
very strong suspicions that the output
integer regardless of the value of the
input integer is going to be a positive
but how long are we going to test it
before we convince ourselves well if I
give it to my students they'll still be
busy doing it but we won't go there
instead we're going to take this clever
idea of executing the program with those
tokens neg 0 and pause that requires we
have to reinterpret the assignment
statements for successor the negation
statement in the original domain we have
to abstractly interpret them so they
compute on these tokens so we have to
have a way of converting the concrete
program into an abstract program that
understands how to read one of these
sign tokens and compute on it that's an
abstract interpretation and you can see
that I've proposed the abstraction of
suck and negate in the middle of the
slide and negate really works like a
charm because it's easy to talk about
what it means to negate an egg or a
pause or a zero there's a technical
problem with suck you can see that suck
can be described as suck sharp but what
does it mean to take the successor of a
negative token I don't have a precise
enough description language just now to
say that
when you add one to a negative you get a
non positive somehow they got left out
of my domain of properties boy this is a
standard practical problem lots of times
when people want to analyze a program
for a correctness property they have a
pretty good guess of what properties
they should use what tokens they should
use to do the analysis but they don't
always think of all of them all the
intermediate values that might arise so
as simple and contrived as this example
is it's critical because it shows that
sometimes your initial guess is it good
enough and you have to refine it
nonetheless we can go ahead and we can
calculate these tests and I calculated
at the bottom of the slide the test 40
the test for pause the tests were neg
I've covered all the integers this way
and I was unable to validate for a
negative input that the outputs positive
and that's because I lost precision
right there but you get the basic idea
it's a simple idea you execute on the
properties so you're doing an abstract
interpretation now this was the idea
Patrick who so proposed in his PhD work
and he and his wife raw do develop these
ideas over the decades and almost all
the material may show you today was
inspired by what they've done and so
this is really kuso kuso presented by me
there are of course some interesting
little angles on it but it's their
spirit is very strong in this we're
going back to this picture and I've
introduced a reverse map alpha and alpha
is what you use if you have a particular
data test set and you want to find the
token that most compactly covers that
test set so maybe I want to test that
little program for the data 2 4 6 8 10
12 what token best covers that dead data
test set it's alpha because alpha will
map all of those circle values into
pause so oops I'm sorry we want the
laser button where is the laser button
let's go back to where I was and let's
push this button so this said here maps
to pause but of course all of these do
the important point is if this test set
is abstracted and then I use gamma to go
back I get a larger set so these are not
exact inverses but they are a weaker
form of inverse called a Galois
connection or resituated residual pair
and of course if your category theory
should say it's a simple
a simplest example of an adjoint
situation so they work together yeah
Peter I'm going to add it later I want
to put this in just to point out the
very practical problem that sometimes
these things catches and in practice
it's extremely difficult to get all of
these covered say for a really large
program you just can't see all the way
through it so yes we will in a bit but I
wanted to bring up this point that
imprecision sneaks in so quickly in this
kind of work it takes a real talent to
design the domain correctly from the
beginning but yeah that's that's a
really important question we always want
to address so this is called a Galois
connection is the first important point
in abstract interpretation theory if you
set up a collection of properties
organize them as a complete lattice have
a gamma map try to ensure you should
ensure you have an adjoint situation
because you're going to win a lot when
you build your implementation we'll see
what you win here in just a moment now
now that we have that where did those
suck sharp and negate sharp operations
come from well in an algebraic sense
they have to be related to the original
suck and negate operations in a
homomorphic way but it's not an exact
homomorphism it's called a semi
homomorphism because if we write it out
as a commutative diagram which i did in
the middle of the slide what we see is
the original operation blue f and in
fact blue f can take a single input and
produce a set of outputs that means we
can handle non determinism no sweat blue
f has to be modeled by red f sharp and
you have to have the community diagram
with respect to gamma or alpha its
equivalent when you factor in that less
than ordering on the complete lattice of
the properties or the complete lattice
of the sets of data test values order by
subset either diagram works for you and
so in particular if you pretend these
properties are like compositions it
turns out that red f sharp is a post
condition transformer since blue f is a
function what if you have an exact
equality in your homomorphism diagram
well that's called a complete
at least that's what Roberto G occupancy
likes to call it and interestingly
enough if you have a commutativity with
respect to alpha if you have any quality
here that's different from having an
equality with respect to gamma you kind
of sense that because these arrows are
going in opposite directions it's a
little tough to first see why they're
different but remembering that the image
of gout of gamma turns out to form a
collection of sets which is a closed
family of sets like you haven't apology
then gamma completeness is what you get
when you get a function that's a closed
mapping that's closest closed sets in
topology and alpha completeness of a map
f is what you get when you have a
topologically continuous map entering in
verse images of closest to clone sense
later on we're going to find that gana
completeness is super important for
understanding the logic inside the
lattice of properties so that's why I
wanted to mention that now well turns
out in topology remember its equivalent
a function topological tenuous if and if
in matteson person she opens to open to
close to close we're doing an over
approximating analysis so it turns out
the image of gamma defines apology of
clothes sets when we move to
precondition analysis and we want to
having Terriers we'll end up with a
topology of open sets and then you get
the standard continuity that it's taught
so it turns out there's actually a
surprisingly a large amount of useful
topology inside this stuff but I won't
be able to talk about that today it's
just cool stuff you know you take your
algebra your topology and there it is
this is where program analysis comes
from so the second important point about
abstract interpretation is that you have
to have semi homomorphisms you can build
those mechanically in the sense that the
best abstraction of blue f is what you
get when you take the blue f and you
compose it inside the alpha in the gamma
and that gives you the strongest liberal
postcondition transformer and so that's
your goal now of course in practice what
people do is they don't always get
exactly this thing but they come up with
something which at least we'll do this
community commutativity stuff which of
course can also be defined in terms of
the less than business
so that's the second important idea so
we're dealing with complete lattices and
we're dealing with these semi
homomorphisms now it's a matter of
getting this machinery to work in
practice and so I've been working with
the sign domain but you know that real
programs they work with aggregates like
storage vectors in a raise and a lot of
times the property you want to track is
a relationship between elements within
the aggregate how do you state that wow
there's been a lot of work on that now
in terms of the sign domain let's say
that oh dear let's say that we have some
collection of storage vectors here which
define x and y and it turns out that all
of my collection there are my 15 storage
vectors the values of X&amp;amp;Y they're all
positive well my little baby sign domain
that i showed you 10 minutes ago it will
allow me to express that property but
notice the covering i mean wow it's its
way in precise we ought to be able to
tighten that up by at least stating the
bounds that x and y have and that's
called the interval domain but what
about the relationships of x and y maybe
x and y are intertwined in some
mysterious way well there are some
really cool abstractions similar to you
know building convex halts and one of
the most useful ones in practice is
called the octagon abstraction and too
long mean a develop this it's
characterized by set of linear
inequalities where the multipliers are
either 1 or negative 1 on the variables
and you get this really cool covering
which always has eight sides and this
was used by Antoine and the team at
ecole normale superior to validate the
absence of array indexing years and
arithmetic overflow and underflow in the
500,000 line flight controller of the
Airbus super jumbo jet Airbus gave the
CUSOs and their team a bunch of money to
basically employ these techniques to
check for these kinds of standard
concerns and they are able to do it they
mechanized almost everything but then
they took a couple of smart people like
Antoine send them down to two loose and
they sat down with the engineers you
know and it took them a couple of months
but they got through the whole half
million lines of code and they said
you're safe you can burn this into the
chips you can put this in the software
you're not going to have any worries yes
there were some errors yes there were
some ears absolutely were there any
errors that were critical I I don't know
about that I don't remember that they
documented this in a pldi paper a few
years ago and I bet you could find some
of those answers there Peter so this is
really useful in practice I know the
Boeing people in Seattle they do these
things but they're much more secretive
so they will tell you that they're doing
these things the polyhedron domain here
is a generalization of the linear
inequality and the polyhedron works in
similar way what school is about the
octagon domain is that the canonical
representation is a set of these linear
inequalities but Antoine show that you
can implement that either as as little
matrix values or little graphs and you
can actually pass those around is
abstracted versions of arrays and
storage vectors when you do this
execution like what I tried to show you
about 10 minutes ago 15 minutes ago on
the baby program instead of dealing with
a single variable and asking about its
sign you can carry around a little graph
and keep track of the relationships and
so this is the key to making this
technology work in practice and if
you're interested in inequalities and
relationships you got to ask the obvious
question well why don't I just model my
aggregates my storage vectors my data
structures by the inequalities
themselves and let those be the token
values and that's the basis of what's
known as predicate abstraction and that
lives in the heart of these model
checkers like slam and blasts which have
been used to validate properties and
device drivers because rather than mess
around with whether a number is positive
or negative instead you use the language
of inequalities in the standard starting
point this little naive example actually
is really useful notice that I got some
baby program and I want to ensure that
the value of Z is at least as large as x
and y at the conclusion I notice also
there's an internal predicate that seems
to play a big factor and that's that one
right there so what i can do is i can
outline those primitive inequalities
line them up as a little baby modeling
the value of this one the value of that
one the value of that one and then do a
kind of what you call a symbolic
execution but it's actually an abstract
interpretation
acting the concrete operations so now
they operate on that triple of
inequalities and then I want to see when
i hit the exit point whether or not i
was able to validate what I wanted and
this one I did because my post condition
is characterized as the conjunction of
those two equality's and and so what you
have here is an abstract model where the
program points are annotated with these
predicates and then you can model check
that abstract model and that's what
happens in blast and slam you've got to
be honest though that starting point
doesn't always work out with a happy
ending and you sometimes have to augment
the the abstract domain the properties
because maybe there's an internal
predicate you couldn't see and that's
what happens in this baby example where
we're trying to validate a simple value
that X is greater than or equal to Y but
there's in some internal messing around
at program points p2 and p3 you can't
see that at first but you do find that
the execution traces fail to validate
the output goal when you go through the
then arm so what do you do you have a
heuristic like wonder I wonder why that
failed right there I bet if I can
calculate the weakest precondition that
would need to go through this assignment
to get to my goal I could find another
predicate that I could add into this
storage configuration and so I rerun I
reanalyze this path here and I'm able to
validate it and this is also a critical
part of the refinement that happens in
slam and blast because they never almost
never validate some kind of correctness
property first time out they go through
this iterative process and they may
iterate forever because I know that you
realize and you know from your own
experiences that you're not going to
discover loop invariants in such a
simple handy way and so this is just a
reminder that the game that I've just
shown you of starting with some starting
inequalities and running them through
and trying to come up with an analysis
might not be able to validate what you
want because what you really need is a
loop invariant and so you can see that
this iterative refinement would add lots
of internal equalities and inequalities
but would never would never discover the
loop invariant on its own so you got to
add a theory
improver some heuristics or a human like
Antoine to help you out yes really what
a see garlic process here that's correct
this is what most people model checker
people call cigar counterexample guided
abstraction refinement yeah there's sort
of two very different things going on
right one is the act of coming up with
more more predicates these are sort of
these hidden invariants that you're
trying to discover he also modifying the
abstract domain itself yeah because the
original example you presented with like
the missing you know the joint of zero
yeah it is really so I think most people
understand it the first way you
explained it but if I was Patrick who so
I would say that you could then use that
information you can refashion the
abstract domain and build a new abstract
domain because it's implicit because
underneath it you have basically some
sort of disjunctive normal form you have
a lattice of formulas in disjunctive
normal form and in each refinement step
you throw a new primitive atom in there
you re complete or rebuild the lattice
of disjunctive normal form formulas you
don't actually build the lattice and the
implementation all this is done of
course incrementally and I mean real
analyses now they'd never build the
lattices it's all done incrementally
when we look at modular analysis i'm
going to say you build up the
information in the modules incrementally
because that's how it's done in practice
to scale up words otherwise you don't
survive but yeah that's a good
observation there are those two
philosophical view points and then
Patrick would say well it's the same
thing but the first the first way is
probably the way to think about it I
know that's the way tomball would
explain a killer yeah that's great thank
you anything else before we continue on
okay so I've shown you now they're
really critical parts so even if this
starts getting a little bit a little bit
boring or mind-numbing it's okay we will
come back to these same examples and
we're going to try to hit on some
important points towards the end but now
I feel at least an obligation given the
nature of this meeting to point out that
even though I've been looking at the
generation of traces because all along
underneath this was the idea dear in the
hearts of model checking people that you
want to generate execution traces or
test races abstract interpretation is
used to build a summary this graph
covers these kinds of test cases there
are other ways of doing it too but this
is where we are so far let's say that I
have a domain another baby domain where
I'm keeping track of the parody of an
input integer this is the Collatz
algorithm and I'm wondering about the
pattern where the end of input integer X
is maybe flashing back between even and
odd how does that happen maybe I got
some temporal operators and I want to
check the alternation between even and
odd good luck but in any case I can
generate this abstract model because I
start unfolding and then I get a
repetition right here I get a repetition
back to there so I draw a backwards arc
and so this abstract model lies at the
core of what most people call abstract
or model checking you know the software
is too big to really just grind it out
and check these things so people who
really want to do validation using malo
checkers on software have to build an
abstract model like this and then they
check of course the patterns of flow in
that you know the bounded model checking
people say no will just run until
storage filled zup and that's cool too
but in abstract software model checking
based on abstract models we're doing
what I've been showing you so far but
dataflow analysis is the origin of all
this because if you don't care about the
fact that a program point like p0 starts
out with an even number and are 18 steps
later repairs with some number that
could be even or odd if you just want to
know what could arrive at program point
p0 then your abstract model is the
program flow chart a control flow graph
and that's the basis of dataflow
analysis when Patrick kuso was a
graduate student he starting these
papers like killed all and cam and home
and realized that those flow charts and
all of that that equation stuff was
really describing this homomorphism
property and he wanted to tease that out
and show there's many more variations
than just ordinary first order equation
all data flow analysis but this was a
starting point and this is a very simple
example of an abstract interpretation
where we just write a set of first order
equations and we fold at the program
points and of course all of that
underlies the calculation with least
fixed points which is the third
important principle of abstract
interpretation that once you have an
abstract domain and you know how to
abstract the
tation functions to work on that
abstract domain then you want to do the
computation and use fixed point
techniques to get a convergence and you
want to finite convergence because
static analysis means you really really
want useful information in finite time
prior to execution so it's not run time
monitoring there is an issue of
termination if you use the interval
domain your maybe you're checking the
range of indexes of a variable that's
indexing an array you could have some
trouble and here I'm using an interval
domain trying to keep the range of
values of variable I in check so that I
may be don't abuse array a in the
interval domain well this one terminates
thanks to that nice test right there the
interval domain would calculate inside
the body of the loop and these program
points that eyes ranging like this and
on the exit we go to there but it's easy
it's easy to think of examples where you
don't have the assistance of the test
and oh no when you try to enumerate the
range of values you don't terminate and
so the CUSOs pointed out from the
beginning when you set up that abstract
domain and it might have infinite
cardinality infinite cardinality
vertically not just horizontally you
need a heuristic to force a leap to a
fixed point to a stopping point we call
that whitening and I'm just mentioning
that we cannot escape the dangers of non
termination in these analyses it depends
on how you fashion that abstract domain
so when you calculate at least fixed
point you can do it in a finite number
of repetitive steps and and so of course
there's there's the the religion of
whitening and there's all sorts of stuff
here I have to skip past because time
runs faster then my mouth does so let's
look at these definitions now we can
structure this stuff we've done state
transitions let's say your programs
define you an instruction way maybe used
a grammar and maybe you wrote straight
to equations to define how it is that an
input storage vector is transformed to
an output storage vector in that
structured way and maybe you've
reformatted that looks baby you've reef
I'm sorry I'll get these buttons right
by the end of the hour maybe
maybe you've reformatted up so here's
the abstracted version of assignment
abstracted version of composition I'm
ignoring tests on the conditional the
loop generates a local fixed point I can
abstractly interpret that no problem and
what's cool is the abstract
interpretation can be annotated on the
syntax tree because its attribute
grammar like isn't it that's no surprise
so here I went ahead and just defined as
this example or reaching definitions
analysis or you get out of any compiler
text and I annotate the syntax tree
showing the abstract values going in and
the abstract values going out and in
effect on generating what you call whore
triples for all the phrases of the
program and you'll generate the loop
invariant in this way or a loop
invariant i should say when you get to
the local fixed point that would arise
right down in here because you end up
getting up a set of repetitions when you
try to get a convergent value val right
here for the wild loop and so the
important point is like we heard in the
very first talk on tuesday if you have
your computation and definition
mechanism set up in a well form coherent
concrete way we're just doing a
homomorphic translation from the
concrete world into this world of
properties and we use the same
underlying mechanism for semantics and
that's why it's an abstract
interpretation as opposed to the
original concrete one and so now that
you've seen that then you say well oh of
course that means if I've written my
relational definition I have multiple
free variables i can draw proof trees
and there's one right there i inverted
the proof tree into a goal tree for a
big step semantics and I I read it from
left to right filling in values from
left to right and if I can do that in
the in the concrete interpretation I can
do that in the abstract interpretation
and if I have an abstract domain and I
know how to bound the range of values
that can arise I get of course a regular
tree that is I get repetition and a
backwards arc and then I get another
abstract model i can do property
extraction from and so it means that you
don't have to fear that abstract
interpretation is going to complicate
your view of the semantic world it's
it's just using
homomorphism on what you have to a
simpler collection of domain values and
and so in practice this has worked
really well with interprocedural
analysis I'll have to skip past it and
in practice it has been used as i said
in abstract testing we also have that
when when you are doing this kind of
analysis you can do lots of low-level
checking I know there have been analyses
for object-oriented programs trying to
keep track of the class values
associated with constructed objects so
you can do pre execution safety checking
of casts to see whether or not they're
reasonable this works exactly as you
would expect and as I mentioned earlier
there's been a lot of effort working
with device drivers real-time software
monitoring bounds of integer variables
to ensure absence of overflow underflow
and array indexing errors the Java
Virtual Machine of course has a flow
analyzer in it which Patrick kuso would
say is just a simple simple instance of
abstract interpretation which checks for
dead code does constant propagation
analysis you can do a lot you can
formalize a lot of the standard
checkings in this way because it says
general framework where you just plug in
your choice of the properties and then
use the homomorphism and the fixed point
computation yeah compilers of course
they're based on data flow analysis you
do program transformation working with
the flow graph tracking what values go
through the program points to see if you
can change the code yeah all these
applications are justified or formally
explained the soundness of these
applications are validated using the
explanations from abstract
interpretation so in many ways static
analysis starts from this foundation
even though there are many variants on
how its implemented with the with the 15
minutes I have left I'm going to
emphasize now more and more the
synthesis and validation of propositions
logical properties because that will be
of interest to someone who's defining a
language defining a semantics and
wanting to understand if programs are
written in a certain
I can be understood through the
semantics as having these certain
logical properties and so starting from
this there's a baby program up in the
upper left corner and as Peter pointed
out about a half hour ago my original
proposal for the sign domain was pretty
weak because it didn't really have
enough properties inside it so I added a
couple more the avi there a couple of
obvious ones that are missing there's
actually another one we could throw in
maybe you can think about what that is
and I decided that I am very very
interested in whether or not if this
program must terminate with a
non-negative number what's required at
the entry to ensure that i want to
calculate a precondition a sound
precondition so i want to run the
program backwards and all this really
hairy heavy technical detail is saying
that these functions here can be
abstracted in a forward sense and then
you can define the inverse image
functions and so lots of symbols you
know we can delight in these symbols but
the intuition is I'm just going to use
the inverse image function on these
abstracted versions of this stuff now if
I'm gonna use inverse image functions
I'm going to actually start working with
sets of properties not just pause or non
neg I'll be able to work with things
like non neg or non pause things like
that that's going to become important in
a bit but anyway pushing this backwards
using all this technical stuff which can
be of course automated we can understand
that the entry point is if the value is
not negative and the downward arrow
means not negative says oh yeah also if
the if the entry integer is zero or if
it's positive if it's nothing any of
these will ensure that when you execute
to the end you'll get a value that's
falls under the range of being not
negative as a lot of compiler people
know who do code improvement sometimes
you can take an entry precondition you
can run it forwards and get sharper
information within the internal program
points and that's what happens here and
so the idea of iterating forwards and
backwards is used in price
this in code improvement modules and it
can be used here to show with this baby
program that we learn by taking this as
our entry condition that when we
calculate the exit condition we saw find
at this internal point something new
that X must be zero at Point P 1 because
now I'm running forwards I'm bringing
this up because I want you to start
thinking about these paws neg zero
tokens as propositions in a
propositional logic because when we make
a complete lattice there is a
propositional logic hiding in there of
course you know about boolean you know
boolean lattices and hiding lattices and
so on every abstract domain defines its
own internal propositional logic which
gives you the expressivity of the
properties you can state and validate
with this machinery and so that's why
the selection of the abstract domain is
so important it gives you the range of
expressivity what can you talk about so
with a few minutes I have left let's go
forward into that idea going back to the
original picture that started this
presentation here's our our little sign
domain somewhat impoverished but
nonetheless we do have neg and pause we
have non in any which you could actually
think of as false and true respectively
and if we start looking at this as if it
were propositional logic we know that
the gamma map is defining a satisfaction
relationship semantic satisfaction means
a set of concrete numbers satisfies or
has or makes true proposition little a
one of those propositions if of course
it's contained in the concretization the
image of gamma over a and then syntactic
entailment is of course just the
ordering here so the important point
about that is it opens the door to ask
us what else is in that logic over here
in the sign domain soundness of course
is again postcondition transformer I
mentioned all this earlier that gets
that reappears but here's the cool part
should preserve the meat operation in
the abstract domain has intersection in
the model domain the power set domain
which means that meat is logical
conjunction and thank goodness for that
because it means that and works
correctly in the world of abstract
interpretation and so indeed just the
obvious point is that the delay
conjunction is defined of course in
propositional logic is saying exactly
the gamma preserves meet in the abstract
world and that must be the case when you
have a Galois connection so that's
another reason why we want this guy
walkin ection machinery sometimes people
complain it's a little arbitrary no it's
not really because if you want to think
about this is doing something logical
you need that now what about or is it
the case the gamma will preserve join as
Union therefore giving disjunction
sometimes it will but it doesn't in my
example my little sign domain is so
impoverished that if i take this join
that goes to any and that concrete
eise's to way too large of a set and so
this is not unknown to define a
collection of properties and find the
gamma does not preserve join as union
which means you cannot prove or
propositions with your abstract
interpretation and oh that's maddening
certainly maddening to the people who
work with blast and slam so the trick at
least the mathematical trick you don't
implement it this way unless you're
forced into the corner is to complete
the original domain with what you call a
disjunctive completion or down set
completion or an ideal completion you
lift two sets and each one of those sets
in my enrich domain are down closed
because of the monotonicity properties
but you can read those comments as oars
and you can see in effect I have forced
the or operation into the domain so this
is disjunctive completion and this is
implicitly what happens in
side blast and slam they're working with
an and or disjunctive normal form logic
based on the original primitive
propositions which in my case would negs
ero pause any none the Galois connection
lifts in a very simple way gamma lifts
into something called gamma bar very
trivial alpha listen to something called
alpha borrow because this is an /
approximating Galois connection and the
cool part is we have a little baby and
or logic and so that's good but we got
more we can go further if you're a logic
programming person you're doing what's
called groundless analysis you actually
need intuitionistic implication in your
abstract domain and so what you learned
when you studied intuitionistic logic
about hiding lattices you use it here if
you're doing analysis of prologue
programs you need that stuff and if
you're muli sagiv and you built the TV
la analyzer you in fact have full
propositional intuitionistic logic in
the abstract domains that you use so I'm
just mentioning that you can go back to
that later the key idea is that gamma
tells you everything the gamma map tells
you which connectives which operators
belong to your logic and what's
interesting is sometimes as operators
are surprising so with my baby sign
domain it had just my original signed of
main had just this baby stuff before I
did disjunctive completion I know I had
a hand and I knew I had these guys here
neg zero possini and my logic this is
cool the negate sharp operation is a
logical connective because it is a gamma
gamma acts as a homomorphism gamma
completeness with respect to neg sharp
and that means you actually have a
logical operator or logical connective
that could be of use to you now this is
not the usual negation that you have an
abelian lattice this is negate sharp and
negate sharp as it says their flips
paused an egg one proposition gets
changed into another this proposition
gets changed as proposition gets changed
and you can start talking in the logic
of your abstract domain and this is so
important in practice you've got to
choose your abstract domain so you can't
all
about the properties that matter to you
so every abstract domain with its gamma
map has its own internal logic with
respect to the gamma and a standard
trick when you want or in there is to do
the disjunctive completion so if you
ever decide to go to work with building
these sorts of things please be thinking
about this but you've already heard
enough that you can go to a meeting like
the static analysis conference and you
can bluff now because you know you
you've seen all the keys stuff now wow
we can apply this to calculate post
conditions you can find that so far I've
been working with this kind of situation
if i turn the orderings upside down
instead of calculating post conditions
that over approximate or cover or are
closed set representations of properties
if I turn this thing upside down turn
the ordering upside down gamma bar
survives alpha changes a bit I get an
under approximating interpretation which
can be used for calculating
preconditions or topological open sets
or interiors and so there's this
beautiful duality and that is useful
because sometimes well if we're
calculating preconditions you have to do
all of this machinery with the under
approximations and you can define what
the model checking people call like you
know they call it there abstract model
checking logic which is usually not the
internal logic it has to be narrowed
down using the tricks I've just flashed
by but I do know the time's running down
and we got to get to the last part
otherwise I've not I've not earned my
lunch here today we've we've gone now
through the principles and I've
emphasized the fact that when you make
this collection of properties there's a
logic there and I've also pointed out
that people have implemented these
things and used them with massive pieces
of software and this is the last part
that I'm going to try to develop before
we're out of here it's a little bit a
little bit discouraging to find out that
some of the biggest successes is just to
take a big hunk of program and analyze
it all at once but this does work and
this machinery is highly automated if
you have one human who can look at the
results of the
sis and maybe make some internal
annotations it's surprising how much
progress can be made and so in
particular see programs say like the
Linux kernel have been analyzed by just
doing an abstract interpretation much
like you would say like type checking
works because you have a very simple one
pass analysis over all the code you in
nor the control flow structure you don't
care about how many times a loop repeats
you don't care which branch of the
conditional is taken you just assume all
the codes going to get executed you
Union together a bunch of constraints
and you solve them quickly and this is
work great for coming up with
approximate descriptions of pointer
points to analysis in C and then that
information is used to validate other
safety properties of C programs so
people do this stuff and it actually
works so you don't even have to worry
about modularity if you don't want to
Dawson Engler who is an operating
systems person from Stanford a few years
ago took some really huge hunks of
software you know linux kernel style
stuff and he tried to do model checking
on it and he tried to do this form of
simplistic abstract interpretation on it
and he said this one because with model
checking the model checker got lost in
all these details examining all these
different orders of control flow
execution and it soon hung up and so the
Dawson said well then I had to buy can
abstract model and I left out some
details I messed up he said this stuff
gave me complete coverage it didn't go
very deep it didn't tell me sometimes
really interesting weird counter
examples but he was able to validate
basic safety properties so people
actually do this what about modular
analysis I got a couple of minutes here
and then I think I've used up my time
the the idea which has been explored and
this is still under exploration this is
hard it's hard for all of us is I think
that each component needs to be analyzed
separately and a summary needs to be
built some kind of table some kind of
collection of inequalities something
which describes what happens inside that
component the key idea is that when you
analyze that component at link time you
don't want to re enter the code and
reanalyze the component you want to take
the summaries from the link components
combine them and then if you have to
analyze those a computer fixed point
over those do that but don't reenter and
reanalyze code that can be a challenge
because that means you want to solve as
many internal local loops or fixed
points in the components as possible so
that you gain some real speed up in
doing this idea of separate component
analysis this is a huge challenge to do
right I'm going to try to give you a few
suggestions about this I'm not going to
do terribly well but i'm going to try
you might be familiar with strictness
analysis or termination analysis I'm
going to use this one because I can
pretend that that a function in a
functional language is like a component
that links to other components other it
takes other functions as arguments and
so we have the simple typing system and
so of course in in strictness analysis
we might say that this value means that
there the argument or the value might
normalize and the other value here means
definitely it's not normalizing and so
for the base domain like integers or
bullion's or whatever they're just two
options two properties we might want to
track and so if we have a component
that's parameterised on a base value
here are the options of how its
termination behaviors would behave
there's only three of them and then you
go up one level higher in the ordering
there are actually only six different
functions thanks to monotonicity with
regards to how termination operates with
such a simple base domain it is not
unrealistic to just enumerate as the
summary of this component the entire
function graph and that's what's
happened here down at the bottom I've
taken this program here F which takes 24
to external inputs but they're both
first order and I've abstracted a def
sharp which is tracking termination
possibilities and I finally here are my
options and this is somewhat similar in
spirit to what predicate abstraction is
doing because this is trying to track a
property or an invariant in this case
the property is will not terminate so
you can imagine if you want to global
and variant checking across an entire
system you could set up domains with
this kind of hierarchy there are some
details to be filled in here here's an
example more challenging like what we
confront I might have a component which
is
parameterised on another component as
well as maybe a base value and it does
something like this and if I enumerate
the complete graph for this component
yeah it's kind of huge it's got 12
entries but that's kind of like what
model checkers do they just enumerate
the possibilities and exhaustively check
them this will actually get you some
mileage for medium-sized programs it
really will but if you're going to
programs that are hundreds of thousands
of lines you got to do something which
is a little more clever if you get it to
scale up and and so there's a notion of
frontier which is a partial summary
those people have study strictness
analysis know that you don't have to
carry around the whole graph for a
component there are only some entries in
the middle which are the critical ones
because they kind of predict how other
arguments that are a little bit smaller
or a little bit larger in value will
behave when supplied and so you can
imagine an incremental accumulation of a
summary which saves only values that are
distinctive or tell you more than other
values in the graph summary and that's
what a frontier is and and so if you go
away and study the paper by Chris clack
and Simon Peyton Jones they have a very
nice explanation of why it is you don't
have to save the whole graph and the
reason our whoops I'm sorry the reason
off the reason is that there are
properties of monotonicity that if you
have some entries that you've saved then
other values which are similar can be
predicted so here if I have an argument
this little smaller than one entry I've
saved I know that the answer from that
slightly smaller argument is going to be
covered by this value here and so I
might for that previous example squeeze
down the summary of 12 values down to
the three on the bottom of the slide and
those three are pretty useful from those
three which I've repeated over here from
this original example and there's my
hierarchy of domains I can make all
these other cool inferences and in
practice that's what happens when people
are building modular analyses static
analyses of huge systems what they'll do
is they'll do an increment
they will lay out a component and
frankly they'll put together some sort
of skeleton information and as that
component is inserted or link to other
components that generates more needs for
more summers and and so finally you can
start with symbolic evaluations and and
this is where the state of the art is
that you have a mixture of symbolic
evaluation generating these instances
are increments so it has a feeling of
partial evaluation where you build up
the modular information I don't have
time to talk about solving local fleet
and local fixed points but it has
something to do with approximation
jumping to limits and that's an idea
that if you can master it it's really
cool but in practice what happens is
that we can take techniques from
symbolic evaluation Miriam to this
frontier idea and get a lot of mileage
out of it what up there is a link to
this talk as well as the papers that
I've drawn from the most but the
literature is just huge and and so I
hope this gives you an idea of how this
foundation works how it's been applied
and how it can be relevant to analyzing
properties are expecting properties from
language definitions as all those
particular programs so yeah I think I'm
ready for questions here
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>