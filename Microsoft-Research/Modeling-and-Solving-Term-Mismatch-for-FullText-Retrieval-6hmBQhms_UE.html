<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Modeling and Solving Term Mismatch for Full-Text Retrieval | Coder Coacher - Coaching Coders</title><meta content="Modeling and Solving Term Mismatch for Full-Text Retrieval - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Modeling and Solving Term Mismatch for Full-Text Retrieval</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6hmBQhms_UE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
it is my greatest pleasure to welcome
seller back to Microsoft many of you
probably overlap with choler as he was a
former isrc intern with an in our group
working with Charlie but we all remember
the Louis actually from CMU yeah in the
LTI language technologies that you can
see em you are very famous institution
are in term in the HLT area and for
those of you who are active in the trek
community you probably already know him
know he's a very active contributor in
the track community contributing to the
lima toolkit and his earlier work spent
from the structured document retrieval
to the recent one most most most
recently on this term image problem but
in addition to that our little also is a
very from alt I so he's also way around
it in the alt community so he has done
work in pattern retrieval biomedical
document retrieval and today he's going
to tell us he says the PhD thesis work
so without further ado hello everybody
I'm really glad to be here again to see
my old friends here and to make new
friends and I'm especially glad to see a
reasonable turn out on the Monday
morning in this talk so first if I get
picking with with the talk I I want to
comment that I know there are people
joining through the video link and
there's no way for you to participate in
the question answering session so I
let's hit my email up here you can send
your questions through my email and i'll
try to get through them them at the end
of the talk okay
so this talk is about is about text
retrieval and let's first see what is
text retrieval the task case the user I
confused the user generates a query
which contains certain query terms to a
retrieval engine refuel engine which
returns a set of results from a document
collection and feedback to the user so
hopefully the user will be happy with
the results this task and the retrieval
engine is usually evaluated using a
cranfield style evaluation where this
evaluation abstracts away the user by
retaining only the query that the user
issues and retaining only the results
that users are happy about which are
called relevance judgments and these
relevant documents help us evaluate
together with the queries help us
evaluate the retrieval engine in a
relatively objective way an automatic
way so here comes the big question where
are we and where are we going the
current retrieval models dates back to
the early 1970s the best ones are from
the still from the 1990s these current
models are based on simple collection
statistics like tf-idf TFA's term
frequency which measures how frequently
are term occurs in the result document
an IDF basically measures how rare a
term is in the collection so what these
retrieval models doesn't do is any kind
of deep understanding of natural
language texts so given this current
status what's the ideal lent like what's
perfect which you like given a query
information retrieval given an answer
text text search a perfect tree fuel
model should be able to judge that text
search implies information retrieval and
that is called a textual entailment task
inferring whether one sentence in first
another sentence and is known as a
difficult natural language processing
task
so and also searches are frustrated
frequently when they are doing
informational searches so we're still
fairly far away from the perfect land
but what problems have been holding us
back all those years and this work
argues that perhaps the following two
central and long-standing problems in
retrieval are the culprit one is called
the term mismatch problem where curry
terms fail to appear in relevant
documents it happens sometimes because
people describe the same thing using
different vocabulary and this general
vocabulary mismatch problem is studied
in the 1980s when sue was still working
form for bear laps so that's how long
standing this problem is however there
is still no clear definition of term
mismatch in retrieval the second problem
is query dependent term importance if
you are familiar with retrieval models
it's the probability of a term T
occurring given the relevant set of
documents so the probability or term T
occurring in a set of relevant document
of documents relevant to the query
traditionally term weighting is assessed
using IDF basically how real term is in
the collection because it has nothing to
do with the query it has nothing to do
with relevance this probability PTR is
known to be important for retrieval is
known to improve retrieval a lot it
appeared very early on and have been
studied by scarcely by my skills
research and these research provided
very few clues about how to accurately
estimate this probability in a very
dependent way this work connects these
two problems shows that they can result
in huge gains and uses a predictive
approach to try to solve both problems
and in this talk i will use turn
mismatch problem as a thread because
it's a more general version of two
problems as I wish you first what his
term mismatch in why do we care in job
search you might be looking for
information retrieval jobs on the market
a job posts could as well say text
search this could easily cost you
potentially fifty percent or even more
of the job opportunities even if you are
careful when you're formulating your
query in legal discovery you may be
looking for bribery or foul play in
corporate documents but they'll never
see that at most they will say Greece
I'll pay off and this cost two cases in
patent publication search it caught it
could cost businesses in medical record
retrieval successfully finding the
record regardless of vocabulary mismatch
could potentially save lives so in areas
where the user cares most mismatch can
hit most people know that mismatch is an
important problem and it they have tried
to solve mismatch from different angles
from the documents hand from the queries
end of on both ends our approach is
different so suppose you are given a
problem of any question any problem how
would you proceed to solve that problem
like what's your first step the first
step is always to clearly clearly define
the problem what the problem is and then
to see if it is a real problem so in
this case I will show you that in theory
and in practice this is a real problem
and then we should try to understand it
the underlying mechanism of how this
problem is created and try to solve the
problem using principled ways so I
promised to show you these in this talk
and I will come back to this slide
throughout the talk so let you know I've
fulfilled my promise so first definition
we define the probability of term to
mismatch relevant documents to be the
probability of a term not occurring in
the relevant set of documents
relevant to the query so suppose we have
collection of documents here the larger
bubble represent the documents that
contain key smaller bubble represent the
documents that are relevant to your
query mismatch is the proportion of the
relevant documents that do not contain
the term T so if you use the word
retrieval as your search term then these
will be the jobs that you miss matched
in all the relevant jobs on the market
so first I want to comment that the term
mismatch probability is related to the
term recall probability which is which
is just the complement of mismatch so
recall is the complement of mismatch
secondly this probability can be very
directly calculated if you know the
relevant set of documents for query so
if you have relevance judgments from
queries you can accurately estimate this
probability basically how many documents
contain how many relevant documents
doesn't contain the document e divided
by the size of the relevant set
now let's look at some examples these
are some queries and query terms and
these are the term recall probabilities
for these pirate terms in the query so
for the word spews in oil spills it
occurs in ninety-nine percent of the
relevant documents so it has a very high
rico lo mismatch why is that the case
perhaps there are no other ways to
describe oil spill yes question you know
which documents are relevant you mainly
live yes the user menu many labels the
documents and then we collect those
thanks for the clarification so perhaps
there are no other ways to describe
spills except oil spills because oil
leaks means something else term
limitations term the world term appears
in ninety-eight percent of the relevant
documents perhaps they also know other
ways to describe the world term here
however the same word term in long-term
care appears only in sixty-eight percent
of the relevant documents why is that
the case because long-term care can be
described as elderly care or home care
etc so it's not really a necessary term
for relevance the word effect appears
it's an abstract term and it only
appears in twenty-eight percent of the
relevant documents because effects could
be described as improvements decrease
impact etc ailments is not only abstract
but also a rare term and that makes the
situation much worse so these queries
are from check data sets where the
government provides users to generate
the queries and do the assessments to
generate relevant judgments and we as
participants can freely participate and
evaluate our systems so this is a very
nice deal so by now there should be a
lightning strike in your head we have a
very simple definition which allows us
to estimate the probability of mismatch
from relevant documents and to analyze
mismatch
this probability the probability term
occurring in a relevant set occurs in
one out very early retrieval models if
you assume that thermal currencies are
binary and conditional of each other
given the relevant information then the
optimal the patient optimal ranking
skull is given by this formula if you
are familiar with machine learning this
is just a naive base model the review
model the retrieval model aggregates
relevant scores for the terms that have
appeared both in the query and in the
document and we are scoring whether
document D is relevant and in this case
two probabilities to conditional
probabilities determine two sets of
probabilities determine this retrieval
score and two probabilities determine
the optimal term wait here one is the
probability of a term occurring in the
null relevant set because the relevant
set is usually very small compared to
the collection this probability can be
accurately approximated by the
probability of a term occurring in the
collection and results in the
traditional inverse document frequency
based term weighting so term weighting
based on how rare term is if it is real
term it is more important the other part
we now know if this is determined by the
term recall probability now this is a
very basic model however more advanced
models use this as the so part is the
only part that specifies term weighting
how important attorneys and more
advanced model other advanced models
behave very similarly and these models
these model and models have been used as
very effective features in web search
and to recognize that it is important to
recognize that this probability the term
recall probability appears as the only
part in the query that is
relevance this part has nothing to do
with it with relevance because it's a
collection statistic it has nothing to
do with the query this full formula has
been called relevance weight or term
relevance but term recall is the real
part about relevance so in theory it's
as important as IDF and the only part in
a retrieval formula that is about
relevance in practice because people
know this probability is difficult to
estimate you need to you need you need
information from the full set of
relevant documents in order to estimate
this probability so people typically
just ignore this part and only using
idea based time waiting and when people
do that it causes the emphasis problem
where the retrieval model try to
emphasize will try to emphasize the high
IDF terms in the query so for example
for this query prognosis or viability of
a political third party in the u.s.
prognosis and viability at the high IDF
they're rare terms so they are being
emphasized if we look if we look at the
term recall probability political
third-party should be emphasized instead
when the retrieval models assigning a
wrong emphasis to the query terms there
could be top four top false positives
where that this is a rank result given
using advanced retrieval model language
model and the all these top ten results
are false positives meaning they're
irrelevant documents that happen to
contain the real terms prognosis and
viability but they are not about third
pattern on top 'ok and it's important to
recognize that this is an emphasis
problem instead of the precision problem
because if you just look at the top
results right you see that program is
and viability are being used about
something else instead of political
third-party so you might think that if
you require
prognosis and viability to be about
political third party it might improve
the situation but in fact increasing the
match strictness of the match can only
improve can only increase mismatch and
can make the situation worse so this is
a mismatch problem not a precision
problem recall not precision and even
Google and Bing still have top ten false
positives I should note that Bing is
behaving much better two years now then
two years ago when first I first tried
this carrion be and Google actually
decreased performance a little bit on
this query I don't know why so false
positives throughout the rank list
increase the authority waiting precision
at all recall levels but I've shown you
that this is an important problem but
how frequent does it occur how
significant is the problem turns out in
the 2003 real reliable information
access workshop it gathered many groups
of experts research I assistance top
research I our systems evaluated
language models p.m. 25 sidwell feedback
all the very standard techniques that
have been still being used now and then
a failure analysis and discovered that
out of the 44 filled the topics
sixty-four percent of them so we are
summarizing the results here they did
not summarize the results that way out
of these failures sixty-four percent
because of emphasis and we now know that
term recall base term weighting can help
solve this problem another twenty seven
percent is the mismatch problem where
you need kind of a query expansion to
solve the problem and we now know that
if we know which terms tend to mismatch
we can guide our expansion toward
solving these problems terms so
underlying more than ninety percent of
the failures is our need to predict this
term mismatch probability
so in practice it explains common
failures or between models not only that
but also many other behaviors of the
retrieval techniques such as when you
are combining by grams with yuna grams
in your query the by grams tend to have
a much lower weight than yuna grams why
you doubt the case because the by grams
increase mismatch right so the city
should have a much lower recall
probability and much lower weight than
uniforms and personalization what sense
disambiguation and structural retrieval
which in for structural mission of
structural matching between Korea and
the Randall documents these techniques
are increased precision and are shown to
be less stable for improving retrieval
like that a case perhaps the problems
that the queries are suffering from are
the mismatch problems not the precision
problem ok I've shown you that it's a
frequent problem now let's focus on the
emphasis problem it's a frequent problem
but what about retrieval performance
game right what kind of gain are we
talking about four basic models if you
apply the true term recoverability into
the retrieval models it can get a
hundred percent gain in retrieval in
more advanced models it can still get 32
eighty percent gain so for a new query
without relevance judgments we need to
predict that probability but that for
addiction perhaps doesn't need to be
very accurate to show a performance game
because there is a huge potential so
huge potential game now on to prediction
how do we predict this probability this
mysterious probability that people find
no clue to predict very few clue we look
at the data i first hit varies from 0 to
1 so we need prediction second the same
term in different queries can have
different term recall probabilities so
we need a query dependent way to predict
probability third it's different from
idea what have so these three trends
also occurs in more larger scale
analysis here I'm listing the term
record probabilities each point is a
term the term recoverability other term
sorted in descending order as you can
see here the term recall probability
varies from 0 to 1 almost uniformly this
is from one track data set which
includes 50 queries so these are fairly
long queries for fairly so reasonably
sort of shorter Paris the term recall
probability there is a bias toward
higher recall and you should be
surprised to see this statistic because
on average a query term mismatch is
thirty percent of the relevant documents
and that's a high return from a short
query so imagine you are as a user
you're typing into your search engine
one query term you are excluding thirty
percent of the relevant documents and as
you type the next term you are excluding
another thirty percent from the
remaining right so that gives you less
than fifty percent of the relevant set
to even begin with
you're not dreaming
good point I'm doing standing yeah curl
that STEMI and globally the plotted here
are the term recall probability the mean
and the variance of the term
recoverability for the same term that
are cursing more than one query as you
can see it still varies from 0 to 1 and
the spikes are the variance so for a lot
of the terms there's there is a variance
but the variance is small but it could
be large for certain query terms I'm
plotting here the points are the Prairie
terms and X is the DF IDF why is the
term we call probability as you can see
there is a slight correlation but it's
still messy so you cannot directly
predict this using ideas but what have
prior approach which is done by
approaches have tuned this as a constant
or use IDF as the only feature to
predict and success is limited over only
very basic models and the missing piece
is not is the knowledge that this
probability measures term recall and is
related to term mismatch with this
knowledge we can ask ourselves what
causes mismatch what my clothes mismatch
first a term being not sent to all atop
a concept being not central to the topic
the words related or potentially related
are not really central to this topic
propounded not really essential so these
terms tend to mismatch second synonyms
tend to occur in place of the original
carry turn causing mismatch abstract
terms tend to be replaced by more
specific terms in relevant documents and
cause mismatch and given these factors
we can try to design features to model
these factors and to do prediction so
I've shown you the mechanism of how
mismatch
curse and how mismatch causes problem in
a retrieval in terms of features what we
need to do we need to identify synonyms
of a character in a query dependent way
we've made specific choices in our
design of the features to be general to
only depend on the query and the
collection not to use the external
features external resources but
certainly it's not the best it's not the
best way there should should be better
ways to design these features but this
is the first set of features that have
agree that have been shown to work for
this problem and let's see how we do
that so first for external resources
like we were met wikipedia paralogs they
have a coverage problem and they tend to
be more static not quite dependent so
they are not easy to use using them
research topics themselves what we did
is use a term from similarity measure in
the concept space to help us identify
the synonyms so it's called a local
latent semantic indexing because we
given the curry with the initial
retrieval get the top set of documents
from that retrieval and apply latent
semantic indexing on that top documents
so for example we can get 200 documents
the dimension reduction keep only 150
dimensions and here are some examples
this is the query term these are the top
similar terms identified by latent
semantic indexing so we use as a feature
the self similarity so we use in the
product as a similarity measure we use
the self similarity of the term as one
feature that measures how well the term
correlates with the latent concept space
and the latent concept space is related
to the query so how well this term
relates to the query we also use as
features the average of the supporting
terms average the similarity of the
supporting terms as a measure so
basically we're also requiring
not only the term correlates with the
topic but also the supporting terms
basically the concept is essential to
the topic and also we use as we design a
feature which measures how likely these
synonyms replace the term in collection
documents right if the scene if the
synonym is are frequent terms and then
it's very likely that these terms will
appear in documents that the original
character doesn't appear you so these
are the features and we can measure yeah
yeah I understand number two were
supporters supporting terms are the top
similar terms measured by latent
semantic indexing so basically we we
represent each term in the concept space
and we compute in the product
similarities and rank yeah rank of the
terms and these are top similar terms
yeah we pick up top five and then three
years so synonyms are also the top this
is bryn terms yes synonyms are the top
supporting terms and we're measuring how
likely these terms occur in collection
documents that this term doesn't
occurring as a measure of how likely
these terms replaced the original
character in collection documents
the entire collection yes it's not just
the top dolphins we can measure how well
they correlate with a target Tamra Cole
turns out term sensuality has a very
fairly high correlation because IDF has
a point 13 correlation right negative
means also means helpful positive or
negative as long as this absolute value
is large is helpful sensuality concept
centrality also very helpful replace
ability well understandably it's
negative correlation and it's fairly
high abstractness the abstract nice
feature is based on the observation that
users tend to modify more abstract terms
using more concrete terms for example
educational is we modify the world
program in this query the program tends
to read the more abstract term and so on
effects also tend to be an abstract term
so we can use a dependency passer to
pass the query and if a prior term is
what be modified by other query terms
and we say this is more abstract this is
a binary feature and it has a
correlation of 0.12 with the target so
this is also very helpful mr. retreat is
you just say yeah using yes yes using
the baseline which we will model what
have a baseline I'm comparing to
I was careful Stephan Pastis not
designed for me
no no so eight races
I haven't verified but for the for the
small set of queries that I looked at
the past looks very accurate so the
passes also behave fairly accurately on
short texts so given these features we
can model the prediction of term recall
as a standard regression problem we can
use training data with known relevance
training queries with known relevance to
train the model and use another set of
queries without relevancy information
and the test set and here we use
Gaussian kernel support vector
regression is the prediction model you
can also use other advanced prediction
models like boosted posted a decision
tree or boosted regression tree it works
similarly any experiments we are
measuring two things one how accurately
we are predicting term recall using how
closely the prediction is from the truth
second we are measuring retrieval
performance using overall ritual success
and precision at top ranks so what
percentage of the top ten are relevant
so here is one example for this query we
are getting the correct emphasis here
although the absolute value is still
very is still not very close and more
globally this is the method that uses
the training set average to predict for
the test set it gets an error of 0.3 so
suppose the dispersion is uniform from 0
to 1 the term recall is uniform 0th one
if we use the training set average it
should get a error of 0.333 so it's not
completely uniform using IDF alone
increases error a little bit using our
features and tuning the meta prior
metals our features we can reduce error
by half and this shows if we use the
recurring terms that terms that occurred
in more than one query in the training
set to predict so use the previous
occurrence to predict the next
occurrence we can get a fairly low
prediction error but these two values
are not directly comparable because they
are not measured on the same set right
this is measured on test set is a
measured on the FINA set so it can be
predicted and I want to just briefly
insert that our method demands a more
general view of the retrieval modeling
problem traditionally which will
modeling is seen as restricted Lee as a
document classifier for a given curry to
classify whether a document is relevant
to the query or not the more general
view see is a retrieval model as a
matter classifier which is responsible
for many queries it takes in a query as
input and outputs a document classifier
so given this view learning a retrieval
model basically is just transfer
learning in machine learning right
you're using we're basically using the
knowledge from related tasks training
queries to classify a document learn a
new classifier for the test query our
features and model are just facilitating
the transfer from this more general view
what perhaps lead to more principled the
investigations of the problem of how to
learn visual models and also allow us to
apply more advanced transfer learning
techniques into retrieval ok that's the
insert we're measuring retrieval
performance now if you are familiar with
the retrieval models this is how we
insert the probability into the
retrieval models as the term waiting at
home waiting in language model if you
are not familiar with retrieval models
it's okay we're just waiting the query
talks we're not doing any kind of
expansion this is the performance on six
different test sets each test set
from check contains 50 queries and we're
using one as a training set and one as a
test set in a cross board there is a ten
to twenty five percent gain most of them
are significant so this is measuring
overall retrieval performance gain and
in top precision as predicted by theory
there is also a ten to twenty percent
gain although not always significant
because the measurements has passed so
it can be used in retrieval as term
waiting to help solve the emphasis
problem and leads to significant gains
what about the mismatch problem right if
we can successfully solve the mismatch
problem increasing the term record
probability of every query term we can
at the same time solve the emphasis
problem so let's recap
sorry Tuesday original yes yes we need a
truce data retrieval one to generate the
features one to do the retrieval
yes yes general features and their
problem model to wait these deaths yes
to study always yes that's a good point
I do have but i will come back to that
after the talk this is a slide you're
showing on the transfer learning or key
go back to that sorry
so this more general view that this is
sort of like a learning try saying
something different so most retrieval
models now are usually learn to give in
sort of a training set does it raise it
are you seeing something different from
another good point so by now there's a
the dominant model is learning to wreck
basically learns a global retrieval
model out of the training set for just
one retrieval model so good basically
one classification model right and
applies the model the same model on the
test queries but here we're learning
from these training queries and generate
a new classifier so the classifier is
different while the learning to rank
learns a global model one classified
does that answer your question so the
fire in your case is the product of the
view
p of t you in arms or do you actually
have an underlying model as well in
addition to the underlying classifier is
just PM 25 or language models which is
just the traditional models with that
probability inserted into the model
improve probability yes but that's at
the same time with a different term
weighting that is a different model
right the data is a different
classification model it's the same
receiver model but there's a different
classifier because you're classifying
the documents differently with a
different time weights yeah yeah no
you're so that is it yeah I asked you my
brothers there it seems like it seems
more like different feature values yeah
thank you I can perhaps come to that
later so let's recap mismatch mismatch
ranges from thirty to fifty percent on
average relevance matching can degrade
very quickly for multi world queries the
solution one solution is to fix every
part turn right if we fix every claritin
by expanding a required term using its
synonyms it results in a conjunctive
normal form query so in this case this
keyword curry is being expanded into a
conjunctive normal form curry it's very
expressive and very compact as a 1-1
conjunctive normal form curry in this
case is equivalent to hundreds of
alternative queries using keyword
queries this is used very frequently by
lawyers and librarians in this case it's
a very legal track query which is
created by lawyers as you can see they
spend lots of time trying to expand
every creditor so it's a very tedious
task and what we propose to do is given
this term is match probability given the
prediction we can help guide for the
expansion to focus to let the user focus
on the terms that have the problems
right so here placement and children are
being expanded and keeping the other
term
untouched the goal is to expand let's
say two times and still getting ninety
percent of the improvement that would be
great so how do we evaluate that ideally
have a user we let the user propose a
key will carry keyword curry is sent to
the diagnosis system being diagnosed
which are the problem terms and the
problem terms are being fed back to the
user the user expands the problem term
and the query reformulation strategy
generates the query submit to the
referral engine and do evaluation so in
this case we can have different
diagnosis methods in the diagnosis
component and we can have different
query formulation strategies in the
query formulation component and we can
compare these different methods that's
what we intend to do however online user
studies of such a complex system need to
control many variables without millions
of users this kind of only study cannot
be carried out what we end up doing is a
simulation we have the expert user give
us a fully expanded CNF query beforehand
we extract the keyword query by taking
the first term out of the each conjunct
similarly the pipeline goes so this is
one simulation and the user expansion is
also one simulation where the expense
term is being extracted out of the
existing CNF part so this simulation is
fairly realistic it's use partial so you
to use full expansions to evaluate
partially expansions to simulate
partially expansions that extend
everything it's good to expand
everything but I'm just saying that
given our probability how are we using
that way we can use that to guide the
expansion to save some time for the user
the expansion happens automatically
great what is the use of you oh the
expansion doesn't happen automatically
so the CNF carry you are seeing is being
expanded by a lawyer spending lots of
time on one query you know the synonyms
right i mean we have our we have
dictionaries with synonyms and what i'd
i'd understand what was the manual part
the synonyms are difficult to get these
are the gold if you can get the synonyms
you get going to retrieval but we don't
we don't have automatic ways to get that
yeah perhaps the ping has that but even
Google and being have this problem so
that means if we can manually expand
more terms right we can do better
manually can really see it see here
Curry's you need
yeah yeah so I'll show the extent so
here we are using two sets two data sets
one with CNF queries created by lawyers
when we see queries created by such
experts in terms of ritual in terms of
diagnosis methods right I'm plotting
here on the x-axis the number of query
terms that are being selected for
expansion on the Y the retrieval
performance gain the relative
performance game so the upper bound is
expanding fully or expanding not so full
now it but close so a fully expanding
sort of giver upper bound you can
observe here these two points are using
the PTR based diagnosis so project PTR
and the diagnosis and M do it a
simulation expansion expansion
simulation and evaluate expanding to
terms using pto based diagnosis can get
about ninety percent of the performance
gain while if we are only using IDF as
diagnosis we need to expand three terms
so we are effectively saving users time
what do you mean the very best to try it
all all pairs to see oh no I haven't
done that but it's good point so we are
using a greedy strategy here greedy
strategy not necessarily the optimum
that's right i mean i know that unless a
misunderstood you so so great at least
like to the to Frank but but you also
you have like the four or five
expansions right so you could would you
be able to just you don't do be able to
try all to and see which one gives the
very highest resuming I'm spending which
character or do you mean how many Clara
times to expand for each character which
query terms to expand oh so we know we
haven't tried permutations we're only
using a greedy approach to expand the
old impossible company no we haven't
tried that just yeah yeah yes yes yes
yes that's an Oracle expand
so in terms of expansion forms of
expansion we're comparing CNF expansion
versus the traditional bag-of-words
expansion key with the expansion as you
can see C&amp;amp;F gets better then keyword CNF
gets better than keyword so we're using
the same set of menu high-quality
explained in terms from the users but
instead of doing CNF expansion we are
combining these set of expansion terms
as a group and combining that set of
credit terms with the original query
using a weighted combination and we have
also tried waiting the query expansion
terms but that doesn't help much so we
are we have tried several different ways
to do the traditional way of the
expansion you also sealed a
candy dimensions are from CNN
yes yes the candidate terms often seeing
that extension I have also tried using a
automatic so the standard relevance
rather relevance model so stand the
pseudo relevance feedback method to
automatically extract the expansion
terms but that's worse then menu
explained in terms
so we showed that CNF queries so the
proper diagnosis can produce simple and
effective CNF crows I've also worked on
other aspects of the problem such as
improving the efficiency of the
prediction so here we can use a one-pass
prediction one pass retrieval with three
to ten times speed up and close to cure
the visual performance while still
retaining seventy to ninety percent of
the performance that's based on dia on
the reservation that many of the pirate
terms for many of the quarter terms the
terminus match probability doesn't very
much across queries so we can use that
to speed up our prediction
not we are not doing around for you
back in this case God yeah so one stage
yes yes we need a catch but luckily the
training set is a lot
the probability is the problem
sorry I don't get your point you're
predicting from the previous scene where
is where the day seemed very good George
good rather than go with judge relevance
from the thing isn't so I also worked on
yeah yeah coverage could have been
something and yeah but luckily you don't
need much high much higher coverage it's
about fifty percent term coverage gets
about seventy to ninety percent gain if
you have larger finis sets definitely
better will have also worked on
structural retrieval using semantic role
based structure so we can annotate the
question which turns out the target
which terms are the agents and subjects
objects we can annotate the answer in
the same way and try to match the
structure not only get a key words right
at this but this problem causes that
this structure retrieval causes a
mismatch at the structure level where
because of a switch of the key term key
verb hear the arguments 0 here becomes
the argument to hear and it's a
different target so we have used that
undirected graphical model to learn a
jointly learn the field level
translation from the question to the
answer using the training set which are
basically true question answering pairs
we can learn the translation and we can
predict which are the likely structures
given a curry and use the prediction we
can use envy lima search engine to query
these alternative structures so injury
allows us to query this kind of
structure we can get twenty percent gain
in a wall retrieval versus using just a
strict question structure alone
okay conclusions this talk is about two
long-standing problems in retrieval to a
mismatch and term late estimation we
have provided the definition and initial
analysis of term mismatch future work
will they explore new features and new
prediction models that will improve our
prediction even further we have showed
the row of term miss match probability
in basic retrieval theory and use the
principled approaches to solve term
mismatch but what about more advanced
models like learning to rank a transfer
learning what L term mismatch play how
does it play out in these models models
we have used automatic ways to predict
her mismatch and done the initial
modeling of the qualities of mismatch of
the possible causes of mismatch and we
have provided an efficient way to
predict this probability future work
should explore better ways to model
these causes or other causes that
haven't been explored here in terms of
effectiveness in retrieval we have used
term weighting and diagnostic expansion
better techniques are needed right like
automatically expanding the query into a
CNF form better formulas like transfer
learning might facilitate facilitators
to extend this work into more tasks like
relevance feedback etc we have done
diagnostic intervention diagnostic
intervention can happen at different
levels of the retrieval process we are
applying the diagnosis at the term level
and only diagnosing the term is math
problem and we have shown that this
guided expansion can help retrieval
future research should explore the
diagnosis of specific types of mismatch
problems is it because of abstract knees
or synonyms it
help it could also explore different
problems not just mismatch but also
precision problems right so that we can
guide lots of different NLP techniques
personalization etc to solve the real
problem of the curry and to improve and
even further we can proactively diagnose
the user right we can see what problem
the user is having and suggest searches
or results even before the user types
into the search engine so that's my
thesis work I have also worked on lots
of other things at CMU building data
sets like clue level 9 which is being
used by more than 200 research groups
worldwide more than 7 track retrieval
evaluation tasks I've worked on the Lima
toolkit which is an open source I our
toolkit which can do lots of fancy
things I've worked on large scale
computing and I do a fairly popular dupe
tutorial at CMU in terms of as a
research I've worked on structural
retrieval I've worked on legal discovery
tasks patent retrieval in biomedical or
chemical domains and I will I have
worked on information retrieval for
human language technology applications
like question answering of tutoring or
knowledge base extraction from the web
and information extraction and with that
I end my talk and now I'd like to take
feedback questions
I just check if it is a lot more patient
mother's mental level
wow I thought
how their batter labor I'm just you see
so you are using propane this time that
would so
so don't liable specific paths I see ya
I should have
government pension
practice
like me walk
but the genes point is just is that
better represents emetic representation
of the text will help solve the mismatch
yeah I'll just check who else might have
questions during the talk right now
thank you thanks the speaker gay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>