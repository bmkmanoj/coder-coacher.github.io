<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Shared variables and data races in C++11 | Coder Coacher - Coaching Coders</title><meta content="Shared variables and data races in C++11 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Shared variables and data races in C++11</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/owmKOUWUfoo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it's my pleasure today to introduce
someone who looking around the room
really doesn't need an introduction Hans
Boehm has been one of the leaders and
the programming languages and compilers
and memory system community for as long
as I can remember and knows many of you
quite well we're fortunate to have him
here today giving a talk on some of his
more recent work with memories models
and C++ so thank you Jim okay um before
I go on here I should say that what I'm
describing he is mostly joint work with
lots of other people some of whose names
are up here in particular I'm going to
be advocating a in approach to memory
models which was really pioneered by
sarita odd way a long time ago and has
actually been ago and was in some less
formal less precise form actually
incorporated into other programming
languages even before that good I'm
going to be talking about peds and
shared variables so occasionally the
question comes up why that's the right
why that's actually the right subject to
talk about it does seem to be
controversial these days whether we
should be communicated whether we should
be writing parallel programs by actually
sharing memory or not occasionally so if
you live if you type the phrase threads
are evil if you look for the phrase
threads I evil in Google for example
last time I tied you get about 32,000
hits and that number is increasing
rapidly every time I tie it it's more on
the other hand that's likely a
convenient way initially to express
processing of multiple events claims or
to keep gooeys active while you're doing
some processing in the background and
they're also at the moment aside from
the HPC community the dominant way to
really take advantage of multiple calls
in a single machine so whether you like
them or not they seem to be pervasive
so what I want to talk about is first of
all sort of the basic C++ 11 and c11
memory model this is the approach to
threads and shared variables that was
incorporated into the the new standards
for both C and C++ those rely heavily on
a notion of atomic objects that somewhat
different from what programming
languages have traditionally had so I'll
spend a little bit more time talking
about those and then i'll spend actually
a lot of the talk talking about data
races in particular which play a call
role in the memory model and trying to
convince you that those are just a bad
idea and that you should never write
code that has data races in them I'll
also talk a little bit about about the
implementation consequences of this the
C++ and C love and memory model so the
state of the world before we stop in and
this is now quite a while ago this is a
gallon sort of 2004-2005 was that it
went for at least people programming in
C++ and C the the story was that they
were basically programming in a single
fitted programming language the
programming language specification said
absolutely not nothing about threads
though lots of people were actually
using feds they were using feds in the
form of an add-on goods library either
windows kids around here partly mostly
windows threads elsewhere sometimes
POSIX threads unfortunately if you
looked at those specifications that they
were either sort of intentionally or
unintentionally unclear about what even
some very basic multi-threaded programs
meant so if we look at this is about as
basic as a multi-threaded program that
you can think of as you can think of you
have one thread that assigns 12 x + 1
thread that assigns wonder why you would
really like to know that by the time
you're done both x and y have a value of
one if you look at the POSIX
specification that was actually
intentionally unclear and it was left
intentionally unclear motivated by some
hardware and compiler implement
considerations basically what people
were afraid of is that in assignment to
a variable here which was particular
which was small might in fact read and
rewrite over right adjacent memory which
in fact in some implementations it did
so it could in fact over right in
adjacent slack field and this was based
on my reading of POSIX it was also
allowed to to read and overnight in
adjacent unrelated variable so even if
these were not select feels so there was
in fact very little you could you could
say about what this program and and
there were much more interesting
examples which actually caused serious
problems occasionally not that
frequently on the other hand when they
did arise they were really diff it was
really difficult to figure out what was
going on probably the worst consequence
in my opinion of the situation was that
it was it's really very difficult to
teach people about multi-threaded
programming which is difficult enough
without being able to give them a
consistent story of how things actually
work so if this doesn't work all the
time then it's really difficult to teach
people how this does work at all and so
what we started with in in the for the C
and C++ memory model which is sort of
what everybody starts with and what
we'll start with here is the the model
that everybody thinks they want from a
programming language which is sequential
consistency so what usually when people
think about pets they think about kids
executing as though the actions of the
individual feds were just interleaved so
if we have read one doing this and
thread to doing that they it might be
executed as though the statements will
sort of shuffled up well not travel
interleaved in this way so that they so
that the actions performed by each
thread are performed in order and
whenever you look at a shared variable
and here I'll use X X and Z and Y to
denote shared variables and the things
starting with all which you can think of
as registers denoting local variables so
that when I look at a variable here the
Val
you that I see is the last value of the
last value the value that I see is the
last value assigned to it in this
interleaved execution in that
interleaving so that's what everybody
thinks of as sequential consistency
that's what what people usually
initially at least believe that should
behave like I in addition to just sort
of interleaved execution of feds we
sometimes need to control that as I
think everybody in this audience knows
so in reality there are many cases in
which we don't want to allow arbitrary
interleaving of fed actions we want to
control those for example if we if we
want to increment a variable which
really is going to be implemented under
the covers is loading the variable and
stalling back the result if we want to
do that concurrently in two threads
typically we want to prevent the
execution of those two threads from
being interleaved in the way that I've
shown here where I read acts in one
thread read X and the other thread
increment the temporary valid value in
both and then right back the same value
in both feds because that ends up
resulting in X getting only incremented
by by one instead of by two as I had I
had intended so the way we do that is we
introduce some sort of mutex or lock
mechanism that prevents this kind of
interleaving that forces that basically
prevents one of those threads from
executing while the other one is busy
accessing X so we make sure that we have
some notion of lock so that only one one
thread can hold a lock at a time we can
define that sort of very precisely if we
want to buy in this interleaving based
semantics by saying that we only allow
interleaving interleaving in which one
of these these lock actions you can can
be scheduled only while no other that is
holding that lock on mutex so with that
restriction then rather than allowing
leavings between these two threads we
only allow these two into these two
possible into leavings because we can't
schedule the second lock operation until
the first unlock completes so the only
acceptable schedules are those two and
that's all easily easy to make precise
however it has two problems actually
before I get this sorry i should say how
this is actually written in c++ 11 so in
C++ 11 we have we now have a mutex con
construct we can lock it and we can
unlock it when we're done alternatively
since this is c plus plus since this c++
you would like a more C++ like syntax
where you declare variable whose
constructor acquires the mutex here
whose constructor execute MDOT lock and
whose distractor execute MDOT unlock so
that if you actually if you leave this
block by an exception the lock still
gets released and the right things
happen for those of you who are used to
java synchronized blocks this sort of
gives you the same effect essentially or
something like that and so that gives us
a very nice model based on sequential
consistency but it actually has a couple
of problems the one that everybody I
think realises is that there's their
implementation issues with sequential
consistency it gets in the way of some
optimizations that we would really like
to be able to perform so here this is a
fairly sort of standard example that
people use in this context which is
based on decades mutual exclusion
algorithm and the idea here is that I
assign 1 to X and then I assign 1 to y
and then in each said I read the other
variable next if I view this execution
as being in purely interleaved it has to
be the case that either the assignment
to X or the assignment to Y goes first
one of those has to execute
us meaning the other fed whichever that
gets get scheduled second has to see a
value of one so if this one gets
scheduled first that lead of Y into our
one has to see a value of one for y
guarded than the initial value of zero
which I you I will assume i have here
everywhere so basically one of these
loads has to see the store and the other
said on the other hand what happens in
real life there are a number of
optimizations that counteract that and
that generally prevent this from being
too so what in particular both the
compiler and the compiler and the hard
way is likely to want to break this so
what compile list will typically observe
is that if if this appears in some
bigger context if I'm loading why I and
I use why further down I can give the
hardware more time for the load to
complete if I schedule this load of why
olya so the compiler is likely to want
to perform this load up above the store
2x and since the compiler it looks like
especially for a compiler that sort of
was initially a sequential compiler it
looks like these two statements I don't
interfere they're independent they don't
touch the same variables so it looks
like these can be perfectly safely
interchanged so as a result that
compiler transformation makes sense even
if your compiler doesn't interchange
these it turns out any sort of
reasonable modern hardware that you're
likely to run this on when you when you
assign 1 to X is really not going to put
one into the memory or even the cash at
that point it's going to put one into a
store buffer someplace scheduling it to
eventually make it to the cash and it's
not going to wait on the it's not going
to wait on that story reach the cash
because that would slow down the
hardware so as a result of that when you
put one when you assign 1 to X here that
assignment isn't visible to the other
third for a while because the store
buffer is only visible
Joey so that has the same effect that it
looks like these these two statements
are interchanged by the hardware which
again makes it possible that we end up
in this scenario where i read the
initial value the pre assignment value
of zero for both x and y because in fact
these two assignments can in the simple
case these two can both occur really
your code before the stalls that's
actually not the only reason as it turns
out that i want to do this it turns out
as a programming model a log u
sequential consistency isn't all it's
cracked up to be either so in particular
the really nasty property that
sequential consistency has is that it's
dependent on the access granularity to
memory so if i have a hypothetical
machine which is which can only access
memory a bite at a time for this example
and I stole three hundred 2x + 1 thread
and 102 X in the other thread each of
those assignments 2x in fact I going to
get decomposed into assignments to the
high byte and the low byte and they can
get interleaved in this fashion and I
can end up with the final value of x of
356 in this case which is not a variant
very intuitive of expected outcome and
not something that people really want to
program to maybe even more important if
I sort of move this up a level i think
really when anybody rights
multi-threaded code they already assume
that we in fact have more than
sequential consistency we don't really
reason about programming program
interleaving at the indus tate at the
level of individual assignments or store
instructions so for example if i have if
i have two arrays which are distinct at
a1 and a2 and I call the sort function
on a one and a two in two different
threads I'm not going to explicitly
reason about the difference in two
different interleaving of those two
salt operations because I know somehow
that those don't interfere that
independent it all doesn't matter so
what I really want to argue is that
these things don't interfere and
therefore I don't have to look inside
them I don't have to worry about what
actually happens and the the actual C++
and then see memory model relies sort of
leverage is that and this is the the so
called data arrays three model which
I'll describe your next so the real
memory model for both see an 11 and C++
11 relies heavily on this definition of
a data race which is more less standard
but I'll reproduce it here anyway I so
we say that to memory access is conflict
if basically they're automatic which
basically means that they have to access
the same memory location and they can't
both be a read that's a fairly standard
definition so in the preceding example
the assignment x equals 1 the store to
ax and the load load for max conflicted
we then say that to memory accesses
participate in a data race if they both
conflict and they can occur
simultaneously meaning they perform by
different threads and there's nothing to
prevent them from the there's nothing to
enforce an order between them program is
data arrays free and really we mean on a
particular input when we say that if no
sequentially consistent execution
results in the database so the notion of
sequential consistency here is still
significant so it's it's used in the
definition of the data of data is here
for example in hypothetical light
machine
is this is at the programming language
level none of the machine sequential
consistency right and if the problem is
it actually matters which level you're
talking about right so enforcing
sequential consistency at the
programming language level is actually a
hard thing to define because usually the
programming language doesn't actually
define how many accesses are involved in
a particular assignment at the machine
level it makes more sense but you don't
want to program at the machine level
memphis here in prison like you have
access the same scalar objects so that's
for that hey that's really where that
where that issue comes in right so the
the definition of the definition you the
way this is actually presented in the
stand-up is they have to access the same
memory location and the memory memory
location is defined as a scale of object
and then there's this footnote here for
c c and c++ or it contiguous sequence of
bit fields counts as a single memory
location so updating any any bit field
in a sequence is viewed as potentially
updating all affecting all of them I'll
have some more examples dealing with us
later you have before in the sunset XE
gets 300 on fight machine you would
consider the whole sequence as one
skater object I in this case a in that
case yeah the whole thing is once gala
object and the whole thing is a memory
location yeah but so far we haven't
we've only here we've only defined your
data arrays so let me go on for a little
bit I so then they guarantee what we
actually get out of the C and C++ memory
models is we get sequential consistency
again but we get it only for data arrays
free programs I for anything that
contains the data race all bets are off
normally what's referred to as
catch-fires semantics so which is the
the same sort of semantics we assigned
to an out of bounds subscribe to
generic bands of a assignment or
something like that anything can happen
and we'll see examples of that so it's
the programmers responsibility to
prevent data races and you get to do
that by either using locks or and I'll
talk about this in a few slides using
atomic operations which are which are
new to C++ 11 and c11 and I'm cheating a
little bit here and I'll talk a little
bit more about that later in C++ 11
they're actually ways to relax the
sequential consistency guarantee for
data arrays free programs in order to
get more performance whether or not that
make sense sort of depends on the
context so if we look back at decades
example from before what happens here is
that if x and y are just declared as say
integer variables then this program just
has a database because x equals 1
conflicts with r2 equals x and those two
cannot be executed simultaneously so
basically all bets are off you this
program in C++ 11 has undefined
semantics anything can happen one of the
nice the sort of core advantage of
programming in this database free model
is that we no longer have to really
worry about instruction interleaving I
already hinted at that before so by not
having to worry about instruction
interleaving we also allow a whole bunch
of optimizations that we're used to
having performed which now become legal
so to illustrate this problem illustrate
is basically the property that we're
promising here is that any region of
code which involves no synchronization
acts as though it's atomic we don't have
to worry about what happens in the
middle of any code region that has no
synchronization so long as we know the
program is database free in sort of the
rough hand waving argument here is that
consider some sun block of code that has
no synchronization say assigns 12 a and
12 B let's assume that I could somehow
notice I could somehow or observe a
state in the middle of this that would
demonstrate that this isn't executed
atomically I can only do that by having
an absorber head that looks at a and B
and determines its in the middle this
absorber fed unavoidably has to have a
data race with this with the third
assigning one day and one to be in fact
on both a and B so things look atomic
because any any program that could tell
the difference that could prove that is
really not atomic is outlawed so that's
sort of the quick introduction to the
memory model let me say something about
these atomic objects now which I've said
something which I've hid it at before so
the the basic problem we had with this
model before so in a less well-defined
state this was this is actually very
similar to what was we quiet by P
threads before and even ADA 83 back when
so we're not sort of changing the
approach here fundamentally in some
sense but if we look back at something
like the the psets model basically what
what went along there is that the model
outlawed data races on the other hand as
I think many people in this audience
know the problem was that la in order to
avoid data races you needed to use locks
basically a mutexes those were viewed as
heavyweight and expensive so what people
did is they cheated they wrote programs
that had data races in them anyway so in
if you look at real pthreads programs
and many people have demonstrated this
in various research papers real real
pthreads programs typically have to
typically contain data races so what we
really wanted to do is make sure is not
give people that excuse for writing
programs with data races and really
leave make it possible to write correct
code so the solution is that we
introduced this notion of atomic objects
these are objects that sort of
superficially behave like ordinary data
on the other hand they do allow
concurrent access so they you can access
these things concurrently without
introducing databases the actual
database definition excludes these
operations it only applies to ordinary
memory operations on atomic operations I
by default unless you tell us otherwise
these preserve the simple sequentially
consistent behavior they also act
indivisibly so they also address the
granularity issue but they do give you
sequentially consistent behavior these
actually turns out have a huge impact on
the memory model specification and if
you actually try to read this memory
model specification in the c++ standard
which i don't think i would recommend
you'll find that it's mostly dominated
by describing the behavior of these
atomic slowly and everything else sort
of falls out as an easy special case I'm
so I just by way of illustration these
are these are easy to use at least in
the simple case if I want to increment X
and I don't want to use all using mutex
in order to protect X I can do that by
just declaring X to be atomic so instead
of saying index I say atomic of index
this is C++ syntax and C syntax is a
little different and I'm actually by
doing this here it turns out i get an
overloaded version of the increment
operator that actually does an atomic
increment I guess what I don't hear
would be called enter line and it a lot
in common implicitly so that works if we
go back to decos example and we actually
wanted to work correctly by where I work
correctly I mean that if x and y
initially 0 then r1 and r2 can't both
see the value of 0 I can make it work
correctly by simply declaring X&amp;amp;Y to be
atomic in this world the requirement
here is the the catch here of course is
the bottom line
the compiler and hardware have to do
whatever it takes in order to actually
make that work correctly and make sure
that all one equals R 2 equals 0 can't
happen I I should say that this is not
I'm this actually fits in fairly well
with a whole bunch of other different
languages that have sort of vaguely
similar conflicts which I should mention
here because some of them are really
more similar than others so as I said
C++ 11 has atomic and it also has a ton
something has types like atomic under
school in for some special cases
basically for see compatibility c has
these atomic underscore tights which was
a which it was originally meant to have
and late in the standardization process
the sea committee introduced this
version which is really similar to that
but has different syntax just to keep
you on your toes Java has volatile or
java dot util dot concurrent atomic
which actually has really similar
semantics to this and on the other hand
their various other languages that have
conflicts that are profoundly different
but related in the sense that they have
a similar goal so C sharp volatile are
different in that they don't give you
this at least last I hope they don't
give you the sequential consistency
guarantee quite they give you a weaker
guarantee openmp 3.1 Atomics are a lot
weaker still in that they give you a
very weak ordering they also give you
operations that are indivisible and
exempt from databases on the other hand
with very weak ordering guarantees and
officially completely unrelated but
unfortunately with confusing naming our
C and C++ volatile switch really do
something else but unfortunately in the
meantime sort of often used as a hack to
get similar semantics to the two Atomics
year which is jello bowels have
semantics which affect
memory accesses for non ball storage
locations like the site can be used as a
signaling mechanism to say these
envisage has happened in this case for
me that that's the case here that sort
of implicit in this statement that they
preserve sequentially consistent
semantics so long as there are no
databases on on non atomic variables
without that guarantee you don't have
that property because anything that uses
the that uses this an atomic variable
for signaling might use the atomic
variables to present to prevent a race
on something else but would not have
sequentially consistent semantics is the
last bus operator is a special case
that's actually a place with C++
deviates from some of these others and
that has that sort of an artifact of the
language because the way Atomics are
defined Atomics are a template that
class so they have a bunch of overloaded
operators anyway and the natural thing
to do in that context is to just define
plus plus to be an atomic increment
there are some p defined operations that
in fact our atomic but I mean the only
ones that are there's also a compare and
swap called compare compare exchange but
you have to do it yourself to make sure
it works as the way one
you can I mean get all of these
operators can be implemented with
compare exchange more or less you know
more about the more or less than most of
us met okay so I'll say a little bit
about sort of the loophole that was
introduced into the language here in
order to address performance concerns so
the as I said so in the we sort of
hinted at this the atomic operations
have fairly strong properties and the
compiler has to do a fair amount of work
in order to preserve those properties
that can be fairly expensive it's
actually getting less expensive on
modern hardware than it used to be but
it's still fairly expensive so in order
to prevent that from getting in the way
in those cases where people would
otherwise be tempted to write data races
instead this this actually is another
facility that's part of the Atomics
library and C++ which is that program
has allowed to explicitly specify weaker
memory ordering than sequential
consistency even in the absence of data
races this as it turns out greatly
complicates the specification it also
greatly complicates the programme's job
the programmer actually has to
understand a really complicated
specification in order to make this work
correctly this ongoing work here in s2
how to isolate that complexity in a
library that's a non trivial exercise so
if you do the the bad news is that using
this facility actually is is much more
complicated and much more backbone than
just sticking to the default
sequentially consistent semantics
probably more so than most programmers
appreciate which makes us this dangerous
on the other hand it sometimes is
significantly faster on current hardware
unfortunately
atomic object has to be implemented is
the hardware's when you're enforcing all
the hardware companies well what's what
actually happens these days is that
atomix are typically implemented with
ordinary loads and load and store
instructions for small objects they
implemented with ordinary load and store
instructions and memory fences but
that's hardware-specific so on some
hardware you want to use there are other
primitives other than memory fences that
enforce ordering so itanium and arm v8 I
have other primitives to do that but the
default mechanism these days are memory
fences yeah and get rid of the bridge of
consistency so you have a gold standard
of correctness I you mean a compiler
flag to basically ignore all of these
things I that's not something the
standard can really address I mean
that's something that a compiler
implement I could reasonably do that the
standard doesn't address compiler flags
or and yeah strong at always faster
that seems to be going on so I think
it's public that on v8 has has hardware
primitives that model these really
closely on VA doesn't exist yet but
clearly we're moving in the right
direction suppose everything for me is a
dog and I am just you know program like
that how slower with my bro and I'll
actually say a little bit about that at
the end if I have time it will probably
on existing hardwood will probably be
quite slow on x86 hardware turns out
store instructions end up being quite a
bit slower load and selections are
unaffected essentially so basically for
these low-level Atomics we still have
the wall that pails of atomic operations
can never form a data race that's
unchanged but it's hot these atomic
operations can specify explicit memory
ordering and the way that works is
normally when i load an atomic value
probably typically I would just write it
this way as I would just mention the
atomic in an expression it turns out
under the covers there's an implicit
conversion here from the atomic type to
the underlying type that's really
equivalent to writing it this way but if
I did this way I can give it an explicit
argument that specifies the ordering so
if I write memory order sequentially
consistent that's a very verbose way of
saying exactly that so the ordering
types we have here is that the this
actually one more that i haven't
mentioned but the the main ones here is
we have what's known as a query lease
ordering so the idea here is i won't say
very much about this is but if you
specify memory order release on a store
operation that basically means that when
you perform a load operation that sees
the value that was stored by the store
operation and the load operation uses
memory order acquire then you guarantee
that after that load operation you can
see all memory operations that were
performed before the release operation
so this is the ordering that you want if
you use a flag to communicate from one
thread to the others of the minimum
ordering you need their memory order
sequentially consistent in force
additional ordering so that things like
decorous example work memory order relax
basically doesn't ensure any order thing
almost it actually does ensure what's
called cache coherence which is that
operations on a single variable appear
to occur missing all in a single total
order so if we wanted to increment a
counter in a way that we don't actually
need to read the value of the counter
until the end of the program until all
the threads are finished then there's no
reason to enforce ordering when we
increment the counter here so we can so
we can implement the increment operation
by X dot fetch add and then specify
memory order relax as the ordering type
and on some hardware that will make a
difference on x86 that won't make any
difference but but when you do that you
have to be really careful because their
usage more their usage models for which
this doesn't work so for example if
you're using this to increment a pointer
into a buffer that another thread is
leading that's the long way to do it it
will not work so let me try to clarify
some of this by going through a bunch of
data arrays examples and and hopefully
give you some insight as to how these
things actually work so databases are
crucial here if we want to really show
that a C++ or C program is correct so
rather than in the old model well
basically in order to show that anything
is correct in this model we first have
to demonstrate that there are no data
races once there are no data races well
in improving that there are no data
races we get to assume sequential
consistency because databases are
defined with respect to sequential
consistency once there are no data races
we can prove correctness sort of more
traditionally of the program assuming
both sequential consistency and since
there no data races we also get to
assume that synchronization free coded
agents are at Omega indivisible making
some of making that proof easier it
turns out as we'll see later there's
actually sort of a third proof
obligation here which will run into in
one of the examples yeah but so as a
simple example if we use the simple flag
case of communication case yeah i
initialize a set some valuable 242 then
I tell another thread that I'm done
initializing X the other thread waits
for the done flag and then reads X if I
declare down here to just be a boolean
flag this is incorrect there's a data
race undone as many people have found
out the hard way this the hard way this
is actually one of the few uses of data
races that actually fails Philly
repeatedly and repeatedly in practice
because what typically happens is that
the compiler notices that done is loop
and very and then moves it out of the
loop and checks it exactly once so that
will fail if I want to fix this what I
have to do is I have to declare other
than declaring done to be a bullion I
have to declare to be an atomic of pool
and this is precisely the model for
which acquire release memory ordering
was designed so we could probably get
away with using memory order release
here and acquired on the other side so
he is actually a confusing example which
as a ton doubt caused us a lot of
controversy typically and when people
asked whether or not this this has a
data race this is an example that study
dot B is actually fond of pointing out
and as a counter example to all sorts of
interesting things so we have a program
he again we assume as in all of the
examples that x and y initially 0 i I
then
one thread that checks X if it's nonzero
sets why do one and the other thread
sort of does the converse of that the
question is does this have a data race
many people look at that and say well
they touch the same variable so yes but
actually it does not have a database the
way to convince yourself of that is data
races defined with respect to
sequentially consistent execution
there's no sequentially consistent
execution of this in which either
assignment is executed so therefore this
is a program without any assignments so
there's no way they can possibly be a
database so this gets back to the sort
of original motivating example if we
have a structure with two character
fields and we assign one to the a field
and the sign one to the B field and the
other thread does that have a day two
days in C++ 11 this does not have a
database x basically the a fields and
the B fields are separate memory
locations the separate scalar objects
they have nothing to do with each other
so an assignment to one does not
conflict with another one in this has to
work correctly under POSIX rules this
was this is complete this is actually
intentionally implementation defined if
you try to do this on an alpha you may
well get the long answer but yeah
laughing certain high
kind of is her news I actually
fortunately it turns out the only major
architecture that really had trouble
with this was alpha I which is no longer
a very interesting architecture and
alpha only had trouble with it until
nineteen ninety-five as it turned out so
even if alpha was still around this
wouldn't be a problem the constraint on
the hardware really is that you need
bytes store instructions in order to
implement this and everything other than
pre 95 alphas basically have white store
instructions these days they were big
fields to those fools reference right I
know I actually those are the next
examples yeah so if I try to do what
sort of logically the same thing now the
answer is different this has a database
and the difference here is that a and B
a pop of the same contiguous sequence of
bit fields technically this is zero
length bit fields they play a special
hole here but they already did before
this but that aside these are part of
the same memory location so therefore
this and that both assigned to the
memory located same memory location in
the terminology of the standard and
there is a data race so this is not
allowed the mixed case is interesting in
that by the the walls of the standard
this this luxury has two memory
locations the a field is a memory
location and the sequence of bit fields
containing only be is the other memory
location so this does not have a
database so this should be okay on the
other hand it turns out if you tie that
sort of with every 2011 and all year
compiler for x86 or something this will
give you the wrong answer so the
standard way to implement the assignment
to be there is to read the whole incised
would replace the bits and write the
whole in size feel back and the standard
basically made that illegal so compilers
have to change to deal with that that's
not particularly expensive but it's a
change
the code sequence to implement the
assignment to be there changed so far
we've been talking mostly about scalar
objects so I'll have one sly or a couple
of slides you're on library issues so
what happens if I now rather than
performing operations on a scalar object
I perform an operation on a sort of
library container so in the case of C++
11 you say I have a list of ins and I
simultaneously a executed push front and
a pop fund operation on that list so
that also it's not clear whether that
has a date it's not immediately clear
where that has data raised because it
depends on whether I access the same
scale of object at the same time but the
crucial convention here is that the the
crucial convention for library writers
is that libraries shall only introduce a
data race at the scale of object level
if there is sort of logically a data
race at the library object level so if I
have two lights to the same list object
at the same time as in this case the
library is allowed to introduce the data
race that's not allowed so this has a
data race if I perform two updates to
different library objects at the same
time that's not allowed to introduce a
database so if i have a user implemented
allocated underneath this that's share
they cause all instances of lists it has
to make sure that it does enough locking
so that operations on different lists
can proceed in parallel without
interfering with each other but it
doesn't have to do locking in order to
make this sort of access actually safe
and that's the default convention for
libraries and c++ it's kind of
interesting because usually when you
look at the traditional literature hear
the people distinguish between Fred safe
and tread on safe really the default
convention I claim the convention you
usually want this actually somewhere in
between it's precisely this convention
that the the notion of data arrays at
the library level reflects what it
be at the scalar object level yeah to be
completely screwed self potatoes you
couldn't have a library object that
exposes some internal part as a separate
object because then you know all are
going to be able to follow this rule and
you know races so I mean this is a
default wall so I mean you can have
suddenly have libraries that are
exceptions to this so their libraries
that are going to be strong that are
going to export stronger properties and
that they design for concurrent access
the more analogous to atomic scalars
then if a we care at least I mean you
basically have to document such behavior
if you're going to use it this is not
not a hot wall it's the one that's fault
that's followed by standard the standard
library except when it specifies
otherwise yeah sufficient
it's okay to have a race on the object
but new race in the implementation I
that's okay right yeah I yeah this is
probably this is probably stronger than
it should be here you're right yeah yeah
two concurrent leads on objects have to
work without introducing a data race so
if you're implementing a splay tree
beware then or specify that this is not
read safe and the client has to be where
so this is somewhat weird case here what
happens if I have an infinite loop and
then as I assign 1 to X while assigning
to 2x and the other thread is that a
data race actually in Java it's not but
in in C++ actually it turns out the
answers also it's not it's not a data
race and it's really hard to define this
to be a data race on the other hand it
turns out that their various reasons why
you would really like compilers to be
able to interchange coda cost that
infinite loop so for that reason and for
other weird historical reasons it
actually turns out that in C++ 11 the
wall is that infinite loops like this
that have no I Oh effects and no
synchronization effects themselves and
vogue undefined behavior so this is not
does not technically have a data race
but it has the same semantics as though
it had it as if it had one but that's
because the infinite loop itself is
about
okay um so here's one I'm not sure that
that's particularly relevant to this
audience here the important point here
is that what I'm doing is I'm setting a
variable to X and then i'm using a
condition variable I'm notifying a
condition variable when I'm setting when
I'm done setting X and the other third
sort of checks if X is equal to zero if
I happen to execute this is all done
inside a critical section here if I if I
happen to execute before this critical
section executed then I wait for this
critical section is set X equal 42 and
the question is does this have a day
today's can I safely access X after that
do I know that X has been initialized
the answer is yes it does have a data
race the reason it has a data race is
because in C++ like almost every other
language condition variable weights can
wake up spurious Lee so the fact that
you executed a condition variable way
tells you absolutely nothing about the
state of the computation so this which
is if you put it in a loop then this
does not have a database and it's okay
so I mentioned this primarily because I
regularly see research paper submissions
about how to do flow analysis how to do
how to analyze programs with condition
variable weights and the answers you
don't and this actually is similar
somewhat somewhat less expected
situation with Thai lock this is a
really weird program which uses locks
backwards or uses mutexes backwards so
what I do is I set X 2 42 and when I'm
done I lock the mutex the other thread
waits for the mutex to be locked and
then the question is going to conclude
that X is now 42 and the answer is no
and there's sort of a difference here
between the official explanation in the
real reason for it the official
explanation in the standard is that tie
lock it can spurious Lee fails
just the fact that that tie lock failed
to acquire the lock doesn't mean it was
actually available I the deal
explanation is that in order to you
don't really want to implement highlight
that way on the other end and in order
to make this work correctly you would
have to prevent the ordering of those
two and it turns out that's expensive on
a bunch of hardware and useless for real
code it's only useful for for code that
you really don't want people to write i
double-checked locking this is a for
many of you a well-known example this
used to be an advocated idiom for
programming with with feds the issue
here is that I want to initialize X on
demand before I access it but I want to
do this in a way that I don't have to
acquire a lock every time I access it so
I could obviously do this correctly if I
just use the code here if I left off the
conditional at the end at the beginning
here if I just a quiet the lock
protecting x and then checked as it has
it been initialized if not initialize it
and so on but it was recommended that
you check at the beginning before you
acquire the before you acquire the mutex
and then if it's not initialized you
reacquire the mutex so that only one
thread can initialize it and the answer
is as many of us know I think at this
point that's still incorrect the problem
is that this assignment to initialize
the laces with the the initialized
access outside the critical section and
in fact this there's no real guarantee
that this will work in particular I can
interchange these the compiler Canada
change these and went break the code
oops
um this one is sort of I don't know yeah
yeah I should have mentioned the way to
the way out of that is to make the it
make the net flag atomic war good point
yeah so yeah I was told it's okay to run
over a bit here hopefully that's so
here's another example which is really
of interest mostly to C++ programmers
it's a much more C++ specific than the
rest this is sort of a trick question do
these things days so what i'm doing here
is while in a critical section protected
by mu takes em i push some something
onto the front of the list and then i
have an infinite loop which goes around
and acquires the mutex occasionally and
checks whether the list is empty and
does something with the entry on the
list if it's not empty does this have a
race a database the answer is yes but
not the one you expected maybe these two
don't actually days because the axis is
2x all here inside the critical section
protected by M the problem is with
having an infinite loop that's providing
this service year looking at the list
leg yoli the problem is at some point X
when the program shuts down X is going
to be destroyed the disliked afore X is
going around and this guy is still going
to be running because after all it's an
infinite loop so you end up introducing
a data race here between excess
destructor in the infinite loop so in
this model basically having preds that
run forever until the process shuts down
is really not acceptable it turns out
c++ 11 provides some notion of detached
threads which to first sort of
approximation you just shouldn't use
it's all let me quickly say something
about implementation consequences I
think we've already gone puse a lot of
this so the main implementation
consequence is that implement
patients may not visibly introduce
memory references that went there in the
source so one example of that was
reading and rewriting in adjacent
structure field when you assigning to
one field of a structure so there are
lots of implementations actually do this
i'll give you another one really quickly
other than that basically this model
restricts the ordering of memory
operations around synchronization
operations substantially and the
compiler has to be careful and
synchronization operations need to
include memory fences and so on to make
that sure to make it to ensure that on
the other hand within within a region of
code that contains no synchronization
operations the compiler is free to the
order basically at will because those
look atomic because of the data arrays
free property that's true for both
weekend strong Atomics but the week
Atomics count as synchronization
operations in terms of determining the
synchronization for your agents so
hardware requirements we said we already
said that we need bites stores we also
and this is sort of a longer taught by
itself the hardware has to be able to
enforce sequential consistency if I if I
end up writing all my code using Atomics
I have to be able to implement that so
it really look sequentially consistent
and it turns out that introducing fences
between every pair of instructions that
does often doesn't work so for example
on titanium that's not sufficient it but
those architectures generally have other
mechanisms for enforcing that
the basic poly yeah you do you know
about the independent reads of
independent rights example let me talk
to you after about it afterwards I think
that's a fairly long discussion so
compiler requirements we don't get to
introduce memory references we don't get
to introduce databases that went already
there so and that part of that is that
select fields and especially bit fields
need the careful treatment it turns out
there are also other cases though where
compilers naturally want to introduce
stalls that went there originally so
this was really sort of the example that
motivated by looking at this in the
beginning so we have a we have a loop
here which every once in a while checks
am I multi-threaded if I multi-threaded
a quite a lock so I should have said
something dot lock here this is an old
slide so if I multi-threaded acquire a
lock and at the end if I multi-threaded
it quite an unlock multi-fit so at least
the lock in between there I use some
global variable G so the problem is that
with fairly traditional compiler
optimizations as you sometimes find in
textbooks you can optimize this in the
following way especially if you have
profile feedback information that tells
you that usually this program is single
threaded so you know that typically
these lock and unlock operations are not
executed so what the compiler can do is
promote the global variable to register
as far as the compiler is concerned lock
and unlock a function calls that it
knows nothing about so what it's going
to do around these lock and unlock
function calls i well a loaded G and or
global register into the global G into a
register I found these function calls
that I know nothing about I take the
registers stall it back into the global
and reload it after the function call do
the same thing down here and at the end
I you take the register value and assign
it back to the global for sequential
code where lock and unlock
just function calls that I know nothing
about this is a perfectly good
optimization and the code runs faster if
in fact MT is usually false if I look at
this at as a multi-threaded program and
I understand that this is checking
whether it's multi-threaded and these
are lock and unlock operations this is
just complete gobble to the outcome
output is complete gobbledygook right
the what I'm doing is G was access to
only inside the critical section now
it's access repeatedly outside the
critical section so basically don't do
that on the other hand compilers did do
that they actually do it fairly
frequently in this case which is sort of
easy to understand why they would do it
if I have a loop that counts the number
of positive elements in the list it's
tempting and say count is it global it's
tempting to promote count to a register
so they register equals count increment
register in the loop and then stall it
back at the end again this is
potentially introducing an efference if
there it's if there are no positive
elements in this list I've just
introduced this stall to count where
there wasn't one and I've introduced the
data arrays so don't do that either in
compilat I just like that last
implication
a compiler has to be conservative right
if it has a function call you could
implement block from an underlying
atomic the I attend a function call
could be a locking operation or not in
operation so so the mother's now have to
be conservative with respect to whether
calls might do synchronization
operations in a sense I mean in general
what compilers are just not justifying
in introducing not justified in
introducing stalls two variables where
there was no stolen the source that's
really the war so you have to be really
careful about this sort of speculative
register promotion you don't get too if
you had a CSE involving memory
okay yeah I mean it's actually it's not
too painful the latest version of GCC
actually does this correctly i don't
know about visual studio but would be
fridge do it at the end when you do the
right if the register has changed you
know and that enjoyed that scheme in
general works and that's what a GCC
ended up doing the Cleverley schemes as
well which turned it out turned out not
to work as well but so but that one that
one suddenly seems to work if in
addition if you do this but also in
addition he has set a flag whenever you
actually increment register so whenever
you would have assigned to count is set
a flag and then do this store back only
if the flag is set that solves the
problem it solves the problem and pop
because of the next slide here but so is
some sometimes adding data races at the
object code level is actually ok so long
as they're not observable so if you're
writing a C++ 11 to c plus plus 11
transformation system you never allow to
add a data race because that changes
defined semantics undefined semantics
but if you're compiling C++ 11 to some
machine code a machine code does not
have undefined semantics for data races
in general it might it might effectively
because in some hypothetical ideal world
we might want to have the hardware check
for data races but come hardware doesn't
unfortunately so in a case like this
when we're only reading the global it's
actually acceptable to read it
speculatively outside of the loop where
we might not otherwise have read it
because we can show that sort of based
on the semantics of the underlying
hardware this actually has no impact the
user can't see this but as the C++ 11 to
c plus plus 11 transformation this
transformation is not legal
the news isn't all bad actually so I
don't want to go into details here as a
result of clarity in the memory model
they actually certain kinds of analyses
that we traditionally haven't done which
we now know for sure actually correct so
for example if we have this program here
which assigns to 2x and then execute
this loop which performs a critical
section in here but doesn't assign 2x
inside the loop only references at X
there we naturally now know that X is
constant we can prove that based on the
data arrays free assumption in spite of
the fact that there's a critical section
in here so long as there's only one
critical section in here and this is
this is joint work with the student at
the University of Washington two of my
colleagues at HP Labs so i'll conclude
conclude here with some sort of
explanation if i've tried to convince
you that data races are bad and the
standard basically tells you don't use
them however yeah researcher is to
gradually figure out that the five
learner is free of data races how are
already program is to manage their
business oh not that this is free of
data races the fact that that x actually
is constant here so this is something
that the compiler will figure out the
programmer doesn't need to figure this
out there's an optimization problem on
this the the other question you're
asking is is still a good one though
which is how do you know that your
program is actually free of data races
and the answer is I though I haven't
been a good example of this I I think we
actually should rely on database check
us a lot more than we do and personally
I think the right place we should be
headed but it's going to be really
difficult to get there is to get to the
state where the hardware actually does
data is checking but we're going to
accept some small after accept some
small performance loss in order to do
that and we're going to need hardware
support in order to do it so what I
wanted to convince you is that even if
you don't believe the language
specification they're actually all sorts
of things that actually
do go along and practice all it can
potentially go on in practice if you
program with databases the other way to
look at it is these are the kinds of
transformations that you may see that
actually motivated the catch fire
semantics for databases so here's a
simple example or some where things can
go along in very unexpected ways as a
result of putting a data race in your
program so what I'm just checking is X
is X less than three if X is less than
three I perform a switch on the three
possible values of X and and now let's
see what happens here when the compiler
it translates this let's say the
compiler translates this sort of
relatively naively with with one
exception here which is it transforms
the switch to a branch table which is
common so it will use the value of x to
index into a plan into the table of
branch targets and then branch to the
right table entry the one clever thing
that it decides to do is that it knows
that x is less than preform up here so
it gets rid of the bounced check on the
branch table index so now what happens
if there's a race and X changes in the
middle here to let's say 3i access a
branch table entry that's out of bounds
and I branch to nowhere so that's one of
the things that can happen there bunch
of other failure modes for code
containing data races that we've already
seen so the invariant code moved out of
loop is one failure that's actually
somewhat to be producible if in the
presence of data races we have to worry
about bite operations and unaligned
operations being used to access
variables so we see fractional updates
um for something like you're done flag
here if I don't declare doneness atomic
at all what's going to happen and
practice is that these two the compiler
or possibly the hardware may order these
two operations so that in fact when done
when I see done set in the other thread
here I'm not guaranteed
data has actually been set to 42 one
interesting case here that people always
bring up of databases that I definitely
be nine and we should all wear we should
be able to use is the case of redundant
lights so if I have two threads both of
which set X 2 17 that should be even
better than having one of them said X 2
17 right so I should definitely know
that exit 17 at the end the answer is in
this blade new world not necessarily I'm
yeah and then I want to do this for some
reason well but you can still do it
using Atomics right but do I know that I
don't pay for extra panties or x hey
well I mean if you're willing to live
dangerously but not this dangerously you
can always say memory order relaxed
which basic and again I don't know what
happens at all maybe what if i want to
taste if you laugh at nothing else bad i
mean it's tough you don't really see a
performance difference there right i
mean i think this is very hard to come
up with architectural architectural
cases where that actually makes a
performance difference that i have a
narayanan computing injuries and edge of
three times a week zero are no nobody
has completed it and then i computed in
right exactly
can't think a girl you have stuff
happening here in the memory model on
the real machine but very difficult to
express that here yeah but I mean the
problem is in order to actually get that
guarantee and it was really difficult to
express that guarantee in a language and
it also ends up inhibiting some
transformations that probably in the
long run we want for performance benefit
that's really very nebulous i think i
did not nebulous in this case it seems I
straightforward is not already
well except I mean if you do there's no
performance I don't I think you'll be
hard-pressed to come up with a case on
mainstream hardware on which there's
actually performance difference between
memory or a relaxed and the desi store I
guess so what you're saying is I use
memory order relax and we all
essentially just drop down to the
hardware in terms of guarantees that i
get i it's not quite the hardware you
get you get cache coherence which is why
I'm weasel Wooding a little bit but I
mean that's actually useful guarantee I
mean people are very surprised when they
don't get cache coherence and on some
hard way you don't so I thought about
hardware you need ok that's easy yeah so
did you explain the problem yeah i know
i'm about to you got sorry and that's
the then i'm pretty much done here sorry
looking so the problem here the reason
that why assigning a 17 0 X twice isn't
necessarily better is not we've already
seen a bunch of cases with self
assignments introduced by the compiler
cause problems and where the compiler
really wanted to introduce a self
assignment but it introduces erase so it
like in this lecture case if we look at
the other field what we're doing is
we're basically assigning the other
field to itself behind the covers
normally in this memory model it's not
okay to do that because if we introduce
the self assignment as we saw earlier we
can hide an assignment in the other
thread so if I just have absolutely
compiler introducing x equals x while
another threat is executing x equals 17
that's not okay because the assignment
to the assignment to exia can be hidden
right I on the other hand compilers
sometimes want to do that and the
problem is that in a it actually turns
out in this memory model if I see x
equals 7 17 it becomes legal to
introduce the self assignment because I
know that there's no data arrays nothing
is racing with this so therefore
if i put x equals x after that that's
okay so some of those dubious
transformations that i told you about
before actually can be reenabled in
cases in which i have a visible
assignment to X without intervening
synchronization so tell so x equals 17
again we can be safely transformed to
this if i assume the data races right
and the compiler will assume no data
races right so so the problem i have now
is if i have x equals 17 here it's
actually legal to transform this to a
self assignment x equals x and thread 1
followed by x equals 17 in the same way
but in the opposite order here and now i
can construct an interleaving where's a
result of the redundant assignments of
17 2 X I actually seen either so okay
and actually let me skip this because I
think we already motivated y self
assignments or interest it can be added
let me just conclude with this quick
slide here so the other question is if
you ain't want to introduce data races
what they actually buy you and this is
sort of more or less asking the question
here it does need some explanation here
what i'm doing here is i'm running
apparel apparel sieve of eratosthenes
program which is sort of propagating
different times eliminating different
multiples of different times by
different threads the top line well it's
doing the elimination by just stalling
to abide at a each time the top line has
the store to each fighter a protected by
by a critical section by a mutex lock
unlock the middle line uses a
sequentially consistent atomic operation
the bottom line uses a plain store
operation as you would get from either
or memory auto relaxed operation it
turns out those would probably generate
this exactly the same code here since
this is on x86 and in fact memory order
release would also generate the same
bottom line here so what's what was
interesting to me here at least is that
if you look at the single fitted
performance and I sort of cut off the
top here so you can't really see it but
in fact there's a huge overhead
associated with either a mutex or
sequentially consistent atomic because
this is really saw heavy code and it
turns out on x86 sequentially consistent
Atomics need offense so the fence sort
of dominates the running time at the
single threaded end as you scale up to
higher thread counts that difference on
this machine at least essentially
disappears which initially people might
find surprising but in retrospect
actually makes sense because i think
what i'm doing here is i'm scaling this
up to sufficiently many feds this
completely memory bandwidth limited and
it turns out all the synchronization
overhead that I'm occurring in order to
protect the the store with locks or the
additional memory fences introduced
primarily much more local behavior which
doesn't interfere with the other calls
so in some sense the point here is that
by going through this all this effort
what I'm actually benefiting is
primarily sort of performance at local
councillor other than hike accounts at
least based on this example yeah those
are all over the place this is a highly
non deterministic example because then
they know data races in the blue one but
they're definitely racist as to who gets
which time so that that's my explanation
I didn't go far I didn't get very
detailed profiles and but it's not too
surprising so the summary basically is
my way over time here is that basically
don't use databases databases are evil
ok any questions and they're they're
sort of some this my usual list of
references the best description of the
the C++ 11 memory model is probably by
the the Cambridge group here which is
also the the most mathematically intense
or depending on which vary and you want
this is a lot more precise than what the
the standard actually says yeah same
image languages like Java or.net the
same subset thereof anyway we try hard
to ensure that data races cannot cause
violations of type safety and I guess
there's two motivations one is limiting
what your program can do its erroneous
and the other is debug ability in the
face of data races how that I will do
you think those are and how problematic
is giving them up
in the catch fire semantic stealing
that's an interesting question i mean as
you probably know I'm we have I didn't
talk about Java here very much at all so
the state of the Java memory model is
sort of clear in the data arrays free
case I mean it everything works the same
way as in C++ there was an attempt to
define what data races mean and I think
that that define that the attempt is
generally viewed as unsuccessful so the
and as you said I'm but I'm not sure the
motivation is just to put dues to
preserve type safety I mean I think in
the job are setting the way people
usually perceive it is the security
model allows you to run untrusted code
inside unit enough space and once you
say that you're going to run on tastic
code inside your address space I think
you do have to preserve type safety I
think we could probably ensure that you
preserve type safety in the presence of
data races just by mandating that
explicitly without saying anything
terribly complicated that itself is not
sufficient for java I because it turns
out i also need the it's a sort of
absence of thin air results guarantee i
need to be able to guarantee that
somebody can't manufacture a perfectly
typesafe pointer to a password string
that they weren't supposed to have
access to and ensuring that turns out to
be incredibly difficult but I think it's
also quite important unless we really
give up completely on the Java approach
to security
so one of the ways we could make sure if
the model is useful to programmers is to
see how hard it is to put programs
correct that are written in this model
and see a clear path for the programs
that stick to the DRF strong stuff right
not so clear for the programs that
really try to you know use more relaxed
the corded more relaxed do you want any
successful intends to full correctness
of such low level I don't know of any
lily and I I completely agree that's a
concern I mean so long as you stick
purely to the sequentially consider to
the sequentially consistent subset in
the end data is three programs it's fine
and I think in many ways this simplifies
matters a lot there right because of the
the interleaving granularity issue and
then ignore the rest of the program this
would allow you to actually do that
right yeah and in some sense it sort of
gives you a solid footing for what I
think everybody has been doing already
anyway which is to prune the number of
interleaving you need to consider
effectively to only interleaving zuv
synchronization for your agents and you
can do that also for a for proving the
absence of data races i believe so yeah
for that one part we have a good story
but I completely agree that for the for
the general model we don't have a very
very good story
the other question</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>