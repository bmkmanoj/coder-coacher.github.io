<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lazy Transaction Execution Models | Coder Coacher - Coaching Coders</title><meta content="Lazy Transaction Execution Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lazy Transaction Execution Models</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BKI3mFeqS9Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good morning everyone thank you for
coming it's my great pleasure to
introduce udeploy he is joining us from
Cornell University where he's Co advised
by Johannes gurkha and Christophe cock
or cock how you correctly pronounce that
and also he has done a microsoft
research internship here and he's also
done a google internship he is the
co-winner of the sigma 2011 best paper
award together with some of his
colleagues at cornell and today he will
talk to us about lazy transaction
execution models ok thanks for the
introduction Christian so today I'm
going to present my thesis work on lazy
transaction execution models so let me
start by reminding you of what a
transaction is a transaction is a single
execution of a user program over a
shared database tate informally it is a
basic unit of change which the date
which the database sees and this
execution of the program is guaranteed
to satisfy the acid properties and I'm
not I'm sure that all of you are
familiar with what acid is so I'm not
going to go into the details of that but
let me show you what such a user program
usually looks like so consider Mickey's
transaction to book a seat on flight 123
so the fact that this program has to be
executed transactionally is indicated by
the keyword start transaction it has
these four statements first Mickey
selects a seat on flight 123 it checks
if there is something which is available
if not then the transaction rolls back
if there is something available then it
does to update so the database it
deletes that seat from the available
table and it inserts a tuple
corresponding to the reservation into
the booking stable now this of course is
a very simplified form of what you would
see in a real world so before I talk
about how and why lazy execution as good
let me show you how we can execute this
transaction in a classical model and
why that leads to sub optimal results so
consider the following scenario in which
you have a flight in which you have
three seats available now the available
seats are in green the holiday reserved
seats are in red so 1a 1b and 1c are
available let's say Mickey issues the
transaction the program which I just
showed you to book any seat and he gets
seat 1a after that let's assume that
Donald issues a similar transaction and
he gets seat 1b finally many issues a
transaction however many has an
additional constraint that she only
wants a window seat now the only window
seat which was available only 1a has
already been allotted to make and
therefore minis transaction had to abort
now if you assume that you knew that
minis transaction was going to arrive
then you could have given Mickey the
seat one see Mickey don't really care
about which CT got in which case
Minnie's transaction would have
committed so let us see how in a lazy
execution model addresses this issue so
again consider the same scenario ah now
Mookie Mickey says book me any seat and
instead of assigning a single seat to
Mickey I'm going to come at Mickey's
transaction and I'm going to defer the
assignment of seat to binky subsequently
I'm going to do the same for Donald's
transaction I'm going to ensure that
Mickey and Donald both have some seed
but I'm not going to tell them which
exact seat they have finally in this
case when Minnie say is that I want a
window seat I can actually assign mini
the window seat which was available in
this case 1a so I made these two
assumptions of what these transactions
are doing one is that there is a
flexibility in the value that is being
written that is Mickey does not care
which exact seat he gets as long as it
satisfies a certain set of constraints
and second that there is a delay between
the point at which the transaction
commits and the point at which you read
the values which are written by the
transaction now human be for now and
assumed
there is a broad class of applications
which satisfy these two assumptions and
I am going to return back and precisely
identify what this class of applications
is now assuming that there is a class of
applications over which these two
assumptions hold the key idea is we can
lazily bind the unread values in the
transaction and by doing so we are
creating some room to maximize some
notion of global utility in this
particular application the global
utility was to satisfy the maximum
number of user constraints are
equivalently to allow the maximum number
of transactions to successfully commit
so let me give you another example in
which being lazy helps so consider a
simple voting application in which we
are using this votes table to keep a
tally of the election status so in this
case the Democrats have 100,000 votes
cast for them the Republicans have 75
thousand volts and I have three
transactions which the application or
three user programs which the
application can execute as transaction
one is to cast a vote for Democrats
which basically just goes and updates
the count variable of Democrats by one a
second to cast a vote for Republicans
which increments the value corresponding
to Republicans and a third transaction
which checks who is leading so it reads
the Democrats and Republican counts it
compares to two values and displays who
is the current leader in the election
furthermore let us assume that this is a
nationwide election and I am replicating
this vote stable across two data centers
one in the east coast and one in the
west coast and also that the initial
state of the database is that the
Democrats are leading the Republicans by
around 25,000 volts so what happens if
we execute these transaction under
strong consistency so whenever I cast a
vote in this case of voters cast for
Republicans I need to consistently
change my replica state across these two
data centers right so I need to inform
so if I am executing the transaction
West Coast data center I need to
synchronously inform the east coast data
center of this change and at least I
have to incur one round-trip latency now
of course and the strong consistency the
programming model is very simple because
the user never has to bother about
perceiving inconsistent or two different
replica states the other extreme is to
be eventually consistent in which you
say that I am NOT going to inform the
other data center synchronously I am
going to do that asynchronously so when
my transaction execute on the west coast
data center I commit it locally and I
keep my fingers crossed and hope that
this transient inconsistency between the
Drude applica states is not perceived by
the application and in this particular
case it is actually not perceivable by
the application because all the
application cares is who is leading and
that transaction is going to evaluate to
the same thing over these two states
however you can imagine scenarios where
it is actually where the Democrats are
leading by one at which point if you are
executing an eventual consistent model
then transaction 33 can see two
different states and it cannot the
application can basically perceive one
in which the Democrats and Republicans
are tied and another in which the
Democrats are actually leading so this
exposes a to this type of inconsistency
then has to be handled at a higher level
in the application right so can we get
the best of both worlds that is can we
get the clean semantics of strong
consistency as well as the fast response
times of eventual consistency and I'm
going and the answer is yes we can at
least under certain assumptions and for
certain class of applications we can so
let me show you how so the key idea is
to exploit flexibility in the reeds
which the application is making so in
the earlier case for this particular
application if these are the only three
transactions which the application can
secured then it doesn't really matter
what exactly the Democrats and
Republican vote counts are as long as in
both these two data database states the
Democrats are leading because the
application cannot actually perceive
this difference so in some sense they
belong to the same equivalence class of
database states in this case a class in
which the Democrats are leading so the
idea is then to be lazy yet strongly
consistent and how can we do so we
instead of requiring that the two
replicas are always identical which is
what strong consistency does we are now
going to enforce that the two replicas
are always in equivalent States as
opposed to being completely identical
and this will allow my two replicas to
diverge but i am going to establish
certain bounds within which they are
allowed to diverge and these bounds are
again defined by the equivalence class
in this case so how do I do it I do it
by using these global treaties you can
assume these global treaties as as a
contracts which all the replica sign and
say that as long as any changes which i
am making are not going to violate the
global treaty i am good but whenever i'm
good as and i can execute things locally
but whenever I am in danger of violating
a global treaty I need to inform the
other replicas so of course I have
shifted the onus of communication from
the transaction to enforcing this global
treaty and unless I have a mechanism of
efficiently enforcing this global treaty
in a distributed manner I would go back
to the strongly consistent case so how
do I enforce this global treaty I
project it into two local treaties such
that if these each of these replicas are
making changes which do not violate the
local treaties I am sure that the global
treaty is not going to be violet so
intuitively you can imagine that I had a
budget of around 25,000 volts till the
boundary of the
violence class and I have partitioned
that into two one of twelve thousand
five hundred volts so and if it is a
little vague right now at you become
more concrete when i get to the
technical details and precisely define
what this projection is how we get this
global treaties yeah cloud works that
relate to this idea as well because i
will then hold off on question yes i'm
going to ask my question and then if you
are going to talk about it you can
decorate fractures how does it relate to
two balls one is consistency vation
right 2009 i think we're doing yeah also
as escrow transaction models right so um
consistency rationing it basically said
that we are going to classify we are
going to have three types of three
classes of objects one which have to be
strongly consistent one which can be
eventually consistent and watch one
which are in between so our work
basically says that you don't have to
classify right between category right so
because based on certain constraints you
switch from eventually consistent so
strongly consistent otherwise you know
we are going to be always strongly
consistent okay except that if the
application doesn't actually require
strong but if the application always
requires strong consistency but the
application cannot perceive some
inconsistencies then I'm going to
exploit that flexibility in the
application to be inconsistent sometimes
okay but using these treaties i am going
to ensure that from the applications
point of view it is always strongly
consistent
regarding the escrow transaction and
demarcation protocol and other protocols
like distributed divergence control
protocols let me come back to that and
they're right so how a transaction is
executed in this lazy yet strongly
consistent way so now when you issue a
trance when you issue a set of
transaction they go to the west coast
data center they are executed locally
and you can execute them locally as long
as this local treaty is not violated now
in this case the Republican count has
reached kind of a border of the
equivalence border of the local treaty
and the next transaction pushes it over
as an it violates the local GT at which
point I synchronize with the East Coast
data center I update it with the changes
which had happened in the East Coast
data center and I renegotiate and
establish a new set of local treaties so
again I made two assumption one was from
the applications point of view there are
many database states which are
equivalent and it doesn't really
perceive what how they are different and
the second that communication is
expensive which is a very mild
assumption and is true and many
scenarios and again I'll request you to
humor me for now and I will come back
and identify a class of applications
over which these two assumptions hold so
assuming that these two assumption hold
the key idea is to lazily synchronize
distributed state and by doing this lazy
synchronization we can minimize the
amount of coordination without actually
sacrificing the consistency requirement
so the takeaway from so far is that many
applications have some flexibility in
the transactions by exploiting this
flexibility in transactions we can be
lazy and I have shown you one example in
which this laziness creates room for
optimizing and I have shown you another
case where we can exploit this
flexibility to be lazy and this laziness
would reduce the amount of coordination
required without sacrificing
consistency so that was my introduction
and the outline for the rest of the dark
is that I'm going to first present a
solution for the for how we can be lazy
and optimized resource allocation which
is the class of applications for which
it is applicable second i'm going to
show how laziness allows us to minimize
coordination and that's my project on
homey estate stasis and finally i'm
going to show some experiments so any
questions on the high-level idea so far
so let me start by revisiting the
original example which I just showed you
so had told you that we had these three
transactions there was some flexibility
in the values which were written and I
had also made the second assumption that
there is a delay between the point at
which the transaction commits and the
point at which the values are red and
the key idea was that we are going to
delay the binding for these values which
are not read by the transactions and
this will create some room for
optimization we can maximize the global
utility and in this particular case it
was to allow the maximum number of
transactions to go through so coming
back to my earlier promise of
identifying what this class of
application s so there are many database
applications which use transactions to
allocate yeah listen are you look at
flight reservation applications of today
they don't necessarily commit you to see
unless you explicitly asked for it sozin
they will not be solving at the database
level filter already saw lots of traffic
flow so the idea is that yes but this
particular okay the idea is not that you
can do it on ok let me rephrase it so
yes you can write custom application
logic to do so which is outside the
database but what we are claiming is
that it is a more fundamental problem
and therefore we are presenting a
abstraction for all of these
applications which can use more than
that there are some interesting issues
which arise out of now that you are
executing this transaction and you have
removed some part of the transaction
error executing at a later point what
happens to the traditional properties
traditional acid properties in some
sense you are not executing it
atomically how do you reason about
isolation because now one transaction
actually can can be affected by another
transaction I don't know if that answers
your question
right so I'm going to use the word
resources as an abstraction for these
objects which are allotted and I'm going
to assume that they are represented as
data items and the database and you are
using transactions to change the state
which is associated with these data
items so this is precisely an example of
an application where those two
assumptions hold so CID is basically a
social seating platform which provides
social plugins so that you can basically
choose who you sit next to in a flight
it may be somewhat one of your friends
it may be you know you can specify
constraint like I want to set someone
else from microsoft research or someone
you know another technical guy of course
you do not want to be in situations like
this another field is another area where
this kind of assumptions hold is Hotel
Reservations where you make a
reservations you don't really know which
room you are allotted you had allotted
the room when you actually get to the
check-in point and front desk capsule is
basically such a piece of you know a
hotel reservation software which as the
advertised intelligently makes the right
offer for the right guests checking but
from the hotel's point of view they are
maximizing the revenue by allocating
rooms efficiently finally I'm sure all
of you have run into a scenario where
you have some meetings which are
scheduled and someone higher higher up
in the hierarchy schedules another
meeting which leads to a cascading
rescheduling of meetings in this case of
course the time slots correspond to
these resources and this is usually bad
for graduate students like us ah who end
up with no sign of our advisors
yes use phantom databases and right so
again going back to our solution we are
going to delay the assignment of
resources but beyond the transaction
commits so as opposed to a classical
model in which the first you first a
user requests some resources with
constraints the system assigns a
resource and then the transaction
commits now we are going to move to a
lazy model where the user requests the
resource with some constraints the
transaction commits if there is a
feasible assignment which exists and the
actual assignment of resource takes
place at some point in the future when a
deed is performed over the database that
is when Mickey Raeleen won't needs to
know which seat he is sitting in and
between the point at which the
transaction commits and the seat
assignment takes place the transit the
database is in a partially uncertain
state and we call this state of quantum
state in this state Mickey has a seat
but which seat is unknown and the
database which manages this uncertainty
is called a quantum database so let me
first show you at a conceptual level how
quantum data base supports this lazy
execution model so let us assume a
scenario in which we have an empty
flight reservation table now Mickey's
transaction arrives and is executed as
opposed to a classical model of
execution in which the database
transitions to a single next state which
corresponds to whichever seat was
alerted to Mickey a quantum database
transitions to three possible states
that as it maintains all possibilities
one in which Mickey is sitting in one a
100 Mickey is sitting in 1b and a third
in which Mickey is sitting in 1c after
that let's when Donald transaction
arrives Donald transaction execute in
each of these three possible worlds and
that leads to even more number of
possibilities finally when many strands
action arrives Minnie's transaction can
only execute on two of these possible
worlds the one in which there is
window seat which is available so what
we have effectively done is delayed by B
by delaying mickey seat assignment and
this we delayed it by maintaining all of
these possibilities we have allowed
minis transaction to successfully come
more formerly a quantum database is
nothing but a set of possible database
states which are reachable through
different choices made in the
transactions and you may find them
similar to uncertain or probabilistic
databases and they basically differ from
probabilistic or incomplete databases
and three main ways one we are
deliberately introducing some
uncertainty and we are doing so to
enable this late binding second we
always need to maintain a guarantee that
it the quantum database eventually
results to a single state it doesn't
really make sense for Mickey to have two
seats and a third is a key design choice
which is ah from where the name quantum
database arises and that is to keep
insert uncertainty internal to the
database and let me come back to this
key design choice in a few slides so so
far I have introduced what at a
conceptual level quantum database is let
me now give you one specific way of
implementing quantum databases clearly
enumerated all of these possible words
is infeasible in fact there can be an
exponential number of possible boys
exponential and the number of
transactions which you are delaying and
there is a literature rich literature on
maintaining these uncertain databases
god table see tables and PC tables how
are we choose this simple representation
so what we do is we partition the
quantum DDPs into two states one which
is deterministic and we and the other
which is a sequence of transactions
which have committed but whose seat
assignment or whose us whose value
assignment has not taken place
so because these sequence of
transactions are already committed the
quantum database needs to ensure or
ensure that there is a feasible
assignment of resources we do not want
Mickey to be in a situation where the
transaction has already committed and
later you see that well I do not have a
seat for you anymore so we need to
maintain some sort of system invariant
we need to maintain a logical formula
which would guarantee that this sequence
of transaction can always execute so the
next question is how do we construct
this invariant automatically and in
order to do this we want to we need to
extract the users constraints from the
transaction itself automatically and
doing this in its full generality is
difficult and therefore we restrict we
require some hints from the user and we
require the user to write the
transactions as these resource
transactions in extended sequel language
which looks like this so it has a sequel
it has a conjunctive query initially
which says what are the resources which
are acceptable to me in this case only
window seats on flight 123 as opposed to
a limit one word a keyword we now use a
choose one keyword which explicitly
encodes this choice or flexibility and
finally we have a followed by clause
which are all the rights which are
dependent on the resource which is
selected and these are the rights which
are going to get delayed and are going
to get executed at some point in the
future now given transactions which are
written in this sequel form I'm going to
use a equivalent data log will like
representation in which the body of the
data log is going to correspond to this
conductive query which is up here and
the followed by clauses will be in the
head and I am going to use the minus
notation for a deletion I am going to
use a plus notation for insertion and
updates can be modeled as a sequence of
deletion followed by another insertion
so going back to the problem of
constructing this invariant we can do it
and now in two steps first we convert
these transactions to this equivalent
data log like form and now we want to
compose these transactions to construct
a single logical invariant and we do
this by unification yeah what does your
followed by the class of sequel inside
the fall out there what is a class of
constraints what is it delete values
insert values of do yellow and sub
queries and stuff no just just deletes
and it just it is instead of atomic
Reuters yes it's probably possible to
extend it further but we haven't looked
into that right so let's say that this
is Mickey's this is the data log like
representation for Mickey's transaction
this is the data log like representation
for Donald's transaction now I construct
a eq equivalent larger transaction which
is a sequential composition of these two
transactions now you want to be careful
because Donald's transactions execute on
a database state which is obtained after
Mickey's transaction has executed and
therefore it should perceive the right
switch Mickey's transaction would have
done in this case it would have deleted
this particular seat and therefore it
results in this additional constraint
which is derived based on unification
between the heads of all previous
transaction and the body of the latest
transaction now this of course is a
simple example of how we do this
composition we have a general algorithm
for composition and proof of correctness
in the paper but I won't have time to go
into that the stock but I'm happy to
talk about it later offline so assuming
that okay so let me just point out that
now that we have this we have this
compost transaction as long as the body
of this composed transaction has a valid
grounding over the database we are
or that this sequence of transaction can
commit so that was the original goal of
constructing this invading so how does
the transaction execute and the over a
quantum database effectively it
basically checks if the invariant which
can be if if the invariant with the
extended sequence of transaction has a
valid assignment or a valid grounding if
there is if this is so you then you
update the quantum state and you commit
the transaction but now the assignment
has not taken place if not then the
transaction abouts so finally what
happens when you perform reads over the
quantum database at some point of the
time Mickey has to actually know his
seat so what happens in that case and
this go back goes back to the design
choice which we made earlier to keep
uncertainty completely internal to the
quantum database so let's say that this
is the quantum this is the initial
quantum database one in which Mickey has
both seat 1b and 1c and now if Mickey
queries am issue Mickey issues a read
query over this quantum database the
quantum database in order to keep the
uncertainty completely internal
collapses all possible balls in which a
Mickey can have two different seats
sorry it it it collapses to a set of
possible worlds over which the read
query has a completely deterministic
answer in this case it has eliminated
one of these possible worlds in general
it can actually be a set of possible
worlds and we have a unification based
algorithm which is not optimal but it
works in practice in fact the optimal
solution is actually pypy to complete
and it can be related to a completely
different problem of you know
information disclosure through views you
know at this famous paper of McCloud and
suits you so I hope if you understand
this then you now understand why we call
it quantum database there's an analogy
which you can draw to scarring your
sketch
that when the cat is inside the box it
can be both dead and alive but as soon
as you open the box which is in this
case issuing a read query the cat can be
either dead or alive but not both after
the query is it you take it back to the
quantum state of it stays no one said it
is so in effect what is happening is a
reed is now also changing the database
Tate it internally may be converted to
an update yeah so the impact is
basically you know in order to so the
whole point of having these possible
balls was to by maintaining as many of
these possibilities I can optimize my
resource allocation right so to minimize
the impact of reeds I want to maximize
the number of possible balls which I
retain and yet can answer the deity
deterministically Jacob function you're
optimizing very flexible so in this case
we are just maximizing the objective
function on maintaining the maximum
number of possible was after the
collapse
we assume that that's the default in
some sense you can you can think of
applications where you would want to
maximize some other notion so let's say
that you know if you want if you want to
maximize revenue then some possible
words may be more beneficial for you
than others you need an extension to
some syntax or new syntax to specify
these early you would set English yeah
you would we don't support that as of
now but it's definitely something which
can be extended
right so the takeaway was that we
exploited this flexibility in the
transactions which are executing to be
lazy in binding some of the values which
are not read immediately in the
transaction and I presented quantum
database which basically optimizes this
resource allocation using lazy piling so
that concludes the part on quantum
databases and I am going to now move on
to homeostasis so any other questions on
quantum databases so far your comments
numbers in terms of doing it outside a
database versus what you benefit do you
get again in terms of performance are
you clean episode you gain in terms of
utility so it's not exactly in terms of
you may gain in terms of performance by
implementing quantum database inside the
database our implementation was in the
form of meditative it sits outside the
database and actually i'm not going to
show you the performance numbers for
quantum databases just due to lack of
time and have some backup slides but we
can go for them yeah but this double
utility you need to whenever rich
framework items also give it a very
special
so take your gear to the governor's
flight another whatever the hotel case
there was an explicit goal that you want
to maximize the mm-hmm so how do you
express that in your body so as I as I
why you don't support at this point we
don't support it but yes a you know if
if we are to build a real system then
that's definitely a useful add-on which
has to be supported other questions
okay so let us go back to this example
in which we were lazy yet we were
strongly consistent and we achieve this
by exploiting the fact that we are going
to allow these two database states to be
in two different states yet to two
different states as long as they are
equivalent to each other and I i kind of
said that we are going to use the slow
build et we are going to project it to
these local treaties and all of this was
a bit abstract so in this part i'm going
to formalize and make all of this
concrete so before i do that i had also
promised that i have these couple of
assumptions and i'm going to come back
and identify what exactly this class of
applications are so let us see a few
examples firstly why is low latency
important right why do we really care
about saving on the network round-trips
now there have been a number of
anecdotal evidence which suggests that
even a hundred millisecond latency in
the course of Amazon causes a one
percent loss of revenue and usually this
figure Rises exponentially with the
added latency so clearly latency is
something which is important in order to
you know which which can directly be
related to dollar values and there are
many applications which satisfy the
previous assumption let's say online
shopping in which the data is actually
replicated across different data centers
and the flexibility is you can imagine
and I'm going to actually show you my
experiments are going to be ntpc double
over T pcw benchmark which is an online
shopping benchmark so you don't really
have to know how many items are there in
the stock exactly so there's a
flexibility in that as long as you know
they are sufficient for your order to go
through similarly in oxygen systems you
only need to maintain which are the top
set of options it doesn't really matter
what the other lower values of auctions
are
and finally this is something which
probably doesn't directly apply at least
right now but you can imagine that if
you can partition the application State
for mobile devices in which part of your
application status on the mobile you
know you are basically saying that you
can make changes to some part of the
application state which is on your
mobile device and as long as you are
doing that you don't have to communicate
to the server then you can improve the
app response time because not every of
your action is now going to require
communicating with a server so here's
the overview of our solution so in the
first step we are basically going to
analyze the application transactions to
automatically identify this notion of
flexibility and the intuition is that we
want to partition the we want to
basically identify which database states
are equivalent and therefore we are
going to partition this piece of
database Tate's into equivalence classes
and we are going to build upon a rich
literature on program analysis because
effectively the transactions as I said
initially our user programs and in the
second step once we identify these
equivalence classes we are going to
exploit this flexibility to minimize
coordination and again the intuition is
that instead of trying to enforce that
the two replicas are in completely
identical state I'm going to instead and
force that the two delicas are an
equivalent state they may be
non-identical and they're coming back to
sleep this question on escrow
transactions and demarcation protocols
and distribute to divergence control
protocols it may be a big bit vague
right now but we are a significant
generalization over each of these
techniques whatever we do a number of
other things and I hope it will be it
will be more obvious by the end of the
talk and I'll let me come back at the
end of the talk to revisit how exactly
we are different from each of them so
let us apply the solution to the voting
example right so the input in the voting
example was this set of three
transaction types the output of the fur
step would be these three equivalence
classes one in which the Democrats are
leading one in which the Republicans are
leading and one in which they are tired
and this is going to feed into the
second step and then the output of the
second step is going to be a protocol
which ensures consistency by requiring
that the replicas always stay in the
same equivalence class so there whenever
you are actually changing from one
equivalence class to the ANA to another
then the protocol is going to ensure
that that happens consistently and no
one perceives that you are in two
different states and that's how we are
going to achieve strong consistency so
with that let me dive into how we do
step one and I'll get on to step two you
later so doing this analysis and full
generality is difficult and I do not
expect you to parse this so we restrict
the transactions to be expressed in a
particular subset of the language this
is the language I do not explicitly I do
not expect you to actually parse this
let me just highlight a few key points
we assume that the database is a
collection of integers we have these I
or statements to read and write from the
database right now we support only
conditioners if then else we do not
support for loops and while loops but
for oltp transactions this is not a big
restriction and finally we have
arithmetic expressions and boolean
expressions so assuming that
transactions are executed transactions
that expressed in this language this is
how a transaction would look like right
so it has a read statement and I'm use
the hatch notation to indicate local
variables the non hat variables are
stored in the database so the read X X
hat would read the value of x from the
database into the local variable X hat
we'd why would do the same for Y and
then the transaction checks if X plus y
is less than 10 then it increments x
otherwise it decrements x
and finally it writes that value back
into the database so this is going to be
my running trans a running example for
the rest of the rest of the talk so let
us try to formalize this notion of
flexibility a bit more assume that we
have these three database states these
three all have you know different values
of excess and wise and yet from this
transactions perspective if you execute
these transaction on each of these TATP
states it is going to produce an
identical effect the effect being
increment X by one so how can we
represent concisely this or entire set
of data be states to do so we use
symbolic tables which basically have two
columns the first column corresponds to
a partition of the space of database
states and the second column is what
effect the execution of the transaction
has so if you consider this tuple it
says that overall ddb states in which X
plus y is less than 10 executing this
transaction would have the effect of
incrementing X by 1 and similarly for
the other case now of course a
application would have multiple
transactions not just one transaction so
let let's add another transaction to the
mix it's very similar to the first
transaction except that now instead of
writing to X it is actually writing to Y
and also I have changed the threshold
from 10 to 20 and here you can see that
that's basically the symbolic table for
transaction t2 now if these are the only
two transactions which are executed in
the application i can combine them to
construct a joint symbolic table and i
do so by taking a cross product now in
normal cases the cross product would
have four tuples one of them is
degenerate and therefore have eliminated
it and therefore it has three tuples
what does this say it basically says
that overall database states over which
x plus y is less than 10 executing
transaction one has the effect of
incrementing X by one executing
transaction t2 has the effect of
incrementing why buy one
so I basically didn't explain how we
construct the symbolic table from this
transaction so let me show you how we do
that and again we have a set of
inductive rules for constructing the
symbolic tables from the transaction
code I do not expect you to pass through
them instead let us look at an example
construction so this is again a control
flow graph for the transaction which I
showed earlier and we construct the
symbol table symbolic table in a
bottom-up manner so we start with the
last statement in this case it is a
right in which case executing only this
statement over all database states would
lead would produce they would have the
effect of assigning the value of the
local variable X hat to X and that's why
the true indicates you know that it will
have the same effect overall database
states and as you work your way backward
in this case you see that well it is
going to have the effect of incrementing
along this branch it will have the
effect of decrementing along this branch
when you see a if statement you see that
in order to take this path in the code x
+ y must be greater than 10 to take the
other path it has to be less than 10
when you see a read statement then you
basically remove the local variables and
substitute it with the corresponding
database variables you do the same thing
for a read X and finally you end up with
the symbolic table and this is exactly
the symbolic table which are shown you
earlier now the keep up key thing to
note here is that the symbolic table
only uses variables which are in the
database and does not have any
references to local variables because we
have already substituted these local
variables with their corresponding so
when when the word read right
so now that we have constructed these
symbolic tables let us see how we can
use these symbolic tables to construct a
protocol so again the input to the
second step is the output of the first
step in this case this joint symbolic
table and let us assume for simplicity
that we are in a distributed case in
which one of the sides has the variable
X the other side has a variable Y and
the initial states is 12 and 13 so what
the homeostasis protocol does is it
checks to which equivalence class does
my current state of the database belongs
to so in this case the values of X&amp;amp;Y
being 12 and 13 indicate that it belongs
to the third equivalence class and it is
going to use that to be a global treaty
let us assume that this there's an
efficient way of actually maintaining
this global treaty without requiring
communication and I'll come back to that
in the next step so now when I execute a
transaction what I do is basically I go
and look up what effect that transaction
t1 has in this particular equivalence
class in this case it just determines
the value of x so I can keep on
executing these transactions as long as
the overall state satisfies this global
treaty so once I so finally I will reach
a stage where a transaction may actually
cause a violation of this global treaty
at which point I recheck and establish a
new global treaty and that begins a new
round of this homeostasis protocol so
what we have done is basically we have
executed six transactions in this case
and incur the cost of only two network
latency if you had done it in a strongly
consistent manner you would have entered
six network leads latencies now of
course how many network latency is you
actually incur will depend on how big
your equivalence classes yeah when you
are in the state why close
how you don't know what is the global
value of x right so how do you validate
the global treaty locally without
communicating right so that comes back
to the question of this magic which I
was am going to come to in the next
slide but before I do that so we have a
theorem which proves that the homie
status protocol actually produces you
see realizable schedules so of course
the naive approach to enforce this
global treaty is to require
communication it is to you know be aware
of the global state and that will
require communication and knowing the
values of both x and y at every step
which kind of defeats the whole purpose
because we'll be back in the world of
strong consistency we want a lazy
approach and to do this we basically
project this global treaty into a set of
locally and forcible treaties and of
course because we are projecting it into
this locally enforceable treaties and
this locally and forcible treaties are
working on a limited state they have to
be more conservative but we require that
these locally enforceable treaties wood
together imply the global treaty so in
this case to enforce that X plus y is
greater than equal to 21 possible set of
local GTS would be x is greater than 10
and y is greater than 10 so as opposed
to now enforcing the global treaty I am
now going to enforce this local treaties
so I keep on executing transactions
until I run into a treaty violation now
given that these local treaties have to
be more conservative this violation is
going to occur up earlier than in the
previous case which I showed you at
which point you renegotiate and
establish a new set of treaties and the
protocol goes on so of course there are
multiple possible ways of projecting a
global treaty into a set of local
treaties and if you assume that you know
something about the workload that is you
assume that you know you know that
transaction t1 is more frequent than the
other transaction and then you can find
an optimal projection projections which
are least likely to be violated
so in this case this was the sub optimal
solution of choosing 10 and 10 which
only allowed to execute allowed you to
execute for transactions as it turns out
this for this particular sequence of
transactions the optimal projection is
to have x is greater than equal to 9 and
y is greater than equal to 11 this will
allow you to execute six transactions
without a violation yeah the rights
except side is idea or something wrong
if you are writing X my focus
in this example you're writing X in one
side and why the other side yeah so you
can kind of look around it right put the
right sides intersect at Casa phone it
does cause a problem yes um so in which
case you would actually be back in the
world of strong consistency and there is
nothing which you can do now of course
and it replicated scenario all the state
is available locally but the same
problem would show up in here let's take
it what do you know so two tables yeah
not they replicated you can't actually
increment the houghton either place your
district all the rights to one place for
the region internet no no I can make
rights to both places so in the indie in
the example which I should i was
actually you know casting both
Republican and Democrat votes at both
the data centers to some subset of wars
got incremented near some subsequently
we just know the result you one of the
actual diagnosis no I yeah but that
that's kind of the whole point that the
application doesn't really need to know
the exact tally at some point when you
synchronize you are going to know that
Ali right you are going to merge these
two states so it's not that the state is
going to be you know always divergent it
is going to reconcile periodically at
synchronization points
no okay um without knowing the work load
of course you don't really know what the
optimal global treaty should be right
yeah you can have all the workload on
one side or the other and all the
updates to all we do to wanted a
variable that you wouldn't your global
treaty wouldn't be able to adjust I
money for that right so in which case
you can have something which is similar
to the idea of you know doing
dynamically estimating what the workload
is that is that is if you know during
the day some items are ordered more
frequently in the west coast sigh in
America than in down on the other side
of the globe then you would allocate
more budget to the data center in the US
um
so putting all of this together we
basically developed this system called
homeostasis and it has a number of
components and let me just briefly walk
you through it so we assume that we are
given a set of transactions which have
to be run and then we use a compiler to
construct these giant symbolic tables of
course we do not actually construct a
single large joint symbolic table for
the entire set of transactions we in
fact use techniques from the sdd one
paper which partitions it's actually
fells work which partitions these sets
this an entire set of transactions into
groups of interdependent transaction
based on conflict graph analysis and we
construct a joint symbolic table for
each such interdependent group of
transactions we maintain a treaty for
each such group so whenever a
transaction is executed the treaty
enforcer allows a local execution if the
local treaty is not violated if it is
violated then it goes and talks to then
initiates a round of negotiation the
treaty negotiation the treaty negotiator
goes and talks to the other replicas it
merges the changes which have happened
at the other replicas since the last
since the last synchronization based on
this new state new synchronized state of
the database it constructs a instance of
a satisfiability problem and the
solution to the satisfiability problem
is the optimal partitioning of the
global treating into local treaties and
then it sets a new treaty and that
starts a new round of this homeostasis
protocol so the overall takeaway is that
there are a class of applications which
have some flexibility in their
transactions we can exploit these
flexibility to lazily propagate rights
without sacrificing consistency and I
showed you a homie status which is a
system that identifies and exploits this
flexibility to minimize communication
between different no
in a distributed or a replicated system
so with that let me present you some
experimental results ah and as I had
pointed out earlier my experiments are
going to focus on homeostasis I'm going
to but I'm happy to talk about results
on quantum databases after the top so
the goal is to evaluate the
applicability of ah yet you had a
question we're talking about the
squirrel or feeding you have any
constraints on what kind of role video
you can support and are there any
guidelines as to once given the global
fede how can you translate into these
more these locally you gave an example
it was red um so the first question was
what are the constraints on the global
treaties we actually restricted the
language to the fragment which I should
just showed you and by analyzing those
transaction you can only get a certain
class of global treaties and precisely
that's going to be piano arithmetic
first-order logic now in general you
know solving satisfiability problems
over piano arithmetic first-order logic
is undecidable we use some tricks to
actually convert it into Pressburger
arithmetic first-order logic and that is
decidable as well as solvable in fact we
use XIII solver to do this uh which is
actually a microsoft research technology
is that answer your question
right so coming back to the experiments
are we want to evaluate the benefits of
homeostasis in a Geo replicated setting
and more precisely we want to answer the
question as to how often can actually be
avoid coordination for realistic
application workloads secondly we want
to study this trade-off between how much
time we are spending and finding this
optimal projection of global treaties
into local treaties and how does that
correlate to how much savings we get in
coordination so for work y we use a tpc
w by confirm like transactions so we
assume that there are 10,000 items in a
database we are assuming that each
transaction is purchasing one to four
pieces of particular item initially the
database is populated with stock levels
ranging from zero to hundred from each
item and based on the TP CW specs every
time the level actually goes to zero the
transaction automatically replenishes
the stock level by adding a hundred new
pieces of the item and we basically run
our experiment on ec2 we use m three
extra-large instances and the system was
deployed across five different data
centers Virginia Island Oregon's Apollo
and Singapore for the first two
experiments I'm just going to use two
replicas that is going to be verging on
an island and for the third experiment
I'm going to show a you know from two to
five at the behavior of the system as we
add sites sorry so let me explain what
this graph is so on the x-axis I have
the sequence of transactions issued for
one particular item in this case let us
say item a on the y-axis on this side I
have what is the view of the stock level
from the point of view of replica one
and on the y-axis on the other side I
have the transaction latency so the red
line here corresponds to the stock value
and the green line cord
response to the transaction latencies so
let us walk through this graph from left
to right so let's say that I will start
with this value of 100 for the red line
so now I'm executing transactions
locally using the home a status protocol
and that manifests itself enter in the
Indies low latencies because the
transactions are executed locally at
this point I witness a local treaty
violation so I need to run a synchrotron
dove synchronization and that requires a
communication between the replicas which
is why there is a spike in this green
plot and of and also I witnessed a short
cliff in the red line which corresponds
to that and that is because I am now
synchronizing the state so this change
was the number of purchases which
happened at the other replicas while I
was running my transactions locally at
this point I have established a new set
of local treaties and the execution
continues locally again until I reach
zero at which at which point i replenish
my stock and at and the protocol
proceeds so how often do we benefit from
this to do this to understand this
basically this is a transaction latency
profile and the x axis I have Layton
sees in the log scale on the y-axis I
have the cumulative probability that is
what fraction of the transactions are
executing under a particular latency
value so we are comparing against to PC
which is strongly consistent and it
always takes a round trip hit in this
case 200 milliseconds and of course
there's a sharp cliff because after 200
milliseconds all transactions will will
be able to execute however and these
four lines basically correspond to
different settings of the optimization
parameter so the higher value of L means
that you are spending
more time and finding optimal treaties
and therefore you expect more number of
transactions to execute locally so in
this case almost for all four of these
parameters you see that more than
eighty-five percent of transactions were
executed locally now how does the
behavior change as we increase the
number of sites so what exactly happens
when we increase the number of sites as
I pointed out earlier when you factorize
a global treaty into locally enforceable
treaties you need to be conservative so
there's local GTS have to be more and
more conservative and if you are
factorizing it into more number of
fragments then they have to be even more
conservative so as you go from two
replicas 25 replicas your local treaties
are going to be increasingly more
conservative and therefore more likely
to be violated easily and this manifests
itself in in this a downward shift of
the inflection point which basically see
is that slightly lesser number of
transactions are executed locally and
you witness the treaty violations more
frequently now the takeaway from this is
basically even with five sides more than
eighty percent of the transactions were
executed locally so with that let me
mention some of the related works there
has been quite a bit of interest in the
database community yeah I'm sorry so
good I probably have a backup slide on
that we did have an experiment on that
I'm happy to show that that to you
offline the inner cell communication do
is
so between so of course that depends on
which two data centers we are talking
about it ranges between 100 milliseconds
between East Coast and West Coast
actually around 85 milliseconds to more
than 250 milliseconds between Virginia
and Singapore caption so when you're in
the 10 millisecond range everything is
local yeah so it's and then you have
this
Chief Justice and as as soon as the
communication as soon as you start
renegotiating then into cross datacenter
communication yes that's what yes
you know um well that's when you get
this jump and the x-axis yeah yeah so
what happens so you're going to actually
do better you can do some you can use
some anti entropy protocol which runs in
the background and periodically
reconciles the state between the two so
that you can eliminate some of some of
these local treaty violations however
you cannot eliminate them completely
because you know whenever your
transition across an equivalence class
boundary that has to happen consistently
yes so all you're really doing is you're
just looking for better consistency of
reeds so what's going on just trying to
fit because if the updates commute then
you can do multi-master replication with
impunity and you don't need to worry
about cross
database delays
as long as the updates eventually reach
their destination or show however I
showed you in the pudding example where
yeah so yeah you were right earlier when
you said that we are doing read as we
want to mention and Shari transistor yes
well because his reordering transaction
sort of requires read consistency at
least at that point right but you can do
it incremental you could have saved
yourself a lot of renegotiating of
treaties by by simply incremental they
sending sending the updates from one
side to the other sort of all plot if
you will or get background so in in this
particular case you would you would
actually not witness this this
particular latency you don't do this
it's across the back and they if you
were if you were willing to soften your
requirements about updating so that you
had to had reordering by saying any time
anything less than 10 you'll be order
you can also soften despite that
so going back to the related work that's
been quite a bit of interest in this
field in general but we are the first
ones who actually adapt the consistency
which the data store provides to what is
required by the application and we do so
by doing this program analysis whatever
we are also the first one who tried to
adapt based on the workload which the
transaction based on the transaction
workload which none of these other
protocols do whatever we these other
protocols always assume that you are
given with a simple constraint which is
like an equally inequality constraint
and you want to maintain that constraint
our protocol generalizes what this class
of constraints is as well as allows you
to switch from one constraint to another
only that has to happen consistently
there's also been some work in the
programming languages community for
program analysis and automatically
identifying atomic sections and also in
systems community to assert a formula in
a distributed manner so with that let me
summarize ah so the key idea is that we
want to exploit flexibility in
transactions and I have identified
classes of transactions where such
flexibility is available to be lazy when
possible and I showed you one example in
which this laziness created room for
optimization and I presented quantum
databases as a system which does that I
have showed you another instance where
this laziness minimizes the amount of
coordination required without
sacrificing consistency and I presented
from your status which provides such
semantics based adaptive consistency so
let me mention something about what I
plan to do if I get an opportunity here
at Microsoft Research one of the
interesting things which I want to
pursue is can we synthesize concurrency
control
protocols automatically now assume that
you're given a correctness criteria in
terms of let's say one copy serialize
ability or few serialize ability and you
are given some information about the
environment as in what kind of hardware
support you have what is efficient what
is not efficient some specification of
the environment then can you automat can
we automatically synthesize the best
concurrency control protocol and that's
been quite a bit of recent very
interesting work in programming in
program synthesis some of it from
microsoft research itself and it would
be really interesting to investigate how
we can apply some of those techniques to
automatically synthesizing concurrency
control protocols the other direction of
a research which would be interesting to
pursue is a you know no knob cloud
services now as computing moves to the
cloud managing cloud services by
administrators becomes increasingly more
and more difficult so you would want to
have a system in which the clouds some
somehow automatically detects
performance anomalies or any other sort
of anomalies and takes actions to it
takes actions itself as far as possible
to remove any kind of anomalies and the
first step in this is of course
diagnostics and we have done some
initial work with question on this of
course the interesting question is once
you have identified what these anomalies
are with some high-level idea of what
the reason is can you close the loop and
automatically improve the the
automatically take actions which improve
the performance of the cloud service so
i talked about homeostasis and quantum
databases today I have also worked I
have also done something you should work
on the Utopia project which is about
designing declarative abstractions for
data
coordination you may have heard of
entangled queries and transactions so
the key idea there is basically with
with the rise of social networking you
would want users would actually want to
issue transactions which can now talk to
each other and take joint decisions and
we designed abstractions which allow you
to do this in a clean and efficient
manner finally I have done three
internships one with question in which
we initiated this new project for robust
diagnostic for cloud platforms I have
done to other internships one and
actually both of them in Google research
in the Fusion Tables team for the first
i've worked on spatial query processing
and the Fusion Tables back end in fact
if you you if you have used fusion
tables are you used it if you use it
today it's very likely that my code is
executed and the back end and in the
next internship I worked on faceted
navigation for data exploration and I'm
happy to talk about any of these
projects and the one-on-one meetings
which I have so with that that concludes
my talk thanks a lot for attending I'm
happy to answer any other questions
which you may have
hey do more questions
alright let's select the speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>