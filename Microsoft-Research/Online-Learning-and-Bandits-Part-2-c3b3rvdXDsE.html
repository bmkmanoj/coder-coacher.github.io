<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Online Learning and Bandits - Part 2 | Coder Coacher - Coaching Coders</title><meta content="Online Learning and Bandits - Part 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Online Learning and Bandits - Part 2</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/c3b3rvdXDsE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so let's start off where we let us pick
up the thread where we left off so last
time we were talking about the case of
online learning with with so-called full
information and we came up with these
natural kind of exponential weights
penalization algorithms to basically try
and select actions so as to minimize it
regret okay so we saw what regret was
and so on so now we'll come to a
slightly not a slightly actually
fundamentally more challenging problem
which is how to try and optimize your
choices when you have much lesser
information that you can get okay so
you'll see the difference very soon okay
okay so let's consider a simple game
where you are trying to basically
compete within experts or in action so
think of so think of n action so think
of the experts as just an action that
you can choose at one time right so it
every time you have a choice between an
alternative so if you have to play one
among several actions right so each time
so you start at time equal to one in
discrete steps okay the environment
essentially pics essential each action
suffers a certain loss in a given period
of time ok so the loss suffered by
action I at time p is a number scalar
number li d you can think of this as a
loss that each action suffers by
predicting i may just a number ok so
these losses are set by the environment
for all the all the action is possible
at a given instant of time they're not
revealed to the forecasted then the
forecaster basically has to pick an
action I of T ok which is one among the
N actions you have to basically decide
as an agent which action to play at time
T ok and the action that you play
essentially you will get the loss which
corresponding to that action ok at that
time you will suffer that much amount of
loss in that time step and then you
basically get to see essentially you get
to see so since you suffer that loss you
are allowed to see how much loss you
suffered ok so suppose there are n
actions right at each point in time and
at time equal to one you chose action
number five wait to play right so will
essentially get the fifth component of
the vector that is the loss of all the
actions at time one ok so this is
crucial this makes the problem very
challenging because
if you recall in the full information
set up we could in fact compute the
losses that every expert or action
suffered in in hindsight right after
looking at the outcome right so here you
are not looking really at the outcome
which help you to determine all the
losses or the exact performance of all
the experts in that round but you only
get to see exactly what you pick okay
and this is important because it
connects to a lot of problems in online
let's say recommendation system so
suppose you are a user that is browsing
on a website right the recommendation
system let's say has the capacity to
show you one advertisement right and
there are certain types of
advertisements that you can show you
think of all this as actions right so
when it shows you a particular action
you basically decide to click on it or
not okay you click on advertisement or
not right indicating sort of you are
utility for that i wrote Iseman right
and the feedback that the system gets
its is only about the ad that you showed
it right you will not be able to observe
consequences of actions that you did not
pick okay in hindsight there is no way
you can you can essentially find out how
the relative work of every action in a
given time slot in a single timeslot
unless I mean so unless you actually
explore across actions across time slots
okay so there is an element of
exploration that is necessary here okay
so if you got to see basically the
losses L I T for every I at time T after
you made a decision then that would
correspond to the full information
setting okay so just to take an example
sample path right so at time 1 let us
say you just decide to pick go with
action number one okay right so you know
that a certain vector of losses was
chosen right in by by nature but you
only get to basically see what loss you
suffered right so time goes along the
horizontal axis and essentially just get
to observe the coordinate of essentially
this vector that you chose at a given
point in time so at time to let's say
you decide to pick action too you only
get to observe this component of the
vector okay at time three let's say you
again want to play action one you get
only you suffer this much loss okay so
this is exactly the model the feedback
that you get that you are allowed to get
in this
winchell kind of game it is so this kind
of partial information about the vector
is called bandit information right and
you can still think you can still talk
about minimizing and regret so the
regret here is similar to what we saw
earlier so consider any possible setting
of essentially all these losses right
consider any possible way to set all
these losses ahead of time and with this
exact manner of being of revealing the
losses to the agent right corresponding
to any such allocation we want to try
and compare the total loss earned by
your algorithm versus the minimum loss
that any particular one action across
the entire interval time suffers okay so
what you want to in some sense is you
want to compare the sum of all these
numbers which is exactly what you are
algorithm accumulated versus basically
the row sums in this matrix ok so the
row sums essentially are sums of all the
losses for a given action across time
you will still not done them along yes
you'll only know one person so
essentially the state or the history
that you accumulate is only essentially
one coordinate per time you will not be
even able to see you okay but you still
can actually maybe in some sense hope to
compete with the best possible so what
would happen if basically only stuck
with action one or only stuck with
action too or only stuck with action p
for the entire division yes it's very
very close to setting orientation so all
that all that is required to sort of
turn it into something that looks
exactly like the enforce my clinic is
some assumption that these losses are
being generated according to some
probability probability distribution
okay and we will discuss this exactly
when we move on to the stochastic
setting of so there are problems called
stochastic bandits which essentially are
things that become reinforcement
learning problems sorry so it this is it
depends on what you mean by the word
stationary in this sense they can be any
arbitrary sequence I am not telling you
anything about how you can generate
these but you can definitely so I will
we will look at strategies that can
actually cope with any kind of setting
of these numbers okay so I should
probably make one thing clear is that in
all these discussions you should just
imagine that this matrix of losses is
set ahead in time so one way to think
about it if you so nature basically
writes down all these numbers ahead of
time and then reveals it to you
incrementally depending on what you ask
like or what you play right there are
other possible models of interaction
where the environment can actually react
to what you do okay so that is called an
adversary of a different kind okay it's
an adversary which is reactive as
opposed to an adversarial setting of
losses here that is static ok so this
allocation of losses a priori doesn't
really depend on what the algorithm asks
for but you can also imagine settings
where the adversary
it actually conspired to sort of give
you the worst possible time right during
your operation so those are things that
we will ignore for the time week so it's
just it just helps to think of any
possible setting of these losses fixed
before
this one ah so it's a yeah so it's a
sort of its AIT's a it's a good question
but the answer is not a trivial I mean
so it is not possible to compare against
strategies that can pick essentially the
best element in each column okay so in
order for you to try and compete with
essentially picking the the the lowest
element in each column so ideally the
holy grail would be if you were somehow
able to predict the minimum element in
each column in this play reduction but
that's not really possible you can show
that it's impossible given the
information limitation that you have you
cannot observe an element you cannot
observe any vector beforehand right so
that is basically impossible so you have
to compare with a reasonable notion of
performance which is either comparing
the best possible individual row of
losses suffered by each individual
action or in some cases you can even
enlarge the set of strategies that you
can compare with by let's say comparing
against any strategy that that can
switch between actions a finite number
of times a small number of times okay
but you can't allow arbitrary switches
okay then the problem this become you
get regret that is always linear okay
you won't be able to really learn
anything okay so that's a limitation of
course of the online information model
all right so so this this the outside
supremum is just saying that you
consider the maximum possible value of
this term over any setting of these
coordinates of all these vectors right
and this is the loss accumulated by our
algorithm and this is the least possible
loss I killed by any single arm in
hindsight so this is a notion of loss
this possible loss in hindsight wait
when you play the whole game look back
at the past and then figure out which
possible arm in hindsight had the best
possible loss and you want to be as
close to this as possible right so if
you are able to minimize this you are in
some sense learning something implicitly
you are actually trying to learn to
stick to the best action where the best
action is defined in this is right so
whatever what is what is hard about this
game right so of course you can get a
little bit of thought tendency are
that if you have a deterministic
algorithm to play this game it without
any form of randomization across actions
then no algorithm can get a regret that
is better than sublinear better than
constant times time because you will
always you can always construct bad
sequences of loss vectors such that if
you have a deterministic algorithm let's
say that the algorithm starts by picking
one at time one you can always put the
put a very high loss can always
construct a sequence that has very loss
hi Lawson on time one and then once you
fix that action then you know what you
know exactly what what action the
algorithm is going to ask for in the
next time then you can again go and
design a really bad sequence for which
the algorithm software is a large loss
and so on okay so you can always without
randomization you can't hope to you know
reduce your regret so in some sense you
need you need to use randomization
linear algorithms fundamentally and
inherently in order to somehow induce
exploration or transactions in the hope
of trying to reduce a regret what keep
it low right so one more issue of course
is the fundamental issue of a lack of
information right so in the in the full
information setting you could
essentially absorb the losses that all
the experts are all the action suffered
right so in some sense the amount of
information that is being revealed to
you is of size n you observe n bits of
information one one for each action you
can object the loss of every action
whereas here you can only observe one
unit of information at this for the
action the deepest right so there is a
fundamental difficulty of you know your
your rate at which you can gain
information is very limited it so would
this how how does this impact how fast
you can learn or how fast you can
minimize the grid it's not here so you
can directly run an algorithm like so
recall the exponential weight scheme
right you basically at each instant of
time you know how bad or how good each
expert or each action did you can just
go and penalize every expert or action
by the last edit incurred right and you
can just carry on this game but here you
don't even know what losses other
experts suffer tonight so you there's no
explicit way to carry this forward so
this is where some nice pics come into
the picture so I don't know how many of
you were there in Sanjay's talk but
if you are allowed only a single
coordinate access to a vector you can
still build an unbiased estimate of the
entire vector okay so I tell you that
there is a vector with ten coordinates
and you can only sample one coordinate
of the vector you can still in a
randomized fashion give me an estimate
for the entire vector which actually is
going to have as expectation the same
the actual vector right so does anyone
recall how to do this
right so here is okay so suppose you
could do this right so suppose there is
some way to build a blackboard to build
some scheme that given access to only
one coordinate of the entire Los vector
of all the losses of the experts helps
you to build some kind of reasonable
unbiased estimate of the entire vector
right so once you have an unbiased
estimate it is at least a reasonable
estimate of the entire Los vector then
you can just pretend that you are in the
full information setting right and you
can just feel all these estimated losses
to an algorithm like exponential weights
which penalizes these experts by those
appropriately estimated losses right
right so before presenting the algorithm
so the idea behind trying to build in
the idea behind how you can build an
estimate of the entire Ector it as
follows right so suppose you have ten
coordinates to sample I love you would
only sample one coordinate right just
start with let's say your favorite
distribution x8 so uniform distribution
across the ten coordinates right sample
any one of the coordinates randomly with
probability 1 by 10 and when you get the
value of that coordinate just / just
multiply that by 10 okay and the
estimate of the vector that return is 0
in all the other coordinates where you
didn't sample and 10 times the value
that is that you that you sampled in the
coordinate that you decided okay is it
is it clear that this actually gives you
an unbiased estimate of the entire
vector right because if you look at the
expected value of a particular
coordinate that particular coordinate
will be sampled with probability 1 by 10
right so with probability 9 x 10 it will
have a value of zero and with
probability 1 by 10 it will have a value
of 10 times the real value right so the
expected value still turns out to be the
actual value of the coordinate right so
it's just a simple trick to try and
estimate an entire vector given only
single sample axis and this idea was
crucially used to essentially give a
complete solution to this the setting of
regret minimization the Bandit setting
with this algorithm which is based off
of the exponential weights algorithm
it's called exp three so XP 3 is
basically x33 exp so X exponential rates
with exploration and exploitation
so 3x so so the the intuition behind
this as follows we still have the same
learning rate parameter ETA at the
beginning you essentially initialize a
uniform probability for essentially you
have uniform probability distribution
for playing across all actions okay this
is the same as setting waits for all the
actions and the normalizing and building
a probability distribution right and
each time you randomly you randomly
sample an action to play according to
the current probability distribution
that you have which in the beginning is
uniform right so I T is the current
action at time T which is sampled
uniformly according to the current
distribution across actions the morning
do you play an action you basically
receive you get to see its loss that is
the loss at coordinate i T at time T and
then you just set the loss estimates for
all the other actions as zero and for
the current action that you sample they
essentially set it to be the loss that
you saw divided by the probability with
which you actually sample the action ok
so in the simple case I explained you
could start up the uniform distribution
and sort of divided by 1 by 10 right in
for 10 coordinates but if it's any other
distribution that you used and got a
sample you have to divide by essentially
the probability with which you sample
that particular part right so that will
still make it into an unbiased estimate
of the entire wit ok is it here and once
you have this estimated loss which I
denote by L tilde right so this the
algorithms estimate for the losses of
all the actions at time T you can just
go and update the probabilities by
hitting them with e to the minus ETA
times the estimated losses because that
is exactly what you have access to ok
and this is the same as this is same as
the exponential bits update only with
estimated losses into real losses ok
that year turns out this is really this
is something that actually works you can
in fact show that it gets you a very
similar regret guarantees like what you
saw earlier so there is a log in n is
number of factions which was the number
of experts earlier so login / data plus
a tie n over 2 okay so the only
difference here being that there is an
extra in term here okay so there is an
extra number of experts term sitting
here earlier there was only a tee time
theta okay in the full information
sitting so in some sense you are already
seeing the price of
pandit information coming into the
picture right so you can't so there's a
blow-up of sort of n in here okay i mean
if you actually optimize for the best
possible value of ETA so i should just
say that the proof of how you show this
is again very related to how you prove
the regret for the full information case
for the financial bates algorithm along
in the fact that you are using sort of
now inaccurate or estimated values of
these losses but there is a nice way to
sort of control the variability of these
estimates ok so it's fundamentally the
same idea just require a little more
careful accounting so if you doin this
ETA optimally ahead of time you can get
something like square root tea and login
so this is where this extra n term comes
here with full information you would
just have square root t log in right
over to or something if you recall so is
really the price of mandate information
there is an extra square root n term
which is of the order of the number of
actions so intuitively you are observing
much less information per per round so
that has to translate into sort of a
slower rate of learning ok but it's
still nice because it's going only a
square root and then the length of the
time horizon right so this is still a
great given that you only had individual
coordinate axis you're in the bandage
reading okay so if you just say i
compare this with the full information
setting regret you actually suffer an
extra factor of square rooting so which
is typically the the price of being able
to get only bandit information right so
i guess i will so for the non stochastic
setting i will stop here there are some
more items that i would have liked to
cover but maybe I they'll show up again
when we discovered discuss the story in
the stochastic bandits world okay so I
leave that to the next set of slides
okay so this is basically a from a
separate the second lecture essentially
on stochastic mandates with how can you
learn with stochastic or distributional
assumptions and you will see similar
kind of band
problems okay so online learning with in
a stochastic world so to speak right so
again what's a mighty unbanded okay this
is not a multi am back okay it is also a
mighty a minor but not the one that we
are interested in so just to give you
some bit of background I mean the term
you might have wondered why the Time
Bandit is used to define problems where
you can access any sort of you can only
observe consequences for actions that
you take right it turns out that the
historical reason for this is so bandit
is actually a special type of gambling
machine in a casino so long ago they
used to have the slot machines in
casinos which were actually called
bandits the reason it is called the
Bandit is basically it essentially eats
up your money okay if you they rigged in
such a way that on an average will
essentially lose money okay but assuming
that you have a bunch of in slot
machines in a casino each of them could
essentially have a different probability
of you winning at the game right it's in
some sense a randomized machine and
let's say each of these machines gives
you returns at different at different
rates which you don't know about a
priori right so the game is essentially
how would you try and play these arms
the slot machines to try and maximize
your reward okay in that is this is
essentially the motivation for studying
other problems which have similar
objectives right and then you can call
out them as derivatives of bandit all
right so let's consider a stochastic
version of the multi-armed bandit game
you have n actions right think of these
are the same n actions that we saw you
okay this can be arms of a slot machine
of diff of every every action can be an
individual slot machine which you pull
and then you get some toward with some
probability right they could be for
instance in online advertising you could
have each arm being let's say so let's
say you are a noose advertiser you have
one slot worth of news content that you
can serve up on a webpage and you have
different categories of ads sorry
different categories of news articles
that you can sample from right so let's
say action one is let's say sports news
action to is entertainment news action 3
is movie innocent alright and exhale
you're interacting with a particular
user and you don't know essentially what
the users preference is for various
categories of news articles a priori so
you
not essentially try and discover
interactively which kind of categories
work best with given user right so
whenever you select and place an item
from a particular news category and give
it to the user the user has some
intrinsic preference and the user will
essentially either read the article with
some probability or discard the article
right so it is a probabilistic model
okay for generating rewards our
operation so think of n actions in an
abstract sense right they could be ads
to show news articles they could be
frequencies to transmit in our
communication system or so great so each
arm is associated with an unknown
probability distribution right will
represent the probability distribution
by theta with some associated mean okay
so think of HMS as giving you a reward
distributed according to some
distribution and with some average or
expected report right so for the whole
talk it suffices to think of each arm
being just a binary reward a binary
random variable with some probability of
success ok yes we just think of each
reward distribution is being a bundle
its vision which has a simple single
parameter right so at time 1 let's say
you at every time you have to pick one
action so let us say a time one you pick
action too it gives you some reward at
time one which is distributed according
to the distribution for that particular
action ok the probability distribution
from it just randomly and independently
generated and given to you so each time
you pick one action and you accumulate
some reward whenever you pick whatever
the associated action and you play this
for some time you play this let's say
for capital T number of rounds as before
ok and it could be interested in several
things so if you are a gambler in a
casino you could just be purely
interested in the maximum reward that
you make right so since the rewards are
being randomly generated I could think
of trying to collect as much reward an
average as possible right in given an
interval of tea please of this game
right this is exactly equivalent to
trying to minimize an ocean of regret
right so in some sense the best thing to
do here if you knew an entire set of
probability distributions would just be
keep playing the arm that gives you the
highest average to work right that is
the best thing you can do on an average
right and so if you denote that
particular best possible average / mu
max right so it's the best possible Mui
overall the arms I some some omniscient
agent that always puts the term will
accumulate T times mu max averaged word
in time T whereas you will accumulate
something in painting when you want to
try and see if you can reduce this
difference it's how well you can keep
this difference one right now this the
great notion is different from what we
saw in the lawn stochastic setting
because now you have a set of
distributions that parameterize the
system so you can talk of something
called the best um which doesn't change
with time okay so that's an important
distinction you could be interested in
something different slightly different
so for instance you could keep playing
arms or apt afflictions adaptively and
at the end of the end of the entire
horizon you could probably be asking the
question which arm do I think is the
best right so I just have to guess one
of these arms at the end of the entire
procedure and suppose I guess I guess
that arm number 80 is the best arm at
the end of Tiryns I want to try and
minimize the probability that i may can
make a mistake in that guests okay so
you just asked for one guess at the end
of the horizon do whatever you want
during the horizon and I just want to
find out which which which which ever is
the best possible action so this is
relevant in settings where you have some
budget of time slots to experiment on
the system so this even occurs in online
advertising settings where some initial
sort of exploratory time that you can
spend in probing for options and at the
end of the day I sorta at the end of the
time horizon you basically decide which
the best option and then in the future
you just stick to it okay slightly
different than trying to maximize your
reward or minimizing is it for is a clip
there could be other more complicated
performance objectives from this
procedure you might not be for you might
for instance not just be interested in
maximizing your expected reward but you
might also be interested in maximizing
your expected reward subject to
controlling the variation in
in whatever rewards you get right so is
suppose you are an investor in the stock
market you would be sensitive to some
amount of risk so you want to try and
maximize the average report while still
keeping the risk or the or the or the
variance of the rewards look okay so you
want to try and get higher expected
reward but at the same time you don't
want to also increase your variance that
you know reward at some time will
drastically dip ok so these problems can
also be formulated using various
performance measures but I get to keep
things consistent we will again try and
consider the problem of regret
minimization in the stochastic bandit
city okay or its analog it's it's
equivalent to basically maximizing the
total drove our technical expert repair
work okay so that again so regret in in
that sense has a nice motivation to just
being equivalent to maximizing your
total return that you connect over the
entire horizon of time right so a bunch
of areas essentially so this is so the
stochastic multi a minute essentially is
a problem of resolving the trade-off
between exploration and exploitation
right so if you don't explore enough
across all the options that you have you
might try to discover an option that
performs well because you have only
information about the action that you
choose unless you explore you will not
find out how other alternatives look
right whereas if you explore too much
you will not be good at exploiting the
information that you have if you explore
too much you might explore across bad
options a lot and that meant essentially
hurt you in the sense of collecting a
lot so there is an inherent rid of it
it's a trade-off between collecting
information and also optimizing
simultaneously so which is why it shows
up in a lot of domains clinical trials
and gambling were sort of old or
classical applications nowadays have lot
of problems in the online advertising or
use news news placement domains which
fit very well as multi um bandits and it
has a sort of broad connections too
noisy function optimizations on maybe
I'll talk about talk about it a bit
later right so recommender system you've
already seen this so common scoring so
I believe reddit.com actually tries and
uses bandit algorithms to try and score
comments so if you've gone to read it
it's basically a discussion side where
people post something and then other
people keep posting comments perhaps
like you've seen on facebook and if you
actually go to a page it has the option
of trying to decide which options which
comments are likely to be most relevant
to use it so it can actually change the
order of comments that have shown to you
hoping that you would like or read some
of those comments so ordering in the
comments is dynamically decided and you
can in fact use variants of multi
embedded algorithms to to formalize this
problem so the arms in this case so if
you if you just assume sort of let's say
only one slot you can only show one
comment then it's a simple multi-armed
bandit where the set of comments
available is there is the set of actions
available like if you think of more
complicated allocation settings like you
can show let's say a bunch of ads or a
bunch of comments together then you can
model it either as each slot beam sort
of separate independent bandit or you
can model essentially a searching
through the space of every decision will
become essentially a ranked order of
some number of documents or or or
comments so these things become a little
complicated but you can still think of
them as bandits because after all a
bandit essentially requires you to think
of several alternatives and response to
each alternative so the set of the space
of alternatives can be fairly
complicated like the set of all rankings
or set of all orderings on a set of
solve it yeah what we are talking about
is a simple version of the multi-armed
bandit where you can play one action a
time and then observe a response for it
right so there is this inherent explore
versus exploit trade-off here and with
some thought you can easily convince
yourself that being greedy is absolutely
the worst thing to do okay so you could
say look I just start playing the game
randomly and sooner or later I will just
try and stick to the I'll just maintain
a running average of all the rewards
that I get from each individually and
I'll just complete a running average
right so I want to try and maximize the
expected reward that I get from an armed
attack cool so I just maintain
i estimate of its reward right so you
know that the sample mean is a good
approximation to the actual expectation
so i'll just maintain a running estimate
and this after a while i will just stick
to the arm that looks the best in that
sense right it's a reasonable we had to
start thinking about this but some of us
but this this does not work at all in
the sense of trying to minimize the grid
so why is this so just consider a simple
bandit problem with only two actions
okay so they are both Bernoulli
distributions one has success rates
probability of point for the other one
has a success probability of point right
so obviously wanted try and play the
armed with point for much more often
ideally you would like to play it always
right but you have to learn about
doesn't come for free so let's say they
start arbitrarily at time one you play
arm number one if you're unlucky and you
get reward 04 right it's just a random
out complaint let's say a time to you
play let's say I decide to play I both
arms in sequence for the first two time
slots and then stick to the which gives
you the best reward you can do this for
even some finite number of time that's
okay so if you play am to let's say you
get a reward of 1 your your again sort
of your super lucky there and you get to
the word of one okay so after you
essentially your view of the magnet is
biased because you've collected some
samples and you're in a sort of some
particular unlucky situation and this
happens with a decent probability this
the sequence of events can actually
happen with probability 0 point 6 here x
0 point 6 here x point2 rate which is
twelve percent so it twelve percent of
the time you are always going to get
stuck in this case and after this the
running mean of r 1 is 0 because you get
gotten only one sample 0 the running
mean of this arm is one so if you decide
to go with our number two you will
always play out number two upwards so
being greedy right from the start is is
bad okay so you'll at least get regret
that scales as you'll always keep
playing the bad i afterwards and will
accumulate regret at a constant rate
right
so why is this happening right this
happening because you are not aware I
mean you're not being cognizant about
random statistical fluctuations in your
reports you have to somehow account for
that otherwise you are going to get
stuck or if this is in some sense
analyzed to the phenomenon of
overfitting right so if you just place
too much trust in your data you might
over fit and sort of go with the wrong
size in this case it is really bad
because you actually end up you end up
getting very high regret whereas you
want to try and keep it sub linear so is
it even possible is the question right
so of course it is indeed possible based
on what we've seen earlier we can just
pretend that we can ignore the
stochastic part of this entire model and
pretend that this is a sequence of
losses generated in some arbitrary way
you can run the xp3 algorithm and so on
and you can of course get lead the
regret that is something like square
root tea or something right so we know
that these things are possible but is
there a way to exploit much more this
probability probably stick structure in
the rewards and do much better so this
leads to yeah so there's been a long
line of work studying this multi
unbounded problem the stochastic multi a
month on there are several strategies
that attempt to get around this problem
of overfitting and getting into bad
spots by some clever way of exploring
across amps okay you can find these
references maintenance but probably the
most classic and well known algorithm
simple algorithm for playing the Bandit
and giving an almost optimal rate of the
grid is this very well known algorithm
called the upper confidence monitor
right so it's also popularly called UCB
so it came out came around as late as
2002 but it is really something that is
simple yet basically does the job for
you so what's the idea behind this okay
so remember that we got into a back spot
earlier because we essentially were not
cognizant about the fluctuation of the
estimates that we had right so in this
case we need to atone for the fact that
you've actually sample arm
one and two basically very few times is
likely to be a high variation in these
divorce right so how do you reason about
these things okay so in a basic sense
let's say you toss a coin okay so you
have a coin with the unknown probability
of heads and you're trying to learn
about you try to estimate the
probability that it is heads okay so
suppose you toss it some k number of
times right and you write down all the
rewards right you get one in the coin is
heads and 0 in the coin steals and you
get head seventy-five percent of the
time okay so what will be an estimate
for the the bias of the coin it would be
point 75 naturally because it's an
unbiased estimate of your actual reward
but you also have to be aware of how
much confidence you can put in that time
it is time to five percent right so what
is the typical range in which can you
say something more about the range in
which the actual bias might lie around
the point safety purposes right so
anyone know what's the rough range if
you toss k times with what sort of what
interval can you put this true bias in
around 75 as a function of K right so
naturally if you if you take more and
more samples if K is really large you
will be very very confident about the
point seven five right you will see that
it doesn't do it cannot deviate by more
too much from point seven five no it's
not locked yeah so it's 1 by root K
exactly so this is because of I mean one
way to think about this is buy something
for the central limit theorem that all
of us have seen right so typically if I
toss a coin roughly k number of times
and I observe that the running mean is
something with reasonable confidence I
can actually tell you that it is going
to be at point seven five plus minus 1
over square root K okay the square root
turns out to be the right sort of
confidence interval that you can bracket
the actual answer your guess okay so if
you essentially guess that the true bias
is in the range of 0 point 7 5 minus K
20 point 75 sorry Oh points and 5 plus
minus 1 over square root K you would be
actually this guess would
claim would actually be correct with
very very high probability okay would be
this would actually be a very very
reasonable guess okay rather than just
saying it is exactly point something
okay so with this in mind what you see p
does is as follows okay so it uses these
this idea combined with another idea
called being optimistic under
uncertainty which is probably also a
general principle for most of us in life
okay so so you're working with these in
arms right at each time point in time
let's say you've played this game up to
some point in time and you have built
these running estimates for each the the
reward the average report of each action
okay so i call it calm you i hat so me
one hat me to hat you three had as the
running estimate of a particular arm Mui
I ok so what you do is essentially you
write down all these sample means okay
and don't just play greedily with
respect to that you try and add this
extra amount of bias which is something
like 1 over square root number of times
we've played them okay so you know that
each arms actual reward lies in an
interval of plus minus 1 over square
with the number of times you played it
around each corresponding point right so
what it is you we put all these
intervals you write down all these
intervals and just pretend that you have
the most optimistic setting right so
each arms expected reward is actually at
the highest possible plausible value
okay so you add this this bit okay so
forget about the two lot T it is a
technical term that helps to add just
the right amount of constant or
additional amount of confidence to to
make things work but essentially you add
and a small bias term of size 1 over
square root number of times you played
particular action to account the
uncertainty right so if you played a
particular arm a lot of times this bias
term that you add will be small right or
I mean arguably so hopefully so you will
not essentially end up distorting the to
estimate but you don't want to disturb
estimate when you play a large number of
times so it's it's all consistent with
reasoning right so when you have
uncertainty just try and be optimistic
assume that each arm has the best
will be and now you can try and play
greedily with respect to this this
select the best possible arm that
maximizes this sum of two terms which is
called a UCB index of the particular arm
so instead of maximizing instead of
playing according to highest possible
sample average you just play according
to highest possible you see okay at the
current point in time then you play an
arm you get its reward update all the UC
peas and so on right so this is a
trading process okay write a list so
this is just a small tweak you add an
artificial bias to account for the
variability so this is in this is in
fact very closely connected to the idea
of bias variance trade-off right so if
you don't add this there is too much
variance in your estimates and that
causes things to feel as we have seen
whereas if you add this control bias
then it helps to reduce the variance at
the at the right amount of exploration
so this gives essentially the
possibility to explore arms that have
not been exploding them but this right
amount of exploration so that regret is
controlled and and how much regret can
we get the in fact can show that UCD
gets expected reward of this much so if
you just subtract this term away the
regret is of order n log so this Delta
is basically a problem dependent
constant right it Delta essentially
represents the gap between the highest
possible reward and the second highest
possible in on average so Delta is
something called the gap but the
takeaway from here is that this
algorithm at she actually gets you
regret which is number of time number of
actions times lochte it's not even
square root ok so it's able to use or
exploit the statistical nature of the
rewards much better right
right so wine and it grows at a really
small rate with time which is
essentially logarithmic quiddity right
so it's almost like a constant if T is
large does not matter lottery is quite
small okay the only thing I mean which
is potentially worrisome is a dependence
on the number of actions but this cannot
be avoided in general you have to
actually explore some action every
action at least a few number of times
right so this is inevitable but this is
the best possible in terms of how you
can control this regret performance with
time with the total number of rounds
which is in this case is small T okay
right so perhaps i will show you in a
few lines why this why this works really
right so right so the UCP proof is
actually fairly simple to follow i'll
try and give you an approximate version
version with proof at least which has
all the ideas but some of the
technicalities may not be exactly
correct I'll point it out to you okay so
so so why should this so why should a
rule like this right so why should I go
like this try and reduce the regret okay
so let's start by so let's draw so you
have n arms okay let's assume that all
these arms have Bernoulli rewards right
so we are trying to build an estimate
for the Bernoulli parameter of each arm
which is the the main report okay so
this is our number one it's parameter
can be arranged anywhere from 0 to 1
okay this is our number two
okay so this is up one
right so why does addition of this kind
of term help you classically rayon is as
follows okay so these are all the arms
that you have without loss of generality
just assume that I'm one is really the
best possible am so right so its average
reward is the highest symbol right so
let me write down the average rewards as
in so I'm number one let's say as a
reward distribution that is Bernoulli
with parameter mu 1 okay so this is the
highest possible let's say i am number
two as something smaller see and so on
right let's say i am number n has the
smallest person right so in reality you
would like to actually play arm number
one all the time so let's see how you
see the tries and sort of figures out
how this happens and at the same time
controls the regret okay this is clear
this is sort of the so at each point in
time your your algorithm will basically
maintain a running average which can be
anywhere here to be a fluctuating
average rate because of statistical
fluctuations right so let let not those
a circle so this is mu 1 hat let's say
this is mu 2 hat and let's say this is
mu n hat right they can be anywhere but
we know that they're they're essentially
sample averages from the corresponding
distribution right so if you take large
enough samples from every arm these 22
will be closer okay and how close enough
exactly is the subject of will be
discussing okay so okay maybe I'll just
take this pace
right so let's try and let next time how
close mu 1 and mu 1 hat can be or new to
and me to head can be okay so i don't
know maybe some of you and see what
purushottam will also be talking about
this probably tomorrow but there is an
extra up trolling the deviations of
sample averages from there from their
averages this is called Holdings
inequality okay so what does Holdings
equality
right hope these inequality says that if
you have a sequence of independent and
identically distributed random variables
X 1 X 2 up to X n right so these are
independent each of these is an
independent sample coming from the same
distribution right you have n samples
let's say these are coin tosses right
and you know that let's say each X I is
bounded absolutely by one and you also
know that the expected value of all the
excise are zero ok think of this each X
I as the outcomes from a sample from a
coin toss minus its actual me ok so so
that each each random variable 0 on an
average in their independent samples
right then it just says that the
probability that if you form the sample
mean of all these right so what's the
sample mean it's 1 over N times the sum
right
right the probability that this will
actually exceed so idly what should this
be this will be close to zero because
the mean is really zero right so this
mean is zero as you take larger and
larger number of sample you should see
this sort of being close to the me right
so probability that this is something
which is larger than some number C
cannot be too large ok it cannot be to
that how large can that be it has to be
something like erased 2 minus n c square
by 2 okay so this gives you an estimate
for how far the sample mean can deviate
by some number C okay so see let's say
see the positive number okay so see if
she is really large then this number is
really small right so this essentially
controls the probability if things can
go wrong right to the probability by
which your sample average can be very
far from 0 0 X here as implicitly as a
minus you right is this care and maybe
you'll see this tomorrow in push or
thumbs down it's a very basic and very
powerful and useful inequality in
several of these analysis involving
sample averages and so so let's let's
assume that so right so this is mu1
right so mu 1 essentially recall is the
best the best possible have a expected
value of any arm and you have mu 1 hat
here right so you can use this kind of
thing to try and bond how far me one hat
can be below me one ok so this I should
mention that this also goes the other
way around also this is the deviation in
the positive side the deviation in the
negative side will also have the same
problem symmetric okay so let's train
bound the probability that mu and hat
and women are you one had dips very much
below me it cannot happen right too much
so let's bound this in terms of openings
in coordinated so
so in particular yeah let us mount the
probability that mu and hat falls below
right so I'll put this term here okay k
1 is the number of times i have played
the arm one up two up to certain time t
okay no I want to so in this for 4am
number one I am going to do the lower
side of deviation for all the other arms
which are bad i am going to the upper
side do you see why this is happening
right so you need I mean so so muon is
essentially really 1 over K times K 1
times the summation of samples k 1
samples rates of the reward of m1 right
and if you basically take the MU and
inside each sample you will see that
this is exactly like one of these X
months okay which has mean 0 it so think
of so this really has mean 0 and you
control the probability that I'm sorry
should be minus beliefs on ok let's
analyze that this is less than this
negative okay so i am putting a gap of
minus square root so this this gap is
basically minus square root 2 log T
divided by Q I am analyzing the
probability that mu and hat falls below
a one- this much gap okay at time T
right so this should be by holdings in
equality right so 2 times number of
samples collected which is in this case
k1 times the square of that number see
there right this is this basically the
c.c for us right so if you square this
sorry baidu right sorry not not two
times half times right and if I the
square this number here it's designed to
be such that
ok
so the probability that your mu and hat
actually dips below this level at time T
is no more than 1 over T this term oh
you'll see why this term is designed it
this way okay design this way such that
with probability 1 over T with
probability so with probability at least
1 minus 1 over T you are in good shape
that is mu 1 hat minus 1 is greater than
this right so if you take this to the
other side the UCB of m1 is higher than
v1 okay or rather I can just write it as
mu 1 hat is larger than mu 1 minus
square root 2 log P over Q right so with
a reasonable probability which is one
minus one over T let us come back to
this figure now okay so this is me one
with reasonable probability you are
above this line here okay this is
essentially square root 2 log T over k1
and let's say there is some other arm
you too did say right so mewtwo is lying
here somewhere right you can use the
same technique here to show that new to
a hat will not be more than some number
above me too okay so you can say that mu
two lies essentially in this range you
too hard right you know that mu 1 hat
lies in this range it cannot be too far
from me one on the negative side mu 2
hat cannot be too far from you two on
the positive side just by virtue of
those bounds and essentially you can
show that after a certain interval of
time after time becomes larger these two
segments will never intersect okay
because as time grows large enough if
you set the value of time to be large
enough you can show that these two
intervals never intersect that means you
will always essentially start using am
Number One right and that essentially
leads to this bound here
you will not make sort of mistakes more
than in Lochte hotel that's the
reasoning behind this right so this is a
very simple in some sense the multi am
the basic my team man it is rather
simple and stylized way to model an
explorer exploit trade off you can have
many more complicated reward or action
models here right so the standard
assumption implicitly being made in the
stochastic multi a magnet is that all
the arms have to be learnt individually
wait you cannot learn anything about a
particular arm unless you actually go
and sample it right so in some sense the
arms do not share any structure that
allows you to generalize across amps so
you cannot for instance hope to learn
something about a particular arm and use
that knowledge to try and infer
something about another rom so that he
can save some more samples right so but
but however this is something that crops
up in many sequential learning problems
there is much more structure or coupling
across the arm space the space of all
arms right so I'll give you an example
okay right so suppose you have a bunch
of arms bandit terms with this
particular concrete structure of robots
so each I'm actually you know that it is
associated with some parameter in D
dimensional space okay so each arm is
not just an arm but it is actually
associated you also have information
that this arm sits in some some part of
D dimensional space right so each
associated with each I'm a vector X I
which is known in advance okay think of
X is features okay so arms might
actually be advertisements or news
articles or news feeds having certain
features so suppose someone has already
learnt and given you features okay and
the reward that you get while playing if
you decide to play arm I is basically
its feature vector inner product with a
certain unknown vector that we will call
sign and some noise our attitude 80
members so this is actually a linear
it's saying that the reward from every
arm pits
model okay the parameter of which you
don't know in advance right so noise is
assumed to be lets say Gaussian noise a
small amount of 0 min noise but the key
thing you'll notice is that this
essentially couples all the arms rewards
together you cannot have you need not
actually go and actually learn each arms
reward individually it's enough if you
just somehow build a reasonable estimate
of sight right this vector side this in
some sense help you help to generalize
across arms even when you have a really
large number of apps because in some
sense what you implicitly need to learn
about the system is somehow this vector
set then you can actually apply it to
any possible even new arms that you get
in try and estimate have a good estimate
for the rewards okay so how do you try
and design more efficient or more faster
online learning algorithm that can
exploit such structure okay so for
instance is linear so learning this
linear structure has to deny the cross
arms okay so in that way you can
probably hope to figure out or reduce
your regret in a faster way is that
clear right so online learning with this
generalization structure is something
that is very actively studied area in
different researchers consider identify
often different kinds of structure in
these manned apollo which might
typically have a large number of
decisions to be made but ideally which
are parametrized by and into a small
intrinsic dimension right in this case d
could be small but the number of arms
could give you large right so you don't
want something you just don't want to
play a standard barnett algorithm that
gives you something which is which
scales as n but you can hope to reduce
it to something like d okay and we
actually see that this is the case
right so yeah you can think of several
online settings fit this paradigm of
linear models it's not something that I
just made a so for instance you can
actually think of a problem called
online shortest path routing right which
is sequential decision-making problem as
being being written down as a linear
bounded problem so this problem is
called linear bandits so let's say you
have a problem very out to make a
decision of a path in a network to
follow it from source to destination
right and whenever you pick a part you
basically get as as reward the sum of or
the sum of all the costs incurred along
edges of the path okay so that can be
model so every arm can be modeled as an
entire path which is essentially sitting
in D dimensional space where D is the
number of edges in the graph and when
you select the path it basically you can
think of it as being encoded by a vector
that has ones wherever you have parts
and zeros whenever you do not have parts
and the reward that you get is simply
the inner product of that vector with
the vector that contains the actual
costs of every edge in the network ok so
you can parameterize it in this way as a
linear bandit you can in general cast
more complicated problems where you can
you have to select a bunch of subsets of
universe a set of universal arms as in
this linear bandit where each point will
just represent what's upset you choose
right so you can represent subject
choices as as vectors with 10 increase
and you can cache many problems in this
fashion ok and the parameter sigh in
this case we just represent the cost or
reward or utility / edge or per item of
this universe ok so that when you take
an inner product of the indicator
vectors with the parameter you will
exactly recover the sum of the
substantive chosen so subset some
problems can be model in this way and a
nice generalization that works here is
the UCB algorithm again right so recall
that in the UCB algorithm that that we
saw it basically goes and builds
estimates of running estimates of every
possible action using just the rewards
that were gained by observing that
particular action right it just uses
that information isolation to build its
estimate
then go work us all the amps right
whereas in this case really the the
unknown parameter here is really the the
hidden or latent variable side okay so
in some sense if you actually were to
try and figure out with high probability
where this point sigh lies in the whole
space of D dimensional vectors then you
would have at least a good handle about
how to use this site to predict the
rewards of every possible arm that you
understood right so really UCB can be
generalized to work here by building an
estimate and a confident set not across
every individual arm but in fact for
this hidden variable site okay so so the
the point estimate of psy at each time
turns out to be the least squares
estimate that you would ideally build
when you have a linear regression
problem and not not only this we also
saw that we need to build estimates of
confidence around point estimates
otherwise you will end up getting into
bad spots it's not good to just build a
single estimate and then play the best
I'm corresponding to that right so you
can design a clever ellipsoid around it
a region in space that depending on the
samples you've observed so far confines
the true parameter site to do a region
around this i had with I property okay
and this can be there are geometric
constructions available for these and
now what you do is now that you have
bracketed the set of all possible
parameters for the system in in some
region of space you be optimistic under
this uncertainty so what what does that
mean you have to finally pick an arm to
play at every time right so suppose you
have this confidence set go to every
point in the set okay so go to every
possible candidate parameter sigh in the
set if you go to as I ask it what is the
best possible action for that particular
site and what reward does it actually
give me okay and find the parameter sigh
in this whole set which gives you the
best possible or most optimistic and
this play the best action possible for
that so this is how you can be
optimistic when you on uncertainty yeah
and it turns out that if you just play
the most optimistic looking armed with
respect to this ellipsoidal confident
set you in fact get regret that where it
goes as
something like D times square rooting
instead of n times square rooting so
this is very useful when you you have a
model that says look at the reward that
I gained from basically showing you an
item with a few number of features is
accurately modeled by a linear function
on only those features okay so there can
be infinitely there can even be
infinitely many am let me huge numbers
of items but as long as the feature
dimension is fall you can actually learn
a trade proportional to the feature
dimension instead of the number of arms
right and this is exactly happening
because by implicitly learning the
feature map you are able to generalize
across actions okay right so you can
take this even further you can actually
deal with different ways of trying to
generalize across arms so this is a
common variant called the ex armed
bandit problem okay so X just stands for
any possible sort of arbitrary decision
space so imagine that your set of arms
is the uncountable set of points in 0 to
1 okay that is each point in the
interval 0 to 1 index is enough okay and
when you pick a particular arm to clay
you can pick any possible arm in 02 on
the set of all real numbers in 0 to 1 to
play there is an infinite number of them
when you play an arm you basically get a
reward that is some expected value plus
some noise around it right so this blue
curve here denotes the let us say the
expected value of arms in the set of
armed 0 to 1 and we make the assumption
that it's a fairly smooth function right
so this captures the fact that nearby
arms share similar rewards this is
another way to model the ability to have
structure in the set of amps or to
utilize some sort of smoothness property
among different terms in space ok so
these settings occur often menu for
instance it is a typical case of trying
to maximize a function y are noisy
samples right so you can think of the
blue function is a function and the
whenever you sample whenever you give it
an x-coordinate it gives you a
y-coordinate with some miser ok so it's
about this point right so you pick some
point you get a reward you pick some
other point to get a reward right you
keep doing this and you are
opposed to basically minimize the grid
with respect to picking the best
possible report the highest possible
point right so if you pick it would be
idle to pick some point here but you
have to essentially learn about this
given that you are searching for you're
searching over the space of smooth
functions right it turns out there are
variants of UCB that even extend to
these kinds of complicated set of sets
of actions okay but what is essentially
helping you is that some some form of
smoothness else helps you generalize so
basically if you learnt about the reward
of a particular arm well enough you
don't need to sort of keep searching for
arms around it you already know have a
good estimate of the the main returns
from arms around it right so the next
part is rather different it's a very
different way to approach this problem
of playing a stochastic bandit he saw
already that the UCP algorithm is
something fairly natural based on the
idea of bracketing the variance of
running estimates of arms and trying to
use it in a nice way to basically get
very low regret okay but there's totally
different uploads that was proposed in
fact several decades ago in the 1930s in
fact it was basically I guess the first
bandit algorithm ever but it seemed to
perform very well it always performs
rather well in practice but nobody
really knew how why it works so there
was no real proof of this this this form
for why it really works well until in
fact maybe three or four years ago right
so for a long number of decades it was
just use in practice for a lot of
researches and it was popularly called
Thompson sampling or posterior sampling
okay so our idea behind post air
sampling is fairly simple it's a totally
alternative way to alternative way
compared to UCB to try and approach a
bandit problem
so recall that you basically have n arms
and you can play basically one arm at
each time and then you can observe the
road right so this is motivated more
from a bayesian perspective to
decision-making problems it's not
exactly base in but but it's a sort of
fake or fictitious basin of birth okay
right so the the algorithm works as
follows let's just consider that we have
two arms to basically choose between at
each point in time right so if their
arms that let's say museum that they
come from a Bernoulli distribution right
all that you care about is what is the
Bernoulli parameter for each arm if you
were to know that then the problem would
be solved right nothing to learn you
just go and play rom that as a maximum
parameter right or the maximum mean so
evasion in the sense is someone who will
come and say look I don't know the means
for these I don't know the parameters
for these arms a priori so let me just
pretend that there is a prior
probability distribution on the
parameters of these arms ok so in
particular I can choose my favorite
distribution let's assume somehow that
nature is assigning a parameter to each
arm by using any uniform probability
distribution right it could be any other
distribution but uniform is turns out to
be good enough ok so let's assume that
you have a uniform distribution for the
I mean in some sense nature chooses a
parameter for these two arms based on
uniform distribution this is the beige
and classic bajan assumption that
everything in the world is drawn
according to a probability distribution
yes including even the parameters for
the for the rewards ok so let's say this
uniform now what you do is you just
randomly sample an entire parameter from
this prior distribution ok so just go
ahead and sample one point in 0 to 1
uniformly and call it the mean parameter
for arm one right and sample do the same
thing for our number two and you have
basically one sample florita so in this
case this might be something like point
eight this sample might look something
like point2 right now in this just after
sampling you don't need to do anything
else just pretend that they are actually
the real parameters and make a greedy
decision
okay so in this case what will happen is
if you just decided to play arm number
one okay so after you play on number one
you just may the best time assuming the
sampled means are the true me and once
you get a sample from it you can think
of updating your prior probability
distribution on the set of mean
parameters for arm 12 a posterior
distribution right this is by something
called Bayes rule which all of you have
studied it Bayes theorem just lets you
update the prior probability that the
arms parameter was the probability that
the amps parameters lies in some
interval here given that you observe the
sample that was 0 or 1 whatever it is
right so you get a true sample and you
can update this posterior so it'll
update from uniforms or something that
is slightly different okay and the hope
is that if you if you accumulate enough
samples then this probability
distribution will actually shrink
towards the actual true parameter value
okay this is something that has been
that is well known in in Bayesian
statistics if you keep updating the
prior to a posterior with increasing
number of samples you know this will
actually concentrate to something which
sits exactly on the it will converge to
something which is close to the real
parameter ok so you repeat now that
you've finished one time instant you
repeat this exactly for the next time
you take another random sample hopefully
this will be close to sort of the peak
of this distribution and in this case
just by pure chance algorithm to sorry
arm to happens to be larger in this case
since you decide to play am number two
for a change observe a sample and update
its prior ok so now we are belief about
the parameter the the means of both
these arms has changed in this
particular way okay and so this is the
algorithm we just keep going forward
there is no artificial bias you have to
add and so on the moment you fix a prior
everything is different okay so your
choice of prior is up to the designer
but after that everything just follows
bees
right is as simple as it looks you might
wonder you know why is this reasonable
does it give you the right so of course
it's a randomized algorithm as opposed
to UCB because it it can do random
things at each time depending on what
samples it picks right isn't that way
it's very different from UCB and you
might always wonder you know why why do
I should even even work why does it even
work in practice you're just seeming to
do too many things at random right so
why should things sort of concentrate or
tighten of the right rates and try and
get you some form of learning right so
this was actually shown to be in fact
not just very performing but optimal in
the sense of dignity I mean it can
actually obtain the optimal n log t type
regret rates as recently as a few years
ago okay so of course that will probably
also tell you that one cannot even write
down the proof on the board here will
require probably an hour or saturated on
it since it's fairly involved it's not
as simple as this but it's really nice
to know that such a simple and what was
what looked like a heuristic strategy
actually has optimal technicality right
so you can you can extend this in fact
too much more structured settings like
the linear bounded set up that we
considered okay so recall what happens
in the linear bounded set up okay so by
now in the last so after the the
analysis for Thompson sampling came out
it's been found that it it's actually
very it's a rather broad property that
enjoys corresponding to any kind of
structure structure bandit prediction
problems so for instance in the linear
bounded case so recall the linear
bounded setting so
right so you pick an armed with some
feature X i we can are my future ex and
you get a reward that is a noisy version
of sy transpose X n right or where sighs
the hidden parameter right so what you
can do for a linear bandit if you want
to use this Thompson sampling kind of
approach is actually to put a prior
distribution on the is to put a prior
distribution probability distribution on
the on the set of all possible site so
for instance you can start out by
imagining that the the unknown latent
variable side is actually distributed
according to let's say a Gaussian
distribution right in D dimensional
space right just start with some prior
distribution what you do is just sample
a possible value for the latent
parameter side from your current
posterior distribution in D dimensional
space now once you have samples that
pretend that it actually characterizes
the entire model right then go and
basically figure out the best arm which
will give you the best inner product
with the particular value mu hat that
you sample this is a proxy for five
sides and just play that particular
action ok so knowledge notation might be
slightly different but the idea is
exactly the same you have an uncertainty
or prior distribution on site you pick a
random sample play an action which
attempts to maximize that sample my
optimal with respect to the sample and
once you observe a reward a noisy reward
you can update the Gaussian prior to a
Gaussian positive right so this is the
well-known principle that if you have if
you if you know X and if you have a
Gaussian prior distribution form you and
your observations are of the form x
times mu plus noise then the posterior
distribution after you observed x times
mu plus noise is essentially still a
Gaussian distribution you can just
update the main and covariance so this
becomes really convenient because you
started with the Gaussian distribution
as a prior you continuing with the
Gaussian distribution is the posterior
you don't need to maintain too many
parameters other than the mean and the
covariance of the Gaussian vector and
this somehow I mean this this this also
in the nice way converges
and gives you optimal square root tea d
times square root II regret that you see
in standard linear palettes it so this
is a powerful way to implement this is
also very very efficient in terms of
implementation because at each time we
are essentially maintaining a Gaussian
distribution which is essentially a mean
vector and a covariance vector in indy
dimensions and it's rather easy to
sample from our caution okay so it's not
at all you can implement implemented in
matlab with one line of code okay so it
doesn't matter even if the
dimensionality d is large you can easily
sample from our Gaussian given a mean
and covariance it and this turns out to
be actually practically a game-changer
in solving several linear one right so
so this is probably so this is an
example of the the online shortest paths
decision-making problem that I said
could be cast as a bandage so you have a
network with a source and destination
right so you have a fixed source on the
fixed destination think of this as you
want to go from home to office every day
okay and each particular day you decide
to choose a path you choose an entire
part each path has some mean let's say
cost associated to traversing that pub
let's say it could just be the amount of
delay that you suffer along the path or
the amount of transition or pollution or
whatever it is it so if you go along
this particular path the river that you
get will be sorry the cost that you
suffer will be this cost plus the first
does that cost plus that ok so it's an
additive cost model which which is
reasonable in several settings so the
model is you choose an entire path you
essentially have realizations of rewards
along the path and finally essentially
observe the not revolt i should say cost
ok you would probably so you cannot you
can have different feedback models in 11
model the cost that you might observe
might actually be the accumulated some
of course that is the sum it is a sum
function in another in other settings
for instance if you are interested in
flow type problems your revert actually
might be essentially the bottleneck path
along the storm in which case it might
correspond to a min function ok so if XT
of IJ is the stochastic capacity of link
IJ right the capacity of
this part essentially becomes the most
bottleneck the capacity of the most
bottleneck link on the spot right so
this is essentially the least possible
capacity in an entire path right so
supposing you have reward feedback which
is of this particular form right so we
essentially observe your actions here
are essentially the set of parts in the
network and each part in the network
when you select the path you get
basically a reward that is some
complicated function of the atomic or
individual rewards that comprise the
path ok so this can arise in several
settings it need not be exactly a linear
function so this is going away from the
linear bounded setting into more much
more nastier forms of feedback ok so you
continue playing this game the field
that only feedback you get essentially
is the min of all these things you don't
know you don't even know any of these
other variables on the path otherwise
you could learn much more but you are
only given information about the min of
these random variables so every time you
select a part you get to see the minimum
of the random variables that comprise
the part so how can you hope to learn
this such a setting is the divide is not
linear you can't resort to a linear
model of the reward as a function of the
parts that is selected and so on it but
you still want it interested in
minimizing the regret with respect to
let's say the best possible paths in the
network right so more generally you can
come up with an entire class of bandit
problems which we call complex magnet
problems right where you have a basic
multi am banned within actions and
different probability distributions and
you have a bunch of derived actions
corresponding to these actions right so
in the shortest path routing case each
action you can think of each of these
basic actions as each edge of the
network ok and each complex action is a
pair path so it's it's section of
certain edges on the network ok so you
have these derived actions your basic
essential basic essentially the
distributions of every cost on every
edge of the path but each action is a
much more complicated combinatorial
object ok and you can pick only complex
actions and whenever you pick a complex
action to play you get some reward which
is a noisy version of a function of the
basic variables that comprise the
complex action ok so the online shortest
part setting fits into this as
several more settings so for instance
you could be dealing with a simple
subset selection problem in bandits
where you you you're not allowed to
display one arm of a bandit we can
decide to play let's say five arms of a
mandatory stay right and think of these
as let's advertise means this for the
purpose the purpose of example and if
you decide to put 500 advertisements on
a page the next Valley or utility of
that collection might not just be the
sum of all these it might be a
complicated model for the rewards that
you get or utility that the user gains
by showing different types of ads
together right for instance certain
combinations of ads which have similar
categories might not give you my word as
much as other combinations of that so
you might have in something like a
submodular function on the reward that
you get by showing a collection okay you
might have some max or min type feedback
so essentially the feedback that you get
after showing a subset is some
complicated function of each of the
rewards of the items in the subset okay
and it's a very aggregate form reward
because you are not even allow allowing
access to observing individual rewards
of elements in the subject but it's some
aggregate function of all right so
there's an example from job scheduling
where the reward function plans are to
be something popularly called as the
makespan it's the makespan essentially
is if you split a set of jobs across a
bunch of machines the makespan is
basically the time taken by the last
machine to finish all jobs okay and that
turns out to be a very nonlinear
function of the individual finish times
of all jobs right so you might be
interested in sort of minimizing
functions of this type which are in the
intern derived functions of more basic
rewards and you can actually extend the
the fundamental idea of Thompson
Franklin sampling even to these settings
right so what you do is all that you
have to do is some nice way of putting a
probability a prior quality distribution
on each of these basic parameters or
basic reward distributions after which
so think of this entire circle as the
set of all possible basic parameters at
all possible basic reward distributions
you put a prior probability on that set
itself right
every point here corresponds to
specifying the distributions of all
these arms and that in turn completely
induces the distributions of rewards for
all these complex actions why are this
function okay so you just you can just
sample a random parameter from here once
you sample the parameter right so this
is the true parameter right if you are
discovered suppose you sample some
parameter here from the prior assuming
that that is actually true you can try
and compute the best possible complex
action that you can play right and this
turns out often to be a simple
combinatorial algorithmic problem in
networks like let's say finding the
shortest path in a network given all the
edges weights or something like shorting
a vector okay which is polynomial time
once you've done that and played the
best action for the sample parameter you
can so assuming that you can go back and
update the posterior distribution over
the space you can hope to do this
iteratively and converge some sense to
the actual parameter and in the process
minimizing it and it turns out actually
that this is so this is just in notation
the entire algorithm it is exactly what
we went through and you can actually
show that I mean a fits with some
careful analysis that if you start with
a reasonably nice prior and you have a
bunch of actions and even if you are
working with the model which has very
complicated reward feedback you can
actually control the regret in a very
nice sense in the sense as some constant
times log T where in fact the constant
now need not be the set of all possible
complex actions right so in fact the set
of all possible complex actions might be
really large so think of the set of all
possible parts in a network it could be
exponentially lies in the size of
network network trade so you want to
avoid a dependence on the total number
of complex or derived actions and you
want to in fact have a dependence which
is closer to the real sort of unknown
number of variables which in this case
made number of edges in a network or the
number of elements in a set promise
you're picking subsets at the set of
possible subsets might be exponentially
large but you really don't want you
don't want to ignore the structure and
sort of learn each subset as it as if it
were its own because you know that all
the rewards are
okay and you can actually show that this
analysis captures the sea constant as
something that is related to the
inherent description length of the
problem in the sense that it essentially
specifies the number of independent
dimensions so this is like the D in the
linear minded problem where D is really
the number of degrees of freedom in
terms of the unknown parameter yeah it
can be in in some cases that we looked
at much smaller than the number of
actions and it has interpretation as the
solution of a particular optimization
problem so this is a satisfying in some
sense because it helps me understand why
Thompson sampling works in in with far
more generality than just the simple
multi-armed bandit which has been well
studied and it can actually have the
potential to generalize across actions
by assuming some form of structure in
the model of rewards from actions okay
so i guess i'll stop here you i just
mentioned that this goes even beyond
bandit problems you can use thompson
sampling to solve reinforcement learning
problems where you have states and
actions not just actions so basically
when you play an action right so you
have three different states for instance
you are at this state when you play an
action you just don't get a reward but
you actually also go to a different
state right so now you ought to be aware
of actions that might not just give you
bad rewards but might take you into
different kinds of states where you
might sort of be forced to pay penalties
for a long time right so this forces in
some sense this is an even a much harder
problem and the Bandit which had only
one state so to speak with several
actions where you have to actually be
cognizant of how future states will
evolve depending on what you do today
okay and you can actually show that
Thompson sampling works well if you have
a parametrizations parameterize version
of an entire reinforcement learning
problem and you can use it in the same
way as you did for bandits to draw
parameters except that if you draw a
parameter you have to actually compute
the optimal policy for the Markov
decision process as opposed to just
playing the right the best possible
action right so it's just a small change
in definition but the basic prescription
brains the same and
turns out to actually work fairly well
if you can actually prove regret bonds
and stuff which depend on essentially
something that inherently is the
dimensionality of the of the of the
reinforcement learning problem that you
consider so I guess yeah this is all I
have to say so this wraps up sort of the
Bandit setting
on the other school as also post</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>