<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS Poster Spotlight Session 2 | Coder Coacher - Coaching Coders</title><meta content="NIPS Poster Spotlight Session 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS Poster Spotlight Session 2</b></h2><h5 class="post__date">2016-06-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9xdHCGJFLlM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged showing internet or public
display is forbidden and may violate
copyright law
okay hi everyone so my talk is going to
be about Frank wolf vision quadrature
probabilistic integration with
theoretical guarantees and this is joint
work with Chris oats in Sydney margiela
me who's my supervisor work and Mike
Osborne from oxfords okay so why should
you actually care about the method well
as was described by zubeen car man ye're
prob a stick inference is one of the
main topics in machine learning and in
this case you have to do integration all
the time so for example to do
marginalization conditioning then these
are all integrals and in most cases
they're not actually going to be
analytical and you're going to have to
use some kind of numerical scheme so a
typical way of approximating integrals
is using quadrature rules so you
approximate your exact integral which is
here on the left with a weighted
function of a weighted combination of
function values and you have to pick
some points the z axis and some weights
the w's now this is usually very very
slow and well of course it depends on
the method but why I mean by slow is
like your error between the exact
integral and the approximation decreases
very slowly in N and so for example if
you take Monte Carlo methods then you're
going to sample some X's frumpy and then
you're going to wait two more equally
and this will converge as n to the minus
one half and in a lot of cases this is
going to be way too slow and you might
not be able to afford taking a lot of
samples so one particular thing that you
can do is use Beijing quadrature and in
this case what you see idea is to use
some prior information you might have
about your integrand to guide your
choice of the design points and the
weights so what I mean by prior
information is you might have some idea
of how smooth your function is or other
properties and you can encode this
directly into your numerical scheme now
ok so this is a solution but it's
actually just moving the problem around
because there's not actually any
convergence guarantees for Beijing quite
a true for now and so
you don't know if it's actually worth
using it but that's actually why you'll
want to go and check out the paper
because in this case we showed that if
you use a particular convex optimization
algorithm called the Frank wolf
algorithm then your beige in quadrature
scheme can have exponential convergence
under certain assumptions and obviously
exponential is much much faster than one
of the square root of N and so you
should definitely come and check it out
and it's post 257 and there's also a
whole workshop on Friday on pro basic
integration which I invite you all to
come thank you hello everyone I'm my
name is sushi chef is Isaiah water I'm
from EV FL the title of my presentation
is distribution robots logistic
regression this project is a joint work
with payment module and as funny and
Daniel logistic regression is one of the
most widely used classification
technique in logistic regression we aim
to construct a classifier where a
maximum likelihood estimation and we
also wish to quantify the classifiers
risk in both cases we need to evaluate
an integral with respect to the
distribution P of future label purse
however and the distribution P is
unknown and must be inferred from the
and set of training data and classical
logistic regression just replaces a p
with the empirical distribution P hat
and on the training samples however this
leads to overfitting our suggestion is
to construct a set of all distribution
that could have generated training data
with high confidence we call this set as
the ambiguities that so that the
proposed distribution robots logistic
regression model and choice to find the
minimizer of the worst-case expected
loss where the worst case is taken over
all the distribution in the immunity set
and we also aim to quantify the best and
force and miss kayla situation
probability over the
ambiguity set the challenge here is to
construct an Ambu to set such that and
the resulting classifiers offers
out-of-sample performance and it leads
to a tractable reformulation in this
study we use the day and we construct
the ambiguity set as a ball in the space
of probability distribution and around
the empirical distribution we use the
water h9 distance to quantify the
distance between two distributions more
specifically the water a strong distance
is defined as the minimum transportation
costs for moving for moving the
probability mass from and distribution
p1 to p2 our approach has several
benefits first due to the recent measure
constant concentration result it offers
after sample guarantees second it leads
to a tracker program and finally we can
find a probabilistic interpreter
interprets for well-known regularization
technique also as you can see in the
figure and we consistently outperformed
the regularization the growler logistic
regression and classical model thanks
for your attention
hello my PhD advisor foster on your
teams and I studied how we can learn
from the logs of interactive systems
that only provide us banded feedback we
identified an interesting new kind of
overfitting in this setting which we
call propensity overfitting our
conditional random field prototype that
learns from Bandit feedback avoids this
overfitting using the ideas from this
paper is available online the formal
setting is batch learning from Bandit
feedback imagine you have collected X Y
Delta triplets from your search system X
denotes the user queries y denotes the
action that was taken by the system so a
ranking was shown and Delta denotes the
users feedback on the presented ranking
typically we don't know how the user
would have reacted to some other
possible ranking this aspect of the logs
is called bandit feedback we want to use
these logs to find a good policy H that
can generate Y given X bandit feedback
makes this problem harder than
supervised learning where we are told
the best possible action on the training
set it's easier than reinforcement
learning where we also have to solve
credit assignment and it's not quite
online learning for contextual bandits
because there is no trade-off between
exploration versus exploitation going on
here this is a very practical problem we
have collected terabytes of logs from
search from recommendation from ad
placement and that is a general recipe
for reusing the logs from these systems
in a model-free manner first recognize
that these logs are biased the if the if
certain actions were preferred by the
system then those actions would be
over-represented in the logs so use
important sampling to d bias then next
do empirical risk minimization or erm
this gives you training objectives very
similar to supervised learning a trained
error plus regularizer except the
training error is now there's our hat
that's computed by our important
sampling in practice erm with this R hat
often fails a common failure model is
where erm completely ignores the
feedback that you have collected in the
logs just over Fitz to the propensity of
seeing your data items in the logs and
finds the H
that's very simple who's empirical
variance is going to be very small and
you think you're doing really good but
you're actually very very wrong no
capacity control or variance regulator
variance control regular risers we have
so far fix this issue and the insight
really is these are hat estimators are
subsequently getting used in an
optimization problem so what we really
need is an estimator that works well
with optimization this our hat is not
one of those we identify a particular
property called equivariant which allows
for robust erm and we point out that the
self normalized estimated is indeed
equal variant and it's much better than
this picking the best estimator for this
problem is still an open question but
surprisingly the self normalized
estimated runs very fast taking just a
few seconds to learn multi-level
classifiers join me on number 59 for
more
my paper works on asynchronous parallel
stochastic working for non convex
optimization I'm sure Julia and my
closers are each new home internally and
geo we're from the University of
Rochester nonconvex taught meditation is
quite common in applications such as
deep learning natural language
processing and recommendation system and
asynchronous parallelism is widely used
in deep learning asynchronous paralyzing
has a variable property that it can
substantially reduce the idle time in
synchronous parallelism because it does
not need the synchronization step our
people provide some selective analysis
for asynchronous stochastic gradient
especially for non convex optimization
in asynchronous stochastic gradient
there is surely central node keeping the
optimization variable X and there are
many workers running traditional SGD our
wisdom independently and concurrently
the main difference from the standard
STD is that the ex had used to compute
the stochastic gradient might not be the
same as the current optimization
variable X in the central node in a
synchronization this is also a key
challenge in analysis the other key
challenge in analysis is that different
implementations lead to different forms
of X hat for example the X hat in
cluster implementation and not Colin
petition are quite different our paper
actually covers both implementations and
shows that the asynchronous stochastic
gradient the convergence rate of it is
consistently stochastic gradient descent
and we show that a linear speed-up can
be achieved up to the order of square
root of iteration number walkers here is
an intuition of linear speed-up the
deviation from the true gradient caused
by SGD actually dominates the division
called by asynchrony thus the
asynchronous will not see this effect
the convergence treat with the encoder
rewards for their constructive comments
and we'll come to our poster tonight our
post number is 63
good morning everyone so I'm Andre I'm
from for instance I'll be talking about
a joint work with Angela wasti who is at
Rutgers about some cases where we can
say something provable about very sure
inference as applied to topic models
okay so let me give you some context
first so in recent years it's been
getting increasingly more and more clear
that most problems that you want that
you care about in machine learning tend
to be non convex in nature and so to
give you some sort of stereotypical
examples here it would be things like
clustering map approximating posteriors
etc so from a theoretical point of view
these ought to be intractable but what
happens often in practice is very simple
local search or descent type of
heuristics tend to work quite well so
what am I putting in this category while
it's things like am variational base
back propagation other variants of
gradient descent etc so the sort of made
the sort of very big question for theory
here to answer is why does this happen
so because of time and space constraints
vastly an egregiously kind of
oversimplifying and omitting let me just
mention a few kind of a few success
stories where this has been done before
so this is we done for for various
variants of k-means for example in the
case of mixtures of gaussians
alternating minimization has been
analyzed for a matrix completion for
phase retrieval for dictionary learning
and so with that in mind I can tell you
what we do so we basically add one more
heuristic to this list of things that we
know how to analyze and we analyze a
version of mean field variation in
France as applied to topic models so the
things that you need to know are the
following our assumptions are very
reasonable they're similar to what
priority reticle works on topic models
assume but not have been as these these
works do not actually use these local
search for the same type of algorithms
our initializations are also reasonable
in fact they're inspired by what we
think people use in ldac which is the
most probably the most often used
software for for doing virtual inference
for lda and our proof methods are
somewhat different from these other sort
of prior works that I mentioned so
hopefully there'll be some other
applications of them so please come to
the
poster it's not the number is not on the
slides but it's 48 please come to the
poster to hear more hello everyone my
name is Igor Cara and I'm here to talk
about extending the superga wisdoms to
distribute the decimation of houston TX
this is joint work with a legend belief
from inner a lil and my advisors josep
sermons different chromosome from
telecom vitac let's consider the program
where some observations sample is
distributed over connected Network and
notes of the network can be smart phones
and solves what you want and you want to
estimate a pairwise min statistics fine
stands variance area under the ROC curve
or any x y statistics you involved in a
machine learning problem what we want
here is not go with them that can
perform in a synchronous setting with
respect to some communication and local
storage storage constraints this this
problem is very fairly typical in
domains such as telecommunications or
sensor networks we based our approach on
gosee protocols that means that at each
iteration only one edge is activated
this protocol has been widely studied
for estimating the sample mean in this
case selected node just avoid their
estimates however extent knively
extending such algorithms would lead to
massive data transfer in the in the case
of pairwise estimation how solution
consists in the following each node
stores three elements its own
observation eggs and actually auxiliary
observation why initialize to X n it's a
statistic estimate edge heart at every
step selected nodes average their
estimate as in the standard
mean estimation then they update their
estimate using the two observations old
and finally the swabs auxiliary
observation this algorithm can perform
in a fully asynchronous setting inition
to have a theoretical convergence weight
of 1 over T with an explicit dependence
in the spectral gap of the network which
is a measure of a measure of
communication capability of a network it
also shows good practical performance in
simulations thank you for attention and
I'm looking forward to seeing you at
poster 72 tonight hi everyone I'm
background mr. Solomon and this is a
joint work with a macabre see I should
model is your land on your scrolls and
distributors of modular cover problem
suppose that you have a large data set
and you want to find the subset idea is
as small as possible that achieves a
desert quality q according to a utility
function in many applications such as
clustering recommendations document and
corpus summarization and nonparametric
learning this utility function is
monotones of modular which means that
adding an element to a smaller subset
helps at least as much as adding the
same element to a larger subset for
monitors of modular functions the greedy
algorithm that iteratively selects the
best element achieve the logarithmic
approximation guarantee for this NP hard
problem but for large data sets the
greedy algorithm may be impractical for
example if the data set is not even fit
on the memory of a single machine in
this work we propose the first
distributed algorithm for sub modular
cover problem where the data set is
partitioned and set up and machines
operating independently the key idea is
a reduction to the problem of
cardinality constraints of modular
maximization for which recently many
distributed algorithms have been
proposed a technicality of our approach
is to estimate the size of the optimal
solution
and while coordinating different
algorithms and sharing partial solutions
between consecutive rounds we also
provide approximation guarantees for our
algorithm in particular we show that the
solution quality in terms of the
solution set size is competitive to that
of the centralized greedy algorithm
while the number of rounds compared to a
naive distributed version of the greedy
algorithm improves from a linear
dependence on the size of the optimal
solution to only a logarithmic
dependence and optimal solution subsides
we also applied on algorithm and large
data sets here you can see the result
for vertex cover problem on a network of
two billion edges or I were term has a
trade of parameter alpha for smaller
values of alpha as you can see we get
very close to the result of the
centralized blue algorithm which is
shown as dashed line but in much smaller
number of rounds come to your poster
number 65 and I will be happy to discuss
this more thank you hello everyone I'm
rat and i'll be talking about new clean
stein method so in this paper we
consider the problem of finding the
maximum likelihood estimator in
generalized linear models when the
number of observations much larger than
the dimension of the parameter so we
minimize the negative log likelihood by
standard iterative updates but the goal
is to find the scaling matrix QT that is
computationally efficient and also
provides sufficient curvature
information so i'll give you the basic
idea about Newton's time method so we
start writing out the passion of the glm
problem and we notice that it's just the
sample mean as the major of its
expectation which is written on the
right hand side and if we pour this as
an estimation problem we apply steins
lemma and find an equivalent expression
which is written on the
second line and instead of estimating
the expectation the first time we
estimate what the steins lemma give us
so this algorithm takes a Newton step
relying on a Stein type lemma so we call
it Newton Stein method and just by using
this equation we can reduce the Newton's
method order MP square per iteration
cost to order MP which is just the cost
of computing the gradient so we show
that this algorithm Newton Stein method
gets a composite convergence rate which
is it starts with a quadratic rate and
then transitions into linear rate later
on this is also consistent with our
experiments as you can see that the
initial convergence is quite fast and it
transitions into linear near the true
minimizer so I'll talk more about the
composite convergence rate and give you
more compare experimental comparisons
with other existing methods in the
poster session thank you so this
concludes this morning session let's
thank all the presenters once more
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>