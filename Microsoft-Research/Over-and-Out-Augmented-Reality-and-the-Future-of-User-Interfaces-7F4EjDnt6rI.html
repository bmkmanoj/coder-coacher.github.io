<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Over and Out: Augmented Reality and the Future of User Interfaces | Coder Coacher - Coaching Coders</title><meta content="Over and Out: Augmented Reality and the Future of User Interfaces - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Over and Out: Augmented Reality and the Future of User Interfaces</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7F4EjDnt6rI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
the next speaker is Professor Steven
fenna processed Ian forgot his PhD from
brown university and he's a professor of
computer science at columbia university
now where he directs the computer
graphics and user interfaces lab his
research interests includes human
computer interaction augmented reality
virtual environments 3d user interfaces
knowledge based design of graphics and
multimedia mobile and variable computing
computer games and information
visualization Proserpina is the
co-author of computer graphics
principles and practice an introduction
to computer graphics is the name of the
book and he's also been the program
chair of various conferences and
committees I request Josefina to come
and deliver a stock thanks
hey good afternoon it's a pleasure to
have an opportunity to tell you about
some of the fun stuff that my students
and I get to do in the name of computer
science research so look at that rather
long title i have up over there and it's
probably afraid that you might not know
which is augmented reality and so i'll
begin by telling you what augmented
reality is so admitted reality refers to
the idea of taking virtual content media
various sorts and combining it with our
perception of the real world and the
idea is to do this interactively and do
it in a way in which the two things what
you experience in the real world and
what the virtual stuff does end up being
registered this is hey be they end up
being aligned in 3d to give you an
example of this I have an image from an
old piece of work that we did back in
the mid-90s this shows some cylindrical
struts and one spherical node that are
part of the building system this is what
you actually make what are called space
frame buildings from here we're seeing a
photo and here we're seeing a very very
simple very primitive example of 3d
graphics there is a abstract
representation in bright red of one of
those struts a little wireframe node and
a little arrow that actually spins and
this is running live and by itself this
is well you know what's going on there
it is a strut there's a note as a little
arrow and some text now when you combine
them together on the other hand in place
this is basically instructions that are
explaining to someone which strut to put
in place against which node and then to
turn by turning a little collar that you
don't actually see there to attach the
strut to the node with one piece of
instruction applied at a time to
essentially show you in place how to put
the building together so interesting
thing over here it's two things actually
one of them is that the way i defined
augmented reality at the top i didn't
actually mention graphics in particular
it turns out that a lot of what i'm
going to talk about is visual augmented
reality but this can be any
kind of modality candy things that you
see but also things you hear smell taste
feel etc so interesting thing about the
virtual content because it's
supplementing rather than replacing what
you're experiencing of the real world
the real world still there we have to
pay attention to it and therefore the
virtual stuff needs to complement needs
to respect the stuff that's there for
real this is not like being given a big
black rectangle of a monitor for example
as dark as possible when there's nothing
up there you own every pixel and you
control everything that you see instead
we're looking at the real world there's
something over there that that's
important to us we have to make sure
that the virtual stuff doesn't obscure
it you're crossing the street you don't
want to have a car that's maybe getting
ready to run you over be concealed by
virtual stuff you want to pay attention
to the important things that are
happening in the real world so that's
actually an interesting design challenge
that I'll try to address during the
process of talking about this work so
what I want to do is give you a little
bit of an understanding of how we
display stuff and augmented reality
there's many different ways to do it and
just a very very high level sketch of an
understanding of how we do this kind of
registration this kind of tracking we
often call it if things in the real
world so in terms of they are displays
one of the main kinds is what we call
optical see-through and here i'm
actually going from talking about things
in a modality independent way to really
talking about things that are visual in
an optical see-through display what we
have basically is a display which are
seeing positioned somewhere above and
near the users head there's some optics
think of lenses and mirrors and you're
basically looking through those optics
at the real world and you're seeing as
well in this case may be a reflection
perhaps just a simple piece of glass the
little display and if the display has
virtual graphics on it then its image is
being combined with what you see in the
real world and we now have virtual and
real stuff combined old courtesy of
optics now there's another way to go and
do AR and that's what we call video see
through and here instead of looking at
the real role directly you're actually
looking at a display and there's one or
more cameras that are looking at the
real world and then the camera view and
the virtual graphics were being combined
in the computer using techniques pretty
much the same as the ones that are used
in movie special effects and this
combination then gets fed to the display
so we look just at the display and the
only way we see the real world is
through the camera imagery and then
finally just for completeness sake we
can do something else we can take a
display let's say it's a projector we
can project onto the real world directly
and then we can combine what we see in
the real world with what the graphics
provides us by literally just looking at
real stuff that happens to have
projected stuff on top of it so we're
actually combining in the environment
these are the three main ways in which
we do augmented reality and then we can
take a cross product with the various
ways in which these displays and optical
elements can be located relative to you
in particular you can wear the display
light wear it on your head for example
in which case it might be even closer
than those pictures are showing over
there you can hold the display perhaps
in the hand as you might with a
smartphone that has a camera built into
it so I'm not wearing it on my head but
I'm holding it my hand and looking
around I can even have it in the
environment where I can walk up to it
and I can see courtesy of the stuff in
the environment this combination of real
stuff and virtual stuff so given you
kind of an abstract understanding and
during the course of the talk i'll show
you some examples of some of these kinds
of displays and try to explain what some
of their differences are now i also said
we have to track we need to register
things and so i'm going to try to give
you a very high level kind of meta
understanding of how this stuff works
basically the goal here is we wanted to
determine
the position in the orientation of
objects for example your head or your
hand if you're holding something or
objects in the domain maybe it's a
ballgame and we want to be able to
augment the ball or it's a race we want
to augment the racers with information
about them and so basic approaches you
have some kind of sensor that's
sensitive to some kind of signal and
it's going to determine its position and
orientation relative to a single that it
detects from some source so the sensor
could be on an object that's actually
being tracked and viewing some source
that's at a particular known position
and orientation or we could have the
sensor itself be at some known position
and orientation and it viewing the
source that's attached to or maybe even
is the object being tracked so again I'm
being very very generic over here trying
to cover pretty much every way of doing
this and furthermore you can actually
use the signal properties that they can
be categorized in some interesting way
to determine not just that you're
tracking something but the identity of
the thing that you're tracking so let me
try to make this a little bit more
specific there's lots of ways to do this
I've mentioned some unsolicited here you
can use computer vision you can use
optical things that aren't really quite
yet cameras you can use ultrasound
electromagnetism you can pay attention
to things that the earth does for
example the earth has a magnetic field a
gravitational field you can send up
little satellites full of all kinds of
expensive technology and use those which
is what gps does lots and lots of ways
to do this let me give you a couple of
examples let's say we have a camera the
camera can view patterns of features
that are in the real world and I can
give you an example over here on the
image at the very top shows a bunch of
these little black and white pattern
squares those little patterns make each
one of those little pattern squares
capable of being uniquely recognized as
having a particular identity and the
system is told in advance what the
patterns are and as well it knows
they're all going to be squares perfect
squares of a particular size
if you look at those squares with a
camera depending upon how the cameras
positioned and oriented the squares will
project not as squares but more
generally as quadrilaterals whose sides
are not all the same length these angles
are not all 90 degrees and it's a very
nice elegant mathematics that lets you
if you know the lengths of the sides and
the angles figure out position and
orientation of the camera relative to
these little pattern squares that it's
looking at in the real world now those
are kind of ugly and up art looking if
you go with one image down below what
you're looking at is a image of a bunch
of little stones and it turns out that
as you can see very fact that you can
actually make them out to these stones
is in part due to the fact if that's
been very dark parts and very light
parts there's lots of contrast
especially around the edges of the stone
and there are ways of being able to go
and look at or have an algorithm look at
this collection of pixels and find
places especially ones where the
statistics are kind of interesting like
is a very very dark thing fading to
something very rapidly that's our avery
bright and the idea is that the system
can essentially take note of these
features and compare them with ones that
have been analyzed to be in that source
material before actually looking at it
with a camera and then by determining
the sizes and shapes and positions be
able to figure out the camera position
and orientation relative to it and then
finally just for completeness I've
mentioned yet a completely different
approach in which a GPS receiver and
listen to signals coming from satellites
and any of a number of different
constellations GPS being the u.s. one
but there are other ones as well up
there circling the Earth and the idea is
that the signals that it listens to have
information about where the satellites
are very precisely located information
about when the signals are actually
being broadcast and therefore can figure
out how far it is from each of a set of
satellites and known locations plug that
into lots of really really cool math
that happily if you
you don't have to understand but it gets
very very messy and then be able to go
and figure out the position of that
receiver relative to all this stuff so
I've given you a sort of high-level
overview let me talk about a couple of
specific examples the things we can do
with augmented reality so for doing this
outside we can overlay information on
things that we'd like to find out more
stuff about and so here's an example
from a restaurant guide that my students
knighted back in two thousand one for a
conference that was held in New York in
which you're seeing as you look at this
restaurant across the street you're
seeing a little information sheet that's
overlaid on top of it off to the side
that lets you have access to its web
page to its menu and to some reviews
about it now back when we did this in
the mid-90s to early 2000s there were no
smart phones you could do this with the
computers you needed to do with her kind
of bulky and we actually had this
ridiculous-looking backpack that my
former student heard about zdenko over
there is wearing with this crazy-looking
antenna arising at the top and the sort
of weird looking outer space headworn
display and complimenting that this
handheld a tablet and all of the things
that we did on this kind of device back
around the turn of the century are
things you can now do even better on
essentially any smartphone so other fun
things we can do Oh games there's all
sorts of wonderful applications for
games I get paid in part to go and come
up with cool games so this is an example
of a marble maze game in which you're
holding in your hand a bit of fiber
board and all it has on it as you can
see in that picture at the upper left is
literally just one of these patterns
like I showed you before there is a
camera that's being worn in the head
worn display or two cameras if you're
doing this in stereo and then the camera
or cameras are tracking the board its
position and orientation relative to the
display and the display also has built
into it some sensors that can sense the
acceleration earth
gravitational field so they know which
way is down and put that all together we
can then decorate that otherwise
completely absolutely empty board with
some virtual stuff and so here you're
saying basically that blue board game
you're seeing that instead of the
playing yes I dope it the dice move
around full roll we have to get your
hands with the bouncy things over there
nice and this is all done just by
tilting the board which if you're
someone who likes to play games or
maybes a little intimidated by a say an
Xbox controller or similar kind of
control with lots of buttons joysticks
here it's just a little panel that you
can go and play with and it's much less
intimidating than if are trying to play
with something with a lot of buttons and
switches and he goes on it so here's
another application things aren't all
fun and games let's say we have
something that's really important that
it be in good shape it be maintained or
something that when it breaks that has
to get fixed so augmented reality can be
used provide instructions to explain to
a person how to fix something and as it
turns out try to explain to you you can
actually show that there are situations
which augmented reality lets you fix
things better faster with fewer errors
than if you're using more conventional
kinds of documentation so this is an
image of a marine US Marine who's taking
part in one of the studies that my lab
is done trying to compare using a are
versus using more classical kind of
computer based laptop based
documentation to find where things are
and fix them so what I want to do is to
tell you a little bit about one project
we have in which we're looking
particularly at tasks that involve doing
things with parts of the environment
assembling pieces together to make
something disassembling replacing
pushing pulling poking stuff in order to
be able to go and in this case put
something together this
an aircraft engine that we happen to
have sitting around at our lab because
it provides a very nice domain in which
to work so I'm going to tell you a bit
about this work which is a part of the
thesis of my now former PhD student
Steve Henderson and to tell you about
this i'm going to show you a little
video this one that i'm going to show
you now like the one that i just showed
you before is shot with a video
see-through display and then i'll show
you some stuff shot through an optical
seats or display and so in this case
we're going to see a little bit of some
instructional material created with a
system that's going to show you how to
pick up a combustion chamber that's part
of this engine that's currently not
where it should be on the engine and
then orient it the right way and insert
it into the engine where it belongs so
here we're seeing a view through one eye
of the stereo display and we now have a
label so its combustion chamber three
there's a little red arrow pointing to
it reach out grab hold of it and then
the system is now tracking using that
kind of pattern i showed you before the
arrow means gee I have to turn that
combustion chamber following the arrow
and soon I see this little placard on it
well the placards upside down we have to
make it right side up and if we make it
right side up we're now going to be
holding this thing in the right
orientation and then we're going to go
and move it over to where it belongs on
the engine and you're going to see this
little representation they're sort of
transparent representation of where that
physical combustion chamber is supposed
to go to in the engine and so if you
look at something like that just a GQ
this looks kind of cool we think it
looks cool most people who see it think
it looks cool but of course if you're
trying to fix something you know looking
cool isn't enough the question is does
this work is this really a good way to
go and do something and so what I'm
going to do is to tell you a little bit
about a study that my students and I did
to try to be able to figure out whether
they are can actually be shown to be
better than conventional which nowadays
in high-end maintenance means
computer-based documentation so a lot of
documentation provided in the military
now is done with stuff running on a
laptop that essentially shows you how to
put
form tasks and you follow one step after
another press the appropriate button to
indicate that you're ready to do to the
next step and essentially we're being
shown how to perform a task so the test
we're going to do over here is one that
involves those combustion chambers I
just showed you a fully completed
combustion chamber and the one that we
decided to do is one that involves
assembly the combustion chamber you're
seeing on the left the bottom part of
one is the top part of one at the bottom
the two of them assembled one over and
then the workplace in which we did this
in which we have essentially a set of
bins with three of those bottoms three
of the tops and the idea is to have the
system explained to you which bottom
which top and how to orient a top
relative to the bottom so it's correctly
oriented and then there's going to be
two places where you're going to stick
some pins in to finish the job normally
if this were being done for real there's
actually twenty bolts that need to be
inserted but augmented reality is really
not going to help with that so we wanted
to just point out two holes to put the
pins insider so we did a study that's
what's called a within-subject study
that means that each one of the subjects
tried things both different ways and one
thing we want to avoid is giving bias in
favor or against one way versus the
other so has the subjects basically
started with the augmented reality
documentation and then did the
conventional i'm calling it LCD based
doc and the other half did it the other
way around the randomized which chamber
bottom in which chamber top and which
pairs of holes a person was going to be
instructed to line up and put together
and you're seeing over here just a
couple of images of one subject in our
AR condition and then one subject in our
LCD condition so let's talk a little bit
about the AR condition so what does a
person actually see this is an optical
see-through display and what they're
going to see is what they see in the
real world through the optics of the
display and then overlaid on top of that
you can actually
see it on the monitor of the background
one of the eyes is the extra material
that's being overlaid and registered on
what she see in a couple of seconds I'll
show you some stills and I'll show you a
little video of what this looks like
actually running so what do we do to try
to explain to a person how to perform
the task and in our case we actually did
a whole bunch of things some of which
might maybe not be necessarily all that
useful we're kind of throwing in
everything but the kitchen sink as we
would say and hear what you're going to
see is and this is kind of a key to the
video you're going to see which will go
by fairly quickly you're seeing some
highlights of the particular bottom or
top the source and destination
highlighted in yellow on the left there
some motion paths that are giving sort
of an approximate idea of which way to
move the piece that needs to be moved
into place we have some arrows once the
top is on the bottom the system is
tracking these pieces as they move so it
can actually move you on to the next
step and so that little red arrow for
example means you should really turn the
top relative to the bottom in that
direction that's the most efficient
direction to turn it to get it to line
up and red means you're kind of far away
in this case the J has to line up with
the 17 over there and you can see those
are the labels which were basically made
to always face you and if you actually
look up a little more closely over here
who we've got a green arrow on the top
and bottom are nearly in place and
there's some little highlights you're
seeing about the holes that need to line
up and then when they really line up the
arrow goes away and the two little
highlights kind of merged into one over
there so now I'll show you an example of
what this looks like this is actually
shot with an optical see-through display
and the way we do that is we can't
really put a camera literally in your
eye and so we have a fiberglass head we
have a camera inside the head the head
where is the display and then to make
this particular video we had one of my
students was holding that head as if it
were his own head and another one was
kind of
reaching around to go and do this of
course that's not the way we actually do
the study but it's the way in which we
recorded the video so let's actually run
that and see what that looks like
probably want to lower the volume over
here maybe a little bit lower and so now
we're seeing through one eye and there
is the bottom that we need to pick up
grandpa Louie the system is tracking
these things using the kind of motion
capture the toilet system that's used in
a lot of filmmaking and now we have to
get the top part and then as you suppose
to the bottom the system knows ok we're
ready to go and Orion correctly and the
yellow arrow says that's the way you
need to turn things in this case lining
up that choo with the 19 over there and
as they get closer and closer Lee goes
to green and then ultimately it kind of
disappears and then we're going to go
and put a pin in the queue slot this is
on a little lazy susan so it turns
around easily and then put the pin in
the J slot you plug and we're done over
there so that's what AR looks like thank
and now we're going to see what the
quotes classical documentation looks
like and so this is an LCD panel up in
front of you we're actually doing some
of the same tricks we did in AR really
to make this a fair comparison between
AR and non AR we didn't want to have the
user have to say when the next test
needed to get done so we're tracking
things the same way and we're basically
automatically moving you on to the next
step based on the same motions of the
parts which unfortunately current
documentation doesn't do we wanted to
make believe we had maybe another turn
around the wheel for existing
documentation systems and so all the
images over there are actually being
rendered on the fly by the system from
very nice high resolution models of the
parts and we like to think we did a
pretty good job and was concerned
confirmed by folks who actually did
these things saw both of the the
different ways of doing things in
creating this kind of quotes classical
documentation so when you tell you a
little bit about our study we had 22
participants and what they experienced
is when they came in they got a little
introduction and we had a little video
prepared to explain things to them it
gave them a stereo vision test it's very
important for them to have stereo to do
this kind of near-field tasks like
putting that pin in for example and then
for each of the two conditions depending
upon the order in which they had to do
it they got an instructional video for
that condition they got a little
practice block of trying it out until
they got comfortable with it and then we
gave them a set of trials and when they
were done we then went on to the next
condition whatever that was for them and
then they got a questionnaire when it
was all over in which we asked them
questions about what they found
comfortable uncomfortable which one they
like the best etc and it turns out that
based on some hypotheses we came up with
during an earlier so-called pilot study
we hypothesized that AR is going to be
faster for the alignment and pinning and
it turns out it took just a little bit
more than around half the time as we
expected AR was going to be more
accurate and measuring accuracy by
whether the right holes were aligned
then a half whole width of each other
it's significantly better accuracy and
this actually very hard to do task if
you're doing it without the overlaid
stuff looking up at a picture in trying
to match the various features just as a
human being doing the matching on the
top and bottom pieces to get them to
line up right and as well people told us
in their questionnaire that they
preferred AR and they found it more
intuitive and one thing that we were
very happy about is that display as you
saw before that big bulky that was
around one kilogram display that they
were wearing that was worn only in the
air-conditioned in the LCD condition we
were tracking their head because we
wanted to be able to say things about
how their head was positioned and
oriented boy use a very very lightweight
band around the head it didn't weigh
anything near what that very big very
bulky head or display wait so having
told you a little bit about some of
these things we've done what I want to
do is to talk something tell you
something about where I think AR is
heading in terms of both things in the
lab and as well things out there in the
real world is I think this is going to
be something that ultimately is going to
be a fundamental part of the way in
which people interact with computers
because right now you can pretty much
any smartphone platform you can actually
download for free a variety of AR apps
but not a lot of people download them
and the ones who download them not a lot
of people actually use them so the first
thing I think it's going to happen is
that as I said this is going to become
ubiquitous AR is going to become a
fundamental way which most people
interact with stuff now the images i'm
showing you here are a variety of
applications some research ones like the
one in the upper left from a number of
years ago and a number of systems that
that again are you can just download
right now running on a variety of
different smartphone platform now the
problem with these is you have to take
your phone out and turn it on and hold
it up like this or in one hand maybe
like that and kind of looks funny and it
takes a long time to set up and
you know it's a lot of other things that
are wrong with this model so basically I
think this stuff's going to become
ubiquitous when AR eyewear becomes
ubiquitous and commonplace now when I
say that I showed you some pictures
before this is the one we used in that
maintenance experiment a thing weighs
around a kilo it costs a lot of money
you don't want to walk down the street
wearing this thing and it's
uncomfortable okay other than that it's
actually fairly nice the one at the
bottom much less expensive commercial
video see-through display and both of
these are really things that researchers
would use maybe if they were good enough
and cheap enough and better than they
are enough people playing games might
use them probably sitting down rather
than standing up because these things do
a pretty good job of weighing heavily on
your head and blocking your experience
or degrading at least for experience of
the real world so I think this is really
not what I'm talking about okay now
here's something another picture that
many who may be seen this particular
picture or variants of it this is
something called google glass and many
of you may recognize the person wearing
it over there and this is actually a
very very nice example of being able to
build something courtesy of current day
technology that can be very lightweight
self-contained the computer is actually
part of this headworn display little
tiny display positioned off to the side
and above one of the eyes it's a little
camera built into that and the problem
is well this can be a very nice way of
getting computational stuff into your
field of view but it's not really
tracking things in the real world it's
not over laying directly on them because
as you can see if he's looking at you
well he's not also at the same time
looking at that display he has to look
up to go and look into it it's not
overlaying the majority of the field of
view it's not really getting stereo
either and in fact nicer this is
actually owes a lot to a variety of
things people have been doing back in
fact last century to me that sounds very
wrong to say that but to you it probably
feels like oh of course that was like
not before I was born but a long time
ago but last century there were a number
of companies that made things that
didn't look totally different okay but
in both of these cases over here the
displays that are being warned are
literally just displays they don't have
cameras in them they don't have
computers in them and they need to go
out to external computation with a cable
as you're seeing on the right at the
lower left picture and in fact the
person on the right bottom at IBM
Research is actually holding the
impressively small for its time computer
that was powering this thing and so
really neat how advanced we've gotten
even though at first glance these might
look the same from the end of the 20th
century to 2012 but again I don't think
this is it okay now here we're getting a
little warmer these are kind of stupid
looking but these are both stereo
optical see-through displays you can see
the users eyes directly even though
there's a lot of bulk on the sides over
there but these both have a kind of
smallish field of view and again they
look kind of strange now here I think
we're getting even warmer these are all
working a prototype systems monocular at
the top over there from just around the
year 2000 and then to stereo displays a
one from a little company in Israel and
one from a rather large company in Japan
both of which are ones where you can
look the person in the eye and although
these don't look really gorgeous they
weren't things that have their
industrial design done by wonderful
industrial designers these are more
designed by the kind of people who did
the technology itself and there's no
reason why something that has that kind
of functionality can't be designed to
look really really gorgeous and
desirable and work really well and also
be really really inexpensive if it's
made and large enough quantity so let me
move on to talking about another thing I
think it's going to be important and
that's the notion of hetero
I think at this point you can probably
note i'm a big fan of the future
prospects a very small very powerful
wearable displays but on the other hand
i don't believe that that means that you
know everyone else can sort of pack up
and go home and we're not going to have
very large displays on walls and floors
and on tabletops let alone ones that you
hold in your hands and so this is some
work in which we put together a set of
different displays this is a rear
projected first generation Microsoft
Surface display which is the substrate
on top of which you're seeing a set of
buildings courtesy of being on Google
excuse me bing maps which are basically
being seen through a video see-through
display that is looking at the table top
surface and being tracked relative to it
and so I show you some video over here
this might make a little bit more sense
again we're looking through the video
see-through display all of those
buildings are positioned on top of their
footprints and then we can look around
and see some of the other fun stuff by
having a lab off in the distance there
and then we can do the classical things
of being able to go and scale and
translate and rotate as we move around
over there in this case we're just
putting up just plain building models
and here we're seeing the same kind of
thing but we have some additional
tutorial pictographic representations of
data coming from a variety of different
sites like Yelp for example and now
we're going to bring in a smartphone
that's also being tracked and we can use
that to communicate with the stuff over
there get more information about some of
the things we're looking at interact
with some of the pieces over here so in
this case you'll see that I can't come
forward as you're trying to get some
more information about it and finally
the last thing I want to talk about
thank you
lots and lots of student time put into
doing it that was Nick dead wall and
carbonyl desi OHS work and then I
neglected to mention that the stuff
maybe I did that you saw before that for
maintenance was work done by Steve
Henderson so now I'm going to talk about
collaborations I think collaboration is
really important especially as things
get smaller and smaller and we can walk
around in the environment and there's
one very interesting prospect I think
and that's for collaboration between
multiple people and systems in tracking
and modeling people and stuff in the
environment so we have users who want to
know their own location and some
interesting information like maybe we're
the good restaurants are and we have a
system that wants to model the world
including the users and their
interesting information and so is there
some way to sort of broker the users in
the system into getting them to work
together well you can imagine that
people want to find out where they are
and consequently be able to get some
really good information about things
that are richest where they are well
maybe they'd be willing to provide their
current sensor data including Mady
camera imagery history and things kind
of like a query in fact exactly like a
query of the sort that people use right
now with search engines and imagine the
system being able to go and match that
sensor data and the history from the
users against the database containing
lots of camera imagery for example with
first guess is being made from satellite
tracking from the orientation tracker is
that any smart phone nowadays would have
and then returning courtesy of doing
these matches into this very very rich
very large database the users precise
current position and orientation based
on matching against the material in it
and then of course also answering a
user's query like you know where we're
the good restaurants based on the kinds
of restaurants I like and then updating
the database with that sensor data that
was being used in part to go and do the
matches okay so in a system like this
who are the users well all of you guys
right you know many of you in fact
probably all of you have some kind of
feature phone many of you probably have
smartphones many of you might even be on
your end smartphone at this point over
here and imagine that those things that
i mentioned that we're headworn displays
and that have cameras in them suddenly
cost what a smart phone costs and then
maybe even less than what a smart phone
cost okay so basically imagine everyone
or at least a lot of everyone's and the
result is you can get the idea of a kind
of up-to-the-minute or even up to the
second augmented version of what Google
would call street view or Microsoft
would call streetside this idea of being
able to go and see imagery of the world
around you and now see your relationship
to it really really accurately with
added virtual stuff and in fact if you
have a lot of people milling around as
I've certainly seen you have in India
imagine that instead of having imagery
that was oh maybe a month old if you're
lucky more like months to maybe even a
year old the imagery was literally
really really up to date because it was
being augmented with imagery that people
had taken two seconds or even fractions
of a second before so all the shop
window contents are up to date you don't
get just pedestrian or road traffic you
actually know which pedestrians are
there and then something it's a little
bit scary right now if you don't want to
use a search engine will just keep your
fingers off the keyboard or don't talk
to whatever engine you currently are
talking to our typing too but if you're
walking down the street and someone is
passing you and their camera is
capturing you and their microphones are
capturing you then it's not just the
people who want to be captured but it's
the people who are just captured through
other people looking at them and they
based on face recognition natural people
up against their Facebook pages
recognition of clothing and gate I'll
coalesced across lots and lots of time
stamped georeference data and at least
in the US for example all this totally
legal because it's all publicly
observable behavior you have no real
right that kind of privacy
when you're walking around outside so
what this ultimately gets you is it's up
to the second augmented world model
think of this as being essentially the
ultimate social network and whether you
like it or not you're going to be part
of it and you know I think you could
maybe get from some of the things I've
been saying that there's some privacy
issues over here and you want to know
the consequences of screening searching
matching storing all of this stuff and
part of what some of you maybe you're
going to be able to do is to both work
on the technologies as researchers that
make this possible and also maybe if you
go into other careers work on some of
the legalities work on some of the
philosophy and sociology and just the
deep understanding of how this stuff
works and so I've told you a little bit
about AR technology and applications
I've talked a bit about future
directions and I want to conclude by
just acknowledging colleagues and
students and participants in the work
and the funding agencies that made it
possible thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>