<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Belief propagation with strings. | Coder Coacher - Coaching Coders</title><meta content="Belief propagation with strings. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Belief propagation with strings.</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/94hfI6qiI6Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so um we've seen some nice examples
of probabilistic programming and that's
great that means I don't have to explain
what problems thick programming is but
when you write a program you want to
have variables that aren't just numbers
and most machine learning models that we
build the variables are like either real
values or boolean variable values or
they have one of the seat as a set of
states right that's not very exciting
when I when I write programs i want to
have variables that are that are strings
or dates or images and so how can we
start to sort of move machine learning
out of this sort of numbers zone and
into more interesting objects so what
I'm going to talk about today is strings
so here's a piece of code coming out of
Microsoft and the NSA has backed out
some of this code but luckily you can
buy it from us and run it and so when
you run it it says hello uncertain world
and my question to you is what was the
blacked-out code so we've got two
strings a and B and we're concatenate
them together with a space internet !
the result is hello uncertain world okay
so I'm sure you're all thinking it
you're just so shy that you're not going
to shout it out but yeah so the first
string egg could be at hello I'm certain
or hello and the second string could be
world or uncertain world so how can we
solve this kind of problem using
probabilistic programming so here we
have the C soft probabilistic
programming language which we're working
on and again this is powered by
infertile Nets this is a new way of
driving infinite and so what we say is
suing a is random this is the key or key
word and see soft that says create me a
random variable given this distribution
this distribution we're going to give is
an improper distribution overall
strength so it just says uniform overall
strength don't worry about the details
and then the second string Bay is also
going to be uniform over all strings and
now we can do stuff with these unknown
strings with these with these uncertain
strings that we can we can concatenate
them together and then we can observe
the results so you can say observe so
the result is hello uncertain world and
infer back distributions over what those
strings were a and B so in this case a
gives us fifty percent hello uncertain
hello and big is fifty percent world
uncertain world and you can download
this and try this out this this is not
vaporware this is downloadable code so
let's look at a more interesting example
than a Hello uncertain well so let's
talk about and a more exciting operator
in string space string format or
actually quite a remarkably complicated
operator what string format does is it
takes a string with some playfulness in
it yes I've got a laser on here
somewhere can you see that so thanks for
stringing it has a placeholder in it
like this and then it substitutes in
some other string for that place on that
ok so let's look at a simple example
here so what I'm going to do is I'm
going to take say my name a name is
drawn from the distribution of all
capitalized strings so that's some some
uncertain capitalized strengths that's a
string with an uppercase letter and some
lower case letters after it and I'm
going to use that uncertain string
format it through string format so put
it in with that place holder is and
observe the result and the result here
is not too surprising it's one hundred
percent John it's a great result
let's look at something a little bit
more exciting what if we don't know the
template okay so name is still stringing
capitalized but template now has this
more complex prior so what do we what do
we think about when we have a template
will be typically a template the
placeholder tends to correspond to some
words that we were going to put in and
so we would expect it to be word
boundaries at either end so we can say
the template is some string that obeys
this this structure so it's like any
string plus an onward character plus a
placeholder string plus an onward
character plus any string okay and
obviously you can make more
sophisticated versions of this but now
we're going to take that random string
and use that as the template in string
format okay so now both arguments the
string format are uncertain they're all
just strings so now if I observe the
text my name is John what will the
result be the result is a hundred
percent a lot of inferring template is
one percent on my name is plate over
that's the only string that's consistent
with those priors I cheated a little bit
here right because there's two
capitalized words in this string um but
my prior said that I have to have stuff
either side of the name so that's how i
teach it but supposing i was unlucky and
the string was instead hello my name is
John now the cylinder guilty right
there's two possible placeholders that
could have happened hello my name is
blank or hello blank name is John the
system doesn't know which of my and John
is is the actual name and there's two
ways you could get around this problem
one is you could build much more
sophisticated models of what a name is
so you could have seen a user sensors or
something tues you can throw more data
at the problem so let's do two so into
what we're going to do is just a throw
more data at the problem and there's not
no shortage of text it
turns out so we just create a second
piece of text which also talking about
greetings and names but the crucial
thing is so it would love to a second to
the textile have second name this piece
of text where the template is now shared
the same template variable is used in
but in both places and now if you an
inference the ambiguity goes away
because once the result from the first
string is my old John and the second
string is my or Andy when you put them
together that means sorry um I got that
the wrong way around when the template
is either hello my name is blank or
blank name is is John or my name is
blank or blank name is Andy the common
one is my name is bike which is what we
wanted and you can go further so in
addition to inference with strings we
can start up these objects like dates
and now we need to convert date two
strings I mean to have to handle
uncertainty in that conversion so here
I'm now going to say I have some date
it's a date time that's see sharpening
it has a month and a day in a year and
all this kind of stuff so it's not a
string object anymore and so what I can
want to convert it to a string I'm gonna
have to format that somehow which is
going to involve some format string
which is itself may be uncertain and so
I can take that date pass it pass in
this uncertain string format so this
case I just picked one of two formats so
I converted into a string using one of
these formats um and then I've got a
slightly more sophisticated template
here that I probably won't go into and
then I just now format up that template
the two arguments name and date string
and so now if I do inference on some
text like fred was born on sex of May
1963 then we get back the name Fred the
date is a date time the format that's
written in this
drink corresponding to that date and the
template or from that from that short
program so this isn't very powerful
framework now that we're sitting in for
doing inference over much richer objects
than int and doubles so the title I gave
was belief propagation in strings so I
should talk a little bit about how this
works so you want to work in infant net
we have to make string distributions and
date distributions that look like they
would fit into standard inference
algorithms like expectation propagation
belief and belief propagation so in
order to do that when we're sending out
beliefs about strings or dates we need
to have distributions over those things
so what does the distribution over a
string look like it looks like this and
automata so I assume most people have
come across all familiar with or
automata our but i'll just briefly go
through them but automata has a set of
state a start state and some some some
of the states are marketers and the
state and then the edges are marked with
in this case distributions over
characters so here the first edge is
uniform over lowercase H either case H
and the rest of them are point masses at
particular letters with his hello with
either capitalization and we can
associate going on then we can also have
loops in our automata so we can say any
number of spaces before hello and any
number of spaces after hello and then we
can have alternatives so we could say hi
instead of hell I and end here instead
and we allow these epsilon edges that
say just instantly transition do not
emit a current okay and this actually
defines a distribution over strength as
well as it's normally seen as something
that accepts certain strings but if it's
a weighted automata it defines a
distribution of strings and these have
been widely used
in speech recognition for for a long
time but what we've done is make it so
these are the messages past inside
expectation propagation and that is
actually very useful because it means
that you can combine them with all the
other types that are in internet so you
can combine them into your PCA algorithm
or your LDL or whatever it might be and
do combine coherent inference across all
these data types at once so no longer
when you're faced with text do you have
to convert it to a bag of words before
you do anything with it okay so am a
little bit of details one one important
thing is that you can determine eyes
where your finite automata so that and
things are substantially more efficient
and if you use expectation propagation
and you get this backwards message
coming in its context message which
allows you to massively simplify a
little bit and speed up a lot of the
automata operations and that's a very
important trick and so what's good about
automata so they have some nice
properties their clothes under mixture
operations their clothes under product
operations and they close then the
concatenation operations and these are
some essential properties for doing
string in sequence operations if you
want to actually do stuff to them like
string format or substring all the kinds
of things that you want to do the
strings and sequences and well you can
do that using in a finite weighted
transducer which is the full of an
automata over multiple sequences and
I'll show some examples on the next
slide and once I've talked a lot about
strings you can actually use this
approach for any list at anything so
we've actually been using it for a list
of things other than character so a
strange list of characters but if you
wanna use it for a list of people or a
list of countries or a list of whatever
then you can use them for that as well
so just to give you some examples of
what transducers look like for the
concatenation transducer starts here and
the transducer
you sort of have operations on the edges
that map from from the various sort of
sequences involved so you copy from
sequence 12 sequin 3 a lot of times and
there's an epsilon edge and then you
copy from sequence to just sequence
through a lot of times and then you stop
and so that concatenate so you can't 1x
and sequence 2y three seasons three it
set now it's clever about using
transducers is that you automatically
have all the belief propagation messages
in all directions so where the
transducer you can start with Zed and go
to x and y or you can go you can
complete the projection of transitions
in all these different direction so i
define the transducer you're defining a
you have leave now at the ability to
send and compute the belief propagation
messages in all directions which is
pretty complicates pretty complicated in
many cases as you can imagine you have
an uncertain format an uncertain string
coming in to be passed and some
uncertain arguments trying to do infants
in that situation can get quite hairy
and trans due to just sort of normalize
it all I'm not saying there's no issues
the main issue is that they sort of can
get the the resulting Thompson get very
large and so we use some tricks like
projecting them down to simpler automata
as part of the EP step okay so just one
example before I run out of time so and
this is a problem that lots of biomes
petitions have looked at which is motif
finding so the idea is that in your DNA
there's some motifs that means something
as a motif is a small sequence of bases
and so people have built mighty Finers
and what I'm going to show you is a
motif finder on one slide okay so here's
a motif and what you find is that that
it's not the same sequence of characters
every time but there's a lot of
similarity between the sequences and you
will try and discover them so um here is
our data this is synthetic data that I
just made up and I'm sure you can all
see there's a motif in each one of these
sequences everyone spot the motif you
see it yeah
you got it yeah it's really quite hard
isn't it so so let's let's write a
program to find the motif in this data
and here it is so um this is the entire
program well with some boilerplate
missing so we're going to assume some do
if a prior has been provided for our
motif that's just going to say what's
the probability distribution over the
confidence of the of the probability
distributions are the basis at each
point in the motif so I'll just draw
from that that's going to give us our
motif probabilities which is which
defines the motif of each of those
sequences you saw in the previous slide
we're going to instantiate that no T so
find the exact sequence of base pairs
that are going to appear in that
sequence by sampling from the motif
probability distribution for that
character then we're going to choose
where the motif appears inside that
sequence and now the string stuff comes
in so now we're going to create a
left-hand string of background
characters and then we're going to the
bottom can take a right hand string of
background characters and then in the
middle are going to plug in this motif
okay so just a simple motif generating
process then we observe that the
sequence is equal to left past motif
plus right and now we can go back and
apply that to our to our synthetic data
and boom these are the highest
probability positions inferred by that
program and now you now hopefully you
can see the motif in all of these
sequences that's been found by the
program they're not identical in every
case and in one case it the highest
probability position was not actually
the one that was created in the
synthetic data set because actually that
one does look more horrible things huh
and this is the motif
that was used that was inferred back
from from this data it is pretty close
to the ground truth okay so a motif
finder in one slide how inventive of
strings enables a wealth of new machine
learning applications particularly when
the sequences in volatile in
bioinformatics but also when you're
looking at text information extraction
translation all these kinds of things
and so you can try this out right now we
got this into the last release of in
phila net in the autumn so you can
download it and it's early days for this
feature so please do give us feedback
and and we very much welcome sort of
input on and where to go with this next
thanks very much question time
interesting priors over strings and I
and stood up I sure what you mean by
string dodge uniform and any sample from
that yes so you can specify any prior
over string that can be represented by
an automata now your tongue has leaps in
it it's going to display an improper
distribution and we wrestled with that
for a long time do we need to make sure
that all our distributions are proper
and it turns out to be a nightmare to
try and do that and actually a lot it's
ok to have improper distributions as
long as the result is proper and so
that's what we do so we allow improper
and proper distributions to coexist so
anything that's expressed will buy an
alternative so the main things that are
not easily expressible by a Taunton are
things like a string that contains these
end strings in some order right that's
now common totally expensive to
represent fire burn out of lots of cases
things are quite compact and efficient
did it
he said I'll tomtar closed under mixture
product and concatenation is do you need
division to do EP of automata that's a
good question you can with a slight of
hand get away without doing division as
long as you only care about the
structure of the automata so what you
can do is you can take an automaton and
say give me a automaton which is uniform
over the support of this automata me and
then you and then what you can then you
need to care about the division
basically so you can then multiply that
in as the inverse method and you'd have
to divide it out because it was all
either 0 or constant and that's the
trick that we use we'd love to be able
to divide them but we can't right now
yeah so for your multi-fiber no vision
particularly at the moment nothing we've
done a certain amount of work on it and
we're using it on an internal system
that I can't talk about a large scale so
it's it's somewhat efficient but the
main problem is that you get like this
state automata blow up and so we've done
some work on projecting I say projecting
automata that have high complexity down
to low compiler complexity ones and some
more definitely more work to be done
there and I'd be very interested if i
wanted to work on that problem that's a
really interesting problem i think
there's some really big performance
gains out there it's decently fast for
for now its use ibly fast if you keep an
eye on the automata sizes and so far
pushdown automata or other more
complicated us and in choosing the
particular class or Tom's that we did we
sort of we originally chose a simpler
class and then variously use cases sort
of pushed us into this class I think
it's a really interesting area of
investigation I know that that this
style of automaton has
limitations the terminal ization helps
and so forth but it you know there's a
lot of backtracking needed in certain
cases that can be very expensive so I
think I think it's for a night to be
very nice to explore other kinds
essentially we're using it as a function
that gives you a weight back for a
sequence and so anything that could plug
in there that supported the same set of
operators but had better efficiency but
bounds or something like that would be
would be super useful yes what was
modification what does that mean a long
piece of automata is um what in the non
promised it case it means finding the
automata that accepts all strings
accepted by both of these or tom I and
be so it's the intersection in the
probabilistic case it means you say
there's two probability distributions
you must buy them together in other
words you want to get a distribution out
such as the probability of any sequence
under that distribution is that is
proportional to the product of the
probabilities under the two input
distribution great let's thank the
speaker again thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>