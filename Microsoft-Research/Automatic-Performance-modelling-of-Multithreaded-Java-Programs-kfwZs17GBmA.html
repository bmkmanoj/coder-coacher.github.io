<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Automatic Performance modelling of Multithreaded Java Programs | Coder Coacher - Coaching Coders</title><meta content="Automatic Performance modelling of Multithreaded Java Programs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Automatic Performance modelling of Multithreaded Java Programs</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kfwZs17GBmA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
right we're going to get started this
morning I'm pleased to introduce
Alexander tavo who is visiting us from
brine and has a background also before
he started at Bryan he was at in Windows
so he's got a bit of an engineering
background as well and he's going to be
telling us this morning about some work
he's been doing on automated performance
modeling of multi-threaded programs and
related work good morning can you hear
me thank you very much for this
opportunity to talk here so I'm going to
present my thesis work and this is about
automatic performance modeling of
multi-threaded programs so in fact
during my work what I did so I developed
a an approach model specifically aimed
to simulate multi-threaded programs and
methodology to build these models
automatically for large complex programs
like Sun flow for example it's a 3d
render an industrial program and tomcat
which is apache web server so we
developed this methodology and we
verified it with a set of large
industrial applications so this is a
brief agenda of my talks first of all
I'm going to talk a little bit about
motivation of this work I briefly touch
approaches existing approaches for
performance modeling I'll talk about
definition and structure of our model
then I will tell you how we build those
models automatically and finally I will
present you some results about
revelation of our model so the
motivation he is simple understanding
performance of multi-threaded programs
is really hard problem these programs
they engage in synchronization login
activity some sections of their code my
run in parallel some sections they might
run in not in parallel like it's a
single to edit code and these programs
they utilize resources of the computer
concurrently like they can currently use
cpu hard drive Network and as a result
the performance of this programs is
going to be really hard to understand
strictly speaking there's going to be a
very complex dependence and only a non
linear dependency between the
configuration of these programs and
their performance
here under the configuration I mean both
characteristic of the hardware where the
program is deployed upon the program
configuration options itself like the
number of working threads for a program
or the size of the internal q I as one
of the parameters of the program
configuration is the workload parameters
like if you have a web server how many
requests per second the web server gonna
take so all these parameters they
constitute the configuration of the
program just to give you an example
let's suppose that you are trying to
deploy a web server suppose that this is
a simple case the web server is going to
serve a request the static web pages and
you are wondering how many work in
threads you should have for that web
server how you should configure it or
how many requests per second the server
should receive from your load balancer
because if you're going to have too many
threads you're going to for example hit
the input-output bottleneck of the hard
drive but if you got too few threads you
will get your CPU saturated quickly and
same with with workload if you have a
low workload you end up with a low
response time which is good but you will
have a low throughput which is bad and
if you will receive too many requests
per second for your web server you're
going to end up with a high throughput
but it will be the miserable response
time so in fact answering this question
in practice it's a really hard thing to
do because as I say that the answer
depends on the configuration of the
system it depends on the architecture of
the program something that we cannot
really control but it also depends on
the parameters of the hardware it
depends on the characteristics of the
workload and it definitely depends on
the configuration of the Webster a good
solution to this would be building a
performance model of the system
hopefully you can do it fast and the
minimal effort so this performance model
it should predict the performance of the
system the performance of your web
server for a wide range of
configurations here and it will act as
an Oracle so you can use this model to
for example deploy your web server
automatically to ask what if questions
about it performance maybe to detect and
performance anomalous in it
so it's really convenient so with regard
to approaches that people take nowadays
to performance modeling so in general
you can say that you know modeling the
performance is trying to find out the
dependency between the configuration and
performance of the system in some cases
people they try to know to model this
analytically so this requires strong
mathematical skills good understanding
of the system which rarely coexist in
developers and analytic models is just
set of equations what is matin be good
about it it's if you have a very large
very very complex program with a very
complex behavior it might not be the
good at capturing its details of ECE of
each behavior another approach is
statistical models when you just run
your program in many different
configurations record performance for
each configuration and try to
approximate it with like a new L network
or non-linear regression so this is
better you don't need to know about your
internals of your system but you have to
run the system in many different
configurations which might not be
possible in production environment so
the ultimately good approach to this
would be to build a structural model of
the system of simulation model of the
system which would try to replicate the
structure of the system in its behavior
what it can be a simple example of such
model is a queuing network right you
have you trying to you know to replicate
the structure of your system you're in
the structure of your queueing model
there are other like frameworks for this
like a base framework palladio component
models and so on so the good thing about
these models is that they are very
flexible they can represent a really
complex behavior on the structure of the
system but they're really hard to build
you have to know the details of your
system fairly well you have to know with
sauce demands of of your program and as
a consequence most of the simulation
models that are built currently most of
approaches to automatic building of
these simulation models they are done
for modeling of message passing systems
and if you will try to apply this
message passing system model if you try
to you know to build just a model of web
server or some scientific computing
application
which relies on logs which relies on
multiple threads and has some thread
interactions you will have a sub optimal
performance like for example here people
try people didn't take into account the
synchronization operations in the system
and they'd like go to fifty percent so
our contribution is a simulation-based
performance model for multi-threaded
programs which is specifically designed
to take into account synchronization and
concurrent resource usage and we predict
performance across multiple
configurations with a high degree of
accuracy and we build this model
systematically oops I'm sorry so now I
will define the structure for model how
it looks like and what is do so for the
purpose of the modeling we represent
computation represent our program as
request processing so it naturally suits
a model enough servers interactive
programs but it also allows assimilating
a wide range of applications like
scientific computation applications
financial applications because they can
also be represented is this request
processing system we redeveloped a
three-tier more discrete event model for
simulating this model these programs so
at the high level we simulate the flow
of the request for the program at the
middle level we simulate delays that
occur in the program working threads and
at the lowest level of our model we
simulate hardware &amp;amp; locks which exist in
the program so the high level model it's
essentially a modified queuing model
some flavor of the queueing model so it
simulates the request flow as the
program as the program processes this
request so here in this model queues
they correspond to the buffers and cues
that are present in the program and
service nodes of the queuing system they
represent the program working threads
for example here you might have a model
of a web server which have an accept
read which accepts incoming connections
and connections are put in the queue and
then one of the working threads it
speaks up those connections and
processes them one important aspect when
we are talking about multi-threaded
programs is the notion of tread pool so
tread pool is the set of treads which
have the same functionality and can
process requests in parallel and very
important because the size of the thread
pool it becomes one important factor one
of the most important factor which
affects the performance performance of
multi-threaded program so usually this
is an important factor in the
configuration of a multi-threaded
program it's an important configuration
parameter so as I said that this is one
factor that affects how fast the program
can process requests namely how many
service nodes you might have in your
queue in model right so the more nodes
you have in perspective the faster you
can protest request another factor which
affects the performance of the of the
multi-threaded program is how fast these
threads can process requests does it
really take long time or they can do it
really fast obviously any questions so
far these trades experience contention
when you're accessing some absolute
listened so many many this list is
incomplete yes this list is incomplete
and this is where I'm talking just about
high-level queueing model at the 10,000
feet over you but you're absolutely
right the question is also when we will
talk about like how fast the program is
processing request the question is what
this program is doing inside what is the
behavior what are the computations that
are performed by the by the treads and
this is what we simulate with the
mid-level model so mid level models they
simulate computations which are
performed by working threads and we
simulate those as probabilistic call
graphs so they essentially capture the
call graph of the tread vertices of
these models of these graphs they
represent pieces of code we call them
code fragments they represent pieces of
code in the tread and edges they
actually correspond they represent the
request flow in which sequence these
code fragments they're going to be
executed so here we simulate request
flow probabilistically so say after
executing this code fragment this is of
code the program can with a probability
of 0.7 engage in a say disk i/o or it
can go directly and try to so to say
excess elecric you so we simulate these
computations the net in a trade
probabilistically so here it's just an
icon for lock I will come to it in a in
a second mm-hmm so now I just wanted to
give you an example what are the code
fragments so as I say that we split the
code of the tread into these pieces code
fragments what are the code fragments so
the code fragment is the continuous
piece of code in a program which
performs one specific activity that it
performs either computations or
input/output like disk reads
synchronization operations or reading
and writing request to the queues of the
program so in essence we say that these
types of code fragments they represent
the different sources of delay what can
cause the delay request processing delay
in the in the in the multi-threaded
program so just for an example here's an
example of simple program which sort of
tries to compare images so it reads a
request so it so it acquires the lock it
reads an image from the file then it
gets the another say image from the
queue compares this like reads another
in add another image from the file
compares the these images and put the
request betting back into the queue so
like some part of this like request pots
in queue in program so the code
fragments you can find here so when you
are acquiring the lock here it's a
synchronization code fragment so this is
a piece of code that is doing exactly
synchronization then
yes then once you acquire the lock you
read something from the file you have a
disk i/o operation so this is in the
model this is represented by the input
output code fragment next when you are
done releasing the lock it's another
synchronization code fragment so there
are two different ones your
correspondent Lee getting the request
from the queue this is what we call an
in code fragment which corresponds to
access in the queue in the higher level
model this is another read code fragment
it's another input output operation then
we are doing image comparison which is
the CPU intensive raishin it's
represented by the computation code
fragment and finally we push the result
in the queue which is another Q
interpolation it's an output code
fragment so this is an example of
program student how we can divide it in
code fragments and this is
correspondingly going to be the
probabilistic call graph for this for
this program and this is going to be a
like a very simple like straight line
any questions a synchronous oh yeah so
right now we're working on this for some
cases what we can found like like in
Java you can represent it as a
synchronous sir I oh but it might be in
certain k in other cases you might be
able you might have to handle it in a
different way so with regard to
performance factors what can from the
standpoint of this mid-level to head
model obviously the factors that
determine how to read how fast the tread
can process request one factor is the
structure of the call graph which code
fragments are present here which
computations are performed and in which
order they performed like if you have a
loop like for example hidden many times
from a hard drive it's definitely going
to take time so this is one factor
another factor is how much time these
particular code fragments how much time
they need to execute so in terms of the
model we say that execution of each of
those code fragments it introduces some
delay in the system and what is this
amount of delay it can really different
because of resource contention like if
you have multiple threads accessing the
same hardware resources the CPU you
might start getting contention between
them the execution time will start
growing right if you will have many more
work interests than the number of
available cpu cores so the same about
looks if you have multiple locks
multiple threads competing further lock
it will become more and more time they
will require more and more time to
acquire that look so the execution time
for this code fragment for example here
which represents acquiring the mutex it
will also start increasing right so from
the standpoint of modeling this means
that we must simulate blocks and
hardware in the multi-threaded system
and this is exactly what we do with the
lower level model so our low level model
it simulates locks and hardware it does
it in a following way so instead of
explicitly specifying execution time for
each code fragment we rather specify
parameters maybe it's a resource demand
for a CPU code fragment or for a disk
i/o code fragment so these constitute
parameters of the code fragment and when
the code fragment when we execute it
when the model needs to know how much
time it's going to take it gets these
parameters it sends them to the
corresponding low-level model and low
level model it sort of tracks the state
of the system contention of these logs
and based on these based on the
parameters based on the resource demands
of the tread and based on the state of
the system the low level model it
computes the delay what is the delay
necessary for this code fragment to
execute increase number in principle
years this definitely lows this so this
I believe it it's quite flexible think
you know to model another question so
far about the structure of the model
along if you have a model of contention
lock in each ear which lock it is just a
green box don't tell you enough
absolutely ups and that might itself be
hard if you just look at the source code
you say it's called loc k that doesn't
help me know when it's the same one this
is what I'm gonna tell you in the next
slide and the other problems is another
question say modeling the CPU shaky
there as well yes we do so this is in
fact part of this model CPU schedule or
are you scheduled we have a simple
models for that yes so they form a
hierarchy at the higher level you have
this model of you in system high level
model the high level model it calls when
it works it calls the model of working
threads and when the model of working
threads work during it work it's calls
the models of the hardware so it's like
strictly hierarchical system so if you
don't mind I will move forward because i
have to finish a little bit early today
so with regards to model building just
as you said you're absolutely right in
order to model locks you have to know
what are the log stop so initially we
did it manually like trying to build
those models manually and it quickly
become obviously is not practical you
have to scan the power the source code
of the program you have to understand it
it just takes lots of time it's very
error-prone so it's obvious that in
order these models to become practical
we need some automated solution but
automatic modeling is very difficult
because it needs information about
program semantics and it information
must be retrieved from the program
itself in the ideal case not from the
developer not from anyone else but from
the program itself so now I will shift
to the next part of my talk which is
about automated model building how we
can do this automatically so in order to
build this model automatically we need
following pieces of information we need
information about treads and thread
spools which are presented in the
program we did information about how
these threads communicate with each
other through pues so we need list of
q's and really need list of these in an
hour
code fragments in a program that access
those cues we need semantics of
synchronization which means we need know
about all the locks in a program there
types and about synchronization code
fragments which represent
synchronization operations performed on
district on these logs by treads and we
definitely need the information about
computation sent I owe these code
fragments and we need to restore the
structure of this code graph how these
works altogether so probably the hardest
part here is finding the semantics of
parallelism what are the locks what are
the cues in the program how treads are
using those locks and Q's so in a
general case it can be a very difficult
problem because you know you might
implement these locks in a myriad of
ways you can for example use like
low-level constructs like in Java these
critical sections or synchronized
sections and monitors so you can use
some high-level libraries for that so we
figured out that this problem is
becoming much easier if the program uses
a specific implementation of locks a
specific library that implements looks
like Java util concurrent in Java or
system trading in a beloved C sharp or
some other library which provides
implementation of locks just for example
wide simplifies things let's just take a
look at the Java program so in a Java
program which utilize java.util
concurrent alloxan cues adjust instances
of Java util concurrent classes and you
can infer the type of the look like is
it a semaphore or is it barrier or
whatever you can infer get just by
looking the class name and with
regarding to the synchronization and in
and out code fragments which represent
cause 2 q's you can just detect them by
looking at the method names because
these operations are methods on the
corresponding Geritol concurrent classes
for example and parameters of these
operations you can also infer x values
of arguments which are positive to those
methods and again if you will take
another library
will be also other way to know too for
example in system trading I believe it
it's the same approach in boost it's
very similar so you can just buy known
the by sort of looking at the specific
implementation of locks you can get all
this information from the program by
static and dynamic analysis which heart
with high degree of accuracy to know
that contention what do you know that
lock contention yeah you need to know
when two variables refer to the same
value you know so that's not it's sort
of more analysis to be done Oh analysis
we look at the so with our like
implementation in Java we look at the
reference to the Java object so in Java
for example every object can be like
identified by ID so yes you have to do
alias analysis and we are doing this you
yeah essentially with so steps for with
regard to steps for building model our
methodology it involves three distinct
steps for model building so first of all
we run the program template stack and
detect red poles in it then we scan the
source code of the program and discover
code fragments in this source code then
finally doing the third step is the
dynamic analysis we instrument the
program run it in a specific
configuration and get all the rest of
information from the model trace so I
will briefly review briefly briefly
touch all these steps so the first steps
is at red bull detection here we detect
water that'll help boost in a program
and what can be the size of this thread
pools so we rely on the observation that
usually trade threads from the same
thread pool the execute same methods and
so what we do we take snapshots of the
program as a trance we get the structure
of the call try for this program I mean
which methods are called and for each
you said we record a set of treads a
tuple of threads which executed and
frequently encountered tried to post
like here for example you can see
pattern that for example these methods
they executed by thread one these
methods they executed by thread two and
three and these methods they executed by
three four and five so these are
essentially trade pools represented the
data present in a program so in fact so
this is like an overall approach to
triple detection in fact we do in much
filter in but in essence this is how it
works so with regard to so once we done
detecting thread pools we are doing
static analysis which during which we
detect code fragments so we analyze the
source code of the program and track
specific construction it so for example
calls to the program logs as it shown
here as I said it these are becoming
synchronization code fragments in our
model and calls to the program queues
these ones are becoming in and out code
fragments we also track a initialization
construction of these locks and Q's
because in this sense by looking at the
adults constructors we can essentially
infer what are the locks present in the
program again assuming the use under
standard libraries you have currently
best human this because otherwise the
technique of q's can be a big topic on
its own yes my advisor also didn't you
know this is a whole different separate
kind of forms we can open so this is how
you detect this locks and cue in
operation during the static analysis we
also detect the input/output operations
here in this case we track calls to
certain functions and methods which can
access the hard drive which we track
both calls that I cancers access the
system metadata and that can actually do
file i/o operations and then once we
detect all of those we instrument
program and the by inserting probes
between on the boundaries of this code
fragments so code fragments this so
these probes each probe is identified by
identify separate ID and we use this
pair of ID's to identify particular code
fragments here so when the program runs
it generates a trace so the trace
includes the number of the idea of the
probe which has been executed the time
when it was executed and also additional
information which is used to establish
the locks and queues in the program and
their parameters so but in this trace
the code fragments they represented as
coincident probe hits like here so these
are sort of overlap in the trace so most
code fragments that we knew from the
static analysis these correspond to code
fragments that synchronization
operations to hard drive I operations
and to to Q excesses and all the code
fragments that reside between those they
represent computations so we don't
explicitly track computations during the
static analysis but we detect them in
the dynamic analysis here the trace also
provides us parameters of code fragments
so for example tracking the calls to
constructors and initializers of logs
and cues it allows us to establish
parameters of vlogs for example here you
created a semaphore with number of
permits equal to 4 and this initializer
by looking at the value which is
returned by this constructor we can get
the lock ID for example so this is lock
resolution right it's implemented in
this way and every time you see a call
to this particular log you can establish
that this is this locker corresponds to
this particular semaphore for example
and correspondingly if the semaphore if
this synchronization operation involves
certain parameters like timeout these
parameters they will also be
the trace so you can we can get it from
the trace as well or with regards to
other types of code fragments like CPU
code fragments and input/output code
fragments we also retrieve their
parameters from the trace like for CPU
it's easy it's like the amount of CPU
time which requires the code fragment to
execute for input/output code fragments
it's a little bit harder because for
those parameters of those are sequence
of low-level hard drive accesses
performed by that particular particular
code fragment and those we can retrieve
in kernel mode only but we essentially
in our work we use the cross correlation
to to match these kernel mode operations
to i/o mode to user mode I operations
and get the parameters of i/o code
fragments one last step that we need to
do is to restore the reconstruct the
structure of the call graph so we do
this by just tracking like for example
this particular code fragment as one we
track which code fragments were executed
were ever executed in the trace after
this particular code fragment and this
gives us the both the structure of the
call graph and by looking at the
execution counts you can infer the
probabilities of transition from one
code fragment to another and this gives
us the labeling the label infrarecorder
graph the probabilities so this was more
like a high-level overview of what we've
done in fact this detection this
automated model building is a much more
complex problem and it requires
additional steps additional cleanup for
example you need to address the blood of
the code graph of the of the call graph
if you do it this sort of playing form
you will end up with a very large and
readable and recognizable model you may
for example need to look in the
libraries of the program or because you
have to represent them separately in a
sort of you have to address them as you
build a call graph but if you have any
questions about this I will be able to I
will happily speak about it offline just
because of lack of time I cannot go in
the deep details now
so this was about automated model
generation do we have any questions
about this part that you are you looking
the source code on one hand and you're
generating dynamic traces on the other
so and in your previous slide you had
something about well we could know the
lock ID yeah right but of course the
same source line my access many
different locks if you're walking over a
tree in locking every node you say right
so there isn't a simple one to one
correspondence oh no it's not simple
okita program source code is yeah so in
this case you can yes it can be hard so
one approach to look at this is to again
probabilistically that you can be
certain probability access like this
particular code fragment it might with a
certain probability access lock another
approach is to try to establish some
determinism we do establish some
determinism in the program by certain
degree in order for this case but yes
this is some case that might might be
taken into account probabilistically do
something yes so we sort of like
probabilistically try to know to say
that there is a certain probability of
calling this particular look if you're
executing thing and proving what data
are using cruise for example the lock
problem is completely different if
you're working in a small data set
locking each element then you have high
lock contention if you have a very large
woman it's completely different so what
data are using for that for what excuse
me closer for the dynamic execution
where you extract information I mean you
what your workload nothing yeah so it
really depends on the structure of the
program like looking ahead for the
programs that will looked at the
overhead was typically under ten percent
with regard to execution time is this
like what you asking data are you using
when you're executing it for when you're
driving the workload for the program so
in fact we are trying to like for this
part we did like for the web server for
example example we just send the request
of the Webster
to the instrumented web server and then
the instrumented web server just when it
runs so it just gives you the trace of
flocks and the same regardless of the
workload regardless of the type of the
program the sort of the general idea the
structure of this instrumentation it
remains the same for testing web servers
web service so right now we used
synthetic better so we use to like like
speaking of evaluation we mostly use
like a Wikipedia data set to host our
web server and we try to generate
requests randomly to that so we didn't
use any standard benchmark so essence
the bent of the type of the benchmark it
will correspond in terms of our model to
slightly different call graphs and two
slightly different resource demands so I
will talk about evaluation in a second
if you don't mind so if you have any
questions I will be happy answer to them
but if you don't mind I will just shift
to the revelation partner so with regard
to our work our primary concern with the
accuracy of the model so we try to
reassure accuracy in the following way
we build the model using one
configuration of the program like one
combination of working two ads and the
workload in a case of a web server we
use this configuration to build the
model then we ran the program in many
other configurations we rent the model
in the same number of configurations and
for each configuration we recorded the
predicted an actual performance we
compared them we calculated the relative
error hopefully for a good for a good
program for a good model the relative
error should be minimal in this case we
can say that the model predicts the
behavior the predicts the performance of
the system with a high degree of figures
and you change the configuration where
you trying different workload so the
structure of the workload was same like
the probabilities accessing web pages
was the same so lie
addressing the different like shifts in
the workload it's a part of our future
work that I hope to work on the
intensity of the workloads the number of
requests per second was different so it
was one of the configuration parameters
split speaking forward so with regard to
the test programs I mean we tried a
couple of those here I want to
concentrate on in large industrial
programs that we try to model it's a Sun
flow which is a 3d render that has that
parallelized the rendering across the
pool of working threads and this is an
Apache Tomcat which is the large complex
web server in the servlet container for
tomcat we tried to rebuild two models
for like two distinct configurations of
the Tomcat one of them was the model of
the observer in this configuration
Tomcat was just employed as a regular
web server to edit to host like a number
of static web pages it's a disk i/o
bound world Claude and the other
configuration was a servlet container
it's a CPU bound workload in which case
the Tomcat it was generating a PDF
documents which is CPU bound operation
so these are like medium to large
programs for example Tomcat it's like
200 to 300 thousand lines of code the
number of probes it depends on the size
of the program for like the Sun flow it
was like about 400 probes for the Tomcat
it was close to 4,000 so with regard to
execution time the overhead was from
like two to seven percent and in general
our model it runs in general from 10 to
1000 times faster than the actual
program the overall speed up it ranges
from I mean it can really depend on the
structure of the program also for this
study were mostly concerned on the
accuracy and validity of the modern
speed up can be increased in futures
significantly so with the hardware
configuration rear end this on a pretty
much standard like a commercial hardware
is the quad core CPU system for
frame so these are results for the sun
flow the configuration parameters that
we try to take into account in our study
was the number of active CPU cores that
were available to the system so ranged
it from one co 2 for core system to see
how programs responds to this and the
number of working threads which the web
server ahead so as you see this test it
really tells you that interview from the
hear the actual performance by the way
is showed in red and the predicted
performance showed in blue and these
charts they compare like how actual
performance it's close to to the to the
predicted one and by the way the error
bars here they they show the standard
deviation across across the Hogan yes so
it this is easy indeed so this program
is in terms of parallelism in terms of
like high level parallelism it's
perfectly parallelizable almost
perfectly parallelizable so but I would
say that the important part he is that
this was generated automatically so the
model was the our framework was be able
to you know to pass through this program
to capture all the semantics of
synchronization and to you know to
translate into valid model which was
quite accurate in predicting the
performance of the program so with the
tomcat each this is probably a much
larger program with a more complex
behavior so here the parameters the
configuration parameters for tomcat was
the number of working threads for there
for the system again and the workload
the intensity of the workload how many
requests per second the web server was
receiving and here as you say that we
are interval predicting not just they
mean the average response time but
although also the range the sort of the
variance in response time that the
server or the server has in this
particular setup
and with the throughput I would say this
is very good thing that we accurately
predict the point of saturation here so
at this point the server which is the
saturation point and this is what you
probably would care about it would
really concerned if you deploy it in a
in the industry yes lytic model j this
will be here in modern how much at
reverse with that bathing in the
simulation Oh first of all you know
before getting the simple analytic model
you would need to get the resource
demands of the system right so this is
one thing with the analytic model it's
hard to say honestly we didn't do this
study what so here the next part is the
run in the web server in a higher bound
setup so here what is interesting here
is that some part of the resource
demands of the web server it's I oh it's
disk i/o but some part is the CPU demand
so for example for here again the
parameters they work load intensity and
number of working threads like here if
you would compare one working thread and
for working trails you will see that the
performance here have increased because
about the corner of the world the server
is doing its CPU activities and here you
have one hard drive which you know it
won't give you a speed up when the
number of working towards increases but
this cpu part of the workload you can
paralyze it across course and you know
in this case the model correctly
captures that there will be some speed
up due to the cpu cores and i would say
in this case you know capturing this
type of behavior with a simple queuing
model it would be probably harder
because by the way with the queuing
model with a classic queuing model you
will assume that in many of those that
you know the resource demand the service
time by the service node is the same
right
and the resource demands of the service
time the service time certainly it will
be like when you have this hard drive
problem when you will increase the
number of working treads you will end up
with a bottleneck on the side of the
hard drive right so for each of those
threads the service time by the by the
program treads it will start increasing
increasing increasing in essence more or
less proportionally to to the number of
working threads because of the
contention so with classic queuing model
you can hardly simulate this because the
process in time you know by by the tread
it will be different across
configurations so this is the throughput
for the web server which is again we're
sort of mood and prediction the
saturation point with which can be as
you see it's quite different across the
number of working treads and a part of
this as I say that we simulate we try
down where our approach we tried it with
a different number of with the different
programs and we also go to decent
results here but I can show it here
because of lack of time so with regard
to comparison to other systems like it's
really hard to compare because there are
so many different programs and different
types of programs and different types of
models but still we are sort of
somewhere in the middle so we're still
sort of correspond to the state of the
art so I would say that there so to
summarize least a little bit i would say
that the contribution of this study is
that the model was particularly
developed to simulate multi-threaded
applications these models are built
automatically for large complex
industrial applications and they were
they were able to accurately predict
your performance in a variety of
configurations so from like findings of
this research I would say that for to
simulate the multi-threaded systems
accurately you definitely need two
models log behavior and
we're behavior and accurate data
collection is very important here for
example here in the Tomcat like ten
percent error in collecting the
parameters resource demands here it
would correspond to ninety percent
prediction error here so this is
something that we quickly figured out
that the accuracy of data collection is
very very important here that in order
to be useful models mod be simple and
compact that's for sure and another
thing that we also become aware is the
debugging of performance model is very
hard it's very uncivil thing to do so
maybe there is some area to research to
look at the bugging of this models in
particular so as I say that currently we
have a number of assumptions on the
program that with regard to generation
to model generation we assume that the
model has standard implementation of
logs and q's some sort of library we
also assume that the workload
characteristics like the probabilities
of heat in the particular web pages it
doesn't really change in time that there
is nothing significant running in
parallel like you don't have to programs
deploy it on the same machine it's more
like a limitation of current
implementation but not a principle
implement principle limitation and the
hardware it doesn't really change
between configuration as we say that we
tried with a number of different number
of CPU cores but you cannot really
simply take it from one machine apply to
another on a separate memory
architecture and you will get accurate
prediction so we are aware of these
limitations and we want to address them
in a future work so the main goal of our
future work is to develop a more
flexible approach to modeling which can
allow us to simulate more wide range of
programs and workloads because in this
study we were mostly concerned about the
accuracy now we want to know to make
this more broad so first thing that we
did we are doing and we're trying to do
with my advisor steve race is to infer
semantics of complex look like which
were built from the lower level
primitives like cues and semaphores and
so on
so there was a preliminary paper and
this published second we also really
want to address the changes in the
workload so we will either like updated
model parameters in line from the
running program or employ a hybrid of
discrete-event a statistical model we
want to address changes in hardware and
one particular thing that we want to
address is the to see how changes in the
program itself how they affect the
performance so this I would say it's an
interesting program the interesting
problem that we want to look at because
you know it would set the criteria for
model construction another approach to
you know to this do to make this model
more flexible is to shift from this
completely automatic approach that we
have now two more automated modeling
like one approaches think of designed
for simulation like in which way you
want to design your multi-threaded
program so it would be easy to you know
to understand so you could easily
understand in performance may be easily
simulated how you can do this and what
can be the trade-offs here how much for
example the performance impact you might
have another thing that another approach
from like for automated modeling is to
use some user input that the developer
or analyst might annotate the key
construction a code of the program like
Q excesses logs like say that this
particular class corresponds to
semaphore for example so to sort of to
guide this model generator to matric
model generator what it is to some
extent yes yeah and more broad vision
like what we can do here in this area is
again to apply this principle that we've
developed for building these models to
different applications like for example
we are now predicting performance of the
system we want more to understand what
make what is causing the performance
degradation for example we want to
visualize these performance models we
want to
lies this movie's application in a
programmers terms visualize its
performance aspects in a programmers
terms like where the programmer should
look at in the program what are the
possible improvements to this program
also maybe it's a different interesting
problem is like you have a particular
program what might be a good hardware to
run it is it like an MD oriented think
or maybe you should run on Intel with a
different like memory architecture with
a different slightly different CPU
architecture and definitely we want to
apply this to new types of systems like
hybrid programs that that do both cpu
and GPU computation and for mobile
upload in venue or flawed something from
your cell phone to the cloud for to save
some energy and how it is going to
affect the overall performance of such
system so these were some publications
here that reproduced in the process and
if you have any questions I will be
happy any more questions I will be happy
to take those now and so are there any
more questions conventional or cash
contention so right now yes the right
now we simulate all memories
computations so we aware that there is
some problems especially for scientific
applications from probably for hpc
systems for the massively parallel
system so this is indeed one of the
things that we want to look at it's like
changes in I mean we want to address it
as one of our like future books maybe in
a hardware section developing like
compact and fast models for memory
indication he 30th question aggregation
we looked at the trade-off between speed
and accuracy teach Act are aggregating
your models into larger blocks me do
what would lend itself to essentially
coalescing no you uh you mean the speed
of model or overhead in yoga the
accuracy of the moment can actually in
essence the morn essentially the
complexity he good question so in fact
we did some optimization heavily
optimization on the as I say that in
many cases you might end up with a very
large model so we try to you know to we
did all our best to make the model more
compact without the loss of accuracy so
we did it in a couple of ways first of
all we try to you know to get to the
phases of the system like you know that
in certain cases the system is maybe not
processing request at this particular
stage so we are not simulating this
stage or generate a simplistic model for
this stage and also we detect those like
code fragments those operations that
have a marginal if any impact and the
performance and we are sort of skipping
those elements of the program from
modeling all together so all those
results that I presented you here they
this model
unter underwent this reduction and we
had to retell to reduce the size of our
more the leg by orders of magnitude does
it answer your question I wanted to ask
you have these three levels in your
hierarchical model give can you give us
some intuition as to which one you know
how much of your accuracy comes from
each of the models what would happen if
you only had the top model or would
happen if you only had the top and
middle layers in your model I would
these affect your performance prediction
oh ok so with regard without the middle
model as I say that the model would just
sort of fall apart if you would assume
the same same execution time regardless
of say the number of walking trails
which is essentially the number of nodes
in the in the high level model you will
I mean you will have a probably playing
line here ok apart from the number of
working threads what other configuration
parameters if you evaluate right now we
evaluated the cpu core count the this
one thing that we have related the
workload like intensity of the workload
for the web server we also looked at the
system-wide parameters like the possible
number of file descriptors that the
program is allowed to have so how this
affects the performance of them what
will be had some work on that so these
are main parameters that we looked at
and by the way with regard to the low
level model again like if you won't
simulate the contention you might end up
with a like a straight line from here to
here so these three parts of the model
they are sort of integral they are very
important for for each other you know
you cannot really just throw it out on
if you are talking about the
multi-threaded system resource
contention Simon I understand your
thread pool you identify the threat
posed by saying I look at one time and I
see that they're executing similar work
mm-hmm is that why
do that some of them just take lists of
work items from a work list and run them
like a something that servicing an
f-sharp but I'm you're so consumed
architecture right you are you talking
about a producer consumer architecture
just something worthy the thread pool
simply services some kind of work list
of jobs to be done that might be no
coming from different parts of the
program mm-hmm so in this case I mean
yeah but still if you will so here in
this approach we look at the aggregate
mode at the aggregate performance of the
system so you are supposed to observe it
for some amount of time to sort of to
get the average like high level view of
it so essentially you know this will
average out like statistically yes and
yes we like we can definitely have about
it more I mean that I can talk much more
about this Deadpool detection I have
slides for it I'm here for the next two
days so okay we yes thank the speaker
again because when you get on to the
next talk thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>