<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Privacy, Audit and Accountability | Coder Coacher - Coaching Coders</title><meta content="Privacy, Audit and Accountability - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Privacy, Audit and Accountability</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7O2UqT3Qwbg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
uh
ana poem was once at Stanford but has
been at CMU now for 6 years 5 but 5
years and has done a lot of interesting
work both on verifying the correctness
of various cryptographic protocols and
we're recently on looking at various
aspects of privacy from differential
privacy to private audit logs and trying
to apply a lot of rigor to a difficult
area in addition to understanding things
like HIPAA laws which has the seven
funding challenges there so today is
going to talk a little bit about the
audit piece all right thank you very
much Brian and thank you all for coming
I want to summarize some work that we
have been doing over the last five years
or so at CMU on trying to understand
privacy at a more semantic level so that
it's more clear what privacy policies
actually mean operationally and it has
led us to mechanisms based on audit and
accountability for enforcement and just
to place things in perspective we now
live in a world where personal
information is everywhere information
about individuals is collected through
search engines and social networks and
online retail stores and cell phones and
so forth and all this information is
being collected by these come in many
cases by companies which now hold vast
repositories of personal information so
the this raises a basic question a
privacy problem how can we ensure that
these organizations that collect these
vast repositories of personal
information actually respect privacy
expectations in the collection
disclosure and use of personal
information one approach that has
evolved in practice is that we are
beginning to see emergence of more and
more privacy laws in various sectors as
well as privacy promises that companies
make like Microsoft and Google and other
companies that hold personal information
about conditions under which information
will be collected
then used and shared just to make things
a little more concrete here are some
snippets from the HIPAA Privacy Rule for
healthcare organization that gives you a
sense of the kind of examples of
policies that exist that exist in these
in this space so here's one example
that's as a covered entity that's a
hospital or a healthcare organization is
permitted to use and disclose protected
health information without an
individual's authorization for treatment
payment and healthcare here's another
that says that a covered entity must
obtain an authorization for any
disclosure or use of psychotherapy notes
so in general for personal health
information so long as the information
is being shared for these purposes there
is no need to get authorization from the
patient except with a special case of
psychotherapy notes and these are just
two snippets in a large and complicated
law that includes over 80 other clauses
it's the kind of thing that is going to
be extremely tedious to enforce manually
so we would like some algorithmic
support in enforcement of these kinds of
policies and there are many other laws
this is just one example there's the
ground leach bliley act that financial
institutions have to comply with there
is FERPA for educational institutions in
the US the you data privacy directive
and so forth yes concerning the first
two points okay
you somehow increasingly assumed deny
over rights
so the way that's not so the way the law
is written I have not shown you the
entire piece but it uses mechanisms like
exceptions so the way it says is that
this is the clause except if and then
this and so the the priority if you will
what gloss preamps what other is encoded
into the law using this exception
mechanism and it that has a natural
logic that leads to a natural logical
presentation of the law also now so
those are some examples of the kind of
policies we see in the written regulated
sectors like health care but on the web
also companies make promises and their
privacy policies that they are expected
to respect otherwise the FTC can and
have impose fines on them and such so
here's here some examples so Yahoo's
practice is not to use the content of
messages for marketing purposes and then
there is something from Google's policy
here now it's one thing to have laws and
promises but the question is really are
these promises actually being kept right
or are these laws actually being
complied with and there's a lot of
evidence that we have lots and lots of
violations of these the kinds of clauses
that appear in these laws are there are
examples of health care and the health
care sector reaches and violations of
HIPAA this was an example of where
Facebook was called had to pay a price
for not living up to their promises
Google ran into rough waters and so
forth and this wall street journal
selection series of lectures on a series
of articles on what they know what they
call other what they know series has
lots and lots of such episodes about
where primacy privacy promises are not
being respected or the practices are
somewhat questionable so that's the
broader context but the the technical
question that I and others are
interested in
is what can we as computer scientists do
to help address this problem so let me
focus on a very concrete scenario and a
snippet of a privacy policy from HEPA to
motivate some of the technical questions
that I'll get into later in the talk so
we will consider a simple scenario from
the health care privacy where a patient
comes and shares his personal health
information with the hospital the
hospital might use this information both
internally for purposes like treatment
and payment and operations and share
this information with third-parties like
an insurance company for the purpose of
say in this case payment but when this
information flows in a manner that's not
permitted for example if this
information goes out to a drug company
which is then used for advertising then
that would be a violation right and in
addition to this cross-organizational
flows inside the hospital also there's a
complex process and there could be
violations internally also very
information is used in inappropriate
ways now let's I guess the one thing I
want to point out here is that many of
these violations are going to happen
because of authorized insiders people
who have been given access to this
information for certain purposes like
treatment and payment and and operations
but then they either use that
information in ways that are not
permitted or share it under conditions
that are not permitted so we we do not
expect that the problem will be solved
using well known techniques from access
control which is because the people who
have a legitimate reason for access are
exactly the ones who are committing the
violations so access control is of
course necessary in this setting to
ensure that employees people who are not
employees of the hospital do not get
access to the information but that is
not going to be sufficient for enforcing
the kind of policies that are appear in
this application domain so let's look
not a very concrete example from the
HIPAA Privacy Rule and it's going to be
a bit of a mouthful but the reason I
picked this particular class from HIPAA
is that
all the concepts that are relevant in a
whole bunch of privacy policies
including HIPAA and a lot of other
policies show up in this one example so
the the policy says that a covered
entity again think of that as a hospital
may disclose an individual's protected
health information to law enforcement
officials for the purpose of identifying
an individual if the individual made a
statement admitting participating in a
violent crime that the covered entity
believes may have caused serious
physical harm to the victim and so it's
a very long and convoluted sentence but
let's look at the different pieces and
the corresponding formal concepts that
they represent so first of all when we
talk about privacy policies there's
often going to be an action involving
personal information so in this setting
in this example the action is a
transmission or send action that goes
from a sender p-12 a recipient p2 and it
contains the message m in this example
the sender p 1 is the hospital the
covered entity be to the law enforcement
official NM is the message that contains
the information that is being shared but
in addition to transmission actions
there could be accesses or use actions
other kinds of actions that involve this
personal information the second piece is
that this is common in HIPAA and many
other privacy policies in order to get
some abstraction often roles are used to
group large classes of individuals so in
this case the recipient of the messages
in the role of law enforcement and the
same policy will apply to all law
enforcement officials not just to p2
there are data attributes of the message
that is being transmitted sin this this
clause applies to messages that are of
the type protected health information
and you could imagine a characterization
an attribute hierarchy if you will that
says that various types of information
are protected health information so
prescription might be protected health
information whereas maybe in a
via email that contains some scheduling
appointment scheduling is not right then
there are these temporal concepts in
this case the policy permits the flow of
information if in the past a statement
was made to this effect and temporal
concepts capture a bunch of pre privacy
idioms for example often in privacy
policies you see things like consents if
in the past a customer or patient gave
constant then it was okay to share
information there are also things like
notification that require bounded future
temporal constraints a notification
policies are of the form that if any
organization loses their personal health
information our personal information
about customers than within a certain
number of days like 30 days they have to
inform the customers that they have lost
their information so that's a bounded
future requirement so temporal
constraints about the past and bounded
future timed temporal constraints are
going to show up a lot in these kinds of
privacy policies now in addition to
these concepts now we will begin to see
some other gray or concepts that are
also somewhat bigger right so in
particular there is this notion of a
purpose which shows up all over the
place in privacy policies right so in
this case the disclosure is permitted
for the purpose of identifying criminal
activity in many cases you'll see
companies promise that information will
be used for one purpose it improve the
search experience or to improve
treatment but not for other purposes
like marketing and so forth right so
that raises a basic question what does
it mean for an action to be for a
purpose or information to be used only
for a purpose or not for a purpose and
how what does that mean semantically and
how would we enforce these kinds of
promises or policies another thing that
shows up is beliefs you see here there's
in this example there is this notion
that it's okay to do the disclosure if
if the hospital believes that the crime
cause serious harm so this requires some
understanding of beliefs now I'm going
to separate out these two classes of
concepts into what I'm going to call
black and white concepts these are
things for which semantics are not that
hard to give based on previous work on
giving semantics to policies and and
then enforcement techniques the
enforcement will be difficult for some
other reasons that I'll explain and
these are much more vague in purposes in
particular is one thing that we are
going to focus on beliefs I'm not going
to look talked about too much today
partly because I believe that a lot of
the work that has been done on
authorization logics could directly
apply to to the belief part of this so
we are going to not deal with beliefs
appealing to prior work but I do want to
highlight these two broad classes of
concepts that arise in privacy policies
and I'm going to discuss some methods
for their enforcement so very broadly
the 10 10 oops
that the 10,000 foot view here is that
we are there's a research area to be
explored in formalizing privacy policies
giving precise semantics to various
privacy concepts and their enforcement
and in enforcement I'll talk about at
least two classes of techniques want to
detect violations of policy in some
cases as we will see it'll be very hard
to prevent violations and also
accountability whereby accountability I
mean that when a violation is detected
in a multi-agent system where many
people could have done many different
things that ultimately led to the
violation how do we identify agents to
blame for policy violations as well as
incentives to deter policy violations so
much of this is work in progress I'm
going to focus primarily on the audit
based today so at a very high level one
way to think about this is this is this
parallels how the justice system works
in practice where there is a law but the
enforcement is not always based on
prevention it requires police officers
to do detection of violations and give
parking tickets and other mechanisms for
assigning blame and appropriate
punishments and that's what we are going
to try and mirror in the digital world
and detection will be the first step of
that process now we are not the only one
to talk about the importance of audit
and accountability in this setting there
have been a couple of recent position
papers from MIT from how labels and
analyzer and others and also Butler
lampson and in fact there is also in
that in the corporate world there is
increasing push for accountability based
privacy governance in which Microsoft
the corporate privacy people at
Microsoft have been largely involved the
goal one big goal of this work is
suppose that there is a regulation or
that organizations are expected to
comply with how do they demonstrate to a
third party that in fact they are
complying with these regulations and so
far much of this has been very ad hoc
but part of what I'm going to talk about
today
is a step towards producing algorithmic
support for these kinds of tasks so at a
very high level the approach for audit
is going to parallel the separation
between the black and white concepts and
the gray concepts so we start off with
the privacy policy and put it into a
computer readable form and we have
actually done complete formalization of
both the HEPA Privacy Rule and the
gramm-leach-bliley act for financial
institutions and one way to think about
this audit box is that it takes as input
an organizational audit log that records
what software systems and people have
touched what pieces of data are shared
what pieces of information and the
policy and then the audit log audit box
comes back with well this policy was
violated on this execution that kind of
information whether the violation
whether a violation happened or not and
paralleling the informal separation that
I mentioned when I walk through the
policy example we will look at we have
one algorithm that does fully automated
audit for these black-and-white concepts
and a different algorithm you can think
of that as an Oracle that provides
guidance on these grey policy concepts
so the second algorithm is going to be a
little tricky to get at because it's
trying to in particular it focuses on
this purpose piece and the reason it's
it's it's complicated is that we are
trying to figure out whether a person
access to information with thinking
about achieving a purpose like treatment
or not it's as if we are trying to
understand the human psyche right we are
not there yet we don't have the we don't
have the Oracle from the matrix yet but
we will try to approximate it using some
AI techniques so that those are the two
big pieces of the technical presentation
so let me first talk about auditing
black and white policy concepts and this
is just John work with two of my
postdocs former postdocs debug is now
faculty at Max Planck and lemon gia is
still at CMU so in order to audit these
black and white concepts although I said
there are somewhat simpler than the gray
concepts there are still some to main
technical challenges one challenge is
that these audit logs are incomplete in
the sense that they may not have
sufficient information to decide whether
a policy is true or false so if you
think about access control say let's say
file system access control when I try to
read the file there is a reference
monitor that's sitting and it'll either
let me access the file or not depending
on what the policy says so there is
often in access control there's enough
information to decide whether to allow
access or not so the reference monitor
will come back with a yes or no answer
we'll see that in the presence of
incomplete audit logs we will not always
get a yes/no answer it could be that the
parallel of the reference monitor this
audit algorithm can either say yes the
policy was satisfied no the policy was
violated or it can say I don't know but
we want to deal with the I don't know
scenario you know graceful manner so
there are a bunch of sources of
incompleteness one is future
incompleteness so since we might have
these notification like laws that talk
about what needs to happen at the in the
future there may not be enough
information in the log at the current
moment to say whether the whether it's
violated or not but at the hope is that
as the log grows over time we will get
to a point where we will know for sure
there may not be information about some
of these great concepts the subject
somewhat subjective concepts there may
not be evidence may not be recorded for
purposes and beliefs and things of that
nature sometimes logs may be spatially
distributed and there may not be
information in one log to decide whether
information whether the policy is
violated or not
so we are going to ya can what and
keeping a log violates these policies
good yes it could this is this is the
class of policies that I'm enforcing
here primarily policies that will talk
about conditions under which information
can be shared or not or used for a
certain purpose or not we don't we don't
have mechanisms to deal with say data
retention policies and things like that
they have to be dealt with using other
mechanisms the log is is operating on
human subjects data and were due to all
of the people who contributed to this
system with data consent that their
medical records would create records in
a law which then you would access and do
Studies on that's an interesting that's
an interesting point so under law HIPAA
requires these healthcare organizations
to maintain these audit logs so
healthcare organizations have to
maintain these audit logs and then the
there is often the way it works in
practice so I should say that there are
all these other tools that are now
appearing in the market for healthcare
audits that are getting bought and used
often the way that they're getting used
is that there are some designated people
in the audit office if you will who
access these logs and these existing
commercial tools do very simple things
you can only issue SQL queries so you
can check if find all employees who
access more than hundred times in the
last two days things of that nature
right and then the audit these tools
there is the fair warning tool and then
which is a company a startup that is
doing reasonably well the other tool the
p2p Sentinel tool which required by
Cerner does the similar thing what they
also do in addition which is partly
maybe what you're getting at is that
they also keep track of who access the
audit log so there's another layer of
but that's about as far as there is a
trail of who is accessing the audit log
information
ready it's their required to keep the
audit log yeah but the moment our
researcher goes in and does research
using the audit log that's different
than keeping the other the researcher
going in so HIPAA allows so this is I'm
not particularly happy about this clause
in HEPA but HIPAA allows de-identified
so we have not done any stuff you are
asking me whether I have looked at these
logs of the answer is no but if you're
asking whether it is permitted under law
the answer is yes HIPAA allows
de-identified for a very operational
notion of D identified but that might
have let me there may be very unrelated
to protecting privacy it allows
de-identified information to be shared
for the purpose of research alright so
under HIPAA it's permitted you don't
need consent from patients to do that
alright so this is one big challenge the
dealing with incompleteness and the way
we are going to do that this very simple
idea we are going to model these
incomplete logs uniformly as three
valued structures meaning that given a
predicate the log might tell us that the
predicate is true or false are unknown
meaning that it doesn't know doesn't
have enough information seems
there was a presumption the things that
consistent
category you mean the policy of yeses
the policy we are assuming if the policy
is consistent the inconsistent and all
bets are off because false will imply
anything so we haven't found it's a good
question we haven't found any
inconsistencies in HIPAA and part of the
reason for that HIPAA is very largely
operational so that was a good thing
that's part of the reason why we looked
at that but part of the reason for that
is inconsistencies might arise when one
part of the policy says do something and
on other part says don't do it but
whenever that has arisen in HEPA there
is it has always come through this
exception mechanism so that it's clear
what overrides what now we haven't done
a we haven't done of a mechanical
automated and lysis to check for
consistency but maybe that's something
we can do because now we have it in a
machine readable formalization
you're fully about too difficult
together or something they're all
insisting to use my famous
it might be missing pieces that's not
that's incompleteness incompleteness we
can deal with inconsistency in the logs
will also be problematic for the same
reason because we are assuming that if a
predicate is true then it cannot be
false but the logs are not necessarily
pulled from different places so use the
the next at the application we are going
to do with the real logs which we has
taken like more than a year to get close
to it is from the Northwestern Memorial
Hospital which is part of the Sharps
project that I'm on that Carl Gunther
has already done some experiments and
publisher is also on that it's one place
it's a like a Cerner log that it's it's
not a distributed lock right so it's a
very simple abstraction given a
predicate the log will tell us whether
it's true or false or unknown right and
then the meaning of larger policy
formulas can be defined using this 3
palette logic now the idea of this
iterative reduce algorithm which youth
which is a the audit algorithm is that
it takes a log in a policy but unlike
standard access control or run time
monitoring instead of coming back with a
true false answer it'll check as much of
the policy as it possibly can given the
information in the log and output a
residual policy which will contain only
those predicates for which the log says
unknown I don't know whether it's true
or false and so that'll be a simpler
policy and then when the log is extended
with additional information you can run
the algorithm again and you proceed in
this way I trait of Lee right so
pictorially you have the log in the
policy think of this Phi 0 as HIPAA you
run reduce we get the residual policy 51
when the log is it grows there's
potentially more information now about
the predicate sin Phi 1 that there
wasn't information about here you run
this again and this process continues
and at any intermediate point we can
invoke the Oracles for the gray concepts
like we have if you have an algorithm
for determining purpose restrictions and
you can call that algorithm because this
algorithm is not going to deal with
those
concepts right so that's the picture
then a little bit more detail the policy
logic looks a little bit like this I
don't want you to try and read
everything on this slide it's a fragment
of first-order logic we need / unbounded
domains with the interesting technical
challenge here is that we have to allow
for quantification over infinite domains
because HIPAA talks about for all
messages the messages sent out by the
hospital has to respect some policies
and because of that that's the technical
challenge where we have to go beyond
what is already known in runtime runtime
monetary and the logic is expressive
since it has quantification over these
infinite domains you can quantify over
time so you can express timed temporal
properties now if I take this policy and
write it out in this logic it looks a
little bit like this again I don't want
you to necessarily read the formula the
important thing here is that there is
going to be a distinction well there's
quantification over all messages the set
of messages in english is infinite and
and all time points and the other thing
to take away is that there are these
black the black part of the policy which
the algorithm will deal with
automatically and then there are the red
bars which are really the gray concepts
and these this algorithm will not deal
with so it talks about things like
purposes and beliefs and so forth right
now the formal definition of the reduce
algorithm we show you a little bit of
little snippets of it if the formula is
just a predicate then the algorithm will
find out from the log whether that
predicate is true or false or unknown if
it's true it returns true if it's false
it returns false and but if it's unknown
then it returns the whole predicate p so
in this case the residual formula is the
entire predicate p right and then we do
require apply this recursively so if
it's a conjunction you just apply reduce
on the two parts and so forth the
interesting case is when we have
Universal quantification over an
infinite domain now 19 way to try and do
this is to consider all substitutions
for
and then this becomes a conjunctions of
five with X set to X 1 and 5 the text
settings but that's going to be an
infinite formula and obviously the
algorithm will never terminate if you do
that instead that we are going to
restrict the syntax to have these guards
so since this is an implication it's
going to be trivially true when C is
false the interesting case is when C is
true now the sea will be such that
there's only going to be a finite set
number of satisfying substitutions of X
and that finite substitutions can be
computed so intuitively one way to think
about this is if you think about HIPAA
then what is HIPAA saying hit by saying
that every message sent out by the
hospital should respect some complicated
logical formula that represents HIPAA
but the number of messages sent out by
the hospital is finite the hospital does
not send out every possible message in
the English language there are only
maybe a few messages that the hospital
sent out to third parties so for this
predicate is going to be true only for
those messages and in that case you get
this as a finite the instances you'll
get as a finite conjunction all right so
let me when you write this out in Greek
font then you get one conjunct for each
of the finite substitutions that makes e
true and then the rest this is saying
allowing for the incompleteness in the
audit log since in the future you might
get others substitutions more messages
might get sent out or even the log was
not passed complete so maybe there were
some messages that will show up as the
log expands we have to somehow deal with
that and that's captured by this by this
con job which is saying if you I get
other substitutions other than the ones
that I've already considered then I
should also have a piece for that in the
formula yeah
you say that Hubert it is always the
case that appropriate God finalizing God
can be found that's right that's right
why would it not been acceptable to
convert for all into for almost into
none may
must there is no message sent by the
organization that violates the role
rather than all of them must follow the
rule so that's fine that that doesn't
change anything right so if I express
that well if I express that in this
first order logic that's an existential
quantifier over an infinite domain and
that'll become an infinite disjunction
converting Universal to existential will
not help well maybe we can take that off
mine all right so so I guess coming back
to your question the general theorem
though we have is that if this initial
policy satisfies a syntactic mode check
where we are using the idea of mode
checking from logic programming then the
finite substitutions can be computed so
that is general here and not so we now
have a syntactic characterization of
these guards so r which the finite
substitutions can always be computed and
it turns out that as an an instance of
that we get the whole of HIPAA and the
whole of gramm-leach-bliley so if
someone comes up with a third law that
we want to look at then we don't we
would like to see first whether this
theorem applies to that law or not if it
does so the generality has that nice
property and it and our argument for why
this somewhat esoteric theorem that uses
techniques from mode checking is useful
in this setting is that look the whole
of HIPAA and gramm-leach-bliley actually
satisfies this test
so if you if you look at this particular
going back to our policy example and
here's a example of an incomplete audit
log now if you look at all these
quantifiers then we are going to find
substitutions for the variables p1 and
p2 mm and so forth by mining this log so
mining this log you see that the send
predicate there there's only one
instance of send that's this instance
over here and that'll give us the
substitution that p 1 corresponds to
UPMC and p 2 corresponds to the
Allegheny police and that message m
corresponds to exactly this message m 2
and so forth so we can mine these from
the log and when you do that now we know
the truth values for various predicates
and we are left with a residual formula
that only contains the gray part of the
policy the rest all become true and
disappear and we have actually
implemented this and applied it to
simulate it audit log so this is not a
worry allotted loss we haven't gotten
our hands on audit logs on Hospital
audit logs yet when we wrote this paper
for ccs last year and it turns out that
the average time for checking compliance
of each disclosure protected health
information is about point 12 seconds
for a 15 megabytes log so this does
scale reasonably well now so that's
performance so one thing to be careful
about for performance is that as you
apply this algorithm the residual
formula can actually grow because we
have the finite substitute you know the
for whenever we see a for all it becomes
bigger because you get one entry for
each of the residual each of the
substitutions that you mind from the log
and then there is the residual piece so
after a few iterations the policy will
become too big for the algorithm to work
because the residual formula largely has
these things like purposes and such
which this algorithm cannot handle and
that's something I'll come how to handle
purposes the next part of this talk the
other thing that is a relevant question
for this application is you know reduced
and only
deal with what I'm calling black and
white concepts everything else shows up
in the residual formula in HIPAA about
eighty percent of all the atomic
predicates is actually what I am calling
black and white which means that the
algorithm can deal with that completely
automatically so that doesn't mean that
eighty percent of the clauses of HIPAA
are black and white though right so that
I'm just counting the number of atomic
predicates now in HIPAA they're about 85
clauses of which about 17 which is
twenty percent is completely black and
white so so for those twenty percent of
the clauses this algorithm will be fully
automated and for the others there are
things like purposes and beliefs that we
have to deal with so purpose remains a
important problem and that gives us a
sense of purpose now in terms of related
work there has been a fair amount of
work on specification languages and
logics for privacy policies some of them
are either not expressive enough to have
their doesn't have the richness to
capture this kind of policies or they
are not designed for enforcement like
p3p is primarily a specification
language not a mechanism for enforcement
and they're our prior work so this was
Warwick I did back at Stanford is did we
used quantifiers for specification but
the enforcement was really using
propositional logic so we couldn't
enforce the whole of HIPAA using that
prior work in terms of actual
specification of laws we had looked at a
few clauses in this earlier work and
Gunter looked at one section and some
work at Stanford from John Mitchell's
group looked at a few more sections but
this our work is the first that does the
whole of HIPAA the nearest technical
work in terms of monitoring runtime
monitoring is this work from David
basins group some of the they have a
much more comprehensive implementation
and evaluation they have actually
applied it to real audit logs from Nokia
which we are also trying to get
and they seem to work quite well the two
things that make that approach
unsuitable for this application as one
they also have a kind of mode checking
which they don't call mode checking but
they call it a safe rain check but that
is a much less expressive way of we were
restricting the guards and in fact this
restriction is too restrictive it cannot
express the kind of clauses we see in
HIPAA and she'll be a the other thing is
that like much of the work in runtime
monitoring they assume that audit logs
are or the executions are passed
complete so the only source of
incompleteness is future incompleteness
and that's also something that we wanted
to avoid it be more general about
allowing other forms of incompleteness
now moving on to this piece on purpose
restrictions right so this is work that
will appear next month the Oakland
symposium and its joint work with my
student Michael shanz and Jeannette wing
at CMU so purpose restrictions I've
motivated before show up in a lot of
privacy policies and there are at least
two kinds of purpose restrictions there
are the not for restrictions that say
that information is not going to be used
for a purpose and the only for that say
that we want to use information only for
a certain purpose and potentially not
for other things right so the goal is to
really give us semantics to not foreign
only for purpose restrictions that is
parametric in the purpose information in
action meaning that you wanted to apply
for all purposes for all types of
information and all types of actions and
then to provide an auditing algorithm
once we have a sense of what it means so
let me try to motivate give you a sense
of how we arrive at the final notion by
using this running examples imagine that
an x-ray is taken in a medical office
it's added to the medical record and the
medical record is then shipped to a
diagnosis for diagnosis by a specialist
who is a specialist in reading x-rays
and the policy says that medical
are only going to be used for diagnosis
right so the question is is this action
for diagnosis or not if it was not for
diagnosis and this is a violation of the
policy so how can we how can we go about
defining what it means for an action to
be a purpose so here's the first attempt
the first attempt is that we are going
to say that an action is for a purpose
if it is labeled as such so someone
whoever is sending out the message is
going to label the action with a purpose
and an obvious problem is that it kind
of begs the question because how do we
know that that labeling is correct what
is the basis for doing that play bring
the other problem is that one action can
have different purposes depending upon
context so if I go back to this example
intuitively it appears that these two
actions are for diagnosis but imagine
another send record action from up there
that cannot be for diagnosis because
this was an x-ray specialist and the
x-ray wasn't even added to the medical
record so although these two actions are
the same syntactically they're both send
records one is for diagnosis and not the
other right so these two appear to be
for diagnosis and this is not for
diagnosis so actions cannot direct just
labeling actions with purposes is not
sufficient the natural next step is to
try and also whether this action is for
diagnosis are not depends on the state
from which the action was done so maybe
we should take the state into account
which is very natural right so our
formalization of purpose must also
include States now if I look at this
example it appears that these two
actions are necessary and sufficient for
achieving the purpose of diagnosis
whereas that one up there is not
sufficient because no diagnosis is
achieved right okay
not necess okay suppose we go from
correct state and
sin but sent to someone who is not a
specialist cannot possibly do they
right so then it's not going to be
sufficient for diagnosis I'm going to
say I'm going to say that at this
attempt attempt two is that an action is
for a purpose if it is necessary and
sufficient as a part of a chain of
actions for achieving the purpose so if
you send it to someone who's not a
diagnosis then he will violate this that
action could not be for the purpose
because it wasn't sufficient but now I
have presented it only to say that this
is not very great so now ilex argue that
necessary is too strong and sufficiency
is both too strong in two week in a
specific sense right so let's first look
at necessity so here's a slight
modification of this example now you see
neither of these actions are necessary
because instead of sending to the first
specialist I could have sent to the
second specialist and vice versa so
necessity is too strong so instead we
are going to use non redundancy so non
redundancies necessity with respect to a
fixed sequence of actions so given a
sequence of actions that reaches a goal
state an action in that sequence is non
redundant if removing that action from
the sequence results in the goal state
no longer being reached so if I go back
to this example both of these actions
are non redundant because if I fix an
execution like this one if I remove this
then the purpose is no longer achieved
so non redundancy is the weakening of
necessity that we are going to try and
work with right so the 10 3 now is an
action is for a purpose if it is part of
a sufficient and non redundant chain of
actions for achieving that purpose and
this actually coincides it's a slight
adaptation of the counterfactual
definition of causality that people who
have worked on the causality
philosophers and also computer
scientists like Judea pearl have very
similar notions of causality yes
to a specialist and specialists as I can
that's good next slide so I have now
said why I've now argued why necessity
is too strong and we have to replace it
by non redundancy the next part I want
to argue that sufficiency is too strong
partly because of what you're saying
probabilistic outcomes right the probe
it's too strong because of probabilistic
outcomes but also it's too weak in a
certain sense because if you look at
this picture then you might have a more
accurate diagnosis if you use one
specialist versus another and in that
case we would like the record to have
gone to the right specialist rather than
to the one who always does better if we
know that that person always does better
right but you could instead of saying
all this does better you can also have
an expectation over it things like that
right the other thing is exactly what
you're saying you may not always get a
diagnosis even if you did an action with
the intention of getting a diagnosis
because of probabilistic outcomes so
that that's why that's the difference
between the adapted counterfactual
definition of causality and what we are
going to end up with so so there are at
least two things that motivate our TSYS
which will be based on planning because
of probabilistic failures we cannot
require that a sequence of actions
actually furthers a purpose we can only
require that agent plans to perform
actions to further the purpose and
quantitative purpose is saying we
require the dejan adopts the plan that
optimizes the expected satisfaction of
the purpose so that leads us to the
thesis that an action is for a purpose
if the agent planned to perform the
action while considering that purpose so
if if the actions or in other words an
action is for a purpose if it is part of
a plan or part of an optimal plan for
achieving that purpose now having
reduced the question of what it means
for an action to be purpose to this more
concrete thing about planning now we can
hope for algorithm
algorithms for checking thank you speak
about not for the purpose that seem
reasonable you see only for a purpose
now it's very common to both I'm good
x-rays to send those specialties we work
so our purpose also there is another
purpose to up to increase our Damon
right to send our specially they outside
right so good so what was so so what
you're getting at is that this algorithm
is not going to be both sound and
complete it's going to be let me try to
think about this work it's going to be
useful if it says not for an action is
going to so this is saying that an
action is for a purpose if it is part of
a plan for achieving the purpose and if
I show that the action could not have
been part of a plan for achieving the
purpose all possible plans that you can
think of given the environment model
then I know for sure that there was a
violation but I don't know that the
policy was respected if I cannot argue
that so there will be a space of tenable
deniability and for not for a purpose
it'll be the other way around
ok
it seems you know we all come
my philosophy pets
and i think the logic appropriate here
for what chords
watch you goof
not making something to a false and
achieving a young reasonable doubt I see
Lord lowering yeah Lori good yeah that's
a very interesting point so this is as
far as we have gotten i i'm not i think
this we are presenting this as where we
are after two years of thought and this
is by no means the final word on this
topic so i'll be happy to hear about
your thoughts and greater detail when we
talk and we haven't even gotten to a
logic we just have a model at the moment
we are wondering what the right logic
for it will be the initially we were
stuck for a long time on the
counterfactual the non redundant but
sufficient definition which seemed very
close to causality and so all the work
that has been done in that litter but
since we got to this planning which is
that's the it seems a little bit
different from what exists out there so
in the remaining few minutes don't want
to exceed the time i want to give you a
sense of what we did in terms of eval
trying to get a sense of whether this
hypothesis this is our thesis but is it
true or not what does that formally mean
what does it mean for an action to be
for a purpose formally and how do we
audit for it right so for that first one
you know one obvious way is try little
examples which is what we did for two
years before we got to this point and
the sequence that i showed you is a
condensed summary of the kind of thought
process that went wind the other thing
we try to do was we gave a survey using
em in this mechanical turk to about 200
people and we compared the predictions
of two hypotheses to provide responses
due to the provided responses the
planning hypothesis is ours it says that
an action is for a purpose if and only
if that action is part of a plan for
furthering the purpose and the other
hypothesis is an action is for a purpose
if and only if that action furthers the
purpose so this was the current kind of
reigning champion before before this
work and the survey kind of
overwhelmingly shows that
at least those 200 people cannot
generalize too much from a sample of 200
but at least those 12 200 people when
there was disagreement between we had
four scenarios one in which we satisfied
both these hypotheses and they all
decide yes a scenario in which both
decide hypothesis were false and they
said no one in which this was true and
this was false and many more people said
yes and very few people said no and when
this was true and this was false most
people said no and FIFA pupil will said
yes in the details of the survey I don't
have time to get into but I have some
backup slides at the end to show you the
kind of questions that we ask them now
coming to auditing the picture now is in
addition to the privacy policy and the
audit log there is an environment model
for planning which is essentially the
state machine kind of thing that I was
showing you which formally is a Markov
decision process which is a standard
model that is used for in the planning
community so given a privacy policy a
Markov decision process that keeps track
of the environment model and an audit
log which records the actual execution
what actually happened this algorithm
comes back with obeyed inconclusive or
violated and the basic idea for only
four policies is that you want to find
all strategies that are consistent with
the observed agent behaviors under a
model for the purpose p and if none of
these strategies optimize the purpose be
under the model then the agent violated
only for P and there is this can be
mapped to the Markov decision process
and the details I don't want to get into
but I'll show you an example in the next
slide the soundness theorem here is
saying that the algorithm returns true
if and only if the actions recorded in
the log are not for the purpose so if
the algorithm returns true you know that
not not for purpose policy is satisfied
and an only for policy is violated
but you don't the other hand side you
don't know there is tenable deniability
on the other side now if coming to this
L this example let me illustrate how the
algorithm works this is the Markov
decision process I've been sure I've
shown you states and actions and there's
the reward function the reward function
is essentially when you go from this day
to the estate you get the difference in
between these diagnosis scores so you
don't gain anything coming here but if
you go from here to here you get 6 and
here to here you've got to write so the
optimal path through this meth through
this model is this pot if any other path
happened then that would be a violation
so if you went on this spot for example
then this action was not for diagnosis
because it could not have been part of a
plan an optimal plan for achieving this
purpose now this can be algorithmically
done using adaptations of bland
recognition algorithms so this is where
we are now in terms of past approaches
while past approaches were useful they
were not semantically justified so
they'd included things like labeling
actions or labeling sequences of actions
or agent roles or code but there was no
justification for how the labeling
should be done so if the labeling was
done incorrectly then there wasn't any
way of catching it so our work provides
a semantic foundation for these
approaches that provides a basis for
correct labeling and the paper discusses
and great details and with the
limitations now in order to make this I
am NOT claiming that this work is usable
as it currently exists there's a bunch
of reasons for that well I guess one
extension that we have worked out is
enforcing purpose restrictions on
information you so all what I was
talking about so far is whether an
action is for a purpose or not but
whether information is used for a
purpose or not is a little bit more
subtle because there could be implicit
flows so suppose you want to say the
gender will be not will not be used for
targeted advertising then we have to
show that the planning process is
unaltered whether you
the gender is male or females it's
similar to notions of non-interference
and information flow it's a combination
of planning and information flow if you
will now the other thing that gets in
the way of making this approach using
this approach directly in practices we
are assuming that these environment
models this Markov decision processes
are an input to this algorithm but
constructing them for complicated
organizations is going to be a lot of
work that we ideally don't want to do
manually so one of the things we would
like to explore once we get the real
audit logs is to see if we can learn
these things from the logs right and
it's not out of the question because
there's a lot of work on cue learning
and reinforcement learning that tries to
learn markov decision processes given
observed data another thing here is if
you think about this model of planning
it with the reason we chose md pieces
because it's kind of the most developed
formalism for planning in the high
community but it's a good model of
planning for things like robots you know
where you can program them to act in
certain ways people not so much right
baby it's assuming that people are
completely rational and they can do all
these complicated calculations so if
people if the AI community comes up with
better models of human planning then we
could instantiate this framework with
other models however I should say that
although this is not such a great way
great model for human planning it is
something that an organization could use
to analyze this environment model and
prescribe more operational policies for
humans where the calculation is done by
the organization not by humans and the
finally you know we really want to push
on getting it applying it to practice
will as we get the audit logs from the
Sharps project from Northwestern
Memorial and Johns Hopkins and
Vanderbilt that's a next thing on our
agenda to try
like this I'll leave you with this
picture of how odd it works and is
separated into auditing for these
black-and-white concepts and great
concepts like purposes and the final
kind of big picture of using audit and
accountability for enforcement thank you
very much yeah how will is he supposed
and salt right so HIPAA hippo was very
operational so that was one very good
thing about HIPAA and that's part of the
reason why this the first algorithm that
I presented was quite effective for big
chunks of the four it's especially for
disclosure conditions under which
disclosure can happen is very
operational now the the the other thing
that the specific question that you
asked earlier about one thing having
higher priority over another that was
also spelt out quite explicitly the the
structure of the clauses against the
board was often of the form that this so
there were a bunch of what we were
calling positive positive clauses and
there was a conjunction over these
positive logic clauses so all of these
conditions are negative causes whatever
recommend thing you're calling them a
negative plus all of these policies have
to hold all of these clauses have to
always hold and then there were a bunch
of positive clauses that said if any one
of these held it was okay to share
information so one way to think about
this is these are saying under these
conditions it's okay to share
information and these are saying it's a
generalization of the denying clauses
that all of unless these things are
satisfied you cannot share information
now the interesting thing about the way
the law was structured is that the
positive glasses had conjunctive
exceptions and the negative clauses had
disjunctive exceptions right meaning
that these this condition has to be
always satisfied except if this other
thing happens which means that this can
be written as negative core or an
exception and these had conjunctive
conjunctive exceptions meaning that if
this holes you can release information
to this third party except if this other
thing also holds so these had like a
core with a conjunctive and so the
preemption mechanism the priority
mechanism was encoded and we try to
formalize that this is what it looked
like in the logic and you could imagine
multiple nestings of these kinds of
exceptions but in HIPAA there was only
one level deep
we just use
oh right right yeah so if there's a if
you have I guess we'll talk with you in
greater detail if you have suggestions
about we're using first order logic
right but you could imagine using better
logics that are more suited for this
application that will because of these
exceptions and their encoding in this
manner the formulas get a lot bigger
right yeah i know that in simply try to
do something with deontic logics but for
a very different applications i don't
know what other logics might be suitable
here but that would be an interesting
conversation yes
yeah I think most of the world you know
it required a clever CMU grad student
the formalization took over a year to do
but but he was Henry was also doing
other things with Frank Fanning but
there were only a couple of places where
we had to go talk to a lawyer and the
lawyers came back with some answers that
helped us a little bit but I don't want
to claim that this is a authoritative
encoding of the law I think we have to
make these things open source and up for
discussion and debate we do need input
from lawyers but for the most part the
high level bit here is HIPAA is largely
operational and that's actually the way
the law should be written the places
where it's more abstract are things that
have to do with purposes and beliefs but
at some level that seems like the right
level of abstraction because you know
how are you going to unless you say this
unless you say that personal health
information can be shared for the
purpose of payment treatment and
operations if you try to make it more
operational then you have to list all
the conditions and that seems like it
will vary from organization to
organization right so I I wouldn't know
how to make it more operational on the
other hand what will impede one other
thing that will impede these kinds of
techniques to be applied if there are
lots and lots of laws and policies is
you will need a Henry Dion for every one
of those laws and policies right so if
it could be written in a more structured
if we can figure out an input language
that is more structured while being more
usable so that it's not first-order
logic or some other complicated logic
that could then serve as a way
for an automatic compilation that would
be I don't know that it will ever happen
to it loss is it's interesting I gave
this talk and Michelle dennedy who's now
the chief privacy officer of mcafee but
was its Sun and oracle before that she
went to war she got very excited she
went to Washington and tried to convince
the Senators to write and some stretch
their laws in some structured language
and she didn't get very far
unsurprisingly but maybe that's
something that companies can do right a
company like Microsoft if they have a
usable way of writing policies that
would then be used to compile down to
something that an enforcement engine
could operate on that seems much more
doable I guess what gets ultimately
enforced in an organization is not the
law because the law tends to be more
abstract but internally the policies are
tend to be more operational so yesterday
when I was talking with the chief with
their you did the corporate privacy
people here that's one thing that we
discussed we might want to do you know
look at the internal privacy policy and
see if they're working on making it more
operational apperantly right now see if
we can work with that yeah
one of the places where you at
oh I will have to look at my email I
it's been this the formalization was
done about three years ago so I don't
remember but I'll dig it up and let you
know yeah yeah I have no memory of it
anymore blissfully forgotten thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>