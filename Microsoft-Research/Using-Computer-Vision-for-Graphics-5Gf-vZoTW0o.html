<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Computer Vision for Graphics | Coder Coacher - Coaching Coders</title><meta content="Using Computer Vision for Graphics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using Computer Vision for Graphics</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5Gf-vZoTW0o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
now I would like to introduce dr. sim
bintang same being is a principal
researcher in the computer vision group
in Microsoft Research please join me
welcoming singh bing thanks for the
introduction Jaime it's it's a great
pleasure to give a speech in front of
all of you and today I'm going to be at
talking about how computer vision
techniques can be used to benefit
graphics and I'm going to show you a few
examples to illustrate that point so as
everyone knows graphics is about
generating compelling looking imagery
and it has a lot of tools in its arsenal
I mean you can there has you can do
solid modeling you do appearance
modeling by looking at illumination and
surface property and then there's a big
area called non-photorealistic rendering
hatching is one example of it another
example is painterly rendering and of
course we have hardware to facilitate
rendering well graphics can do a lot of
things but to generate really comparing
low key content is more often than not
laborious and despite all the progress
that's been made photorealism is still
difficult to achieve so if you think
about it computer vision is actually
kind of very complementary to graphics
graphics is about image synthesis it
assumes that you know everything about
the world and uses that knowledge to
generate your output on the other hand
computer vision is about trying to
figure out what's in the world by
looking through images by analyzing the
images
that you've captured so you could
potentially do things like you know
stereo are you generate that you can use
it for robotics applications or generate
new view new views you can do object
recognition and it's used for search you
can do I tracking or face tracking and
so and so forth and it helps with you
know human-computer interaction so in
this talk I'm going to illustrate my
point about how computer vision
techniques can help graphics and I'm
going to talk about animating paintings
looking at you know 3d videos where you
can actually you know change the view
point while you're you're viewing a
video how do you generate you know
three-dimensional models of plants and
trees and automatic image enhancement so
I'm going to start off with any meeting
paintings so suppose that you see a
painting and you really like and you
take a snapshot of it's just a picture
right and you know you take a picture
and you want to do something interesting
with it for example you know being able
to animate it so the question is you
start off with an image it's just pixels
how do you do something like this right
it seems rather difficult you might say
okay I'm just going to segment it I'm
gonna you know use a computer vision
technique and say I'm going to segment
based on intensity alright and then you
may have several versions so so I
basically show you three versions
depending on the parameter settings you
have but the problem with this kind of
very simplistic segmentation is that
this semantically meaningless right you
give this to an animator and the
animator would have a tough time
reproducing what I've just showed you
what makes most sense however is to
figure out how you decompose the image
into hypothetical brushstrokes right as
I've seen here so for example the
painting of the fish may consider
of these kinds of brush robes and if you
present these decomposed brushstrokes to
an animator the animator would have a
much easier time to figure out how to
animate the painting so what's the key
to this I I think there is some theme
here which is when you have a problem
but it's extremely ill posed you need to
have prior information and in our
particular case the prior comes in ish
in the form of shapes so we got a
Chinese painter to paint all sorts of
you know brushstrokes sorts of that but
you can think of and we digitize it we
got like 60 to brush strokes in the
library and essentially what we're doing
is we're trying to do is to say okay
these are the strokes that you can
assume and this is the observation if we
decompose them we want the brushstrokes
to resemble any of the brushstrokes in
the library it cannot be outside it can
be arbitrary so that makes the problem a
lot easier so here's an example so this
is a painting of flour and just look at
this one so the first thing we do is we
always segment you don't care we just
always like meant you know have tiny
little segments and now our job is to
group them in such a way that the
grouped segments resemble the
brushstrokes in the library so we have
this optimization problem that we solve
to generate a result that looks
something like this so anyway here's
another example so this is the snapshot
of a painting and we ran through our
algorithm we pass it to an animator I
think I mean it took like a few hours to
a day and this is the result that you
see
right so I'm going to go on to the next
topic which is virtual viewpoint video
so I'm sure everyone has seen the matrix
back in 99 right the problem is in those
days you know you have hundred twenty
cameras and and it's a lot more
difficult to generate the freeze-frame
hear what you're seeing is the user can
actually interactively change the view
point while you're watching the video
freeze when you want to and change the
view point so it's also known as
steerable video or in the MPEG
communities let's call free viewpoint
video and at the way that we generate
this is we use a camera system that
looks like this the separation is about
four and a half between cameras we have
the concentrator here that synchronizes
all the cameras and pipes all the
cameras directly into the hard disk
here's an example of what it looks like
that's been captured a very brief
snippet so it's perfectly synchronized
and you can see the viewpoint change in
in rusty russell style so so what we do
is we calibrate the cameras and we apply
stereo the story's a little bit more
complicated than that but anyway so we
managed to get depths on the per frame
basis and we use the depths to
reconstruct the in-between views so I'm
going to show you a short clip of what
we have so all the in-between views are
synthetically generated so we hired a
group of break dancers they actually had
won competitions in Seattle
so we have horizontal and vertical
configuration disguiser
something to does it look great and
literally wash them okay
so this has a rather cheesy storyline
you'll see later
so one of the effects are about to see
is since we've managed to compute that
we artificially defocus things are
further away
so we kind of play with different kinds
of effects another interesting fact you
got about to see is she's gonna leap and
then she's going to freeze while the
background moves all camera movement
within this video was virtual agency to
the camera customers we used to shoot
the film for each camera depth the mats
and matting information was
automatically extracted this information
was useful so if you jump from camera to
camera you get a very discreet view but
because we are computer depth we're able
to seamlessly in total it off the beach
and we can do this in real time as well
well once everything is preprocessed
anyway so now I'm going to move on to
modeling plants and trees now plants and
trees are extremely difficult to model
because of their very complex geometry
so we resort to of course computer
vision to extract the models for us
before I start doing that um what we did
for our work is basically say let us
sort the you know plants and trees based
on the leaf size this is kind of an
abuse of terminology but plans are the
types of plants are those that has a
relatively large sized leave relative to
the whole plant where else trees have
very very small leaf so we use two
different techniques depending on
whether we classify as a plant or tree
so I'm going to start off with plants
first and so what we did is we take
pictures all around
ant and we track the points and we apply
what's called structure for motion where
we extract two three dimensional
positions of the points and extract
positions of the cameras as well so that
technique is called structure promotion
once we have that then we use the
information that we have extracted on
the 3d and the texture as well so we
make use of texture information as well
as a 3d three-dimensional information to
segment into the separate leaves it's
not perfect so we do still do to
interact with a little bit but most of
it is done automatically so once that's
done we then extract or recover the fine
geometry using the 3d points that we
have extracted unfortunately the
branches are a lot more difficult
because they are highly occluded so as a
result we have to resort to our user
interaction but again the user
interaction is made more easy because of
the pre process data that we have
obtained I'll show you a bit later so
once we have the the 3d branches and the
leaves we can put them together we
generate the plant model so as I've said
so we capture a bunch of images between
35 to 45 images and we apply structure
from motion and we get a cloud of 3d
points in this case is colored here
obviously this is not the end result
then as I've said earlier we then
construct a three dimensional graph and
we make use of the three dimensional
information as well as the texture
information that's been that you can get
when you back project to the images we
use that to segment into individual
leaves then what we do is then we pick
one of the leaves as as the model it's
it's it's basically a flat mesh and for
each leaf in the scene we try to fit
and add local geometry to it using a
low-dimensional polynomial model and so
do we do this for every single leaf that
we have found and at the end we you get
what you see over here so what about the
branches well this is what I mean you
make it easy for the user to generate
the branches notice stabs as you draw on
the 2d it actually projects back into
space close to where the 3d points on so
you saw an arbitrary back projection so
now you can actually edit in 2d in image
space and in 3d so as you scroll the
view you'll see that it has maybe you
know move a different way that you want
it to you can actually refine the
three-dimensional location you can also
you know change the thickness of the
branches location of the nodes and so on
so forth
so even though its interactive it would
probably take you minutes if it's a
simple plant okay so here are some
results part of the fun was actually
figure out the names of plants that we
are trying to model
so here we try to do this fall for a
tree and you can see it's not as
effective so I'm going to describe an
alternative method for modeling trees
that that's much more effective in this
so obviously because you have three
dimensional models you can actually you
know do texture replacement if you if
you want to
it's great what texture mapping can do
so here's a final example coming up next
and again because you have the geometry
you can actually edit the geometry
okay so I talked about modeling plants
now what what about trees you can't use
the same technique because it's
extremely painful to model each leaf
individually so with what we do instead
is we do exactly the same as before for
trees where we catch a lots of images
and we apply structure from motion to
extract the three-dimensional points and
as well as the camera poses then here
this is where does a big change in the
algorithm we use let's see ok so we
model the branches right we model the
branches because it's self similar what
we do is we say let's let's figure out
what the building blocks are building
block is basically a parent with
immediate child branches and because
it's self similar we just randomly place
them and generate occluded branches that
way as far as the leaves concern we look
at it from the image point of view we
look at each image we segment two leaves
and we are back project we don't do
stare at this point which is back
project and where it touches the 3d
branch we let it stick and as a result
we get something that looks like that so
this is as before capture between
ten-thirty images we get a cloud of
three-dimensional points and again this
is not sufficient for rendering we
convert the segmented branch points into
three-dimensional served surfaces and
once we have done that we kind of know
that you know which are the parent
branch and which are the child branches
and we separate them to generate
building blocks so what we do next is
that given these building blocks we
randomly place them on top of the
visible branch models up to a point
where it touches you know where the
leaves are
and as I've mentioned earlier we have
the input images we know which part of
the image are the leaves we segment into
leaves and we just back project directly
into the three-dimensional branch and
wait where it gets very close to the
branch we let it stick so I'll just show
you results in here so this is the
recover three points and and we recover
the branches and then this is where we
randomly place our building blocks to
hallucinate the occluded branches and
this is what it looks like at the end
and of course after you place the leaves
here's another example in this
particular case there's only one trunk
of one branch so we basically stole
building blocks from another tree and
apply to this
this is the branch model followed by
application of the leaves so anyway I
think you get the idea there are a few
more examples well capturing images and
generating models using images is one
way but I would imagine it's simpler if
you can sketch it right you you sketched
something in 2d and it it pops it up
into a 3d model and this is what we did
as well so but the thing is again
it'sit's ill-posed so what do we do we
put prize in we in our prize in the form
of a database we have a database of
three-dimensional trees we select one of
the trees is the template because each
of the within each example are we have
information about how the tree grows we
there's a bunch of prime ministers in
there and if you select that particular
example are it inherits all the
parameters and is used to then generate
the branches and you can grow the
branches similar to what I've xpower
explained earlier and you can just add
leaves and the leaves again is part of
the exemplar in the database so this is
what example looks like you have a 3d
geometry could be scanned it could be a
model that you sort of generate on your
own and the tree parameters are
essentially angles between the parent
branch and the child branches and we
assume that these angles are Gaussian
distributed and in addition to that we
generate the two-dimensional silhouettes
and the reason for that I'll show you
very next slide and of course we have to
leave template well the 2d syllabus is
important because we use that to compare
against what you've drawn so what you've
drawn is the projection of a 3d and now
we actually find a distribution of the
2d angles and we compare against that of
the silhouettes and we pick the one
with the closest set of parameters and
then we kind of inherit those parameters
to build a three-dimensional model so we
rely on self similarity and that's the
reason why it works very well we also
the user can choose to draw a crown to
constrain where it can grow so here's an
example of branch propagation so once
once you have drawn it you it will pop
it onto 3d and then using the idea of
self-similarity will randomly place our
branches there and after that you'll
just put in the leaves so here's an
example what happens if you have a crown
we don't have a crown if you don't have
a crown the it will grow like five
generations install where else if we
have the crown you'll grow up to the
limits of the crown install you can see
that the appearance is different
you can actually generate a plausible
looking tree just with a few strokes as
you can see here now if you add in a few
more strokes and just draw a crown all
the sudden have a very different looking
tree
here's another example you can draw as
many strokes as you want and what is
interesting is it it tries to generate a
pretty plausible looking tree
what is interesting is that if you draw
if you use the same sketch but you apply
a different example are you get pretty
different results so you can imagine
that just by changing a few strokes
adding a crown choosing a different
example you get very different set of
three-dimensional trees so this is a
pretty flexible tool I think where we
did this few years ago we had only about
maybe 12 a dozen different example us
but we were able to generate a lot of
different looking trees so finally I'm
going to talk about image enhancement
I'm sure a lot of us have taken photos
that could have looked a lot better like
you need color correction any contrast
enhancement and there are tools out
there that would allow you to
automatically enhance it picasa has it i
was called the i'm feeling lucky button
and windows live photo gallery also has
it that the problem is that you give it
the same image it will give you the same
output no matter who does who did it
right and i think intuitively different
people have different preferences on how
to enhance images for example certain
people might prefer images to be a
little bit more saturated or or have
more contrast than others and this is
not reflected in tools like picasso and
windows live photo gallery because it it
applies some generic algorithm that
analyzes the image and always
predictably give you the same output so
what we did is we basically say okay
let's let's try to capture these
preferences suppose we have a small set
of images in the database and we present
the images one by one to the subject and
the subject basically you know kind of
click along and basically figure out
like which other how you would be how
the subject would basically in
the images and BB EB will save the
enhanced parameters for each image and
given an input image it would try to
figure out what is the closest image in
the training data base use the
enhancement parameters associated with
that closest image and enhances it right
so well does this work we we did some
user study we use 25 training images
this was the the training images were
automatically I don't have time to go
through this but they were automatically
selected from 5000 random images that we
got off the web it was they were chosen
in such a way that it sort of spans the
maximize the span of our image
enhancement space so we have a training
interface where the middle one is
original in the image and we have
different enhanced versions so if the
subject Peaks you know wonder periphery
images you will go to the center and
then it will enhance that version so it
keeps enhancing up to a point where the
best version is the middle one stops and
so at the point then you kind of know
you know the enhancement parameters that
would map the original image to the one
that the subject likes the best okay so
these are some of the results so this is
one example one input image this that
this is this input image is not part of
the database so picasa gives you just
allow for a gallery and you can sort of
see that different subjects give you
different upwards which is interesting
you know when we first started up with
this project we didn't know what to
expect we know intuitively that
different people have different
preferences but we don't really know how
significant this differences are
so like I said we did use a study we
actually started with 15 subjects it
turns out that one of the subjects can't
see color so we had to exclude him we
should have asked them from very
beginning but anyway so we have 14
subjects we get them to train using the
the training 25 images that you've seen
earlier and then we did a pairwise
comparison so we have the input the
subjects we apply the enhancement on all
these unseen images using the database
that we have collected the medium
subject so you have the subject and
everyone else so kind of pick the median
of everyone else so that's the median
subject and then we compare against the
windows live photo gallery and Picasso
so we randomize the order or pairings so
that the user would have to choose the
left or right or no preference so these
are the results it's kind of interesting
um if you if we compare the subjects own
result versus the input it's the the the
mean is significantly more this is the
one Sigma bar so this is significant the
difference is significant subjects
versus the median subject again it is
significant but what is interesting is
that even mean is better than Picasso's
it's not statistically significant which
means that we can sir actually had done
a very good job in optimizing the
enhancement settings but you could
imagine like you know putting our system
on top of the castles where it spits out
an output and then we personalize that
output however if you if you take the
metric where you you say do you prefer
all the images predominantly in one area
one in the air suppose that you know
you're comparing a subjects on versus
the input because we have 20 images
if you prefer 11 or more of the subjects
own we will add +1 to this otherwise
will plus one to the input and by this
metric we are basically kind of doing
well against all the rest so this is
just figuring out if enhancement
parameters make a difference and indeed
it does so the next question is are
there clusters of enhancement right now
we only dealt with 14 people if we do
dealt with hundreds of people is is
there a cluster amongst them why is this
important well if we discover that there
is a small cluster then training is so
much simpler right let's say we have
three clusters then all we need to do is
just maybe give you one or two images
for you to enhance and then we'll figure
out which cluster you belong to and
that's it alternatively you sorry
alternatively we can have a tool
interface that gives you a small number
of buttons each button correspond to a
cluster and that's it so what we did is
again we did not use study you see this
time using Mechanical Turk and this is
the web interface that the subject sees
basically just clicks you know until the
middle one is the enhanced version that
the subject lights and then we build
databases you know we have more than
three are about 330 something people and
we have 330 something databases so then
we analyze them and lo and behold we
actually got a very small number of
clusters which was really surprising to
us we we also had a questionnaire that
we asked them to feel like you know the
gender the age and and the experience
with cameras and so on so forth but we
couldn't tell if there is a preference
one way or the other I mean the old
they're all very similar
in terms of gender &amp;amp; age distribution or
very similar so we couldn't say anything
about that so I've just talked about
four different things animating you know
Chinese paintings and the computer
vision technique that we use with
segmentation for free be free free
viewpoint video we use the computer
vision techniques of multi-view stereo
and for planetary modeling we use
structure for motion and multi-view
stereo and for the image enhancement we
use image auto image correction
techniques and of course we also use
machine learning which everybody's using
these days and of course all these was
made possible with with help with other
people and these are the cast of
characters that help make all this
happen that's it thank you very much
thank you very much the same being we
have time for a couple of questions any
questions for symbian
no questions okay thank you very much
swimming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>