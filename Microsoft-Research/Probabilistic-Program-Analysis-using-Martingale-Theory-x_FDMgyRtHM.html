<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Probabilistic Program Analysis using Martingale Theory | Coder Coacher - Coaching Coders</title><meta content="Probabilistic Program Analysis using Martingale Theory - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Probabilistic Program Analysis using Martingale Theory</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/x_FDMgyRtHM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
thanks for having me here it's a great
pleasure to come to MSR Bangalore
finally I've been wanting to come for
many years now so this is based on
mainly on our recent cap paper which
appeared it's joint work with my student
alex chakra who's also very interested
and he's contributed a lot to this this
project and it also involves some
previous work done and I put some its
name in red because he is our person as
well right so I'll be talking about
these and I'll try and omit a lot of the
tedious details and and just give you
the high level picture if you have more
questions you know I'll try and keep it
informal please just stop me ask
questions okay so what are probabilistic
programs I don't think I need quite to
introduce what probabilistic programs
are to this audience but let me do so
anyway these are programs which instead
of non-determinism you replace it with
random numbers the ability to generate
random numbers is there in every
language so if you look at Python or KML
C C++ dotnet you always have
constructors for generating
pseudo-random numbers but let's imagine
for this talk that they are perfect
random numbers they are not
pseudo-random they're perfect random
doesn't make any difference for us and
you can think of a program like this
where X gets a uniform random value
that's uniformly distributed between
minus 5 and 4 it can be real it can be
integer and and you have the same
programming constructs your while
programming constructs you can have
branches that can flip a coin with
probability 2/3 take the branch with
probability 1/3 not take the branch
right so the goal of this talk is to
study stochastic systems which are
designed in terms of these programs
because these are mathematically
stochastic systems they are random
systems and we would like to understand
these systems exactly the way we
understand programs ok so and let me
give you the target of what we are after
so one of the targets and there there
will be two I will introduce the second
one one of the targets is we want to
answer queries queries could be
the end of the program let's estimate
the probability of X being greater than
or equal to nine okay in a regular
program you would put an assert there
and you would say what's the probability
that the assertion fails or not in a
probabilistic program you you want to
know the answer to the probability you
know in a regular program you just want
to know whether it fails or not okay so
so you can have these queries and our
goal is to calculate these queries the
second goal is to think about
termination of these programs okay
question is does this program terminate
under what sense can you think about
termination of these programs and I will
talk a little bit about how do you talk
about termination of probabilistic
programs what's almost sure termination
and how do you find proofs of almost
sure termination okay so we'll so I'll
talk a little bit about that right now
I'll talk about termination in the sense
that either the program terminates on on
all behaviors or if it doesn't terminate
then I'll show I will give you a proof
system that shows that the measure of
non terminating behavior is zero we call
it almost sure termination so so we
don't think about probability of
termination just yet I haven't done that
work but then it's ill-defined so you
assume that when you ask these kinds of
questions then the the loop has at least
one terminating behavior otherwise in
measure theory the empty set is not
allowed to have a measure so so it's
ill-defined you don't have an answer yes
assuming it's probability conditioned on
termination right so you are estimating
probability X greater than equal to nine
conditioned on the fact that control
actually reaches the end of the program
or wherever you put the assertion right
in the program I'm just putting it at
the end to keep things easier you had a
question you know right so first thing
I'll point out is that there is
no reason really to look at this problem
in you know this is a very well known
problem it can be solved quite easily
just by running the program we often
lose sight of that what can be done by
just running the program and here you
can actually solve this by running the
program many many many times you get a
great estimate of the answer okay I
wanted to state that up front which
which is you know true of probabilistic
programs just by running it you get lots
of good information and probability
afterall is number of successes by
number of trials okay but we want to do
something better because when you run
the program many many many many times
the answer you get is statistical in
nature right and and that's how things
have been done for many years now I mean
for hundreds of years now and the
question is can we now start to think
about a static analysis sensibility to
this problem where we can place actual
upper and lower bounds on the
probability much like in static analysis
we want to try and get the best kind of
bounds that you can place on the actual
probability and and if you get very
tight bounds then then you win if you
don't get very tight bounds maybe you
put in more computation and you get
tighter bumps right so so that's the
kind of algorithms that we want to get
at where the actual probability is
somewhere between a lower bound and an
upper bound and that's going to be the
goal of answering the queries not
estimating the value statistically which
you can always do by running the program
and using your favorite statistical test
we want to do something more along the
lines of static analysis again I'll
explore an approach in this in this talk
on how to do that okay the second thing
is let's talk about termination okay so
if you think about this program and you
think about the non-deterministic
semantics of all the random variables
which is usually what tools do when they
find Ram they treat it as non
determinism verification tools you treat
it as non-deterministic choice right and
if you do so then this program is non
terminating very easy to see there is
one branch here that does nothing to the
value of x and y and you can keep taking
that branch forever and you will never
terminate right in that sense this
program is non terminating but in this
talk we'll talk about putting in the
probabilistic semantics and we'll show
that this
actually terminates with probability 1
and this is called almost sure
termination ok and and we will talk
about how do you prove that a program
even though it may be non terminating in
the actual semantics in the
non-deterministic semantics is almost
sure terminating in the probabilistic
semantics yes domination termination is
is orthogonal right it's like
correctness partial correctness total
correctness right so like you said we
say that this probability of something
holds given the fact that you reach
there now on the side if you prove that
the program is almost surely terminating
then you have completed the picture
right and another thing is termination
is an independent question all by itself
right so we want to look at both angles
which is establishing partial
correctness like properties for
estimating probabilities establishing
total correctness like properties which
also in requires you to talk about
termination also competition based on
observations that's right and then you
don't terminate so yes so right now I'm
not looking at that that part of it
that's that's a more patient view of the
world this is more frequent test view of
the world so so there's needs to be some
work to be done to kind of understand
what's different between the Bayesian
view of the world and frequentist view
of the world but in this talk I'm going
to take this view of the world where
I'll assume that the program you are
trying to reason about is almost surely
terminating and I want to find a proof
of almost sure termination I'll show you
how these proofs look like ok so so
that's going to be the agenda for this
stuff right so I mean let me quickly go
over some of the some of the
applications of doing this kind of work
so one main application is is what we
call robustness of programs so there are
lots of systems that compute risks risk
of earthquake occurring in a certain
place risk that the earth is warming and
and when you look at these computations
they are a computations based on
uncertain data the data that comes into
these computations is not known to a
certainty it's data that's uncertain in
the nature by which you collect data
it's uncertain so for example if an
insurance company wants to say should i
insure this person and how much should
their premium be right if you and
companies are using programs to do this
kind of calculation like large risk
calculating programs right and and the
kind of data they take in our lab values
which will always depend on you know
when you measure the lab values and the
statistical error in the test that you
are using to measure lab values
lifestyle parameters which could be you
know noted down wrongly life expectancy
data which could have errors in it and
so you're computing based on erroneous
data and the question of robustness is
what how does that noise in the data
affect the decision so you want to
perform an analysis to see like how does
the noise effect the decision for every
possible input or in general what are
the inputs for which the noise can have
an adverse effect on the decision right
so those are questions that you can ask
and there you have to compare two
versions of the program a program which
has the input data noise-free computes a
decision input data with some unknown
noise Computers a decision and you want
to know what's the probability that the
two decisions diverge from each other
and that's an example of a probabilistic
program it's an example of the kind of
querying that I just showed you in the
previous slides
another example is state estimation
really common and control systems where
you have sensor data coming in you do
some kind of algorithms like Kalman
filtering sensor fusion algorithms
that's right so you we assume that the
noise we know what kind of noise these
lab tests are subject to and that's if
you assume that the kind of errors that
you are dealing with at least the
distribution of them is some somehow
known then you can do this kind of
analysis to say for every individual
case is it going to be robust or is it
it's the decision non robust and if it's
non robust with what probability am I
making the right decision right so so
you may those are good questions to ask
and I think you can do program analysis
and see if you can answer those
questions another example is state
estimation very similar in flavor
you have sensor data that's coming in
and based on the sensor data you
estimate state this is almost if you fly
in a aeroplane drive a car these are the
kinds of algorithms that are constantly
running in the background to estimate
your speed estimate your position so on
and so forth right and the question
again is how does noise in the sensor
data affect the state estimate and in
turn how does that affect the working of
the system as a whole there are tons of
other applications randomized algorithms
are a classic application as I just
mentioned sensor fusion algorithms
uncertainty quantification is a big deal
in space systems they are interested in
seeing if two satellites can collide
they are interested in calculating what
the risk of it is and this is very
relevant for us at Colorado because we
are very close to the the main control
center for satellites called NORAD
scientific computation you want to know
the effect of roundoff errors tools like
Osprey assume the worst case in terms of
the roundoff errors but roundoff errors
do have a documented nature where they
are probabilistic which is why when you
run a computation it doesn't blow up
after five steps even though your static
analyzer tells you correctly that it
could blow up right so there is a
randomness to roundoff errors as well
and if you analyze that you can see if
you know how long can you continue to
accumulate errors without without having
a big variance in the in the actual
answer so you can treat that
probabilistically anyway so with those
kinds of mode
patience let's move on to what I'm going
to talk about so I'll give you a brief
flavor of something called concentration
of measure which is the main phenomenon
that we are going to attack okay so it's
what is different between non
determinism and randomness the answer is
concentration of measure so we have to
understand that then I'll talk about a
deductive approach so the P LDI paper we
wrote was similar to to Aditya and Sri
Rams work in the sense that we use
symbolic execution along parts of the
program in this paper what we are doing
is we are looking at what is the notion
of an invariant of a probabilistic
programs and we find our answer we
propose the well-known answer called
martingales martingales are very well
known to probability people in
probability theory and what we are
proposing is these are the notion of an
inductive invariant for a probabilistic
program can I'll show you how they work
and and why they should be the right
notion of inductive invariance okay and
I will talk about super martingales and
program termination I will show you that
super martingales are like ranking
functions they behave like ranking
functions and somehow you can use them
to prove almost sure termination just
like ranking functions prove termination
on all behaviors super martingales with
some some restrictions which I'll show
prove almost sure termination and I'll
show you it's very very similar tracking
functions just a few things have to
change and then it becomes very
naturally like a ranking function okay
and and I will show you a few
applications and then this is the
strongest part of my talk
lots of unresolved questions okay so let
me first highlight what concentration of
measure is lots of books have been
written about this so I cannot do
justice in one slide but I'll try okay
so what is concentration of measure so
take a random program like this which is
purely stochastic no non determinism and
let's replace it every pair randomness
occurs with a non-deterministic choice
maybe ok so this becomes a non
deterministic boolean this becomes a non
deterministic number and we will
preserve the ranges as they are okay so
what is the difference between the
original program with randomness and the
program with non determinism okay so if
you send this program to your favorite
abstract interpretation
to whichever one you're using then you
get invariance so you get invariance
like count greater than equal to zero X
plus y greater than equal to minus seven
and so on okay so that the exact
invariants are not as important as an
invariant that's missing count less than
equal to something you will never get an
invariant like that unless your analyzer
is wrong why is that because the nature
of the program with non determinism
tells you that any value of count that
you imagine is achievable 1 billion 1
trillion 10 trillion you know is
achievable because you can always take
the other branch of the non-determinism
right I always take the else branch and
just keep incrementing count and
incremented ad infinitum so the abstract
interpreter is not wrong right but it
doesn't capture the probabilistic
semantics in the probabilistic semantics
if you run 10 to the 7 simulations which
is a huge number for a program this
small ok more than enough so then you
get a nice-looking in a distribution of
the values of counts measured at the end
of the program so I just put a histogram
on the values of constants right and I'm
interested in what's the probability
that the value of count you end up with
is greater than or equal to 35 that your
loop runs for 35 or more times what's
the probability of that ok and if you
look at the Instagram it's barely
visible dislike few behaviors so out of
10 to the 7 you probably get around 100
behaviors or less ok and that's
statistically insignificant so what you
have to do is you have to do 10 to the
10 to get something statistically
significant otherwise your you know your
your statistical test will tell you that
the intervals that you can put on on the
probability are too large to be
meaningful right so you need to run 10
to the 10 or even more simulations which
is an astronomical number right and the
point here is that that behavior is very
rare so even though abstract
interpretation tells us everything is
reachable which it is right the
concentration of measure tells us that
as you get to the tail the behaviors
become rarer and rarer okay so so the
main challenge here is how do we reason
about this the concentration of measure
that some behaviors are getting rare
as you and that's the extra information
that you have to add on without that
there is no value in doing this kind of
analysis and there are many approaches
to this all I'm going to show you is one
approach okay and it works only in some
situations and and but it's an
interesting approach I will show you so
since I talked about the many approaches
what are the approaches so again there
are too many that don't fit in this
category but the nice thing about
PowerPoint is you can make these two
categories and I made two categories
instead of three and I said approach one
is called capture the distribution I
will explain what that is
approach two is a deductive approach and
I will explain that in a second so our
approach in this talk is going to be
approach two but let me explain the two
approaches so what is capturing the
distribution mean so you can think about
in abstract terms in a non-deterministic
program when you do abstract
interpretation or model checking what do
you need you need a representation for
sets of states you can use BDDs you can
use polyhedra you can use you know
combinations of abstract domains but
what they are representing are sets of
program states right in a probabilistic
program you need extra information you
need to capture the distribution among
the sets of states not just the set
that's reachable but what's the
probability of the individual points in
the set that's reachable and in an
infinite state system you need something
more than individual points you may need
a more continuous distribution so you
may need to abstract the distribution
again and this kind of approach is
called capture the distribution what you
do is you define abstract domains not
for sets of states but for distributions
over sets of steps again this has been
done by me I am NOT the first one to
talk about this David Mohney or did his
thesis on this and he has done some
fantastic work in the late 90s early
2000s on this problem I'm sure this work
that's been done before Moneo but let me
just start somewhere and then there's
been a ton of recent work including some
of our work and our PL di papers also
you can think about it as something that
falls in this direction right what's the
flavor of this kind of work well you
take existing abstract domains that you
know like polyhedra or you can take any
existing abstract domain and you simply
attach a number and that's
simplest incarnation of this idea there
are much more complicated incarnation
simplest one is this they can abstract
domain slap a number on it and the
number gives you an upper bound on how
much measure resides in that set so you
can have three objects three polyhedra
the left one says point three which is
an upper bound on how much measure does
your distribution put in this one okay
the middle one says point four which is
an upper bound on how much measure you
put there and and the idea here is okay
you don't know where how the
distribution looks like inside this set
all you know is that if you integrate
this set or sum up inside the set the
answer should be less than or equal to
0.3 and that's a way of making up an
abstract domain and that's David Manos
original abstract domain okay
and people like you know Mike Hicks for
example have done a lot more work on
this to make it much more scalable and
much more robust okay that's right
so you say it should be a distribution
additionally you're placing these
constraints on this part of the space
you say it's less than equal to 0.3
implicitly everywhere else it's 0 right
so it's it's a way of minimally adding
on to your existing program analyzer to
build some capabilities of talking about
probabilistic programs the biggest
problem however is loose if you have
loops expecially like this indefinitely
loops loops that don't have any bound on
how many times they can run so if you
have a simple for loop for I equals 1 to
100 unroll it and we can all go right
but if you have a loop like this right
which does not have an upper bound on
how many steps and absolute upper bound
on how many steps it takes then these
kinds of approaches are going to be very
bad which means you have to keep
unrolling and keep unrolling and you
eventually have to do widening or you
have to keep joining otherwise for the
number of these disjunct just blows up
right so so then what you have to do is
whatever you do loses precision very
fast and you lose precision on meats so
every operation that you do you're
losing precision and usually the unless
you do all these other
you know complicated things that others
have suggested if you use more newest
abstract domain usually we'll end up in
zero one on most programs because you
know it's a good idea but when you have
loops it doesn't scale very well it
doesn't work very well you have to do a
lot more work two loops and you can read
all these papers and they talk about how
much more work you have to do okay what
we are going to do is take a different
approach completely orthogonal instead
of dealing with loops by chasing around
the program and doing some symbolic
execution or abstract interpretation
what we are going to do is find an
invariant okay which is the other
approach in program verification right
find an invariant somehow without
necessarily doing propagation and
widening right there are other ways of
finding invariants as we all know so
maybe we can find a loop invariant the
question of course is what's the right
notion of the loop invariant for a
probabilistic program okay and what we
will show today is read and invent this
notion so McIver and morgan for example
pioneered this whole approach of
deductive program verification but their
approach is mostly for finite state
probabilistic programs you cannot have
these kinds of distributions you can
only have flips and and it's mostly
meant for finite state systems okay what
we are going to show is there is a
notion of loop invariant that extends
McIver and morgan okay and it's called
martingales and it's a very and the nice
thing about it is it's very well known
to probabilistic people who do
probabilities if you go to any
mathematician who does probabilities and
talk to them about martingales they know
it I mean it's very well known in
probability there are been published so
what we are going to show is martingales
are the right notion okay and this
notion X plus y minus 2 con will show is
a martingale I will explain what that is
in a second and and because of that
we'll show some some very interesting
properties of estimating probabilities
will show that we will also show how to
reason about termination okay right so
the one thing I have to notice this kind
of approach is less general if you found
a martingale using your technique you're
good you can do something with it but
there is no guarantee you will find one
just like everything else in
so that needs to be said that's a great
question so so the the strongest I'm
willing to venture right now because I
haven't thought about your question is
they may not exist in the language that
you are interested in and that's true
even for loop in variants you may not
get a linear loop-invariant and it's
true even for modding if you won't get a
linear martingale but I'm sure there's a
construction which is similar which
could construct a martingale for you I
mean the way you construct the strongest
loop-invariant is by doing gÃ¶del
numbering and all kinds of tricks where
you won't get that loop invariant you
won't actually be able to derive it you
know it exists right it's the same I'm
sure the same kind of construction can
be carried out but I haven't tried it
yet but the one thing I can show is
maybe you don't find it using the
techniques that you have to find it
which is very primitive as you can see
it's the you know we are barely three or
four people have touched this problem so
there's not much that has been done so
the state is very primitive so we don't
know answers to questions like that
right so to summarize in our deductive
approach we will find a martingale X
plus y minus to count and we use this
nice inequality called a Zuma's
inequality which works for martingales
and will show for example that the
probability that count is 35 and you
haven't still terminated at this point
in the program now we can start to put
bounds on it will show its upper bounded
by four point four ten to the minus five
even though that's conservative it's a
small enough number that any other
technique for doing this is going to be
expensive because it's a small number
right and when you have to find out
small probabilities with a lot of
confidence you have to put in effort
whereas in this case because you found
this martingale the amount of effort you
put in for finding this is very simple
it's so so so this is orthogonal to
everything else that you can do okay so
alright so without much further ado
what's a martingale martingale is a
stochastic process so what's a
stochastic process then stochastic
process and it's
this stop I'll talk about discrete-time
stochastic processes these are a bunch
of random variables which are just
indexed by natural numbers so x0 is a
random variable at time 0 so if you
sample from it you get a value for x 0
likewise x1 is a new random variable for
time 1 if you sample from it you get
your sample for time 1 so these are just
random variables at every point in time
you get a new random variable if you
sample from it you get a new point in
your process so so then you get your
stochastic sample path ok a martingale
is a special kind of stochastic process
and this is everything that I'm going to
say in my talk there's a message of my
talk a martingale is the process which
satisfies this very strange-looking
thing ok what it says is suppose I have
seen everything from start of time to n
minus 1 I have seen everything that has
happened from the beginning of time to
current time n minus 1 what is my
expectation of what I am going to see in
the next step ok so what it says is the
expectation of what I am going to see in
the next step I don't know what I'm
going to see in the next step because
it's a spread of values it's a random
variable the next step right but the
expectation of what I am going to see or
the average of what I'm going to see in
the next step is my current value now
I'll try and motivate this with an
example but this is it I mean if if you
have to say what a martingale is this is
the definition and there's a couple of
other simple regularity conditions which
element ok so this is all the
martingales it's the expectation of what
I'm going to see in the next step is the
current value ok
so for example suppose you have a sample
path you go along sample sample sample
and you have v-0 as your first 0 at time
sample v1 as your first time sample VN
minus 1 as your n minus 1 at time sample
what's the expected value at time n so
what you can see is that the average of
what you will see at time n is V n minus
1 that's the expectation okay even
though you don't know what it will be
you know that its average is going to be
what is the current value so let me give
you a
averages what the average of what you
would have seen all so the average of
that's right the average of what you
would have seen in all possible worlds
it's like a branching time thing right
so so currently you're at n minus 1 and
the future is branching away from you
right there are lots of possible futures
but the average of what you would see
and the next time is the current value
or what you would have seen in the next
time so let me be it's the current value
let me give you an example so this
example involves gambling so I put in an
old currency so suppose you start with
zero on us okay so you don't have any
money okay and you gamble in the
following way toss a coin okay and if
you turn up heads you get one honour
from the bank and if you turn up tails
you lose
one arm okay so so so you're going to
keep playing this game ad infinitum so
so if you lose one Ana you get to minus
one Ana which means you're owing one on
out to the bank and you keep playing
this game the second time step you can
see you can get either get 2 or 0 from
that from the top branch or you can get
0 or minus 2 from the bottom branch and
you keep playing this game so let X I be
the fourth zone the amount of money that
you have or that you owe if it's
negative at the iteration number I let
that be X I can easily see that X is a
martingale so it's a first textbook
example of a martingale why is that
well what's X suppose x i's the comment
of money you have in hand or youÃµve
right now what's the expectation of how
much money you will have in one step
half probability X I plus 1 with other
half probability X I minus 1 cancel out
you get X I so suppose you have hundred
on us right now you will expect to have
hundred on us in the next step okay so
that that makes it a martingale so if
you have thousand you will expect to
have thousand in the next step okay
so that kind of a process is called a
Marty it's a strange definition but once
you get used to this then everything in
probability theory can actually be
understood very nicely in terms of
martingales it's super fundamental to
probability theory even though
that's computer scientists we don't
quite encounter this randomized
algorithm people do they use this all
the time right so let me give you the
second example of martingale after the
textbook example which is the example we
are interested in so take this program
I've simplified it just a little bit to
fit it in the slide and print X plus y
-2 times count every time you're on the
loop head just print it and if you
terminate just keep printing the last
value you've printed every one second
just keep printing it okay and imagine
that as your stochastic process okay now
question is I'm going to write this
notation to say the value of X plus y -2
count at step n plus 1 ok what's the
expected value of this guy given you
know the value of x at and the value of
y and the value of count at what is the
expected value okay so with 2/3
probability 2/3 probability X becomes X
plus 1 Y it becomes y plus 2 corn
becomes con plus 1 with 1/3 probability
X becomes X X remains X YN plus 1
remains yn and count becomes count plus
1 so do a simple calculation and you get
back X plus y -2 times count so what it
says is the expectation of X plus y
minus 2 count at the n plus 1 at
iteration of the loop is the value of x
plus y -2 times count at the current
iteration ok and and this makes it a
martingale in that the only small thing
is we don't condition it on X plus y
minus count we actually condition it on
the state and in martingale theory this
is called a martingale that's adapted to
the program state it's in terms of the
program state it's a martingale and
that's perfectly fine
nothing changes even though you don't
put the random variable here ok so so
with that we can now start to build upon
martingales and programs right so we
have made the connection we can start to
build upon it right so then the other
notion of super martingales which I will
come to towards the very end and now a
martingale would have been this equals
xn minus 1 in a super martingale it just
becomes less than or
well - and then trivially a martingale
is a super martingale if your equal you
are also less than equal to and if X and
minus X are both super martingales and X
is Amati so these are all very trivial
concepts I mean so I'm not going to go
deep into this if there are books
written on this go to the library if
you're interested you can look this up
it's it's fairly straightforward because
I don't need it that's just - that's
just - not necessarily so so it doesn't
have to be Markovian that's what you're
asking so it doesn't have to be
Markovian all it says is the value is
the value at time n but but that so the
expectation is the value at time n right
but the process the distribution may
depend on everything that happened in
the past so it doesn't have to be
Markovian but we are working with
programs and programs are Markovian if
you know the current state you know
everything else that's known to be known
about the future of the program you
don't need to know how you came upon the
current state right so programs in that
sense are Markovian right so so
therefore that depends on you your
definition of state being appropriate
right but let's assume all that is true
right so then we just need x and y and
content truly but in general you will
need to have everything in your
conditional distribution but then you
will just depend on n minus 1 the
expression will depend on it's a very
subtle point but I'm I'm glad you raised
it but it's really really subtle what
the point you raised is pretty circling
you will typically not need and in
general you won't need it except that
that's because you're only talking about
expectation you're not talking about X
and itself you are only talking about
expectation so therefore you won't need
but X n itself may depend an arbitrary
base that's very very deep
I'm impressed right so we can now say
okay what's a martingale expression so
martingale expression you can say take a
probabilistic program your state is the
location value
to the program variables and a
martingale expression could be a simple
expression on the program variables so
that would be the flow insensitive
version there's a flow sensitive version
which could be different expressions
labeling different locations of the
program and that would be martingale
expressions and and the criterion is I'm
just writing it for the flow insensitive
one flow sensitive is you can imagine
how it's going to look it's going to
look a lot more lattic but not not going
to be any more informative so what it
says is the expected value of expression
as a function of xn given everything
that's happened in the past is
expression of xn minus 1 it's the
martingale condition that that I stated
previously that's just going to now you
are now putting it on program
expressions ok
and likewise super martingales will be
less than or equal to okay there's one
one thing I want to quickly mention
let's say you for all quantifier here
which I haven't placed this it's a for
all quantification on xnxn minus 1 X 0
just like you'll have in a horrible for
example question by the expected labels
and then doesn't mention that's right so
it's like a single martingale for the
whole program but but you don't need I
mean in many cases when you have
different expressions at different
locations in the program then the
martingale has to be thought of as a
function of L and X where it's a it's a
if then else expression if LS l 0 then
its expression 0 with the low head
that's right that's right it is flow
sensitive in that sense it's at the loop
head but the way I will model the loop
is I will take everything happen
in the loop as a single transition in
that case you know it doesn't really
matter there's just going to be one
location in my transition system that's
right when you have nested loops for
example right then you'll have to cut
points and then you'll have to have
different martingale expressions at cut
points we have examples where we show
it's needed I'll give you an example
that's one but I didn't want to put that
whole condition in there because then it
starts becoming tedious right so let me
give you a very you know synth you also
asked it was quite timely let me give
you very quick you know flight through
on how do you prove that a particular
expression is a martingale it looks a
lot like you know proving invariance
okay it has that flavor so what let's
take this expression X plus y minus to
count we would like to prove that it's
actually a martingale okay so then how
do you now chop up the program in a way
that makes it easier to prove it
otherwise it becomes harder you have to
go into all these issues like flow
sensitive and so on right so one way we
chop up the program is we look at the
cut points we just add one cut point to
the end of the program not you know and
then we add a cut point at the while
loop we think of all of this green part
of the initialization of the program so
since it's a stochastic program and we
want to throw away non determinism all
variables have to be initialized if you
let the initialization be
non-deterministic you can sneak in non
determinism into your program and we
don't want that right if you want it to
be purely stochastic then you have to
treat all of this as initialization of
the program okay so now that you have
that we have a way of looking at
transitions where our transition has two
parts one part is the god of the
transition X plus y less than equal to
10 which says you're going to take the
loop and then there's a fork and this is
a probabilistic form okay so we have
this transition system defined in our
cap paper again and then reason we do
this it makes it very convenient to to
reason about probabilistic programs it's
not a big deal but you have to have some
notation for how you're going to slice
and dice a program right and if you use
a CFG then it's not it's not that easy
just take this make it into a CFG it
becomes a little hard so if you do a
little bit of work and make it into this
transition systems with these Forks
probabilistic Forks which I'll explain
second then life makes life becomes
easier and in this case you have two
forks in one fork with 1/3 probability
you can take it and you do these actions
and the other fork with 2/3 probability
you take it and you can do these you
know you do these actions and you end up
back in the red location for this fork
as well as that form so so our
transitions become slightly more
complicated and the traditional
transition system or the traditional
actions that programs can take again and
this is just meant to make life easier
for us make X the previous variable and
X prime Y prime count prime the next
state variable really standard and what
you're trying to prove is for all X
prime Y prime count Prime expectation of
X prime Y prime contra my is equal to
you know X plus y -2 so it's it's like
it you have to discharge that that thing
and in many cases you just do rewriting
but I will show you in a second I think
of it as atomic right and and it's still
not quite I'm not 100% sure that you
know I can defend every action that I
have taken to move from here to that
that kind of a transition system but but
with this kind of a transition system
model it makes life much easier for me
otherwise I have to now start talking
about flow sensitive martingales from
the right from the get-go I have to have
a different expression here here here
and and so that makes it harder so I
just made life easier on myself and and
in this kind of a model you can now
start to calculate the expectation of
this guy it's a and you can use
precondition computation to calculate it
and and when you do the simplification
you get X plus y minus to count and that
discharges the assumption the martingale
proof thing that you have to prove that
across every of these transitions now
that the Forks the the value expected
value of the martingale at the next step
is the value at the current step right
so you have to prove that at every every
transition and it looks a lot like
checking inductive invariance it's not
the same but it looks a lot like it it's
you have to go across
every transition and prove there is no
initialization for martingales because
you don't need it right so let's now
think about proving properties using
martingales okay so since I am running
out of time I'm going to try and move
fast so one of the first things I will
explain is asuma's inequality okay this
is going to be the basis for trying to
find out how to prove probabilities with
martingale so one of the things is let
me try and explain a Zuma's inequality
okay so suppose you start with y as a
martingale and surf X I'm just changing
notation calling it Y okay and one of
the conditions you need is the absolute
change in Y is bounded in every step so
yn cannot be see more or see less than
then yn minus 1 so it's absolute change
yn minus yn minus 1 is bounded in every
step okay and to illustrate I start at y
0 the initial value ok and the red curve
which looks like like a stochastic
process assume that's the value of the
martingale over time and x-axis is the
number of steps ok now what I am
interested in is knowing the probability
that starting from y 0 the the
martingale goes t more than y 0 for some
number T I want to know how it's a
probability that it goes T more than y0
ok at the NX step so there are two
things involved here I want to know at
step n what's the probability that Y n
is steam more than y 0 there is a
symmetric version where it's t less okay
and asuma's theorem shows the following
so it says probability y and greater
than equal to y0 plus T is less than
equal to and this is the interesting
part it's exponential decreasing on
minus T squared which means that as T
goes more and more and more it gets less
and less and less probable okay so the
the message here is that martingales
concentrate around their starting point
they don't they they won't go too far at
least at any fixed point in time they
won't go too far okay so that's that's
an interesting observation about
martingales
okay so they tend to stay
some finite number of steps close to
where they start again you can use that
in very interesting ways and I will show
you one way to use that that's right not
the expectation the absolute value which
means that any possible value you get in
the NH step must be C bounded from you
know so if your current value is 100 the
next step you can be between 110 or 90
if C is 10 or if C is 100 then you can
be between 200 or 100 I don't care how
much the C is even though it goes into
the factor downstairs it can be any
number but it has to be finite
it can't be infinite and there this one
oh this this says okay so why is a
martingale so it's a stochastic process
so yn is the value of y at the NS step -
value of y at the n minus 1 X step
absolute value is bounded is less than
or equal to C which means sure yeah just
keep adding a Gaussian some summation of
gaussians right that's right now if you
don't satisfy for example summation of
gaussians then you go to sure enough of
thing bounds there are but there are
other inequalities so this is just one
inequality for Martin yes there are
other inequalities that are true like
sure enough of thing for example works
for summation of gaussians right but
they're the condition is independence
what you sum up at every step must be
independent so basically you have a
whole book I will show you a bit my bag
you have a whole book of theorems like
these which talk about different
conditions under which you get
concentration of measure okay and and
this is one condition that's very
important where you get concentration of
measure whenever you have a martingale
with this boundedness condition but
there are other inequalities for other
conditions so so it's a very rich field
in in in mathematics and
that's right that's right the expression
doesn't change too much in the coin the
gambling example you said you can either
lose one more than what you lost or
that's right so so you're always bounded
and you cannot you can do it but your
martingale cannot that cannot happen to
your module so if your martingale is X
by two to the end that's okay that's a
program expression that's that's right
yeah so so for example I can show you
now how we can apply a Sumas inequality
and some of the problems or future work
is also very clear when I show you how I
do this okay so I want to do probability
count equals 35 and X plus y less than
equal to 10 at this point in the program
meaning being what's the probability
that I reach 35 and I haven't yet
terminated that's another way of asking
what's the probability I reach here and
count is greater than or equal to 35
it's the same question I asked a way
back right
okay so ivory formulated the question
okay I know the martingale I want to use
X plus y minus to count so how do I use
it so let me take you through some of
the steps and this is the part where you
know we'll have to make an X and I will
show you some of the challenges that
come up okay so suppose you want to do
this we want to use a zoomit inequality
that's why I introduced it okay so let's
do this so let's look at this expression
X plus y minus 2 count
okay so initially I claimed that it's
between at N equals zero and being time
it's between minus 7 and + 7 X plus y
minus 2 count at the start is between
minus 7 count to zero you can forget it
and plus there okay and I would like to
know at N equals 35 what's the
probability that X plus y is less than
equal to 10 and count equals 35 that's
what I want to find out I use one step
of Farkas lemma and I get a consequence
in terms of my martingale because my
martingale is not in terms of these two
my martingale is a different expression
but I can do a simple trick I can say
those two X plus y less than equal to
ten count equals 35 in turn imply this X
plus y minus 2 count less than equal to
60 okay
so therefore to estimate the probability
using a Zuma's theorem what I have to
show is the best case the martingale
starts from this step minus seven and
goes to minus 60 in 35 steps if I know
the probability of that that's an upper
bound that's rewriting it as an
implication on the martingale expression
so if this holds this implies this on
the martingale expression why is that
you can use Farkas lemma X plus y less
than equal to 10 two times count equals
70 then this is less than equal to minus
it's a standard move that you can make
and you can say together they imply this
bound on the martingale expression
because that's expression that you can
work with into that's right you want to
convert it the probability on the
martingale taking a value right and then
you can you can put asuma's inequality
you find the right numbers to plug in
which is part of the challenge in
mechanizing this whole thing and when
you find the right numbers to plug in
you get an answer right away
and here I just did very simple
calculation always kept rounding up to
the nearest integer so it's not it's not
very conservative it's pretty
conservative right so but you still get
a good answer you get okay it's less
than equal to four point five ten to the
minus one right just by knowing the
martingale you can do that right right
but the challenge is this is this is
kind of like theorem proving up your
you're given this martingale and you're
allowed asuma's inequality but but the
point is there is some human guidance
right I use some of my intuition here
and the challenge of course is how do
you make a nice this whole thing as much
as you can so this part whatever I told
you in this slide can be
tonight's we've done that it's not it's
not too hard but of course if it's a
different pattern it fails right so so
we have to watch out for that for
example you can ask a very simple
question come greater than equal to 35
and I haven't yet terminated it's not
the original question but it's a related
question now how do you find out okay
what we can do is we have to do a series
summation but that's that's a little bit
harder to do okay but but you know these
kinds of mechanization is harder to do
but but if you can conquer the the act
of mechanizing this then you can start
to use martingales to prove some very
cool things about these programs right
so that's that's the whole so let me
very quickly go through this part
because I'm already running out of time
so we can talk about two things
mechanizing the discovery of Martin how
do you find these martingales in the
first place without having to provide
them and computing the probabilities
using pumps so those are two things we
have to make a nice so right on this
part of the talk I am talking about the
probabilistic invariance or proving
establishing bounds on probabilities
I'll give you a quick overview on
termination like a couple of slides
overview as well no it wasn't about
termination but there is a version where
you'll also we can also use the
martingale to prove termination that's
right right yeah so this is for
probabilities finding probabilities
they're the cap paper has two parts
finding the probabilities and proving
termination so and we show that
martingales are useful for both so I'll
give you a to slide overview on that to
its end so how do you discover
martingales okay so if you know anything
about the work we did in the past then
you will have an idea of how we do this
but let me explain so the goal is given
a program we want to synthesize
martingales okay so how do we do that
well we do the standard trick assume a
template expression for the martingale
so at every location
L
assume that there's an unknown
expression CL of X plus DL it's a linear
expression in this case otherwise you
can use a nonlinear expression if you if
you so wish okay but you have your
template I see that's how we discover it
and and then derive constraints on
unknown coefficients that's how we do it
okay and what we have done in our gap
paper is extended the constraint based
approach using Farkas lemma so we have
taken that approach and extended it
rather than synthesizing invariance we
can now synthesize martingales using
that it's not hard to imagine how here
we are going to do it let me give you a
very quick to slide overview on how this
is done let's take this program okay
let's take the transition model that I
have and let's say C XC sub X of X plus
C sub y times y plus C sub count times
count is the unknown martingale that we
want to synthesize now I have to
translate that into constraints on C sub
XC sub y C sub count right in this case
it becomes really simple in general you
will need to use Farkas lemma okay and
here what I do is I write the expression
expectation of the next step in terms of
this arithmetic so the arithmetic is
quite easy to do and when you do this
arithmetic you get something that looks
like this so you get C sub xx prime C
sub xx C sub YY prime C sub YY and so on
but you have this extra 2 by 3 CX plus 4
by 3 CY plus C count that's sticking
along there now if you wish that away if
you say that whole thing is 0 at the end
it will magically become a martingale ok
now this can be mekin X we use Farkas
lemma and we have a systematic approach
in our paper this is just an example I'm
just showing you how it works right
right so you get this constraint 2 by 3
CX plus 4 by 3 CY plus C count is 0 and
if you satisfy this you get a martingale
right and so immediately solve it you
get 2 martingales you get 2x minus y is
a martingale X plus y minus 2 con this
is a simple example but we have a more
more systematic way of doing the same
thing using constraint based invariant
generation right so we applied this on
some other examples like we have an
example where we model a submarine
getting a navigation fixed submarines
get a GPS fix and then they go
underwater where they may not have GPS
so what they do is they integrate their
last known position and every move that
they are made since then every direction
that they are traveled and how far they
are traveled how fast they are traveled
and how long they are traveled they
integrate that to find where they are
right now to estimate where they are
right now and you can think of sensor
errors in estimating their their
direction for example you can think of
water currents that are pushing them
along and if you model them as
stochastic the question is can you
analyze the resulting program so we made
a program up modeling a submarine
modeling some stochastic moves of the
submarine the noise that comes in and
then we wanted to find martingales we
synthesized martingales for example we
showed that the the state x actual
models the actual x position x estimate
models the estimated x position and we
get a martingale that says x actual
minus x estimate is a martingale what
that means is those two things remain
very close to each other X actual and X
estimate the difference behaves like a
martingale so those are useful things to
know in in examples like this and the
program is already bigger than what I
can fit in a slide but it's there in our
paper and you can you can take a look at
it okay so this is to give you an
example of what the kind of things we
can do here right so let me give you a
one slide overview on termination okay
so let's take a simple program here is a
program that terminates and the way you
prove termination is using the ranking
function 10 minus X what's what's great
about this ranking function well it's
lower bounded whenever it hits zero the
loop terminates right and at every step
it takes a decrease right but suppose
you replace the point 1 by a uniform
random number between -5 and 5 point 2
then 1 right then you can start to say
okay maybe the program doesn't terminate
right or maybe it does how do you reason
about termination what's the status of
10 minus X now so what we will observe
here is two things one is 10 minus X
does not decrease in every step it can
increase okay but in expectation
it decreases so it behaves like the
martingale as supermartingale okay and
what we are going to show is that's
enough that proves that the loop is
almost surely terminating okay so it's a
easy application of martingales okay so
what what are we talking about so we are
talking about what we call
supermartingale ranking functions are
what we call Smurfs and the idea is you
want an expression such that the
expectation of expression at in its step
is less than or equal to the expectation
at the value at n minus ten minus some
Epsilon it looks a lot like a ranking
function okay except that you have an
expectation here if you didn't have the
expectation it could be like a ranking
function it would be a ranking function
except that there is an e here which
makes it a smurf okay and it's also a
super martingale that's why I call it as
much right so that and the other thing
is in expectation you decrease and and
when you hit the end the loop terminates
but unlike a Zuma's inequality there is
no need that the absolute change the
expression is bounded all those
conditions don't don't matter to me
anymore just this condition okay
epsilon needs to be a small but fixed
constant it can be it needs to be
positive and fixed that's all we need
for epsilon so the main theorem in our
paper is that if a loop has a Smurf then
it terminates almost surely okay the
proof very interestingly enough is
nothing like the proof that used for
ranking functions which is using
induction
right it's using a form of reflection
that you use to prove or well-founded
next right there's no such reasoning in
this kind of a proof so we don't use the
fact that the domain is well-founded
even though it looks well-founded what
we instead end up using is something
called doobs forward convergence theorem
which is a standard result on martingale
so if you open up any martingale book
within the first 15 pages you will get
to this result and it's like the fourth
or fifth theorem they'll prove it's a
major result and we use this result to
to actually prove that fact which is
very interesting so we don't we don't
quite know what what's the status sorry
I'm running over time so for example in
this program we find 10 minus X minus y
is this muff right
so in this program for example we find
flow sensitive Smurfs
so at location L zero it's one location
L one it's zero I won't go into the
details we modeled a large this is
fairly large I haven't I haven't put
everything here it's like 50 lines or so
but this is a program that models
roulette so it's a program that models
the betting strategy for roulette we
have the person as long as their money
is more than ten
they keep betting and once money becomes
less than ten day they exit and and then
there's all the probabilities that can
happen in roulette okay so for example
can you prove termination we did and the
supermartingale we found was money so
money is the thing that you end up
losing right so all right and so in
terms of ongoing research I want to do
some stuff with hybrid systems which is
my usual hunting ground so but there I
need to work with continuous time
martingales and not discrete time
martingales and life is much harder with
continuous time articles everything I
said in this talk does not hold for
continuous time writing it so we have to
rediscover things there what are we
doing right now so we are doing some
interesting things with what's called
the do martingale method of bounded
differences if you take up any book on
randomized algorithms you'll find a
chapter on it it's it's a wonderful idea
and we are trying to mechanize it so and
that's mostly it we are trying to make a
nice the inference so that we can apply
whatever I told you much more robustly
to a larger variety of programs right
and that's mostly it thanks for
listening and you know I will take any
questions you have offline for lunch</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>