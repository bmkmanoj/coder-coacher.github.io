<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Software Listens In: Emotional Intelligence Through Affective Computing and Mobile Sensing | Coder Coacher - Coaching Coders</title><meta content="Software Listens In: Emotional Intelligence Through Affective Computing and Mobile Sensing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Software Listens In: Emotional Intelligence Through Affective Computing and Mobile Sensing</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4W1VmledkRg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay folks I think we should get started
I'm very happy today to introduce you to
now yang she's from University of
Rochester and she works in a very
interesting confluence of areas
including wireless sensing speech HCI
and most currently affective computing
so I think she's going to tell us a lot
about that today and I just want to
welcome you yeah thanks Mary thanks for
having me so today it's my pleasure to
come here and thank very much for
attending my talk and so today my talk
is about emotion sensing so emotions the
right really primary form of
communication between humans and that
originated a lot of HCI project like
behavior sensing or designing
context-aware systems so today out about
how we can use speech for emotion
sensing and how we can come back noise
in the mobile scenario if you want to
apply that into the mobile scenario so
we talk about emotion sensing their lot
of application can be enabled for them
powerful son I think most people want to
do that in the gaming scenario or this
robot is a relief the last order last
month by softbank the code pepper so
kind of a member in the family that can
talk to people also for health like we
want to might hurt the emotion emotional
state for people and a pleasure like for
drivers or representatives working in
call centers and also asked I designed
these very quick and pretty West that
can one of the application I think can
be used for children suffer from autism
so they can better communicate with them
by visualizing their emotional state
also in behavior study and that actually
originated our research in University of
Rochester so our engineering team is
collaborating with some psychologists to
deal with
some problems that teenagers and the
family members has so that's a very
interesting interdisciplinary research
going on and of course mobile so with
increasing applications in mobile
applicable platforms so there are a lot
of contexts can be provided through the
mobile platform like how fast we have or
how many arrows we've made but of course
still speech in the primary form and
well I will talk about how we can sense
the ascent emotion through speech so at
micro cell research is a great
opportunity that so many researchers are
actively working in this field like in
the web group effort I've affect our
scent and visualized through bio sensors
or some effect effective fabrics also
evan has one project working on try to
detect motion using speech signals and
deep neural networks and apply that in
the Nordic gaming scenario also this
direction of robot so researchers has
been try to design systems that can
communicate with people in a more
natural way also some other research
going on by using social media and also
mobile sensing some other contacts for
emotions and then academia that's still
very hot and very active field that a
lot of researcher Buddha put their
effort here just to mention the field
that motivated my research so but seeing
that the research I focus on is motion
sensing using mobile platforms the
reason is that mobile is very personal
items so air people want to carry that
all the time so it might be easier to
capture the users real emotion and also
if we can apply the system by training
using the users own voice that can
generate the system with the more with a
higher accuracy by using speaker
dependent
also during our chat with Paul the other
day because it's rather difficult to
detect passive and the negative emotions
especially when the user has carried
that emotion for a long time so mobile
phone on the other side is very good at
long-term monitoring and of course
mobile can provide other contacts but a
my oldest contact speech is easier to
capture can better to save battery life
and also if we can just detect motion
through speech features not what the
people speak they can better prefer
preserve the users privacy but if we
apply speech a basis in a speech based
emotion sensing in a mobile platform
noise is a factor that cannot be ignored
so that formed the starting line that I
will first talk about how in the next 40
minutes how we design a speech noise
resilient speech signal extraction
method and especially since pH is very
important feature out how about how we
design noise within the protection and
how we design the system to classify the
emotion based on the speed features
extracted and finally how we apply the
system on the mobile platforms ok so
here are just a given notion what peach
is so peach is defined at the highest or
lowest at a tone as perceived by the ear
so if we play this sample 309 309 you
know which a curve in those the highs or
lows of the peach in frequency 309 309
sure how many things that's a in ok and
the rest are be ok right that's actually
be cuz that 309 so yeah we really need
the computer to help us do this
protection
it's a very simple illustration so pitch
has can be used in a variety of ways
example you can detect pH and do the
emotion sensing the first one and also
this the katana so we can do speech
recognition and also in music we can do
automatic music transcription or music
information retrieval so this plot will
show show us what Peter looks like in
the Frankie domain so if if the house is
a earth dream is generated from our
lungs like a supply power supply and we
the peach are vibrating are generated by
vibrating of the local court like an
oscillator so this sequence as we can
see here so this in the frequency domain
for those are spectral Peaks so we can
see the first down in the peak here is
the ph value that we want to detect and
although some are all the peaks followed
by our code harmonics after the signal
go through the vocal tract like a
resonator it is shaped by ups and downs
so this is the this figure on the top
shows how the speech signal looks like
in the fruits domain that comes out of
our mouth it's not necessary that the
the pitch it has the highest amplitude
so that as a difficulty to detect peach
compared with that signal this signal is
0 what do we generate with their ODB
babble noise so we can really can't tell
which are from the noise signal which
which pigs are from the speech signal
okay let's bring back the clean speech
signal so we can see the peach located
at 192 hurts if you met that to the
forensic frequency horizontal axis and
it is followed by the first harmonic
second third and the fourth but you can
see the peak corresponding to the noise
speech can also be very high in terms of
amplitude so our key notion for the
algorithm we proposed called banner is
that we just use peach frequencies not
amplitude so our algorithm is just based
on calculating the frequency ratio
between those slightly the peaks the
first step is to calculate the five
picks with the lowest frequency so this
time will be this one two three four
five picks will be selected and this lip
look little bit complicated but I will
explain it in more detail so this picks
this one two three four and five peaks
are summarized in this table and we
calculate the frequency ratio between
each pair of them for example is 2.04
calculated here and then we met all
these values to this table so this table
is what the ideal harmonic ratio is the
expected harmonic ratio with ample the
first harmonic should be true located at
twice of the peach value ideally for
human speech harmonics are placed at
integer multiples of the peach value so
that for example 100 200 and 300 but in
this time we can now tell what these
five selected peaks corresponding to
whether to noise or speech and which
harmonic which order of harmonically
belong to so we calculate the frequency
ratio and God is peach candidate and we
combine two additional peach candidate
won it from the cat from method because
our method just to focus on the low from
C domain and by incorporating the
capture on ph value we can have a better
view of the global information in the
fringy domain and the other additional
pitch candidate is code lowest frequency
because for some of you familiar expert
in the signal processing domain for some
speech signals there's if we look at the
spectrum there's only one down in the
peak so using this panel or with them we
cannot calculate the ratio which into no
there's only one so using this by
combining all these pitch candidate
calculated here and above we put them
all into a table and we can how many
clothes candidate are they and we come
them at a confidence score so that the
number of multiples remember you can see
this candidate one or 90 has the highest
number of heightened number of multiples
so the higher the Covenant score the
more likely that is the real ph value
the final step of this banner witham is
to use builder call function to
calculate the cost between all the peers
candidate between neighboring frames and
the cult function the first term is if
the difference between two kinda dar
small which is more likely for human
speech we give that a lower number and
the cost will be low this is a covenant
score the higher confidence score the
lower the cost and then we go through
all those frames and pick up determine
the final pitch from all these
candidates and that form our uh now with
them so to evaluate the band algorithm
will introduce one moment metric is
called growth page error rate if the
desired pH debate more than ten percent
of the real peach value we think that
Peters Ronnie detected and the growth
page are already can't the percentage of
those Ronnie had hit the frames so the
values if that value is higher that that
is worse and we use the ground choose
for example I can play this audio file
again 309 so that is simple for the
previously and we combine the detector
peach value from three very well
performed algorithms and averaged their
value to be the ground truth so the
noise will add and wise to the clean
speech for them poll today that one who
ate in the baboon North scenario today
why'd ya so we use evaluate our system
in a different type of newest and in
different noise level 108 the most
cleanest one what did you do today
that's their DB most annoyingly one and
we compared our banner I rhythm that's
our banner with them with both classic
and very modern algorithms so now we're
going to list it in blue other evidence
that i will compare with later because
double noise is the most common very
common type of noise so we can see we
try as in our level from the ODB with
you the worst so you can see the growth
speed are rated highest to the very
cleanest one with only 20 DB snr and we
can see our out our banner algorithms
perform better than all the previous
algorithms and this is just shows the
result on one dataset way valid and in
our research we've added on in total
three different data set so i just a
present several without here with that's
a twenty sample for this one but that's
by arranging o different type of noise
and old different of noise levels so
that's still a large it as data set and
we have another called CSTR and cue yeah
maybe you're familiar with that more
than a hundred clean samples and if we
multiply it by one two six one two three
four five different eyes on our values
and x eight different types that's a
huge data set
are they all coming from are they all
the same samples with noise added or any
coming with the environmental noise uh
with with a pop of North added they all
start office between samples yeah and at
elaine's typical of the example that you
should or bracelet are some samples are
around 10 10 seconds or some are two
second or three seconds and are they are
they are prompted speech reading
conversational speech Darryl promised
speech that's more easier to gather
ground truth speech value yeah although
there are still several algorithms test
on speech recorded in real Northeast
scenario but that's hard to get a ground
truth yeah that's real culture so since
PFLAG and Ian are the two most
competitive algorithm we compare the
performance at zero DB for different
type of noise so you can see bana get
the best performance for four out of
eight different type of noise and also
the opener algorithm is open source and
we made it available on our group's
website and also we develop a
application that can visualize your pH
value if you're interested in your when
I welcome to download it ok so the
second topic how we can use these
extracts speech features to do emotion
classification so emotions are can be
classified in the arousal level active
or passive or in a valence level
positive or neg view some people also do
categories with ample all these blocks
here show different emotion categories
in our research we only pick up the six
oops we only pick up the six basic
emotions because that's easier to
compare with others work that also use
the same six emotions and also these six
motions are what we used in psychology
studies there are so many data set out
there with emotional state some are
acted or acted with new natural
conversation but for
network incision we have to use let
human coders to label those emotions so
we just use acted ones at least once are
we invite actors or actresses to perform
different type of emotions some use
audio or audio + visual of course isn't
it will be more complicated for the
system if we combine other modalities so
we just use audio and also one of the
challenge that previous researchers
proposed is that it will use both audio
and visual some speakers extend to
express their emotions only by facial
expressions so if we just gather audios
information it doesn't tell us much
about the emotion so we just use audio
so we only focus on english so we choose
this LDC data set there are other data
set out there for different languages so
the LDC data set it just contains
numbers and dates so the speech content
is neutral meaning but are expressed in
different ways with ample this male
speaker with different emotions 108 it's
happiness August eighteenth you can hear
the difference December first March 21st
2001 December twelfth yeah so that's a
very good data set and especially we use
that because a lot of literature use
that so it's a good way to compare our
result with other benchmarks also
through our collaboration at unity
Rochester with unity with giorgia we
also glad over 10,000 samples from anna
grass and express different ways but of
course there are not professionals so if
you listen to their recordings october
12 until the
4001 203 yes so if my part of you that's
quite similar and especially that's
apparently recording the relative and
widely environment so be sure yeah what
conditions where they requirement were
they with the recording over the phone
or yeah i think the the phone call is
not that good but they do over the phone
yeah yeah those ain't killers yeah yeah
so yeah i'll show the result on these
two data said she has numbers on how
well people do a classified US um they
just do the recording but we don't have
human coders to actually label whether
that really match that emotion yeah so
these are the speech features we use
both in the frequency domain and in the
magnetic energy so these are just very
basic features that people already
widely used and we also include the
difference of ph and the difference of
an energy because that can give us a
better picture of how peach the tone
changes how human speech changes over
time and for all these features we
abstract five statistic values and plus
speed speaking rate so in total our
feature set is 121 metrics the
classifier we use a support vector
machine the reason is that comparing
with unsupervised learning method we can
take advantage of the labels that for
the emotions and also we can we use our
RBF kernel so that can better do with
the linear inseparable data and also we
have the C printer that can be tuned to
prevent overfitting of course there are
other method out there and the deep
neural networks are cited by the method
used by a president ellen and his intern
so the novelty I'll talk about nobody of
our system right now if we listen to
this sample okay the second quiz comes
what is the emotion for this
simple jun 28 may 29 September third 505
312 to yep okay yeah ready the second
one October twelfth 8005 906 203 so how
many think that's okay and but I saw
some didn't ok so here it comes the
diversion so for some samples we can
clearly see what they are want to
express but sometimes samples it's
relatively ambiguous so our approach
just use these psychotic concept to try
to throw away those ambiguous sample is
ambiguous or someone knows maybe yeah
someone with them I'll throw away you're
throwing away based upon people labeling
that's ambiguous or by your algorithm
coming up with ambiguous things yeah by
the system by the confidence of the
system of the output that sounds really
easy idea but we want to see how that
can help us improve the accuracy of the
system yeah so our season is quite
simple we just use support vector
machine once with well against all
classifier for different emotions for
example happy or not and we join the
system using some part of the LTC data
set and we do the cross-validation to
test our new samples or we use a speaker
independent method to try using an eel
speaker to tell the performance of the
system so the fusion center just to
compare you can see here just compare
the one guest
with a higher confidence core with a
threshold gamma so gamma can can be
controlled by the user if the rat is
greater than gamma then we are confident
to clarify that sample to be a certain
emotion category otherwise we just
reject that sample in our system we use
three additional enhancement strategies
speaker normalization oversample
training set and a feature selection to
better improve the accuracy so this
shows the equity versus the rejection
rate so rejection rate means how many
how many percent percentage of samples
we throw away we can see that as we
throw away more samples the higher
accuracy can be again with a curve
growing from around eighty percent if we
throw away half of the samples let's
around nineteen ninety two percent so
eighty percent if we throw away nothing
eighty percent is can paired with one
over six because we classify one emotion
out of six emotion categories that
around 70 seventeen percent so you can
see that a huge improvement so this is
based on the cross validation for the
LTC data set and the red curve is for
the general test this blue curve it for
the gender dependent on male and the
black curve is our internal design our
females you can see if we tryn system
using only females be female speech and
test it on female we can have a better
improve better accuracy here but still
we are treating off the higher speed
system accuracy with throwing wastes are
several samples and that's based on the
understatement that was some speech
based some speech based systems or
applications we can throw away several
if we are not confident with them
it seems like there's a important thing
to man hbu maybe also screening the
stuff earlier askew bring it up earlier
that like you know probably it probably
matters whether the classifiers notion
of confidence corresponds with human
judges notion of confidence because
those are different it might put you in
a funny place like you might be
rejecting things that humans don't find
in the US yeah yes you want to make sure
those are aligned that's one thing and
another thing would be to save it like
how you know the notion of confidence
that you have what does that look like
when you're looking at say for instance
natural speech and you know is it does
it end up because I would expect that
app that samples tend to be quite
extreme yeah you know that's really
invested in natural speech very
interesting yeah Beck's actually bring
us our two current ongoing research help
to answer your first question we're
actually comparing the without the
performance of our system with human
coders because we're uploading all the
samples to Amazon Mechanical Turk her
Turks and we want a human coders to code
and it's really interesting to compare
how the system works and to answer your
second question are right now we just
based on LDC data set because that's
been widely used I will show the
performance comparison with other papers
later and it will and that bring us our
future work because we are collaborating
with some psychologists they are doing
interesting user studies on using real
conversation between family members or
some conflict between teenagers as an
interesting group so yeah that should be
josie experience to make the accuracy
based on the samples that left after
drawing fifty percent of them or
vegetable all samples yeah that's very
good question we just ignore the sample
throwing away and justice based on the
sample that are we're confident to
classify their emotions and then used to
make the accuracy basically what's left
yeah so that explains the monotony yeah
we go yeah yeah
for adding that in the previous light
collation speak another patient how do
you do this speaker annihilation yeah
okay so each speech if the future
calculated based on one speaker because
we have one speaker express different
emotions so that's a around 100 samples
for each speaker and we for each matrix
which is the statistic value of the
future we carry the mean and standard
deviation and we do the disc
ornamentation so use knowledge of the
screen grab it yeah so that's risk the
challenge if a new speaker comes we do
know what a normal speaking rate for
that I think that's a quite worried
challenge right now you yeah thank you
so yeah that's what our knowledge is
based on twinning off accuracy with
throwing way data so here comes the
interesting corpus at how about the UGA
data set so we can see the performance
drops a lot from around eighty percent
with no data rejected to around forty
five percent and we also compare our
without with others work and because the
arrow measurement metrics are different
for different work so then the numbers
are not consistent so our system is
listed on the bath and the numbers from
the the emotions test paper is lifted on
the below and the numbers showing both
are witticism doing better and this is
based on the speaker independent
training because this the voice from the
new speaker is not used for training the
system so you can see perform the jobs
by up around forty percent in the stable
taking some level of gay rejection oh
that's note that there are occasions
yeah that's a starting point of the
curve AC
like in your class medevac receive your
numbers are in the 90s oh yeah that's
Roku clutter so clogged for our level
means that we just measure the accuracy
for one Gasol classifiers vetting your
previous figures when you had your
percent data rejection they started off
in like the 70s yeah so classifier level
means um we just a measure the
performance here whether it is happy or
not so that's just a binary
classification we are comparing with the
baseline of 54 that's 5050 yeah decision
level means we after diffusion yeah here
here the division of the final outcome
if you go set aside for single slides
forward to the second database results
are here forward Oh other without you
yes so you get the heater fix' often
found the recognition ariat obviously
because the left is artistic in
transformation and it's kind of higher
emotional contrast yeah this the correct
classification rate is weighted or non
waiter now why did I what is the
proportion of different emotions in your
dataset uh that's almost a balance so
pretty much equal amount of different
emotions in your dataset yeah neutral
Realty is small that's very much awaited
this uh yeah yeah yeah thanks yeah right
thanks for adding that so uh you might
feel a spin with a term with code here
it's called division level correct
classification rate so we just sum up
all the samples from all the emotions
correctly classified with all the
samples we tested yeah so
thanks in those of the absolute number
of utilises recognize it correctly I 10
percentage presented yeah so this plot
shows how new ice will influence our
result we are comparing with the red
line here with a chewing and the test
unclean speech if we try and test on
Norris speech you can see the
performance a lot oh sorry if we chill
unclean and pestle annoyed speech we
wanna see the Performing drops allowed
and the two curves in between are if we
turn on the same type of noise and
passed on the same type of noise that
then dropped too much for the
performance so that conclude that if we
twin a test illinois d speech we would
rather chew a non-royal speech so
finally for the mobile app
implementation doing my previous
internship here at a macro cell research
three years ago I got honored to
collaborate with my mentor argument send
me the connections group so we developed
our this kind of a prototype for the
emotion sensor mobile a motion sensor
and that's a quite simple one can only
classify the user is happy or not happy
so it would just capture the users wise
in real time and between the system here
which win the system offline so we can
train on the LDC data set and we apply
the twin model to combine with the
extractive speech and get a pretty
emotional result and this work has been
presented on microsoft research tech
fest in 2012 and also we want the the
ability value award for the microsoft
science car garage sign fire yeah that's
my mother holding app so ok here comes
the great time
that's just a very simple prototype for
binary division and this can implement
the entire system of course that in my
lab who can visualize the extracting the
speed features and we can select which
model we can use okay can put out the
Glee here
so this we can free them pull the court
a load of sample from the LDC or UGA
data set which just a random choose one
4012 okay so the the real emotion is
anger and that's expressed by a female
speaker if we set a search word gamma
value to be zero we don't want to throw
away they sample and you can extract the
features pixel sometime
oh sorry I didn't drag the window here
so leave the speed features it's
extracted and we if we choose to tell
the system using the model train by LDC
data set okay let's plug here we can see
the plot is showing on the forecourt
quadrant and that's a angry emotions so
I can show they squeeze in more detail
after the talk so that can just okay
here comes without happier or other
emotions to summarize my research is
based on how to come back noise in the
mobile gaming or mobile voice based
application scenario and that will let's
motivated us to design noise resilient
speech feature extraction method and we
have apply oh the banner algorithm and
also other state-of-the-art speaker
extraction method to clarify emotion
based on a standard data set and based
on and apply that on real users finally
I just show a very simple prototype
further to how that can be used in
mobile platforms the app is called least
in a few and these are the papers and i
also have background in sensor networks
and what is communications and these two
journals are still in a review for
future work it besides the pitch
detection algorithm we are still working
on how to improve other speech feature
extraction method like mfcc or other
features to improve those algorithms in
noisy environment also we can want to
continue to implement the entire system
in the mobile platforms also as you
mentioned we're continuing to product
one on Mechanical Turk and the other
through the
real data gathered through the
collaboration with a security department
so actually my research are just based
on if we profound the first sensing and
interpreting these two stages for the
entire affective computing that that's a
really broad range of research and
applications also we can work on
visualizations and designing interesting
designs how to visualize people with
emotion also we can take interventions
when that is necessary to help people
for example with depression we can
monitor in their term and get an
introduction into interventions but if
we just step back a little well we look
ahead so at the end of my talk I want to
share some note i read the reasons to be
cheerful part for that from this blog
article sometimes we want to step back
and rethink whether our emotion sensing
work is really tailored to one's need
whether the user may feel impelled to
those to do those suggested cheer work
whether we can just to present the user
to be cheerful or actually we can
improve their emotional state so this
author did a quite interesting kind of a
survey for all these ads ran out on the
market that can try to cheer your app so
by trying all these different kinds of
apps she thinks whether the user just to
feel happy after playing the app or that
can really tailor to that person any
person's personal need and really
improve their emotional state I think
that's one of the kind of a very good
guideline for my research in the future
okay so for some possibilities applying
this emotion sensing system to Microsoft
product that can be applied across
different devices or across different
services and a device with a mac
can apply this system and for with one
of the kind of an interesting one is
SmartWatch so fitness emotional fitness
is I think proposed by Mary because yeah
that's also an emerging market right now
for all the currently available product
for them postmarked band or SmartWatch
they just focused on fitness for example
monitoring your heart rate but if you
imagine your watch can listen to your
voice and really understand your feeling
that can help to improve your emotional
fitness so that's a very interesting
concept I want to brother here for
different services that can use this
white space in motion sensing in gaming
scenarios like connect and call centers
to monitoring those arm representatives
and working under pressure white search
Fernando on the beam platform and target
targeted ass of course Cortana if you
can imagine your digital assistant can
understand your feeling and that's
provide a similar scenario like the
movie whore so Justin not just to
communicate with you tell what the
emails are there but also can comfort
you when you are done so that should be
very interesting scenario and scalp
translator so this is the demo on the
for the set Skype translator and the
code conference and a person is speaking
English is communicating with other
speaking German and we can see that the
big smile on the screen and you can tell
the emotion but what if we can just hear
the person's voice and that is in a
totally different language you know
nothing about so if we can just to
detect the emotion and tell the user and
beside that I can have a very better way
of communication and let's sum up my
talk and want to thank you all my
collaborators medivizor when he has
Allah and makes a sturdy Apple in the
psychology department
so our work has also been featured in
several media's and all to thank you all
and welcome to any questions that you
might have yeah already yes this emotion
expressed and interpreted the same way
of people by people of different
cultures yeah actually that's one food
for node I pull in the second scab
translator and I forgot to mention that
because emotion differs from culture to
culture especially between different
country culture so it might be you very
useful if we can convey this emotion
through the system to help you
understand what the other person's
feeling is yeah if that's true then when
you did a mechanical turk study did you
try to only pick jurors who were of the
same culture as samples uh yeah that's
what exactly what we are doing right now
we make sure that it'll are considered
very rare American citizen doesn't
necessarily being there regularly from
the same we have we can't look you never
know that simulation virus implications
of this emotion detection let's make a
step back and assume that you have a
perfect a motion detector okay and we
know the motion for sure you mentioned
that one of the petitions is helping the
person to improve the emotional fitness
what else can be basically what other
applications we can have the perfect
emotion detection how do you see them
yeah so that's a really interesting
question cost for humans if we talk face
to face it's quite a simple to just
within several seconds we can catch the
other person's emotion but the system
that we think is necessary that we can
do it in the obtrusive and objective way
so with ample just like I mentioned the
lefty
they can help children suffer from
autism to better communicate their
emotion so there are needs and also in
gaming scenario if the person can
reflect his or her emotion in the
character that in the game so that
should be really interesting but of
course that totally pizza that emotion
sensing system is perfect because if
that made the wrong decision that
sometimes can be unlike yeah yes so you
compared using the trained actors in the
emotions more more strongly versus the
college students you're saying like it
wasn't as clean what is a natural person
just he naturally do they sound more
like the trained actor or is there more
ambiguity actually we did a very first
very arty study by try to analyze some
real data collected in the lab
environment and that's just in an
elusive way of collecting some
communication between the child and
their parent and we some students in our
society car department actually label
the emotion and most of these samples
are labeled as neutral so that just
labeled as happy or upset and that does
a single you can scroll down let's go up
to go scroll Apple scroll down so I
think most of the speech our new tool or
ambiguous meaning with very subtle amor
express emotion that it can sense and
that's I think that's why we also want
to throw away those samples of mighty
bad system but it seems like if you had
a user like talking to Cortana most of
their speech be neutral so you're saying
yeah so that's the one of the challenge
that most speech are neutrals who want
to do long-term monitoring yeah yes is
there a notion of like a emotion for a
group of people so like if you were
recording at a party or in the case of
like you're adding noise to a clean
sample what the Babel itself had an
emotion quite happy Babel versus sad
bevel uh like your recorded em alone
Alcoholics Anonymous meeting or
something imagine that my alter the
interpretation or there might be
different a different set of emotions
yeah I study the Kennedys use whenever I
did my energy back out the connect and
s5 bundle and my home becomes the
entertainment center and I think the
gaming's now is most involved by many
multiple people so i think evan has done
some research that we want to evaluate
the system with the loudspeakers and
with the some background music or dublin
lies in the room and for them for some
new ID from the air conditioner so there
are all kinds of noise and relations in
the room so that it's still an open
question and for that has to be done by
source separation for multiple people
and a lot of papers are working on that
so that can seat of multiple challenges
yeah now with a focus on deciding 11
sticker yes focus on speech here is
there a sense in the community of people
that work on the month model aspects
like is there more of the signal in the
visual channel in the speech channel and
are those orthogonal I can you hope to
get all the samples that are hard hard
in the same way and then you're not
getting more bite by too involved yeah i
think it's i think that can improve the
accuracy by combining other modalities
busty we have to determine how to
combine those madad the decision from
different modalities if it's do another
gray and i think we can also sample the
speech or video for several times and
try to see what the majority motion
detected so there are several fusion
algorithms yeah but i think for the
surface sophisticated and more that
realistic system we need to combine
animal
his of course yeah yes I was just
wondering what are your thoughts on what
do you think what you have now could
extend beyond of six basic emotions oh
how do you see that working or not
working and what yeah so if we look at a
60 motion they are spaced quite a was
different to each other that's like we
can better that we are life easier to
differentiate them but freedom Paul
anger or discuss these two emotions
within the six we're currently studying
are already relatively close to each
other and some emotions are actually
emotional state are combined several
emotions some some other researchers
also give confidence score for each of
the six emotions fluency maybe maybe
that speech is seventy percent simply
percent happy or with some or with some
other combinations of some subtle
emotions like relax or hesitated yeah so
that should be interesting question cuz
in real scenario emotions are
complicated you cannot just say that it
is which emotion yeah hope that can
offer your question yeah thank you
hearest you went when the psychologist
ago was labeling the natural speech from
the parent-child interaction were they
were they listening to the direct audio
of the english or was it garbled like
when able to hear the words that the
person is saying as well we you should
use a flash card you mean the LDC that I
said with
oh no no like when you said there's a
student the psychology department about
labeling the natural interactions and
apparent in the teenager where they
listen the actual after Nathan to the
audio audio because then the problem is
that you don't know how much station the
content yeah yeah this is the speed yeah
the really good actually you know
garbage some garlic stuffed look done
where you retain the pitch for instance
name FCC's but but you know speech
content is bonds and it so much harder
task for them of course but then they
have to actually label based on what
they're hearing yeah maybe what they're
seeing my god
alright let's thank our speaker again
yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>