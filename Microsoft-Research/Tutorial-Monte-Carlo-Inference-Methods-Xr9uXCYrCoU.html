<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tutorial: Monte Carlo Inference Methods | Coder Coacher - Coaching Coders</title><meta content="Tutorial: Monte Carlo Inference Methods - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tutorial: Monte Carlo Inference Methods</b></h2><h5 class="post__date">2016-06-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Xr9uXCYrCoU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm incredibly happy to introduce the
speaker for the second introductory
tutorial today Ian Murray who is going
to talk about Monte Carlo inference
methods if you were the deep learning
tutorial this morning you probably heard
young Li kun say that he's allergic to
sampling so I'm pretty sure he's not in
the room right now
Ian is a lecturer at University of
Edinburgh and I met him a while back
when he was a PhD student at the Gatsby
unit at UCL in in London and my
understanding is that Ian was actually
meant to be a physicist I think until he
met David Mackay and took a machining
course with him which sort of changed
his life and what interesting thing
about Ian that you might not know is
that when he's not sampling he's
actually juggling and he's one of the
few people actually the only person I
know who is able to juggle with five
clubs
all right so without any further delay
I'm incredibly happy to introduce Ian
thank you very much can you hear me at
the back
so just as the last few people filtering
I want to set expectations this is an
introductory tutorial so my aim is to
get you to understand about Bayesian
inference and how we use Monte Carlo
methods to solve these problems and
methods some of you might know about
called Gibbs sampling slice sampling and
how to make these things work so if you
are already the sort of person who's
derived probabilistic models and
implemented something methods for them
you might be better slipping out and
going next door to a more advanced
tutorial I'm not going to assume
anything right now so Monte Carlo
methods are things that I think everyone
in the nips community should know about
even if you're not going to use them
yourselves or even if you just use them
to quickly sanity check something you're
doing but it doesn't make it into the
published paper so this term Monte Carlo
is a bit funny
it's refers to just using random numbers
to do something on a computer some
people give it a precise meaning but
people been using random numbers for a
very long time the term Monte Carlo was
introduced when people were developing
nuclear weapons as part of Manhattan
Project there were a lot of physicists
who were using random numbers and this
term Monte Carlo sort of evolved at that
time so Nick metropolis wrote this very
nice review about the birth of the Monte
Carlo method and one of the anecdotes I
really like from this story referred to
this guy Enrico Fermi so family was a
physicist and he was doing neutron
scattering experiments and he would come
into the lab with predictions of how
they were going to come out and he'd
made those predictions by staying up
late at night and running simulations
but unlike ask now when we have
computers kicking around all over the
place they did have some of the early
computers there but he didn't have one
he had a hand adding machine on his desk
and he was doing Monte Carlo simulations
by hand and it was an incredibly useful
thing to do and gave insight into the
physical models he was running so now we
do have incredibly powerful computers
just kicking about all over the place
and we can use Monte Carlo methods to
get insights
to a machine learning systems so what I
want to do in the next couple of hours
is to give you some examples of how we
get insight into our models by just
drawing some random numbers and having a
look how we use those random samples to
do actual computations to do math and
we'll look at some of the methods so as
I said we're going to look at important
sampling rejection sampling and these
algorithms called metropolis Hastings
Gibson's life sampling one of the most
important things with any method is how
do I actually run it so that I get
correct answers so I'll talk a bit about
that at the end but for the moment I'm
going to have a running example and I
want to I want this seminar to be
entirely understandable so my running
example is going to be linear regression
we've got a straight line with a slope
and an intercept so our parameters here
are two numbers theta and if you're
Bayesian you'd put a prior on these
things so you say there are these two
numbers I don't know what they are you
could imagine the blue line corresponds
to some slope and some intercept that's
the correct answer but we're never going
to know what that is so instead we
consider all the possible lines that
might be the true model so we could have
this distribution just for simplicity a
Gaussian these are the slopes and
intercepts I think might be plausible
and we could look at that model by for
every vector of two numbers we sample
drawing a line saying this is the
corresponding model and if you do that
you immediately notice stuff like
there's a big bunched up region here
near the origin and maybe you hadn't
thought about that when you wrote down
the model maybe you need to think a bit
about how the data will be centered or
maybe we need to reaper ammeter as a
model to try and make the position of
that bottle more flexible so it can be a
good idea just to do a forward
simulation like look at realizations of
your model in question whether they make
sense of course what we usually
interested in is having data this is
just a model so the way we'd get data
for this model would be we see these
black lines their data points yn they're
evaluated at our line plus some noise so
if we have data we can use this prior
distribution to come
with the posterior what do we believe
about these two numbers given the data
which is proportional to how well those
two numbers explain the data and how
plausible a priori they are so we can't
really see what's going on in this graph
because that region is really tight so
normally if we get lots of data
hopefully that data is informative and
our beliefs will be much tighter and in
a smaller region than our prior beliefs
so I'll zoom in on that region this is
exactly the same graph I've just zoomed
in and you can see the the data points
and this posterior distribution what
I've done is I've drawn samples to
understand it so I've drawn twelve
samples from the posterior distribution
none of these purple lines are the
correct answer I will never be able to
discover that correct answer the blue
line from a finite amount of data all I
can do is say which models are plausible
and which ones aren't so the twelve
samples there are showing twelve
examples of the sorts of model that
could have generated our data and you
see they surround the correct answer
near where we've got lots of data we end
up really certain so we know exactly
what's going on there
and as you move away from the data you
get the possible explanations really
spreading out what predictions they'd
make so Bayesian inference just
automatically does this very clever
thing that if you go to a region where
you don't have data you can
automatically be less certain and that's
just fallen out of the maths we didn't
have to put that in as a hack we've got
this nonlinear envelope over where the
lines are even though we've got an
underlying linear model so we get an
interesting predictive distribution by
looking at these samples I just pushed
this example slightly further there's
only so interesting linear regression
can be but I'm what happens if a data
aren't linear so usually in the real
world we're going to get data which it's
more complicated than our model so here
I've got some data that if we look at it
that's clearly a nonlinear trend we
could ask ourselves what would happen if
we did Bayesian in four
with this dataset but using the same
linear regression prior that I've
already showed you and this is something
I asked my students back home so I give
them a quiz question I'd say we've got
this data set and I'm going to do the
same thing I did before I'm going to
draw 12 samples from the piste area and
say what possible lines are plausible
given this data set and various things
might happen so for example is the model
going to choose to explain just the
left-hand part of the data or the middle
of the data or the right-hand side of
the data or maybe all of those things
will happen so maybe if I draw 12
samples I'll get four of each or roughly
four of each and maybe something else
will happen what is not going to do is
come up with a nonlinear prediction
because it's a linear model and that
maybe there's something else we haven't
thought of so something I do with my
class is make people raise hands there's
a lot of you here and I'm not gonna be
able to count say I'll just let you
pause and think for a moment so you come
up with your answer and what happens is
I can draw my 12 samples from the
posterior and here they are
so there are 12 purple lines but they're
all almost sitting on top of each other
so from these samples I can see that the
posterior distribution is incredibly
certain that it knows exactly what the
explanation of this data set is it's
clearly wrong like this point here is a
12 Sigma outlier like if you just look
at the data set and you look at the
model you can tell that it's massively
it'll fit but nothing in the Bayesian
computation for computing the posterior
notice is that it assumes that your
model is correct so if you have a model
which is too simple that's not able to
express express the flexibilities in
your data set then you can end up very
strongly peaked around the least bad
explanation you don't get this trendline
here because that would be a 20 sigma
outlier which is almost infinitely bad
so you're forced to find the least bad
situation and
so looking at the samples and doing some
sort of model checking can be a really
important part of a Bayesian procedure
okay so looking at samples is something
I do all the time I'm a bit of a nerd
and I like something so I just like
looking at these things but really it's
a computational audience we want to know
what math can we do after we've drawn
these samples so this equation is what a
lot of people mean by Monte Carlo some
people if they say the term Monte Carlo
they mean what's on this slide and it's
about integrals here I've got an
integral over some parameters weighted
by a distribution over those parameters
of some function and the meaning of this
integral is it's just the average value
of that function if I were to consider
the distribution PI as where the samples
come from and just saying that it's hard
for me not to say oh it's the average
value you'd get if you just drew a bunch
of samples from that distribution time
so the almost obvious Monte Carlo
estimate of this integral is to draw a
bunch of samples from the distribution
evaluate the function for each of those
samples maybe twelve and then just take
the empirical average of the samples
instead of the true average which is an
integral that means you'd have to
consider the whole space every possible
setting of the parameters so Monte Carlo
summation is in some ways an obvious
idea it's it's something that people
running surveys do all the time so for
example imagine I wanted to know the
average IQ of attendees at MIPS okay
that's a big sum it's an alien integral
and the exact answer would be I would
round up all 4,000 of you administer an
IQ test every single one and take the
average value and I could clearly get a
pretty good idea if I did the much
cheaper operation of grabbing 12 of you
at random
giving 12 of you IQ tests and taking an
average of those 12 numbers and that
would tell me sort of what percentile
roughly you were and I'd have to be a
bit careful about how I gather those
sample
I probably don't want to just take the
12 people on the front row down here
that might not be a fair sample the
distribution I'd have the Keener's so as
with the rest of this talk we're going
to have to be a bit careful about how we
gather those samples Monte Carlos got
some nice properties it's naturally
unbiased so each of these function
values that I I take by definition their
average value is this integral so every
single term in this monte carlo sum is
an unbiased estimate of the integral i
want and if i averaged unbiased
quantities then my estimator is still
unbiased but as i averaged more and more
quantities the error bar of the the
variance of my estimator falls down so
the variance falls inversely
proportional to the number of samples i
draw if I want more accurate answer I
just survey more people or gather more
samples but it falls pretty slowly if I
wasn't happy with the variance of my
estimator and I wanted an error bar that
was ten times smaller then I need to
gather a hundred times more samples
because an error bar is a standard
deviation it's a square root of the
variance so Monte Carlo is not very good
for getting really accurate answers like
six significant figures but it's really
good for getting a quick idea of things
how does this apply to statistics so
these integrals come up a lot in machine
learning algorithms if you look at the
Boltzmann machine learning rule or the
e/m algorithm for interesting models you
need to compute integrals of this form
and integrals of this form also pop up
in bayesian statistics so in our linear
regression example here is why we need
to do integrals I've got a data set but
what's the good in having data unless I
can do something with it like make a
prediction so what I want to do is make
a prediction at this location X star and
say where would the next label appear
along this dotted line so what do I
believe about the test label Y star at
this location and the correct Bayesian
answer to that question is an integral
it doesn't involve fitting one line and
then assuming we know what's going on
it says you know how to make a
prediction if you assume what's going up
and if you assume the slope and
intercept we know how to make
predictions but we don't know that slope
and intercept you should consider all
possibilities weighted by how plausible
they are so that's an integral and we
can approximate it with a sum so instead
of summing over all possible settings of
the slope and intercept we can get
plausible examples which we can sample
so here are our twelve plausible
examples again I don't know if any of
these purple lines are correct none of
them will be exactly correct but they're
all reasonable and if I assume
temporarily that one of them is correct
I know how to make predictions so if I
assume this top line is correct there's
a gray bell curve which is the
prediction of where a label would appear
noisily around that line and I get a
different prediction for each of my
lines and the correct way to make
predictions is to average those
predictions so if I average each of the
predictions from each of my plausible
models I get the green curve and that
gives me a very reasonable description
of where that test location could appear
and it's much broader than the beliefs
I'd have if I just fitted one line I
could fit a line I could regular eyes I
could be very careful about how I fit my
line but fundamentally if I assume that
I know what the line is I'll end up
being too certain some of you who are
familiar with all this stuff will know
that this predictive distribution should
be a Gaussian distribution if I have a
linear model with Gaussian noise
everything is Gaussian and there's
clearly a weird bump here and that's
because I've only drawn 12 samples so if
you draw a small number of samples you
get a really good ballpark idea of
what's going to happen but the details
are a bit shaky and the joy with Monte
Carlo methods is that you don't have to
go away and derive anything new you just
throw more computer time at it if you
want to more accurate answer so here's
exactly the same slide but with a
hundred samples and my predictive
distribution is now nearly on top
of the black line which is the predicted
distribution that I analytically derive
so with incredibly simple code that just
drill some samples and takes an average
I get like the correct mating prediction
without having to drive loads of stuff
those curves are never going to like
exactly overlap if I wanted to make this
slide have the green and black lines on
top of each other so that you couldn't
tell the difference I'd need something
like a hundred thousand samples and to
make them agree to several significant
figures they'd need more so if you want
to know something to six significant
figures you don't want a vanilla Monte
Carlo method but I don't really care
about the position of that curve to six
significant figures because it's a bit
sensitive to the precise modeling
assumptions I've made and I don't really
believe that is significant figure of
any number anyway so in this figure I
admitted that I could actually make this
prediction exactly so I drew this black
line which is the true answer to this
integral so what's the point in doing
the sampling stuff if you can do things
analytically and the answer is there
isn't one you should do the math but
this model linear regression or
generalizations of it like Gaussian
processes are about the only raising
models for which we can do the math
analytically as soon as we make the
model at all more complicated you
suddenly don't know how to do this
integral to make predictions so examples
of more interesting if even if we're
still drawing assuming that labels come
around some function with some noise if
our function is a nonlinear function of
our parameters for example a neural
network I hear that pop hill ahead these
days or if you had a more interesting
noise process or if you had hierarchical
beliefs like you thought there was some
structure to the weights and you wanted
to infer hyper parameters all of these
things would suddenly make the model
hard to deal with analytically there's
another example which I'll come back to
you later
you might want to make the model robust
so rather than assuming your data comes
from some simple form you might want to
explain away some data points as
outliers so one way to do that is to
have a binary variable Zn is can be 0 or
1 if it's 1 then we assume the label as
an outlier it's drawn from some junk
distribution that has nothing to do with
the input location X and our model could
have some probability that an indicator
is 1 so epsilon could be 0.1 saying 10%
of your data points are going to be
outliers so if we wanted to infer that
quantity epsilon and deal with all of
these extra quantities we don't know z
we don't have an analytic way of doing
all of that and we'll need some sort of
computational method that's practical so
if you build one of these models you
have to make a series of choices you
have to say and what hyper parameter
would I have what weights will I have
given that hyper parameter what noise
processes do I have and then given all
of those parameters how would I generate
all of the data given those parameters
so what you can do is imagine trying to
build a simulation of this model if I
wanted to draw some synthetic data and
ask have I accidentally made some silly
modeling assumption you'd need to write
down all of these probabilities and
you'd sample from them in turn and a lot
of people in this community would
represent that equation as a graphical
model in the tutorial next door they
represent it as a computer program where
you would have a series of steps saying
where each variable comes from in code
so for this tutorial what I'm going to
do is not have this splurge of maths on
every slide I'm going to try and keep
the notation simple so I'm going to talk
about things that work fairly generally
so they do work for models with a huge
bag of unknown things but I'm just going
to call all of those unknowns the
parameters and the latent variables
theta so theta is going to be everything
that we don't know everything that we
would have to generate before generating
our data but that we don't
in the real world and everything that we
do observe that could be an input
location and a label but it could be a
structured object like a graph that's
going to be D so what we're going to be
doing is looking at methods that will
let us look at what's a plausible
explanation of our data what do we
believe about this whole bag of unknowns
in our model given the data that we have
observed that comes from Bayes rule and
all we need to know usually is that it's
proportional to this probability of
everything so the probability of
everything is this spray of maths I
wrote down on the previous slide it's
just the product of all of the
probabilities that you would compute
while sampling these objects if you were
doing a forward simulation and having a
look at what happens so if we can sample
from this distribution we can then make
predictions and there's another piece of
maths we can do which we might do if
we're interested in say just one of
these parameters so say I had scientific
interest in how corrupted my data set
was for some QA reports I had to say how
much of my data I thought was corrupted
I might want to estimate that number and
I might care about how certain my
beliefs in that number are so then I'm
not really interested in an explanation
of all my data I just want to know what
do I believe about this one parameter
given my data and that's another
integral so this integral is what people
call marginalization and for a lot of
people in the NIP community this
equation is called approximate inference
so a lot of people when they say I work
on approximate inference or I have an
approximate inference algorithm a lot of
the time what they mean is I've got a
method that will compute what to believe
about something given data in the
context of a larger model that talks
about lots of other stuff you don't care
about so algorithms like expectation
propagation and variational methods and
message passing are really good at this
approximate inference problem coming up
with marginal beliefs and to be honest I
Eve usually forget that marginalization
even a computation when I'm doing
sampling because when you're doing Monte
Cali
there's no integral to do or no
complicated math to do what you do is
you sample everything you sample
complete explanations of where your data
came from and then you just throw away
the bits you don't want so if you sample
entire vectors theta and you throw away
all but the AI element then by
construction those things automatically
come from the right distribution so I
could sample explanations of my data set
and then I could just look at the values
of epsilon and those samples and plot a
histogram of them and that would be my
beliefs about that parameter so
something in some sense solves a harder
problem than a lot of these other
algorithms you may have heard about
coming up with a joint explanation of
everything that's behind your data might
be more than you need and that might be
part of the reason that it doesn't work
as well as some of the other approximate
inference methods on the other hand it's
a much more powerful tool it gives you
whole coherent explanations of what
could have gone on behind your data set
and that can be hard to pull out of an
algorithm that only gives you marginals
so those are the computations we want to
do we want to solve integrals
marginalization one is trivial the
prediction one is easy and we can do
that if we can draw samples from all of
these distributions and you might have
noticed that I didn't actually tell you
how I did that so I had these plots and
I had samples but I just sort of said
trust me I've done this correctly so
what we're going to do is look at the
algorithms to actually generate these
samples and I'm going to start off with
sampling synthetic things from the model
so this forward simulation where we just
want to look at what our model would do
and see whether we believe it how do we
implement those well the answer is we
don't say if you want to sample from any
distribution that has a name like a
gamma distribution or a beta
distribution or a Gaussian then that's
something that I've never well now I
have implemented it because I'm in there
but there's no reason to implement those
things there are library routines in
MATLAB and R and the GSL that you can
just call so for every sequence in your
simulation you can just call standard
library routines and generate synthetic
parameters and then data and this book
which is free online explains how some
of those library routines work and I'm
going to have to explain how a couple of
them work because we're going to need to
understand those to do more interesting
and Friends problems so in all of this
explanation I'm going to use the
following notation there's going to be a
distribution we're interested in called
pi all of our unknowns are going to be
theta and we're going to want to sample
from that and it might be a simple
distribution like a gamma distribution
or it might be a posterior distribution
a distribution over what models are
plausible and in those interesting cases
we normally can't evaluate pi for a
particular setting of the parameters
usually we can just evaluate some
unnormalized version of it that I'm
going to call PI star so the Bayesian
inference PI star would just be our
probability of everything which we can
usually evaluate and this normalization
constant
the probability of data or the
likelihood of our model for the purposes
of this afternoon we don't know how to
compute so we want sampling algorithms
that all that we're able to do is for
some settings of our model evaluate some
relative score that says how good they
are and then we want to be able to
sample given that okay so the first
standard distribution that's really easy
to sample from is an arbitrary discrete
distribution so this is something that I
do write code for because it's usually
one line if I have here a discrete
distribution over three values theta can
be a B or C then you create a stick of
length equal to the probability of that
value so the probability of getting a
see here is 0.2 so I've got a stick of
like point a you lay those sticks
side-by-side and because probabilities
add up to one your stick is of length
one you draw a random number between 0
and 1 here 0.4 and that tells you to
sample the value B so any discrete
distribution we know how to sample from
there's no big problem unless maybe
there's a huge number of values on this
stick and I'm going to tweet a link to
the slides after the talk so you can get
these references in the slides it turns
out this algorithm isn't the best way of
sampling a permit discrete distribution
it's what I'd use if I had two or three
values but if you have a large number of
values there are cleverer things you can
do and I'll just leave it at that
continuous distributions are harder so
there is a generalization of this laying
things out on a stick idea if I want to
sample from some continuous distribution
pi I can transform a uniform random
variate so if I draw a random number
between 0 &amp;amp; 1 there is some maths I can
do that will transform that value into a
sample and the maths involves doing the
continuous generalization of laying
stuff out on the stick like this you
have lots of little elements that you
lay out on the stick and you have to
solve integrals and invert them to work
out where on the stick your sample is
landed so you can do that math for some
distributions but for an interesting
model a posterior if even one quantity
you often can't do those integrals
there's a nice geometric interpretation
of how sampling works they which is if I
wanted to sample from some arbitrary
distribution it doesn't have to be
normalized what I can do is throw darts
at the area underneath that curve so
here I've drawn for samples fairly
uniformly at random from the area
underneath this curve and if you do that
and read off the value that they'll and
that gives you samples of theta and the
probability of landing in a small region
around here it's proportional to the
height of the curve so it's doing the
right thing things that sampled in
proportion to their probability if you
wanted to implement this the area to the
left of one of these samples is
uniformly distributed so we could call
that area a and that's precisely what
this maps up here is doing so this is a
nice picture but it doesn't necessarily
tell you how to implement the algorithm
how to draw these samples an algorithm
that does let you draw samples under the
curve is called projection sampling so
rejection sampling assumes that you have
a set of library routines that can
sample from some convenient
distributions so for example a Gaussian
and you use that distribution to upper
bound the distribution you're interested
in
so here I've multiplied the nice
distribution Q by a constant K so that
the green curve is entirely above the
blue curve what I can then do is sample
points uniformly underneath the green
curve because I know how to do that I
know how to sample points from the green
curve and then select random Heights
underneath it some of those samples will
land above the blue curve and I'm not
interested in that region so I just
throw them away and as soon as I get a
point underneath the blue curve I can
treat that as a sample
I haven't unfairly biased myself towards
any location underneath the blue curve
and so I'm sampling uniformly underneath
it and then I can read the location
often that's an exact sample
so rejection sampling is widely used and
may have been used by almost every
person in this room without thinking
about it so if you open a MATLAB session
and you type r and n to get a Gaussian
random number somewhat surprisingly the
fastest way to generate a sample from a
Gaussian distribution is rejection
sampling they have an incredibly
cleverly derived tight bound which is
designed to work well with binary
arithmetic that upper bounds the
Gaussian curve and it accepts samples
like 99% of the time and so is
incredibly fast to sample from if you've
got a complicated posterior distribution
it might be harder to come up with a
good upper bound and if you've got more
than one parameter and you want to do
the multivariate version of this figure
things get very difficult it can be hard
to provably upper bound the curve at all
and if you can you might want to be
clever and come up with the optimal
constant that will bound it as tightly
as possible and even then you might have
a large area where you reject a lot of
computation so rejection sampling is one
of the main methods used in library
routines to sample from standard
distributions but it's basically not
used very much in basic computation
because there's pretty importantly it's
not by itself because it's very hard to
get working so we need some other ideas
and one of the ideas it's still very
simple to implement is to not throw away
all this computation we're doing so here
I've generated three samples and done a
load of computation I evaluated my
distribution pi started work out that I
need to reject them and I just threw all
that computation away and there are
methods that think well maybe you don't
need to do all this stuff what if you
don't need an exact sample you're
solving integrals maybe we can solve
those integrals more directly so that's
the trick of important sampling if what
we're mainly interested in doing is
making a prediction or doing some
integral we need for machine learning
then we can rewrite that integral by
multiplying by the distribution that we
know how to sample from and dividing by
it
so here I've just multiplied by one I
haven't changed anything as long as I
didn't divide by zero here that would be
bad so I have to take a convenient
distribution Q which isn't zero anywhere
that my distribution is nonzero and now
as I've colored in blue this is just an
expectation under Q so I've rewritten my
integral so that it naturally looks like
an average under say a Gaussian
distribution or a Kashi distribution or
something I know how to sample from so I
can now do simple Monte Carlo I can draw
samples from that distribution and
average what's left this is called
important something because this
quantity pi over Q it's called an
importance weight it's saying some of
those samples that you're something a
lot because Q is high they're not as
important as you think they are but some
of them where the distribution has a
really high value you need to up weight
those that's an important region where
you need to pay attention to the
function values there so this is easy to
implement you just need a standard
library routine and to be able to
evaluate this thing oh except we don't
know how to evaluate that thing so if
our target distribution comes from Bayes
rule we often don't know how to evaluate
this quantity here we only know that up
to a normalization so if we can evaluate
that this thing is unbiased and it's
great but if we can't we need a
different version of the algorithm so
there's another version of important
sampling where you still draw some
parameters so this is like the slope and
intercept of our line or the weights of
a neural network from a distribution but
we compute unnormalized importance
weights so we evaluate the function we
do know how to compute the probability
of everything and compute these weights
and then we make them add up to 1 so we
create normalized importance weights
that add up to 1 so now for every
setting of the parameters I've sampled
I've
a positive number and those numbers add
up to one so these quantities are are
like a discrete probability distribution
or they are a discrete probability
distribution as a vector of numbers that
add up to one and what's important
something does is it replaces this
integral which is an average under pi
with an average under this distribution
R and if we draw a lot of samples this
distribution R which is a spiky
distribution it's a discrete
distribution will eventually have the
same effect as pi for most reasonable
functions so as we draw many many
samples we'll get a consistent estimator
and we can approximate any integral by
something from a distribution that's
convenient to sample from so the
downside with any estimator is that
maybe it doesn't converge very quickly
or it's very noisy for a small number of
samples so if this was just some general
integral you could compute its variance
and do a lot of maths but as we're
interested in doing Bayesian inference
I'll just show you pictures to see what
what goes wrong with an important
something so here's our linear
regression example and we need a
convenient distribution to sample from
if we pretend we don't know how to
sample from the pastiera directly we
need some other distribution to sample
from and here I've drawn 60 samples from
the prior because that's a Gaussian
distribution I have kicking around and
it's meant to be reasonable values of
the line and what important sampling
corresponds to here is precisely
assuming that the true line is one of
those 60 gray lines we don't know which
one but we're going to assume it's one
of them so what I'm going to do is I'm
going to compute the importance weight
for each of those 60 lines and recolor
them with intensity proportional to
their probabilities so then we're going
to see what the posterior distribution
looks like and it looks like that so 59
of the lines are so faint you probably
can't see them and one of the lines has
probability 0.9999
so given this overly simple model that
our regression surface was one of this
restricted set we end up rammed into
believing that the least worst
explanation is the correct one and
that's something we've seen before and
we're going to then make wild
extrapolations like believing we know
exactly what's going on over here even
though that's not justified so important
sampling breaks down if we don't draw
enough samples to be able to get a
reasonable renormalized distribution so
it can just draw more samples and run
the same code but with more samples if I
draw 10,000 samples from the prior and I
recolor them I get this bigger which is
much more reasonable so I have a spray
of lines the darker purple ones are the
more probable ones but the fainter ones
you can see might also happen and
they're spread around the true answer so
now I get reasonable beliefs that I
could use to make predictions and again
I see the time not very certain far away
from the data so that's the right thing
so incredibly short code I just drew
some samples from the prior which meant
sampling slopes and intercepts from a
Gaussian distribution computing how
probable the data was under each of
those possible explanations and then
coloring the lines we want more
interesting models than this one if we
have a nonlinear data set I might need a
more complicated model so here are 12
samples in gray from the prior of a more
interesting model so these are curves
with roughly the right sort of frequency
or length scale and amplitude to match
this data set it's a reasonable prior
that I've constructed by hand and in
purple I've drawn twelve samples from
the posterior so I did that exactly just
to show you that this is a sensible
model that will make precise predictions
near the data and be uncertain away from
the data so what happens if we do
important sampling here if I draw 10,000
samples from the prior so instead of
drawing twelve great curves I draw
thousand and I recolor them here is my
posterior distribution over what I
believe generated the data 9999 of the
explanations were terrible and the
10,000th who is also terrible but not as
terrible as all the others so I end up
really certain that this is precisely
what's going on even though it's
obviously wrong so I could draw more
samples that's what happens after a
hundred thousand that's what happens
after million so if I draw a million
samples that's still pretty bad that one
on the other 999,999 were worth
important sampling simply leave this
vanilla version simply doesn't scale two
interesting problems if you have more
than a few parameters or if your data is
informative so that it will rule out a
lot of models that you sample from some
reference distribution then you simply
can't complete predictive distributions
using this method so what we need to do
is go back to the 1940s and 50s and see
what the physicists did when they wanted
to sample from complicated distributions
so for the next sort of fifty minutes or
so I'm going to explain some of these
methods which are based on Markov chains
and it dates back to this paper from
1953 I just had to put up this paper
because the opening of it is so awesome
and I really wish I could write a paper
like this one day the purpose of this
paper is to describe a general method
suitable for fast computing machines of
calculating the properties of any
substance that's pretty cool especially
it's our fast computing machines are a
lot faster than their ones so we should
be able to do a lot with this method
including computing the properties of
almost any model so a lot of what we do
is still very much based on this paper
you've probably heard of the metropolis
algorithm and you will do shortly so
nicholas metropolis is the first author
of this paper you'll notice the authors
are in alphabetical order or as is
common in some
some of the other authors you might have
heard of so edward teller is known to
some as the father of the h-bomb and he
apparently was the one who had this idea
of using Markov chains to explore models
and he had an idea of using some
convenient system we could simulate on a
computer to explore models so there's a
paper a link to at the bottom by
Marshall Rosenbluth that explained what
these authors had to do with this paper
and you can read the paper for yourself
if you're interested in the history but
something I thought was interesting was
that Nick metropolis apparently provided
the computer systems for this work and
you know that shouldn't be knocked
running a computer system in the 1940s
and 50s was a serious task but
apparently he had nothing at all
specifically to do with this paper he
didn't come up with the algorithm he
didn't run any of the experiments he
didn't write the paper noting a bunch of
other people did have something to do
with the paper for example Arianna
Rosenbluth apparently wrote all of the
code and she ran all of the experiments
okay so what was the idea in this paper
and the idea was to use Markov chains
and the Markov chain is just a model
where if you have a sequence of
parameter settings then the next
parameter setting in that sequence is
drawn from some distribution and that
distribution only depends on the
previous setting of the chain so an
example that you're probably familiar
with is our transition probabilities are
a Gaussian centered around the current
state then we'll get a figure like the
one on the right we have this slow
diffusion which doesn't depend on where
we've come from but we'll slowly drift
away from where we started over time of
this Markov chain would be divergent it
would wander off to infinity but it does
so quite slowly so if we take s steps
then the distance that we travel scales
like the square root of s so that's one
thing a Markov chain can do another
thing a Markov chain can do is that
can fall into some hole and never come
back so you can get the absorbing Saints
another thing a Markov chain can do is
form some sort of cycle so there might
be a set of states that it just keeps
hopping around or there could be a set
of regions that it hops around but never
goes to other regions
and after a certain number of steps you
always know roughly where it's going to
be so none of these types of markov
chains are very good for exploring
models what we're wanting a Markov
chains that will explore parameters that
might have generated our data and if a
Markov chain doesn't do one of these
three things there isn't much else it
can do except fall into an equilibrium
distribution so these chains that form
equilibrium if we start them somewhere
they'll fall into some region but rather
than disappearing down a hole or forming
into some deterministic cycle they'll
hang around that region in a random way
and after a very large number of steps
the distribution over where they end up
will tend to some fixed equilibrium
distribution and I've called the
equilibrium distribution here PI because
what we're going to do is set pi to be
the distribution we want it to be so the
sort of math exercise I'd have had at
University of Markov chains would be
here's a definition of a Markov chain
derive its equilibrium distribution but
what we're going to do is say we know
what equi Librium distribution we want
we want to explore the plausible
parameters for this model please give me
a Markov chain that will have that
equilibrium distribution so I can
simulate it so that's what we're going
to do I need to cover a little bit of
theory not very much theory one thing is
this technical term agogic which is
really annoying because no two papers
seem to mean exactly the same thing
whenever they use it but one definition
of ergodic is that this equilibria
distribution that it forms it's the same
no matter where you start so if I'd
initialize the Markov chain somewhere
else and followed it it wouldn't have
like wandered off and reach some other
equilibrium distribution it would have
fallen into the same
place and it's pretty easy to make
Markov chains ergodic so at least for
discrete spaces if I discretize this
finally the maths is very easy and it's
very short to show that a Markov chain
is ergodic if the state space is
connected if in a finite number of steps
like K equals a hundred steps it's
possible to get from any place any
parameter setting to any other parameter
setting then it's not possible for
either of these bad things on the bottom
to happen you don't get stuck in an
island or a cycle or fall down a hole
because you can always get everywhere in
the distribution so we're going to be
interested in a Ghatak Markov chains and
then there's only really one equation
that we need to satisfy for using Markov
chains to do inference to this idea
called Markov chain Monte Carlo and
that's to understand this one equation
on the slide which is called the
invariant condition or the stationary
condition and it's a basic
self-consistency property that Markov
chains have to satisfy so we're wanting
Markov chains that reach equi Librium
over time so that the distribution over
where you end up is some distribution pi
so if you ran the chain for a long time
and you sample the state from pi if you
then took just one more step to a new
place theta prime you could do this
integral to work out the distribution
over where you end up where do I end up
if I draw a sample from my chain and
then take one more step and what we want
is that distribution to be pi we want it
so that if we've reached equlibrium we
stay in equilibrium so our task is to
construct Markov chains that have this
self-consistency condition and the turo
ghatak and then we can simulate these
markov chains and they will draw samples
from a distribution for us and
satisfying these conditions turns out to
be remarkably easy so the solution by
metropolis and others was this very
simple algorithm
what you do is you use an arbitrary
convenient distribution like a Gaussian
as the basis of the Markov chain so the
basis of the Markov chain could be one
of these Gaussian diffusion so by itself
would wander off to infinity and you
start simulating that but you just use
that as a proposal so if we start here
we might propose going to some other
place under the Gaussian diffusion but
the algorithm rejects some of the moves
and says no don't go there
stay where you are it tries proposing
somewhere else
the algorithm says no don't go there
stay where you are but sometimes the
algorithm lets you just follow the
diffusion so here we happen to a
followed a diffusion that was heading in
the right direction after a couple of
false starts we explore the distribution
we're interested in and whenever we make
proposals that would wander off outside
the support of our distribution the
algorithm rejects some of those steps
and kicks us back into the region we're
interested in so it makes these
decisions it says if you're going to
accept then the next state of the chain
is your proposed place if you reject
then the next state is just where you
want we're already so you record a
duplicate in your sequence of states and
these decisions are made randomly so
there's a probability of accepting
remove and it's some expression we know
how to compute it only needs to involve
the unnormalized quantities here that we
know how to compute so within the
algorithm we think about going to some
new model explanation maybe this slope
and intercept would explain our data
better or maybe this slope and intercept
set of binary variables in which data
points are outliers and a bunch of other
parameters would explain our data better
we look at whether there's a high
probability of the data for those
parameters compared to where we are now
and then we decide whether to go there
or not the Hastings version of this
algorithm
Hastings is a statistician he wrote a
paper in about 1970 generalizes the
method so we can have interesting
proposals here so instead of a Gaussian
proposal we might have some clever a
distribution queue
and what this ratio does is stops us
from something from the wrong
distribution in various ways one thing
we might do if we were too clever is
make you behave like an optimizer so we
could optimize and find the best
parameters we could propose going to
those and if we do that a lot will
propose going to those parameters with
high probability and this term in the
acceptance ratio stops us behaving like
an optimizer and says now you're
proposing those parameters a lot more
often than you should do fairly and so
I'm going to reject that move so the
metropolis Hastings algorithm is this
and it gives you a Markov chain that
satisfies this stationary condition and
you get to choose the proposal operator
here so you can be creative about how to
set that so what I'm going to do for
about three minutes is check your
understanding see whether you've been
awake so I have a question for you this
is an example where I'm going to make
the state space theta very simple so
theta is going to be a real number
between Norton 10 and in this problem
I'm going to assume that the region
between naught and 1 is interesting for
some reason and the region between 1 and
10 is interesting but different so there
are these distinct regions of my state
space that I'm interested in and I want
to be sure that I explore both of them
so I'm going to try and be clever which
is always bad idea and I'm going to
construct a special proposal
distribution Q and there's math but I'll
talk you through it so if you were in
the first region if you're over here
what the math does is say it's how far
through this region are you like 90% of
the way and it proposes going to the
other region so that you're the same
fraction of the way through so if you're
a 90% of the way through the first
region you propose going to be 90% of
the way through the second region if
you're in the second region this bit of
math says if you're halfway through the
second region then bounds back to be
halfway through the first region so this
is a proposal that used by itself would
just bounce you back and forth between
two particular play
is on this line so by itself it wouldn't
be a valid MCMC method but we can use it
anyway within the metropolis method and
we should be able to get a Markov chain
that leaves our stationary distribution
invariant so for example I should be
able to run a Markov chain method and
get a sample that sample might end up
here and then I could use this proposal
to propose taking one more step so that
the next place the Markov chain visits
might be in the other region so that I
might get two samples that could tell me
about different regions so if I wanted
to run the algorithm I need to complete
this acceptance probability these
proposals are deterministic so the
probability of proposing this particular
move is 1 so I've got 1 divided by 1
which cancels and I could accept with
that probability and what I've just
tells you is totally wrong so if you run
this algorithm you don't leave the
stationary distribution invariant and
I've done something mathematically evil
so what I want you to do don't go
anywhere because there's too many of you
here talk to your neighbor for like 2 3
minutes and try and understand why does
this update not leave the distribution
invariant you could consider the uniform
distribution between norton 10 you can
make pi uniform can you see why this
algorithm is wrong and then can you
understand what I've done wrong in the
math so you chat with your neighbor
introduce yourself to two or three
minutes and then I'll give you the
answer okay you're not all there yet but
I'm going to pass on so the main purpose
of this exercise is to get you to think
about what it means to leave the
distribution invariant so for example if
I sampled uniformly between naught and
ten
90% of the time I'd end up in the second
region right so if I applied this update
and I accepted with probability one then
90% of the time I would sample here and
then teleport to the first region and
ten percent of the time my initial
sample would be in this first region and
I'd move it over here so we can think
about what would the distribution look
like after one step of
transition I'd have a big peak that says
90% of my masters in this first region
and then a longtail thing in 10% of my
masters over this big region in other
words the distribution over where I end
up would be totally non-uniform even
though my target distribution is uniform
and I started out with an exact sample
from a uniform distribution so this
algorithm clearly doesn't do the thing a
Markov chain Monte Carlo algorithm is
meant to do it doesn't leave even a
uniform distribution invariant and so
then the second part of the question is
like well why not this is general
algorithm the metropolis Hastings
algorithm and I just implemented that so
I thought I was free to pick you however
I want and now you're telling me I have
to be more careful than that so the
problem here is that these numbers Q are
not 1 I've got a real line and the
proposal here is actually a delta
function saying you go precisely here
and the density of a delta function is
either 0 or infinity so really I did
infinity divided by infinity and they
weren't the same infinity so bad things
happened so if you ever are doing
anything with probabilistic modeling and
you have something deterministic like a
delta function or a transformation of
variables you often have to be a bit
careful most of the math for Markov
chains is really simple if you assume
you're in a discrete state space and
usually even if I have real numbers I
just imagined would this be ok if I
finally really finally discretize my
state space and then ran this algorithm
because that's what I'm going to do on a
computer anyway I'm going to use
discrete binary numbers to represent
things and this is an example of where
if you discretize things it doesn't
actually work out so those of you who
know a load of real analysis will hate
me for this but this is the low-tech way
of understanding what's going on I've
zoomed in to the picture and I've really
finally discretized things and if I have
a parameter on the right-hand side there
are nine different bins
discretized numbers that would all
propose going to the same bin over here
because when I divided the number by
nine I scrunch down the range and so the
proposal to go from one of those bins to
this one has probability one has now
discrete I can talk about probability
one but what about going back I've
forgotten about where I came from so you
could think of making this choice of
which Bank bin to land in arbitrarily
maybe if you sort of run out of
floating-point precision you could just
make up the final few significant
figures at random so something you could
do is just pick one of these bins at
random which would give you a
probability of 1/9 here so you get a
factor of 9 in this acceptance ratio
it's not 1 and the reason I'm telling
you this is partly to be careful with
deterministic transformations but also
because there are algorithms that are
really popular in this community where
you have to be careful about this issue
so Hamiltonian Monte Carlo or hybrid
Monte Carlo was made popular by Radford
Neel and the nineties for bayesian
neural networks and there's a lot of
activity around these algorithms that
Nets still now and these papers contain
cryptic sentences that say things like
you need to maintain phase space volume
or your transformation has to have
Jacobian one and what these papers are
saying is don't do this sort of thing
don't stretch out your space or if you
do you better correct for it very
carefully if you hate the level of
technical explanation here you can go
and read Peter Green's reversible jump
paper from 95 and he gives the correct
generalization of metropolis Hastings
for deterministic transformations and
more complicated settings so greens
algorithm tells you to include a
Jacobian term in the acceptance ratio
and that derivative is exactly the
factor of nine that I've got here by
hand-waving
so that's something that you should be
aware of but you don't normally run into
very often so where we left off before
our break was metropolis Hastings
algorithm
and this algorithm is very easy to
implement we have to decide where we're
going to initialize we have to decide
this distribution queue we need to be
able to evaluate pi and we need to
decide how long to run for and that's it
we just need to be able to evaluate this
function we don't need to know all of
the details the algorithm doesn't care
what your data are or what the meaning
of your parameters are you just need to
be able to evaluate this function so
here's the complete code for the
metropolis method you give it your
initial condition you give it a function
handle saying this is my target
distribution or actually the log of my
target distribution for numerical
reasons how long do I want to run for
and I'm going to assume a Gaussian
proposal mechanism and so I just give it
the step size the width of that Gaussian
so the algorithm has a main loop to the
iterations the proposals are a Gaussian
perturbation of your chosen step size
you evaluate how good that proposal is
using the function handle that you gave
it so that's the only place that
knowledge of your model enters the
method you decide to accept or reject
with this probability ratio and that's
it so it's very short piece of code that
lets you explore plausible explanations
of a vast array of models
it might not scale that well to enormous
datasets and enormous models that's why
it's still an active research area at
namsom there are several workshops on
this topic but for a lot of problems
it's a good starting point that with
very little mathematics get you answers
so here's the the simplest case of
running this code so here I've created a
new little function Sigma which will
create these plots and it just calls the
metropolis code from the previous slide
for given step size and it's going to
run for a thousand iterations and the
function handle I'm giving it is just a
half x squared or a half theta squared
minus 1/2 theta squared which is the log
probability of a unit Gaussian or a
standard normal distribution up to a
constant so my target distribution here
for testing
purposes is just a Gaussian distribution
and in this middle plot I've drawn the
graph for a step size of one so I
initialized at zero I instead of drawing
a sample from a Gaussian independently I
sample from the Gaussian centered on
where I currently am and then I decide
whether to take that move on not if I
didn't reject any moves I'd wander off
to infinity but by rejecting about a
third of the moves this sequence over a
thousand iterations explores my space
between plus or minus 1 or 2 and
explores this distribution so if I crash
this plot horizontally and drew a
histogram of where the Markov chain and
gone I'd get a bell curve I'd get my
target Gaussian distribution it was a
free choice here a step size and I could
set this step size to a hundred so now I
start at zero and I make ridiculous
proposals I say maybe try going to plus
78 or minus 48 and those places have
basically zero probability under this
Gaussian so for hundreds of iterations
at a time this chain just stays where it
is it's valid algorithm if I ran this
for billions of iterations it would
explore the correct distribution it just
does so very very slowly so the step
size is important if rejecting a lot is
bad I could decrease the step size to
say 0.1 and then I could accept more so
in this trace
I only rejected twice in the entire run
and that's terrible that means that the
method basically doesn't work and that
possibly surprising and rejection
sampling we don't like rejections that's
just wasted computation we want
acceptance sense but in the metropolis
method rejections are a key part of how
it works the whole point is the
rejections tell you where your target
distribution is they tell you how to
explore your distribution rather than
wandering off to infinity or staying
where you are and this trace that we're
looking at is almost exactly a Gaussian
diffusion with step size 0.1 because it
roughly experience no rejections it
hasn't actually seen anything about our
target distribution we need to run for
many more iterations before we get some
sense of the support of our distribution
and its relative probabilities so we
need the rejections and you can
theoretically derive what the right
acceptance rate should be so for
one-dimensional proposals like these the
optimal acceptance rate turns out to be
about forty four point one percent and
any value is valid so you can aim for an
acceptance rate of roughly a half maybe
a bit less and the method will work well
and it will still work to some extent if
you get it wrong so you tune the step
size on a preliminary run until you get
the acceptance rate you want if you're
doing proposals in a large number of
dimensions then the optimal acceptance
rate turns out to be 0.23 for and so
about 1/4 and again you would tune the
step size and tell your acceptance rate
was about 1/4 so going to shift slightly
a little bit more into understanding how
we run these things and what it means so
it's a reminder we're doing Markov chain
Monte Carlo
you the user tell me what model you're
interested in you write down the
probability of everything which defines
a stationary distribution that will tell
me what plausible parameters are you
initialize the parameters somehow to
some setting the vaidhi makes sense and
you run a Markov chain that then ends up
exploring the space of parameters so it
will explore the slopes and intercepts
of lines that intercept your data or the
weights of your neural network that give
reasonable predictions on your data set
and if you run this train for a long
time then the distribution over where
you end up is the target distribution if
you ran for one more step the
distribution over where you end up would
still be your target distribution and so
what that means is we can now form
estimates of integrals so the average
value of our function evaluated on one
of these some one of these states from a
Markov chain is going to be integral
if we run the Markov chain for long
enough if s is a large number and we
could use an adjacent sample and the
average value of this function will also
be this integral because this parameter
here has come from PI so here I've got
two unbiased estimates of my integral I
could run my chain for s steps or I
could run for s plus 1 steps and I get
two different Testaments because they're
both unbiased I can average them I could
add them up and divide by two and that
would give me a new unbiased estimate so
I don't need to throw one of these away
two adjacent steps on this markov chain
are going to be really close to each
other these aren't independent estimates
but they're both unbiased and they don't
need to be independent I'm still allowed
to average them and it will still be
unbiased so in general what we can do is
we can write down the simple Monte Carlo
estimator that we would write down if we
had exact samples and use it anyway even
though these samples came from a Markov
chain and they came from adjacent steps
of a Markov chain and for large time
steps we will get unbiased estimates of
this integral for small times is this
bad thing that happened here we had a
transient phase where we're not really
something from our distribution so that
contaminates this some a bit and we
might not have an unbiased estimate of
this integral but in the limit of taking
a large number of steps it doesn't
really matter whether we include this
transient phase or not so this
expression is still a consistent way of
estimating the integral whether we
discard this burnin period which some
people do or not I would tend to throw
away like five percent of my chain just
because but it doesn't really matter
whether you do or not alright so then
how long do we have to run this faithful
and figures like this one a really
misleading because they're in two
dimensions and we're not really
interested in doing MCMC on
two-dimensional distributions there are
better numerical methods than Markov
chain Monte Carlo for two parameters
we're interested in high dimensions that
they're hard to draw on a slide
so here's my attempt high dimensional
spaces are really spiky and weird so
there are these corners of parameter
space which kind of far away from other
corners of parameter space but they
might also be good explanations of our
data so this is a cartoon and it's
really a two-dimensional distribution
but it's got isolated spikes which is
more what you like in high dimensions
and what I've done here is I'm exploring
the uniform distribution over the gray
region and I initialized here at the
bottom so I ran the metropolis method
for 2,000 steps it took some random walk
around the support of this distribution
it rejected whenever it stepped into the
white void and I ended up here on this
run I'd run up I'd end up somewhere
different if I ran it again so I'm going
to claim that that point there is very
very nearly a sample from my target
distribution from the uniform
distribution over that grey star if I
hadn't run for 2,000 steps if I'd only
run for a hundred then I'd be less happy
so it took me a long time to escape from
this arm down here and what you see in
the top right is the distribution over
where you end up if you only run the
Markov chain for a hundred steps so it's
really probable you're stuck in the
bottom arm of the distribution and the
some probability you escape elsewhere
but not enough on the other hand if you
run the Markov chain for 2,000 steps the
bottom right of the slide shows the
distribution over where you end up after
2,000 steps and to a few significant
figures it's correct it is the
distribution we aimed for so here two
thousand steps is long enough I've done
the maths and I've shown it so a brute
force this numerical computation this
slide took the longer start for the
whole presentation to create and it's so
I hope you appreciate it it's it's
possibly a bit surprising so if we look
at this Markov chain it didn't wander
into this arm at all or this one or this
one or this one it ignored most of the
state space how can I possibly claim
that that there is
fare sample when it has no idea what's
going on up here because it didn't even
go over there so I think there's it this
is a fair confusion it takes a while to
get in your head how these algorithms
work for this particular distribution
2,000 steps is long enough the
distribution over where you end up is
this
we didn't go up this arm this time but
if I ran it again I could do if I wanted
12 fare samples from my model so that I
could make a reasonable prediction I
could maybe run this thing for 24,000
steps and I could get 12 independent
samples or nearly independent samples
and I could use all 24,000 steps in my
average and I'd have a lower variance
estimator so I could get really good
predictions here even though on a short
run the Markov chain doesn't explore
some of the modes of the distribution
and on interesting distributions you
could easily have thousands or an
exponentially huge number of modes and
your Markov chain is never going to
visit the vast majority of them and it's
a mistake to think it has to so if you
had to enumerate your whole state space
why are you doing MCMC why don't you
just do your sum by hand like so the
whole point of these methods is that you
can ignore most of your state space you
gather samples that are representative
and you can use those to make
predictions all right
so now we've got this technology we've
got the metropolis Hastings algorithm
and we've got some understanding of how
to use it and we doesn't run a long
Markov chain and use wherever it goes as
plausible settings from our model even
though they're not independent even
though it willing to explore the whole
model and there's still a lot of choices
about what we would do so if you were to
create an MCMC scheme you need to make
choices it's this Q distribution going
to be a local diffusion like I've shown
in almost slide so far or are you going
to be clever and do something like
approximate your model and try and
propose from an approximation to your
model are you going to update just
all of your parameters like Patel all of
them or are you going to pick a
parameter at random and we've just one
of them at a time it might be easier to
control the step sizes for that or you
might be able to share a lot of
computation if you only move one
parameter so for all these different
choices you get a different transition
operator and any of these transition
operators are valid Markov chain methods
that you could use in an algorithm and
something I really like about MCMC
Markov chain Monte Carlo is that it
composes very nicely you don't have to
come up with the best method what you
can do is have several transition
operators and use all of them so if you
had three transition operators a B and C
you could use them each in turn so if
you started out with a sample theta1
which came from your stationary
distribution you could apply a
transition and that would give you
another sample which also marginally
comes from pi because this is a valid
transition operator so you can feed that
into the next transition operator and by
induction every time you take a step
with any of these transition operators
you will have a valid sample from your
distribution so the concatenation of all
these operators a then B then C will can
take a sample from your distribution and
spit out another sample from your
distribution and so it leaves the
distribution invariant and together
they're a valid MCMC method if there are
Ghatak if all of these operators
together can get you from anywhere in
your state space to anywhere else then
your proof is done you have a valid MCMC
method so these things individually
don't actually have to be a Ghatak
transition operator a might only update
variable number one and trusted
transition operator B might own the
update variable number two so transition
operator B is not a ghatak it only moves
variable two and never explodes the
others but that's okay as long as
together it's a valid method so the
concatenation of different methods can
be better than any of the individual
transitions by themselves as long as
they contribute something towards
exploring the state space the most
famous example of having a series of
operators like this is Gibbs sampling
Gibbs sampling says take one of the
variables from your model and resample
it from its conditional distribution so
if you had a bunch of binary variables
like in an image model or binary
variables like our indicator variables
saying whether we had outliers or not
you take one of those variables out to
get what your current setting of it was
you look at all the other variables and
then you decide do I want to make this
variable black or white and then you
select another variable either at random
or in turn it doesn't matter and you
make the same decision again you
resample it so because we can always
sample from discrete distributions Gibbs
sampling is very easy to implement for
discrete variables here I just need to
sample from a one dimensional binary
random variable and I know how to do
that continuous variables a bit more
tricky here I only move horizontally or
vertically I update one of my two
continuous parameters at a time or in
general one of my several at a time and
given a current location you need to
work out what the conditional
distribution of the variable you're
updating is so if that conditional
distribution happens to be something
with a main a gamma distribution a
Gaussian you can use library routines to
sample from it or you might have to
prove something about it like it's log
concave and then you can use clever
adaptive projection samplers that know
how to sample from those conditionals so
Gibbs sampling can involve doing some
Maps and being clever although there is
software that does that encode some of
that cleverness for you so one of the
most successful pieces of statistical
software is bugs or wind bugs and its
successors like Jags and now in this
community we have Stan and other
software and what bugs and Jags does is
derives these conditionals for you for
your model and works out how to do
the updates and then you don't have to
set step sizes and it explodes your
whole model if there's a variable that
doesn't know how to update it can fall
back to doing a metropolis method and
update that variable that way so when
you implement Gibbs sampling it might
not work very well this red distribution
is correlated if we know c21 is high
then theta2 is probably high as well and
that means that these horizontal and
vertical moves can't be very long
compared to the distance that the Markov
chain has to traverse to explore the
whole distribution and that will get
worse if the variables are more
correlated so there are a bunch of
things we could try and do about that
one is we could try and transform the
space to make the variables more
independent I have to be slightly
careful about how we do that there's a
method called adaptive Direction
sampling where there are Delta functions
and you have to be careful about what
they are and include a Jacobian turn
another clever thing we can do is
blocking which is updating more than one
variable at once so to give you an
example of that I'm going to have to
talk about a particular model and I've
made this as simple as possible so here
we've got two unknowns regression
weights and binary variables saying are
we an outlier or not you know the
Gaussian prior and our regression
weights we think 10% of our data points
are going to be outliers and labels are
either noisy versions of a straight line
or they're junk if the binary indicator
is one so this is an example of a model
where lots of it is quite simple
tractable stuffed we know how to deal
with we're really good at doing
computations with Gaussian distributions
and linear combinations of Gaussian
distributions so for this model we can
derive exactly a Gaussian distribution
on the weights if we temporarily pretend
we know which data points are outliers
so if we pretend that we know which data
points are outliers we can ignore them
and then we just have a linear
regression problem and then we know
exactly what this posterior distribution
is so it turns out you can have one line
a vector
code and MATLAB or numpy or whatever
that will sample all of these weights at
once they might be strongly correlated
but we can just replace them all and
then we can have another line of
possibly nice vectorized codes that will
update all of the indicator variables
temporarily pretending and we know what
the weights are so this is nice block
Gibbs sampling scheme which is a for
loop containing two lines of code that
just updates W and then update said
alternately very easy to implement and
might work quite well so that's a clever
trick we can do
the unfortunate thing here is that
there's lots of clever things we could
do and we don't know which ones to do so
there's another clever thing we can do
when we know the whole distribution over
the weights that's a sign that we
probably know how to analytically
integrate them out of the model as well
if we're able to form and normalize this
distribution we know how to do integrals
involving W and indeed here we can
analytically work out up to a constant
the posterior distribution over the
indicator variables given the data by
integrating out the weights so you can
evaluate this probability up to a
constant for any setting of what's an
outlier and what's not because if we say
these are the outliers you ignore them
and then you have a tractable linear
regression problem so you can run MCMC
on a collapsed problem where you don't
have to talk about the weights at all
you just update what you believe is an
outlier and you could do that by Gibbs
sampling the Zed's would no longer be
independent so we won't bail to
vectorize this we'd have to sequentially
go through the Zed's but we don't have
any real numbers and we just update
these things and as easy we can also
collapse out the Zed's
and update only the weights maybe using
a fancy method like Hamiltonian Monte
Carlo and we then don't have to deal
with the discrete variables at all and
this is kind of annoying because it's
not at all obvious which of these three
ideas is the best one and it will depend
on stuff so it depends on are you going
to implement this on a GPU and you
really care about vectorizing
and it might depend on the structure of
the model and the data you have so maybe
it's really obvious what all those Ed's
are for your data and then exploring
those exits just go to the setting which
is obviously correct and then you're
done you have an analytic answer so this
method might work really well but it
depends and in most of the probabilistic
models that appear in nips papers the
models are a lot more complicated than
the one on this slide and there's a
bunch of choices about what you can
collapse out of the model
so you'll see papers on things like
Layton to reach the allocation or topic
modeling where there are different parts
of the model that you can collapse out
and there'll be different trade-offs on
which one you should do and this isn't
just an issue with Monte Carlo methods
if you're doing variational inference
for these models you can also collapse
out different parts of the model so one
of the great challenges for people doing
probabilistic programming is that
they're claiming you can define your
model and a clever compiler will work
out how to do inference for you which
means including deciding how what
choices to make here and these are
choices I don't know how to make half
the time so you know it's a really
ambitious research research agenda that
these people are attacking and there's a
lot there's a lot of theoretical
problems that this light has this idea
that sometimes we can do analytic math
so we can integrate out analytically
some of the unknowns in our model and
then sometimes that makes things better
but sometimes it's good not to integrate
things out because we might get
convenient updates that work well and we
can take that idea to an extreme and
actually introduce extra variables just
for our own computational convenience so
there's a whole family of Markov chain
Monte Carlo methods called auxilary
variable methods that do the reverse of
being clever and integrating things out
and put in extra variables that we
didn't need to so we form a new target
distribution over the unknowns that were
interested in that actually appear in
our we're sorry appear in our model and
some extra variables I'm calling H which
might have nothing to do with our
problem at all we just put them in for
the fun of it and we need this target
distribution to be consistent with the
distribution we're interested in and
then we can sample a Markov chain that
explores both theta unknowns and the
auxiliary variables H we then
marginalize by throwing all the H's away
and then we're left with samples from
our unknowns and most work in MCMC can
be interpreted as coming up with a
clever representation of your problem
which might involve some exhilarate
bulls and then running the table
tastings on it there are a few methods
that aren't metropolis Hastings but most
of them are basically metropolis
Hastings so you know most of what you
need to know is then just down to the
details of how you're going to be clever
so the first exhilerated was called the
Swenson Wong algorithm and I've already
mentioned Hamiltonian Monte Carlo is
very popular in this community at the
moment there's a large amount of work on
so-called pseudo marginal methods these
are methods where you can't actually
evaluate pi-star
even you can't evaluate your target
distribution up to a constant but you
can estimate it and pseudo marginal
methods are just clever auxilary
variable methods where the auxilary
variables you introduced allow you to
use estimates of your distribution
rather than exact computations
so the axillary variable method I'm
going to quickly tell you about is
called slice sampling because it's sort
of a beautiful simple method and it's
one I think is a really good entry into
running MCMC if this is something you
want to try out yourself so this is a
method by Radford Neal and the idea goes
back to rejection sampling so I'm going
to show you a one-dimensional figure
because I'm going to assume we're just
going to update one parameter at a time
and in rejection sampling we need to
sample uniformly underneath the curve
right and that can be hard to do but
we're doing Markov chain Monte Carlo we
don't actually need exact samples
we just need to take a Markov chain a
random walk so what slice sampling does
it's so instead of sampling exactly
under this curve I'm going to take a
random walk underneath this curve and
see where I end up so to talk about
being somewhere underneath this curve I
introduced a new variable H which is the
height where am i between zero and the
height of the curve which is PI star
so given this distribution over two
variables theta and H my target
distribution is just a uniform in this
area under the curve and I can do Gibbs
sampling I can update one variable at a
time so I could update the height given
where I currently am so I'm going to
move vertically and I just sample from
your uniform distribution on that line
so that's easy and then I can turn my
attention to the parameters and I need
to update myself along this horizontal
great set of segments that segments that
set of segments is called the slice say
we slice sampling boils down to how do I
move around the slice for unit modal
distributions this is really easy to
implement you can do rejection sampling
but the rejection sampler is really
simple
I want to sample from this segment
underneath the curve I just write down a
bigger segment than I need so I just
push out a bracket until it's clearly
outside the distribution which is easy
to check then I try sampling on that
interval and in this case the sample is
not on the slice so I'll need to try
again but I can be clever
I now know this whole region over here
isn't acceptable so I can shrink in my
bracket so the algorithm very rapidly
exponentially quickly makes the red
interval tight and at some point I will
definitely get a sample somewhere on the
slice so these rejections are just part
of the internal working of how I'm going
to move horizontally they're not part of
my Markov chain so this is a method that
we'll keep looking until it finds an
acceptable point and then it just goes
there
unlike metropolis Hastings I don't need
to record rejections they don't have
horizon--
lines on these trace plots and what's
good about this procedure is that I
don't need to know in advance how broad
this distribution is it automatically
shrinks in to find a sensible step size
so I don't need to tune the things as
much as for metropolis Hastings if the
distributions not you know modal you
have to be a bit more careful so Radford
Neil's paper had a series of updates
that tells you how to move around on the
slice and we give up on gibbs sampling
so given that we're currently here it
might be that we never transition over
to this part of the slice we just move
somewhere within the local region and
that's okay because all we need to do is
leave this conditional distribution
invariant we don't actually need to
sample from it exactly we're running a
Markov chain and the Markov chain will
be a Ghatak if it's possible to go down
along and up and eventually explore the
whole area under the curve so really
you'd want to look at the paper to get
the details right because it's very easy
to get the details slightly wrong on
this algorithm but it's very easy to
implement you slap down some initial
interval you extend it until it sticks
out at the curve and then you just
sample from it as I showed you on the
previous slide so these algorithms are
really easy to use because unlike Gibbs
sampling you don't need to derive a load
of mathematics you just need to be able
to evaluate your target distribution
which is the probability of everything
it doesn't have these rejections so it
doesn't have these long horizontal
regions and a trace plot and it adapts
the step sizes so there's a first method
to run it's often the one to try if
you're wanting to parallelize things or
really get the fastest performance you
might want to tune and clever metropolis
Hastings method but if you've got a
simple problem and you want to try out
MCMC methods for the first time this is
what to do bradford Neil's paper which
I've linked you here and the slides will
be available is a great read it has a
lot more ideas and I've explained here
with co-authors I have a paper
elliptical slice sampling which is a
multivariate version of Sly something
that's good for Gaussian processes and
we have a paper on archive called pseudo
marginal sly something this tells you
how to use this black box simple updates
where you don't even know how to
evaluate pi you can just estimate it
randomly so the these algorithms are
really broadly applicable ok so what I'm
going to do for the sort of final 10 15
minutes I'll say before questions is
tell you a bit about the sort of
practical issues of like actually
running these things and getting them to
work and I think I know what a lot of
you will be thinking because I've given
a few of these tutorials and I the main
questions I get are things like and how
long do I need to run this thing for and
how do I diagnose if it's correct and if
you look at papers that use MCMC methods
you'll see that they're full of these
sorts of Diagnostics so here are some
figures I've stolen from an eps paper
and one of the things you might do is
plot a trace plot this was run for 5000
iterations and some quantity some
unknown theta is plotted over time
there's a burnin period so maybe when
you're estimating things you want to
throw away the first thousand steps
hopefully this thing has reached some
sort of equilibrium but eyeballing that
I'd probably run it for at least another
ten times longer and see what happened
it's now 15 years later so that would be
easy to do another thing that a lot of
papers view is plot Auto correlations or
Auto covariances so how correlated is my
state theta at time 0 compared to 400
times steps later and if any of your
variables are sort of quite predictable
or correlated if you know where they
where 400 steps ago then you know that
400 steps isn't long enough to it like
get an effectively independent sample so
if you have high ortega variances it's
bad news if the auto covariance is all
appeared small that doesn't mean you're
ok but you know that you're in trouble
if you get larger auto covariances
so the standard software like our code
oh that will create these plots and run
a whole bunch of other Diagnostics for
you and I've linked here a really nice
review on some of the Pratt
all issues of running multicolored but
actually none of this stuff is I think
the thing you should worry about first
the thing that I worry about first is
that when I've implemented a method is
probably wrong I probably just screwed
up my code right and there's this really
nice paper by John Gorky where the title
of the paper is getting it right we all
want to get things right and the the
thesis of this paper is that you're MCMC
code becomes big and complicated
you've probably messed up somewhere and
then there's no point running a load of
Diagnostics if you're busily exploring
the wrong distribution so what we'd like
is some way of unit testing our code and
these are randomized algorithms so they
can be a bit hard to test so he had a
neat idea it's related to other checks
and other authors have done which was to
implement code that can draw samples of
data from your model so we spent ages
writing MCMC code that will explore
plausible parameters it would be really
easy to write code that would generate
synthetic data I recommend you do that
anyway just to psych eyeball what your
model thinks so if you have these two
pieces of code you can check that
they're consistent with each other and
as this second piece of code is fairly
easy to write if there's a mistake it's
probably in your MCMC method so here's
how you do it here's how you get it
right you generate some synthetic data
so make up some parameters generate some
data and you know what parameters
generated those data you can use your
Markov chain code to move to some other
parameters that could just as easily
have generated that data if you didn't
know otherwise so you move the
parameters you then throw your data set
away and generate a new synthetic data
set from scratch a new data set that
could also come from those parameters so
you move the data set you then clamp the
data set and move the parameters and you
go back and forth so what we're doing is
we're running a Markov chain on the
joint space of unknowns and data
exploring every
possible setting that we think a model
thinks is reasonable before we see any
real data and if that code is consistent
we'll do that correctly and the
parameters will explore their prior
distribution so if you had some
parameter like a regression coefficient
which had a Gaussian prior you could
plot a histogram of that parameter and
it should look like a Gaussian and that
doesn't happen when you've messed up
your code Wow if you introduced
artificially a small mistake into MCMC
code what normally happens is some whole
region of the state space becomes
disfavored and you get a trunk cut out
of your histogram so you plot this
histogram it should have been a bell
curve
it's obviously not you know something's
wrong and this isn't like a hypothetical
story John Gorky admits in this paper
that he went back and checked his
previous papers and found mistakes and
has published results embarrassing
fortunately I read this paper after that
and I submitted a paper to nips 2010
where I know that in the day of the
submission deadline I made some change
to my code I ran this check and when few
realized I'd made a mistake and fix it
up so you know this check meant that
either my paper wasn't rejected or I
didn't have to embarrassingly correct it
later there's plenty of other papers
where I've had to do embarrassing
Corrections later but this wasn't one of
them because there was this useful tool
you're a bunch of other consistency
checks that are useful so I started out
by saying there's not much point being
very carefully Bayesian if our model is
overly simple or wrong if you read an
Gellman at all's Bayesian data analysis
book there's a section on posterior
model checking that says you can use the
samples from your posterior not just to
check your code is correct but also to
sanity check whether your model makes
any sense and that's sort of a really
valuable tool kit that comes easily with
MCMC methods you will be creative and
for your problem there's all sorts of
things you could do like if I draw
synthetic data
what sort of predictions do I make
compared to if I knew the true
underlying the data if I can't make good
predictions in a synthetic world then I
know my system isn't going to work well
in the real world and I should go back
to the drawing board so that's something
in the way of practical techniques what
should you actually do which method
should you use so if you're running
forward simulations you're generating
from particular distributions to
simulate something then you need exact
samples the Markov chain doesn't really
cut it so you're going to use something
like rejection something to draw from
this distribution you want to but if you
have a complicated distribution like
from a Bayesian posterior rejection
sampling won't work so you're going to
use important sampling if you can get
away with it which is almost never but
on small nor easy problems and for more
interesting problems I suggest that you
start with MCMC methods and if your
values a real value if your variables a
real valued I'd suggest starting with
slice sampling if you're careful and you
know what you're doing you might get
metropolis Hastings to work better and
if you're clever about deriving your
updates you could do give something or
one of these other methods so I'm going
to point you to some reading these two
reviews are what I learned Markov chain
Monte Carlo methods from so David
McKay's textbook is excellent radford
Neil wrote this literature review you as
his transfer document to becoming a PhD
student is about 130 pages and it's
amazing I learned something new every
time I read that document there's a new
review of Monte Carlo methods in this
very expensive book by CRC but there are
several free chapters online that cover
things like the the math behind the
green method so if you're doing clever
deterministic updates getting that right
describes Hamiltonian Monte Carlo and
also a bunch of stuff to do with
Diagnostics and checking if you want to
go beyond this introductory seminar at
NEMS there are a lot of relevant
shops and I'm probably missing some out
but the ones here all have a very large
Monte Carlo method content so these
methods are interest to real problems at
nips the first two are sort of how do
you run these things when your models
are a lot bigger when getting a Markov
chain to mix is going to be hard or
computing with a large data set is hard
blackbox learning is about well how do I
make all these free choices like how
would I represent my model when I don't
want to have an expert at hand and
Bayesian on parametric's is one of the
biggest uses of MCMC methods there are
also a couple of workshops that I put at
the bottom where they think Monte Carlo
methods are a terrible idea and they've
got much better solutions so go to those
workshops too and they have ways of
solving inference problems that don't
use these nor easy random numbers and
get more accurate answers but even if
you were going to use the methods from
these workshops what would you check
them with I check them with MCMC and the
still a lot of problems Bayesian
statistics is dominated by MCMC methods
not yet the methods from these
communities so that's the challenge to
them and I hope they succeed to replace
MCMC methods so what I'm going to do
just to finish with is show you an
example of MC MC running before I have
some questions so three years ago there
was a cattle competition to do with
predicting locations of dark matter in
the sky and unlike some cattle
competitions this didn't have an
enormous data set and it didn't have a
really complicated model but it required
doing the statistics kind of correctly
so the top three entries to this
competition were all basically
statisticians who knew how to do
statistics correctly and the top two
winning entries both used MCMC
approaches so the one of those
approaches used light flow something and
here's an example of an easy data point
from that ciao
so this is a synthetic image showing you
the locations of galaxies and a patch of
sky and lines showing the orientation of
those galaxies so most galaxies appear
to be elliptical disks and they point in
some direction and naturally they would
have a uniform distribution over how
they're oriented and you might be able
to spot and it's not very hard to see in
this figure there's a suspicious region
where the orientations don't look random
and that's caused by a lensing effect
there's dark matter between us and the
galaxies which are bending the light and
causing this distortion so what you can
do is solve the inference problem where
is that dark matter and what it's water
its properties so we have theta are
unknowns are the XY positions of this
dark matter halo its mass its size and
its shape so you might have five or six
parameters describing the dark matter
and then you can run a Markov chain in
that six dimensional space so what I'm
showing here is just the walk the Markov
chain did on two of those parameters
showing the position so I initialized
Dark Matter halo at a random location
here it used slice sampling so it
adaptively saw that it could take a
massive step and then some bigger steps
and then it took a load of really tiny
steps I didn't have to tune these step
sizes because it did that for itself and
saw that it knew the dark matter
location was around this true location
with high precision so this was really
easy to implement like literally a few
minutes once you know what the model
should be but this is an example of a
problem where you probably shouldn't do
MCMC so here if I ran an optimizer I
could just say that that is the optimal
location I could measure a curvature or
something else to come up with a
gaussian approximation and I could put
Aero bars on that answer so this is an
easy inference problem it's not that
high dimensional and MCMC is just not
worth the hassle a lot of the data sets
were a bit more complicated place so
here's a more representative
patch of sky from this challenge and the
we were told there are three Dark Matter
halos and this patch of sky somewhere
and you can try and spot where they are
but it's not particularly obvious so
what we can do is we can initialize a
Markov chain where we explore this
larger state space of maybe fifteen
twenty numbers saying what are the
parameters of these three Dark Matter
halos the black crosses show the right
answer and then a Markov chain can
explore the positions of three Dark
Matter halos and these three Dark Matter
halos move around a lot because it's
incredibly uncertain where they are
after a long time you can look at all
the places they visited the colors here
aren't particularly meaningful because
the halos can swap over so going to
remove the color and the intensity here
is showing the mass of the Dark Matter
halo so with exploring the position and
the mass and a whole bunch of other
stuff I'm not visualizing and here you
can tell that yeah we're really sure
there's a massive Dark Matter
concentration around here and around
here which is correct
we're not sure there's one here because
there if there is one there it's not
very massive and there all sorts of
other places where there could be Dark
Matter halos so the posterior
distribution here is really complicated
and I'm just showing some of it here
that I can visualize and if you fit this
and then try and make conclusions about
physical theories how does Dark Matter
interact with gas you just come up with
spurious results which are due to errors
in your fitting what you need to do is
propagate all of this uncertainty
through and use it to compute what does
this patch of x sky
tell me about physics so that I can
combine it correctly with the inferences
I'm running on several million other
patches of sky which the MCMC is running
on separate machines so this was a
challenge where the MCMC was very easy
to implement and really the only way I
know of getting sensible predictions
about the physics so I will stop there
and take questions
for much
please come up to the microphones to ask
questions there's Microsoft as there's
microphones on the aisles they used to
be
is that working yeah first of all great
talk thank you and thanks for the red
mitten that really made it for me oh
thank you
so the the high dimensional space you
talked about you're doing this walk and
you say the corners are all heavily
separated you have that visualization
you say you jump to another corner if
you like and you can say that that's
enough steps to get there and I guess my
question is how how would you assess
whether that was a lucky early number or
an abnormally long number how many of
these long iterations do you have to do
to be sure that that's kind of the right
number right
okay so there's there's a question here
that I have brute-force this problem so
I did a massive numerical computation
here which means that I know that 2,000
is long enough and that's the short
answer to the question is I brute-force
this and then I know but for another
problem the real question is well how
would I know for a real model rate so if
this arm had some massive blob attached
to it and that's where the distribution
really is then 2,000 steps wouldn't be
long enough because I need enough steps
to make sure that I would explore down
that arm find where the real action is
and spend time there so if there any
real model well not any real model but
for most real models it's really hard to
know that that isn't going to happen
maybe if you run it for longer you'll
discover that you just had pseudo
convergence and you're actually going to
spend the rest of time somewhere else
and I don't think it's unfair to say
that the majority of nips papers over
the years have used MCMC are probably
not running for long enough they are
almost certainly trapped in some small
part of the distribution and if you
could run them until the age of the
universe you'd discover that they did
something slightly different so no no
easy fix basically you just I don't
think there is an easy fix so there was
a lot of excitement in the 90s about a
method called perfect simulation so
there's some very high dimensional
distributions
eating models with a million spins
amazingly it's possible to prove that
you how many steps you need to run for
and people like Jeff Rosenthal and
Toronto you have some great theory for
some realistic statistical models saying
how long you need to run for but most of
the Bayesian nonparametric models used
here no I don't think we're there yet I
guess have a quick question but you
might imagine that you might not have to
mix thoroughly in different applications
so in some applications you might just
do sort of partial mixing but if you're
computing some sort of expectation that
might be reliably estimated well before
the mixing is complete you have any
comments on that yep so great I entirely
agree sometimes you might not care too
much about mixing sometimes MCMC methods
in this community are used within the
inner loop of an optimizer and the sort
of stochastic approximation theory that
says that maybe your optimization will
run out correctly even if you don't take
Calibri at each step
sometimes you're doing this thing
basically as a really good heuristic for
approaching the right thing but you care
about engineering performance so there's
a lot of recent nips papers on
large-scale MCMC that haven't tried to
look for convergence but have compared
themselves to non-bayesian things by
looking at test error so in terms of
engineering performance the question
might be if I'm running for a certain
amount of compute time what's the method
that will give me the lowest test error
and if that method is some MCMC that
might not quite be reaching equilibrium
then great I'll do that if on the other
hand I'm doing MCMC for scientific data
analysis and I'm trying to say what we
believe about physics after a three
hundred million dollar experiment then I
might care a bit more about whether this
is the correct computation or whether
it's some hack that might kind of sort
of roughly be right so I think context
is everything here and I think there's
you know the space for people playing
fast and loose and trying massive
systems and
also space for people really trying to
get the right answer when you've got
seven parameters and I think both areas
are important
look if there's no more questions I'll
ask I'll actually ask one man I remember
reading Radford Neal's thesis a while
back and and of course being very in
love with the idea of you know using
MCMC to do inference with neural
networks and I wanted to ask you like I
know that Radford and his students have
won some competitions a couple of of
years yeah but but I don't see that much
activity with like you know Bayesian
treatment of neural networks and any
particular back MCMC do you think that
this is just something that will sort of
come back or do you have any
perspectives and of course any
inevitable connection to deep learning
with it sort of pretty interesting okay
the question is why when Radford Neal
wrote this amazing code fbm that he
released in about 1994 and you can still
download it compiles it works very well
and he ran he won competitions and nips
workshops in 2004 by just dusting out
that code and running it why is it that
there hasn't been so much activity since
and I think part of that is that that
code doesn't really scale to enormous
datasets so they're batch methods where
for every MCMC update you need to chew
through your entire dataset before you
make a single move and to have these
really valid Markov chain methods the
statisticians have traditionally tried
to construct you kind of have to do that
and so for the next community that
wasn't so interesting it's not true to
say that there isn't lots of interest in
bayesian neural nets though in the last
two or three years there's been a lot of
work so starting with welling antes work
and a series of papers following from
there they've been mini batch versions
of hamiltonian monte carlo and to be
honest the theory is a little shaky but
empirically these things work very well
and these methods are getting on firmer
ground over time so that's an
interesting area there's also been an
explosion in interest in stochastic
variational methods so some people are
using neural nets using variational
approximations rather than multicolored
approximation
so that work is interesting and then the
Cambridge engineering group is even
gayer money in history and gal and
others have interpretations of drop out
as doing variational approximation so I
think that's still very much interest in
regularizing large systems sometimes
using random sampling it's just that it
might be we need to move away from some
of these traditional statistics
algorithms to slightly updated versions
all right well thank you again for a
really really great introductory
each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>