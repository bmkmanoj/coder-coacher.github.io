<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Architecture Support to make Concurrent and Parallel Software Less Buggy and More Reliable | Coder Coacher - Coaching Coders</title><meta content="Using Architecture Support to make Concurrent and Parallel Software Less Buggy and More Reliable - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using Architecture Support to make Concurrent and Parallel Software Less Buggy and More Reliable</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/E3UETa19__w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
it's my pleasure to introduce Brendan
Lucia from the University of Washington
he's a grad student working with
professor Lewis says a who and he's
finishing up this year he's done work I
think that a number of people in this
room before and he's going to give a
talk on his PhD research so right cool
all right thanks Jim for the
introduction today I'm going to talk
about my research on using architecture
and system support to make concurrent
and parallel software more correct and
more reliable so as you mentioned this
is the work I've done during my PhD at
the University of Washington with my
advisor Lewis says a so I'm going to
begin my talk today by talking about
what are the key challenges that I've
looked at in my research and the key
challenges that I've focused on are
those posed by concurrency and
parallelism and in particular what the
impact of concurrency and parallelism is
on the problems of correctness and
reliability then I'm going to dive into
the key research themes that show up in
my approach to solving these research
challenges and the focus of my work is
on using architecture and systems aboard
to solve these problems then after that
I'll dive into a couple of my research
contributions and some more detail to
give you an idea for the kind of
projects that I like to work on and at
the end of my talk I'm going to talk
about some of the things that I'm
interested in in the future going
forward so first I'll talk about the key
research challenges that I've looked at
so i'm going to show you now that
concurrency and parallelism are
essential and they're unavoidable and
there are two main reasons why
concurrency and parallelism are
essential an unavoidable that is that
there's pressure from the bottom because
technology is changing and there's
pressure from the top because the
applications that people care about
running on computers are changing so
i'll talk about technology first the
best example of a change in technology
is the shift to multi-core devices
everyone has a multi-core in their phone
and in their laptop and everywhere else
in order to get energy efficient and
high-performance computation out of a
multi-core you need software that
exposes peril
ilysm down to the multi-core and at the
other end of the spectrum we have
warehouse scale computers these are the
things that run our data centers and to
fully utilize these and get energy
efficiency and performance you also need
to write software that maps computation
across all the nodes in a data center so
we see a need for parallelism because of
the shift in technology this is also
true because of the shift in
applications that we're seeing so we
have mobile applications and server and
cloud applications are two examples of
this in mobile devices we have little
devices they run on a multi-core and
they're powered off a battery in order
to get energy efficient computation out
of a multi-core you need software that
utilizes that multi-core and that kind
of software requires that there's
parallel ISM to map down to the to the
multi-core so mobile applications demand
this especially because energy
efficiency is important when you're
running off a battery and in server and
cloud domain you're running on warehouse
scale computers so you need parallelism
to get performance in addition there's
also some concurrency constraints here
mobile applications need to communicate
with cloud applications and in cloud and
server applications you need to
coordinate sharing of resource races
resources across simultaneous client
requests so there's really a need
because of these applications for
concurrency and parallelism so there is
a need but why is that an interesting
research question so to talk about that
we can look back at the model that we
have we're all familiar with this model
for sequential programs in a sequential
program there's one thread of control
and execution hops through that thread
of control by doing a series of steps
like a and then B and then see and if
you're a good programmer you write your
software and then you give it an input
and you run the program and you see what
the output is and if you try enough
inputs and you see enough outputs that
match the specification that you have in
your head or you have written down
somewhere then you can put that software
out in the world and have some assurance
that it's correct however the story is
different when we look at multi-threaded
software multi-threaded software shared
memory multi is the most common idiom
these days for writing concurrence and
parallel programs in multi-threaded
software there's not just a single
thread of control there's multiple
threads of control they can interact by
sharing reading and reading and writing
a shared memory space and explicitly
synchronizing with one another to order
events in different threads any
events in different threads that aren't
explicitly ordered with synchronization
execute independently this leads to what
we call the non-deterministic thread
interleaving problem the
non-deterministic thread interleaving
problem manifests in the following way
if we take this program and we give it
an input and we run it and we see its
output we might get one output on the
first execution with that input if we
take the same input run the program
again we could get a different output
and the reason is that independent
operations in different threads could
execute in any order in those executions
potentially changing the result of the
computation so the non-deterministic
thread interleaving problem has several
implications the first is that these
programs are hard to write they're hard
to write because it's hard to understand
how different interleaving zuv
independent operations will impact the
execution of the program these programs
are hard to debug bugs might only
manifest as failures in some program
executions and when they do it's hard to
reason about what the effects were that
actually led to the failure and finally
testing these programs is currently
infeasible because testing requires
looking not just at the space of all
possible inputs but also at the space of
all possible interleaving although there
have been some advances in recent years
in the area of testing multi-threaded
software by some people in the room so
it's getting better but at the moment
it's still infeasible to comprehensively
test these programs and just to show you
this is not an academic daydream this
isn't just something we do in the lab
here are three examples from recent
headlines that illustrate that
concurrency bugs cause problems in the
real world we've seen infrastructure
failures security holes that have led to
millions of dollars being stolen amazon
web services went down so this is a
serious problem and these problems were
all the result of concurrency errors in
software so they're difficult to write
they're hard to debug and they're
infeasible to fully test so bugs are
going to find their way into production
systems and cause those kinds of
problems what we want is that these
programs are simple to write we want
them to be easier to debug so when we
have bugs we can fix them and we want
them to be reliable despite the fact
that we can't comprehensively test them
and bugs might find their way into
production systems so I've just
identified three key research challenges
programmability debugging and fail
your voice and these are the challenges
that I've looked at during the work that
I've done in my thesis so now I'm going
to talk about the themes that show up in
my approach to solving those key
research challenges there are four key
themes that I'm going to talk about the
first is looking at system and
architecture support across the system
stack second i'll look at designing new
abstractions that allow us to develop
new analyses third i'm interested in
systems that leverage the behavior of
collections of machines and finally i'm
interested in mechanisms that are useful
not just during development but for the
lifetime of a system so first i'll talk
about my approach to using architecture
and system support so many people see
the slide they see the word architecture
here and they think well this guy works
on hardware hardware is part of
architecture but the way that I think
about architecture is that hardware is
where the architecture begins and we
have to think about the interaction
between hardware and the compiler and
the compiler and the operating system
and the hardware and language runtimes
and programming languages and software
engineering tools and even things at the
application level like statistical
models of the behavior of a program so
my approach to architectural support is
to look across the entire system stack
the second theme i'm going to talk about
is the use of new program abstractions
that allow us to develop new analyses
that are more powerful and an example
more powerful than prior work for
solving some problems and an example of
a new abstraction from my work is the
use of something called a context-aware
communication graph I'm going to
describe this in more detail later but
I'm showing you now because this is an
abstraction context-aware communication
graphs correspond to the interactions
between threads that occur when a
program is executing and this is
important because it allowed us to
develop a new analysis that helped us to
find bugs that are the result to find
concurrency bugs better than prior
techniques the third theme is that I
like to take advantage of the behavior
of collections of systems and the way
that I do this is by collecting data in
individual machines and incorporating it
together in two models that help us find
bugs when they show up as anomalies help
us to predict where failures might
happen to avoid failures and to feed
information back to the programmer so
they have
a better set of data to work from when
they're trying to fix bugs and finally
i'm interested in systems that remain
useful not just during development but
also in deployment for the lifetime of
the system so we've developed mechanisms
that help during debugging and these
have remained useful in deployment
because they feed information from
deployment machines back to developers
failure avoidance mechanisms are useful
during deployment because they continue
to provide value when systems are
running in production so I'm interested
in these kinds of mechanisms especially
when their hardware mechanisms that
provide provide benefit for the lifetime
of the system so I've used these themes
to address these key research challenges
and my publication record shows that
I've worked in all three of these areas
it also shows that I've worked across
the layers of the system stack I have
papers that showed up at micro and iska
which are architecture conferences but
also at oopsla which is a programming
systems conference in pldi which is a
conference on programming language
design and implementation I'd like to
talk about all three of these today but
unfortunately i'll only have time to
talk about my work on debugging and
failure of woods and i'm going to do
that now by jumping into two of my
research contributions in more detail to
give you an idea for the projects that
i've done in this two areas and there
are 22 efforts that i'm going to focus
on the first is a project called recon
this is a project in which we used
architecture and system support to make
it easier to debug concurrent programs
and the second is a system called a visa
a visa as a technique that helps that
enables production systems to cooperate
by sharing information so that they can
avoid failures that are the result of
concurrency errors so i'll talk about
recon first and if the lighting was a
little better in here you so recon is a
technique for finding bugs in programs
and lighting is better you'd see that on
this antelope there are birds that are
finding the bugs and pulling the monitor
the antelopes first yeah okay so what's
that sorry trafficking
yeah that's actually that's my future
work section I'm going to see if we can
recruit pigeons there are nuisance in
the cities that we can put them to work
so in order to understand a technique
that helps with concurrency debugging I
unfortunately need everyone to just read
a little bit of code the codes pretty
simple I know I know how much everyone
loves reading code but it's it's going
to help a lot so there are three threads
in the program the program is pretty
simple Green is setting some flag called
ready equal to true and then it's
setting a shared object pointer equal to
some new object blue checks that flag to
see if it's true and then takes a copy
of that shared object pointer now the
programmer when they wrote this they had
this invariant in mind whenever ready is
true shared object is going to be a
valid pointer you can see that they've
implemented this incorrectly if you are
good at reading this kind of bubbles on
slides code what blue does with that
pointer that it copied is to put it into
this queue called cue that it shares
with the red thread the red thread dq's
objects from the queue and uses them ok
so the program executes like that and
what happens we see that green sets
ready to true blue seas that ready is
true and takes a copy of that pointer
but green hasn't set the pointer to a
new object yet so it has an invalid
pointer at this point it accused the
invalid pointer and then red uses the
invalid pointer when it deke used it
from that queue so what's interesting
about this bug is that the root cause is
over here in the interaction between
blue and green but the failure manifests
over here in red and that makes this a
very difficult error to debug luckily
there's been a lot of prior work in
debugging this kind of error one
category of prior work is work that uses
program traces to debug these kinds of
errors and program traces are big lists
of everything that happened during a
program's execution the programmer can
look at this list of things that
happened starting from the point of the
failure and hopefully eventually they
get back to the point in the execution
where the root cause happened and these
techniques are effective but they're
limited in one way that is if the
execution is very long and there's a
large distance between the root cause of
the bug and the failure liked you this
could be several days then the trace is
going to be huge
and the program is going to have to look
at gigabytes of stuff to try and figure
out what's wrong with the program so
this techniques are useful but they give
too much information so there's been
other techniques that try to help
debugging by focusing in on a narrow
subset of the operations that happened
during the execution these are
techniques that that help to debug using
dependence information so dependences
occur when operations access the same
piece of memory like the green and the
blue thread are at both accessing this
ready variable and these techniques are
useful because they focus in on just the
operations that the programmer might
care about but in fact they sometimes
give too little information like you can
see here these operations are dependent
so they might be selected in one of
these techniques but they don't tell the
whole story they don't include
information about the accesses to the
shared object variable and that would be
important for understanding why this bug
happened so what we want to do is
develop a technique that gives neither
too little nor too much information we
want to show the program of the root
cause but we don't want to distract them
and we want to take a cue from those
dependents based techniques and we want
to give the programmer information about
communication communication is inter
thread dependence when one thread writes
about a variable and another thread
reads or over writes that that's when we
have communication so we want to show
the programmer when that happens too so
our goal is to develop a debugging
methodology that can reconstruct the
root cause of failures we want to
include all the code that's involved in
the root cause and we want to show it to
the programmer in time order and we want
to give them the information about the
communication that occurred when we do
that we have a reconstructed execution
fragment that's one of the main
contributions of this work these
reconstructed execution fragments are
actually derived from a model of inter
thread communication that we also
developed in this work yeah absolutely I
was strong in purple it was
second I have a strong man proposal for
solving this okay what is it proposal is
you ask the programmer to write down be
invariant that you mentioned and then
check it and then check in and will fail
exactly of them so the problem is that
the programmer often doesn't really know
that invariant explicitly they have it
sort of implicitly in their brain and I
think often programmers have it thought
far enough ahead to really encode that
and crystallize it and put it down in
code there's also the problem of asking
people to express in variance in code
which can sometimes be complicated
actually writing these things that was a
simple invariant but invariance could be
much more complicated you could have pre
and post conditions on API entry points
and you can have data structure and
variants that are not so simple to
express that work is complementary
though I think that's a great idea I
wish people would do that so cool yeah
feel free to interrupt if you have
questions we have a fairly long time
slot and we can we can talk in the
middle of the top if you like okay so we
want a debugging methodology that
produces those reconstructed execution
fragments and here's an overview of the
methodology that we developed in this
work the first step is that the program
crashes and someone sends a bug report
to the developer the developer looks in
the bug report and sees that there's
some bug triggering input and they use
our tool to run the program repeatedly
with that bug triggering input our tool
generates communication grabs in
particular context aware communication
graphs which I mentioned a little
earlier in my talk and I'm going to
explain in detail in just a second
having produced a set of context-aware
communication graphs for many executions
the programmer labels each of them as
having come from a program execution
that manifested the failure or did not
so buggy or not buggy then our tool
takes that set of labeled communication
graphs and it builds a set of
reconstruction's that might help the
program or understand the book and the
last step is to look at that set of
reconstructions and assign a rank to
each one so the programmer knows which
one is most likely to be beneficial to
look at when they're trying to figure
out why a failure happened okay so now
I'm going to go through each of those
steps in a little more detail starting
with what communication graphs are and
how we build them so communication as I
said a second ago happens when one
thread writes a value to memory and
another
reads or over writes that value we can
pretty naturally represent that as a
graph where the nodes our static program
points and edges exist between nodes
whenever two instructions have executed
during the execution so we have the
source the sink and we have shared
memory communication code indicated by
the edge if we do that we have what we
would call a simple static communication
graph static because the nodes represent
static program instructions and in fact
this is a little too simple the way that
it's too simple is that representing
static program instructions in this
graph doesn't differentiate between
different dynamic instances of the same
program instruction so if you're going
around a loop the first iteration of the
loop is the same as the second and so
forth whereas for understanding why a
bug happened it might be interesting to
differentiate between X instructions
executing in different contexts to get
around that we could look at a dynamic
communication graph in a dynamic
communication graph every different
dynamic instance of a static instruction
would be differentiated so the way to
think about this is there's some
monotonically increasing instruction
counter whenever an instruction executes
it adds a nodes of the graph that's
identified by the instruction address
and that counter this is essentially a
program trace in the main so while this
gets around the problem with the static
graphs the problem with this is it's
unfounded so we end up with that too
much information problem that we had
before in this work we developed a
middle ground between the simple static
graph and the unbounded dynamic graph
and we call that a context-aware
communication graph the key idea in a
context-aware communication graph is
that a node represents a static
instruction executing in a particular
particular communication context we add
communication context to the notes the
communication context encodes abstractly
a short history of communication events
that preceded the instruction
represented by that note so if there's
some sequence of communication events
and then an instruction executes that's
one node in the graph if there's a
different sequence of communication
events and then that same instruction
executes it's another node in the graph
so we differentiate between different
instances of static instructions sure
if I have a loop with the instruction in
there I could have multiple instances of
the instruction with the same in
communication graft label communication
is outside the group say yeah so if you
are in a loop and there's no
communication taking place then you
would add multiple instances of that
instruction I mean in practice the graph
doesn't grow the note is already there
so we don't add anything okay
communication you mean shared memory
yeah I mean shared memory communication
in the way that I described before when
one thread writes a value and another
thread reads or overwrite sit that's
when communication occurs that's yes so
those would show up as so the question
was whether synchronization operations
would show up in the communication graph
and in fact they would because they
manipulate pieces of shared memory so as
you'll see in my implementation we we
instrument programs at a very low level
and we have another implantation which
uses hardware support so we're observing
the execution from a very low level of
abstraction and all these things look
similar they look like shared memory
operations sure this is big like local
box no I would think of them a little
more like calling context in a compiler
analysis but instead of being calling
context we're looking at communication
context so rather than abstractly
encoding a call stack we're abstractly
encoding the sequence of communication
operations that preceded this operation
did that help yeah so I'm trying to
understand what's wrong with the logical
analysis
so I charism your
encoding through communications happen
seems like the sameness it seems like a
true be a jewel they encode similar
information this I expect is cheaper to
implement which is one of the reasons
that we we did it this way because we
only have to do things when things
actually share not preemptively on other
operations so sure so I'm still I'm just
trying to see what you
so 49 the communication t90 let's say
equals like that red evangelist verge of
money
so you put that green dot there so the
green dot yeah so I was hoping to gloss
over those details but I can get into
those details now so the the entries
that go into the context are indicators
that say a local read or write happened
a reader right that didn't communicate
or a remote reader right happened
meaning that a reader right happened
which did so that's how we abstract we
abstract away the addresses we know that
it's a remote read so you can actually
think of it motivation for getting to
this abstraction was thinking about
coherence so if preceding some operation
there was an incoming coherence request
that showed that some other processor
had written to some piece of memory that
would be that would populate one entry
in the communication context so that
might be another way of helping to think
about it as local operations are just
you know memory operations and remote
operations are incoming coherence
requests sure she was not was not
accessing the same variable s
different variables even then you would
put the communication of the previous
instruction in the context of people's
that's correct yeah so the context is
for local it is in intra infrared
happens before and
right you can think the context as a
threadlocal property the context is
always being updated and whenever a node
gets added to the graph by a particular
thread you grab the current context and
you add it to the node and then the
context changes and you add another note
in you grab the new context okay it's
sort of like um Kailen a day call Ralph
analysis exactly yeah you're gonna go
back so if I think of it as it happens
before graph dynamically you have a
compression technique is doing basically
a passive one okay and that's sort of
that now becomes your context for
identifying I know as you absolutely
your producer that's a great way to
think about it is is as an analogy to K
bounded calling context that's that's
the perfect way to think about it that's
the way I think about it so okay I'm
going to move on just so I can get
through all the count in here so yeah
that's about it I'm answer okay so i
just described how we build these
context-aware communication graphs now
i'm going to talk about how we go from
communication graphs to these things
called reconstructed execution fragments
which i described a second ago so to
build a reconstruction we start with an
edge from the communication graph life
omitted the context just so the diagrams
are simpler okay then oh you know I
forgot to mention something just a
second ago so we got in that discussion
one more thing that we add to this graph
is a form of bounded timestamp and the
way that these works is not especially
it's not especially interesting it's a
monotonically increasing counter that we
update in a lossy way so the
representation remains bounded so we
start with one of these edges and then
we want to build a reconstruction so we
can look at those lossy timestamps that
I just described we can populate three
regions of a reconstruction the prefix
body and the suffix we populate those
regions based on those timestamps so to
populate the body for example we look at
operations that showed up in the graph
that had timestamps between the time
stamp on this source node and the one on
this sink node and we do the same for
the for the prefix and the suffix o
building reconstruction's is very
straightforward yep
no it was a reward me I don't know where
it went so how do you know what is the
source
I don't think I understand your question
so in one explaining is
machinist
which thread you read it from other than
the fact that it
that if you meant in my stomach yeah so
there's we keep a distinction between
the entries in the context which are
abstracted and the entry and the nodes
in the graph so a node is the tuple of a
static instruction address and the
communication context in which it
executed so you know which operation it
was that's how you know if it was a
reader or right arrow right so you know
that the reeboks was actually a week or
some lead of thanks or overwrite yeah
and it was everyone's something right of
things by somebody else another time
let's all these we actually do we keep
track of it we don't record that in the
graph though I'm so in our
implementation we need to keep track of
that because we need to be able to
identify when remotes reads and writes
our remote but the graph abstracts away
threads and that's actually important
for remaining bounded because if you
think of applications that have
thousands of threads like something
that's built with co routines then it
might be a scalability problem for a
representation if we actually encoded
the thread in the graph does that answer
your question good okay like The Times
essentially soon sequencing everything
but can you have multiple instructions
happening at the same time
i yes but our time stamps are sort of a
they're sort of a cheap implementation
of time stamps and so we have this
monotonically increasing counter that
gets updated lawfully so we don't do
that but you very easily could you can
think about things that happen
concurrently and use that as the as the
time stamp instead the reason we did
this was as a convenience in our
implementation because we actually had
sure yeah so we use the real time the
what's it called Intel timestamp counter
instruction instructions happening at
the same time
I'm Sam on different processes yes due
to imprecision in that counter yes you
could okay I'm trying to understand like
this picture makes it look like
everything is serialize I'm trying to
understand if you have to construct an
arbitrary serialization of all the
instructions across all the different
process swords or whether your time
stamp just gives opportunity the time
stamp gives us the serialization yeah
the time think of it as a system-wide
time that we're using to populate this
and I don't understand how you comment
up with multiple instructions occurring
in exactly the same time I mean our VPS
you and two machines we have to say well
yeah so I guess because of you because
of the because of precision in that
thing I guess yeah because of
concurrency and and imprecision in that
counter it's possible I've omitted that
because I don't think it's an especially
important detail but you're right that
that could happen if things did have the
same time stamp because they happened on
two processors that had the same counter
they would end up in the same region of
the reconstruction so you wouldn't
necessarily know the ordering across
those things but you'd know which region
they showed up in there's something I'm
going to get to which makes it less
important to know ordering within a
region yeah and I'll show you that in
just a second we and i'll come back to
your question when i get there if you
want yeah so it's actually this right
here so the reason that that's not
especially important is that we take so
one of the big problems with dealing
with concurrency errors is that you get
different behavior from one execution to
the next and that means you get
different reconstruction's from one
execution to the next even if you start
with the same communication graph edge
so we have a way of aggregating
reconstruction's together that came from
different executions and obviously from
different executions there will be
substantially different in incomparable
timestamps so the way that we produce an
ordering that we show to the programmer
eventually is by aggregating across
executions and combining things that
occurred in the same region of the
reconstruction so the this is why I was
sort of hedging around that question
because going to get to this it only
matters that they end up in the right
region of the reconstruction yeah and
then we know ordering prefix things
happen before source and source happen
before body and so forth in the week
on the right hand side of the equal sign
of blue and the green oval that are so
parallel to each other means one of
those occurred both of those occurred
either of them current post with
semantics so there's something else that
I'm leaving out of this diagram for
simplicity because usually I smoke
through this in about ten minutes but I
know that's fun yeah oh add more details
so in our actual implementation these
things come with confidence values and
the confidence value says this happened
in fifty percent and this happened in
fifty percent or this happened in
99.999% and this happened in one percent
of executions it means so we build
reconstructions from grant from graph
edges that came from graphis graphs from
failing executions and so if we see
green in 99.999% of the body regions
from failing executions then we can have
some confidence that when the program
fails whether this is significant or not
is something else that we decide but
when the program fails that thing tends
to happen between the source and sink
very often happens between the source
and sink so that's what that confidence
value gives us okay yeah question the
back so because it could be that way
fifty percent of the time being happens
you out but they never occur together
right so there's no dependence included
in this problem right now we're not for
a Dakota eight events we're treating
them as independent so I probably just
went out of the range of the camera so
okay okay so I've just talked about how
we build reconstruction starting with
those graphs talk about how we aggregate
reconstructions from different
executions I'm going to talk about how
we figure out which reconstructions are
actually useful we do that by
representing reconstructions as a vector
of numeric valued features and each of
those features represents a different
property of the reconstructions using
the values of those features we can
compute a rank for each of the
reconstruction so our tool works by
generating lots of reconstruction's
computing these feature vectors
computing a rank and then ranking the
reconstructions that were produced our
goal is to produce a rank ordered list
of reconstruction's where the first one
in that list is one that points the
programmer to the root cause of the book
so you're probably all wondering is what
RBC and are what are those features so
I'm not going to talk about all of them
but i'll talk about one to give you an
intuition for how the features work to
help us figure out which
reconstruction's are related to a bug so
one of the features that we use is
something that we call the buggy
frequency ratio and the intuition is
this you build a reconstruction around a
graph edge if the graph edge occurs
often in failing executions and occurs
very rarely in non failing executions
then we assume that that graph edge
might have something to do with the
failure and so we improve the rank of
reconstructions built around that edge
and conversely if the thing if this were
the other way and this graph edge were
to happen often and non failing
executions and and often an unfeeling
executions and rarely in failing
executions then we would say well that's
probably not very useful for
understanding the bug and so we would
give that reconstruction a lower rank so
that's the intuition behind the features
and the other features and codes similar
similar ideas but for different
properties of the reconstructions sure
yes there's an encoder tap
bugs that are two things happening
together these are you defenders yeah so
one of the other features that we look
at looks at the consistency of things
happening in a particular region of the
reconstruction and that captures that
two things that that idea that two
things happen at once so if maybe we
should yeah maybe we can talk about this
later because I think it'd be easier to
talk about it offline than to try and
get into it without a whiteboard right
now so yeah one of our other features
does just capture that property though
okay now going to talk a little bit
about our implementation and our
implementation sure yeah so how
dependent value under all of you labels
a buggy Don but because you could have a
long bout you run but book has just not
caused a crash yeah we are completely
and absolutely dependent on that
property and something that I'm really
interested in my future work is to make
systems that can tell you earlier than
we know now that something has gone
wrong and I think that's actually a very
hard problem in general okay so I'm
going to talk about implementation our
implementation we started with a
software implementation we use binary
instrumentation for C++ because pin and
for Java we used Road Runner and our
instrumentation is simple we inject code
around memory operations and the code
that we inject updates a data structure
that represents the communication graph
so you can go to my website and you can
download this stuff now and you can use
it if you want to and that makes it
pretty cool in my opinion because it's
practical and you can go and run it on
your machine the downside is that using
binary instrument instrumentation is a
little bit of a bummer because the
overhead can range between fifty percent
for some applications to like 100x so
obviously a 100x slowdown is a little
bit of a drag but if you look at tools
like valgrind you see overheads that are
actually similar for some applications
so it's high but it's not unreasonable
people actually use valgrind in
practical software development so we saw
those overheads and we were encouraged
because it was usable but we wanted it
to go faster and so we looked at how we
could use hardware support to make graph
collection more efficient and our base
design for our hardware support
mechanisms was a multi-core processor
that has coherent data caches I'm going
to add some things to this design and
they'll show up in blue and those are
the extensions that we proposed the
first extension that we proposed is
communication metadata communication
metadata is information that we add to
each of the lines in the cache in
particular we record what was the last
instruction to access each cache line
that's enough information to that's the
information that processors need to
build the communication graph we
extended the cache coherence protocol to
shuttle our communication metadata
around and that's useful for the
following reason cache coherence
protocol messages are sent between
processors when communication is taking
place in the application so if we attach
our communication metadata to coherence
protocol messages and a processor
receives an incoming coherence message
they know the communication is happening
and they know the instruction with which
the communication is happening so they
can actually using that information
build an edge that they can add to the
graph yep it's searchable yeah okay no
your previous thing we never liked yeah
I was doing that for illustrative
purposes I'm not sure I think I lost you
beginning about the problem of the
setting identifying static versus
dynamic late ah oh right so the context
is part of our hardware support I've
left it out of this diagram because
usually I actually find that I don't get
into this much detail in the discussion
I'm really glad you guys are asking the
question this is more fun than the
normal talks that I get where it was
just silent but the context is part of
that we have yeah we keep the context on
the course so in the metadata it's
actually instruction context tuple that
gets stored right and we also had a
simple hardware structure to store the
communication graph that's a fixed size
fifo and it's fixed size so when it
reaches capacity software trap happens
we have a runtime layer that empties it
out stores it in memory and you can use
it during debugging we have a software
tool that does all the other stuff that
I described a minute ago sure yeah
absolutely so fall sharing means that
we're going to see communication that
didn't really happen and cash evictions
mean that we don't see communication
that might have happened and things
things get out of date so that's some
imprecision and we have numbers on that
in our paper we showed that it's not a
huge problem for debugging but yeah it
does show up as a problem did you guys
look at in your compiler now her did you
have any kind of compiler analysis
little bit code and said hey this is all
I can guarantee the struggle vs in which
case no but we cheated a little bit and
we excluded stack locations assuming
that they wouldn't be shared I mean
Jetta Java that's reasonable in C++
people can do whatever they want to but
we find that common practice is not to
do that so we omitted those excesses
yeah okay now I'll just talk about some
of our evaluation so we built we built
that tool and we simulated that hardware
and I'm gonna talk about how we
evaluated that so if we had just built a
compiler and a compiler optimization we
can take some program use our
optimization and show that our
optimization makes it go lots faster
evaluating this was a little less
straightforward so we had to come up
with a measure of what was the quality
of our technique and we measured quality
by looking in that in that rank ordered
list of reconstructions that recon
produces the quality is higher if an
earlier entry in that rank ordered list
points us to the
cause of the book and the quality is
lower if there are more things ahead of
that root cause reconstruction that
don't have anything to do with the root
cause we also looked at performance
which is just the runtime overhead and
we looked at some of the hardware
overheads as well for benchmarks this is
also kind of a challenge and I guess
like you some of you in the room can
probably empathize with me here finding
tools finding programs to evaluate at
concurrency debugging tools can be a
real challenge because there's no
standard benchmark suite so we actually
went to the web and we found programs
like my sequel Apache Java standard
library things like that and we found
bug triggering inputs and we reproduced
those bugs and we showed that our tool
can actually lead us to the root cause
of the failures that the books trigger
we evaluated performance using a set of
standard benchmarks parsec tekapo and
java grande so here is a high-level
summary of the results that we found
when we evaluated the quality of our
system the first was that using a set a
reasonably sized set of graphs from
buggy and non buggy executions 25 was
the number and we found that a
reconstruction of the bugs root cause
was first in that rank ordered list that
recon outputs that was nice because it
shows that with a modest amount of
effort devoted to collecting graphs the
programmer is led to the the root cause
the book and we also identified a
trade-off with respect to your effort
and that trade-off was the following if
the programmer uses more graphs then the
quality is higher if the programmer uses
fewer graphs they spend less time
collecting graphs but the quality is
lower so they might have to spend time
looking through what are effectively
false positives in the output sure we're
fracture the 2500 right so that was 25
buggy and 25 non buggy 50 50 in total
yes supposed to look in the car all the
time it was
ten-four so we actually in the
experiments that we use to illustrate
this trade-off we used 5 or 15 buggy
graphs assuming that it was harder to
get buggy executions and 25 correct
graphs because correct graphs are
essentially in limitless abundance okay
our performance evaluation we showed 10
to 100 times overhead in software like i
said before i'm pretty high but
comfortable to other tools and there are
two sources of hardware overhead that we
found interesting one is how often do
those traps happen where you have to
empty out the fifo and stored in memory
and two is how often you need to update
the metadata thats hanging on the end of
the cache line so we found that traps
are pretty infrequent this is less than
one in ten million on average smiling
and have a question what else you have
transfer infrequent but what I would
like you matters more is how this is
taken to handle the trap nekia fine
photo and what's that overhead on the
overall performance yeah so I don't have
numbers on that and we could talk about
that later but the infrequency helps to
amortize that cost but you're right i
mean it's really the increase in latency
they could be a problem yeah and the
second result is how often we have to
update that metadata because that could
be a problem you'll see this is
considerably more frequent two percent
of memory operations is fairly often
however in a hardware implementation
this can happen in parallel with
accessing the cache line itself so it's
not likely to be performance problem
because it can be parallelized okay so
just to summarize those themes that I
described Kirk Aisling
yeah and it's emphasized because of that
yep we have an analysis of that in our
paper if you're in student checking that
out yeah so I just showed you that we
developed in New abstractions context
where communication graphs and we use
those to build reconstructed execution
pregnant there was support across the
system stack I showed you a hardware and
software implementation and I showed you
and our results some of the trade-offs
abusing each of those this is a system
that is useful even in deployment
because with a hardware implementation
we can collect this information all the
time and send it back to developers and
finally this system takes advantage of
collective behavior because information
could be pooled from many systems that
run the same piece of software and the
information can be confined combined so
that's what I have to say about recon
this is a new architecture and system
support mechanism for making concurrency
debugging easier okay yep questions i'm
just letting starting flow via the time
we have been doing a lot of questions I
want to make sure i do get for
everything with that key to every love
you I don't know okay sure seriously so
if I did something really dumb like for
instance I just recorded the last thread
that access to facility would cut that
as my
I said this this is where the potential
bug this is the IP of the sort of aware
of the cop root cause of a bugger okay
what I'm trying to go to this did you
guys do any kind of analysis where you
had some sort of baseline that said that
we've got a very complicated system is
there any kind of based on where you
have some comparison that says you know
something simpler like the strongman
Johnson beginning yeah doesn't actually
do something right we didn't do that but
something I would really like to do in
the future is to actually get some human
subjects into the lab and say debug
using technique aid to bug using
technique be and do a comparison and
maybe not even just across the work that
I've done but across work that has come
from other groups I think would be a
really informative study to see which
techniques are actually good and it
might involve collaborating with some
HDI people because that's a little bit
outside of my area of expertise but it
could be really interesting to see those
results yeah so cool okay well yeah so
that's my answer and I would love to see
more human subject studies going on in
this area of research I just you don't
see that many and I think it'd be really
cool to see more of those so all right
now I'm going to change gears and I'm
going to talk about a system that isn't
about finding and fixing bugs for a
programmer but rather it's about systems
cooperating to learn how to
automatically avoid failures and you can
see just like these Buffalo are all
looking outward they're cooperating to
avoid failures which would be lion
attacks or something in this example so
these photos are you a trip to sim Bob
way so I've got a bunch of stupid
vacation photos and my talk yes and well
the if you have debugging and failure
avoidance are really synergistic they go
together so okay so i'm going to start
this section of my talk with an example
that shows you at a high level how our
system works but first i'm going to talk
about how things work today when you
develop software today you have your
development and debugging system you
make your application and then you push
it out to the deployed systems like this
the deployed systems run and sometimes
they get one of these thread into your
leavings that leads to a failure so this
might be a concurrency bug and if you're
a good developer you collect the core
dump and you have that sent back to your
development and debugging box and with
the core dump in hand you can spend time
with a patch and figure out what went
wrong with the program the interesting
thing about this is the developer is
active but the deployed systems are
passive in this process just waiting for
a patch to come from the developer in
the meantime the deployed systems might
experience the same failure over again
degrading the reliability of the
community of systems so in this work we
had the idea to make the deployed
systems be active in this process as
well we make them cooperate by sharing
information to learn why failures happen
and what they can do to avoid those
failures in future program executions
okay so now i'm going to give you an
overview of what things look like if we
have a visa which is our system that
takes advantage of that idea so just
like you have a development debugging
server we have an aveo server in the
deployed systems we see that the
application is linked against the aviso
runtime which runs on the same machines
as the application itself we see that
same failure and just like we sent a
core dump back in the baseline system in
this the case with a visa we send an
event history back to the aviso server
aviso does some analysis on that event
history and says and the information
that it extracts from that analysis goes
into building a model of what happened
in that failure what happened preceding
that failure and it's important to note
that this is a cooperative model anytime
a failure happens over here it ships an
event history over to the visa server
and contributes them also nodes are
cooperating deployed system nodes are
cooperating by sharing information using
the cooperative failure model the avisa
server generates constraints on the
execution schedule of the program that
restrict certain at the order of certain
events in different threads and when a
visa finds a constraint that prevents a
failure it ships it back across to the
deployed systems the deployed systems
can use those constraints to avoid
failures and note that if one node fails
and has a constraints into it that same
constraint can be sent to all the other
machines trivially and they can share
the wealth of failure of Luton's user
I'll show you I'm going to get to that
yeah so there are three parts to the
system the first is what are constraints
and how do they work and the second is
what are the event histories that we
collect and how do we use the
information in the event histories to
generate constraints finally I'm going
to talk about what goes into that
cooperative failure model and how is it
useful for picking which constraints are
going to avoid books so first I'll talk
about these schedule constraints to talk
about these I need to show you a little
bit more code this code is really simple
though there are two threads you have
the green thread it's doing something
funny which is set this variable to null
and then set it to a new object so it
does two operations blue thread is
acquiring a lock and then using that
pointer that green is playing with over
there and then releasing the lock so
this program is broken in several ways
and we can talk about them at length and
the important thing to know though is it
executes sunder this interleaving blue
uses the null pointer because Green said
it's an L and then blue used it and
that's a problem the way to understand
this bug is that this bug is
characterized by the event ordering that
I've indicated with those dashed arrows
when P happy equals null happens and
then people are use happens we get the
failure only if P pointer use proceeds
the assignment of P to that new variable
we can also observe that if we had a
different ordering of events like p
equals no followed by assignment of p to
the new pointer and then the use of p
well that wouldn't lead to a failure so
the key one of the key ideas in this
work is to shift the execution away from
failing schedules like the one on the
previous slide and toward non failing
schedules like the one on this slide to
do that we develop the idea of a
schedule constraint and a schedule
constraint says that a pair of
operations contribute to a failure and
reordering around those operations can
prevent that failure so a schedule
constraint is really nothing more than a
pair of instructions in the execution
and the semantics of x schedule
constraint are very simple we have a
jewel constraint like this and it has
the green instruction in the blue
inscription the semantics are the
following when in the execution we reach
that first instruction the constraint
gets activated subsequently in the
execution when we reach that second
instruction the blue one that
instruction gets delayed
those are the semantics of a schedule
constraint now i'm going to show you
with that example why this is actually
effective at they'll add avoiding
failures it's effective because in this
example you can see that p equals null
gets executed that activates the
schedule constraint then p pointer use
tries to execute normally that would
cause a failure but instead the
constraint delays the execution of that
operation in the meantime Green steps in
execute sits p equals new P and later
after that delay expires blue gets to
execute its operation without failing
you know thinking of the second green
instruction is enabling a little bit
instructions that's a really good
question so why don't we just figure out
what instruction this is and make
constraints that have all three of those
instructions right okay yeah yeah
something like that the main reason is
you'll recall from the previous example
that the failure occurred at this
instruction and so if we want to do
forensic analysis in our server we don't
know this instruction exists we have an
event history and I'll show you in a
second what kind of event history
history as we keep and the event istry
doesn't say anything about p equals new
p we have the code of the program and
something I'm looking at in future work
is doing a better job of tuning these
delays based on predictions of which
instructions might might be good
constraint deactivators yeah okay static
versus a constraint is a pair of static
program instructions and when a dynamic
instance of the first instruction in the
pair occurs it activates the constraint
and when an active constraint is I want
to concerns active in the second
instruction executes then that causes a
delay like this second we load
instructions for that yeah so if a
constraint is active because the first
instruction executed in one thread in
any thread except the one that activated
the constraint so in not in the same
threat in any other thing otherwise
you'd get some atomic region that
prevented itself from proceeding because
some worried about a scenario where this
causes timeouts and it casts
yeah so that's a problem the delays are
fairly short on the order of hundreds of
instructions and we did a
characterization of the delayed site we
established this empirically one of the
things I want to do in the future is do
a better job of figuring out how long
those delays should be and if there are
program events as Jim point net program
events that we could use to trigger
expiration of a delay instead but we did
this empirically we found a range of
that the range of failures that we were
dealing with in our experiments fit into
a particular dilemma know so that's an
area of I want to look at in future work
what is it was a some of the constraint
between those two games functions that
decided to delay the allocation of be
to satisfy someone orchestrating these
duties cancel each other yes right so
the situation is where you essentially
end up with live lock because delay is
cascade between breads so because you're
using days just because you have two
different constraints they're inserting
delays it just can't see each other
right so yeah I agree this is
problematic one delay could undo the
good of another so the good news is
you're only as bad as the program was
initially the bad news is that means
that this mechanism doesn't actually
work another answer to that question is
that I'm actually trying to work on a
formalism right now that shows that as
long as delays are a cyclic and the hard
part is defining what ASIC licit e is
for these kinds of things then you can't
end up with situations where delays
caused live lock or in the way you
described it is undo one another's work
causes plus plus performance risk in it
yeah with performance degradation you're
right so there is an impact on latency
we did find in our evaluation of this
that delays are very infrequent however
and that's a property of the
applications that we looked at so you're
right to say that if the leys happen
very frequently this could be increasing
Layton sees and causing timeouts and bad
things to happen in practice we found
that's not the case and furthermore in
our model for selected constraints which
I'm going to talk about in just a couple
minutes we can build in a quality of
service constraint that says don't use
schedule constraints that degrade
quality of service meaning cause
timeouts cause unacceptable increase in
requests late and see things like that
biasing this case
a subset of scale and those can you will
screw kind of cause what we expose other
buds that you wouldn't that's a very
pessimistic view I mean the reason we're
doing this is because another well the
reason we're doing this is to bias the
schedule away from schedules that we
think are going to cause bugs you have
no its you it's true but I think that
that's an incredibly pessimistic you
that's saying it when you go to avoid
one bug you're gonna land on another
bucket I just I think that it's possible
when it's caught it's like it's like
what these benches yeah they're being an
innocent invaded that B is not known
before they use this what you do at
runtime is an evaluative invariant and
if it fails be able to crash so rather
crash from the program you just delay
for your hangstrom evaluated again and
hopefully get me a visit you again I
think I just run virtual software sure
real like you know why do all these
problems you're above the crash private
fashion the problem is having
specification we will hear specification
edematous later than in person done here
at a friends there right so you can
generate the offense but actually write
the neighborhood job that you'd have to
null test before sure you could even you
could have ensured this yeah but we
should collaborate on that kind of
project in future Alec other day okay so
decision than your actual and benchmarks
for this assignment npc go wrong or you
see things go wrong going to
semantically fall like you did wrong
you're talking about the question how do
we identify failures in the benchmarks
of the upsetting practice this happens
really what what is it that goes wrong
in practice is it nobody references or
is it we got the wrong answer because
something you reference wrong the point
yeah I touched on this point before you
came in so the the way that we identify
failures is actually looking for fail
stop conditions assertion failures and
segmentation faults and signals and
things in general at finding failures is
a hard problem and predicting when
something has gone wrong is unsolved and
I think a cool thing to look at in the
future so sure yeah different values of
hoping the delay and in production
monitor the effects this has or maybe
the latency that you're seeing you can
and that would end to end latency you
can and that would increase the search
space because you'd have to try to pools
of pairs of tuples of constraint and
delay time but it's feasible yeah we
don't okay right yeah so you'll see when
i got to the loan i get to my discussion
of the model is this you know I guess
yeah the largest delay that has no
effect on the system right something
like that right that's certainly what
you want and when i get to the model i
can talk a look first for a second about
how we can incorporate that information
in the model send out a variety of
constraints with differing delay street
if you have a large collection of
systems right
parallel search the space yeah running
different delays have you seen my talk
before okay yeah this is essential this
is central to the way the technique
works so yeah so something I mentioned
before is that the way we generate these
constraints is by collecting a history
of events that happened before the
failure and we use that information to
generate constraint so I'm going to talk
about what goes into those histories and
how we collect that information so if we
have a program like this we need to
instrument events that are interesting
when we're trying to deal with
concurrency bugs there are two kinds of
events that we think are interesting one
is synchronization events and the other
is sharing events synchronization is
locks and unlocks threads spawns and
joins things like that and these are
easy to find with a compiler and if
there's custom synchronization or
something like that the programmer can
tell our it's just that this is what I'm
using for synchronization sharing events
are memory operations that access memory
locations that could be shared across
threads it's harder to find so the way
we do that is by using a profiler
because using a compiler we have to be
conservative and it's hard to identify a
reasonably small set of sharing events
in the execution so once we've found
sharing about synchronization events we
have a compiler pass that inserts calls
into our runtime into the program and
those runtime calls are used to populate
the event history that I described
before so the event history is a data
structure that exists in the runtime
while the program while the program
executes and when the execution unfolds
we see the event history gets p equals
no because that's an event and then we
see this acquired lock and then we see
people enter use and then we see a
failure a visa also monitors for
failures and it considers assertion
failures signaled terminating signal
deliveries and things like that fail
stop conditions to be indicated as of
failures and like I said a second ago
we're looking at other ways of
identifying failures so after the
program fails we have this event history
that shows what happened leading up to
the failure then we want to generate a
set of constraints that are candidates
for preventing that failure behavior
that occurred we do that by enumerate
all the possible pairs of instructions
in a window of that event history that
executed in different threads so in the
toy example event history here we have
two different constraints that we can
generate the P equals not only acquire
lock and the P equals now on the Pew
pointer use and you remember this is the
one from a second ago that I
actually works to avoid that failure by
adding it the way okay now I've just
showed you how we can generate a set of
constraints but I didn't tell you how we
decide which one is actually useful for
avoiding the failure so that's the last
part of my talk is aviso gets a bunch of
failures builds the mott builds up a big
set of constraints and then it needs to
decide which ones it wants to send over
to the deployed systems so they can use
it to avoid failures which one does it
pick to answer that question we develop
a constraint selection model our
constraint selection model has two parts
the first part is the event pair model
and the second part is the failure
feedback model the event pair model
looks at pairs of events that occur in
the program's execution and in
particular how frequently pairs of
events occur in non failing portions of
the execution to get that information
aviso sparsely samples event histories
from non failing execution the intuition
behind why this is useful is if a pair
of events happens often in a correct
portion of the execution then it's
unlikely to be responsible for the
failure so trying to reorder around
those events isn't likely to have any
impact on whether the failure manifests
are not the other side of the model is
the failure feedback model this model
gets populated when we start issuing
constraints out to deployed systems it
explicitly tracks the impact on the
failure rate of the system when a
particular constraint is active and when
no constraints are active so the
intuition here is that if a constraint
if the instance of a constraint being
used by a system correlates with a
decrease in the failure rate then that
constraint is more likely to be useful
in future executions for avoiding the
failure we have a way of combining that
information together that I'm not going
to describe in detail into a combined
model that is a probability distribution
defined over the set of all the
constraints that we have and aviso draws
constraints according to that
probability distribution and issues them
to the deployed systems so if anyone's a
machine learning person in the audience
this is an instance of reinforcement
lead learning and it's a variant of the
K armed bandit model for reinforcement
learning yep
that feedback model to actually be used
so not only get one sample one data
point right for one crash yep that's
right yeah so we found in our
experiments that it's relatively few 10
to 100 and we start to see the feedback
having an impact on which constraint is
drawn so in a way you can think about
this model as being predicted we have an
infinite amount of correct execution
data and then a failure happens so this
predictive model says which pairs of
events aren't likely to be useful so we
discard those as much as we can but we
have some that we either don't have
enough information about or are actually
useful for preventing the book so we use
the prediction delays from the but it
just occurred and by the time they you
know it's a pretty big emergency maybe I
don't know I'm just I might be a pretty
big emergency but it doesn't fix the
program I mean it's the page no but I
mean maybe a 10 point like your stead
people working on it and they might fix
it and employ it as an anecdote this is
time is becoming an issue but there is
an anecdote that I like to talk about
this is something I saw on the memcache
d developer board they had this bug and
it was open for a year and it was a lost
update that triggered an assertion
failure at some point later and the
developers saw the bug report and then
decided to ignore it because they said
fixing this would introduce a seven
percent performance overhead so it
stayed open for a year who knows how
many people using memcache the
experience this bug saw that their
server went down and then restarted the
stupid thing eventually enough bug
reports came in that they actually went
and finished it I've fixed the bug a
year later submitted a patch with the
seven percent performance degradation in
contrast our system was able to fix that
bug with a 15-percent performance
overhead which I'll show you later and
it did it in the space of 10 to 100
executions rather than the number of
executions that had bug reports
submitted for them in the space of the
year so that kind of helps to tune the
the time frame for how bugs get dealt
with and in general bugs can stay open
for multiple years a year could be
generous for some open source packages
that are pretty what least
right there it could be I mean I but
that's hard to quantify it but i mean
it's its a sega yeah yeah well i mean so
the information we're collecting says a
lot about why the bug happened so send
it back to developers they can use this
yeah yeah well some are ok mine was just
like put this FBI 60 seconds of citibank
window so what your stuff is doing is
you're reducing the movie 45 seconds I'd
rather shut the program down not like
people I don't understand what it was
the window they so so there was the one
of the three things you motivated here
tom casino robbery yes guys exploiting
this race or something yeah so what
you're thankful Kazuma we might do is
you know shrink decidedly window but
still keep it open as opposed to someone
noticing a detecting and just shutting
the system down unless there was some
way of identifying that a failure or an
attack this thing this way yes are using
it but for ya for to keep the system
available in the case of Phil stop bugs
this is definitely it I mean I think
this is a better option than letting the
system crash if availability is the most
important thing sometimes it's not the
real major that's absolute worst even in
the case of security bugs this can still
be useful and I think especially when
combined with techniques for identifying
that something anomalous is happening in
the execution I haven't done that work
yet but if you if you think about a
technique that says hey an attack might
be happening maybe we can use a
mechanism like this in combination with
something like that to keep the system
available and to close the security hole
I think I think that's something
interesting to think about
this is
sure you and I are going to lunch so
everything oh yeah do you want to start
with me because I have a few more slides
of it sure I'm afraid I'm just looking
to start leaving because it is 20
minutes so you know one thing please
like what a piece of information if you
don't seem to use is that incorrect
executions something have something
happens incorrect executions which
doesn't happen in Corrections execution
so you don't have to worry about that
here right we have half of that
information but we don't have we don't
the other half right right so we can
incorporate information from failing
event histories into our predictive
model but I haven't done that because I
couldn't come up with a way that
reliably produced good predictions it's
just it's a hard problem because you
have you also have a data sparsity
problem because you only see you see
fewer failing executions then you see
correct executions and there are lots of
events in a program and so for some of
those events you don't have you don't
have information from the failing
executions which makes it a hard thing
to to incorporate that information yeah
approximately
with it static ideas right yes you're
not adding any projects we have no tits
context insensitive and so have you
thought about adding more content like
your pockets of air I have so call sack
information would eliminate some
spurious delays but collecting it is
expensive I mean it adds overhead to
collect the call stack information
no we're not simply because so in order
to in order to activate a constraint we
would need to know that a particular
instruction executing in a call stack
was happening so we would need to do a
check that computed the call stack at
each activation point so cool our
implementation is simpler than this
slide makes it seem there are three
parts the runtime the compiler and
profiler and the server compiler and
profiler the profiler was written in pin
compiler we wrote as a pass for llvm and
it takes it it takes responsibility for
finding an instrumenting events and
linking to the runtime the important
thing about the interaction between the
runtime and the server is that they
exchange event histories and they
exchange schedule constraints and the
server maintains the model of how to
draw constraints and they communicate
over HTTP so the system is portable and
implementation you can put it anywhere
and it's not it doesn't need to be on a
single machine for example so I'm going
to talk about how we evaluated our
system our goal my goal in the
evaluation is to convince you that we
measurably decrease failure rates in our
experiments with some real applications
and that our technique has overheads
which are reasonable especially when
availability is the key concern our
setup was to use a small cluster of
machines that I'll run the application
and we had a single aviso server and we
use the set of benchmarks that partially
overlaps with the ones I used in the
previous study memcache d and apache and
transmission were the biggest
applications we looked at so here's a
summary a very high level summary of the
results for some of the for one of the
applications we had a reduction in
failure rate of 85 times that was a
failure in the PHP processing subsystem
of apache httpd and we saw 85 times
decrease in the failure rate so it
happened 85 times less frequent this is
one
that's correct I mean that frequently
was over a humongous space in our
experiments we use hundreds of billions
of requests hitting the system so it was
a very large space of execution that we
looked at you had you were actually
avoiding multiple bugs yeah so we have a
study that didn't make it into the paper
where we took two different bugs in a
version of apache and we showed that we
can avoid them and the key to that is
that we have schedule constraints and we
need to decide if it's the same bug
happening or a different one and we do
that by fingerprinting bugs based on the
event history that preceded the failure
so doing that we can send one constraint
for each bug that we've fingerprinted
and we can we can solve that problem it
didn't make it into the paper but we
showed that it does work without
increasing overhead as like the product
of the the delay the overheads the
average case overhead was about 15 times
decrease in the rate of manifestation of
the feathers and the overheads that we
saw were practical especially if
availability is the most important thing
there were over heads as low as 5
percent when we were monitoring the
execution and using delays to avoid
failures and the average overhead was
around twenty percent so these are over
heads that are acceptable in production
systems like I said especially when
latency is not the highest priority and
availability is more important okay so
there were just to wrap up this section
schedule constraints are the new
abstraction that we introduced we have
support in compiler runtime and we have
a statistical model at the application
level there are this is a system that's
useful for the systems lifetime because
it actually helps deployed systems be
more reliable and it takes advantage of
collective behavior by sampling
information for many deployed systems so
that's it for the two projects that i
was going to drill down into this has
been just awesome to have this many
questions i really appreciate it usually
yeah it can get dry to give this talk a
million times so so now i'm going to
move on in like three minutes talk about
some future work and then i'll take more
questions afterwards if there are still
people that are wondering things so in
the future i'm interested in continuing
work in the direction of reliability and
in looking at adapting some of these
techniques to energy efficiency and also
looking at some emerging architectures
which i think are interesting so i'm
going to talk about those now to start
with though
is a picture of the way that I think
computer systems are being built today
and I think it's getting worse so we
have multicores and in order to get good
performance out of a multi-core or a
data center you have to put a lot of
burden on the programmer to get that
programming right so that we get
reliable execution and the burden is
primarily on the programmer to go and
avoid crash it's just like this guy in
the black needs to know that's not this
is stocker this is stock art from the
internet this is stock art from the
internet I just found this photo I
thought it was funny so this guy gets a
brick level parallelism but he pays for
it and that he has to carefully stack
these bricks on his bicycle and he gets
good performance but it's it's really
hard so I think we need to address the
reliability problem in the past we've
been focusing a lot on the performance
problem I think the problem is getting
worse as we move toward heterogeneous
architectures where the programming
problem is going to be more complicated
when when we're addressing reliability
and performance we need to keep in mind
complexity we need to balance where
complexity ends up in a system does it
end up in the architecture or the
compiler or the language or in the
programmers hands or wherever we need to
keep that in mind when we're coming up
solutions so one thrust of my future
work is going to be to continue to look
at reliability reliabilities the problem
that I've been talking about and in fact
I think that as the the performance
benefits of Moore's law are petering out
because of the utilization wall and the
power wall we're going to need to find
other ways of adding value to platforms
and this is especially interesting to
companies but I think that this is
interesting in general and one way that
I think that is a really promising way
to do that is to add features to
architectures and systems that improve
reliability all the time and I described
two of them today one that has hardware
mechanisms and another that's a software
layer so I think there's a big
opportunity to research and reliability
one idea in particular that I'm really
interested in is the idea of decoupling
the process of developing software from
the reliability of the software aviso is
a really early example of this you're
taking some of the responsibility for
avoiding failures out of the hands of
the programmer one area where I think
this is especially interesting is in
shared resource platforms like cloud
applications and in mobile applications
so I think of the process as hard
applications in these kinds of platforms
the programmer doesn't see anything
different they just deploy the software
the user doesn't see anything different
they just get software as its
distributed some interesting points
related to how we can take advantage of
sure so it's about the cats eat these
are this company yeah I've never seen
that anybody cares so much about
reliability especially if nothing is
visible at either end then you know if
the user doesn't see any perceptible
benefit right why would why would a
company in us to do like they see it by
comparison to other platforms so you
have all sorts of reviews on just take
mobile space for an example you have
android verse iphone and if i'm an end
user and i'm saying which phone do i
want to buy the next version of well you
can look at Andrew and you can look at
like at iphone and say which one has
more crashes and then you can go and buy
a Microsoft Windows Phone or something
like that and so this one has fewer
crashes because someone baked something
into the software runtime layers to
improve the reliability and that could
actually it's never happened I totally
agree with you and it's never happened
because people have focused on making
performance better in subsequent
generations at all features butchers but
what are features are essentially
performance features are things like
vector processing and that gets
performance and features are things like
better optimizing compilers it's for
performance so I think reliabilt no
features yeah something you know I want
to talk to my phone sure I think I said
they arguing against times we spend with
knows about
right if you're on Windows one of the
value acts you can get fashion sin to
microsoft and we expose those to be
scrapped
right so I think that because you have
shared resource platforms like that you
can you can do things like you just
described and aviso and like what you
just described is only the very
beginning and I think there's a lot of
other opportunities and so this shows
some of the advantages to looking at
these platforms and some of the
opportunities are there one is that you
have the common infrastructure so you
don't need to boil the ocean if you want
to push a new testing tool or a new
optimization technique or a new failure
of one's technique load it into the
platform and you get it everyone has to
use it you have control over the
hardware so if you get header it if you
you find that you can get easier
simplify the programming problem using
some heterogeneous hardware for solving
some problems or you can get better
performance you have control over the
hardware in the environment you have
massive scale so those models like i
showed the statistical models that we
use in recon and in ibiza they improve
when you have more data and you'll have
lots of data if you're looking at a
cloud system we can also make systems
that do something similar to what a visa
does and that is to make changes to the
way that they behave experimentally some
of those changes might turn out bad but
if one of those changes turns out to be
really good then that change can be
shared with all the other systems on
that platform and I think that's a
really cool idea and finally I think
it's interesting to look at how we can
use a model of behavior built in one
system and we can transfer the
information over to another system so
what can we learn about windows by
looking at Linux for example are there
things at some level of abstraction that
will transfer usefully between those
systems and I think that there are it's
going to require changes to the system
we're going to need new primitives for
introspecting into the behavior of the
system things related to concurrency
like coherence sequences of events
potentially from different threads
exposing that up in an efficient way to
runtime layers or to the developer is
going to be a challenge and energy which
is a problem on everyone's mind
especially in the mobile space I want to
look at new mechanisms for failure of
Wooden's you'll notice that there was no
hardware support in a visa but a visa
one of the challenges that it has to
overcome is the overhead of enforcing
those constraints and I think with
hardware support we could do a better
job of that so I think changing lower
levels of the system whether actually in
hardware or not is an interesting thing
to look at when dealing with failure of
woods and also looking not just that
concurrent programs but it's sequential
programs as well I also think another
way to deal with this problem
the problem that programming is so
difficult is to just do the programming
for the programmer so looking at
synthesis techniques I'm working on a
project with some natural language
processing researchers right now where
we mined a bunch of code from the
internet and we're looking at ways of
incorporating that into an active
learning based code synthesis engine the
last idea I want to talk about is that
power failures impact reliability if you
have a platform that experiences power
failures often that's a reliability
issue so energy efficiency is a form of
being reliable I'm especially interested
in this area in the domain of small and
unpowered devices intel has a little
device called wisp and this was
developed in collaboration with several
people from from academia and it's a
very interesting device because it
doesn't have a battery the way that it
powers itself is by harvesting ambient
radio frequency energy charging a super
capacitor and as the super capacitor
discharges it does a little bit of
computation that's a really interesting
platform because it requires
interruption tolerance during the
execution power failures go from being
the once in a while event where someone
kicks the plug out of the wall to being
maybe ten or a hundred times every
second depending on the size of the
capacitor and the rate of the
computation that fundamentally changes
the way that you design what is an
operating system how do you program
devices like this maybe we want to treat
power failures as recoverable exceptions
what would be the system layers that be
required to do that so I think this is
really an interesting problem to look at
especially as these devices find use and
more ubiquitous computing applications
and I also think that looking at ways of
avoiding power failures by for example
trading off a failure due to trading off
energy-related failures and program
reliability mechanisms a programmer
lighting the reliability mechanism is
like a null check if you can elide a
null check to save enough energy to keep
the system alive you might want to do
that but you only want to decide to do
that when it's really important so you'd
have to have some introspection on how
much energy energy remains and how you
can make that trade-off dynamically is
an interesting interesting question I'm
going to skip this last bullet and just
point out that there's lots of cool
applications for this stuff with people
working in health and environmental
sciences especially in the northwest we
have lots of forestry and water resource
research and there's lots of interesting
health applications that would be
relevant to a company like Microsoft
especially working these small devices
and how they're programmed and things
like that so I think there's a lot of
really interesting and visible
opportunities for collaboration and
applications there okay so that's my
talk there's a big list of collaborators
that I've worked with over the years at
u-dub as well as several people from
Microsoft Research and HP Labs and IBM
and I really appreciate you giving me
your attention and asking so many
questions and I'll take more questions
on the last five minutes if there are
any more questions cool thank you thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>