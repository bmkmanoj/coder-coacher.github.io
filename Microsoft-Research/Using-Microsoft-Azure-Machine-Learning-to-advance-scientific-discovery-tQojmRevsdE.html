<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Microsoft Azure Machine Learning to advance scientific discovery | Coder Coacher - Coaching Coders</title><meta content="Using Microsoft Azure Machine Learning to advance scientific discovery - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using Microsoft Azure Machine Learning to advance scientific discovery</b></h2><h5 class="post__date">2014-07-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tQojmRevsdE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to today's webcast using
Microsoft Azure machine learning to
advance scientific discovery I'd like to
turn today's event over to your
presenter Raj
Roger Barger group program manager for
cloud machine learning group microsoft
corporation roger you now have the floor
great Thank You Kim
and thank you everyone for joining on
the presentation this morning last week
we had um two exciting announcements the
first was that we announced the public
preview for Azure machine learning which
is a fully hosted cloud service for
machine learning and data science on on
the Windows Azure cloud this is
something our team has been working on
now for the past two years working with
researchers across Microsoft Research to
pull together some of the best machine
learning algorithms but also looking at
what are the friction points that people
encounter in using machine learning and
more importantly how can they deploy
their solutions into production as fast
as possible now I'll be telling you more
about the service today another exciting
announcement we had last week is that we
announced the azure ml for research
program that's being being managed by
Microsoft Research and this is this is
very cool because what this provides is
there's two types of awards that one can
apply for and get from from Microsoft
Research one is a data science
instructional award wherein an
individual student will actually get an
account on Azure ml and along with 500
gigabytes of cloud data storage for each
students which a university can use to
actually teach an entire data science
program or apply machine learning
program and again this program is
available to academic and nonprofit
institutions and there's a selection
process that goes on to actually choose
who gets admitted into the program and a
second offer offering within this
program is actually for large research
collaborations in which will offer a
shared workspace where a research group
can stick up can actually store a large
volume of data up to 10 terabytes to
enable a group of researchers to work
together to try to actually analyze the
data gain insights together and because
of as at because Azure ml is
collaborative the data scientists the
research
working against this data can be
anywhere around the world and I can
assure the models and put a put amount
predictive model into production as a
machine learning web service for a
community to offer now the first
deadline for proposals for this program
is September 15th the next deadline will
be November 15th and every two months
after they'll be reviewing proposals you
can actually apply online at the link
that's provided at the bottom of this
page so again two exciting announcements
next week now we have a diverse audience
listening in today and so before instead
of just diving in and talking to you
about and showing Azure ml which I will
later actually wanted it to just step
back a little bit and say what's going
on to machine learning why are we so
excited about machine learning why is
the industry getting so excited about
machine learning what is machine
learning and how should we should be
thinking about in its potential impact
so I wanted to spend just the first 10
or 15 minutes just sharing a perspective
just an overview if you will on what
we've seen the power of machine learning
then I'll turn the conversation over
into why did we build a sure ml why did
we choose the features that we've chosen
for this first dumb version of Azure ml
what's motivated is what we're hearing
from users potential users it actually
influenced what we built then I'll spend
about 15 minutes in a demonstration of
Azure ml so you can understand what the
heck this is and then we'll close and
open up hum for broad questions now if
at any point during the presentation you
feel that there's a clarifying question
that needs to be addressed I would
encourage you to go ahead and actually
send it into the moderator Kim will
actually then surface it to me it will
help you know in the talk itself
otherwise we'll leave plenty of time at
the end for for question and answer so
machine learning simply put they're
computing systems that improve with
experience and experience comes in the
form of data or training sets now in a
world of big data one of the first
things that's really changed is that our
customers are our researchers are able
to capture enormous volumes of data
about a process about an individual that
they actually want to analyze
so having ubiquitous data and relatively
cheap compute it's actually really made
this possible for machine learning to
actually be applied the the potential
for machine learning we actually view it
as being one of the most simple
tactful technologies that's going to
shape IT shape applications and services
of any other technology out there over
the next decade we actually believe that
they were largely unappreciated and its
impact is actually going to be profound
over the next decade and this hasn't
gone unnoticed I mean for many years
bill has been saying this quote he was
noted for if if you invent a
breakthrough an artificial intelligence
so machine can learn that would be
easily worth 10 Microsoft's and even
more recently such an adela here's our
current CEO made the observation that
over the next decade computing will
become even more ubiquitous an
intelligent will become ambient and
that's actually very powerful it's we
have so much data floating around us we
have devices around us all of this
information imagine being able to
analyze that and make sense of it and at
any point in time our devices or
applications are gonna basically be
powered by machine learning to actually
help us to make smarter decisions and in
in a world of connected devices
connected cars wearables we have this
opportunity to basically be surrounded
not only by data but then augmented
intelligence services so it has again it
has great breadth and depth of its
applications and we're just starting to
see the the beginnings of it and it
isn't it helps to go back to the to the
beginning what was one of the most first
successful applications of machine
learning and how did it change the game
and one of the first successful
applications was handwriting recognition
now in post offices they were early days
in the in the late 80s early 90s
basically to actually figure out where a
letter should be routed there were
systems optical character recognition
systems that were written they were
largely rule-based imperative
declarative code that basically would
actually analyze look for the letters
and actually run through a series of
rules now these systems were incredibly
challenged by different letter formats
different handwriting styles and they
fell over regularly and required a lot
of a large amount of maintenance to the
point where I just became untenable next
the US Post Office actually commissioned
a large research home grant funding
called for proposals to actually try to
find new ways to actually process it and
the actual way that that
real um and who was successfully
implemented was to treat this as a
machine learning problem we talked
earlier that machine learning requires
data requires training sets to gain
experience so how it was handled in this
case is it individual letters from
different he had writing styles were
captured and labeled with the correct
output and in fact lots of training
samples in some cases they use computer
simulation to add noise rotation and
look at many examples and then labeled
that data fed all that in into a machine
learning system and what you end up with
is a relatively accurate digit
classifier but something very unique
about machine learning they never they
never ship they're never done they make
a mistake that's no problem you go in
and actually let correctly label put
that back in your trainer training set
and you retrain and so this is actually
resulted in what the state-of-the-art
systems are today for optical character
recognition understanding documents and
document understanding is treating it as
a machine learning problem and letting
the data and a machine learning
algorithm actually build the application
no handwritten code it's actually
learning from the data with machine
learning and of course there's
statistics underneath of the covers
we're not going to get into that in the
talk but basically it it changed how
what was a previously challenging
problem got attacked the fact that a
modern implementation that you can
actually go to the azure data
marketplace and download the Bing
language translator and when you happen
to be traveling around of parts of the
world you can actually point your phone
at a menu in any language take a
photograph tell it what language you
want it to translate it to and it will
translate it into the end of that
language
why because behind the scenes we've used
machine learning from art from our
Microsoft research technology stack to
actually build a language translation
system recognition and translation
system so why do you why do we do
learning well you learn it when you
can't code it recognizing speech
recognizing images written handwriting
gestures you learn it when you can't
scale it recommendation systems at the
scale of Netflix or Amazon this is not
something you could write a
recommendation system for by hand but
you have sufficient data on people's
behaviors and what they do that you can
in fact apply machine learning and build
a highly accurate recommendation
similar with fraud and similar with spam
all hard problems that have all been
successfully attacked through machine
learning and you learn it when you have
to adapt or personalize it predictive
typing on on a small handheld device is
now being attacked by machine learning
actually learning my voice and actually
building a highly accurate
speech-to-text translator just for me in
the way that I talk so this is something
you could not write have a developer
write a custom app for every one of your
individuals but you can in fact learn
from individuals data and you learn it
when you can't track it
I am 'uncle and there's a myriad of
applications where you have the data
it's an intractable problem to actually
address by hand use the data and machine
learning to actually build a solution
and hey if it doesn't work the first
time if it's not highly accurate that's
fine because you're ever essentially
building a training set for the next
generation of the application now let's
look a little bit at what's been going
on over the past years and talk about
what we can do today you know in the
1980s when I was highly active in it it
was the connectionist or the neural
network period of machine learning and
we're really looking at how to actually
evolve neural networks we did Multan
that true proved to be a very tricky
algorithm and a very tricky technology
to get right with a lot of hand tuning
and then the 90s we started moving into
the symbolic era of machine learning
decision trees were actually the
dominant method that what was called
cart classification and regression trees
seed off 4.5 was the classic algorithm
and there were expert systems and rules
as well two started a surface at that
point in mm we actually saw support
vector machines and the rise of
so-called kernel machines to actually
allow us to tackle even more difficult
problems in higher dimensions we saw the
real rise of statistical learning theory
in actual scoring systems this is what
we saw machine learning was being put
into production by fair Isaacson FICO
scoring and risk scoring where
predictive models were put into
operations and businesses started to run
at scale over these over these models
that were making predictions and in 2005
we saw the rise of graphical models
where we're not just learning for the
data we're actually trying to
statistically model the data was an
interesting fork and it's that that
exists today in Fernet from Microsoft
Research Cambridge is a fine example of
that where it's actually understanding
the statistics of the data modeling that
in its dependencies and yes fine-tuning
as you understand the distributions of
data but not just trip not just allowing
a machine learning algorithm to learn
from the data you actually model it
effectively and more recently and what's
really been exciting right lately is Big
Data and deep neural networks I'm not
gonna cover it in the talk today we will
have support for deep neural networks in
fact we do and are current and release
for Azure ml but its implications for
understanding truly understanding what's
in an image truly understanding language
translation and actually being able to
learn multiple languages at the same
time is profound
all because of basically the work that
started back in the 80s which has
continued a neural network theory to the
point we can train arbitrarily deep
neural networks but the real turning
point here has been the fact that we
have large-scale compute in the cloud in
particular or GP GPUs for deep neural
networks for example we have a tons of
data and we can now actually apply this
data even the simplest of algorithms
when given large quantities of good data
can actually become highly effective
machine learners let's just look at a
couple examples here within Microsoft
before we turn our attention to to Azure
ml the Xbox Kinect was a it was a you
know clearly a commercial success for
Microsoft but it was also an incredible
technological success if you look at and
have played with the Kinect you realize
that it it watches your gestures it
figures out what you're doing and if you
think about the diversity of challenges
that had to be faced there you have
individuals in different postures that
you have to be able to recognize you
have individuals of different body
shapes and sizes different ages of
individuals so the whole the
relationship the whole ratio is
completely scaled individuals will have
have accessories attached to them and
you have to be able to figure out still
is that their arm or is that a purse you
have to be able to pull foreground out
of the background you don't have
to clear your living room you can have
plants in your living room couches in
your living room lights yet that Kinect
sensor can still actually figure out
where the individual is in the room and
what to ignore and in fact when you can
do multiplayer you have multiple people
in the room and guess what it actually
tracks each individual independently and
is able to understand what they're doing
this is clearly one of those problems
that could not have been tackled without
Big Data and in fact how we kept how we
built it again it's a machine learning
system so we're gonna have to build a
training set we had individuals wear
these sensor suits in which we could
actually record their gestures label
what they were doing
building a label training set to feed
into a machine learning system and this
was no small task actually we had 800
million training examples from
individuals and that was kind of
expensive to do to spend that time with
that individual in that suit and capture
that data so we actually and we needed
to to get the performance actually
simulated using both machine learning
and simulations to add variations add
noise do rotations of these individuals
to create another 800 million training
samples by the time it was done we had
roughly 2 billion training samples that
we had to fit in this is the era of big
data and big compute meeting machine
learning and that was fed into a machine
learning system so the Kinect sensor
that you see today and they're
interacting with when you pay Xbox is
basically being driven by machine
learning systems the same machine
learning algorithms are actually in
Azure ml to allow you to tackle equally
hard problems and there's a couple other
cases that you may not be aware of that
we've actually applied large-scale
machine learning and search we've
actually treated search as as a complete
machine learning problem and in fact all
the results that you see no less than a
dozen machine learning algorithms have
shaped influenced what you see on the
screen everything from what links are
most likely to get clicked and will
actually put down those closer to the
top with something misspelled did you
really mean to type something else in in
which case we'll actually go ahead and
correct the spelling and do a search on
what we think is the correct spelling
also put a message up there saying did
we get this right you can see or if you
should be connecting the dots we're
actually gonna build a training set from
all this if we got it right what was at
what language were you speaking in what
was your intent and we've been watching
we watch the series of searches we look
at the actual words that were used in
the query to say are they trying to find
information are you trying to make a
purchase are you just serendipitously
surfing and based on that bring up
related searches which we feel are
contextual
based on the on your search the words
that you've used behind the scenes
before we serve up a page we've actually
run an ml algorithm to see if the page
is malicious and filtered it out so you
only see what we believe to be safe
pages and we're also we're serving up
ads and we're asking the question what's
the probability of each click on each ad
and that often deals with what your
context is what you're doing aren't
what's your intent are you trying to buy
are you just again looking for more
information what ads to show and in what
order and afterwards at the end of the
day what pages should we re index to
actually improve the performance and
your clicks and your choices that you
make in the search and how far down you
have to go to make that first click all
become information for us to actually
retrain and recalibrate our models every
night and really machine learning
enables just about every value
proposition out of web search and in
fact the more people search and the more
people use the better the system gets it
learns over time here's another problem
which actually some might find
interesting and applicable in their
domain is a really challenging problem
you know we in our data centers this is
what you're looking at our logs coming
off of our office 365 hosted exchange
from our Dublin data center now our data
center operators are actually you know
very smart individuals are looking at
dashboards that are monitoring their
servers the services the networking the
temperature and the services themselves
are giving us feedback and they attacked
it in a traditional manner they wrote a
series of rules handwritten rules and
code that would monitor look for KPIs
and spikes but you know darn it it never
really helped them with finding the root
and so after several months of not being
able to predict failures not being able
to rapidly diagnose a failure recited a
treated as a machine learning problem
and actually captured all of the sensor
logs that were coming off the the
machines the network cards the services
themselves other ambient information
within the data center and started
capturing them as as logged time series
logged and when an event happened we
just simply had the operator press a
button saying this is the type of nth a
disk drive failed the whole rack went
down or had a reboot the serve service
itself and let machine learning actually
analyze the data and figure out what the
root cause was so hundreds of thousands
of machines hundreds of metrics and
signals per machine and really allowed
machine learning to actually find out
what correlated with the real problem
because again just because it happened
once doesn't mean it was actually
strongly correlated we had logs from
months and also how can we basically
extract effective repair could we do
predictive models that says this disk
drive is going to fail and the next week
you might as well replace it now at or
or this server is going down you might
want to move load off of it to it to
other servers and so now our monitoring
is to a large part shouldered by large
scale machine learning so hopefully
those of you who are thinking about how
you might actually apply this technology
I have got a little bit of a better
sense to the kind of applications to
which you can apply it to sensor data
actually user interactions services even
actually predicting what individuals are
doing off of off of images so if we
think about machine learning it really
allows us to solve extremely hard
problems better it allows us to extract
value from big data and drives a shift
in data analytics and there's actually
kind of something a little bit more
subtle going on here it's really about
changing your way of thinking about a
problem it's about getting the data
getting label data getting good data and
actually trying to let an algorithm
build the solution for you and again it
doesn't have to be perfect but as long
as you have a feedback loop you have a
chance to actually iteratively improve
the performance of an ml solution over
time which is really compelling
so a large number of scenarios just
we've got and had exploratory projects
in almost all of these all the way from
predicting who is going to basically
return to a hospital after being the red
readmission rate doing telemetry data
and analytics for connected cars
predictive maintenance and manufacturing
churn analysis life sciences researches
research targeted advertising smart
meter monitoring maybe we've had a
number over the past year with with
customers and and researchers we've
given access to of these applications of
applying machine learning there's
there's simply no limit to the kind of
applications to which this can be
applied all you need to have is data the
ability to label that data or give
feedback and access to machine learning
at scale to actually start making sense
out of it so this is where Microsoft
Azure machine learning comes in and
we've made some intentional choices
about the functionality we're releasing
in this first version so I kind of want
to walk you through that and why we made
the choices that we made then I'll
actually give you a demonstration we'll
spend a little bit of time wallowing
around with the service since you have a
really clear idea of how you could use
it on what it can do for you
the first was you know we could have
focused machine learning on a lot of
things image analytics speech analytics
Winstead said look where we see people
getting the most value is the ability to
develop and deploy predictive models and
not just build them and keep them on
your workbench but actually put them
into production as a machine learning
web service with a REST API and we'll do
it together today in a quick demo we'll
actually grab some data set we'll train
it we'll evaluate it and we'll actually
publish it as a web service within
minutes nobody else can do that out
there today and the reason we did that
again is just the breadth of
applications in another issue we wanted
to address is that the two impediments
to why people aren't embracing
predictive analytics is as much as they
are today the first there's actually a
handful of reasons but we're not one of
the main ones is the time it takes to
put into production so I talked about
you know that we could put a model in
production in a few minutes out of the
real world it can take three months to a
year to put a model into production
because it's called a code porting
exercise you build it in our studio and
then or
the other favorite tool and put in
production you have to roll up your
sleeves and start sitting down with a
developer to write code our target user
is a data scientist and specifically
what we're calling you know emerging
data scientists this this tool does not
require a PhD to use we really wanted to
make a tool we're seeing a new
generation of data scientists coming out
of certificate programs self-study
they're moving over from their bi
profession or their technical computing
profession and getting interested in
exploring data and we wanted to make a
tool for these individuals to be highly
productive and build models that are
just as good as somebody who may have
been doing this for the past decade and
using SAS and again we're optimizing for
the fastest time to a deployed solution
and something else which was inspired a
lot by our work with with researchers in
the sciences early and understanding
that businesses was what's moving that
way is its support for collaboration
we'll talk about it in the demonstration
and we'll talk about it a little bit
more but we support collaboration and
sharing it is meant to be an experiment
platform you can share data with
researchers anywhere around the world
you can share your experiments with
members of your group or other research
groups you could even share the finished
web services that you build and maybe
make calls to them to augment your web
service so again support for
collaboration and sharing was key to
this something we heard from researchers
but also from business users one of the
things that we heard when talking to
people access to high-quality ML
algorithms is hard
it's either expensive but the reality is
one package might have very good
decision trees another ml package has
great neural network support another
one's got some pretty darn good support
vector machines and you need them all
for various applications but having to
learn four or five packages it's it's it
was a mess and the affinity group was
about on the order of a dozen packages
you had to have access to and it's not
just ml if we're trying to actually
we're doing data science we're reading
raw data in or cleaning the data we're
selecting features which have the most
information we're trying out different
ML algorithms and running experiments
we're going to evaluate which one is
best so you really need to have
end-to-end support you
have to use multiple tools and then
finally that ability to put a model into
production which I've mentioned already
some of our guiding principles was there
should be no software to install them if
you have a Chrome browser running on
Linux or if you happen to have I II or
two other browsers will be testing over
time you're good to go there's no
software to install it's a fully managed
cloud service accessible through a
browser so nothing that basically
prevents you from getting on and doing
data science it is collaborative will
actually show you today how you could
actually invite somebody to join this
workspace which is well I'll just talk
about what a workspace is in the demo
and we've tried to minimize coding to
the extent possible you'll see that
we've got modules which have actually
there behind that box which we call a
module there's actually hundreds maybe
thousands of lines of code implementing
an algorithm to you the user the data
scientists they don't have to think
about that they simply say I need to
read this data source and they drag a
module onto the pallet that actually
reads a data source up I've got a clean
missing values well you drag a module on
that cleans missing values and connect
your data set to the missing values
replacement set the parameters on how
you want missing values replaced when it
comes time to create a training set you
drag a module on that does that you tell
it whether to stratify it or how to
split it between training and testing
set so you're really thinking about your
data and your experiment you're trying
to run you're not thinking about writing
code something else which we're not
going to get into today is that those
modules you don't know one of them could
be a Hadoop job one of them could
actually be reading and running its
execution on sequel one of them is
actually running an r package on a pool
of our servers that we have behind the
scenes so it's something else we've
factored out you don't need to learn all
these different systems again you're
just thinking about what you want to get
done we are running an orchestration and
coordination engine behind the scenes
that moves the data transforms it from
an our data table for the ARP our
services to AR our internal data format
or maybe back over to HDFS and it's
extensible so we have support today that
you can write arbitrary our code we will
have support for Python there's over 400
roughly today 400 are packages that
we've uploaded and we're going to allow
you the ability
to upload our packages I'm not in the
current release madness in an update
coming soon so that basically if you
happen to find an R package that looks
very compelling to you with a few clicks
you could actually add it to this your
workspace your private workspace that
you and your team have access to we also
have an SDK coming out later this year
which in fact you might say hey I'm
gonna actually wrap some of my own code
I've written I've written a simulation
engine or I've written a rules engine or
optimizer you'll be able to create your
own modules with the SDK and then
finally what you'll hopefully notice in
the demonstration it's an experiment
platform data science and machine
learning is really about running
experiments there's no rules on what ml
algorithm is best so we want you to
experiment an experiment fast and be
able to do side-by-side comparisons roll
back to what you're working on two days
ago and stand fork off and start from
there so basically you can try things
out but once a model has been built and
put into production it becomes immutable
so if your research team has built a
model for making a prediction about
whether about earthquake whatever and
it's that the team has decided that's
the model that they want to run on it
becomes immutable but it's not invisible
people can actually search for it they
can discover it and they have the
ability to clone it and create a copy of
it and start from there so that that
allows you basically to protect your
valuable models but also allow other
people to actually reuse them and then
again we keep hitting this on because
it's the key differentiator that nobody
can do today quickly deploy it as an
azure service to our ml api service that
will host it might make sense to say a
couple more words just a few words about
the api service because this is pretty
powerful before running the demo
increasingly we're moving into a
services world you know sometimes we
think about big monolithic services and
in the future apps are going to be made
out of hundreds maybe thousands of micro
services and so this ability to say I've
just made a predictive model that is
going to predict whether this was spam I
made a predictive model to figure out
the intent of the user and so when
creating a web page for example our web
pages the result making calls out to
these dozen you know ml services makes
perfect sense and actually shape or
influence
final result so and having an API
service that you can call and say what
models do you have in your repository
what's their API and then you can make a
rest call too it allows for incredible
agile composition of these web services
but when you put a web service or your
model into production on our API service
actually we don't spin up a VM and start
charging or you know just have it
setting their idle instead we
intelligently keep it indexed and hot if
a request comes in we pull it into a hot
VM pool and start serving up requests
for that that model when there's no more
incoming requests week we ask the model
or if somebody that maybe the whole
research community is pounding away on a
model we can actually start scaling it
up across multiple VMs to satisfy the
SLA ur4 that's been set and we have two
modes that we support today so when you
put a model in your production you
actually get two instances of your
service the first is a request response
which means you send a record and you
get a record back that with the scored
result so is this transaction fraudulent
is this patient going to readmit or
batch mode we could send in an entire
file saying hey here's a hundred
patients that checked out last week
please give me a list of which ones are
high risks that I should actually follow
up on what you get back as a file of
scored results and there's some
monitoring and telemetry we provide to
the person who's hosting these models so
they can see which ones are getting used
by who by whom and how much data is
going through them so there's there's
capabilities there as well so I'll be
spending most of my time talking about
mo Studio or showing UML studio but
there's that second back-end model
management service which is really
powerful so with that I'm going to stop
and go over and actually run a demo Kym
sometimes when I when I change from
PowerPoint over to browser sharing stop
so please let me know if sharing is
still going on okay I'll let you know
thank you so really what at what I just
did is I just opened a browser I just
logged into my workspace and I could
have opened it with Chrome as well and
I'm in what's called my workspace which
I'd mentioned to you earlier
usually on first-time users what you
landed is basically hey welcome to ml
Studio preview which has a forum that
you can ask questions we respond other
users in the in the community can reform
can respond as well we have all of our
online documentation for how to use the
service updates of what we've - what has
changed and samples both experiments
that we've built a gallery of sample
experiments with sample data sets to get
you started videos they'll actually step
you through how to use it but really
when I do my predictive modeling work
what I land into this is Michael my
workspace and what I see here are
samples that Microsoft is ship to help
you get started everything from doing a
timeseriesforecasting with a Rhema which
are all our packages
well back to how to use different
functions such as how to read a data set
how to do cross validation let me just
expand this out these are all
out-of-the-box to help you get started
using regression binary classification
multi-class classification some
uc-irvine reference data sets that we've
built models for for breast cancer
detection flight dictated german credit
these are all either from UC irvine or
former kdd competitions down to
basically how to get make best use or
best practices in using our our service
sentiment classification for example to
allow you to do sentiment analysis and
you can see you can actually build
arbitrarily complex tags all these are
meant these are documented what we've
done each step so you can actually read
how we've used the service to help you
get started and I can also see I've
obviously not been busy because I click
relate recent work that says Roger you
haven't done anything recently I can
actually go and see the experiments that
I am working on right now these are
models that I've built over over the
months but I'm not limited to just my
own workspace I can actually say hey I
want to go join one of my colleagues
Mona can go work with her on an
experiment she's given me access to
her workspace and then go work with her
on a problem or I can go work over with
my colleague Manish so I can bounce
around between different workspaces and
literally change the projects I'm
working on so for example for the
academic program an entire class can
have a workspace or they can have their
own workspace and students can work
together or researchers can work
together let me go back to my workspace
and I may decide that this is our videos
here I may decide that I need some help
on a project and so I can say simply go
over and say I want to invite somebody
to join me this is where I would type in
their live ID which is her email address
I would basically set them as an owner
which means they could delete and do
whatever they wanted to in the workspace
or just a user which means I want them
just to work on models with me hit Send
they get an invite they open up their
browser hit the link and they can
immediately join me here in this
workspace and we can work together let's
actually let's actually take a little
bit of a tour first stop though let me
say new data set somebody could have
given me a data set from my local file
and you can see here that we can upload
to the cloud from from ml studio from
the desktop CSVs tab-separated files
text files a zip file and our data
object an ARFF file format that's how
one way of getting data in now this is
the authoring experience how I said
let's create a new experiment we're
gonna create a new model together what
you notice is a few things have changed
here this is the authoring palette which
is kind of a hint - hey drag stuff on
here to get started but you'll also
notice over here on the Left I have
drawers available to me of functions
these are models that I've trained and
put into production these are data sets
that I've uploaded data input and output
a generic reader I pop this guy on open
up the dialogue box over here I can read
from blob storage as your table storage
I can read from a sequel database on the
cloud and HTTP sites such as the date UC
Irvine or maybe you set up an HTTP
server to
serve up data run a hive query over
Hadoop run a power query all from this
module you see as I make these choices
the dialog box changes it this is the
necessary information this module needs
to run that's the extent to which we
have to do coding let me just take a
data set in fact well let's do one other
thing let's just get started with
something there's actually a very simple
demo I like to run and it's it's this
bike buyer I've got two years of
customer data of people who have been
buying product from me and in fact if I
just click visualize regenerate html5 so
we can visualize it on any browser there
are 10,000 rows in this dataset 13
columns
there's bike buyer whether you've bought
a bike from me or not I click it I can
actually see the frequency go ahead and
close that that's interesting oops for
me to close that completely but from
this pop-up I can actually get all the
metadata that I need marital status it's
pretty pretty good mix of single and
married genders balance I can see income
is is not children well that's
interesting it's like most people don't
have children in my data set different
education levels if I scroll down also
do I have any missing values I have to
deal with so I get a lot of lot of
information from this visualization at
any point in the end in the work flow we
have this information available I just
have to click the pin and it will
actually show me what's inside my data
set so I don't have to deal with missing
values but if I did everything
searchable over here not only is it in a
drawer but if I just type in missing
well there's my missing values scrubber
module go ahead delete that I don't need
that in the interest of time let's just
go ahead and create a training set and
let's just split it and we're gonna send
the data set down to the splitting
routine open up the dialog box just a
couple quick things to note I'm gonna
use 70% for training but we also have
support for stratification those an
applied machine learning will know what
this is I mean if I'm trying to predict
in the future I would use a timestamp
column and stratify it only 70% of the
data that was actually behind some
timestamp would be used for training the
other 30% for prediction so we can
stratify just with the click of a few
buttons we will we don't need to
stratify this
this point let's go ahead just click run
now this is the first time I've really
done any execution on the cloud I just
sent the dag up to our service to a data
center it's in our south-central us it's
done I open this and save visualize and
sure enough seven thousand rows the
distributions look look good nothing's
really changed that's my training set so
at this point let's just go ahead and
train we have a generic trainer that can
train any ml algorithm needs to get
repeat the training set I'm gonna need
to score which is basically I got a test
the fish the the the performance of the
model on data it's never seen this
holdout data which is a great proxy for
how it's going to work in the real world
and because I like to look at things and
run experiments I'm going to need to
have something to evaluate different
experiments and we have that as well
this evaluate model
let me just shrink this down a little
bit it's getting a little cumbersome and
close that so at this point it's like
okay well well what ml algorithm we're
gonna use let's go ahead and close this
up and just give you a sense of what we
have available to us again we can do
filtering cleaning transformation
everything that you need for doing data
science into end is here open machine
learning close these drawers that I had
opened we're doing a classification are
you gonna buy my bike or not and so you
can see we have a large number and
growing daily of classification
algorithms so I'll go ahead and I'll try
you know I'm gonna try a to class
logistic regression I don't wanna do
regression I'm doing classification will
actually say we're gonna go ahead and do
to class average perceptron simple
neural network that's the model we're
gonna train now at this point we've got
a red bang here that says hey I need
some help this guy needs to know what
column we're trying to predict well
we're trying to predict whether you're a
bike buyer or not that's it everybody I
mean - the narrative modo taking me all
of a minute to author a machine learning
experiment I click run this dag is being
sent up to Asher for execution and I'll
start giving me some progress
Barr's so it's split was already done
we're loading the perceptron modeling
and configuring it we're now training it
till it until it converges on on the
best-trained model it can with that
training data its trained now we're
scoring it on data it's never seen
before
and we're done we've actually done the
evaluation as well now of all the
metadata here for this experiment let's
see how it did and what I'm looking at
here you're pulling out a lot of data
information of a feeling a lot of people
are heading the service this morning cuz
it's running a little slow Roger well
that is loading we do have a question
yes please it says what is the
technology to capture the data how would
you capture the data for an IT
environment let's say for an IT
environment where we want to plan for
capacity based on CPU memory disk i/o
etc or predict issues okay so we've
we've done an application similar to
that and so what we what we did is that
they put in the addition while this is
running extremely long we did we did um
interception at the devices itself
basically intercepting and writing to
local file systems for the local
machines that data was then moved over
to Hadoop cluster where we ran hive
queries from Azure ml I'm gonna actually
stop this so basically the bottom line
is is you but you put logging intercept
and logging data close to the service
that you're trying to do predictions on
move those into files then we can run
hive queries over them to actually
aggregate and create the necessary
information so that is just not
rendering there we go
yeah so it didn't one of them failed one
of them succeeded
sorry about that folks that's a cranky
model right now cranky visualization I
think I wedged it when I quote I clicked
it let's go back over to my recent oh
don't tell me I lost my experiment I
didn't so if the answer the question
again we've actually done local
interception written them to local files
move those local files up to Hadoop ran
a hive query over them and then from
Azure ml and pulled that that hive the
result of that hive query in and that's
where we started our model to build
predictive modeling so let's let's see
if this is gonna be a little bit it
should be much snapper there where
that's the way it should work I'm gonna
set a Heisenberg so what we're seeing
here folks before let's just do a quick
experiment this is an ROC curve for
those of you more familiar with it you
know that that basically it's the it's
the true positive versus the false
positive rate for that holdout set so a
perfect model would have had all true
positives no false positives a model
that was flipping a coin would have ran
the diagonal you can see we've done a
little bit better than that
and in fact if you see if you scroll
down a little bit you can actually see
the confusion matrix you can see a
number of performance scores but the
short answer this model isn't all that
good it's better than guessing let's see
if we can do better and what again we're
gonna try to push this experiment
paradigm a little bit I'm gonna train
another model really quick and I know
I'm gonna need to train its score so I'm
gonna clone those those modules I don't
need I'm not gonna train another
perceptron but I am gonna train a
boosted decision tree I'll feed the
booster decision tree into the trainer
I'll train I'll feed the performance of
the model to the same evaluate model to
do a side-by-side I'll click run again
only the work that you have changed gets
re executed everything else stays the
same and again we've done lineage and
provenance tracking so that all the data
from the previous run is retained as
well so while this guy is training
scoring it's already done and I gave it
kind of a let me just let it let it
finish and then show you something so
right now it's doing this side by side a
value
which is generating a lot of html5 it's
done this one I wanted us click the
boost decision tree if you looked at it
we've just trained a hundred trees in
this ensemble and that for those of you
who are understanding decision trees
each one of these is a machine learning
algorithm we've trained a hundred of
them it together to actually help us
make a decision so that's an ensemble of
machine learning algorithms in reality
we hope it does a little bit better
let's see
click visualize and sure enough
significantly better the Roc performance
is much higher is click it in fact a
good proxy for how accurate a model is
our previous perceptron an area under
the curve which is just literally the
the area under this curve is 68% so it's
a 68 percent maybe 69 percent accurate
algorithm this one on the other hand 86
percent almost 87 percent we clearly
have a winner here what if we wanted to
put this in production we wanted to
actually run our business on this so let
me show you how fast we can do this
let's try to clear some room out here
come on Mouse is being finicky today but
that's okay so I saved this trained
model so I'm gonna save the machine
learning algorithm model that I just
trained it's saving it new experiment
that train model back on the pond to the
pallet there it is we overload the use
of our scoring module to be a harness
for building a web service now web
service has to know what what data oh no
don't do that what data with the data
incoming data looks like so I say look
for the web service this is what data
incoming data is going to look like for
the web service this is what the output
should look like now folks we're really
building an arbitrarily complex web
service here if I had modules that might
write to a database run a rules engine I
could have dragged
made that part of the scoring model I'm
about to put into production as a web
service I'm just doing something very
basic today but literally you can build
an arbitrarily complex mission web
service that uses machine learning and
publish it I click run so what's going
on behind the scenes is that it is
looking at the data building and a REST
API it's done
I say publish would you like to publish
the service yes it's now talking to
Asher and we have just published a web
service not just one web service but two
web services to Azure a request/response
web service a batch web service I could
actually test the model running on Azure
right now just enter the metadata here
and say well this person buy a bike or
if one of you said yeah I'll tell you my
marital status my gender my income my
children my education would I buy a bike
or not and we could test our predictive
model right here in it's in the actual
environment which it's gonna run in
production another cool thing is if you
click this and there is the URL the Oh
data endpoint for this model here is the
URI for it scroll down a little bit
further if you wanted to write some code
in Python well we've given you all of
the rest headers associated with that
for what what the request body should
look like what a sample request should
look like scroll down a little bit
further the response coach or models
gonna give you going a little faster if
I wanted to show you this we even
generate the c-sharp code to call this
model so you could copy paste into your
IDE compile and you're good to go we
generated in Python as well we generated
it in R as well so the time not only we
put the model in production but we get
you give you code to help you get it up
and running and if I go back here and I
say okay let's do configuration I like
my service put it into production now
the rest of the world can actually see
this model and start making calls to it
that's the speed at which we can
basically go from experimentation with
different ml algorithms writing a model
putting it into production as a web
service and
people can immediately start calling it
so I'm going to stop the demo there and
just kind of go back and just say a few
closing words before we open for
questions I mean we're doing this on the
cloud because we see a world where
devices vehicles and other other devices
will be sending data to the cloud for
for learning over but also trying to
make informed decisions there's no
reason why why your vehicles can't tell
you in advance when they're going to
fail we have customers right now
monitoring fleets of vehicles we have
university groups who are doing
predictive models for when when a car is
going to fail or when it needs energy
for electric vehicle so a lot of
applications there and and it is the
pull back a little bit and what's really
making this happen I mean it really is
the power of the cloud to support
collaboration of crowdsourcing and
collaboration the ability to have
large-scale machine learning and
analytics because you need it when you
when you're learning but when you're
done the machine learning models so
small so fast it could be integrated
into small devices so it really is the
cloud where we think everything comes
together scalable machine learning cheap
storage of data all the compute you need
when you need it for machine learning so
just a couple closing comments again I
wanted to remind you about the azure for
machine learning award program the
Microsoft Research is is hosting or is
is operating for academics and nonprofit
institutions the first call for
proposals will be for September 15th
the next round for November 15th and
they've also got this as part of a
broader Asscher Microsoft Azure for
research program as well which you can
actually get grants for using Azure for
your research project as well not just
as your ml and a few things to point out
you can actually sign up for a free
trial of Azure ml and play with it if it
is soon if you don't want to wait for
the academic awards or if you're not if
you're an industry and get access to it
we've actually put up a machine Learning
Center which basically has tutorials on
how to use the service samples and a
community we're trying to build around
machine learning at it we also have a
blog that we try to keep in contact with
our users and our customers as well so
with that I would like to stop and see
if we can just take questions in the
we have remaining okay great we do have
a few more questions regarding IT
question is there any documentation
available so there's documentation on
the entire service the modules the the
avala if you're asking about that
particular scenario of how we built it
that would require a white paper we but
we will be preparing a white paper on
that but but the actual service itself
if you go to our ml machine Learning
Center you'll find full documentation on
the service the modules samples videos
to get you up and running okay thank you
and with the large variety of different
tools and information moving between
between different platforms how much
information will there be available on
performance impact of each component on
the pallet yeah that's a good question
so we actually get micro data you can
actually click a module and an
experiment run it's something I didn't
show you by the ways you can actually go
back in time and go back to previous
versions of your experiment but you can
click a module and you can say hey that
module took this much time that's a
bottleneck in my machine learning let me
try another algorithm that reminder so
we give you performance data on each
individual module as well as the entire
experiment how long it took to run so
hopefully that answers the question okay
and will the presenter be covering how
the pricing model will work for Azure ml
okay so our current public preview
pricing and I'll tell you what our so
with the spirit of it is actual numbers
I'll have to pull out on auto out of
some file somewhere but the current
public preview pricing is literally
we're doing cogs recovery for ML studio
that is we only charge you when you're
sitting there staring at your the screen
you're not you're not incurring a dimes
worth of of CPU or Koch or cost when you
hit run and your experiment actually
starts to consume CPU cycles then and
only then are we actually charging
accounting helmut how much CPU got
burned and that's basically what we're
charging a charge back for just cogs
recovery for the CPU cycles that were
burned walks for experimentation which
really brings the cost down now when a
model gets put into production we have a
per prediction price
if your model happens to be in Ted
incredibly in compute-intensive we
actually charge for the compute as well
so that's the rough outline of what the
pricing model is at least for the public
preview until we understand how people
use it how much compute they use and
what you know if we could have a package
deal where we sell blocks of compute
time so we're still figuring that out in
our public preview okay and it looks
like we have one last question is there
support for probabilistic programming in
Azure ml not at present we do have
technology here within Microsoft and
Microsoft Research that we'll be pulling
it in and how offering that
functionality as modules as well as well
as well as support for things like
simulation or optimization as over time
as well modules that do that but again
with that SDK that we're gonna be making
available later this year if you find a
good probabilistic learner or
probabilistic our simulation package or
rules engines that you want to
incorporate the ability to wrap that up
as a module and put that into your
workspace for your team to use is always
going to be an option for you okay and
one last question what is the best
internal contact so we have a support
alias somebody can start with me for it
for internal just barge a VAR GA at
Microsoft comm and I'll make sure you it
gets routed to the right person or
address the question myself if I can
okay and one more
when will Azure ml be available in the
Dublin data center that's that's top
priority on our list we're we we will G
a with with with it and the Dublin data
center in the Amsterdam data center and
there are other data centers I don't
want to get into but our G a roadmap
like all Azure services is to make sure
we roll it out to all of our data
centers by GA and so I won't talk about
a ga roadmap but I certainly don't want
to so we don't want to set on the
service for too long weights it'll be
moving forward fast okay questions I
will thank you everybody for for your
time this morning I know for some of you
is early some of you was late so I
appreciate everyone I'm tuning in for
the talk and I look forward any feedback
you might have on our service and how we
might help great Thank You Roger we hope
that you found today's information
helpful if you enjoyed today's webcast
or
feedback on how we can provide you with
a better event please let us know by
completing our survey you should see the
link to the survey in a pop-up window on
your screen and all materials from
today's presentation will be available
on the archive page within 48 hours I'd
like to extend a big thank you to our
presenter Roger this concludes today's
webcast you may now disconnect from the
call
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>