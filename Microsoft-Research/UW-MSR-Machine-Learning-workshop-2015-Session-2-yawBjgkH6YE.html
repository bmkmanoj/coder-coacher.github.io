<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>UW - MSR Machine Learning workshop 2015 - Session 2 | Coder Coacher - Coaching Coders</title><meta content="UW - MSR Machine Learning workshop 2015 - Session 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>UW - MSR Machine Learning workshop 2015 - Session 2</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yawBjgkH6YE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
welcome to the second session of the
University of Washington MSR learning
day and this session starts with
Daniella Witten from MIT Department of
biostatistics and statistics
who is going to talk about learning the
structure of a graphical model thanks so
much for the intro I'm I actually am NOT
from MIT but I'm gonna add that to my CV
so I'm gonna talk about learning the
structure of a graphical model and this
is um this is actually sort of a summary
of a few different recent and ongoing
projects in my research group in the
last couple of years and so as of course
many of you know graphical models have
been a lot of interest in the machine
learning community for a number of
reasons due to applications in a lot of
fields and in particular I'm interested
in applications in biology but to
briefly summarize in a graphical model
nodes represent random variables edges
represent conditional dependence and of
course as we know conditional dependence
between three random variables what this
is is that X 1 and X 2 are conditionally
independent given X 3 and that's
equivalent to saying that the density of
X 1 given X 2 and X 3 actually doesn't
depend on X 2 and so we can represent
these conditional dependence
relationships in a graph so for instance
in the bottom left here we have an
example of two nodes that are not
connected by an edge and this
corresponds to a pair of nodes that are
conditionally independent and on the top
right we have a pair of nodes that are
connected by an edge and that
corresponds to two random variables that
are conditionally dependent and so the
goal on the basis of n independent and
identically distributed observations is
to learn the structure of the graph in
other words learn what are the
dependence relationships among these P
random variables and so I'm going to
just assume that for convenience that my
observations have some density up of
theta so there I ID from this
distribution f of theta where theta
is encoding some type of conditional
dependence relationship so for instance
the Jeff and Kate features are
conditionally independent if and only if
the JK element of theta is zero and so
in particular this has been a topic of a
lot of interest in recent years in the
high dimensional setting where the
number of variables be P the number of
variables P is much larger than the
number of observations n and this is
motivated for instance by problems in
biology where P might correspond to tens
of thousands of genes whose expression
we've measured or hundreds of millions
of DNA loci and n corresponds to a
relatively small number of tissue
samples for whom we've obtained these
biological measurements and so in the
high dimensional setting the way that we
usually estimate theta is by solving
this optimization problem which we've
all seen before there's some loss
function L and some penalty P and in
high dimensions this penalty is
especially important and so today I'm
going to talk not about the penalty P
but rather about this loss function L
and how we can think about ways to make
our loss function less restrictive in
order to model flexibly data that might
not correspond to an existing loss
function or to a convenient multivariate
distribution and I just want to briefly
mention that my student Qingming has a
spotlight and a poster on the other half
of this puzzle which is these panel T's
P of theta and how we can find a penalty
function that induces a more realistic
structure so I'm going to talk about the
last part of this loss plus penalty
function and basically how do we usually
come up with these loss functions well
typically we have some kind of
assumption for our data like maybe we
assume that the observations are iid
from a Gaussian and then we'll take a
love theta to be a negative log
likelihood for the Gaussian distribution
or maybe instead of working with a log
likelihood we'll work with a log pseudo
likelihood or a conditional likelihood
but typically this is how statisticians
do this is our loss function is based on
some distributional assumption about the
data and I'm gonna argue that often
these types of assumptions that lead to
the loss functions that we use aren't
really that well motivated scientific
and so we may need a more flexible
approach and in particular I'm going to
briefly talk about three more flexible
approaches that we've taken and the
first one I'm calling joint additive
modeling so how do we usually come up
with these loss functions well we say
I'm going to assume that my n iid
observations are let's say Gaussian and
then our loss function is just going to
be the negative Gaussian log likelihood
or more generally we can think about the
conditional distributions like the
distribution of XJ conditional on the
other variables and we're going to
assume that it has some particular
parametric form like if our data is
continuous we might assume that the
conditional distribution of XJ is
Gaussian or maybe if our data is binary
we might assume that there's some
underlying binomial distribution or
something but we're using those
distributional assumptions in order to
come up with a loss function but
actually in a lot of settings if we
actually look at data these loss
functions that we'd be using don't
really seem to fit very well so here's
just a little illustrative example with
some data from a 2005 paper published in
science and this in this data set there
were a bunch of cells and for each cell
they measured about ten or twelve
proteins so the cells are our
observations and there's around ten or
twelve features and so this is actually
a very low dimensional example because
thousands is much greater than 10 but
I'm using this example just to
illustrate the point so what I'm showing
here are some scatter plots and on the
left on the x-axis is one protein and on
the y-axis is another protein and so if
multivariate normality were a reasonable
assumption for my data this should be a
beautiful scatter plot shaped like an
oval and it's not that's that's not a
reasonable that's not data that you
could reasonably approximate by a
multivariate normal and you can see that
in all four of these scatter plots and
in particular this is what the data
looks like after I've already marginally
transformed each variable to look normal
and so this is really the best that you
can do with this data this is the most I
can possibly make it look normal and it
doesn't look normal
and this isn't just like a statistical
concern that I have that
really of practical importance the last
function that you use actually has
really being applications it changes the
edges and the graph that you get even in
such a low dimensional example and we
can only imagine that in high dimensions
when there's even more edges that we
have to worry about the situation's
going to be a lot worse so the question
is can we take a less parametric
approach to coming up with a loss
function that doesn't apply on something
so strict like like multivariate
normality and I should add there's
alternatives to multivariate normality
but really the big picture is how many
joint distributions do you know like if
I tell you you have a p dimensional
distribution where p is really huge now
come up with some joint distribution
you're gonna have trouble coming up with
one that's scientifically motivated or
even a flexible class to work with I
mean there's there's more flexible
alternatives to multivariate normal but
they're kind of not flexible enough for
real data and so we're not the first to
think about this and a number of authors
have taken approach so that they've kind
of described a semi-parametric so in
particular instead of thinking about the
conditional distribution of XJ given the
others some authors have been able to
avoid that by just thinking about the
conditional mean of XJ and they say
let's suppose that the conditional mean
is linear so we have kind of a linear
model here and there's an error term
epsilon J and we won't think about
what's the distribution of epsilon J so
this is sort of semi parametric right
because we aren't making assumptions on
the the mean the distribution of
anything and then we can say that if the
estimate beta KJ is zero then XJ and XK
are conditionally independent and so
people have implied that this is a semi
parametric approach but actually it's
not if we go back to the literature what
we learn is that even though that
equation that I'm calling star has makes
no distributional assumptions if star
holds for every one of P variables then
there's actually only a very small set
of distributions that can satisfy
equation star for every variable and
basically that's multivariate normality
again so this does not work as a way to
get at a more flexible approach to graph
estimation it's just going to give us
back multivariate normality so as an
alternative to make things really more
flexible instead of assume
a linear model for the mean we can
assume a more flexible model and in
particular an additive model where f JK
is just assumed to live within some
smoothness class and then if F JK is
zero we can conclude that XJ and XK are
conditionally independent given the
other variables so we refer to this
approach as joint additive modelling and
we get really nice results from a
statistical perspective we can prove
model selection consistency in high
dimensions but maybe more of interest is
the fact that empirically we get better
results on data sets where some of the
edges are known and we can recover those
edges better so for instance in the same
cell signaling data set the loss
function that you use really does affect
the results that you get not like one or
two edges changed but really the big
picture changes and we're better able to
recover the gold standard edges than
other approaches so the next thing I
want to briefly touch on is is this idea
called mixed graphical modeling and the
point is that in a lot of settings and
especially in biology where more and
more are seeing situations where our
variables are really heterogeneous so
you have P random variables but they're
not all of the same type maybe you're
trying to jointly model DNA loss I and
gene expression measurements where some
of the gene expression is measured on
microarrays and those are typically
continuous valued measurements and
others are measured using RNAi seek
which are discrete measurements that we
would typically model using a Poisson or
a negative binomial where as DNA loss I
our binary and so we would end up with a
graph that looks like this where there
are just three different types of nodes
and we want to understand what the edges
are we want to know what's conditionally
dependent on what but it's not going to
be satisfactory to fit a model where our
loss function is somehow assuming the
same distributional form for every node
and so even though there's a lot of work
on loss functions for for graphical
models most of this past work assumes
that all conditional distributions are
of the same form so for instance if all
of our random variables were Poisson
were from RNA sequencing then we could
assume that every conditional
distribution is Poisson if all of our
nodes are
our count valued measurements but that
wouldn't work in the setting here where
we have notes that are clearly of
different types and so basically in our
approach we're proposing a mixed
graphical model where each conditional
distribution can take a different
parametric form within the exponential
family and so we we get nice theoretical
results but what I really like about
this work is this is actually a case
where we get their article results not
just so that we can put it in a paper
but actually our theoretical results
motivate a more accurate estimator and
so for those of you who are familiar
with these model flection consistency
results in statistics they you know like
every paper it's kind of required to
have one in order to get published in a
good journal and they're not super
surprising and they don't usually give
you that much scientific insight but in
this case our Theory actually tells us
something about how we should be
estimating the edges in the graph and so
in particular in these mixed graphical
models one characteristic that they have
of course is that you have these mixed
edges like an edge between a node that
you're modeling with Gaussian a node
that you're modeling as Bernoulli and
what our Theory tells us is that if
you're trying to estimate an edge
between a Gaussian and a non Gaussian
random variable then you should get that
edge by regressing the Gaussian node
onto the others instead of by regressing
a non Gaussian node onto the Gaussian
node so every edge connects to notes and
you can regress node one on to the
others or regress node two on to the
others and our theory tells us when
you're deciding how to estimate an edge
involving a Gaussian regress the
Gaussian and then in contrast if you're
trying to estimate an edge between two
non gaussians we have a formula that
tells us which node should you regress
on the others in order to recover that
edge so that your probability of
successful recovery will be as high as
possible so for instance in this case in
this toy example some of these edges
between non gaussians I'm going to
estimate by regressing a Bernoulli onto
the others and some I'll estimate by
regressing a Poisson random variable on
to the others
so the the last thing that I want to
talk about is recent work on marginal
screening and so I've been talking about
conditional dependence relationships on
a graphical model where an edge between
a pair of nodes corresponds to
conditional dependence and as a
statistician or as a computer scientist
people who study graphical models
typically think that conditional
dependence is of interest so we are
usually interested in estimating this
edge set II where an edge between node J
and J means that XJ and XK are
conditionally dependent and in contrast
there's a much simpler notion to which
is a marginal dependence graph where an
edge in a marginal dependence graph
corresponds to a pair of nodes that's
not conditionally independent but rather
that's marginally dependent and so I
think that for scientific reasons we
care about this conditional dependence
edge set II and I think a lot of you
will agree with me but what you'll also
probably agree with me about is that the
marginal adds edge set which I'm calling
am is a lot easier to estimate so for
instance like let's say you're working
with a biologist and they have data from
20,000 genes for which they've measured
expression levels on 100 patients if
they're going to estimate the the
marginal edge set e/m all that they have
to do is compute pairwise correlations
because under a simple set of assumption
of gaussian 'ti the marginal dependence
relationships are just given by
correlation non zero correlation
corresponds to marginal dependence so if
you tell a biologist to estimate the
marginal edge set and they can just
calculate correlations then that's
really great that's something that they
can easily do with whatever software
they use whereas if you talk to a
biologist about estimating conditional
dependence even if I as a statistician
think that that's really the important
thing to be estimating it's it's harder
to do it requires some type of paper
published in the last fifteen years ten
years that talks about techniques for
estimating a graphical model
high dimensions you can't you certainly
can't do it in Excel you have to use
software that is like in our something
that that has a specialized estimator
for a high dimensional conditional
dependence graph estimating e is just
kind of a little bit harder and also
less intuitive for a biologist and so so
this is something that I've had a lot of
situations where I'll be talking to
biology collaborators and they'll be
computing correlations they'll just be
threshold in correlations and they'll be
interpreting those results as though
they had estimated the conditional
dependence graph and I spend a lot of
energy saying no no you don't want to do
that you want to ask to me conditional
dependence not marginal dependence so
then we thought okay maybe before we go
around telling everyone we know that
sort of correlations aren't what they
really want and what they should care
about our conditional dependence like
partial correlations maybe we should
investigate this a little bit more and
see how much does it really matter
whether you estimate e or estimate a.m.
and so basically the question is in the
high dimensional setting which is the
setting that my collaborators and
biology care about is a simple estimator
of the marginal dependence graph a
reasonable estimate of the conditional
dependence graph so specifically in the
setting of gas the entity if I just
calculate sample correlations and I
threshold them in order to get a
poor-man's approximation to the marginal
dependence graph does that give me a
decent estimate of the conditional
dependence graph and the answer actually
is yes both in theory under a very
simple set of assumptions that I can
explain to a collaborator this this very
crude estimator of a marginal dependence
graph is a decent estimate of the
conditional dependence graph but also in
practice empirically it actually works
pretty well to be able to just use an
estimate of the marginal dependence
graph if really the object that you care
about is the conditional dependence
graph so in a way this is a little bit
disappointing because as set/sets
statisticians and machine learning
researchers we always focus on
conditional dependence but the idea is
that in a lot of settings marginal
dependence is a fine proxy and really
the intuition is that it has to be a
pretty pathological situation in high
dimensions to be in a scenario where
marginal dependence
nonzero but conditional dependence zero
or vice versa of course I can construct
specific counter examples but in
large-scale in high dimensions there are
only going to be a certain number of
those counter examples and typically a
counter example that you construct in
such a way it would be hard to detect no
matter what even if you're using a more
sophisticated estimator so I'm I want to
thank the people involved in this my PhD
students she's a chen and qing ming tan
very song who's a collaborator at NC
state Allicia j whose faculty at u-dub
and our student who graduated re Forman
and I'm happy to take questions
so I'll start answering the part of the
question I heard which is their
intuition here for why it's better to
regress the Gaussian node onto a non
Gaussian node and so just as a simple
example suppose that you have a graph
with Gaussian and Bernoulli random
variables and you're trying to estimate
an edge between a Gaussian and a
Bernoulli and so you can regress the
Gaussian onto all of the others or you
can regress the Bernoulli onto all of
the others well like Gaussian is just
like a richer amount of information it's
continuous valued whereas Bernoulli you
know there's only two things that it can
take and it's somehow a lot less
informative so somehow you're exploiting
more information if you use the Gaussian
as much as possible instead of using the
Bernoulli as much as possible okay and
the other question is um I guess you
could estimate the edge probabilities in
two directions but why can't you can you
combine them using like if you have some
probability using inverse problem
inverse variance weighting or something
like that yeah so that's a good idea
so the probability it so the question
you all heard that because of the
microphone so um so can we somehow
combine these two probabilities and the
way that these selection consistency
results work is that I get a probability
that progressing a Gaussian onto the
other variables is a hundred percent
completely correct
and then a probability that regressing a
binary onto the others is a hundred
percent completely correct I don't quite
get an edge specific probability but I
certainly could somehow combine these
two estimates like if both the gaussian
regression and the Bernoulli regression
have high enough estimated probability
then I could combine them whereas if
one's really high and ones really low
then I might only use the high one so
that's a that's a good point and that
what's really nice is this actually does
show up in empirical results this isn't
like a cool theoretical thing that has
no practical value but this actually
improves the estimators that you get
I will ask a follow-up question to the
previous one so is there a nice easy
topological ordering to actually do
these regressions so that you obtain the
maximum information yeah so that's yeah
that's a good question so the way that
we were thinking about this and I I
didn't put a ton of details here but we
were taking a neighborhood selection
approach where you basically just
regress one node on to all the others
regress one node on to all the others
and you do P regressions and that's how
you get your estimator but I guess in
principle you could do this better
instead of just having P independent
regressions you could somehow start with
one regression of gaussians on to the
others and then do sub regressions based
on what you've learned I think such an
approach could do better empirically it
would be a little hard to analyze
statistically but that could be a good
way to go and it could do it could
definitely do better
related to each other exactly via matrix
inversion yeah and what you said at the
end of the talk sounded very interesting
so are you saying is there a sort of a
generalization or of matrix inversion
that in the non-gaussian case relates
the marginal conditional dependence
relation my tricks yeah could you
describe a little bit that that that
transformation yes so I mean so the work
that we have so far that's out is in the
gaussian case and it really does kind of
boil down to what you're saying right so
in the gaussian case marginal dependence
is through the covariance matrix and
conditional dependence is through the
precision matrix and like there's not
that many degrees of freedom once you
give me the covariance matrix there's
really only one way that the inverse
covariance matrix can look and somehow
the problem is constrained in that way
and so then once we get out of Gaussian
'ti it kind of depends on what you're
assuming but I guess the bottom line is
the assumptions that you need to make in
order for the marginal estimator to be a
good estimate of the conditional
estimator those assumptions are really
not only plausible but also transparent
and something you can explain to a
collaborator which for me is a really
appealing property and furthermore it's
kind of hard to come up with a counter
example that isn't really contrived I
mean you can you can fuddle with things
by hand to come up with a counter
example but it's it starts to feel a
little bit implausible and so that that
is still ongoing work okay well thanks
so much
to post their spotlights follow the
first one is from Kennington from the
department of biostatistics
hello everyone I'm kenning from the
department of biostatistics at u-dub
today I'm going to present a new penalty
function to learn the graphical model
with heart notes this is joy walk with
Palmer Mayim Faizal Patek soo in and
Hannah Witton suppose that we have a
data matrix X with n samples and P
features the goal is to understand the
conditional relationships among the
features one way to do this is to
construct a conditional independence
graph in the conditional independence
graph is not correspond to a feature and
a niche between a pair of nodes
indicates that the pair of nodes are
conditionally dependent given of the
other variables so suppose that we
observe a my ID observations with some
density function f of X theta the
parameter theta and codes conditional
dependence relationship among the
features two variables are conditionally
independent if and only if their
corresponding entries in theta are
exactly equal to zero to obtain an
estimate of theta many authors have
considered minimize the following
penalized lock like negative log
likelihood it is well known that the
allen penalty encourages many elements
of our estimates theta hat to be exactly
equal to zero however there is a
drawback using the Allen penalty under
the Bayesian framework one can interpret
the Allen penalty as placing an
independent laplacian prior on each edge
therefore it implicitly assumes that
each edge is equally likely and
independent of all other ages this
correspond to an ad or any graph on your
left in which most nodes have
approximately the same number of edges
however an erdos-renyi graph is very
unreleased
in real many real were setting for which
we believe that certain notes half notes
we propose the hell penalty function to
learn graphical models with apps we
assume that the full graph on your top
left corner can be decomposed into a
summation of to graph an ad or any graph
with green edges and the graph solely
consisting of hub nodes with great ages
in this example feature number tree and
feature number seven other hops in other
words we decompose our parameter of
interest data into the summation of
three matrices Z V and V transpose Z is
a sparse matrix which correspond to an
adult rainy graph the matrix V contains
dense columns which correspond to our
hop nodes this motivates the following
happen OD function we impose an l1
penalty on the matrix Z to learn
something that bulan a graph that
resembles the adult rainy graph we
impose a space group lasso penalty on
each column of V so that each color of V
are either very dense or contain all
zero elements we claim that our penalty
is able to learn graphical models with
hops for more information please come to
our poster during the evening session
thank you I'll be talking about
recursive decomposition for non convex
optimization and so recursive
decomposition is actually a
combinatorial optimization technique
that we've taken and applied to non
convex optimization which is continuous
optimization and this is a joint work
with my adviser Pedro Domingues so
continuous optimization is an important
problem in many areas of artificial
intelligence including machine learning
probably stink inference vision robotics
unfortunately many of these problems end
up being non-convex which means that
there are many local modes that one has
to explore you can't just take a convex
optimizer and get to the bottom and do
well and even worse a lot of the time
especially in like AI where we have a
sum of a lot of smaller functions you
get a kind of combinatorial explosion in
the number of modes and so it's scales
exponentially with the number of
dimensions of your problem and so you
can invert this by decomposing the
problem which is something that has been
done many times but unfortunately a lot
of these decomposition methods are
global and they look at like the full
global structure over the entire state
space and this isn't always practical
so consider protein folding here which
is shown at the bottom right so this is
the problem of finding the lowest energy
configuration of this chain of amino
acids and so if you if you look at it
and any specific conformation any two
amino acids most of the time are not
near each other and thus are independent
of each other but if you look at the
entire state space there's always a
state where any pair of amino acids is
interacting so the dependency graph
between any two variables R is an entire
full clique and so there's no global
independence ease in this graph and so
we introduced this idea of a local
structure and local decomposition which
is kind of a subspace based
decomposition method so we look at
specific subspaces and achieve
decomposition within those subspaces
while trying to find the global optimum
and so you can think about this as a
version of context-specific independence
but applied to continuous optimizations
so if you're familiar with probablistic
inference or something context-specific
independence is a more general form of
independence than conditional
independence which has been discussed
earlier today
and so given this idea of local
structure and local decomposition the
goal of our algorithm is to just choose
good subspaces that both result in
decomposition for to avoid this
combinatorial costs of the number of
modes and also to be able to reach the
global minimum which is the desired goal
in the first place so our algorithm
essentially dynamically finds and
exploits this local structure while
moving towards the goal and it does this
using this combinatorial optimization
strategy which is a recursive
decomposition so you choose and assign a
subset of the variables variables are
chosen using a graph partitioning
heuristic which gives you this
decomposition that you want you can kind
of see like a figure on the right side
there so you choose the blue variables
and then you set them to values and
suddenly these are no longer variables
you're conditioning on them and if you
choose good values then you here in some
small subspace that is relevant and you
can get further decomposition as a
result of that and then we simplify and
decompose and recurse and as has been a
very effective strategy in problems like
satisfiability probabilistic inference
weighted model counting a lot of
combinatorial optimization problems have
really avoided a lot of NP hardness as a
result of using the strategy and so we
think this is an applicable method as
well for non convex optimization so our
algorithm is called artists for a
recursive decomposition into independent
subsets as subspaces and yes so we have
some good results analytically this
decomposition allows us to get
exponential improvements in time
complexity versus comparable algorithms
while still being able to converge the
global optimum in certain conditions and
empirically you can see our standard
graphs where we're below everyone else
working
so empirically we can find much better
minima both for structure for motion and
protein folding and we're excited about
applying this to other domains as well
so and the intuition I'll leave you with
if you're familiar with my adviser Pedro
his work is on some product networks and
so you can think of this as like a
continuous analog of constructing of
some product network like approximation
to this continuous function and then
optimizing that to which is where this
kind of efficiency comes from so you can
talk to me about that at my poster later</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>