<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Beyond Mindless Labeling: *Really* Leveraging Humans to Build Intelligent Machines | Coder Coacher - Coaching Coders</title><meta content="Beyond Mindless Labeling: *Really* Leveraging Humans to Build Intelligent Machines - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Beyond Mindless Labeling: *Really* Leveraging Humans to Build Intelligent Machines</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/a8hx-xPWzhU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hello everyone my name is David burak
and i'll be talking to you about some of
my work on leveraging humans to
hopefully make our machines smarter so
i'm interested in image understanding so
i'm interested in being able to take an
image like this and sort of find all the
objects find all the people figure out
how these people are interacting with
each other how the objects are
interacting with each other what sort of
the global activity that's going on
where this picture was taken what's the
scene is it endorses it outdoors and so
on so trying to answer several semantic
questions about these images that we
would expect humans to be able to answer
but we want to try and build computer
vision algorithms that can do that right
but if you look at any one of these
tasks and if you look at sort of the
state of affairs today this is what you
see it doesn't matter what the metric is
on the y-axis doesn't matter what the
actual task is but we find that people
are significantly better at solving
these tasks then machines are today
right and we are all obviously trying to
bridge this gap but if you think about
how it is that humans interact with
machines today it might not be super
surprising why this gap exists right so
for example let's say you want to teach
a machine what giraffes look like right
what are we going to do we are going to
go online we are going to download a
bunch of images and we'll find an image
we find an example of a giraffe and tell
the machine this is a giraffe right and
then we'll go look for another example
and we'll tell the machine this is not a
giraffe right here's another example
this is not a giraffe this is a giraffe
this is not a giraffe this is a giraffe
this is not a giraffe and on and on and
on right but if you think about it we
know so much more about giraffes beyond
just being able to look at images and
say whether something is a giraffe or
not right for example if you take zebras
instead of just saying that this is a
zebra I know that zebras are supposed to
have four legs and a tail and they look
like a horse except they're striped and
so much more but none of this is being
conveyed to machines all we would do is
take this example and say this is a
zebra we might say this is not a giraffe
and that's about it
and now let's look at the flip side
right i talked about how machines are
interacting with humans but if you look
at the other direction of how machines
are behaving right what do what do
machines today do they take an image
like this and with extremely high
confidence they tell us that this is an
airplane cabin right they take an image
like this and again with extremely high
confidence they tell us this is an
amusement park this is an aquarium this
is a badminton court this is a bedroom
and I've only gotten up to be right I
can go all the way up through Z and give
you examples of seemingly ridiculous
response and these are outputs of a real
computer vision system from a few years
ago it was state-of-the-art a few years
ago and these are high confidence
responses for these images right and it
doesn't make any sense we don't know why
it thinks this image is a bedroom it's
not giving us any warnings that it might
be confused it just spits anan set out
and that's all we have right any white
so that well this was from four years
ago we now have deep learning deep
learning I solved everything well not
really here is the same image and if you
look at the responses that you get from
this is back in April so a few months
ago and there's many genetic responses
that are fine but the first time it
actually spits out a basic level
category it calls this a kitchen calls
salon and it also again there are no
warnings these are high confidence
responses there's no explanation as to
why it believes what it believes right
so what we find ourselves in is a
situation that looks something like this
right both sides are frustrated humans
thing machines don't know what they're
doing they can't give us anything useful
machines think well you're not telling
me what you want from me and we are
stuck in the situation where there is
just a gap between what humans want and
what machines can do right so what I'm
claiming is if we have a better mode of
communication between humans and
machines some of these issues might be
alleviated right so what I'm going to do
for the first part of my talk is talk
about how we've tried to use visual
attributes as a mode of communication
between humans and machines and so
attributes are just visual properties
things like furry natural young and so
on their mid level concepts so they're
higher than just
low-level features like texture or color
histograms but their lower lower than
the high-level concepts that you might
care about and the nice thing about
attributes is they tend to be shareable
across different categories so multiple
animals can be striped multiple animals
can be free what I find exciting more
attributes is the fact that they are
human understandable so they have a
semantic name associated with them and
their visual so there's hope for
machines to be able to detect them and
that's what makes them a good sort of
form of communication between humans and
machines so what I'm going to do is I'm
going to give you a brief overview of
the different things that we've done in
this space of trying to use attributes
as a mode of communication between
humans and machines and I'm going to
organize them along to access so I'll
give you an overview of each one and
then I'll delve a little bit more into
one or two examples so these two axes
the first one is the role of the human
in this in this communication is the
Rope is the human a user of the vision
system or is the human a supervisor
who's trying to train a computer vision
system to learn a certain concept right
so that's the first axis and the second
axis is who's the communicator is the
human communicating with the machine or
is the machine communicating with the
human and as I give you a brief overview
of these examples you'll see a little
bit better what I mean by this all right
so let me start with the first one where
the human is the user and the
communicator who is communicating to the
machine so that first application is
image search right let's say you have
there's a user trying to interact with
the system the user has I just realized
that this example is going to be
interesting given that i'm at microsoft
but anyway so you have a user that's
trying to that has a mental model of an
image that he or she is trying to look
for and the first thing that you can do
is you can query for certain keywords
you might get a set of images back and
chances are whatever you're looking for
is not in the top results right so
traditional relevance feedback will just
let you click on an image and say this
image is not relevant to what I was
looking for or this other images
relevant and so on but the issue is
these methods don't let you specify why
something is relevant or how the image
needs to change to
more relevant to what you're looking for
right so what we proposed was the use of
attributes where a user can click on an
image and say that the person in this
case the person that I was looking for
looks something like this but is fuller
faced right so with this mode of
interaction the user can specify exactly
how a certain example needs to change to
better match their mental model or
better match whatever it is that they're
looking for and what we showed in a
paper at cvpr in 2012 was that using
attributes as feedback gives you
significantly better search results you
can find the image that you're looking
for at significantly fewer iterations
then if you were to just give binary
relevance feedback where you're not
explaining why something is relevant or
how something needs to change so that's
one example the second example is if I
if I flip the roles where the human is
still the user of the system but the
machine is the one communicating right
and so what what might the machine
communicate well it would be really
useful if a machine can communicate to a
user the scenarios in which it tends to
fail right if the machine can
communicate a semantic characterization
of its failure modes that would be very
useful to a user for example a face
recognition system might say that if the
image is blurry or if the face is not
frontal then I'm probably not going to
be able to recognize the system
recognize the person right so this
doesn't make the system any more
accurate right I'm not claiming that we
would have a better facial recognition
system as a process of this but what
this gives you with several things one
is as a user you know when to trust the
system and when not to trust it or when
to use the system and when not to use it
if the image is blurry you know that you
shouldn't use the system right the
second benefit is as a system designer
if you have an explicit characterization
of the failure modes if you can
recognize these patterns and the
failures you might be able to design a
solution that is specifically catered
for that scenario and the third thing is
using such a characterization the
machine itself might be able to predict
when it's about to fail and so in our
upcoming paper at ECC V next week we
show that by using an attributes based
characterization of the failures the
machine can predict its failures more
accurately than several strong baselines
for example if you just use the
confidence of the class
for to predict whether the system is
going to fail or not that doesn't work
as well as using attributes to
characterize the failure modes alright
so the third scenario that I want to
talk about is where you have the human
communicating with the machine as a
supervisor right and one natural thing
to communicate is sort of your domain
knowledge or your visual common sense
about the world to the machine so for
example we've used attributes to allow a
human to say things like polar bears are
white and larger than rabbits and we've
shown how you can just take these
textual descriptions in terms of
attributes and convert them to a visual
model that the system can then use to go
ahead and recognize polar bears even
though it's never seen polar bears
before and the flip side of this again
you have a human as a supervisor but the
machine is of the one communicating and
we're trying to explore ways in which
you can make your machine more
interpretable so you can get your
machine to explain why it's making the
decisions that it's making for example
it might say that I think this is a
polar bear because I see something
that's white and furry and the nice
thing about these last two scenarios is
they can give you a way of going back in
force they give you ways of doing
interesting active and interactive
learning settings where now that you
know what the Machine believes because
it's characterizing its decision process
the supervisor might be able to convey
certain domain knowledge or visual
common sense to specifically clear
certain errors that you might see in the
machines current beliefs so you might
realize that it's learned some weird
correlations that might be because the
training data had certain biases and she
might be able to go back and fix that
give it a better training set to
hopefully get more accurate models right
so what I'm going to do now is give you
one example of this active interactive
learning setting that we've explored so
if you look at traditional active
learning right the way that works is the
machine is going to pick an image that
it's currently most confused about and
it might ask the supervisor is this a
forest image or not right and the only
thing that the supervisor would say is
no this is not a forest right so this is
the traditional active learning setting
where the machine is 11 the machine is
not expressing its current belief
about the visual world it's just asking
the user a question and the user is only
giving binary labels the terms of yes
this is a forester no it's not so what
we proposed instead was one instead of
just asking the question the Machine
conveys its belief in this way it's in
in this case it's conveying the belief
in a very simple way it's just telling
the user what its predicted label for
the images so the machine is saying I
think this is a forest what do you think
right and the Machine and the human
instead of just saying or giving a yes
or no answer about whether it's a forest
or not if the machine is wrong the user
provides an explanation as to why the
machine is wrong so in this case the
supervisor is saying no this is not a
forest because it's too open to be a
forest and what this lets us do is the
following if the machine now has a
pre-trained attribute classifier for
openness it can go to a large data set
of unlabeled images identify all the
images that are even more open than this
query image and realized that if this
images to open to be a forest then all
these other images that i have that are
even more open must not be for a Seether
right it can add those as negative
examples for it for training and selasa
fire and update its model right and so
in this way feedback given to just one
image we show that you can propagate
that too many more images in an
unlabeled set and learn the model much
faster than you would if you were only
getting label based feedback right now
one natural question here is well in
order to identify all these images that
are even more open I need the machine
needs to have a classifier for what open
means right and so what we explored
after that was well what if you don't
have these pre-trained attribute
classifiers what if you try and learn
these attribute classifiers on the fly
and the question is can you do that
right and the answer is yes because if
you think about it when the feedback
says no this is to open to be a forest
yes that's giving you information about
what forests look like but it is also
giving you information about what open
means right so if this images to open to
be a forest and if the machine already
has a few example images that are
labeled as for us then those must be
less open than this query image right so
you can use the same feedback
annotation for updating its attribute
classifiers right and so here's one
example result that we have on the
x-axis is the number of iterations is
the amount of feedback that you're
getting from the user and on the y-axis
is the classification accuracy of the of
the classifiers that you're trading to
recognize categories right and this is
the baseline approach this is the
standard active learning approach that
doesn't use any attribute based feedback
this is what you get if you add in the
attribute based feedback at every
iteration right so you see that there's
a significant improvement by specifying
why the classified is making certain
mistakes so in other words instead of
just giving label based feedback you're
also giving attribute-based explanations
and this final curve is what you get if
you learn if there are two things that
change here one is you learn the
attribute classifiers on the fly and the
second is when you pick an image
actively you don't just try to pick an
image that is informative in terms of
the label based feedback but you also
try and pick an image that would be
informative in terms of the attribute
based feedback so you also change your
active learning model to pick the next
image yes actually learning any faster
in terms of the amount of information
you're providing the system because each
iteration of the higher ones you asking
me human to give you much more
information thank you are in the lower
line right so we also have another
version of the plot but on the x-axis
it's the amount of time that the user is
spending and what we find is even if you
scale that out by the extra factor it
you still get these gains but I agree
that is a more accurate representation
any other court yes ice feedback are you
restricting the domain the language
domain that kind of so in the way the
interface is currently set up yes there
is a list of attributes that the user
picks from but conceptually especially
when you think about the case where you
can learn these attributes on the fly
there's nothing that restricts the
interface from being more freeform where
the user can type in any word that they
want to use and ideally what you would
want to do in that setting is so right
now because we have a fixed list they're
always going to pick one of those
options
when you move to the freeform situation
you wouldn't expect the user to remember
what would they used for a certain
attribute in some previous iteration and
then use the same word again and so you
might want to append sort of a language
model that reasons about the
similarities in terms of the meanings of
these words being used but in this
particular experiment it was just in
this that the victim was a predetermined
vocabulary they still had the luxury to
introduce a new attribute for which no
model existed but they could only pick
from a fixed list so we have several
different data sets this is I think this
is the shoes data set that had about ten
attributes we also have the outdoor
scenes that had about I think it was six
to eight attributes right so what we did
not we don't have a plot where we where
the x axis has the amount of it has the
number of bits and that gets a little
bit trickier because these are relative
attributes so you're saying that it's
you're not actually saying yes or no so
I see I see but what we have we've
converted this to the time and I think
that is what is perhaps what really
matters that how much more time does the
user have to spend to drain the system
to get the same accuracy and so when we
scale it by that factor you still see
improvements but we haven't checked no
attributes phase feedback it was it was
always so this is on Mechanical Turk and
people are just clicking on radio
buttons to give you using a mouse nice
nice to be much slower than minus plus
minus plus right yeah but I mean they
would have a list of attributes that
they would need to still so unless you
have more sophisticated sort of
shortcuts to pick one of those
attributes and so on and if it is much
lower that would only make it harder for
us to the green curve oh you're saying
the green curve would be much would be
much faster if I see I see yeah we
haven't we haven't reason this was all
on Mechanical Turk and we have it
reasoned about that that's a good point
the choice of interface I'm sure would
change the amount of time they have to
spend yes
the attribute classifier needs to be
more accurate than the actual scenery
object classifier because if you have a
mistake in the attribute prediction like
things to be to open then you basically
screw up all the recognition yeah um so
the attribute classifiers don't
necessarily need to be more accurate and
in fact the strange thing with this red
curve is that when we learn the
attributes on the fly we actually saw an
improvement in performance in terms of
the classification of categories which
was a little bit strange to me because
if you have your pre-trade vocabulary
that has used much more supervision to
train there to train the attribute
ranking functions and so I expected that
when you try and learn them on the fly
you would see a decrease in performance
but that didn't happen and what we found
was the attribute models that you end up
learning on the fly are actually worse
attribute predictors then what you learn
offline in the sense of them
generalizing to previously unseen
categories which is sort of one of the
points of attributes but they end up
being catered towards the specific
categories to learn these specific
concepts so they're worse predictors in
general but they end up doing a better
job by training the category model that
you're trying to train in this process
and that's why you see an improvement in
performance which was a bit
counterintuitive to the particular
categories not the particular images but
the particular concepts that you are
trying to that you try to learn so if
you're trying to teach it for us it ends
up learning a notion of openness that is
relevant for forests as opposed to
learning a general notion of openness
that we generalized to other categories
right so that's what I was saying the
right now we just have a predetermined
list so there are no two attributes that
mean the same thing that are on the list
but ideally when you have a free-form
interface where you can just type
anything in you would want to reason
about meaning you would want to have a
language model that tells you how
similar two words are and share the
annotations appropriately so that you're
updating sort of all attribute models on
the fly not just the one that they use
at bit but our interface didn't allow
for that right now
yes I'm correctly the work that you're
presenting here this this two elements
to it one is that the computer is
getting attribute information on some
images like saying that this this was
incorrect for example because it's not
open enough and so on and then there's
the other aspect which is that the the
human is guiding the computer through
the learning process so the fact that
you know every time it makes a
prediction you get some feedback on it
is their way of reading these plots to
separate those two out so so the the
green curve that you see here also has a
human in the loop that is actively
responding to every query that is that
the machine brings up right so the only
difference between the green and the
blue is the fact that both of them have
a human involved in both cases image is
being actively selected the only
difference is that in the blue case the
human is also providing an explanation
right and that the benefit of that
explanation doesn't correspond to just
that very image that that doesn't really
matter much the benefit comes from the
fact that you can now use that
explanation and propagate that feedback
too many more unlabeled images and
that's where you get the benefit from so
if you did not propagate that then the
user the way we have it set up right now
the way I have to find it the user
saying that this is to open to be a
forest isn't going to give you any
information if you don't take advantage
of that and propagated to many more
images to find all the negative examples
of forests does that does that answer
your question okay yes and so here you
look at how quickly the classifiers turn
I was wondering if you also also renders
to convergence can you say something
about whether with the additional
information you provide about attributes
you might be able to achieve a higher
performance so if you're if you have so
we have a set of we have an unlabeled
data set of images right and sort of the
point of convergence would be once every
single image in the database has been
labeled by the user and at that point
all all methods will converge to the
same point because they've all been
annotated with the ground truth category
level labels that the user provides so
if you if you have enough time to spend
as
actually if you have time to get your
entire on labels that annotated then
there's no point in using
attribute-based feedback but the idea is
that if you only have time to label a
few images you can still take advantage
of the Sun labeled set and sort of a
semi-supervised learning kinda setting
by using attribute-based feedback yes
with the child development of you and
teach our by just saying yes or no
binary isn't into that as quick as you
explain to a child why something isn't
right so I missed the first part of the
question but the motivation like if you
read the introduction of this paper
that's exactly the kind of things that
we talked about that if you if you try
and teach a child you're not going to
just say yes or no you usually explain
to the child why something is something
or why he or she has misunderstood but I
don't know if you were asking me if
you've done studies along those lines or
not and no no we have obvious parallel
right right right exactly and that is
that is what we talked about exactly
yeah any other questions yeah d now I do
I couldn't come up with it with a
category which will require the
attribute this is too this is less cool
or to be yeah what was this shoe is too
formal to wear at a certain event and
there's a notion of subjectivity from
the human side as well and then who is
your target population how a reason
right absolutely and so Adriana Gavaskar
one of Kristen Grauman students has
looked at exactly this where she looks
at attributes that are subjective and
she tries to build models where she
personalizes the definition to
individuals and I think she's also has
some follow up work where she is not
personalizing to individuals but
personalizing to sort of schools of
thought that this entire group of people
seem to have the same notion of formal
this other group of people have a
similar notion and she tries to build
models that caters to them these four
attributes you said they should be
detectable and they should be
interpreted and basically if you think
about subjective attributes then they
are not detectable by definition because
there is ambiguity in suppose there is a
target user of what does it or what he
or
she sort of considers this weather cool
or less cool and the other thing is
about interpretable against okay you
have a notion of yes I can associate
what that particular attribute could
mean but I do not have a very clear
understanding or it's a very tough
concept right right that's so we've been
talking about this a little bit where
you can you can sort of there are the
subjective attributes and there are
objective attributes things like the
height of a heel if we still talk about
shoes and the height of a heel is
something that everyone agrees on and so
we are looking at ways in which a user
can define his or her notion of a
subjective attribute in terms of these
objective attributes and that one can
help the machine so the Machine knows
what you mean by this and can convince
sort of a similar thing of propagating
that information to build your attribute
model and the other benefit is you can
also use that to guide a crowd to
annotate images for you in a
personalized fashion right so far
there's been the situation where you
either go to the crowd and then you you
lose all personalization if you're
getting things annotated or you have to
have user specific models and then you
can't have been annotated data and so by
having one user described to the crowd
what he means by a subjective attribute
in terms of objective you can guide a
whole crowd in a personalized way so
that's something that we're looking at
right now any other questions all right
okay and you can also use similar ideas
for guiding semi-supervised clustering
algorithms to a desired clustering by
explaining why certain images should
belong to the same cluster or different
cluster in terms of attributes so that's
also next week at ECC v all right so one
last element that I want to talk about
in this sort of attribute-based
human-machine communication is this idea
of trying to read between the lines so
in a lot of these situations I'm telling
you that the machine that the human
provides feedback to the machine and
everything I've said so far sort of
takes that feedback at face value
somebody says I want to shoe that has
there is more formal and that's exactly
what you do you go look for shoes that
are more formal and return them but what
I'm claiming is that a lot of these
situations there are opportunities to
read between the lines of what the human
is saying
and actually extract more information
from the human without the human having
to do any more work and so I'm going to
give you three examples of what I mean
by this and again there won't be too
many details on that but hopefully the
ideas would be clear so the first
example is the following let's say this
is the pair of shoes that you're looking
for your query black shoes you might get
images that look something like this
none of these are quite what you had in
mind and so what I described earlier is
you might click on a shoe and say I want
something that's shinier or click on
another show and say I want something
that's more formal and you hope that you
would get results that look like this
right the shoe that you wanted is in one
of the top k results so now clearly from
this feedback yes you want something
that's more formal yes you want
something that's shinier but I'm
claiming that the fact that you chose to
comment on these particular images and
the fact that you chose to comment on
these particular attributes in itself
convey some information beyond what's
being stated right and let me give you
an example to hopefully convince you
let's say you're looking for a picture
of this person and let's say your top
care is all so far look like this right
now what are the chances that you would
click on this image and say the person
that I was looking for is more
feminine-looking than this person right
that's that's just not what you're going
to do you're probably going to click on
one of these and say that maybe the
person and one look is older than her
younger than her or something like that
right you will probably click on an
image that is somewhat relevant to what
you're looking for and then explain how
it's different rather than click on an
image that's just completely off right
and so what we showed is if you take
advantage of this if you take advantage
of what I just described here reason
about which reference images the user
commented on and which attributes the
commented on in addition to
incorporating the feedback at face value
you can do a better job at image search
without again the user having to do any
more work try the user is still giving
you the same feedback all right so this
is one example here's another example
let's say you had to describe these
images in terms of these people smiling
right now it's very unlikely that you
will say this person is smiling more
than this person right now if you had
smiling ranking function a relative
attribute predictor for smiling and you
ran it on this image that is probably
what it would say it would say that this
person is smiling more than this person
because she seems to be frowning less
right but that's just not how you and I
would describe it we would just say that
this seems to be a picture of two kids
that are not smiling right but for
another image that looks like this we
would be much more comfortable saying
that these two kids are smiling more
than this kid in the middle I mean so
certain attributes we naturally
described with binary at certain images
we naturally described with binary
attributes and certain images we
naturally described with relative
attributes and what we try to do is by
looking at these images try and predict
which description is more natural right
and with that with a model like that you
can do a better job at image search a
textual descriptions of images and so on
right so this is another example of
trying to read between the lines if
somebody says find me an image where one
person is smiling more than the other
person you should not be returning this
image even though your relative
attribute would agree that this is an
image where one person is smiling more
than the other right and here's one last
example if you were looking for an image
that looks like this what would you
query for you might say I'm looking for
white furry dogs right and now how happy
would you be if you got this result back
right now this is a completely valid
white furry dog but that's just not what
stands out that's not what's dominant
about this image right if you wanted an
image like this you would have said
something like finally a scary-looking
dog or a dog that's barking viciously
here at all that has sharp teeth right
you would not describe this as a dog
that's white and furry and so if you if
your query is white furry I should not
be returning images like these because
the whiteness and funniness is not
dominant its dominant here and so these
images should have a high relevance or a
higher rank and again we can model this
and we show that you'd even do better at
image search textual descriptions and so
on
alright so overall for this part of my
talk I have talked about how we've used
attributes to access a user's mental
model when they're trying to do
something like image search I have
talked about how we've tried to use
attributes to make our computer vision
systems more usable while they're still
imperfect by getting them to
characterize their failure modes I've
talked about how we are trying to use
attributes to make our machines more
interpretable where they're explaining
why they're making the decisions that
they're making and not just fitting a
decision out and trying to use
attributes to teach machine sort of
visual common sense or domain knowledge
that we all have and convey that to
machines and in these two elements i
talked about how you can try and make
the humans job easier where you elevate
some of the effort while still getting
the same information out it's all in all
trying to use attributes to enhance
human machine communication with the
hope of doing a better job at visual
recognition alright so the next part of
my talk I want to talk about something
somewhat related but a little different
so I mentioned that one of the uses of
attributes is for humans to convey sort
of visual common sense to machines right
and so far I've talked about a very
direct communication where you
explicitly just say that zebras are
striped and they have four legs right
but the question is can we learn this
can the machine learn this knowledge
automatically just by looking at just by
observing the visual world around us all
right and so if you look if you look at
an image like this and if you had to
describe it you might say that this is a
picture of two professors conversing in
front of a blackboard right and now some
of you might have seen this example but
if you had to if you had to explain why
you think this is a picture of two
professors right we might say that well
these are probably professors because
they look fairly distinguished they have
receding hairlines they're standing in
front of a blackboard why do we know
it's a blackboard because they're sort
of complicated equations written all
over it and so on right why do we think
that conversing with each other because
they're looking at each other right and
if I if I change just one subtle thing
about it right if I show you this image
instead our description would change
right we would no longer believe that
they are talking to each other we would
think that these are two professors
standing in front of a blackboard right
and now the key is
how do we get machines to learn this how
do we get machines to realize that just
one difference one sort of visual
feature in terms of gays when that
changes it changes the entire meaning of
the scene right and so there's a few
challenges and trying to do this just
have machines learn this right one issue
is the lack of visual density we don't
have access to such pairs of images
where the only thing that's changing is
one subtle feature for example the gaze
of two people right we just don't have
that we don't have access to such data
sets so it's hard to learn how just the
change in a gaze changes the meaning of
the scene right that's one problem the
second problem is even if we somehow had
this visual density we would have to go
ahead and annotate every single thing
about the image receding hairlines
distinguished looks suits ties gaze
expression blackboards equations on
blackboards everything right and
annotating that is expensive and quite
boring right it's going to be hard to
get people to do that and so then we all
wish that well maybe we can just
automatically detect all this right
we're in the business of doing computer
vision we can just have it detect
everything but we're nowhere close to
that we don't have accurate algorithms
that can accurately detect receding
hairlines and distinguish looks and so
on so we sort of have this chicken and
egg problem right we want to learn this
common sense so that we can do a better
job at visual recognition and we're
saying that in order to learn this
common sense we need to solve visual
recognition that we can extract all
these attributes and objects of images
to actually learn the common sense right
and so what we're claiming is to break
out of this sort of vicious cycle we
should just maybe give up on photo
realism right I'm asking if we to learn
common sense to learn the sense of the
visual world do we really need access to
real images or is there something else
that we can learn this from right and so
what we proposed was to try and learn
this common sense from an abstraction of
our world so a word that instill visual
still conveys the same sense of the
visual world around us but is not photo
realistic right and so we introduced
these two characters Mike and Jenny when
we created this world of different toys
and background objects different animals
fun hats
food items and so on that they could use
and Mike and Jenny can have different
expressions and different poses and what
we build was this extremely
straightforward interface that allows
people to sort of drag and drop objects
onto the scene change the depth by
changing the size flip objects around
choose different expressions different
poses and you can create this little
scene fairly quickly that's that's very
simple but does have quite a bit of
meaning in it right if I asked you to
write a few sentences describing this
you would be able to do a reasonable job
right and so now once we have this
interface we can do fun things with it
one thing we can do is we can show
people a sentence for example here's a
description Mike fights off a bear by
giving him a hot dog while Jenny runs
away and you can tell this is these are
fun things to work on right but so we
can give this description to 10
different people on Mechanical Turk and
we can have them all create a depiction
of this sentence using our clip art
using our interface and the nice thing
is they're all going to create different
scenes right the objects are different
their locations are different some of
them have trees some of them don't they
have different kinds of trees but the
common thing is they all have Mike Jenny
the bear mike is always facing the bear
with the hot dog in his hand and jenny
is always running off in the opposite
direction right so now with this this
gives us access to a set of images that
are very different but they all have the
exact same meaning and we can now start
analyzing what is it that's common
amongst these images and realize that
those features must be the ones that are
relevant to the meaning of the scene
right and this is something that is
almost impossible to get with real
images you can never find multiple
images with the exact same meaning right
and so we created this data set of 1,000
sort of semantically similar classes so
we have these thousand descriptions and
each one has a group of 10 images
corresponding to that and you can do a
lot of interesting things and so the
nice thing about this data set is it's
completely annotated right you have a
pixel level segmentation I mean by
definition this is an abstract world is
made from clipboard so every single
pixel has a semantic categories
you have all the attributes label you
have the gays labeled you have the poses
labeled everything right all the
parameters of this abstract visual world
you have a direct access to right and
now we can start thinking that okay if
you have object detection attribute
detection post-detection solved now can
we actually do a reasonable job at
understanding the meaning of the scene
and that is still an open problem it's
not trivial to go from these annotations
to for example the description of the
scene that would convey the meaning and
not only relevant details right and so
now we can finally start studying those
problems without having to wait for
object detection and attribute detection
to be solved right so we did a few
things with this one thing is we tried
as I said we try to reason about which
visual features are important to the
meaning of the scene and we can also
start building and explicit mapping
between visual features for example
relative location or gaze or expression
and text textual phrases going so we can
learn that running away means the person
is facing the opposite direction from
whatever subject here she is running
away from and isn't a running pose right
so we can learn that explicit mapping
from from this data set and so with that
mapping we can do something like this we
can take an input description that says
jenny is catching the ball mike is
kicking the ball the table is next to
the tree and we can do some
straightforward natural language
processing to extract tuples from this
description jenny catch ball my kickball
table be and you can see that there's an
error here we are missing the tree and
we haven't quite figured that out but
now from these two poles were using the
mapping that I just described in the
previous slide we can automatically
generate an abstract seen that
corresponds to this input description
right so this the machine has
automatically sort of imagined the scene
that corresponds to this fairly brief
story that you see up there and and it
makes a lot of sense Mike is kicking the
ball jenny is catching the ball and the
tree is missing because the NLP
processing mr. right and just for
reference this was the ground truth seen
that corresponded to it so the ball is
different Mike and Jenny are at
different locations but the meaning has
been maintained
and so then what we wanted to do was try
and use this for something much more
specific right now I'm talking about
genetic scenes and generic descriptions
and what we try to do here is have
people create illustrations of very
specific interactions between people 60
different fine-grained interactions like
dancing with holding hands with running
away from and here they are creating a
scene that person is dancing with person
B it looked like the person is actually
may be hitting the other person but as
soon as they change the expression the
entire meaning changes right and so
people can illustrate whatever they
believe is a good illustration of the 60
different fine-grained interactions and
these are some examples of what
Mechanical Turk workers created for
jumping over holding hands with dancing
with and if you look at the dancing with
theirs it's a it's a nice diverse
collection of different depictions
different canonical poses of what
dancing with might look like and so then
what we do is we use this just these
visual abstractions just this clip art
to train a model for what these
interactions mean and then we downloaded
images from the web that depicted each
one of these 60 different phrases and we
try to use those models trained only on
visual abstractions and use them to
detect these interactions in real images
right so this is this is sort of an
extreme domain adaptation task if you
will right and you can also think of
this as zero short learning right our
models haven't seen any real examples
this is just people depicting their
understanding of these phrases right and
so this is a snapshot of what the
results look like so this would be your
accuracy by chance because you have 60
categories and this is what you get when
you train on these clipart and use it on
real images which is I mean the actual
accuracy is not all that high but it's
it's fairly impressive 40 short learning
that you've never seen what a person
dancing with another person looks like
in real images but you can still detect
it right and so here what we are using
is the ground truth pose on the on the
real images we are not interested in
solving the post detection problem what
we wanted to see was if you have good
post detection does the meaning of what
it means to dance for one person to
dance with another person does that
carry over to real images right does the
sort of the common sense learned from
apps
tract images carry over to aerial images
but for just for completeness and given
that I'm in computer vision we actually
ran the post detectors and this is what
you get with your bows detection on real
images all right so all in all we have
been trying to use visual abstraction
for studying mappings between images in
text or images and meaning I showed you
how we try to use visuals abstraction 40
short learning to drain models for
specific concepts that you then want to
detected real images we are also trying
to use visual abstraction to study
things like image memorability so for
real images people have shown that
certain images are much more memorable
to human subjects than other real images
we found the same thing to be true for
these clip art scenes and we are trying
to understand what is it about these
scenes that make them more memorable
than others we're also looking at
specificity so if I show you one if I
show 10 different people one clip art
scene and I asked them to write a
description for certain scenes these
descriptions are extremely consistent
they talk about more or less the same
thing but for other scenes these
descriptions vary quite a bit and so we
are trying to understand what kinds of
scenes are so specific that everyone
thinks of the same thing when they look
at it but why are some other scenes less
specific that people write different
descriptions and as I mentioned we also
try to see if we can learn common sense
from these abstract images and apply
them to improve visual recognition on
rigal images and the high level point
being that this is exciting because we
can start studying these problems
without having to deal with all the
errors that we get with object detectors
attribute actors pose detectors and so
on and of course this goes beyond my Ken
Chinese so we're in the process of
collecting the data set that is much
larger has significantly more realistic
laporte its people of different ages and
different ethnicities many more poses of
the animals indoor scenes outdoor scenes
and so on all right so with that I'll
conclude I started by talking about this
gap in the performance of humans and
machines as far as several semantic
image understanding tasks go and I
argued that if we have a better way for
humans and machines to communicate maybe
we can make some progress in terms of
bridging this gap I talked about human
machine communication
for two goals one is to let humans
convey their domain knowledge or their
common sense to machines and the second
is to make our machines more
interpretable for humans so that we
understand them better and we can make
these systems more usable I talked about
two different modes of communication one
was sort of a very direct mode of
communication in terms of attributes or
text and the second is much more
indirect it's based on illustration and
it's based on sort of abstracting the
visual world down to the semantic
entities that we care about and then see
if we can study these high level tasks
so all in all tying back to the title of
my talk trying to use humans were more
than just sort of these binary labels on
images thank you what do you visualize
the application is being is it clip art
or personal photos for the image search
using attributes using anything so for
the attributes we had a variety of
applications in mind one was sort of
online shopping that when you're trying
to buy shoes it's very convenient to
click on an image and say that I want
something like this but with higher
heels or something that's a darker color
a brighter color or is a shiny or shoe
it also makes a lot of sense for sort of
law enforcement type applications where
someone may have witnessed a crime they
saw the person didn't get a chance to
take a picture they show up at a law
enforcement office and there's a
database of pictures that you're trying
to find that one person from so you can
click on an image and say the person I
saw had longer hair or lighter skin or
was more was fuller faced and things
like that even take a lot of times when
I'm making talks I have a particular
image in mind that I'm trying to look
forward so all those scenarios where you
have a fairly accurate picture of what
you're looking for you just don't have
the actual picture which is why you're
trying to search for it and so there I
think a lot of times users it's very
natural to describe it in terms of
attributes relative to other things that
you see if I were to just describe it in
free form that might be hard but when I
see something it's easy for me to say
how what I want is different
generalize from those clip clip art
images to real photos and I was
wondering if you couldn't say something
about what kinds of features interline
of picked up right so the so we we
cannot be using any features that are
appearance-based right so we can't
extract any of the texture color etc so
the features that we extracted were
things like the post the joint angles
the expression the gender we also looked
at the location of one person's joints
with respect to the other person sort of
torso head and things like that so all
those things that you can extract once
post-detection is already solved so
those are the kinds of features that we
extracted from there</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>