<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Your Abstractions are Worth^H^H^H^H^HPowerless! | Coder Coacher - Coaching Coders</title><meta content="Your Abstractions are Worth^H^H^H^H^HPowerless! - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Your Abstractions are Worth^H^H^H^H^HPowerless!</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gXsBO6dzZSE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
this is going to be a bit of a strange
talk for me because I'm I'm principally
a security guy but I find that in order
to actually innovate in computer
security sometimes you have to get into
the hardware and we discovered some
interesting things along the way and
we'll talk about computing on this
intermittent power and how horrible it
is for people who have to write software
so lots of fun pain but first I like to
say thank you for welcoming me here I
just stopped by Parker's piece I believe
that's the area of which it's called and
I have to thank maybe it was George or
whoever does any security research on
welcoming me because I took this photo
and and then I later zoomed in I looked
in at the trees and I noticed that you
guys actually welcomed me so i have to
thank you for the decorations in the sky
there I don't know how you did that ok
so again I'm a I'm largely a security
guy some folks know about my research on
the security of medical devices and
looking at reverse engineering for
instance contact lists credit cards but
today I want to talk about the journey
we took in the interesting research we
found when trying to use this lovely
device so in 2006 2006 Josh Smith
previously at Intel came up with this
what I thought was quite an innovative
device it was it behaved like an a you
hf rfid tag but it was completely
programmable it had a microcontroller it
had no battery and you could write C
code and compile your programs to get to
run on this device and we thought wow
that's really interesting we can finally
start to experiment with interesting
security protocols so I had my student
do some simple crypto and she compiled
it and put it onto the tag and it
wouldn't run because this machine would
reboot about every 100 milliseconds
because of loss of power it would be
harvesting energy from a 915 megahertz
uhf rfid reader and then it would just
die so we got halfway through I think
reduced round rc5 which in in plain
terms is a very
simple some at a very fast cipher that
should work on any platform but it
didn't work on this one so this talk is
largely about how to get computation to
work well despite extremely fickle power
sources to make the programmer get get
work done without having to think so
carefully about what's going on in
energy but I think this is a really
interesting area for research because
these devices are shrinking and I'll
just go through a couple examples of the
ever-shrinking computer this is a recent
press release from Stanford about a
device that's designed to swim through
the bloodstream the idea being putting
medical devices literally into your
bloodstream and what's interesting is
they've designed it to be wirelessly
powered because of course putting a
battery through your bloodstream
probably not a good idea might not fit
either in the medical device world
there's quite a bit of an innovation
today pacemakers are typically about yay
big and they're implanted beneath the
clavicle but one of the largest medical
device manufacturers has created an
actual product type that's extremely
small I'm sorry I don't have any any
British coins on me at the moment but
it's extremely small maybe you have tic
tacs here I don't know taking but it's
extremely small and it can actually be
implanted in the body using a catheter
rather than doing any kind of invasive
surgery there's some other interesting
work going on actually in collaboration
with Microsoft Research in Seattle and
you dub on this augmented reality
contact lens is anyone here working on
that project by any chance now I find it
fascinating that you're able to put
computers inside contact lenses such
that you can have augmented reality so
as you're walking around you can see
annotations and it's already been done
with trials on rabbits and at the
hardware level there's some really
interesting research going on with for
instance sub threshold circuits to get
computers running an extremely low power
is this year research by any chance
anyhow and extremely small and being
applied to medical devices but there's a
big problem and the big problem in my
mind is the the energy bottleneck
typically the battery so with that
pacemaker that was
size of a tic tac they don't have a way
to power the chip they figured
everything out except how to get power
to it so the power problem is holding up
the innovation of the medical device
when the contact lens they've tried all
sorts of ways to power it but they
haven't found any that really fit their
specifications the constraints of their
system the space constraints are
extremely constraining and as you can
imagine you really don't want to put a
battery in your eye it's not an ideal
location to have something that heats up
and then with that device that's
swimming through the bloodstream they
point out that energy simply doesn't
have a Moore's law batteries don't have
a Moore's law does not shrink at that
kind of rate so it's going to be
difficult to shrink these batteries down
to give you a more concrete example
who's used a sensor mode if you Oh many
of you okay so this is the typical
device most people think of at least I
think of when I hear the the name sensor
mode what's interesting is it has a what
I call a back pack of batteries to
double-a batteries that take up about
sixty six percent of the entire weight
of the device and I find this amazing
that we tolerate that kind of volume and
that kind of weight there are other
innovative ways to make these devices
last long just add lots of batteries I
think that's maybe a little bit silly I
certainly wouldn't want to implant that
in me but if you are willing to
implanted advice in you say a pacemaker
you might be interested to know that
every five years you'll need a surgical
replacement of your device because the
battery begins to run low there's single
use batteries and moreover if you're
willing to take a dremel to a pacemaker
that you found in a medical waste
container you'll find that the battery
takes up over half of the volume of the
device and well over half of the weight
and even worse a very small sliver of
that energy is actually used for pacing
of the cardiac tissue most of the energy
is used on what I would consider
overhead of the system so these systems
are not exactly the most
energy-efficient in my mind now there
are many ways to take a look at this but
I view this as a problem of abstractions
that don't quite match what the software
wants what the hardware is providing so
today by and large systems hardware
create the
strict hardware software interfaces
where we try to create a world where the
software is running on the ideal highly
reliable system so one example would be
a hardware piece of hardware that
provides the abstraction of reliable
storage and flash memory where the
software doesn't have to worry about any
errors that might crop up all the wear
leveling is done in the hardware and the
firmware of the flash translation layer
so you know we can try to make hardware
perfect we can minimize the consumption
of the energy consumption of the power
would say sub-threshold circuits but if
you work with say beginning
undergraduates you'll learn that it's
very easy to find someone who can create
software that will make your hardware
perform terribly right it's I give you
the example of the prius right you can
have the the hybrid car that has the you
know very good energy consumption
profiles and yet if you use software
like a gps sent you through these clover
loops and lists late you're just wasting
energy because the software is not being
intelligent about how to use the actual
hardware so i think it's really a
problem of the the software and hardware
mismatch and so the question I'd like to
pose today is well what if we just have
the software demand less reliability
from the hardware because maybe that
allow us to save energy to tolerate to
tolerate less reliability sure and so
that's that's the question we'll explore
today it's of course a very undesirable
thing to make a system more unreliable
but we'll see what kinds of energy we
can save by doing so so the approach i'm
going to take here is let the unreliably
unreliable unreliability shine through
the hardware software interface it's ok
to have errors pop up so these errors
are going to permeate from the hardware
up into the software stack this is in a
sense going to create an incentive
system where the software is going to be
more aware of its energy consumption or
perhaps the libraries implementing
routines in the software so it ought to
be less wasteful with that energy it can
make choices between trade-offs of
energy and reliability but
course this could be a complete
nightmare if you're programming and you
your hardware is unreliable that's just
going to be a complete night where's
nightmare so the tension is how to
create these hardware software
interfaces where we can reduce energy
consumption by more intelligently using
the hardware but still not making the
programming more difficult we'd like the
programmer not to have to worry about
all the nitty-gritty details that the
hardware traditionally has supported so
I'll talk about a few different areas of
getting computation to work well on
unreliable power I'm just going to focus
on the first two how to get storage to
work well in particular how to use nor
flash memory on embedded
microcontrollers at sub specification
voltages and then the second part I'll
talk about how to compute when your
system reboots about every 100
milliseconds it's extremely difficult to
get things done on a device that loses
power so often and I have to save the
the secure communication part for
another day alright and feel free to
interrupt with questions sometimes the
hardware software interface between the
presenter in the audience isn't quite
matched up correctly so if I'm glossing
over a fact that that is an obvious just
just raise your hand so the first part
I'd like to talk about is how to use
flash memory at extremely low voltages
what I would consider illegal voltages
by manufacturer specifications the goal
being to reduce the energy consumption
of an embedded system so this is a data
center not exactly the most glennley
data center well I guess they have cable
ties and many people think of this as
well the storage is going on in all
these servers here but when I look at
this picture I think well the really
interesting storage is going on in the
ceiling so if you take a look in the
ceiling there's a smoke detector and
this device has some really interesting
storage going on it has embedded flash
memory in that guy right that's really
interesting and how often do you change
your I think everyone's going to change
your batteries this weekend right with
the time change right no
every ten years okay interesting well
I've got a video on dropbox from a smoke
detector company if you'd like to know
about how they choose their batteries
but anyhow let's take a look at inside
these kinds of embedded devices to
understand what's going on with storage
and its relationship with energy so
again we're talking about extremely
embedded devices I would consider these
sort of these boring but high volume but
important devices things from
thermostats to utility meters to
implantable medical devices what I'd
like to focus on at the moment is a
smoke detector to understand where the
jewels are going as dictated by the
flash memory so I asked my students to
have this one hundred microcontroller
party in a hundred days where they would
collect 100 products over a hundred days
and then take a look at where all the
energy is going and analyze all the part
numbers to figure out how the designer
chose certain parameters of their system
and for this particular device I thought
it was interesting it contains this
microcontroller it happens to be four
microchip and it's got 8 kilobytes of
embedded flash memory happens to be nor
flash memory it's very rare to find nand
flash on these kinds of devices and if
you take a look at the speck of this
particular microcontroller it tells you
well there are two basic voltages you
can use as the supply voltage to your
chip if you just want the CPU to operate
you could run into 2.2 volts but if
you'd ever like to write the flash
memory you must run it at 4.5 volts so
this creates a hard choice for the
person who designs the board because now
they have to decide which voltage do
they run it at so in the ideal world
you'd buy a microcontroller and you'd
have say to power rails or two pins on
your chip one for the low voltage one
for the high voltage you can imagine
many sub components of your
microcontroller but two interesting ones
are the CPU in the flash now the CPU
typically can run on a relatively low
voltage but flash memory has a rather
high minimum voltage requirement it has
a rather fancy charge pump just to get
the voltage up high enough to write
so typically you have this high voltage
now in the ideal world you'd have the
two pins such that if you don't write to
flash memory you would just not even
bother turning on the high-voltage line
just don't draw any crime there and in
that case you'd have energy
proportionality Justin just like in the
data center case you'd be scaling with
your utilization in practice it's not
quite so good in practice due to cost
constraints the chip manufacturer only
gives you one pin for your your voltage
source and that one pin sort of have to
rule all the different components on
there and feed them the same voltage so
what happens with the system integrator
they look at all their components and
they say well which one has the highest
minimum voltage that's the one we're
going to run our system at a hundred
percent of the time and so typically
you'll find them running at this high
voltage whether or not they're writing
to flash memory but just in case they
might write to flash memory at some
point in the future and they're your
energy consumption is going to be
proportional to the worst-case workload
and worse don't forget that power
consumption is proportional to voltage
squared so there's a squaring effect
going on when you run on at a higher
voltage or CPU is basically burning up
energy unnecessarily running at a high
voltage that it doesn't need to get its
job done so we'll be looking at just how
do we relax these constraints to run at
a lower voltage and yet not have to
worry about errors and so today we have
these sort of strict storage interfaces
where an application developer expects
highly reliable storage so if you were
to write a 0 into your flash memory you
would expect that it would actually be
stored as a 0 and then later when you
read it back you would expect that 0 to
come out and this is done because we're
running our flash memory at a relatively
high voltage and what we have a
reliability mechanism built into the
hardware to do so and it does a correct
job so what we're going to explore now
however is well what if we move that
reliability mechanism from the hardware
layer and put it into the software stack
instead instead of having this hard
interface between the hardware and
software we're going to let errors
propagate up this is sort of like UDP
for storage in a way we're going to have
sort of best-effort storage in the
hardware it's going to
try but it doesn't guarantee it doesn't
have strong guarantees and the
software's going to have to cope with
that but as a little bonus we're going
to be able to run the hardware at lower
voltage and therefore save on energy
consumption this is not a particularly
new idea actually von Neumann who I
guess invented pretty much everything in
computing and game theory he had this
fantastic paper in the 1950s and how to
build reliable organisms out of
unreliable organisms and it was sort of
the birth of sarcastic computing which
last held its conference in 1978 but
stochastic computing is coming back and
vote because of lot of these energy
constraints so we're going to try to do
the following where we create this
interface or if we write a zero for
instance the hardware might actually
screw up and write it as a one instead
and then later when it's read back the
software is going to have to have some
kind of coping mechanism to handle those
errors and two at least mass those
errors or correct them all right so I'm
kind of a history buff and I see you
have things in your ear lobby as well
that are interesting pieces of computing
history has anyone used a punch card
awesome okay I loved playing so this is
going to date me but I used to love
playing with my father's punch cards
there's some interesting things you can
do with needle and thread and punch
cards for parallelism foot and now this
is your typical punch card system and
it's a write-once storage medium which
is sort of like flash memory if you
don't think of the erase operation and
it turns out I started looking through
the literature and it brought me all the
way back to my thesis advisor it turns
out he wrote a paper in 1982 Rivest
Shamir wrote a paper on how to use right
once memory and turn it in to read/write
rewritable memory even if you can only
write once they call these right once
bits or wits and the basic idea was that
they would use coding theory so you can
imagine if you punch something into a
punch card and you want to reuse that
previously punched a punch card it's
okay as long as the next value you write
is sort of a superset of what you
previously wrote so just reuse those bit
patterns so we're going to do something
very similar we're going to treat flash
memory as widths these right ones bits
but slightly different we're going to be
running it at a low
voltage and in legal voltage which is
logically going to produce sort of
hanging chads we may not properly punch
our flash memory and we'll get these
errors or we're halfway writing so we
call these half-wits we're going to have
these half written right once bits and
we're going to use our flash memory in a
manner inconsistent with what the
manufacturer dictates some in case you
haven't guessed yet I don't like
Authority so we're going to use their
system illegally but the good news is
we're going to be able right at a lower
voltage and that translates into lower
power consumption and that typically
translates into lower energy consumption
as well the bad news is our storage
system becomes probabilistic so the big
problem is how do we reliably store data
on this unreliable mechanism so there
are a bunch of ways you could do this
depending upon what's your cost factor
so the typical way the typical approach
is you just pick the highest voltage and
so that you can run your flash memory
reliably but this leads to this
excessive power consumption there you
could also choose to implement what i
would call sort of the poor man's
discreet voltage scaling you can add
some discrete components to the outside
of your microcontroller to dynamically
adjust your voltage in practice
manufacturers don't like this because
they don't like to buy two voltage
regulators or an adjustable voltage
regulator it's just cost prohibitive
adding just a couple discrete components
or even a couple transistors what the
manufacturers will just kick you out of
the building if you ask to spend one
more penny on adding a piece of hardware
to these high-volume devices the other
approach and we found this is sort of
what motivated our research was just
don't use flash memory so when we got
our hands on one of the early Intel
wisps we're really excited they told us
it had this embedded flash memory we
were doing our crypto we're going to
checkpoint some state we wrote all the
code we did the checkpoint and it didn't
store and we so we asked the designers
well why isn't it storing and they said
oh oh you're going to use the flash
memory well we run it at a low voltage
so that it lasts longer but
unfortunately that means you can't use
the flash so the flash was just sitting
there teasing us but we weren't able to
right to it so we're going to write at a
low voltage we're going to get errors
we're going to have to correct these
errors and so it's the typical game of
well are we going to be able to actually
ultimately save energy in the long run
even if we have to pay for that overhead
of having to correct any potential
errors and I'll just give away the the
end of the the end of the story yes for
many workloads were able to actually
have a net decrease in energy
consumption for the same workload by
running in a low voltage despite the
error-correction cost and we'll get
right into some of our methods and
methodologies for measuring this so we
attempted a whole bunch of different
ways to add reliability at the software
stack you can think of it as sort of a
software driver I'm mainly going to
focus on one technique that we found
overwhelmingly better than all the
others it's called in place rights as
opposed to more traditional
error-correction codes which we found
surprisingly worked terribly but the
in-place rights algorithm I think is a
very simple one that's very easy to
understand it's a very simple feedback
loop the hard part is understanding why
it works so well so traditionally just
do a write in your software but we're
going to replace every single right with
the following feedback loop we do the
right and then we read it back at this
low voltage and we see if what we read
back matches with what we were trying to
get stick into the flash memory social
reading those or die or leave it on the
local reading is always reliable CPU and
reading is always reliable and we've
done testing to make sure that the reads
it doesn't change its value over time it
could but we haven't found any cases so
we do this read we check if there's an
error and if it didn't work we just try
again we keep writing in place to that
same location again and again and again
and when the error goes away we're done
so all nor flash memory is born as ones
i don't know why they chose one but
that's what it is and so when you
attempt to do a right let's say you like
to write this value where the Tuuli
cinnamon if we can't bits are 0 you're
basically trying to flip those two LS BS
and if you're running in a low voltage
you might get an error so for instance
in this case
we had a bit flip error where we were
not able to accumulate enough charge on
the floating gate to have it represent
itself as a logical 0 so there's an
error that we're going to have to
correct using our feedback loop but
what's interesting because these are
born as ones this is what's known in
coding theory or information theory as
AZ channel if we'd like to do the
operation that where we write a the bit
value one there's a zero probability of
failure it will always work but if we
wish to write a zero it might fail with
some probability P and so you can begin
to explore different kinds of coding to
exploit this asymmetry that doesn't
usually turn up in most computing
systems so we'll get into this in a
little bit so there's a whole bunch of
factors that will influence the energy
performance of this rather simple
feedback loop here are a few of the
different properties we took a look at
and today we're just going to cover this
first element which is the most
important one and that is how does your
choice of the voltage supply impact your
error rate and your overall energy
consumption using this particular
algorithm so from this this is our one
eye candy picture and you can learn a
couple things from this picture number
one we're using a programmable power
supplies we can get fairly precise we
can fairly precisely set our voltage
supply and second you can tell that
we're software developers from our
wiring but we're running our algorithms
on this first device and because the
actual monitoring of the system cannot
be run on the same system under test
because it would consume energy we have
a separate system to actually measure
what's going on in the storage system
okay so let's take a look at what
happens when we run our system at a
legally low voltage and we get errors
and flash memory we're at we're
happening we're using the Texas
Instruments msp430 microcontroller which
is a fairly popular ultra-low-power
microcontroller and it's designed it
said we had the spec says we need to run
it at 2.2 volts to get a correct right
so we're running it as low as 1.86 volts
at which point we're getting no errors
at all this for each one of these dots
represents the average of 50 trials I
call this the
slide of a thousand miss lunches of my
poor graduate student and we don't have
error bars because it's basically one
percent so they're very little error in
this case but as you can see as we
approach 1.84 volts we suddenly hit this
wall where we get a terrible error rate
and we very quickly shoot up to one
hundred percent error so that was one
microcontroller we didn't stop there
right so we we tried a different
microcontroller from the same model and
we got the same shape of ever but at a
very different voltage and this is due
largely to process variation you can
expect this this is why a manufacturer
sets a relatively high safety margin so
they don't have to worry about the
process variation yes this is just
writing a racing there's some
interesting complexities to erasing
because of the segmentation but this is
just writing but we can talk about
erasing at the end if you'd like to know
more about the complexities so we took a
different microcontroller a different
model and you'll again see different
places where you start to see errors
there's actually one microcontroller
where we got all the way down to 12.8
volts and we couldn't get it to produce
any errors and we we had to make sure
that we only used each microcontroller
once because of we're leveling every
time we used it we had to basically
throw it away so we had to eat up a lot
of microcontrollers for these
experiments but what you can learn is
that statically the static approach of
just picking a voltage at which you
think you won't get errors is not going
to work because it's likely going to
change over time it's going to vary by
temperature and it's going to vary by
device so what we're going to want is a
software system that can adapt to
systems that had different kinds of
failure properties the other interesting
thing we're going to exploit to make our
system work well is what's known as the
accumulative property of floating-gate
transistors so this was a weird journey
for me because I'm a software guy but we
found that our software was tickling an
interesting property of some of the
hardware so to explain why our system is
going to actually perform so well let
this this
pardon me if you're a hardware designer
or a circuit designer this is going to
be my interpretation of how floating
gates work and I always mix up electrons
and holes but imagine you have a single
floating gate transistor that's
representing a single bit this is
currently the bit with a logical one but
if you apply enough chart if you put
charge into the floating gate there then
it begins to change its value into a
zero once you accumulate enough charge
that's how flash memory nor flash memory
works by storing this charge in here now
what's interesting is that because of
that accumulative property a failed
right is actually a partial success so
what our feedback loop exploits is the
following when we do say we want to flip
this one to a zero we do it at low
voltage and the first attempt didn't
work out we got an error because we
didn't accumulate enough charge but it
accumulated some charge so the second
time we attempt to do a write we get
over that threshold at which point it's
interpreted as a logical 0 question so
you said before that you have this dead
model where you go 1 2 120 and you never
go back to one again but if you're it
with this accumulative model if you
accumulate up to the point where you
write on the threshold and then maybe
the temperature the chip changes or you
know the supply voltage it wobbles a
little bit could you not go backwards it
is theoretically possible and we
explored the question of can we get the
sort of instability we're at different
times or temperatures would things
differ all I can say is it is possible
but in all of our testing we have yet to
see any bit flip that goes backwards or
changes its direction so one of our
experiments involved writing to flash
memory in this manner and then we stuck
it in a in a quiet location for three
months and then read it back again and
we could find no bit errors it is
theoretically possible but we haven't
found it all right so the name of the
game is again the balance it's a
trade-off so we're going to run our
system at low voltage and because of
that magic v squared property it's going
to have significantly less energy
consumption but we're going to have an
extra cost of this penalty for adding
reliability in the software stack and
the big question is when is it going to
be less than the energy of the overall
workload how do we run it at a high
voltage the whole time so to measure
that what we did was this is a very
standard technique in in measuring
energy consumption we're using what's
what's known as a sense resistor a very
small resistor to measure the current
consumption of our microcontroller so
we're able to compute the current
consumption by the voltage drop across
this sense resistor and then on our
oscilloscope we take a trace and we send
a GPIO hi to denote when we're the
workload is starting and stopping so we
can get exact measurements of time and
then you throw it through a very simple
MATLAB integration to compute how much
energy has been consumed by a particular
workload so that's how we're going to be
measuring the energy consumption of our
different approaches alright so with
that setup in mind the first thing we
did before can actually computing how
much energy was consumed was just how
often do we need to go through this
feedback loop to get a reliable right
and so we ran our system at various
voltages you can see if you run your
system at one point nine volts we never
got any errors and so we never had to
actually execute that feedback loop but
if you're a little bit more on the risky
type if you're a risky kind of person
and you run it at one point eight six
volts you can see that we started off
with a hundred percent errors on our
first attempt to write and it may take
you up to six rewrites to get a nice
right there that's how many attempts it
took to accumulate enough charge so this
gives you a hint that as you can see if
you were to choose one point nine or one
point eight nine volts for that
particular chip you would very rarely
need to execute your recovery mechanism
but you're still able to shave off quite
a bit of your safety margin okay so
let's go right into some micro
benchmarks it's hard to find workloads
for these systems so we're going to have
to rely on some mostly on micro
benchmarks and sensor networking
benchmarks so this benchmark is
comparing two well excuse me this first
benchmark is comparing how well the RC 5
symmetric cipher runs on our system
versus a
high voltage system it's a very CPU
intense operation where we happen to be
reporting the average of five trials so
when we ran this RC 5 encryption it
consumed 43 micro jewels which was about
twenty six percent less than if we had
run it at a high voltage now the reason
why we're able to get this reduction in
energy is not because we're making the
flash memory any better it's actually
because we're making the CPU run better
it turns out every time you run the CPU
at a high voltage it's consuming
unnecessary power so all of our gains
come from running the CPU lower even
though we're paying a little higher
price when we do a right to flash memory
so I also have to show you the worst
case scenario for the system and that is
paradoxically writing the flash memory
so if you do a right to flash memory
because of that feedback loop in one
case for some very strange reasons
actually one would expect it to run
about the same but it actually is about
three hundred percent worse if you try
to do it right so if you have a right
heavy workload you would want to do this
right now the good news is in practice
most embedded workloads are not strictly
right right right right right right
right in fact most of the workloads
we've seen in practice and from industry
are more of the flavor one right for
every say 1 million CPU cycles typically
accumulation question flash controller
should be doing the read after write and
my impression is pleased for multi-level
man it already does a sense it's already
I you're getting somewhere so that's
exactly correct so I went to ram bus the
ram bus is that lovely company that owns
all the patents so they can sue you
about memory and i asked them why the
heck is my system performing so well on
especially on these cpu heavy workloads
and I said you know why doesn't your
hardware do this and they said well you
know what the hardware does have a
feedback loop very similar to the one
you're implementing but the problem is
it's the classic intent argument for
system design because the hardware has
to work for all software it chooses a
very conservative approach and that is
it minimizes delay
it says our spec says our right needs to
finish after a very particular delay
because some application might want it
to finish in that amount of time so it
terminates its loop early it's not able
to just have a until the the right
sticks I'd like to observe that but this
is what I'm hearing from the circuit
designers that it's very typical to have
a feedback loop but because they're
required by the spec to finish with a
minimum to certain delay they're not
able to have a loop that keeps going as
long as ours but because it aborted the
loop it's reporting an error for residue
hours you're seeing correct correct and
you're running it again correct okay I
should note that there is no flash
translation layer this is a very
bare-bones microcontroller they do have
the circuits to to apply to the floating
gates but there's no we're out we're
leveling or anything about nature that's
all manual alright so anyhow that's
basically the same slide if you're
willing to play even a little bit more
fast and loose and run your system at
one point eight volts you may more often
have to use your recovery mechanism but
we're still able to reduce the overall
energy consumption by thirty five
percent and what's interesting to me is
that the workload at one point eight
volts actually runs more slowly it takes
significantly more time than running it
at 2.2 volts because it's it's the good
old the combinational logic takes longer
to finish at a lower voltage so we're
taking longer but even though we're
running the system longer because we're
running at a lower voltage if you
integrate over time we've used less
overall energy so the other benchmark is
the closest thing we can get to an
application-level benchmark so we took a
look at a typical sensor networking
applications monitoring the environment
for instance so we're reading 256 bytes
from our onboard accelerometer and we're
computing some very simple statistics
and then we're writing those aggregated
statistics to the north flash memory
happens to be 256 bytes and then we
report the average of the five trials
and so we get very close to that ideal
workload of a very CPU oriented workload
reducing the energy consumption by about
twenty seven percent by running it at
one point nine volts rather than 2.2
volts and just to give you a feeling of
the power consumption difference between
these two voltages running our CPU at
2.2 volt is about double the power
consumption because of that V squared
operation and again if you're willing to
run your system at one point eight volts
you can get down to about thirty four
percent less which is getting pretty
close to the best you could do all right
Oh question so on that curve you showed
there is it with how the rice
performance you had errors of a hundred
percent and the Kurds move rights for
some some if you had errors one hundred
percent at one point n 6 volts so you'd
have to write many times then or exactly
or maybe that would never so this is for
one of your MC use I'm correcting to
this is your wire MCU this in this
particular giraffe we're using one MCU
but right and you're going to it's
possible that you might find yourself
going ok I can use one point six files
and then it's particularly warm or cold
or it's a tuesday or whatever and
actually this loop runs forever and
never actually manages to write the
bitter so if it were cold outdoors you'd
be correct but actually the interest let
me see if i can find our thermal check
here what's interesting is it hired
temperatures we perform better but so we
get about a sixty-three percent error
rate when we're working in our
laboratory but nearly zero zero errors
if you get up to about 39 degrees but it
is correct temperature will influence
the performance of those floating gates
so what we typically recommend is no
your workload and no your intended
environment if you're expecting to be an
environment that's going to have huge
fluctuations and temperature then you're
going to need to be more conservative
but if you have information then you
have conditional probabilities and you
choose a system that's more appropriate
there's one very concrete question how
many instances of the mt were these
results based on averaging across well
each of those points it well for the
very first one it was 50 but for all the
others who is five and I forgotten how
many hundred microcontrollers we'd burn
through but I'll have to get you that
number later it was a lot of government
money put to good use you would see with
the specs as you can drive it you can
you can run the flash memory at 2.2
volts whereas you know with your pic you
it was for half also right into flash
but for an msp430 unanimous p you can
run relatively low the MSP is used to
require that you run your system at
three volts and then I think a year or
two ago they were able to get down to
2.2 volts what's interesting is from a
circuit perspective the reason why one
of the reasons why you can run it lower
and lower voltages is because they put
special circuitry called charge pumps
into the flash memory and the
interesting thing about a charge pump is
it has an efficiency which is usually
quite poor so there's an argument that
maybe you should be removing some of the
hardware that does this charge pump that
boosts the voltage up to a higher
voltage in order to make it more
efficient but the MSP does does run as
low as 2.2 with correct behavior it does
and they does not have a charge pump or
is that also I don't know these are very
proprietary designs and so I often rely
on friends who likes to like there
happens to be one guy and Pennsylvania
who likes to D capsulate chips and the
companies refused to tell us what their
circuit designs are it's hard enough
just to learn that it had 3,000 logic
eight equivalents but I'm kind of
guessing that the manufacturers would
try and spec their systems so that the
flash memory as fails at the same point
as the CPU would fail right they're
going to choose the step up of the
charge pump such that you can run the
whole system at the very lowest voltage
and ensure that your thumb is that
but like you said you're not doing
you're not using that often you're doing
those right set up there would be nice
but the I'm not seeing that in
microcontrollers such as the msp430
there may be some systems in the future
that can but I suspect it's going to be
because of these charges which doesn't
mean necessarily that it's more
efficient so I'm guessing that you
basically a proven that you can run this
thing at one point nine six volts
instead of two point two goals and well
no I don't think you can really well it
that's also conditional on temperature
basically in athletes I think all the
things going to have this range of
temperatures operating temperatures
right they're trying to do is they run
it as close to the wires they can but
they want to obviously maintain aspects
not but that's because their spec
follows the notion of sort of very
strong guarantees of reliability and
we're saying let's explore a different
direction let's just explore how good we
can get the system if we don't have such
constraining requirements on the
hardware in other words we're going to
feed the hardware a better workload to
make the hard work perform better it's a
good old argument of like micro kernels
and simplicity as the closer you get to
the hardware we're going to keep it as
simple as possible and put more of the
context-sensitive logic up in the
software stack instance if they would
turn you whenever we turn you whatever
the white was successful and you could
decide by yourself without a continuous
that's a that's actually a good point
what we really like to see is some
architectural support to give away
certain hints right now most of this
energy all these issues are just hidden
even if the software wants access to it
they don't get access because the ISA
doesn't expose that there's some really
interesting things you can do by opening
up the ISAs to make this hints more
available so I should probably move on
to the computation story and looks like
I've got about 20 minutes remaining or
so i think i can get through it so the
next thing I'd like to talk about is
just how to get computation to work well
on intermittent power when you're
rebooting about every 100 milliseconds
or so because of energy shortfalls and
we're basically going be doing
pointing and I'm sure most of you are
familiar with checkpointing it's very
popular in distributive systems and and
the dependability community and the name
of the game is we're going to try to
avoid what we call Sisyphean task so
imagine you've got this computation
you're rolling your rock up the hill and
every time you get near completion of
your computation you run out of power
and you drop it and you got to start
over again so we're going to try to get
long-running computations done despite
these very continuous interruptions to
our power so we took the Intel wisp and
we modified it a bit so we started
manufacturing our own device it happens
to be called the UMass mu actually
here's our handy-dandy pamphlet there's
some flyers outside if you'd like to
know more about it but what's
interesting is it's running off a 10
microfarad capacitor it has no battery
and it's doing all of its energy
harvesting from this 915 megahertz to an
antenna using getting the energy from an
RFID reader and because of that we get
very fickle energy we it's really hard
to predict what kind of voltage our
processor is going to see so the
capacitor helps us buffer helps us
smooth out that voltage but even so it
feels quickly and it gets used up
quickly and this leads to frequent
reboots so in the the supercomputing
world such as mobile phones now I think
of mobile phones at super computers
because they have I think eight to ten
orders of magnitude more energy than
this device but on a typical device
where you have a long period of time you
can effectively think of it as having
infinite energy maybe not george's phone
but infinite energy but in our case
we're living in this horrible world so
these are three traces of the same human
activity what we did was we chopped off
the front end of our energy harvester
and put it through our oscilloscope the
units aren't particularly important but
it's just the student holding this
device and from the RFID reader and then
measuring the voltage and you can see
you get radically different traces for
each one it's very difficult to predict
what kind of voltage you're going to see
from the processor perspective on these
kind of fickle energy harvesting devices
just to give you an idea of just how
painful it is for programmers to use
this without some architectural or
without OS support this is a real plot
of every time our
rebooted so my student created this
lovely guy every time it says reboot his
system was actually rebooting and he
lost all state all Ram all registers all
went away and he had to restart his
computation and again that's what we
called a Sisyphean task when we drop her
computation and have to recompute it
from scratch so intermittent power makes
programming a real pain in the neck the
constant power loss and this is not just
a the typical computer system thinks of
power loss as a rare event where you
need a coping mechanism you know F sick
on your file systems every now and then
in this case we're actually going to be
rebooting more often than we're
computing most of the time so the goal
here is to find simple abstractions to
help programs and to help programmers
tolerate these highly unreliable energy
sources such that you don't have to
think about it when you're programming
your system so these are energy where
checkpoints and I found out you already
know about reality check points i found
that lovely guy and in the in the green
over there but we're going to checkpoint
and the hard part here is going to be
doing energy-aware checkpoints at such
an extremely small scale but it's
instructive to look at just how this
problem is 12 today oh yeah actually got
the london oyster card who uses oyster
around here ok so the london oyster
system was extremely innovative and they
made a fundamental contribution to
contactless payment systems the way they
solve this reboot problem is they
constrain the problem they say if your
computation takes longer than 300
milliseconds you're not allowed to do it
and this way they're able to have
guarantees at the hardware level that if
the computation starts they know they're
going to complete it but if you want to
write something that takes 301
milliseconds you go away you're not
allowed to do that this is not a Turing
machine so we'd like to relax these
constraints to make general-purpose
computation more feasible now that
doesn't mean we're going to be fast but
what it means is if we can find a way to
get done with the energy were provided
we're going to have a pretty good chance
of completing it eventually the
interesting thing about 300 milliseconds
is how they actually chose this number
they artificially create they created
artificial Layton sees on the bus system
in London and the
found out that after about 300
milliseconds there's extreme frustration
by passengers so that's why that 300
milliseconds exists globally now as the
standard for contactless smart cards so
our approach is um approach is
relatively straightforward we do these
checkpoints and we try to do the
checkpoint before we reboot because
we're going to try to insulate the
programmer from all this unreliability
so well just to give you an idea of how
this works checkpointing before failure
is the following so this is our voltage
supply again we're not your typical
computer we're not running on a constant
voltage every time we run our computer
we you know spent a few jewels that
means our capacitor loses voltage so
we're literally computing as we're
losing voltage and losing energy so as
we get closer and closer to one point
eight volts which is where our CPU may
begin to fail we need to check point so
you can imagine have your computation
coming down and you do your checkpoint
right before you die and then when you
have a lot of time to recharge your
capacitor you can then resume do a
restoration and continue on with your
computation and so the name of the game
and splitting up the computation into
these smaller pieces spreading the
computation across reboots and what we
checkpoint are the registers the stack
and the Global's so it's a variable
amount of data that we're going to
checkpoint at these at these critical
times so I thought this would take just
a few weeks and it ended up taking a
couple years for instance there were all
sorts of development problems like llvm
not having a back-end support for the
msp430 machines we had to create that
but there are other hard parts to when
checkpointing we're checkpointing when
we have sort of dangerously low amounts
of energy and and you might recall from
the previous talk it's also when we have
that the flash right is sort of the
worst thing you can do when you have
very small amounts of energy so you
really on thin ice to give you an idea
of just how much energy gets consumed if
you want to just ask your system am I
getting close to death that has a
penalty of 100 x compared to a cpu aab
so just asking if you're about to die
makes you die more quickly and then if
you know you're going to die there's
also
100x penalty to do the actual checkpoint
itself so we want to avoid these
checkpoints if possible and then the the
variability of the actual data also adds
some uncertainty the whole picture
because we don't know exactly how much
energy we're going to need to reserve
sort of head room on our capacitor to
get that chip point to stick so I'm
going to use one running example it's a
toy example good old CRC but it
exercises many of the interesting things
to understand about checkpointing
running it on our processor takes about
a little more than half a second when we
don't have any reboots so since our
system does reboot about every 100
milliseconds we are going to see these
problems what we have to do checkpoints
so let me explain how our system works
from the programmers perspective we'd
like to make sure the programmer just
has to think about writing their code
not having to worry about energy so
we're going to try to help the
programmer choose certain energy centric
parameters and how the checkpoints are
done so mementos is effectively a
library that you link against it happens
to be implemented in llvm because we
were extremely frustrated with the
modularity of GCC but llvm is wonderful
because you can add all these nice
optimizations in their intermediate
representation after we throw our system
through llvm to make it energy-aware and
add the checkpointing routines we
simulate the program and something
called MSP sim we augmented that with
the notion of energy awareness and this
helps us choose certain parameters for
our checkpointing strategy at which
point we can dump it to hardware so
there's a we've divided up to check
pointing into sort of two major pieces
the compile time the static decisions
and then at runtime so at compile time
what were our job the name of the game
is to remove checkpoints we're going to
try to just mark places in the code
where we might want to do a check point
in the future so for instance the end of
a loop latch you might expect some state
change that might be a good place to do
a checkpoint or a function call return
where you're popping the stack you know
there you can have large fluctuations of
of your memory maybe that's a good place
to do a checkpoint so statically where
it's not that we're adding checkpoints
we're actually removing
points we're saying there are certain
places in the code where we never want
to check point and this helps us avoid
checking our energy unnecessarily at
runtime but there are few other
instrumentation strategies to choose
where you might do a checkpoint now at
runtime we need to make decisions based
upon how much energy we have left in our
10 microfarad capacitor of whether we
actually need to do a checkpoint if it
turns out we have a surplus of energy we
might be able to avoid the check point
entirely so we're going to try to do
that now unfortunately we have at our
disposal very few things very little
architectural support for checkpoints so
what we use is a proxy for death our
proxy for death is a threshold voltage
not not a circuit threshold but a
voltage at which we believe we're likely
to die so if our deaf period is around
1.8 volts and we're doing our checkpoint
and it's going to have some amount of
time it takes to finish you could check
point conservatively but well before you
get to this point of death but if you do
that you've actually wasted quite a bit
of time the problem is this energy
basically just drains away but worse
than that you shortened every single
life cycle such that you've chopped off
a little bit so you're going to increase
the number of overall reboots to finish
your computation so what we really want
to do is check point as close to death
now if you check point too late
obviously it's you're dead and you have
to start over from scratch so we also
want to avoid that case what we really
want is that Goldilocks checkpoint
checkpoint right before death get as
close as we can to that flat line and to
do that we have the abstraction of this
threshold we choose statically one
voltage it's a proxy for death it's not
the best one but it's the proxy we use
to choose when to checkpoint that's
correct so there are some cases where we
are willing to actually be a little more
risk-taking and get close to death it
doesn't work so well for non idempotent
operations like launch missile but for
most of our applications we're fine with
restarting our computation but still
we'd like to avoid it if possible all
right so there are plenty of other ways
you could do the adaptivity but we're
using this voltage
hold let's move on so one of the hard
parts though is making this reproducible
if you've ever worked with energy
harvesting every time you do an energy
harvesting trace it's different so the
first thing we did was we tried to make
this reproducible in the way we did that
was we took our device and we literally
sliced off the front end and we would
take our energy measurements through our
oscilloscope and then we would simulate
the behavior of our capacitor sort of
physics 101 and then we'd throw this
into our specially instrumented MSP
simulator that's now energy-aware it has
the notion of that certain operations
consume different amounts of energy
depending upon the voltage and it helps
us understand win our system reboots so
to give you an idea of how the simulator
works after you run it through llvm if
you give our simulator both the binary
and the voltage trace it'll tell you how
your system is going to behave so in
this case we fed in the following trace
this blue line is what the harvesters is
effectively seeing from our 10 micro
Fred simulated capacitor and the green
line is what happens after our 5 shade
five stage charge pump so it boosts up
that voltage after we get up to about
3.2 volts is where we decide our voltage
supervisor decides that we have enough
Headroom to start the computation and we
turn on our processor and very quickly
we shoot down to death you know we're
just riding the roller coaster of death
and then this red dot is where we do a
checkpoint right before we die as you
can see it takes about the same amount
of time to recharge back up until we can
do a restoration on the green dot and
continue on and we continue doing these
reboot restoration cycles so what's
interesting is if you start to think
about so this is all heuristics right
and there's no relative there's no way
to compare it really what what's the
ideal so what we decided to do is to
create an oracle that could for
reasonable workloads we created a greedy
algorithm that can tell us what's the
best case scenario it's going to try
every conceivable checkpoint strategy
sort of using a binary search and it's
going to try to find even ones that we
could not possibly create on our own
it'll tell us what's the best case
scenario so the way
we do this is we re execute code this is
the beauty about msp430 simulators and
llvm so we run our system and we do a
checkpoint and if we find that we
checkpoint just fine then we say well
let's be a little more risk-taking and
checkpoint at a lower voltage and then
if we die we say oh we got that wrong so
we're going to backtrack and we're going
to do this binary search until we can
find sort of the perfect voltage
threshold at which we get pretty close
to death but we don't actually go past
it yes right right right well so it is
so it's not exhaustive that is correct
it's only going to work on smooth cases
that's right but the Oracle basically
tells us for these smooth cases what's
the best case you could ever hope for in
terms of how many reboots it would take
as well so to give you an idea of what
happens when you compare it against the
Oracle again here we have our lovely CRC
and it's running a little more than half
a second when we run it to our Oracle it
tells us well on this particular energy
trace here's how the behavior of your
program is going to perform if you're
running the Oracle if you choose the
what it thinks is the best case
checkpointing strategy it's going to
take you 14 reboots and it's going to
take you about 685,000 CPU cycles
running around for hundreds now in
practice when we run it without an
Oracle with an actual checkpointing
strategy that's programmable we're not
quite so good and we actually run it at
a slightly higher voltage just because
it's voltage regulators come in discrete
typically come and discrete the sizes um
but what might surprise you here is just
certain incongruencies so well this is
your typical system where it's not going
to perform as well but we are at least
going to be able to perform the previous
system wouldn't work at all but we get a
one point a 30-percent blow up in the
number of cpu cycles that's as a result
of all the instrumentation but the weird
thing is for some reason we take ten
times longer wall clock time and so
there's this weird discrepancy between
the 1.3 and the 10.7 you
think that they'd be about the same and
so this puzzled us for a minute but then
when we looked at the graph it became
obvious oh this is compared against the
uninst romentic I'm sorry so oh yeah
yeah so the onion strim ented code
versus our best case execution was
thirty percent was thirty percent worse
but the wall clock time was actually ten
times worse and the reason for this
discrepancy you can tell just by looking
at the bottom graph half of the time we
just spent warming up our capacitor to
charge up and you don't get that in a
system that has infinite energy and then
if you look at all the reboots you see
that over half of your time is spent
recharging after each reboot so that's
why we have this large wall clock blow
up we don't have such a bad blow up in
terms of the active CPU but on these
systems you generally want to sleep well
and perform good enough and that's what
we're trying to do so in this comparison
if you don't have checkpointing your
system never finishes you'd have to
manually instrument your code to work
but with the check pointing system we do
at least finish eventually after sixteen
reboots so the idea with mementos is
that we'd like to let programmers just
write their code not have to worry so
much about this don't worry about energy
and at compile time we instrument your
programs and then at runtime we check
point when we think it's necessary
there's some really interesting future
directions we've been experimenting with
other non-volatile memories like f RAM
and there's some interesting things you
can do with smarter checkpointing
compression incremental that turns out
to be really hard when you don't have
registers when you're checkpointing your
registers it limits some of the
algorithms you can actually implement
and practice but that is mementos hang
out so I'm running a little bit over
time maybe I'll just get to right over
here I think that what I hope you leave
the talk remembering today is that you
can reduce energy by demanding less
reliability from your hardware and that
errors are okay as long as you have a
coping mechanism that doesn't cost a lot
so in this case there are two different
mechanisms we tried one with embedded
we're running our system at unreliable
voltages and then we restore the
reliability and software at a very low
cost and then for embedded computation
we're able to create these energy-aware
checkpoint so we can at least get things
done eventually if there is a pathway to
getting things done I think I will just
end there and I'd be happy to take any
questions but I think I'm a little
overtime and George's is really my
schedule</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>