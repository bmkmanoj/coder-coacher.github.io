<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Lazy Transaction Execution Models | Coder Coacher - Coaching Coders</title><meta content="Lazy Transaction Execution Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Lazy Transaction Execution Models</b></h2><h5 class="post__date">2016-06-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JQ1BF0GcLqM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged public showing internet or
public display is forbidden and may
violate copyright law
good morning everyone thank you for
coming it's my great pleasure to
introduce udeploy he is joining us from
Cornell University where he's Co advised
by Johannes gurkha and Christophe cock
or cock how you correctly pronounce that
and also he has done a microsoft
research internship here and he's also
done a google internship he is the
co-winner of the sigma 2011 best paper
award together with some of his
colleagues at cornell and today he will
talk to us about lazy transaction
execution models thanks for the
introduction christian so today I'm
going to present my thesis work on lazy
transaction execution models so let me
start by reminding you of what a
transaction is a transaction is a single
execution of a user program over a
shared database state informally it is a
basic unit of change which the date
which the database sees and this
execution of the program is guaranteed
to satisfy the acid properties and I'm
not I'm sure that all of you are
familiar with what acid is so I'm not
going to go into the details of that but
let me show you what such a user program
usually looks like so consider Mickey's
transaction to book a seat on flight 123
so the fact that this program has to be
executed transactionally is indicated by
the keyword start transaction it has
these four statements first Mickey
selects a seat on flight 123 it checks
if there is something which is available
if not then the transaction rolls back
if there is something available then it
does two updates so the database it
deletes that seat from the available
table and it inserts a tuple
corresponding to the reservation into
the booking stable now this of course is
a very simplified form of what you would
see in a PL world so before I talk about
how and why lazy execution as good let
me show you how we can execute this
transaction in a classical model and why
that leads to
to sub optimal results so consider this
following scenario in which you have a
flight in which you have three seats
available now the available seats are in
green the holiday reserved seats are in
red so 1a 1b and 1c are available let's
say Mickey issues the transaction the
program which I just showed you to book
any seat and he gets seat 1a after that
let's assume that Donald issues a
similar transaction and he gets seat 1b
finally many issues a transaction
however many has an additional
constraint that she only wants a window
seat now the only window seat which was
available only 1a has already been
allotted to make and therefore minis
transaction had to abort now if you
assume that you knew that minis
transaction was going to arrive then you
could have given Mickey the seat one see
Mickey don't really care about which CT
got in which case Minnie's transaction
would have committed so let us see how
in a lazy execution model addresses this
issue so again consider the same
scenario now Mookie Mickey says book me
any seat and instead of assigning a
single seat to Mickey I'm going to
commit Mickey's transaction and I'm
going to defer the assignment of seat to
making subsequently I'm going to do the
same for Donald's transaction I'm going
to ensure that Mickey and Donald both
have some seed but I'm not going to tell
them which exact seat they have finally
in this case when Minnie say is that I
want a window seat I can actually assign
mini the window seat which was available
in this case 1a so I made these two
assumptions of what these transactions
are doing one is that there is a
flexibility in the value that is being
written that is Mickey does not care
which exact seat he gets as long as it
satisfies a certain set of constraints
and second that there is a delay between
the point at which the transaction
commits and the point at which you read
the values which are written by the
transaction now human be for now and
assume that there is a broad
of applications which satisfy these two
assumptions and I'm going to return back
and precisely identify what this class
of applications is now assuming that
there is a class of applications over
which these two assumptions hold the key
idea is we can lazily bind the unread
values in the transaction and by doing
so we are creating some room to maximize
some notion of global utility in this
particular application the global
utility was to satisfy the maximum
number of user constraints are
equivalently to allow the maximum number
of transactions to successfully commit
so let me give you another example in
which being lazy helps so consider a
simple voting application in which we
are using this votes table to keep a
tally of the election status so in this
case the Democrats have 100,000 votes
cast for them the Republicans have 75
thousand volts and I have three
transactions which the application are
three user programs which the
application can execute as transaction
one is to cast a vote for Democrats
which basically just goes and updates
the count variable of Democrats by one a
second to cast a vote for Republicans
which increments the value corresponding
to the publicans and a third transaction
which checks who is leading so it reads
the Democrats and Republican counts it
compares to two values and displays who
is the current leader in the election
furthermore let us assume that this is a
nationwide election and I'm replicating
this vote stable across two data centers
one in the east coast and one in the
west coast and also that the initial
state of the database is that the
Democrats are leading the Republicans by
around 25,000 volts so what happens if
we execute these transaction under
strong consistency so whenever I cast a
vote in this case of voters cast for
Republicans I need to consistently
change my replica state across these two
data centers right so I need to inform
sir if I am executing the transaction on
the west coast
Center I need to synchronously inform
the east coast data center of this
change and at least I have to incur one
round-trip latency now of course and the
strong consistency the programming model
is very simple because the user never
has to bother about perceiving
inconsistent or two different replica
states the other extreme is to be
eventually consistent in which you say
that I'm not going to inform the other
data center synchronously I am going to
do that asynchronously so when my
transaction execute on the west coast
data center I commit it locally and I
keep my fingers crossed and hope that
this transient inconsistency between the
two delica states is not perceived by
the application and in this particular
case it is actually not perceivable by
the application because all the
application cares is who is leading and
that transaction is going to evaluate to
the same thing over these two states
however you can imagine scenarios where
it is actually where the Democrats are
leading by one at which point if you are
executing an eventual consistent model
then transaction 33 can see two
different states and it cannot the
application can basically perceive one
in which the Democrats and Republicans
are tied and another in which the
Democrats are actually leading so this
exposes a to this type of inconsistency
then has to be handled at a higher level
in the application right so can we get
the best of both words that is can we
get the clean semantics of strong
consistency as well as the fast response
times of eventual consistency and I'm
going and the answer is yes we can at
least under certain assumptions and for
certain class of applications we can so
let me show you how so the key idea is
to exploit flexibility in the reeds
which the application is making so in
the earlier case for this particular
application if these are the only three
transactions which the application can
execute
then it doesn't really matter what
exactly the Democrats and Republicans
vote counts are as long as in both these
two data database states the Democrats
are leading because the application
cannot actually perceive this difference
so in some sense they belong to the same
equivalence class of database states in
this case a class in which the Democrats
are leading so the idea is then to be
lazy yet strongly consistent and how can
we do so we instead of requiring that
the two replicas are always identical
which is what strong consistency does we
are now going to enforce that the two
replicas are always in equivalent states
as opposed to being completely identical
and this will allow my two replicas to
diverge but I'm going to establish
certain bounds within which they are
allowed to diverge and these bonds are
again defined by the equivalence class
in this case so how do I do it I do it
by using these global treaties you can
assume these global treaties are as a
contracts which all the replica sign and
say that as long as any changes which i
am making are not going to violate the
global treaty i am good but whenever i
am good as and i can execute things
locally but whenever I am in danger of
violating a global treaty I need to
inform the other replicas so of course I
have shifted the onus of communication
from the transaction to enforcing this
global treaty and unless I have a
mechanism of efficiently enforcing this
global treaty in a distributed manner I
would go back to the strongly consistent
case so how do I enforce this global
treaty I project it into two local
treaties such that if these each of
these replicas are making changes which
do not violate the local treaties I am
sure that the global treaty is not going
to be violet so intuitively you can
imagine that I had a budget of around
25,000 volts till the boundary of the
dance class and I have partitioned that
into two one of twelve thousand five
hundred volts so and if it is a little
vague right now that you become more
concrete when i get to the technical
details and precisely define what this
projection is how we get this global
treaties yeah cloud works that relate to
this idea as well because i will then
hold off on question yes i'm going to
ask my question and then if you are
going to talk about it even there for a
fracture how does it relate to two words
what is consistency vation right a
thousand nine I can hear me yeah also as
escrow transaction models right so
consistency rationing it basically said
that we are going to classify we are
going to have three types of three
classes of objects one which have to be
strongly consistent one which can be
eventually consistent and watch one
which are in between so our work
basically says that you don't have to
classify right between category right so
because based on certain constraints
your switch from eventually consistent
go strongly consistent wise you know we
are going to be always strongly
consistent okay except that if the
application doesn't actually require
strong but if the application always
requires strong consistency but the
application cannot perceive some
inconsistencies then I'm going to
exploit that flexibility in the
application to be inconsistent sometimes
okay but using these treaties i am going
to ensure that from the applications
point of view it is always strongly
consistent
regarding the escrow transaction and
demarcation protocol and other protocols
like distributed divergence control
protocols let me come back to that right
so how are transactions executed in this
lazy yet strongly consistent way so now
when you issue a trance when you issue a
set of transaction they go to the west
coast data center they are executed
locally and you can execute them locally
as long as this local treaty is not
violated now in this case the Republican
count has reached confer border of the
equivalence border of the local treaty
and the next transaction pushes it over
as an it violates the local GT at which
point I synchronized with the East Coast
data center I update it with the changes
which had happened in the East Coast
data center and I renegotiate and
establish a new set of local treaties so
again I made two assumption one was from
the applications point of view there are
many database states which are
equivalent and it doesn't really
perceive what how they are different and
the second that communication is
expensive which is a very mild
assumption and is true and many
scenarios and again I'll request you to
humor me for now and i'll come back and
identify a class of applications over
which these two assumptions hold so
assuming that these two assumption hold
the key idea is to lazily synchronize
distributed state and by doing this lazy
synchronization we can minimize the
amount of coordination without actually
sacrificing the consistency requirement
so the takeaway from so far is that many
applications have some flexibility in
the transactions by exploiting this
flexibility in transactions we can be
lazy and I have shown you one example in
which this laziness creates room for
optimizing and I have shown you another
case where we can exploit this
flexibility to be lazy and this laziness
would reduce the amount of coordination
required without sacrificing
consistency so that was my introduction
and the outline for the rest of the dark
is that I'm going to first present a
solution for the for how we can be lazy
and optimized resource allocation which
is the class of applications for which
it is applicable second i'm going to
show how laziness allows us to minimize
coordination and that's my project on
homey estate stasis and finally i'm
going to show some experiments so any
questions on the high-level idea so far
so let me start by revisiting the
original example which I just showed you
so I had told you that we had these
three transactions there was some
flexibility in the values which were
written and I had also made the second
assumption that there is a delay between
the point at which the transaction
commits and the point at which the
values are red and the key idea was that
we are going to delay the binding for
these values which are not read by the
transactions and this will create some
room for optimization and we can
maximize the global utility and in this
particular case it was to allow the
maximum number of transactions to go
through so coming back to my earlier
promise of identifying what this class
of application s so there are many
database applications which use
transactions to allocate yeah listen are
you look at flight reservation
applications of today they don't
necessarily commit you to a seat unless
you explicitly asked for it sozin
they may not be solving at the database
level already so elusive graph level so
the idea is that yes but this particular
okay the idea is not that you can do it
on okay let me rephrase it so yes you
can write custom application logic to do
so which is outside the database but
what we are claiming is that it is a
more fundamental problem and therefore
we are presenting a abstraction for all
of these applications which can use more
than that there are some interesting
issues which arise out of now that you
are executing this transaction and you
have removed some part of the
transaction error executing at a later
point what happens to the traditional
properties traditional acid properties
in some sense you're not executing it
atomically how do you reason about
isolation because now one transaction
actually can can be affected by another
transaction I don't know if that answers
your question
right so I'm going to use the word
resources as an abstraction for these
objects which are allotted and I'm going
to assume that they are represented as
data items and the database and you are
using transactions to change the state
which is associated with these data
items so this is precisely an example of
an application where those two
assumptions hold so seed ID is basically
a social seating platform which provides
social plugins so that you can basically
choose who you sit next to in a flight
it may be somewhat one of your friends
it may be you know you can specify
constraint like I want to sit someone
else from microsoft research or someone
you know another technical guy of course
you do not want to be in situations like
this another field is another area where
this kind of assumptions hole is Hotel
Reservations where you make a
reservations you don't really know which
room you are allotted you had allotted
the room when you actually get to the
check-in point and front desk apsa is
basically such a piece of you know a
hotel reservation software which as the
advertised intelligently makes the right
offer for the right caster check-in but
from the hotel's point of view they are
maximizing the revenue by allocating
rooms efficiently finally I'm sure all
of you have run into a scenario where
you have some meetings which are
scheduled and someone higher higher up
in the hierarchy schedules another
meeting which leads to a cascading a
rescheduling of meetings in this case of
course the time slots correspond to
these resources and this is usually bad
for graduate students like us ah who end
up with no sign of our advisors
yes use condom databases and read so
again going back to our solution we are
going to delay the assignment of
resources but beyond the transaction
commits so as opposed to a classical
model in which the first you first a
user requests some resources with
constraints the system assigns a
resource and then the transaction
commits now we are going to move to a
lazy model where the user requests the
resource with some constraints the
transaction commits if there is a
feasible assignment which exists and the
actual assignment of resource takes
place at some point in the future when a
deed is performed over the database that
is when Mickey Dylan won't needs to know
which seat he is sitting in and between
the point at which the transaction
commits and this seat assignment takes
place the transit the database is in a
partially uncertain state and we call
this state of quantum state in this
state Mickey has a seat but which seat
is unknown and the database which
manages this uncertainty is called
quantum database so let me first show
you at a conceptual level how quantum
database supports this lazy execution
model so let us assume a scenario in
which we have an empty flight
reservation table now Mickey's
transaction arrives and is executed as
opposed to a classical model of
execution in which the database
transitions to a single next state which
corresponds to whichever seat was
allotted to Mickey a quantum database
transitions to three possible states
that is it maintains all possibilities
one in which Mickey is sitting in 1 a 1
and rich Mickey is sitting in 1b and a
third in which Mickey is sitting in 1c
after that let's when Donald transaction
arrives Donald's transaction execute in
each of these three possible worlds and
that leads to even more number of
possibilities finally when many
transaction arrives Minnie's transaction
can only execute on two of these
possible worlds the one in which there
window seat which is available so what
we have effectively done is delayed by B
by delaying mickey seat assignment and
this we delayed it by maintaining all of
these possibilities we have allowed
minis transaction to successfully come
more formerly a quantum database is
nothing but a set of possible database
states which are reachable through
different choices made in the
transactions and you may find them
similar to uncertain or probabilistic
databases and they basically differ from
probabilistic or incomplete databases in
three main ways one we are deliberately
introducing some uncertainty and we are
doing so to enable this late binding
second we always need to maintain a
guarantee that it the quantum database
eventually results to a single state it
doesn't really make sense for Mickey to
have two seats and a third is a key
design choice which is ah from where the
name quantum database arises and that is
to keep insert uncertainty internal to
the database and let me come back to
this key design choice in a few slides
so so far I have introduced what at a
conceptual level quantum database is let
me now give you one specific way of
implementing quantum databases clearly
enumerated all of these possible worlds
is infeasible in fact there can be an
exponential number of possible boils
exponential and the number of
transactions which you are delay and
there is a literature rich literature on
maintaining these uncertain databases
god table see tables and PC tables how
do we choose this simple representation
so what we do is we partition the
quantum DDPs into two states one which
is deterministic and we and the other
which is a sequence of transactions
which have committed but whose seat
assignment or whose us whose value
assignment has not taken place
so because these sequence of
transactions are already committed the
quantum database needs to ensure or
ensure that there is a feasible
assignment of resources we do not want
Mickey to be in a situation where the
transaction has already committed and
later you see that well I don't have a
seat for you anymore so we need to
maintain some sort of system invariant
we need to maintain logical formula
which would guarantee that this sequence
of transaction can always execute so the
next question is how do we construct
this invariant automatically and in
order to do this we want to we need to
extract the users constraints from the
transaction itself automatically and
doing this in its full generality is
difficult and therefore we restrict we
require some hints from the user and we
require the user to write the
transactions as these resource
transactions and extended sequel
language which looks like this so it has
a sequel it has a conjunctive query
initially which says what are the
resources which are acceptable to me in
this case only window seats on flight
123 as opposed to a limit one word a
keyword we now use a choose one keyword
which explicitly encodes this choice or
flexibility and finally we have a
followed by clause which are all the
rights which are dependent on the
resource which is selected and these are
the rights which are going to get
delayed and are going to get executed at
some point in the future now given
transactions which are written in this
sequel form I'm going to use a
equivalent data log like representation
in which the body of the data log is
going to correspond to this conjunctive
query which is up here and the followed
by clauses will be in the head and I am
going to use the minus notation for a
deletion I am going to use a plus
notation for insertion and updates can
be modeled as a sequence of deletion
followed by another insertion
so going back to the problem of
constructing this invariant we can do it
and now in two steps first we convert
these transactions to this equivalent
data log like form and now we want to
compose these transactions to construct
a single logical invariant and we do
this by unification yeah what does your
followed by the class of sequel inside
the fall out there what is a class of
constraints what is it delete values
insert values of do you love and sub
queries and stuff okay no just just
delete sentence it's just it is instead
of atomic wedgies yes
it's probably possible to extend it
further but we haven't looked into that
right so let's say that this is Mickey's
this is the data log like representation
for Mickey's transaction this is the
data log like representation for
Donald's transaction now I construct a
eq equivalent larger transaction which
is a sequential composition of these two
transactions now you want to be careful
because Donald's transactions execute on
a database state which is obtained after
Mickey's transaction has executed and
therefore it should perceive the right
switch Mickey's transaction would have
done in this case it would have deleted
this particular seat and therefore it
results in this additional constraint
which is derived based on unification
between the heads of all previous
transaction and the body of the latest
transaction now this of course is a
simple example of how we do this
composition we have a general algorithm
for composition and proof of correctness
in the paper but I won't have time to go
into that the stock but I'm happy to
talk about it later offline so assuming
that okay so let me just point out that
now that we have this we have this
compost transaction as long as the body
of this composed transaction has a valid
grounding over the database we are sure
that this sequence of transaction can
commit so that was the original goal of
constructing this invading so how does
the transaction execute and the over a
quantum database effectively it
basically checks if the invariant which
can be if if the invariant with the
extended sequence of transaction has a
valid assignment or a valid grounding if
there is if this is so you then you
update the quantum state and you commit
the transaction but now the assignment
has not taken place if not then the
transaction robots
so finally what happens when you perform
reads over the quantum database at some
point of the time Mickey has to actually
know his seat so what happens in that
case and this go back goes back to the
design choice which we made earlier to
keep uncertainty completely internal to
the quantum database so let's say that
this is the quantum this is the initial
quantum database one in which Mickey has
both seat 1b and 1c and now if Mickey
queries am issue Mickey issues a read
query over this quantum database the
quantum database in order to keep the
uncertainty completely internal
collapses all possible balls in which a
Mickey can have two different seats
sorry it it it collapses to a set of
possible worlds over which the read
query has a completely deterministic
answer in this case it has eliminated
one of these possible worlds in general
it can actually be a set of possible
worlds and we have a unification based
algorithm which is not optimal but it
works in practice in fact the optimal
solution is actually pypy to complete
and it can be related to a completely
different problem of you know
information disclosure through views you
know at this famous paper of McCloud and
suits you so I hope if you understand
this then you now understand why we call
it quantum database there's an analogy
which you can draw to scarring or scat
that when the cat is inside the box it
can be both dead and alive but as soon
as you open the box which is in this
case issuing a read query the cat can be
either dead or alive but not both after
the query is it you take it back to the
quantum state of it stays no one said it
is so in effect what is happening is a
reed is now also changing the database
Tate it internally may be converted to
an update yeah
what is so the impact is basically you
know in order to so the whole point of
having these possible balls was to by
maintaining as many of these
possibilities I can optimize my resource
allocation right so to minimize the
impact of reeds I want to maximize the
number of possible worlds which I retain
and yet can answer the deity
deterministically Jacob function you're
optimizing very flexible so in this case
we are just maximizing the objective
function on maintaining the mass maximum
number of possible words after the
collapse we assume that that's the
default in some sense you can you can
think of applications where you would
want to maximize some other notion so
let's say that you know if you want if
you want to maximize revenue then some
possible words may be more beneficial
for you than others you need an
extension to some syntax or new syntax
to specify these early you would set
English yeah you would we don't support
that as of now but it's definitely
something which can be extended
right so the takeaway was that we
exploited this flexibility in the
transactions which are executing to be
lazy in binding some of the values which
are not read immediately in the
transaction and I presented quantum
database which basically optimizes this
resource allocation using lazy piling so
that concludes the part on quantum
databases and I am going to now move on
to homeostasis so any other questions on
quantum databases so far your comments
numbers in terms of doing it outside a
database versus what you benefit do you
get again in terms of performance are
you clean episode you gain in terms of
utility so it's not exactly in terms of
you may gain in terms of performance by
implementing quantum database inside the
database our implementation was in the
form of meditative it sits outside the
database and actually i'm not going to
show you the performance numbers for
quantum databases just due to lack of
time and have some backup slides but we
can go for them yeah but this double
utility you need to whenever rich
framework items also give it a very
special
so take your gear to the governor's
flight another whatever the hotel case
there was an explicit goal that you want
to maximize the mm-hmm so how do you
express that in your body so as I as I
why you don't support at this point we
don't support it but yes a you know if
if we are to build a real system then
that's definitely a useful add-on which
has to be supported other questions
okay so let us go back to this example
in which we were lazy yet we were
strongly consistent and we achieve this
by exploiting the fact that we are going
to allow these two database states to be
in two different states yet our to two
different states as long as they are
equivalent to each other and I i kind of
said that we are going to use the slow
build et we are going to project it to
these local treaties and all of this was
a bit abstract so in this part i'm going
to formalize and make all of this
concrete so before i do that i had also
promised that i have these couple of
assumptions and i'm going to come back
and identify what exactly this class of
applications are so let us see a few
examples firstly why is low latency
important right why do we really care
about saving on the network round-trips
now there have been a number of
anecdotal evidence which suggests that
even a hundred millisecond latency in
the course of Amazon causes a one
percent loss of revenue and usually this
figure Rises exponentially with the
added latency so clearly latency is
something which is important in order to
you know which which can directly be
related to dollar values and there are
many applications which satisfy the
previous assumption let's say online
shopping in which the data is actually
replicated across different data centers
and the flexibility is you can imagine
and I'm going to actually show you my
experiments are going to be ntpc double
over T pcw benchmark which is an online
shopping benchmark so you don't really
have to know how many items are there in
the stock exactly so there's a
flexibility in that as long as you know
they are sufficient for your order to go
through similarly in oxygen systems you
only need to maintain which are the top
set of options it doesn't really matter
what the other lower values of auctions
are
and finally this is something which
probably doesn't directly apply at least
right now but you can imagine that if
you can partition the application State
for mobile devices in which part of your
application status on the mobile you
know you are basically saying that you
can make changes to some part of the
application state which is on your
mobile device and as long as you are
doing that you don't have to communicate
to the server then you can improve the
app response time because not every of
your action is now going to require
communicating with a server so here's
the overview of our solution so in the
first step we are basically going to
analyze the application transactions to
automatically identify this notion of
flexibility and the intuition is that we
want to partition the we want to
basically identify which database states
are equivalent and therefore we are
going to partition this piece of
database Tate's into equivalence classes
and we are going to build upon a rich
literature on program analysis because
effectively the transactions as I said
initially our user programs and in the
second step once we identify these
equivalence classes we are going to
exploit this flexibility to minimize
coordination and again the intuition is
that instead of trying to enforce that
the two replicas are in completely
identical state I'm going to instead and
force that the two delicas are an
equivalent state they may be
non-identical and they're coming back to
sleep this question on escrow
transactions and demarcation protocols
and distribute to divergence control
protocols it may be a big bit vague
right now but we are a significant
generalization over each of these
techniques whatever we do a number of
other things and I hope it will be it
will be more obvious by the end of the
talk and I'll let me come back at the
end of the talk to revisit how exactly
we are different from each of them so
let us apply the solution to the voting
example right so the input in the voting
example was this set of three
transaction types the output of the fur
step would be these three equivalence
classes one in which the Democrats are
leading one in which the Republicans are
leading and one in which they are tired
and this is going to feed into the
second step and then the output of the
second step is going to be a protocol
which ensures consistency by requiring
that the replicas always stay in the
same equivalence class so there whenever
you are actually changing from one
equivalence class to the ANA to another
then the protocol is going to ensure
that that happens consistently and no
one perceives that you are in two
different states and that's how we are
going to achieve strong consistency so
with that let me dive into how we do
step one and I'll get on to step two you
later so doing this analysis and full
generality is difficult and I do not
expect you to parse this so we restrict
the transactions to be expressed in a
particular subset of the language this
is the language I do not explicitly I do
not expect you to actually parse this
let me just highlight a few key points
we assume that the database is a
collection of integers we have these I
or statements to read and write from the
database right now we support only
conditioners if then else we do not
support for loops and while loops but
for oltp transactions this is not a big
restriction and finally we have
arithmetic expressions and boolean
expressions so assuming that
transactions are executed transactions
that expressed in this language this is
how a transaction would look like right
so it has a read statement and I'm use
the hatch notation to indicate local
variables the non hat variables are
stored in the database so the read X X
hat would read the value of x from the
database into the local variable X hat
we'd why would do the same for Y and
then the transaction checks if X plus y
is less than 10 then it increments x
otherwise it decrements x
and finally it writes that value back
into the database so this is going to be
my running trans a running example for
the rest of the rest of the talk so let
us try to formalize this notion of
flexibility a bit more assume that we
have these three database states these
three all have you know different values
of excess and wise and yet from this
transactions perspective if you execute
these transaction on each of these two
ATP states it is going to produce an
identical effect the effect being
increment X by one so how can we
represent concisely this or entire set
of data be states to do so we use
symbolic tables which basically have two
columns the first column corresponds to
a partition of the space of database
states and the second column is what
effect the execution of the transaction
has so if you consider this tuple it
says that overall ddb states in which X
plus y is less than 10 executing this
transaction would have the effect of
incrementing X by 1 and similarly for
the other case now of course a
application would have multiple
transactions not just one transaction so
let let's add another transaction to the
mix it's very similar to the first
transaction except that now instead of
writing 2x it is actually writing to Y
and also I have changed the threshold
from 10 to 20 and here you can see that
that's basically the symbolic table for
transaction t2 now if these are the only
two transactions which are executed in
the application i can combine them to
construct a joint symbolic table and i
do so by taking a cross product now in
normal cases the cross product would
have four tuples one of them is
degenerate and therefore have eliminated
it and therefore it has three tuples
what does this say it basically says
that overall database states over which
x plus y is less than 10 executing
transaction one has the effect of
incrementing X by one executing
transaction t2 has the effect of
incrementing why buy one
so I basically didn't explain how we
construct the symbolic table from this
transaction so let me show you how we do
that and again we have a set of
inductive rules for constructing these
symbolic tables from the transaction
code I do not expect you to pass through
them instead let us look at an example
construction so this is again a control
flow graph for the transaction which I
showed earlier and we construct the
symbol table symbolic table in a
bottom-up manner so we start with the
last statement in this case it is a
right in which case executing only this
statement over all database States would
lead would produce they would have the
effect of assigning the value of the
local variable X hat to X and that's why
the true indicates you know that it will
have the same effect overall database
states and as you work your way backward
in this case you see that well it is
going to have the effect of incrementing
along this branch it will have the
effect of decrementing along this branch
when you see a if statement you see that
in order to take this path in the code x
+ y must be greater than 10 to take the
other path it has to be less than 10
when you see a read statement then you
basically remove the local variables and
substitute it with the corresponding
database variables you do the same thing
for a read X and finally you end up with
this symbolic table and this is exactly
the symbolic table which are shown you
earlier now the keep up we think note
here is that the symbolic table only
uses variables which are in the database
and does not have any references to
local variables because we have already
substituted these local variables with
their corresponding so when when the
word read right
so now that we have constructed these
symbolic tables let us see how we can
use these symbolic tables to construct a
protocol so again the input to the
second step is the output of the first
step in this case this joint symbolic
table and let us assume for simplicity
that we are in a distributed case in
which one of the sides has the variable
X the other side has a variable Y and
the initial states is 12 and 13 so what
the homeostasis protocol does is it
checks to which equivalence class does
my current state of the database belongs
to so in this case the values of X&amp;amp;Y
being 12 and 13 indicate that it belongs
to the third equivalence class and it is
going to use that to be a global treaty
let us assume that this there's an
efficient way of actually maintaining
this global treaty without requiring
communication and I'll come back to that
in the next step so now when I execute a
transaction what I do is basically I go
and look up what effect that transaction
t1 has in this particular equivalence
class in this case it just determines
the value of x so I can keep on
executing these transactions as long as
the overall state satisfies this global
treaty so once I so finally I will reach
a stage where a transaction may actually
cause a violation of this global treaty
at which point I recheck and establish a
new global treaty and that begins a new
round of this homeostasis protocol so
what we have done is basically we have
executed six transactions in this case
and incur the cost of only two network
latency if you had done it in a strongly
consistent manner you would have entered
six network leads latencies now of
course how many network latency is you
actually incur will depend on how big
your equivalence classes yeah when you
are in the state why closely
how you don't know what is the global
value of x right so how do you validate
the global treaty locally without
communicating right so that comes back
to the question of this magic which I
was am going to come to in the next
slide but before I do that so we have a
theorem which proves that the homie
status protocol actually produces you
see realizable schedules so of course
the naive approach to enforce this
global treaty is to require
communication it is to you know be aware
of the global state and that will
require communication and knowing the
values of both x and y at every step
which kind of defeats the whole purpose
because we'll be back in the world of
strong consistency we want a lazy
approach and to do this we basically
project this global treaty into a set of
locally and forcible treaties and of
course because we are projecting it into
this locally enforceable treaties and
this locally and forcible treaties are
working on a limited state they have to
be more conservative but we require that
these locally enforceable treaties wood
together imply the global treaty so in
this case to enforce that X plus y is
greater than equal to 21 possible set of
local GTS would be x is greater than 10
and y is greater than 10 so as opposed
to now enforcing the global treaty I am
now going to enforce this local treaties
so I keep on executing transactions
until I run into a treaty violation now
given that these local treaties have to
be more conservative this violation is
going to occur up earlier than in the
previous case which I showed you at
which point you renegotiate and
establish a new set of treaties and the
protocol goes on so of course there are
multiple possible ways of projecting a
global treaty into a set of local
treaties and if you assume that you know
something about the workload that is you
assume that you know you know that
transaction t1 is more frequent than the
other transaction and then you can find
an optimal projection projections which
are least likely to be violated
so in this case this was the sub optimal
solution of choosing 10 and 10 which
only allowed to execute allowed you to
execute for transactions as it turns out
this for this particular sequence of
transactions the optimal projection is
to have x is greater than equal to 9 and
y is greater than equal to 11 this will
allow you to execute six transactions
without a violation yeah the rights
except side is idea or something wrong
if you are writing X my focus
in this example you're writing X in one
side and why the other side yeah so you
can kind of look around it right foot
the right size intersect at Casa phone
it does cause a problem yes um so in
which case you would actually be back in
the world of strong consistency and
there is nothing which you can do now of
course and it replicated scenario all
the state is available locally but the
same problem would show up in here let's
take it what do you know so two tables
yeah not they replicated you can't
actually increment the houghton either
place your district all the rights to
one place for the region internet no no
I can make rights to both places so in
the indie in the example which I should
i was actually you know casting both
Republican and Democrat votes at both
the data centers to some subset of wars
got incremented near some subsequently
we just know the result you one of the
actual diagnosis no I yeah but that
that's kind of the whole point that the
application doesn't really need to know
the exact tally at some point when you
synchronize you are going to know that
Ali right you are going to merge these
two states so it's not that the state is
going to be you know always divergent it
is going to reconcile periodically at
synchronization points
no okay um without knowing the work load
of course you don't really know what the
optimal a global treaty should be right
yeah you can have all the workload on
one side or the other and all the
updates to all we do to wanted a
variable that you wouldn't your global
treaty wouldn't be able to adjust I
money for that right so in which case
you can have something which is similar
to the idea of you know doing
dynamically estimating what the workload
is that is that is if you know during
the day some items are ordered more
frequently in the west coast sigh in
America than in down on the other side
of the globe then you would allocate
more budget to the data center in the US
um
so putting all of this together we
basically developed this system called
homeostasis and it has a number of
components and let me just briefly walk
you through it so we assume that we are
given a set of transactions which have
to be run and then we use a compiler to
construct these giant symbolic tables of
course we do not actually construct a
single large joint symbolic table for
the entire set of transactions we in
fact use techniques from the sdd one
paper which partitions it's actually
fells work which partitions these sets
this an entire set of transactions into
groups of interdependent transaction
based on conflict graph analysis and we
construct a joint symbolic table for
each such interdependent group of
transactions we maintain a treaty for
each such group so whenever a
transaction is executed the treaty
enforcer allows a local execution if the
local treaty is not violated if it is
violated then it goes and talks to that
initiates a round of negotiation the
treaty negotiation the treaty negotiator
goes and talks to the other replicas it
merges the changes which have happened
at the other replicas since the last
since the last synchronization based on
this new state new synchronized state of
the database it constructs a instance of
a satisfiability problem and the
solution to the satisfiability problem
is the optimal partitioning of the
global treating into local treaties and
then it sets a new treaty and that
starts a new round of this homeostasis
protocol so the overall takeaway is that
there are a class of applications which
have some flexibility in their
transactions we can exploit these
flexibility to lazily propagate rights
without sacrificing consistency and I
showed you a homie status which is a
system that identifies and exploits this
flexibility to minimize communication
between different no
in a distributed or a replicated system
so with that let me present you some
experimental results ah and as I had
pointed out earlier my experiments are
going to focus on homeostasis I'm going
to but I'm happy to talk about results
on quantum databases after the top so
the goal is to evaluate the
applicability of ah yet you had a
question we're talking about the
squirrel feeding you have any
constraints on what kind of role video
you can support and are there any
guidelines as to once given the global
fede how can you translate into these
more these locally you gave an example
it was red um so the first question was
what are the constraints on the global
treaties we actually restricted the
language to the fragment which I should
just showed you and by analyzing those
transaction you can only get a certain
class of global treaties and precisely
that's going to be piano arithmetic
first-order logic now in general you
know solving satisfiability problems
over piano arithmetic first-order logic
is undecidable we use some tricks to
actually convert it into Pressburger
arithmetic first-order logic and that is
decidable as well as solvable in fact we
use XIII solver to do this which is
actually a microsoft research technology
is that answer your question
right so coming back to the experiments
are we want to evaluate the benefits of
homeostasis in a Geo replicated setting
and more precisely we want to answer the
question as to how often can actually
avoid coordination for realistic
application workloads secondly we want
to study this trade-off between how much
time we are spending and finding this
optimal projection of global treaties
into local treaties and how does that
correlate to how much savings we get in
coordination so for work y we use a tpc
w by confirm like transactions so we
assume that there are 10,000 items in a
database we are assuming that each
transaction is purchasing one to four
pieces of particular item initially the
database is populated with stock levels
ranging from zero to hundred from each
item and based on the TP CW specs every
time the level actually goes to zero the
transaction automatically replenishes
the stock level by adding a hundred new
pieces of the item and we basically run
our experiment on ec2 we use m three
extra-large instances and the system was
deployed across five different data
centers Virginia Island Oregon's Apollo
and Singapore for the first two
experiments I'm just going to use two
replicas that is going to be verging on
an island and for the third experiment
I'm going to show a you know from two to
five at the behavior of the system as we
add sites sorry so let me explain what
this graph is so on the x-axis I have
the sequence of transactions issued for
one particular item in this case let us
say item a on the y-axis on this side I
have what is the view of the stock level
from the point of view of replica one
and on the y-axis on the other side I
have the transaction latency so the red
line here corresponds to the stock value
and the green line cord
response to the transaction latencies so
let us walk through this graph from left
to right so let's say that I will start
with this value of 100 for the red line
so now I'm executing transactions
locally using the home a status protocol
and that manifests itself enter in the
Indies low latencies because the
transactions are executed locally at
this point I witness a local treaty
violation so I need to run a synchrotron
dove synchronization and that requires a
communication between the replicas which
is why there is a spike in this green
plot and of and also I witnessed a short
cliff in the red line which corresponds
to that and that is because I am now
synchronizing the state so this change
was the number of purchases which
happened at the other replicas while I
was running my transactions locally at
this point I have established a new set
of local treaties and the execution
continues locally again until I reach
zero at which at which point i replenish
my stock and at and the protocol
proceeds so how often do we benefit from
this to do this to understand this
basically this is a transaction latency
profile and the x axis I have Layton
sees in the log scale on the y-axis I
have the cumulative probability that is
what fraction of the transactions are
executing under a particular latency
value so we are comparing against to PC
which is strongly consistent and it
always takes a round trip hit in this
case 200 milliseconds and of course
there's a sharp cliff because after 200
milliseconds all transactions will will
be able to execute however and these
four lines basically correspond to
different settings of the optimization
parameter so the higher value of L means
that you are spending
more time and finding optimal treaties
and therefore you expect more number of
transactions to execute locally so in
this case almost for all four of these
parameters you see that more than
eighty-five percent of transactions were
executed locally now how does the
behavior change as we increase the
number of sites so what exactly happens
when we increase the number of sites as
I pointed out earlier when you factorize
a global treaty into locally enforceable
treaties you need to be conservative so
there's local GTS have to be more and
more conservative and if you are
factorizing it into more number of
fragments then they have to be even more
conservative so as you go from two
replicas 25 replicas your local treaties
are going to be increasingly more
conservative and therefore more likely
to be violated easily and this manifests
itself in in this a downward shift of
the inflection point which basically see
is that slightly lesser number of
transactions are executed locally and
you witness the treaty violations more
frequently now the takeaway from this is
basically even with five sides more than
eighty percent of the transactions were
executed locally so with that let me
mention some of the related works there
has been quite a bit of interest in the
database community yeah I'm sorry so
good I probably have a backup slide on
that we did have an experiment on that
I'm happy to show that that to you
offline the inner cell communication do
is
so between so of course that depends on
which two data centers we are talking
about it ranges between 100 milliseconds
between East Coast and West Coast
actually around 85 milliseconds to more
than 250 milliseconds between Virginia
and Singapore caption so when you're in
the 10 millisecond range everything is
local yeah so it's and then you have
this
Chief Justice and as as soon as the
communication as soon as you start
renegotiating then into cross datacenter
communication yes that's what yes
you know um well that's when you get
these jump and the x-axis yeah yeah so
what happens so you're going to actually
do better you can do some you can use
some anti entropy protocol which runs in
the background and periodically
reconciles the state between the two so
that you can eliminate some of some of
these local treaty violations however
you cannot eliminate them completely
because you know whenever your
transition across an equivalence class
boundary that has to happen consistently
yes so all you're really doing is you're
just looking for better consistency of
reeds so what's going on just trying to
fit because if the updates commute then
you can do multi-master replication with
impunity and you don't need to worry
about cross
database delays
as long as the updates eventually reach
their destination or show however I
showed you in the pudding example where
yeah so yeah you were right earlier when
you said that we are doing read as we
want to mention and Shari transistor yes
well because his reordering transaction
sort of requires read consistency at
least at that point right but you can do
it incremental you could have saved
yourself a lot of renegotiating of
treaties by by simply incremental they
sending sending the updates from one
side to the other sort of all plot if
you will or get background so in in this
particular case you would you would
actually not witness this this
particular latency you don't do this
it's across the back and they if you
were if you were willing to soften your
requirements about updating so that you
had to had reordering by saying any time
anything less than 10 you'll be order
you can also soften despite that
so going back to the related work that's
been quite a bit of interest in this
field in general but we are the first
ones who actually adapt the consistency
which the data store provides to what is
required by the application and we do so
by doing this program analysis whatever
we are also the first one who tried to
adapt based on the workload which the
transaction based on the transaction
workload which none of these other
protocols do whatever we these other
protocols always assume that you are
given with a simple constraint which is
like an equally inequality constraint
and you want to maintain that constraint
our protocol generalizes what this class
of constraints is as well as allows you
to switch from one constraint to another
only that has to happen consistently
there's also been some work in the
programming languages community for
program analysis and automatically
identifying atomic sections and also in
systems community to assert a formula in
a distributed manner so with that let me
summarize ah so the key idea is that we
want to exploit flexibility in
transactions and I have identified
classes of transactions where such
flexibility is available to be lazy when
possible and I showed you one example in
which this laziness created room for
optimization and I presented quantum
databases as a system which does that I
have showed you another instance where
this laziness minimizes the amount of
coordination required without
sacrificing consistency and I presented
home your status which provides such
semantics based adaptive consistency so
let me mention something about what I
plan to do if I get an opportunity here
at Microsoft Research one of the
interesting things which I want to
pursue is can we synthesize concurrency
control
protocols automatically now assume that
you're given a correctness criteria in
terms of let's say one copy serialize
ability or few serialize ability and you
are given some information about the
environment what kind of hardware
support you have what is efficient what
is not efficient some specification of
the environment then can you automat can
we automatically synthesize the best
concurrency control protocol and that's
been quite a bit of recent very
interesting work in programming in
program synthesis some of it from
microsoft research itself and it would
be really interesting to investigate how
we can apply some of those techniques to
automatically synthesizing concurrency
control protocols the other direction of
a research which would be interesting to
pursue is a you know no knob cloud
services now as computing moves to the
cloud managing cloud services by
administrators becomes increasingly more
and more difficult so you would want to
have a system in which the clouds some
somehow automatically detects
performance anomalies or any other sort
of anomalies and takes actions to it
takes actions itself as far as possible
to remove any kind of anomalies and the
first step in this is of course
diagnostics and we have done some
initial work with question on this of
course the interesting question is once
you have identified what these anomalies
are with some high-level idea of what
the reason is can you close the loop and
automatically improve the the
automatically take actions which improve
the performance of the cloud service so
i talked about homeostasis and quantum
databases today I have also worked I
have also done something you should work
on the Utopia project which is about
designing declarative abstractions for
data
coordination you may have heard of
entangled queries and transactions so
the key idea there is basically with
with the rise of social networking you
would want users would actually want to
issue transactions which can now talk to
each other and take joint decisions and
we designed abstractions which allow you
to do this in a clean and efficient
manner finally I have done three
internships one with question in which
we initiated this new project for robust
diagnostic for cloud platforms I have
done to other internships one and
actually both of them in Google research
in the Fusion Tables team for the first
I've worked on spatial query processing
and the Fusion Tables back end in fact
if you you if you have used fusion
tables are you used it if you use it
today it's very likely that my code is
executed and the back end and in the
next internship I worked on faceted
navigation for data exploration and I'm
happy to talk about any of these
projects and the one-on-one meetings
which I have so with that that concludes
my talk thanks a lot for attending I'm
happy to answer any other questions
which you may have
hey any more questions
alright let's select the speaker again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>