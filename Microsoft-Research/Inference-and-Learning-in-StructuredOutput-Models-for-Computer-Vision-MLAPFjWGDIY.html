<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Inference and Learning in Structured-Output Models for Computer Vision | Coder Coacher - Coaching Coders</title><meta content="Inference and Learning in Structured-Output Models for Computer Vision - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Inference and Learning in Structured-Output Models for Computer Vision</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MLAPFjWGDIY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi it's my pleasure to introduce drew
rata he is a research assistant
professor at TTI Chicago last spring he
spent some time as a visiting researcher
at MIT with Bill Freeman where he's
working on learning layton structural
svms before that he also interned at MSR
see with bush meat and and before that
he was a PhD student at CMU underneath
the he was being advised by suitland
Chen and there he was working on Co
segmentation using interactive methods
up images Thanks thanks Larry thank you
all for coming so I gave a talk here at
MSR a couple years ago and I think one
of the first things I said was it's
really nice to be here that Microsoft
and one of the things I've always wanted
to do was give a talk at Microsoft with
a mac that was two years ago I've been
doing that since i'll be giving a talk
with a mac this trip i forgot my my
dongle at home and so yesterday i had to
go to an apple store and buy that so I
guess Karma's got its way for me making
that remark but okay so let me begin so
I'm if I had to summarize you know
machine learning 20 years ago or what we
were doing in machine learning 20 years
ago this picture essentially summarizes
what we were interested in right we were
interested in in partitioning two
classes and finding best ways of
partitioning two classes right faces
from non faces digit 3 from digit six
chairs from tables right this this is
the canonical thing that we were
interested in and if I had to say what
changed in the last 20 years I would say
you know things that change was now
we're interested in much larger output
spaces we're interested in exponentially
large output spaces and I'll show what I
mean by that so segmentation which is a
problem that that I'm interested in you
are given an image maybe some some user
scribbles your output space is either at
each pixel a binary label 01 foreground
background or some K set of labels that
you know exists in the database so the
space of possible outputs is the number
of labels to the power of number of
pixels right that's the output space
that we're essentially
searching over or in object detection
where to find an object we work with
parts based models so we say a bicycle
is made up of maybe a wheel maybe a
bicycle rim what a person is made of
these parts we search for these parts so
when are these parts located and then a
spatial reasoning so the output space in
this case is the number of of pixels
which is possible locations of these
parts raised to the number of parts
right again an exponentially large
output space that we have to search over
this can be not just in a single thing
but in video so you're trying to do a
person layout in in video so certainly
the exponent goes up by the number of
frames that you're dealing with I mean
in super resolution one of the one of
the early one one of the fundamental
models that way that we have is is a
graphical model of it says you know I'm
trying to resolve this image into a much
higher resolution what I'll do is I'll
collect a patch of low and high res
images low and high res I'll collect
dictionary of low and high res patches
and what I'll say is that for each input
patch I'll find the closest low res
patch in my dictionary i'll replace that
by the high res patch right so the space
of output the space of output that
you're searching over is basically a
dictionary size raised to the number of
input patterns right so and again an
exponentially large output space this is
not just vision people in NLP are really
interested in in these problems of
dependency parsing so there's a parse
tree that tells you this word modifies
this other word in the sentence right so
it's a directed tree on the space of
sentences on this on the space of words
how many are directed trees can I form
it's again send n to the n minus 2 well
that's the sentence so that's the space
that you're searching over and it could
be information retrieval right so I I go
on my favorite search engine Bing and I
search for some documents and the space
of output space in this case is your is
document factorial right that's the
space of rankings that I have
dissociable right so insubstantial we're
on this side of machine learning if we
if we have to search over exponentially
large output spaces then we need to
revisit the sum of those same issues
that we revisited what that we addressed
for the to label case we have to
understand
do we how do we hold distributions over
exponentially large objects so my
running example would be segmentation
because it's it sort of makes sense how
do I hold a distribution over the space
of all possible segmentations which
contain you know checkerboard
segmentations the stuff i am interested
in all white all black you know all
possible segmentations given this model
how do I perform influence in this model
how do i how do I find the most likely
segmentation I am interested in and
learning which is how do I learn this
from google data right no no expert is
going to be able to hand this to me all
right so so my work has been on all
three of these of these issues with
structured output models and today I
want to talk about a couple of things
which is essentially I'll talk about
this one work which we are calling the M
best mode problem and I will go into
details of that in the second part so
the first part will be a modeling and
inference question the second part will
be a pure inference question so and you
know time permitting maybe there will be
some other teasers of things that I'm
working on so so let's get started so
here's here's the problem that I that I
want to talk about this this is what
we're calling the M best mode problem
and in order to to tell me it is not for
me to tell you that problem let me give
you a model and the model that we are
going to be working with is a
conditional random fuel just so that I
find you the notation we're given an
image there are some variables so let's
say pixels I'm showing a grid graph
structure but i am not making any
assumptions on the graph structure this
is just a running example each pixel or
node in my graph I have some labels that
I have to assign to so this pixel might
be a car I rode a grass or a person it's
okay set of labels also I'm handed an
energy function which scores all
possible outputs it so somehow it gives
me the cost of each output and that is
represented by a collection of node
energies or local costs so it might be a
vector of 10 10 10 0 hundred so because
you're minimizing cost this this
variable prefers to be the third label
which is gross right also at each edge
I'm holding some sort of distributed
Flyers so nodes that are adjacent to
each other in this graph tend to take
the same label so this is just showing
that if you take the same label your
cost is zero if you take different
labels your costs 100 right
it's just encoding my prior information
and now the problem I am interest and
you know instead of thinking this as a
cost function you can think of this as a
distribution if you just exponentiate
the energy and you factorize it out by a
constant this becomes the distribution
that I was showing that's fairly
straightforward it it's a discrete space
so you can sum it out and it's really
easy to come up with it so that
summation might computing that summation
might be really hard but it's really
easy to think of this as a distribution
now the task that we're interested in
typically is given this distribution
find me the best segmentation under this
distribution right so and that can be
expressed as given this cost function
find me the lowest cost assignment of
these variables I have some node
potentials edge potentials find me the
lowest cost assignment that's my best
segmentation right and this is too
general the problem in general it's NP
hard I can reduce max card to this I can
reduce vertex coloring to this there are
in approximate problems that I can
reduce to this so this is not in the
general case this is hopeless and you
know faced with an np-hard problem you
bring out your standard dual set you
either write down your hero stick
algorithms greedy algorithms or some
convex relaxations but before we do that
before this turns into an optimization
talk let's let's think a second is
computer vision really an optimization
problem or is it just an optimization
problem right if you did have an Oracle
that could solve this optimization
problem would you be happy would
computer vision be solved and you know
we've addressed that question before and
the answer is no we've done large-scale
studies so in this paper by by Italia
met sir and an chan yellow and ya know
where they found that you know if you do
actually spend if you take current
instances or on some datasets take your
models run exponential time algorithms
and find the global optimum under your
existing models it those global optimum
still have some of the same problems
that approximate solutions do they tend
to over smooth they miss certain objects
they're not good enough in fact even
worse than that which was worked on by
by by Rick and others if you actually
find if you look if you compare the
ground the energy of the ground truth
turns out that the ground truth has much
higher end
energy than than the map right turns out
that your model thinks that the ground
truth is much less probable than this
other thing that it believes in so not
only do our models not perfect when we
spend more time coming up with
approximate or better approximate
infants algorithms we've summer move
away from the ground rule more time you
spend coming up in better algorithms
takes you away from from the ground road
in some sense and that sort of
disappointing right the reason for this
is somewhat obvious our models aren't
perfect they're inaccurate they're not
completely garbage though they have some
reasonable information in them so while
one solution to this problem might be
just learn better models right go ahead
and learn by the bottles it seems simple
enough what I'm going to say is that you
should be asking for more than just the
map you're you've learned this rich
distribution from data or from an expert
or from some source why extract just a
point estimate and then the problem that
i'm going to talk about is you know some
people have looked at this problem in
the context of of combinatorial
optimization algorithms this was called
the m best map problem so instead of
just the best solution they will find
the top k best or top M best solutions
can you one think of the problem with
this with this approach if you were
define the top M solutions what would
what is the problem that you run into
that be nearly identical any reasonable
objective function is going to have some
peak when you ask for the top M
solutions you'll be nicely clustered
around that peak and these solutions are
essentially useless for you right what
you'd like to solve is this M best mode
problem where you can do some sort of
mode hopping where you you want to know
other things that your model believes in
right and this is the problem that we're
calling the M best mode problem now I
want to be a little careful we're
working with a discrete space right so
this is not a continuous distribution
what is mode even mean and I former eyes
that in a second but before I tell you
how we can we can find the M best mode
problem how we can solve this what would
you do if you did have an algorithm that
produced some diverse set of hypotheses
what would you do with it right one
thing that you can do is anytime you're
working with interactive systems anytime
there's a user in the loop so this is
interactive segments
right a person scribbles on an image and
you present to the person the best
segmentation instead of just one best
you can present some five best but you
have to ensure that those bests are
sufficiently different from each other
there's a diverse hypothesis so anytime
there's a user in the loop you can
present those solutions and the user can
just pick one so you minimize
interaction time the other thing you can
do is you can rear ank those solutions
you can produce some diverse up our
hypothesis and run some expensive post
processing step that ranks these this is
the current state of art segmentation
algorithm that we have on Pascal
segmentation challenge what it does is
it takes an image it produces close to
10,000 segmentation hypotheses and these
are highly correlated they're they're
highly redundant but there are many many
segmentation hypotheses then it uses a
external segment ranking mechanism to
react these these segmentation
hypotheses now you might ask if I have
access to a rancor why don't I just
optimize the rancor when I search for
the thing that that would be the best
ranking thing ranking can be expensive
you want to evaluate this expensive
thing only a small number of things okay
okay so if we if we now are convinced
that you know this is an interesting
problem let me show you how to do this
this I'm going to present you the
formulation of the problem so I'm going
to be working with an over complete
representation I said that each pixel
had a variable that could take a label
some discrete set of labels instead of
representing it as a single variable I
will represent this as a boolean vector
of length K so there's a there's a
vector of length as many number of
classes that is this node is supposed to
label and an entry one in one of the
position means that that's the class
that this very building so if the
entries in the first place if the one is
in the first entry then X I is set to
one if it's the second place than X is
set to 2 right and we disallow
configurations where there are more than
one ones in the in the vector and or 0
once innovative right and you can do the
same thing for an edge of variables now
the vector just becomes much longer it
comes k square so you are now in coding
all quadratic all k square pairs of
labels that these two variables
so if you have one in the first place
that means X I set to 1 and X 2 said to
one if you have one in the second place
it means X I said to 2x j still set to
one and all quadratic of these right and
notice that these are not independent
decisions right the decisions that you
take an edge has to be has to agree with
the decision that you take it a node
this one this this encoding is saying
that X I said to do in this encoding
sinh X is set to one that's not allowed
right why do we do this why did we blow
up a set of variables the reason why we
did this is now that that energy
function of this cost function that I
showed you I can write it down as a dot
product I pull out the cost of each
label multiply that with this boolean
vector that's it exactly picks out the
cost thereof thus labeling and the same
thing at the edge so it sits nice dot
product right and here's the integer
program that you you were trying to
solve that energy minimization is just
minimizing this sum of dot products at
nodes and edges subject to mui and those
boolean vectors being boolean right
that's the integer programming problem
right this will find you the best
segmentation and in order to find the
second best segmentation here's the
simple modification that I am doing all
the variables stay the same mu 1 is your
map so that's the best segmentation that
you found and I have introduced a new
inequality that says Delta is some
diversity function that measures
distance between two labelings or two
configurations and I force that that
distance must be greater than k k is a
parameter to the problem Delta is
something that you choose I will talk
about both of those in a second but it's
just something that forces you to be
different visually what does that look
like here's what here's what it looks
like this is the space of all
exponential segmentations you were
searching over that space you minimized
over a convex hull of this space so this
is the best segmentation that your model
things that's that's the map you
disallowed some other segmentations that
lied less than K distance away from it
and now when you still minimize over the
remaining configuration you find the
second best solution
right that's what that visually looks
like so now this is the problem that
that we're interested in solving this is
the M best mode problem for this part of
the talk I'm going to assume that
somehow there is a black box that solves
the map inference problem I in the
second part I'll go into how we solve
that in but given a model there is some
algorithm that solves the map inference
problem right but this this is almost
like the map infants problem but there's
an extra constraint it so you can't
exactly plug your existing algorithms
what you can do is you can do lies this
constraint which means that instead of
handling it in the primal form in a
constraint form you add it to the
objective with a Lagrangian multiplier
which means that instead of forcing a
hard constraint now you pay a penalty of
lambda every time you produce a solution
that is not gay distance away right
every time you reduce a solution that's
less than K distance away you pay a
penalty of lung so this is the dual
problem the reason why we do this is
because this is now looking like
something we know how to handle any okay
so you're actually peel up you're
further away you are okay the more you
like yes just any qualities yes yes so
if you search over the best lambda you
will converge to the things that if
there is something that that's yeah
we'll i'll get to that in a second but
yeah the reason why we do this is
because this objective function now
starts to look like things we know how
to handle right this is an additional
term that we knew how to handle and this
in in the literature if you if you've
seen this is the lost augmented energy
minimization problem we handle this
every time we have to train SVM's or
train structural SVM's right you add a
loss and you minimize the original
energy so this is this is the problem
that i'm calling the m modes there are
still two things I haven't told you
about Delta so I've traded the primal
variable now there's a new dual variable
as well and Delta the the diversity
function that I didn't tell you right
and you can think about for each setting
of lambda which is a dual variable this
this relaxation provides you a lower
bound to the original problem that you
were solving and to get to Rick's
question you can try and maxima
is this lower bound what does this
function so I tell you a lambda you
minimize this what is this function look
like as a function of lambda it's a you
can easily show that it's a piecewise
linear concave function so you can
maximize over the space of lambda all
right so let's see what are what's the
Delta let's let's nail that down what's
the diversity function there are some
nice special cases there are a number of
special cases if your if your diversity
function was a zero one function if you
exactly our map then your distant zero
if you anything else even one pixel that
you label different your some different
configuration right if you work with
that then this is the M best map problem
it so we generalize that there are some
other generalizations that I won't talk
about there's we allow a large loss of
Delta functions and I will talk about
one of them specifically today which
will correspond to Hamming distance
right here's a delta function that I'm
going to be working with it says you sum
over each node in your in your graphical
model each each node in your graph and
you take the dot product of the MU I
with the MU I of the map what does this
mean mu is remember is a boolean vector
that encodes what label you two took at
nu I mu is your is your new variable
this is just counting how many pixels
took the same label as last time right
if you take a dot product with a boolean
vector only when they agree do you get a
1 otherwise you get a 0 right so it's
this is exactly Hamming distance why is
this interesting well it it's
interesting because if I now look at
that problem that I was trying to solve
my original energy function minus this
Lagrangian times the the loss this loss
is now linear in mu it's a linear
function of the variable that are
minimizing and it nicely decomposes
across nodes right so all that happens
is that now I have my original node
potentials plus lambda times some other
constant write mu i is a boolean vector
of length K only one of the entries is
set to 1 which is the map entry
the cost for the map label just went up
by lambda right original formulation the
UI j's are independent of your UI their
additional extra variables yes so there
are some constraints that I've hidden
away new ideas are not actually
independent there are constraints that
time you I JS to mu mu I yeah yeah yeah
those constraints exist I've sort of
just hidden them away because they
weren't relevant in the integer
programming formulation it will always
be an integer in order to solve it so
that's the black box that I hid away in
order to solve it you will relax it to
an LP relaxation yeah constraint is
possibly one way to be one is also
straighten that back
a constraint that forces only one way to
be able to be der yes yes that's also a
constraint that's hidden away in this
new so there are there constraints so
that forcing one bit to be one is sort
of a normalization constraint if you
think of it only one of the bits can be
11 after you relax it it will become an
a normalization constraint and the
constraints that they agree with each
other will become marginalization
constraints but yeah they both exist in
you just talked about second moves
second best mood yes the third and the
fourth young have different lambdas four
different ones yeah so in the in the
primal case what you have to do is you
can add different inequalities that find
me at the next best which is K away from
the force the second and the third so
you can either have different keys or
you can have some some standard setting
of case so that would say that I just
plop down this this Hamming hypercube
that disallows some K some solutions in
practice this is going to be a question
of how do i set K right and I'll get to
that question in the interactive
syllabus so then the grand chance yeah
so there's a different lagrangian for
each inequality yeah you'll have to
optimize over those here
alright so so this is nice that all I
have to do now is to modify some
potentials the cost of the so if if node
I was set to label one then the cost of
node I taking label one just went up by
lambda right and I just rerun the same
mechanism that you have for format so if
you had a black box that black box still
runs I just have to put her up the
potential Solomon even better is since I
did not modify this edge potential theta
IJ if there was some structure in your
original edge potentials I preserve that
structure so if your original problem
was pairwise binary submodular
minimization problem for which you have
exact inference algorithms this new
modified problem is still pairwise
binary some maudlin right so I if there
was some exact algorithm for the first
case there's still an exact algorithm
for the second case right and I think
that's the that's the most interesting
part if you have invested some work to
extract one solution out of your model
this can extract multiple solutions or
if you want so what does this look like
in practice so here's an image and
somebody scribbled on it so one color of
the scribble indicates this is the
foreground the other colors colors
indicates this is background here's the
ground truth of on these images so this
is what presumably you would like to
extract right we encoded this with a
with a pairwise binaries of modular pots
model with some color potentials so you
look at the color of those photons color
of the background set up node potentials
in pots one and this is this is the map
that you extract from these scribbles on
these images right this is the best
segmentation this is the exact second
best map so this is this literal
definition of second best map right does
anyone even see the difference between
the first and the second best only yeah
because there's these furious pixels
hear that get turned on the the others
are different as well maybe there's one
pixel hear the term zone but in so I
wasn't making those those those figures
up this really happens in practice that
you run your entire algorithm again and
the second resolution is essentially
useless this is the second best mode
that we can extract so the in the first
case we are able to extract the other
instance of that object
this entire thing was absent in the
first and second in this one we're able
to fill out the arm of the person that
was cut up right all by forcing Hamming
dissimilar right yeah the third row
there's not much difference sorry yeah
so the way we're solving this is instead
of setting k we're setting a fixed
lambda which means that you don't
actually enforce diversity you trade off
diversity with the original energy with
some fixed waiting term which means that
if your original model strongly believes
in the original model you will still
return the first solution back right so
here's the here's the second experiment
that we did sorry here's the second
experiment we did this is poss call
segmentation challenge for those
unfamiliar with this this is a large
international challenge has been running
for a few years now the agent the the
organization that's running this
challenge releases the images but not s
set annotations there's 20 categories on
this on this data set and a background
for each each image what you are
expected to produce is this this is the
ground truth here expected to produce a
labeling of one of these categories for
all the pixels or background which is
shown in black here right and we what we
did was we took an existing model on
this problem which is the Ale model by
the D key cozy and tall they took they
developed this over a sequence of papers
if you if you haven't seen this model
before this is just a hierarchical CR if
there are some potentials of the pixels
there's some neighboring neighboring
pixels up are joined with some some pots
models then there's some super pixels
and some scene level things so it's a
hierarchical CRF it took them a couple
of papers to develop a good inference
algorithm for this model and all we had
to do was modify some terms and run
their same algorithm again right so
here's what I'm showing an input image
here's the ground truth so blue here is
is I think boat this is sheep this is
this is the ground truth this is the
best this is the map under
their model so the first case there is
this large regions as that's labeled as
boat so the segmentation is wrong in
there you know the support is wrong in
that they have labeled it as both but
the support is wrong this is actually
one of the mistakes that the model does
tend to make whenever it finds evidence
for an object tends to smear it across
the image it so it labeled everything as
sheep and in this case it only found one
of the instances of sheep what we did
was we extracted five additional
solutions in addition to the map right
I'm showing you the best of those in the
worst of those best and worst according
to ground truth right so we are we are
checking around truth to see which one's
best in which ones would and in this
case what happens is that your
accurately able to drop out that
segmentation anytime you change
segmentation you know this is Hamming so
you get rewarded for for being different
from that in this case you you're
actually able to get out that original
support of that object get an actual
segmentation and then in this case you
are able to get the other instance of
the object so not just one sheep was
present but a second one was also
present right so these are examples
where we find large improvements in
there additional solutions that we that
we extract these are examples of medium
improvement where again this is the
ground truth this is what the map says
in this case you know there's a there's
a horse and the rider they're both
labeled as the horse in the map what we
did was extracted the horse out the the
rider out lost some part of the horse
right in this case everything was
labeled so far we were able to extract a
person out lost some bit of the sofa
here right and in this case we will
extract an object out here from the map
right these are examples of of cases
where it doesn't make a big difference
of running these additional solutions
sense for its beginning of the talking
machine purchase where they just try you
know
I'm surprised segmentation of bridges
and like save it 10,000 yeah once you do
that the same thing here it's the best
yes relatively Roundtree did you do that
like yes does I do better if you still
like even with 10,000 random examples
essentially random segmentations is it
still not do as good as your approach
this yeah so yes so so we so you're
asking whether we took the wherever we
did a rethinking on these yeah yes we
did so what I'm showing here are
comparison or our new are empirical
results so this is intersection over
Union criteria so you produce a mask
there's a ground truth mask you measure
intersection of a union that's how
accurate this mask is this is the
average over all categories here's how
well just the map under this model
performance so it's about just under 25
m best map in this case is a non-starter
this algorithm is actually more
difficult to implement from their best
mode because it's not just one map
computation again it's it's order n
where n is the number of pixels that
many computations again so in this case
we did back of an unworkable
calculations it would take us 10 years
without paralyzation to get each
additional solution so we just don't
have that and in practice it doesn't
seem to make any difference anyway we
did implement a baseline which would
look at confused pixels and flip them to
their next best thing here's the Oracle
numbers for EM best modes if you extract
five additional solutions in addition to
the map and took the best looking at
ground truth so that's cheating these
are these this number is is not a valid
entry to Pascal because you can't look
at the ground rule right so this goes
from about 24 but this tells you how
much signal there is in those additional
five solutions right you go from about
25 or so to over 36 and to tell you the
scale of these numbers the state of the
this of art on this data set is just
about 36 right so you took a model there
was no very close to state of the art
and is now beating state of the art
while beating in the sense there is a
solution in these best in these five ish
so this so the goal here is now can we
read ank these solutions right can we
take these six and run that rear uncle
we have an if we have an initial
experiment on that already we've taken
the rancors from that work applied it to
these six it already improves on the
map so we are able to do better than the
map but we haven't yet realized this
potential so we're still tweaking that
rear angle to see if we can do better
intuitively it feels like it should be
an easier problem that reranking 10,000
segmentations because now it's just six
right one out of six reranking is much
easier and and we hope this number to
United 10,000 be great to see the graph
of the you know yeah 100 right now ah
check out see where how many random I
see how many random said I think I would
you leave it up to ya yeah so I don't
know so this this line is them reranking
10,000 right so this this is them the
state of the art is them but then
reranking 10,000 I don't I don't know if
there's a curve on ranking as you can go
fewer and fewer know right now we're
just using their anchored we're adding
some new features retraining it on our
own so this this is not the the latest
that way that we've been able to achieve
but just off the shelf if we take their
anchor and run it on ours this is what
that does ok there's some number between
their 10,000 and your five right yes
whether it's forcing them to generate
fewer or forcing your own general
yes two cars which is your best k there
that message yeah mercury right yeah I
don't have access to I I can't really
generate so i'll have to check their
paper to see if that curve is available
i would be so i would expect that that
that their curve that they really need
many many many segmentations because if
you do a course job because these are
all they do is they run st min carts at
where s and T they said this pixel I
assume this pixel as foreground I assume
this other pixel is background and I run
a segmentation on this and I just do
this for many locations of source and
sink right it's just completely dumb
procedure and I would I am fairly
certain that you need a lot lot of
locations of S&amp;amp;T to do this so different
mrf right yes
alright so the thing that that I'm
interested in and that that we have been
doing with this is you know we've we've
also taken pose estimation problems
where the goal is not just segmentation
the hole is not segmentation but the
goal is where is the arm of this person
where is the head of this person where's
the leg so this is a different amount if
it's a it's actually a tree graph but
your labels are locations where are
these people and there has been some
preliminary work on trying to find
multiple hypotheses we have we have the
implementation of diva four for the best
case and we'll modifying his code and we
will find the improvement so that as
well but there are a lot of other
applications that are think that can
benefit from this I think this can
really improve parameter learning as
well though the way we train our models
right now is we run a for loop you are
you use your current setting of
parameters to generate to ask it what's
the best segmentation it believes in if
it's not the ground truth you modify it
a little till it till ground truth is
what wins right and if you had access to
not just the map but also some other
modes that it believes in you can
converge much sooner right because
you're extracting other things then that
this one believes and you know there are
connections to to rejection sampling in
this but but you know just to summarize
this part of the talk I think yours
here's what I'd like you to take home
from this right you're working on some
problem right so think about think about
the problem that you're interested in
whether it's whether it's you know
ranking documents with some retrieval
setting or whether it's structure for
motion there is there is some discrete
aspect to it right and the key thing is
are you happy with the force best
solutions that you have if you don't
have the perfect models then you're not
happy with that and if you're not happy
then you should look at extracting
multiple solutions out of your model and
we're hoping that you know this can help
our additional applications as well
right are there any questions about this
part before I move on
all right okay so the next few things
will go much quicker because i'll go
into fewer details but here's here's the
second part of this talk i want to
introduce a notion i'll give a high
level picture i want i won't go into all
three of the of the things i'll just
give a high level idea and i'll go
through it we work we came up with this
with this idea focused inference and we
applied it to a few different
applications so i will just tell you
what focused influences so I told you
there's this integer program that we're
solving in the first part of the talk I
assume that there's a black box that can
solve it typically these things are
solved with for example linear
programming relaxations so you were
going to solve this linear function with
some with some discrete variables right
that was a linear function right so i
can i can take all your parameters make
a really long vector i can take all your
variables make a really long vector and
that's just a dot product and now the
constraints that i had hid in a way that
i didn't talk about before there are
also linear constraints so this is just
this is the exact form of the
optimization problem that we're studying
it's a linear linear objective function
linear constraints boolean variables
right and what you study using LP
relaxation of this problem that you
replace the boolean constraints by 0 to
1 constraints right this is a continuous
relaxation and what that essentially
involves is replacing the convex hull of
the solutions by an outer bound right
this outer bound looks like a more
complicated structure in 2d but it's
actually a simpler structure to optimize
over because this is a very high
dimensional space in 2d it looks more
complicated but it's actually much
simpler and the way we solve these
linear programs is why message passing
algorithms I won't give you all the
details but essentially what this
involves is you localize each part of
the graph solve a problem on that part
of the graph exactly and then you pass
messages here's what I think my neighbor
should be and that convert that takes
you to the linear programming solution
so you can interpret these messages as
dual ascent algorithms right but in a
sense this is a highly inefficient
procedure what we wanted to do this is
this is what is done right now and what
we wanted to do was improve this
procedure because our observation was
that data does not look like this we
don't have complex
at all scales at all nodes at all
relative locations right our data really
looks like this there are regions of
complexity but there are large regions
which are essentially simple I can look
at the local potentials that I know what
it's going to be right and if I had to
give you an analogy I would say that the
first approach of passing messages
everywhere is sort of like a carpet
bombing approach to influence
indiscriminate deployment of computation
right everywhere and what we'd like to
see is a more focused deployment of of
computation where you find the important
parts of the problem and you only focus
your computation there right so here's
the here's the key hammer that that'll
explain really simply we were we were
going to solve a linear program
corresponding to the primal linear
program there's a duel 1 this is LP
duality theory tells you that there's a
dual lil your program in terms of the
Lagrangian multipliers of your problem
we know from from duality theory that
the primal decreases the each setting of
the dual gives you a lower bound so the
deal increases as you as you spend more
computation if the two meet at any point
of time you know you've converged you've
solved the problem exactly moreover
because these are linear programs you
know that complementary slackness
conditions are exactly conditions that
you can check for convergence when these
conditions are satisfied you know that
you have solved your original problem
right in our work what we essentially
did was instead of using them to check
the convergence we use them to guide
where messages should be passed we use
them we distribute them at various
locations in the graph and tell you
which are the important regions in the
graph right these conditions distribute
nicely over there diagram and that's the
most important concept here that's
that's the key hammer right and we were
you know in a couple of papers what we
were able to show was that we can say
some precise theoretical things about it
we can say that this is a generalization
of complementary slackness conditions we
can say that you know it's exactly a
notion of distributed primordial gap at
any point of time you have a primary
your gap your best primary have a best
deal and the sum of these scores some to
that primarily of the gap and these are
really too cheap to compute constant
time for each edge you can compute this
score so it's not like you have to spend
our computation to compute this score to
help computation right it's really cheap
so we applied this idea to a few
different things one was distributed
message passing how do we speed it up so
in this case here's an image here is the
current segmentation you update this
segmentation somehow so in this case
let's say user says all the white pixels
here or where your modern E has been
updated so a user says you know I think
this is background or you might have
data coming in streaming for the next
frame so the model has changed
everywhere and that's you get the next
segmentation but this is the this is the
key result here this is what would
happen the primal the dual and the
primal if you were to pass messages
everywhere to go from here to here and
this is what happens when you use our
method to find the important parts of
the region and only pass messages there
you essentially converge much sooner and
then and the x axis notice is in log
scale here we're converging 350 times
soon what is this baseline that I'm
talking about that's the tr WS
implementation of vlad medical ma grown
and so if you've if you've played around
with that implementation it's an
extremely efficient implementation and
not an easy one to beat in this case
we're able to beat it by this big margin
and in the other case we're also able to
beat it by a big margin the reason why
I've able to beat it is precisely this
figure right here it's showing you where
we passed messages so the white pixels
are where potentials were updated small
number of updates large number of
updates but it only passes messages
where things really matter where
segmentations are changing between the
two frames right and that's exactly why
you convert so much sooner start off
higher
I think what's happening is that so
what's happening is that this is this is
log scale and I'm zooming into the
region that's closer to convergence so
it might be worse off initially that
it's beating to convergence you can
alternate between the two you can start
off here and then at some point of time
switch to the other algorithm this
algorithm the baseline what is doing is
it's passing messages horizontally and
then vertically so it makes initially it
makes big improvements but later it gets
stuck in this process where it has to
pass lots of messages our stuff is
finding edges where messages need to
pause and and only passing there so
initially it doesn't make a lot of
improvement I'd because it keeps keeps
passing messages locally so there's some
things segmentations have to change so
somehow this node has changed it needs
to let its neighbor know at its neighbor
low so it takes a lot of time initially
but it converges much sooner because
you're only focusing on those so you
know the classic at least that style
crosman trw has a horizontal and
vertical sweet people look at Marco
earlier whatever you want to call it
based techniques for the provocation
looks more like it's happening you know
like multiple scales yeah um not that
I've seen I mean so so scheduling which
is what I'm presenting isn't certainly
knew I mean people have looked at
scheduling before and some things that
people have looked at is let's see where
the messages are changing more right so
if the last time you said sent this
message this time it's essentially the
same then maybe you shouldn't send this
message maybe somebody else should send
this message I I have not yet seen
hierarchical so I know Pedro looked at
this a little bit where but that's
essentially constructing an hierarchical
graph like you have to reconstruct a
different graph it's quite hard you have
to construct it to graph because there
are no connections about your job yeah
it's not
a sore neck zileri graph that is
supposed to mimic a lower resolution
version yes in whatever that means right
it could be used as a hint graph right
nights or propagate stuff up and then
Mercer level and then probably not but
that's down and you have to say well did
my LP at the fine level kind of move
forward faster because the information
number yeah and so you know raster order
propagation is extremely efficient if
you really have a trade right yeah
that's all but in general it's not a bad
heuristic as you know in the linear
system solving literature there's stuff
called alternating Direction implicit
which is decades old but was proven to
not work nearly as well as multi grade
right and now the algebraic multigrid
you know the idea of creating coarser
spaces that adapt to the actual
intrinsic connected alien life and it's
something I'm very interested I've only
worked on the linear which would
it could be equivalent
quadratic energy certain gutsy mrf
version that's all I've worked on the
mat I'm going to start working on a per
article for general its problems yeah
yeah I think I think that would make a
lot of sense because that way you can so
peep yeah that way you can make large
regions flip their labels by just going
one day off yeah I'd be worried about
your this method you have a large region
is that it would never will choose that
region to actually update message
well have a little bit you know it's a
little bit of GIS at a problem so one
thing that I showed was that this
scoring function the way I wrote down
the LP and I said look I can score every
edge our formulation extends to scores
over regions so you can you can compute
scores of a large regions as well so
even if every Edge has a little bit of
score the some might still be the most
important part and so you might decide
to go up if you if you had written down
no hydraulics yeah yeah but if you
haven't written a hierarchical then
you're kind of stuck then easy like that
always the speaker kind of blood yeah
actually never yeah but but yeah so in
order for them to be below threshold
there has to be something else that's
always winning right there's this big
edge that thinks here's where the most
problem yeah yeah all right so you know
these things really help you can you can
make things really faster in fact we
applied the same idea to another
direction we said you know I said that
look you can compute scores on these
edges and i can tell you which ones I
important edges but I was all assuming
that the original LP was was a good LP
to begin with it's a good relaxation
this is an np-hard problem so a lot of
cases are going to look like this the
best lower bound you can extract as not
as it's nowhere near the best primal
that you can extract and so our
formulation you know it there's not a
single LP there's tighter and tighter
LPS because you can add more constraints
to eurasia linear program and our
formulation you know the the first LP
was saying that you know the that edges
are consistent with nodes that the
labeling that you give at edges is
consistent with the labeling that you
give it nodes you can you can write down
tighter LPS by saying that triplets are
consistent with with edges that the
labeling that you give at three nodes
are consistent with the lame things that
you give at edges sorry the original
knows is that freshman uij yes right
marginal yes yes
first so the triplet one you would
introduce a new variable mu IJ K so it
would be a qk cube long boolean vector
and you would force it to be consistent
with something else your original energy
is still pairwise so you don't care
about optimizing over mui mui JK is its
its objective function would still be 0
but it plays a role in the constraints
and that tightens the lp because now it
has more constraints in therapy right
but the question here is while while we
could reasonably think about adding all
edges to the original LP we can't think
about adding all triplets in n nodes
lots of triplets you can think about
tighter relaxations on four nodes how
many how many of these things that we
going to add and so if there was a way
to score it's originally a mesh frap
drive so order and spirit but I saw so
if you restrict yourself to only the
original triplets that are present in
the original graph then perhaps you can
think of adding them but long-range
triplets can also tighten your LP which
might also be interesting to add so you
can you can include edges that don't
exist in your graph but that can still
help tighten there'll be so then you
really have to consider all n cube or in
n choose three you're making a big job
because the original thing was you
encode the problem as a continuous
optimization or any future program right
we're assuming the constraints are bad
right
its exact right yeah now you're saying
let's just throw in lots of extra
constraints so that the solution proceed
faster right no but but even with these
constraints it is still the original
problem because think about so so think
about what was happening if I think
about this way what is the worst I can
do that I introduce a variable that
depends on all pixels and so instead of
being k square or k cube it is K to the
N right what are the constraints i can
add to this that it sums to 1 over all
possible labeling that you choose only
one labeling and that each of these
labelings is consistent with the sub
labelings that you have right that type
of constraint would still be a valid
constraint ELP right in fact if you
threw that in for all the cliques in
your in your tree decomposition so if if
you wrote down a junction tree of this
graph and if you wrote down as many
constraints as the tree width of that
graph then your LP is guaranteed to be
tight and we can show that that you just
need to add as many constraints as as
the tree with and then your LP would be
typed in the worst games so we are
essentially moving towards that by
adding more and more constraints and the
thing to think about is we can't add all
triplets or all those four things or all
four pairs and so if we could somehow
score these things that would be helpful
but which one should we add into the
into the relaxation can you know a
priority so when you add a constraint it
will help obviously because it will
tighten the LP but can you before adding
in there and know how much it is it is
going to help or have an estimate of how
much when you say you can add all of
them right if the triangle consists of
two real edges in the original
mm-hm plus the extra edge there's only
constant if you start with a regular
planar grid there's only a custom number
of such triangles right right and why
would you want to sort of take three
nodes that are all over the map and
you'll make up a hypothetic triangle
over those three okay good other words
it's just using locality almost as good
as using something based on though yes
right right so right so so you can you
can think about you can enumerate what
you're saying is it's really easy to
enumerate over all the local triangles
yes you can do that what about when you
go much higher four five six then that
space is becoming much larger and even
enumerated over that space becomes
becomes complicated and what what I'm
trying to say here is is there a way to
construct these clicks when I can sort
of local eyes that this is where my
primordial gap is coming from can I
directly construct such a high
erotically you can say so so all the
edges in this neighborhood have have
little bit score can I can I just add
this entire region as a click into my
relaxation and that's that's what we
looked at here and and that's that's
what that's what essentially what we did
for this problem there was an original
image we had access to a blurry noisy
version of this image this is the we set
up an mrf problem for denoising and
deconvolution this is the best primal
that we could extract out of the pair
wise linear program so what I was
showing before and this is the best
primal I can extract from the from the
triplet LP if i add triplets into this
it becomes tight this is the this is the
actual integer map so that's fine that
we can extract this from this here's the
objective function not objective
function the primal-dual gap decreasing
as a function of time as i'm adding more
and more constraints so if you add
constraints randomly so if you randomly
throw in triplets not random triplets
but if you numerate over them and
randomly throw one in then here's how
primal your gap is decreasing because
your relaxation is getting tighter and
here's if you choose using our school so
it's look it's converging to 0 much
sooner you're on on some levels you're
essentially three times faster at the
end you know this guy is converged this
guy is not even
close to convergence right that's that
you watch it selects where is it
typically typically it's selecting
things of the boundaries so it's
selecting it's selecting things that
object boundaries so like like here it
might select some triplets here so so in
a sense it is using locality of the
problem but still it's getting a one
step above edges look out based on the
actual job smooth and wrath in inherent
lurie yes or in the solution which is it
looking at this is a dentist's roof look
at the current solution and that it is
or is it more look at the original input
no I think it's looking at it's looking
at the the best solution that it can
extract and the best duel value it can
extract which is a function of this so
it's looking at both what is the best
primal and the best to all right about
ruthless but not on also yeah in this
case I was reasoning only on triplets
but the formulation extends to arbitrary
size subsist in you do they wanted to
give the threes and fours
yeah so you can construct them from
their subsets so you can score so at any
point of time you can only score the
things that exist in the relaxation so
if only edges exist in the relaxation i
can compute scores on edges and summing
up the scores of edges in a triplet i
can compute scores of edges so i can't
give you so yeah so if i had to compute
a score for a for a for a set of five
then i would have to look at all 5
choose 2 of those edges that exist now
and i would serve to some of their
scores that make sense all right so let
me try to finish up quickly I won't go
into the last part but we also took in
to take an algorithm alpha expansion
which in you know at the surface of it
looks nothing like an LP relaxation it
looks like a greedy algorithm but people
have interpreted it as as solving the
same LP and we were able to use this
idea to also speed up standard alpha
expansion by factors of 2 223 and that's
what I'll say about that so you know in
general I'm interested in extending this
to QP relaxations I talked talked only
about linear programming relaxations I'm
interested in you know a lot of the
methods I said are natively paralyzed
able so one of the things that I want to
do is have a paralyzed implementations
there's this really nice work coming out
of CMU which is GraphLab which I lets
you work at a really high level you
specify your algorithm it does all the
low-level parallelization and it's for
multi-core and for distributed settings
and there's a there's a there's a chance
that I'll be a good chance i'll be
working with carlos question scope at
CMU and taking some of my algorithms to
to graph love and I want to also look at
focused learning so trying to do this
scheduling for learning problems and i
think there's there's some interesting
scope there ok um let me show some some
teasers and I think we should be done in
like a minute or so in my in my PhD
thesis I think a lot of people here have
seen this before but we worked on this
problem of interactive co segmentation
where you have a large collection of
images and you know somebody whatever
the same object appearing in these in
these images and you can you can write
down you can build this system we built
the system where someone can just
scribble on a single image or a few
images say that this is foreground this
is the object i'm interested
and our system would go ahead and
segment that object not just in that
image we will see that but in the entire
collection of images and this you know
you have to you in this case you have to
look at all the images and so we also
looked at active learning formulations
where the system would tell you where to
go next west where should I need to see
scribbles legs this was mostly for
building an automatic collage so you use
scribbled wants to build a collage but
what we were able to do was also extend
this with with others cowardly who's who
was an intern here that people here are
familiar with people to extend this to a
volumetric reconstruction by using a
shape from silhouette algorithm so you
use that Stan you use a standard
structure from motion pipeline to find
camera settings back projector their
silhouettes back into into the 3d to
carve out a volumetric reconstruction
and this is the this is the cutest part
I'll skip this video this is the cutest
part others just got hold of himself
with a 3d printer so he was able to
actually produce these little tiny
structures from this this is these are
these were printed using a 3d printer
using our algorithm so it just a couple
of images you scribble it was able to
extract it was able to produce three
model and printed on a on a 3d printers
as well interval I've worked on some
other problems like single image depth
estimation maybe we can talk about that
if we if we end up talking about this we
have a really nice algorithm coming
that's the first max margin learning
algorithm for laplacian crfs which are
models that are really effective for
this problem but hadn't been looked at
before because their algorithms didn't
exist so laplacian CRF is a CRF where
the edge potentials have l1 norm terms
when you have l1 on terms it's not a log
linear model and so some of the existing
algorithms don't work because they make
a log-linear assumption and so we came
up with the first approximate max margin
algorithm for training some of these
things in the past have also looked at
some some retrieval problems where you
know suppose you have an image and
you're trying to find out what is the
content in this image so I give you an
image you give me a textual description
and this was an algorithm that we but
that would first segment it search for
images with respect to this segment and
then do some textual analysis on that to
answer your your query essentially all
right there's been some work on
similarity learning again also so with
that I'll stop here's here's the
quarters that were involved in some of
these things and that's it thanks
it's fixed thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>