<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session: Less is More: Nyström Computational Regularization | Coder Coacher - Coaching Coders</title><meta content="Oral Session: Less is More: Nyström Computational Regularization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Oral Session: Less is More: Nyström Computational Regularization</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-j1oeoeT8hI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
this is a final oral session in this
nibs okay so it's my great pleasure to
introduce the presentation the
presentation is about generalization
error of the common method with Nystrom
approximation
the title is less is more nice aroma
computational regularization by
Alexandre Rudy the Faro chameleon Oh
Robert
Rollins Oh Rose rose ASCO so Alexander
will give a presentation thank you very
much I'm Alessandro Rudi I'm going to
present the work I did with a fellow
Comorian oh and Laurence Rosasco let me
give a bit of introduction for our work
we started from the consideration that
while classically statistics and
optimization were considered distinct
steps in the process of designing a
machine learning algorithms nowadays a
good understanding of the inter
interaction between statistics and
optimization is crucial since we deal
with large-scale learning problems in
this context our work is a step in this
direction as you will see in the rest of
the talk our context is supervised
learning here the problem is to estimate
a function f star given a number of
examples the examples satisfy the
classical input-output relation not here
that the input and the noise are random
with an analyst tribution and f star is
a noun - i will give a short
introduction to the setting of learning
with kernels that is particularly
important for our work and then I will
state our result since we know very
little or nothing about the function of
star we want to use a model that is very
general in particular nonlinear and
nonparametric so Q is a linear function
WI are the centers where we put the
nonlinearities CI are the coefficients
of the combination and M is the number
of centers we want to use not that in or
Ravin over a nonparametric estimator M
should grow with the number of points so
now the question is how to choose the
number of poor centers the coefficients
the location of the Centers for
estimating F star properly given the
data set there is a classical answer to
this question when Q is a kernel so it
is symmetric and positive definite
function in this case according to the
representor theorem we just have to use
and centers one on each point of the
data set in this case we can find the
coefficients just by using convex
optimization so let me give an example
that is particularly important for our
context as you will see in the rest so
this is kernel regression classical
algorithm also known as penalized least
squares here we are minimizing the mean
square error on the computed on the
whole data set plus regularization term
over the space of all the nonparametric
estimators with under baterry number of
centers and the centers located
arbitrarily in the space despite the
fact that the space is huge and possibly
infinite dimensional the solution is in
accordance with represented theorem
indeed it has and centers one on each
point of the data set and the
coefficient can be found by solving this
linear system so we are interested in
kernel integration because first of all
it is simple but because it's
statistical properties and its
computational properties are very well
understood indeed if F star belongs to H
then the distance between the
nonparametric estimator and the true
function is of the order of 1 over
square root of n if lambda is chosen
appropriately so some consideration
first of all this bound is optimal so
Noah
can you prove it and it is a very
interesting result moreover it tolls for
general Colonel's in this case there is
an index of simplicity for the learning
problem and the optimal bound is
sensitive to it in this case if the
problem is very simple the bound is of
the order of 1 over N with respect to 1
over square root of n in the worst case
obviously we have to choose lambda
accordingly so even if we don't know the
index of simplicity we can again achieve
optimal bounds
if we select lambda by using
cross-validation so we have seen that
kernel regression has optimal
statistical properties and quite
complete statistical analysis what can
we say about computations well to find
the coefficients we have to solve this
linear system so it will require n
square in space because we have to build
the kernel matrix u hat and cube in time
in the worst case because we have to
invert it so unfortunately this
algorithm is not suitable for big data
for its computational complexity despite
of the fact that it's statistical
properties are very good can we fix this
problem now I'll just spend one slide on
data dependence of sampling and then I
will state our result the idea of
subsampling is to use a small number of
centers sampled at random in particular
data dependent subsampling works this
way
result we take a small number of centers
uniformly at random from the data set
then we perform again kernel integration
so we minimize the mean square error on
the whole data set but now not on the
original space age we we minimize it on
the space hmm
hence only the nonparametric estimators
with the centers we have picked before
in this way in order to find the
coefficients we have only to solve this
linear system that is way smaller with
respect to the 1/4 kernel regression and
so the complexity shrinks from a square
to an M and from n cube to an M square
in time so this algorithm
very good computational properties but
what do we know about its statistical
properties
is there a statistical price are we
going to pay for efficient computations
this is the question and our work our
result answer to both those questions a
bit of context so there are many
different sub sampling schemes beyond a
uniform sampling and from a theoretical
point of view a lot of work studied the
tragical properties of the subsampled
kernel matrix and while it is
interesting in itself it is not clear
what is the impact of this analysis on
the statistical properties of the
subsample nonparametric estimator
finally there are few words that studied
exactly the statistical properties of
the nonparametric the sub sample
nonparametric estimator but the result
are suboptimal or they work only in a
restricted setting so this is our
theorem as you can see it is very
similar to the serum for kernel
regression indeed the bound is the same
and the condition on lambda is the same
we added a condition on the number of
centers some considerations first of all
this is a proof that subsampling
achieves optimal bounds and it is a new
result moreover it achieves optimal
bounds with a number of centers that is
only of the order of square root of n to
be compared to
of centers you need for kernel
integration that is n so already here
you can see that you are not paying any
statistical price for efficient
computations moreover our result holds
even in the general case and we have the
same result the same bound for a kernel
regression that is optimal so in this
case if the problem is simple we need a
number of centers that is even smaller
than square root of N and this is a good
news so those are our technical
contributions now I give an interesting
insight with respect to the role of
lambda in them this insight is obtained
by writing our result the idea is simply
to swap the role of lambda and M so here
I just expressed the theorem by putting
M star as a fundamental quantity what
you can immediately see is that now the
number of centers controls deliv the
regularization level of our learning
algorithm so lambda in them play the
same role they are both regularizer x'
these leads to a new interpretation for
the subsampling level indeed the number
of centers is a regular irregular Iser
and subsampling is a regularization in
regularization method
okay so from this interpretation we
developed a natural incremental
algorithm for exploring the
regularization level of our learning
problem indeed we pick a center and we
compute the solution by using only that
center then we pick another center and
we just perform an update of the
previous solution we continue this way
incrementally updating the solution by
adding new pole new centers until we
reach the best number of centers so we
don't have to recompute the solution by
zero each time we just update it as you
can see if you cross validate
incremental e on the number of centers
you obtain two good results first of all
you are controlling the regularization
level of your algorithm by a
computational quantity it is the number
of centers moreover they times the time
space requirement now is tailored to the
generalization capabilities of your data
set indeed you continue this process
until you arrive to n star then you stop
this means that you are you are not
wasting any other computation that is
not needed for statistical purposes okay
here we just compelled our algorithm
mental algorithm with the
state-of-the-art sub sampling methods
that are dependent and known that a
dependent in particular random features
and fast-food random features as you can
see for the from the table we achieve
results that are comparable or better
with the state-of-the-art
so in conclusion our main result is an
optimal learning bound for data
dependent subsampling actually our
result holds beyond uniform subsampling
so you're welcome to the posture to for
more details about non-uniform
subsampling
moreover some new questions and new
perspective arise from our analysis let
me focus on the perspectives first of
all we have seen that subsampling is a
form of regularization and it is very
interesting moreover we have seen that
we can control
Nesta the statistical properties and the
generalization of our algorithm by using
the computational resources and it this
can be a good hint in the developing new
machine learning algorithms so as I said
you are welcome to the poster for more
details thank you very much
okay so before we are we are going to
take questions so the speakers with a
spot right says show please come to hear
the move on the front and make a line
okay so now we want to read like two we
want to take our questions and comments
from the pro
I so you describe your work as a work
about Colonel Ridge regression not about
Gaussian processes and I guess that's no
accident because if you were talking
about the option processes you would
have to say something about the
posterior covariance as well so I wonder
if you have anything to say about that
actually our result is for the
statistical machine learning setting so
we assume that our data are sampled from
distribution and so our result is not
for the for the statistic for the
Gaussian process setting we didn't study
that in that direction other questions I
have one question so there is a another
paper which is very related to your work
in the snips so that is also about a
generalization in a bound of a cone and
reciprocation with decimal
approximations also so if you have
checked that paper
could you let me know the difference and
this is a very interesting paper and it
works in a restricted setting with
respect to hours indeed they work with
when the input are fixed while in our
case the input is random so in our case
we can properly speak about the
generalization properties of the
learning algorithm on the questions so
is the reason that you said subsampling
regularizes because you're also
simultaneously decreasing the number of
parameters because it seems like you
should be able to improve your
generalization by using more of your
data if you use it correctly so what we
do is that we minimize the mean square
error on the whole data set but now we
are restrict in the space of our
estimators so obviously if we use very
simple estimators we are regularizing so
this is the idea behind but the mean
square error is minimized over the day
whole data set other questions I have no
more questions
could you extend your your theories to
to a situation in which the true
function is not included in the are
cases exactly good question so okay it
is rather technical and we decided to
study the case where F is in H because
the proofs are less technical so we we
thought that it could be a good starting
point for the analysis but we actually
have the proof even in the up case but
they are a bit more complicated any
other questions
if no so yeah please Sanctus speak again
each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>