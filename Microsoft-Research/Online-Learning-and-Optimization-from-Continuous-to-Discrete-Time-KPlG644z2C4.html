<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Online Learning and Optimization from Continuous to Discrete Time | Coder Coacher - Coaching Coders</title><meta content="Online Learning and Optimization from Continuous to Discrete Time - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Online Learning and Optimization from Continuous to Discrete Time</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KPlG644z2C4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I think we'll start we're very
pleased to have belief today with us our
leads doing his PhD from Berkeley where
he started out in the ee department and
his Benz of doing more machine learning
and optimization related things are off
late and you know is what sort of
touches upon many areas of interest in
our lab including game theory and
optimization so Thank You Alec and thank
you all for the invitation it's a true
pleasure to be here ok so today I'm
going to talk about some of my work on
online learning and optimization from
continues to discrete time so the stock
will be I mean that the general topic is
how can you design dynamics in
continuous time and then discretize them
to get algorithms right and both for
online learning and optimization ok so
let me say a couple of words of first
why we want to study online learning and
optimization of course online learning
is so it's the study or designing
algorithms for solving sequential
decision problems and these are
everywhere so here I'm just you know
citing a couple of examples but there
are many more than that so one example
that's particularly close to my PhD work
is actually inspired from transportation
so my main advisor is Alex buying and
his interested in transportation
problems so both modeling in control of
for example routing or highway traffic
and so on so in transportation typically
you can you know you can route people
and you have information that's arriving
in sequential way so you can model this
as a sequential decision problem so same
thing for routing on the internet
actually so it sits at a much larger
scale and so we have the same types of
problems in power networks for example
although I haven't worked personally on
this and of course sequential decision
problems are also in let's say pricing
and bidding and online advertising and
so on
alright so one particular aspect also
that I want to say about these problems
is that in many large-scale systems it's
impractical to solve this in centralized
way or even impossible so think of
routing all the internet so it's
unreasonable to assume that a particular
node has access to the entire state of
the network for example right so we have
to also think about distributed learning
so online learning that's done in a
distributed way okay so the second part
of the title is convex optimization and
of course I don't have to convince you
that convex optimization is useful in
many application areas so in particular
in data driven decision problems and so
recently you know the size of the data
has become so large that it's
impractical to do to apply second-order
methods and so this is why there has
been an active research on in particular
on first order methods accelerated
method stochastic methods and so on
right and so one particular idea that's
been emerging in the last few years and
that I will talk about today is that
some algorithms can be thought of as
discretization of a continuous-time
process right and so first why would you
care about doing things in continuous
time or why would you care about this
connection so one reason is that usually
in continuous time the analysis is much
simpler right so you can study the
system and prove its properties for
example in a much neater and much more
elegant way and then once you have the
property on the continuous system you
can try to prove it again on the
discrete system right and the second
reason is that usually it also provides
insights on the discrete process so for
example it can explain why things work
and hopefully it also streamlines the
design of new methods so once you have
an understanding of for example why a
method works in continuous time you can
generalize that process and come up with
new algorithms for example so with new
you know dynamics in continuous time and
then after we discretize and you'll get
new algorithms okay so really we need to
understand this connect
between what what happens in continuous
time and then what happens after you
discretize all right yeah yeah because
that's what you can yeah right yeah but
the real world is often much more
continuous that's also true yes it seems
like an honor than when she starts as
continuous discrete and it gives you a
natural pathway back George continuous
that if you want you to that user the
future that's a very good point yeah so
as you mentioned many systems especially
physical systems that inherently
continuous I mean they they should be
modeled as continuous time systems and
then that description is more natural so
in some other problems I mean it's maybe
it's inherently discreet but even in
those cases thinking of them as
continuous systems and then discretizing
can still be helpful but yeah you make a
very good point okay so let's dive right
into it so it's a fairly simple
organization of the talk so the first
part I will talk about you know this
phenomenon this connection between
continuous and discrete for online
learning so in particular I will talk
about the replicator de so it's an OD
that comes up in game theory and then
how to discretize it and the second part
I will talk about optimization and on a
family of OD is called accelerated
medicine so how to get them then how to
discretize them and show some simulation
and then the last part i will talk about
some ideas for for future work okay and
so don't hesitate to interrupt me at any
point you know it's it's good to make
this interactive cool so let's talk
about the replicator G so first a quick
word about online learning in games ok
so here I have hope you can read this
the resolution isn't great but so here I
have a you know fairly simplistic
description of an online learning model
so this is probably the simplest thing
we can write so it's played iteratively
as time advances you
play some action let's say from some
probability distribution XT so if your
player k and then you discover some loss
function at time T and then you incur
the loss that corresponds to that act to
the action that you played and then you
update your distribution okay so this is
the model that I will assume for online
learning okay here is one picture of it
so we can think of this as an agent or a
player k that's interacting with in with
an environment so at each step you know
it placed an action then it gets some
feedback from the environment that's in
the form of this outcome or this loss
vector and then it updates its
probability distribution so this
distribution you know describes what
what the probability of playing an
action is at the next step okay so it
updates that yeah so k is indexing
players so I have multiple players right
so here I'm looking at this problem from
the perspective of one player but of
course you know if you have a game that
has many players they are coupled and
the other players we can think of them
as part of the environment and the
coupling between these problems is
through the loss function because the
loss of one player will depend of course
on the decisions of others okay so now
if we think of this in terms of games we
can of course describe the equilibria of
the game and they usually provide a good
description of of the efficiency of the
system at steady state so it tells you
what happens once everyone is at a state
so at an equilibrium where they don't
need to move anymore right however many
real systems rarely operate at
equilibrium so you know beyond
describing the equilibria you need to
also understand the dynamics that get
you there right and so the dynamics are
important to study for two reasons you
know probably among others but first we
can think of them as a prescriptive
model so given a problem you can define
a dynamics that will get you to the
solution or get you to an equilibrium
but also the descriptive model so think
of systems with human decision makers so
maybe you know human players and they
are making sequential decisions and
you want to model how you know how they
actually get to the equilibrium right so
this is kind of the dual role of online
learning in games okay and so for this
talk my goal will be to try to define
classes of algorithms for which we can
prove convergence okay so what I mean by
this is that we should come up with a
class of algorithms such that if every
player uses an algorithm in this class
then the coupled system will converge to
equilibrate okay and then there are many
related questions that are kind of
peripheral to this so once we prove
convergence you can ask is it robust to
stochastic perturbations so for example
if I don't observe the true loss factor
but you know some noisy version of it or
maybe some bandit version of it so I
only observe one entry also so you know
built in this problem is the the notion
of heterogeneous learning it's the fact
that different players can apply
different algorithms so they're not they
are not necessarily all doing the same
thing and also in some cases you can ask
what's the convergence rate okay so
there's a lot of literature on this this
is a very small sample but this is just
to say that it's an old question so it
dates back to the work of Hannon in
Blackwell and maybe even before then so
Hernan studied this in the context of
games and he defined the notion of an
inconsistency and of course there are
many results so for some classes of
games for some class of algorithms you
can prove convergence okay so actually
there's also a lot of work for this in
continuous time so now you can model the
behavior of players so if I go back to
the previous slide so instead of having
a discrete algorithm you can describe
this using an Eau de right and this is
studied for example in what's called
evolution in populations and one
particular example is thus replicator
dynamics that I will talk about in more
detail okay and so just to have one
specific example of a game in mind I
will so let me give this example so it's
called the routing game maybe some of
you are familiar with this but let me
review it so here I copied the online
model from before so the routing game is
played on a graph so that represents
let's say a transportation network right
and then we have different players each
has some origins and destinations so
here there are color-coded for example I
have a first player that wants to go
from 0 to 1 and every player so the
possible actions is which path I can
take ok so this player has let's say you
can go through this path or this one
with this one then we have a second
player here ok and so the losses then of
the players are of course determined by
the actions of everyone so I don't give
the formal description here but the
intuition is that if there's an edge on
this graph that is used by more players
then that edge will be congested and
that will increase the cost on that edge
so that's how the coupling happens ok
between the players ok so that's the
routing game and now what is online
learning on this game so here I have a
picture of a small cartoon that kind of
illustrates this so so at iteration t I
have some probability distribution of
actions so I I want to pick between
different routes so I pick a sample a
path and then I Drive let's say to work
and then the assumption is that once i
get there i actually discovered the full
loss factor so i know the you know the
losses of the alternative paths so maybe
i can do this by talking to my
colleagues or or maybe going online and
looking at the state of traffic this
morning ok and then at the end of the
iteration I update my probability
distribution ok so that's the model
alright and the main problem again is we
want to define a class of algorithms
such that if everyone uses an algorithm
in this class we have convergence ok so
convergence to what to the set of nash
equilibria this game so let me say a
quick word about what Nash equilibria
are in this case so so one way to write
it is this equation here where L is the
loss factor X is the decision of all the
players and X star is an equilibrium
okay we don't have to go through the
details but if you
look at this you might you know this
might look familiar so this actually
corresponds to the first order
optimality conditions of some convex
functions so if if this L here function
is the gradient of a convex function
then this is exactly the optimality
conditions ok so it turns out that
that's that such a function exists and
so this game is called a convex
potential game and so now you know X
star is a Nash equilibrium if and only
if X star is a minimizer of this fiscal
vex function f right so we can think of
dynamics of players converging to
equilibrium as you know a minimization
of this potential function so it kind of
simplify the analysis in that sense okay
so this is just an illustration of what
the optimality condition is so this is
the gradient and this is any
displacement vector from X star to X ok
yeah yeah so I didn't I didn't detail
what the conditions are on this game but
for example the edge costs on the so the
costs on these edges have to be
increasing so that's yeah for the
specific routing game yeah and so what I
will say applies to any convex potential
game but yeah but not for general games
yeah yes yeah one action or is it giving
the loss
it's for all actions of that particular
player yeah so you do discover the
theatre product yeah so they expected
loss would be the inner product okay all
right so in this case I will focus on
convex potential games yeah yes I got
Nash equilibria yeah Oh what's special
here is just a game you're choosing
yogurt and you are all the games going
to be looking at the gonna be such that
you can conversion or advanced
equilibria yes so here it's a Nash
equilibrium in the sense of the
potential game so so X is a distribution
over actions so but it's still a Nash
equilibrium so they're not correlated in
this case yeah it is you know it is a
special property of convex potential
game so you cannot hope to do this for
any finite player game so yeah you can
adapt this if you if you were just to
observe the loss of the actions that you
actually took and so the typical way to
do it is that you can come up with a
with an estimate so the stochastic
estimate of the full lost vector and
then you can apply the same reasoning
that I should here so this is just a
simplified yeah okay all right so let me
talk about stochastic approximation now
so the idea is to as i said to view the
learning dynamics as discretization of
tio de then we study convergence of the
eau de and then we relate convergence of
a discrete algorithm so once we go back
to the screen time okay so first what o
de and so the one we decided to study is
the so-called replicated equation and
interestingly you can obtain it by
taking the hedge algorithm so it's given
by this simple update rule and then
taking so it's parameterised by some
learning rate or some step size era and
so if you take the step size 20 then you
end up with this o de here so that this
is
think that derivative of the probability
of taking action a is given by the
simple expression so it's proportional
to X a and it's multiplied by this inner
product so we can think of this as the
average loss minus the loss of a
particular action so if an action has
higher loss than the average then you
will decrease its probability and vice
versa okay okay so we can think of this
as there is an underlying continuous
time and you know this great algorithm
is sampling is kind of discretizing this
with steps era alright so you know in
this o de has been studied and for
example it's known that so from the game
theory literature that all these
solution solution trajectories will
converge to the stationary points which
happens to be which happened to be you
know a superset of nash equilibria they
include other points but but they
include in particular all nash
equilibria so there is hope to show that
you know they actually converge to nash
equilibria in the convex potential case
okay so how do we discretize it there is
some work in stochastic approximation
theory that tells us you know that gives
sufficient conditions for the discrete
trajectories to be close to the
continuous trajectories in a certain
sense that i will explain okay the
condition is this so if you approximate
the derivative by this this difference
here this difference equation XT plus 1
minus 60 / the step size so here on the
right hand side I have exactly a copy of
you know the derivative in the
continuous-time and i can add some
perturbation term some stochastic
perturbation so here ada is a step size
you is a sequence of perturbations and
they have to satisfy some technical
condition and so you know it looks
complicated but it's actually you know a
mild condition so for example sufficient
condition is that the cute moment of
this of these perturbations is finite
and the air SDK fast enough okay so it's
it's a simple condition to check and so
under this particular discretization
scheme which is you know very general we
can show that for convex potential
the the discrete sequence will converge
to the Nash equilibria almost truly okay
and so the way to prove this is so the
intuitive way is to show as I said that
the screen trajectories are close to the
continuous trajectories in this
particular sense so in the sense that if
you start at for example X 0 at the
trajectory you propagate the continuous
solution you will end up at some point
here but the next discrete point
shouldn't be too far okay so it has to
be in a ball of radius let's say epsilon
then you continue to do this and at
every step you make you are within a
ball epsilon okay so it's called an
asymptotic to the trajectory if for any
epsilon new pic this eventually happens
so at a large enough time you will have
this property okay and then so that's
the first step this is general for any
you know for any o de not necessarily
for convex potential Oh des I mean for
convex potential games and the second
step is to use the convexity to show
that you know combined with this you
will get convergence two Nash equilibria
okay so that's a good point and that's
why we need also this convexity right so
because all the trajectories will
eventually converge to the same point
it's fine as long as you are close to
some trajectory so okay so the the
technical details you know a little bit
more involved so you can show for
example that any limit point of this
asymptotic to a trajectory is also a
limit point of after continuous
trajectories so of the continuous flow
of the of the eau de and from there
because we know that all solution
trajectories converge to the so to the
set of nash equilibria you prove that
any accumulation point or any limit
point of the discrete sequence will be a
nash equilibrium yeah
okay so yeah that's a very good point so
hedge is obviously one one example
because that's where we started right so
we started from hedge and we took the
continuous limit but you get much more
general algorithms than this so any
algorithm that you can write in this
form is will satisfy this property so
actually it's an interesting exercise to
show that hedge satisfies this condition
it's not obvious so you basically take
you know you take the hedge update you
take the difference with this update and
you show that the difference you know
will be vanishing in this sense I mean
will not be vanishing but will satisfy
this this condition okay but you can you
can do this all right okay so i'll just
quickly mention that this channel method
unfortunately it doesn't give you
convergence rates okay because of its
generality because here you can pick any
perturbation any perturbation sequence
so here let me just show a quick
illustration of what this looks like in
in practice I mean the so here I am
plotting the continuous solution
trajectory of the Eau de and then the
discrete hedge algorithm on top of it
okay so i don't know if you can see this
but so at every point here for example
here there are small circles that show
the discrete trajectory so red is
discrete and blue is continues so from
every point i propagate the OTE forward
you know at the time that corresponds to
this particular step and so you do end
up at the point that's so in the
beginning is pretty far but as you move
forward it becomes you know the
trajectories become really close to each
other okay so this is just a quick
illustration yeah
right so one is session about a certain
structure of discrete-time dynamic
staying close throughout the trajectory
to the corresponding continuous-time
dynamics and then essentially utilizing
the fact that continuous strand dynamics
also actually has good properties
exactly convex potential danger yes yes
I'm wondering so so you said what
specific to connects potential gains
were specific to replicator here so
could I take a different continuous-time
dynamics that also has the right
properties and obtain our discrete time
like this that's a very good question so
my best guess is that if you can show
that the continuous-time dynamics will
converge to some set let's say an
equilibrium set it doesn't have to be
Nash equilibria but any equilibrium
concept and that set also has nice
properties so for example here we use
that very nice convex so for example if
it's may be connected then that's enough
so maybe you don't need convexity and
then if you do this type of
discretization of stochastic
discretization that will guarantee that
the trajectories are closed then you can
prove the same thing let's not know at
least not not to my knowledge okay
alright so let me show a quick example
I'll go very quickly on this so back to
the routing game here I simulated this
with so we have two populations so an i
ran a simulation with so actually with
noisy losses and with two populations
and each population is applying the
Hedgehog rhythm with some sequence of
learning rates okay so in this case
because we're applying hedge we can
actually you know we have an estimate of
the convergence rates okay and so here's
the so the output of the algorithm so on
the left here are the the probability
distributions of both players and or
both populations and on the right are
the paths losses okay and these are the
noisy you know the noisy versions this
is one realization so you can guess that
you know they are converging so the path
losses are converging to the same value
so here there's a path that has a higher
value but you know if you look here the
the probability of that path is 0 okay
so it doesn't matter this is still in
equilibrium so we can also evaluate the
potential function which tells you how
far you are from my equilibrium this is
one realization again so it's it's noisy
but if you average you will see that you
know it does go to zero so this is the
distance to optimum you know as
evaluated by the potential as a function
of time in log-log scale so it does
decay 20 okay at the rate that's
predicted by the theory okay alright so
that was the first part so that was how
to do online learning in continuous time
and then discretize so now we're going
to do the same thing for convex
optimization and so in particular I will
talk about accelerated near descent
which is a family of methods okay so the
problem that i'm going to consider now
is minimizing a convex function and i
will assume that it is smooth in the
sense that its gradient is Lipschitz
okay and I have a subject to some
constraints okay so X has to be in some
convex closed set all right and so I
will focus on first order methods which
means that at each step I can evaluate
either the function or its gradient and
so the methods I will talk about
are so you know we can organize them in
two families so the first one is
mirrored descent and dual averaging so
they are very similar in spirit and
under some conditions they're actually
equivalent and these converge in 1 over
K so where k is the number of steps and
there's the second family of method
which is nesterov sex rated methods and
these have higher convergence okay a
higher convergence rate so 1 over k
squared and the goal and here will be to
have kind of a unified approach to
derive these algorithms in particular to
derive them in continuous time using a
Lyapunov argument so we'll see that you
know one interpretation of all these
methods is using some sort of Lyapunov
function or some sort of Lyapunov
argument and then once we get the OD in
continuous time i will show how to
discretize okay so that's the agenda for
the second part and I will you know to
arrive there to get there I need two
ingredients so the first one is the Eau
de version of marriage assent and the
second one is the OD version of nest
Ross method so let me review these two
okay so before doing mirror descent I
will talk about you know the simpler
version of it which is gradient descent
so gradient descent we can think of as a
discretization of this simple OGE so x
dot equals the negative gradient and
this you can show that this converges in
1 over T in the sense that f of X t
minus the optimal value f star is less
than a constant divided by T and so to
prove this it's the idea is to use the
appropriate Lyapunov function which in
this case is just the Euclidean distance
to optimum okay so it's the norm squared
of X 15 minus X star where X star is any
any minimizer of the function ok so now
and this is unconstrained gradient
descent so then what nemirovsky and you
didn't did when they you know when they
introduced mirror descent is actually
they said why you know why this
particular Lyapunov function so the
akkadian norm seems to be very specific
so why not ciao you know general
distance function so they replaced they
put in norm
with a bragman divergence so I mean the
the actual definition of it doesn't
matter but we can think of it just as a
generalized distance function ok so then
their approach is to start from some
energy function and then design the
dynamics to make that a Lyapunov
function so when I say they happen or
function I just mean it's a positive
function that is decreasing along
trajectories of the system ok so when I
evaluated at X of T then this has to be
decreasing in t all right and so they
they design the dynamics and the mayor
de santo de looks like this so you have
actually some dual variable Z so this is
the other point that they were making so
this here this energy function is
defined you know is a function of X but
they say ok we can actually define an
energy function that depends on a dual
variable Z so they introduce another
variable and so the the advantage of
this is that now you have a couple
system so you have Z dot equals minus
gradient which looks similar to the
gradient descent of G and then to get
the primal variable X you apply some
nonlinear transformation that's given by
this gradient of size that ok the
picture is like this so this X is your
convex feasible set so it lives in some
space e so this is the dual space e star
and the main you know the main updates
or the main algorithm happens in the
dual space so you have Z that is you
know evolving in the dual space without
constraints in fact so it's following
the negative gradient and to get the
primal trajectory you just apply this
nonlinear transformation so this
gradient of psystar we can think of it
as just a you know mathematical object
that map's the dual space the
unconstrained dual space to the to our
feasible set ok there's always a way to
construct such a function so given some
convex set I can come up with was such
an operator ok so that's the picture
that we have to keep in mind ok that's
the first thing region so how do you go
from unconstrained gradient descent to
constraint near
okay so the second ingredient is is
actually a recent work of Sue Boyd than
contest so this is in nips 2014 so they
show that Nestor offs method so nesterov
saturated method can be interpreted as
discretization of an eau de and so this
is a second order o de so in the sense
that it involved the second derivative
of x okay so to contrast with all the
will with the other methods that we've
seen so far okay and so the way to prove
this is once again come up with the
appropriate Lyapunov function so in this
case it looks like this so we don't have
to you know to interpret this but it's a
they come up with the apana function and
that's what they used to to prove
convergence this one over T squared rate
okay so however this only works for
unconstrained for the unconstrained
version of nesterov okay so the question
is how can we apply this how can we
extend this once you have constraints
and there we will use this idea from the
Ready Set ok and now this is what you do
so first you have to define the
appropriate energy function so here I'm
going to apply sort of an inverse
approach usually you start from an
algorithm and then you find the
appropriate energy function for it so
now what I'm saying is so which is the
same as what nemirovsky mu then did is
start from the appropriate energy
function and then come up with the
dynamics to make that decreasing okay so
what does the energy function look like
in this case it has two terms so the
first term is this T squared times the
distance to optimum as measured by F and
the second term is this pregnant
divergence term that we saw from near
its end okay so so one way one intuitive
way to interpret this is that this first
term here encodes the convergence rate
that you're after okay because you have
this T squared times F minus X time so
I'll explain why and the second term
encodes the constraints okay so it's a
very natural choice of energy function
and z here again is a dual variable and
so now once you
come up with the energy function you can
compute its derivative and then you can
choose the you know the Z dot and X dot
to make the derivative less than zero
okay then you guarantee by construction
of the Eau de that this is Lyapunov
function and as an immediate consequence
you get the desired convergence rate why
because by definition of the energy we
can write that F minus X star is less
than V divided by T squared it just
because this is a positive term and then
because we is decreasing this is less
than its value at zero but its value at
0 we know exactly where it is it's the
so this term disappears and this is the
bragman divergence at 0 and c 0 okay so
this only depends on the initial
condition and so you get this constant
divided by T squared okay yeah it's it's
a hard constraint set yeah yeah exactly
so so yeah it encodes it in a specific
way so this it's the choice of size star
that has to be careful you know given a
convex set you can always come up with
such a function of sight and so I have
some details in some backup slides if we
have time for that at the end of the
talk why isn't our always too because so
that provides a little bit of
flexibility in the choice of algorithm
right then so this defines the family of
methods and actually we'll see later
that the choice oh yeah so actually yeah
this is an artifact of the analysis but
in fact it seems that there's you know
the optimal value of R is not too it's
not the smallest one so yeah there is I
think you can do you know better
analysis to prove a better bounds of
this this is just a you know a bound
yeah i'll show some simulations that get
to this point yeah okay so you know so
we have we came up with this o de and
I mean it's still not very intuitive
because okz dot its following the
negative gradient that's intuitive
enough it's x tivo which means that
you're putting more weight on the on the
later gradients but X dot looks kind of
strange I mean what is this oh gee so it
turns out that you can rewrite it in
integral form so if you integrate this
equation then you can show that this is
equivalent to X being the weighted
average of this gradient of size tire of
Z okay so with with the appropriate
weight so here they depend on our so the
picture in this case would be something
like this so I apologize again for the
quality but so as a mirror descent you
have this kind of something that happens
in the primal space and something in the
dual space so nows evolves in the dual
space and it falls negative T times the
gradient and in the primal space you
first apply this mirror operator so you
get this you know this trajectory here
which is grad Phi star apply to Z and
then you do averaging on top of that to
get X ok so it's it's a very simple
interpretation in the sense and it kind
of formalizes this connection between
acceleration and averaging that's been
made before in some special cases ok and
so the next natural question is ok here
it's for a particular choice of waiting
but what if you know why that particular
choice on it turns out that you can do
this in a more general way by picking
any weighting function W so if we call
capital W its integral then the only
condition that you need is that the
ratio has to be bigger than 2 over T ok
so this put some constraint on w and
then you have to adapt of course the
dynamics so Z dot becomes this so minus
W divided by capital W times T squared
but this this increases even further the
family of methods ok so let me give a
quick example so here suppose that the
feasible set is the simplex so we go
back to you know optimizing over
probability distributions so there
there's kind of a common choice of
regularizer so you can pick side to be
the negative entropy in that case
you can actually compute sighs star and
the gradient of size star in closed form
and you can get you know everything in
closed form basically so once you apply
this accelerated marriages and 2d you
get something interesting so you get an
accelerated version of the replicator og
that we saw in the in the first part of
the talk right so the first so what
happens here it's it's another variable
Z tailed so detailed is the mirror of Z
so it's grabs I star of Z okay so Z
tailed here follows the replicator
dynamics it's exactly what we had before
with the difference that the gradient is
evaluated at X which is the this
weighted average of Z ok so it's
replicator + averaging you get an
accelerated replicator that has faster
convergence rate okay so let me now show
some numerical example so I'll show a
few of these plots so let me explain
what what I'm drawing here so on the top
left I'll show the value of the function
so here in this example I'm comparing
mirror descent so the non x-rated
version to the accelerated version so
this is the value for the apano function
so in particular in the accelerated
mirror descent it should always be
decreasing so that was by construction
and the bottom plots show the trajectory
so this is the trajectory projected on a
plane and this is a trajectory on the
actual function okay overlaid on the
function okay so let me play this so
this is just to compare the two methods
this is married descent and this is
accelerated married descent on the
simplex so this example is is minimizing
quadratic over the simplex okay in our
three just to be able to visualize and
so so as you expect this is faster and
actually if you so this is in log-log
scale so if you fit this with the line
you will get the one over K squared rate
so the Lyapunov function is decreasing
for the x-rated version it's not for the
original near descent as expected and so
another qualitative difference is that
for the accelerated version we see these
oscillations right so so it does
get faster to optimum but it oscillates
a lot around optimum right and then so
we'll see actually some interpretation
of why this happens okay so that was a
first example and so this is so this is
the interpretation so another way to
write this the system of Eau de so we
had Z and X before so now you can
eliminate the Z variable and you can
write everything in terms of X so it
looks more complicated but so in
particular it involves all these
nonlinear terms that depend on on sigh
so but at least conceptually it's
similar enough to a physical system so
here you have an X double dot term which
is an acceleration so let's ignore this
term for now let's ignore this term
right so if i had just x double dot
equals the negative gradient of F then
this would be a non linear oscillator
okay so I have some potential F and I
have a particle in that potential and
its acceleration is given by the
negative gradient of the potential okay
so of course such a system will never
converge because it's a it's a
conservative system right so the energy
would be conserved then you would just
oscillate in this potential so that's
why we have the second term here the X
dot term which we can think of as a
dissipation term so it's kind of a
friction term so it's proportional to X
dot and they're all of that term is to
dissipate energy so that you will get
eventually to the to the bottom of this
this potential now this term is more
complicated to understand so I don't
have a clear understanding of it I mean
in this form at least but it encodes the
the constraints ok so you want some how
to change the gradient of F such that as
you're getting close to the boundary or
to the relative boundary fassett you are
pushed away from it right so because you
want to stay in the feasible set ok so
that's that's kind of another
interpretation so special case is if you
are in the unconstrained case then this
term disappears it becomes the identity
and this term as I said we can think of
as a vanishing friction term ok so let
me show you quickly what
so what happens if I replace this by a
constant here so instead of our divided
by T I just have a costume so it's a you
know constant friction term and then I
very that costume so in the beginning
here that constant is is pretty high and
then so or the Alpha is pretty low so
this is our divided by alpha and then as
I increase this is the initial
trajectory so it gets to the to the
minimizer pretty fast and then as I
increase the constant we see these
oscillations occur okay and so which
corresponds to what we see in the
accelerated version of medicine right
and it's intuitive enough if you're
dissipating less energy then you will
oscillate more okay so here's another
example that so i choose i chose it in
such a way that you will hit the
boundary at some point and this
illustrates kind of the effect of of
this term here so as you're approaching
the boundary you kind of you know you
bounce back and you oscillate now in the
opposite direction okay so this is for
constant friction now if we apply this
the same thing the same simulation but
now I'm keeping the 1 over T but varying
are the our parameter and this goes back
to your question John so as you vary are
so here i start with r equals 2 then I
increase it up to r equals 50 so in the
beginning you know I start with a lot of
oscillation so let me run it run it
again okay and then as R increases we
get there faster but if you increase our
too much actually you will be too slow
right so you're dissipating too much
energy so it takes you a very long time
to get there so you know there's an I
don't think there's an optimal value of
our per se but given a particular
horizon you can you know choose I mean
there seems to be an optimal value of
our given a horizon yeah
qualitative difference i see is right so
gradient descent in East Indian
constrained case as a is a guaranteed
descent algorithm monotonically which he
smash drops accelerated gradient is not
doesn't yeah it does increase the
function value and definitely your
radiant seems to have the same property
in this case yeah what I found
interesting was it at least as you were
changing the values of our at some point
yeah do you have any understanding of so
I think I mean a naive answer is
probably from physics so we we know the
kind of the typical oscillators so for
example at pendulum so if you add the
friction term then there's always a
critical damping so there's kind of
under damped over damped and critically
damped so I think what you want is that
your system is critically damped because
you won't get these oscillations so you
guarantee that the function value is
decreasing but at the same time if you
over damp then it will be slower so this
is a more complicated system and you
know understanding what's the optimal
value of R is is higher in that case but
but an interesting question would be can
we actually adapt to the change our
right so not have it be a constant here
but actually a function of maybe of time
or you know function of the gradient of
F or something like that so that you can
adapt our on the fly yeah the analysis
of that might be very hard but at least
come up with some heuristics of how to
choose that okay all right so I realize
that we're running out of time so I'll
try to go a little bit faster okay so
okay so another question of theoretical
interest is to show that this OD system
actually has a solution because you know
it's very nice we've proved that you
know if a solution exists then it will
have the right convergence rate but of
course this is meaningless if there is
no solution and so so there is a little
bit of a challenge there because okay so
the first idea you have if you want to
prove that
exists the solution is you want to apply
something like the the cushy Lipschitz
theorem and that requires that the
right-hand side is lipschutz okay that
these are Lipschitz function functions
for any finite value of T ok this is
this is fine it's Lipschitz so even
though you know so if i fix t if i fix
an upper bound on see this is Lipschitz
but this hair blows up at Z equals zero
ok so there's a problem there and the
way around it is to define a family of
you know quote unquote smooth OD is by
replacing T by the maximum of T and
Delta and then show that for any such
smooth 2d there exists the solution and
then you can extract a converging
subsequence of the solutions that will
go to a solution of the original og ok
that shows existence and uniqueness is
it an other argument ok so at least we
know that in continuous time things are
nice and we have these interpretations
so now going back to discrete time how
do we discretize such a system actually
so the first thing you can try is for
example apply an Euler discretization
scheme and what that means is just
replace the dot by this different
difference quotient right so Z k plus 1
my ck divided by root s and same thing
with X done ok and the way to prove that
this has the same convergence rate so
you'd like to show that this also
converges in 1 over K squared is to use
the same weapon or function
unfortunately that doesn't quite work in
this form so you have to change it
slightly so this is the final version of
the algorithm so in the end what you do
is so you have these two variables so Z
and X and here I'm writing it in terms
of Z tailed which is the mirror of Z so
the gradient of psystar applied to Z so
Z k plus 1 or Z tailed k plus 1 is the
minimizer of this function and this is a
mirror descent step but with increasing
step size ok so the step size is here k
r / s s is some parameter and so I
increase that by s and the second update
is x tilt here is i minimize it's the
same thing except with the constant step
size okay and you can use also two
different regular risers here and then
so once you perform these two updates
you do a convex combination with a
particular choice of land so this convex
combination step corresponds to the
averaging step in continuous time if you
think about it so the next step when
you're doing averaging can be thought of
as a convex combination of the previous
average and the new point that you added
okay okay and so this is the the picture
in discrete time so same thing we have a
dual variable that's following negative
K times the gradient and in the primal
space you do these two steps and then a
convex combination of the two okay and
so with this slight modification you can
prove the same convergence rate in the
sense that f of this x tilt so if the
green update minus f star is less than
some constant divided by K squared okay
so the proof here actually uses the same
Lyapunov function from continuous time
but we had to make that small change to
make things work okay so let me say a
quick word about restarting now so
because of these oscillations that we've
seen you can think you know once you
start oscillating it means that you're
at some point you're going in in a bad
direction right so for example if you
think of this particle moving through
the potential if you're not damping it
enough then at some point it's going up
the this potential again right so if you
can detect that and then somehow restart
the algorithm by you know stopping that
particle and then dropping it again in
the potential then it should get to the
optimal faster right this is what the
what the restarting does so you have
some restart condition that you check
for and then if that's true then
basically you reset time to zero and
then you have to also reinitialize the
so the dual variable at the point that
matches the the current primal variable
okay so one possible
starting condition is you look at the
gradient so if the update XK plus 1
minus XK points in a in the same
direction as the gradient then that's
bad because you're increasing the
function value right so another one is a
speed restart so if your speed decreases
it probably means that you're going up
the function value again so these are
the two that I will compare here so this
is the same plot on the same function
now I'm comparing mirror descent in blue
so accelerated mer dissenting green red
is restarting with so with the speed
restart and the cyan is with the
gradient reach time okay so it does look
like restarting in practice improves
convergence so although here the speed
restarting is a little bit more
conservative so it restarts more often
but it's you know so it's slower in the
beginning but eventually it will beat
the accelerated the non restarted
version okay and so and so in if if I
run one more time so for example here at
this point you see that you know decided
to restart and move in the right
direction whereas the non restarted
version will keep oscillating right so
you do eliminate quite a bit of
oscillation that way okay so fortunately
we don't have any theoretical guarantees
on you know how much restarting saves
you but in practice it seems to work
really well all right okay so another
interesting thing is to observe is that
actually the Lyapunov function with the
restart isn't manat monotonically
decreasing anymore so that's not the
right argument to use anymore so you
have to come up with with another
analysis okay so I'll skip this this is
just another example but with a weakly
convex function so the only difference
is that now ok it only takes a few
seconds I'll show it so now you have a
set of solutions so you don't have a
single minimizer anymore so instead of
all the algorithms converging to one
point you know different algorithms will
converge to different points but it's
interesting
you do converge to some point in the
feasible set because all the theory
tells you is that f will go to F star
but you might have situations where you
know you get arbitrary close to the
solution set but you still you know
oscillate around it but this is not what
happens in practice and actually that's
another interesting question to
investigate theoretically can we prove
that there is always conversions to some
point in the in the optimizers so in
this case it's a little bit more
delicate because the in the dual space
at least the step sizes are t times the
gradient but you could argue that the
gradient maybe goes at the rate of 1
over T squared it's a norm of the
gradient you know it's yeah yeah so I do
believe that it's it's possible to do it
and to prove this convergence but it
will take a little bit of work okay so
now let me conclude with very briefly
some words on future directions so okay
one thing that I did not talk about
today is estimation of player dynamics
so i told you at some point in the talk
that i like to think of online learning
both in its role as a prescriptive model
so when when you have a sequential
decision problem that you must want to
solve but also as a descriptive model
that tells you how you know players
would behave or how they would arrive at
equilibrium so we try to you know play
around with this idea a little bit so
imagine now you have you know a system
with human decision makers and you get
to observe their decisions can you fit a
model of online learning to these
observations okay so here observing
distributions so they're their
probability distribution so here I'm
cheating a little bit by saying by
telling players to input distribution
instead of you know pick a single action
but you could do this if you had a large
number number of players then you can
get
some estimate of distributions okay so
we tried this with the routing game so
we implemented an interface for routing
game and we used it to collect these
observations and then you know try to
fit this online learning model and so we
have some preliminary so this is what
the interface looks like so they once
you login you have a picture of the
graph you have your origin and your
destination and you know you have this
interface to pick your distribution so
you put weights on different paths okay
and then you have some feedback so we
tell you how what the losses are on
different paths and what your history of
decisions were and things like that okay
so that's the interface so I think so
from this problem preliminary run of the
experiment we do they see interesting
things for example that players do
converge to nash equilibrium even though
they don't coordinate so everyone just
gets a feedback on their own losses so
okay convergence here is a big word but
they get close to equilibrium right so
you can't talk about conversion in
finite time and so there are some
theoretical questions that are
interesting there so for example can you
provide some statistical guarantees if
you assume that you know they do follow
some learning dynamics which may be some
distribution on all the parameters can
you recover that give these observations
another one is so once you have this
estimate of their dynamics can you do
optimal control on top of it so you can
think of actuating the system for
example the routing again by imposing
tolls on different edges of the network
right and so I think another fun thing
to do would be to extend this this web
application so here it's specific to the
routing game but actually there are many
I mean it took a while to actually you
know have a robust system so because we
had to think of what happens when a
player drops out of the game for example
this connection and things like that so
I think we can reuse the infrastructure
so it's a you know it's a very complex
infrastructure to come up with other
examples of games okay
so now another question I think of
practical interest is so I talked about
this mirror operator so this gradient of
size tag this is actually you know a
basic component in many algorithms both
for optimization and online learning
right so you have some feasible set X
some dual vector Z and you want to solve
this problem so the gradient of tristar
you can write it as the Maximizer of
some linear function plus or minus a
regularizer ok so this here I mean many
algorithms rely on this so they do such
an update at every step so having an
efficient computation of this is very
important so the question is which
choices of regular eyes herbs I need to
a simple expression of this there are
some special cases for example in the
case of simplex as I said when you take
the negative entropy you have a closed
form actually for this but you can also
do it efficiently for quadratics and
more generally for any F divergence
right there are other special cases for
example in the non-negative orth on
there are some some known choices of
sight or some kind of typical traces of
psy but the question is can we do this
more generally let's say for polytopes
can we do it for special classes of
convex sets you know maybe sub level
sets of quadratics or things like this
right or maybe is there even a universal
choice of sigh for which I mean in some
sense for which gradient of size diaries
is simple okay so then the next question
which is related to learning in games is
how can we do online learning on when
the action set is infinite okay so here
I'm writing again the online learning
problem except here so instead of
picking a probability distribution over
a finite number of actions and picking a
distribution over general set s okay and
so after you pick this distribution you
are revealed the loss function which is
now a function that goes from s to some
bounded interval and then I incur some
expected loss which in this case is is
an integral ok so it's an integral with
a set of the probability or of the
density X of that action times the loss
evaluated at that action so I can also
define the regret you know in exactly
the same way as in the finite case
except now the inner products are
integrals so this has obvious to
learning in games because you know we
can think of games in which the action
set is infinite so it's not finite set
and also we can apply this to other
decision problems not necessarily in
games but other decision problems on on
any set so in particular you know I'm
interested in the non convex case
because if the set is convex and the
loss function is convex then this is the
special case of online convex
optimization and there's a lot of work
on this already very interesting with so
there is less work in the case where s
is not convex and also whether the
function so I guess when the function is
Lipschitz there is work but when s is
not complex there isn't work so we
started looking at this and so it turns
out that it suffices that s is only
locally compact so you don't need this
global property so in particular if s so
locally convex means that at any point
there is a convex subset that contains
that point ok so it's it's only it's
just a local property ok so in that case
you can also come up with no regret
algorithms so but assuming that you
observe the full function ok so a
question is can you extend this to the
Bandit feedback case so instead of
observing the full function at each step
you only observe its value at the point
you played or at the action you played
so you sample some action from your
distribution and then you observe the
value of the function at that point
which is a more I think realistic
assumption because especially in in the
case of games you don't you know maybe
you don't observe the full loss function
except maybe in some very special cases
like in matrix games and so on where you
know the the matrix a priori ok and the
final topic that I'm
you know very excited about is this
dynamical systems approach to
optimization so as I said it's a fairly
recent topic and I think there's a lot
to be done so they kind of the vision
behind it is that instead of thinking of
the problem purely in discrete terms you
you can do the design in continuous time
and the reason is because you know it's
simpler because it can give you some
intuition so if said some intro we've
seen some interpretations of for example
of the acceleration and then so what's
missing I think right now is how can you
do the discretization in kind of a
principled and streamlined way because
right now it still seems to be kind of
an ad hoc process where you come up with
a node Z then you have to be very
careful in discretizing it okay and so
for practitioners maybe they don't need
to necessarily worry about this so as a
first cut you can maybe do the design in
continuous time and then you know feed
it to an off-the-shelf numerical solver
okay for that og then you just have to
think of the problem in terms of
continuous time okay and so there's
still a lot to do to develop the theory
for example in what I should study the
effect of are so as I said maybe even
come up with an adaptive heuristic for
choosing our so that our parameter the
the damping also studying restarting
heuristics and show guarantees so show
that it you know you can guarantee that
it speeds up the convergence also
generalize this to monotone operators
they see I think this is pretty exciting
so I've seen some work in actually in
continuous time Oh des that handle
monotone operators so not necessarily
convex functions but so convex
optimization is a special case because
the gradient of a convex function is
monotone operator but we can potentially
do this more generally another
generalization is composite optimization
so here I am assuming that F is smooth
but what if F isn't or F is smooth but
you add some non smooth turn and also to
stochastic methods so right now it's a
deterministic
discretization but can we do this with
stochastic discretizations of course you
will have lower rates and so on but but
I think it's also important to do this
okay that concludes my talk thanks okay
any other questions yeah yeah sorry that
this took a little bit longer</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>