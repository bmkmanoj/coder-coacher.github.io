<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Clustering for Developers | Coder Coacher - Coaching Coders</title><meta content="Data Clustering for Developers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Clustering for Developers</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6oUW9IYbhEc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged showing internet or public
display is forbidden and may violate
copyright law
you
welcome to our next dev talk today we
have James McCaffrey from the microsoft
research advanced development team James
joined Microsoft around 97 and he has
work on several groups from exchange
internet explorer and being and then he
moved to ms are starting in 2010 he
holds a doctorate from USC on
mathematics and cognitive psychology and
he's also the senior contributing editor
to msdn magazine so if you want to write
from is Dean magazine he's a guy to talk
to and with that I'll give over to James
Thank You Carlos good afternoon
everybody my name is Sir James like I
said first of all the ground rules this
is a supposed to be a more along lines
of a college lecture type thing where
the main goal is to transfer knowledge
type of things and have this
presentation useful as a resource I'm
you can ask questions at any time I have
roughly a dozen slides or so each one
should take about a minute with in
between I expect to talk to take about
30 minutes so here we go we're already
on a slide number to the agenda is I'm
going to describe data clustering a
little bit now I bet you you all have a
rough idea of what clustering is but I'm
going to try to define it more formally
and also sort of give us get us into the
mood of thinking about data clustering
and then really there's two primary
topics clustering numeric data and
clustering non-numeric data each of
those will have a demo and then at the
end we'll have some resources I'd say
the main content of this talk will be
three demos and i will point you to
complete source code at the end okay oh
yeah I guess before i leave the agenda
motivation and goals the goals are
fairly aloft in the sense that
regardless of your background now i'm
going to make no assumptions about your
background whether you're a developer
p.m. or anything like that but the goal
is free to leave have in all the
resources you need assuming that you
have developer skills to actually
implement a production quality
clustering with at least three different
algorithms and have enough knowledge to
customize it one of the things that will
see is that clustering you can't just
drop it in its not one algorithm fits
all they all need to be slightly tuned
so here we go okay yes Rob yeah pretty
much what we'll see you the question was
described when and why to use a
particular algorithm I think that's one
of the main things than any talk is you
know we have too many tools sometimes
and the real question is like when do we
use a particular tool so I'll try to
address that if I don't be sure to call
me on it more poorly I differ so my
assumption is that us as developers
writing software to solve to give a
problem we're going to decide which one
is best based on Richard
yeah it turns out it's not as a hard a
problem as you'd see because certain
types of problem constrain you to
certain algorithms so it'll be pretty
okay so here's our what is data
clustering and I see my boss sitting in
the audience so boss impress us by
telling us how many clusters there are
here probably at least three he says
three in his accent and he is correct so
we have this intuitive idea of what
clustering is but we want to formalize
it and write some code now this is
pretty obvious that there's three
clusters and the cluster membership is
fairly obvious to this will be the the
data for the first demo called k-means
clustering and these data have to a
dimensions height and weight height in
inches and weight in pounds can anybody
speculate so we've got two dimensions
here and the only reason we're using two
dimensions is it makes it easy to
visualize okay can anyone speculate on a
third dimension if each data row or
tuple represents a person what's another
dimension that we could have included
age excellent anybody else sex okay sex
is the one I was hoping for that didn't
come out quite right but what I meant
was when we when we look at a data
clustering problem problems fall into
three distinct buckets those in which
the data is totally numeric like this so
we could add age but we couldn't add sex
because that's a categorical variable
there are types of problems that are
purely categorical okay and I'll show
you an example that a minute and then
there's type with a mixed data okay so
one of rob's question was when do you
know when to use a particular algorithm
that's the first thing you do is
identify the type of data on America or
categorical or mixed now this next slide
just shows I guess my point here is
don't underestimate the power of
visualizing the data before you start
this is the exact same data but without
the visualization and I would defy you
even now you know even from the previous
slide to try to imagine how many
clusters there are so data visualization
can be very powerful pre-processing
stage I know this one the point is this
is a similar data except I rearranged it
this one it's not at all obvious how
many clusters there are or how the data
should be clustered and this is the type
of problem this is where a programmatic
data clustering comes in handy in fact I
stared at this quite some time before
the talk and I was wondering if I if
someone really did ask me to cluster the
data I'm not sure how I'd do it to me it
sort of appears I kind of can imagine to
kind of ovals one here and one here but
it's likely that everybody might come up
with a different type of clustering now
here's a another example to reemphasize
this type of data is purely categorical
although not really you got to be very
careful about this and what will
describe how you would a tackle
clustering this now this I mean that
we're actually going to do this what's
your immediate there's weird psychology
that often goes on in clustering what's
your first impression on how you would
cluster this data
one are by not the technique but just
looking at it how would you divide it up
by the way clustering is equivalent to
partitioning you know I mean
mathematicians would call partition
because you're just you've got the data
and you have to divide it up and or
partition two chunks yeah pick one of
the cons in particular though I
guarantee you that all of you in the
room are going to pick the same one
which is color I don't know why I've
asked many people stay in color just
seems like a natural thing well it turns
out that that's not the best way to
cluster this data and I also said that
this is purely categorical that's not
entirely true look closely at the data I
mean it is categorical but one of the
columns is inherently different how is
that so okay ones binary now in machine
learning that's often a big deal but
that's not a big deal for clustering I'm
sorry exactly could repeat that a little
louder there's an ordering to size um in
other words we could say what's the
difference between red and blue oh I
don't know but compare the difference
between short and medium versus short
and long we would expect assuming that
those things somehow our representative
underlying numbers so you just got to be
careful with that I don't have any great
words of wisdom other than pay attention
to the data the first step in clustering
is to analyze the type of the data so
here's just sort of a summary of what we
just talked about data clustering is
surprisingly difficult to formalize you
know to actually come up with a
definition that you can use to run an
algorithm against but typically most
clustering you know clustering is
typically a dual objective in the case
of the numeric day that we saw I won't
go back to it but basically we all saw
three clear clusters but if I asked you
to articulate why those were clearly the
clusters you'd have to think about it
for a while and you'd say something like
well the dots in one cluster close
together but also the second objective
typically is that
there's a lot of space empty space
between clusters so yeah it's a dual a
min max problem you want to minimize the
distance in a cluster and maximize the
distance between clusters which makes it
kind of a difficult problem
mathematically oh now here's one of my
thing that this talk is titled data
clustering for developers so we're going
to explicitly try to not mention much
theory but an important theoretical
notion for clustering is there isn't one
measure of clustering there's many
different measures of clustering and for
almost anyone that you can come I'm sure
there must be some weird exceptions I
haven't seen any but if you come up with
a definition of clustering the problem
is np-hard now as developers assuming
that we're all developers in here in the
audience we hear np-hard we here in
complete we hear NP this that the next
thing now most of us these do have
formal definitions but from a
developer's point of view I always try
to translate this in my head if
something is np-hard or np-complete what
that means from developers point of view
is that the only way to get the best
answer is to iterate through every
possible combination okay and in the
case of clustering this has the
exponential growth problem it's not
doable for all but trivial prompts even
when you get to something like 50 items
and two partitions it becomes
unimaginably large so np-hard
translation no matter what measure you
pick you're not going to be able to come
up with an algorithm that will guarantee
the optimal solution so that means we
have to rely on heuristics often this we
already mentioned the type of data are
different have to be handled differently
the number of clusters will often be
prom dependent I'll say this now before
I forget because it's not anywhere my
slide um if you read the literature or
Wikipedia by the way Wikipedia is really
good for a lot of things but for most
machine learning topics it's really weak
and for clustering it's really weak take
all the Wikipedia stuff on clustering
with a grain of salt
them but you'll go in and read any of
the research papers say the number of
clusters that there's two kinds of
algorithms those where you have to
supply the number of clusters upfront
and then the algorithm runs and those
that determine automatically the number
of clusters why consider that kind of a
hoax because what they don't say is
those algorithms that do determine the
number of clusters you have to supply
different information that acts as a
surrogate for instance some measure of a
bounding box or something like that
which implicitly determines the number
of things but anyway the number of
clusters there isn't some magic way to
determine how many clusters there should
be so again this is just reiterating the
two key things in any clustering
algorithm is defining clot what what a
good clustering is and then searching
through all combinations because or is
finding way to efficiently search the
combinations data this is sort of some
of the common uses of data clustering
and I have a last minute addition to
this in a second from somebody in the
building but one way to do it is it for
ad-hoc data exploration if you have data
and this would be typically sequel
server data that has some sort of I
don't know what how to describe it
exactly but it'd be like sales data or
transaction log data the data has
meaning in itself you can cluster it to
do ad-hoc exploration just find
interesting patterns okay on the other
hand and also that with the same thing
identify abnormal data points and I'll
show you that later another thing is
that data clustering is used more often
I think as part of other machine
learning algorithms so in other words
you'll be going along in particular is
working with radial basis function
networks the other day and you're going
along and part of that algorithm is
group the data together to find
representative data vectors okay so a
data clustering can be used sort of as a
top-level tool or internally to some
machine learning algorithm now here's my
last one here and this is a pure opinion
which are not shy about spouting ever um
I think that or I would consider that
machine
has three hello world problems and
presumably one of the reasons why you're
sitting here and listening today is that
it seems like every time we turn around
for a company meeting or anything we're
hearing about machine learning machine
more and more and in fact I remember
working years ago I'd heard about
machine learning but I probably never
really used it but now I'd say the
majority of things that I work on use
machine learning in some way so k-means
is one of the three hello worlds and I'd
say I'd climb in the pure opinion but
i'd say that the other two are hope I
can remember these logistic regression
binary classification and naive Bayes
classification that's opinion so anyway
no one knows no one k means sort of is
one of your membership cards into the
club of machine learning maybe does
anyone have an opinion on that because
that's opinion I like oh I don't care i
mean do you care or you like no that's
totally not true or yeah sounds good
okay good what a what a docile crowd I
was expecting a little bit more
feistiness okay so basically where we're
at now in the grand scheme of things
i'll read it we're going to talk about
what data clustering is walk through
three specific algorithms here's the
k-means algorithm which is probably one
of the most famous and all machine
learning if i had to pick one I'd say it
is the most famous luckily it's simple
and elegant now there's different names
for it because it's been around for a
long time in fact i stumbled a crow well
preparing this talk i stack chua lee
stumbled across a research paper that's
the history of the k-means algorithm so
there's research on the research of
k-means it goes on and on it's sometimes
called Lloyd's algorithm and it's also
associated with another researcher named
McQueen but we'll just call it k means
so this is the same data as before and
here it is that the algorithm couldn't
be any simpler but we'll see that you
got to be careful it goes like this
initialize the data to clusters so what
we'll do first is take the data and
assign each data point to an arbitrary
cluster now we know in advance that
there's three so we'll just pretend that
we knew that
okay then compute the mean of each of
these clusters and then update the
cluster assignments based on new mean so
juice now it's it's kind of difficult to
visualize so I harness my power point
skills here okay so here we go so here's
step one initializing the data to
clusters I've just colored in I picked a
one of three colors and the color
represents a cluster assignment okay yes
and in fact I'll jump ahead and say that
the initialization is critical we know
that we're not going to get we're not
guaranteeing a final optimal an optimal
solution and our final solution depends
entirely on the initialization it's
totally deterministic after this point
so the initialization is keen will come
back to this now the next thing is we're
going to compute the means so by that I
mean you should be able to visualize
this imagine the red dots now the mean
of a vector is just simple you just take
the average of each individual
components so where would the the center
of mass of the red ones be so I'll move
my fist and tell me when I'm getting
close to the center of mass of where the
red ones are ya in fact if you look at
it can see the Reds are slightly to the
left of center but pretty much centered
so you'd expect the the mean right there
so you can do that mathematically and
likewise I sort of faked the
initialization so that the the mean of
the yellows is right smack in the middle
and the mean of the greens is slightly
up into the right so if you calculated
them you would get that now notice that
the means and this is why this is called
k-means k is the number of clusters and
we have three means notice that the
means do not correspond to an actual
data point they're sort of like a
hypothetical data points now there are
variations of this called kami toid
where the Kami toid was instead of
computing means you compute the most
representative actual piece of data but
that's just sort of like a sideline okay
now so we just computed me
means now we update the cluster
assignment and this is sort of the key
this is the performance bottleneck of
the whole thing by the way one of the
reasons why k-means is popular and use
it it's easy to take potshots at k-means
in research oh we can improve k-means
k-means oh it's yeah like i studied that
and you know my data structures class
well it's true but one of the reasons
why it's still route is it's good it's
fast and it's efficient it's typically
i'd call it the baseline against which
all others are compared but now we're
going to walk through here's the
perforce ball now we take each point and
determine which mean it's closest to so
this point down here what color mean is
it closest to clearly read what about
this red red red red red red and these
ones are going to be pretty much yellow
yellow yellow then right here green
green and so forth but if you walk
through that now notice that that's
going to be order magnitude n where does
the number of data points it's actually
n times K what you do is you take this
first point compute the distance to each
of the three means and then assign it to
the cluster that is the shortest
distance and you get that and then now
we loop back up and we compute new means
where would the mean of the Reds be
going to go down there meany the yellows
up there and the mean of the greens
right there and then the next step we're
back up we compute the where we at we
just compute new mean so we update the
clustering assignment and we get that
and in fact you can see that we've
converged at this point there would be
no change in cluster assignments the new
means this would move over a little bit
this would move in there that would move
down there but then when we reassign
there'd be no new cluster assignments
and the algorithm would stop the dentist
room so it's almost too easy especially
when you see sort of a little animation
so here's the here's the demo and I know
you're going to be impressed
there it is okay No so this is the
actual data and there's really not much
to see here here's the raw data I am
going to show you know I I really when
I'm sitting in an audience I really hate
it when the presenter shows code because
really to me that's not you know show me
you know I just give me a pointer I want
to look at the code by myself later but
I am going to call out a few key things
not the details of the code but
high-level artifacts of the code so we
set the number of clusters it's k-means
is usually very fast and here's the
result of clustering you have to have
some way to represent the clustering and
there's no you know there's there's many
different ways to do this but here I'm
representing the clustering as an array
where the invisible indices down below
zero one two three four are the indices
of the data tuples and the values
represent the cluster ID ok that's
pretty efficient ok now let me show you
the code well first of all here's the
code and it's not very long which is a
good thing you know I'm a big believer
in short and simple is better than long
and complicated when it comes to any
kind of code here's our raw data and
here's the clustering method itself
there's really only about ten lines this
code I'll point you to it but I I
probably overdid it on the on the
comment commenting you know sometimes
you to add comments it makes it harder
to read and that was a little bit
paranoid knowing that my peers are going
to be looking at the code you know you
go I better write really comment the
heck out of this thing now the first
thing here is I'll claim in a slide in a
second that I researched as much as i
could looking at every available k-means
implementation on the internet you know
what are the some of the you know
developer site psych I can't remember
stack something stack overflow and
codeplex and this and that I could not
find one that I felt was a
percent accurate and the errors fell
into two catteries number one was the
concept of normalization okay well the
first thing we do in this cluster is
normalize the data and that is in our
height and weight thing our heights were
in the neighborhood of 60 65 but the
weights were like double that 100 340
when you compute a distance if you don't
normalize the data the the column with
the large values completely dominates
the distance computation well they okay
everybody this room I just said that and
I guarantee you arrow goes well yeah of
course obviously but nobody seems to
call that out you mean you have to
normalize the data so I'm assuming my
boss and I were talking about this the
other day and we're guessing that this
was just sort of an assumption within
the community well that may be an
assumption within the researcher
community but for the developer
community I think it needs to be
explicit normalize the data and you can
either do that externally in a
pre-processing step or internally here
in this case there are several kinds of
normalizing what'd I what the specific
normalization here is you compute the
mean of each comp so let's take the
weights compute the weights and then
translate the raw data into X minus the
mean divided by the standard deviation
you do that and each value a value
that's exactly the mean will be 0
normalized and values that are lower
than the mean will be like minus 1 or
minus 1.5 and values greater like plus
point 5 when you do this in except in
really um well there's some word bizarre
data really pathological data the values
would be all between minus 10 and plus
ten regardless of the original scale and
for bell-shaped data maybe remember
statistics between minus 3 and plus 3 so
anyway you get the numbers all roughly
the same so that something doesn't swamp
them that's called Gaussian
normalization it's explaining the code
there's alternatives now this so I said
I didn't find anything so the correct so
the first thing was I I just didn't see
anybody acknowledging the normalization
issue then the second thing here is
indicated by these two things here or
I'll go the update means the the danger
the programming danger when you're
actually implementing this thing is that
it is possible in fact it's very
possible during the course of the
iterations for you to reach a point
where a cluster has no data assigned to
it in other words a cluster vanishes now
maybe you want that to happen but
probably not so if you're going to code
this you have to be aware of that and
prevent that and this implementation
does this so we don't just update the
means we attempt to update the means if
updating the means that's actually
happens in both places it's it's it's a
little bit tricky but if at any point
one of the update means or clustering
would result in a situation with a zero
tuple cluster then you bail out and stop
there or throw an exception or something
like that also where you have a data
point that flips back and forth to two
different clusters and you might open it
that is what so the question was is it
possible for you to not converge by
having a data point flip flop back and
forth I'm all of these things are
possible it's very rare but that can
happen especially when you're computing
the distances if you get a point that
exactly exactly halfway between exactly
halfway and you don't have some
mechanism for always going one way or
the other than that camp so the answer
is yes it can happen okay so that that
is that demo let's go back to PowerPoint
we're roughly halfway done here a little
bit more than halfway done I think hey
are you starting yeah I was calling it
that k was 3 so I guess I'm ozone
dressed now so like how do you know how
do you choose K there there's some
research on it none of us very
convincing my approaches to do as
follows is when you you do a cluster but
you can also return a value for K means
it represents how good the clustering is
and that would be considering metric
like the average distance within a group
so you want small numbers and or the
average distance between meet you would
want that as large as possible so you
can have some kind of measure of how
good the clustering is and it's
basically trial and error try to three
your two clusters three clusters four
clusters and so forth looks and the
goodness of the clustering and then
sacrifice cannon goes again as a
formless out no I mean you could do that
yourself but it's a little bit odd I see
if you use the meaning of this custom
will converge to the number of
yes exactly you've got to be it the
you've got to be very careful see if
there was such a way people would use it
and I I fell into that trap early on
where you just go oh let's just minimize
it and then everything just collapses it
you can actually make it collapse both
way where everything falls down to one
giant cluster or everything expands to
one point per cluster so it's not as
easy as you might think good point thank
you for mentioning that okay so why code
your own well the normal arguments apply
but in particular with k-means I haven't
found any I say many web them tations
buggy now they I mean they work but they
have it they don't address the
normalization or the singularity of
empty clusters you can customize the
initialization as was mentioned earlier
you know once you get things going it's
deterministic so you can do different
initialization schemes the distance we
used was Euclidean distance but there's
all kinds of distance measure Manhattan
distance some other distance that I can
never pronounce it starts with M and it
looks like an Indian name of some sort
of others all kinds mito aids instead it
means you can customize the return value
in particular in a lot of cases where
k-means is used within a larger
algorithm you know you don't really care
about the clustering per se you're
looking for the means those
representative points so you can return
those instead an interesting thing here
was that it's this is almost a poster
child for an algorithm that would
benefit from the TP of the microsoft
c-sharp task parallel library when
you're updating cluster assignments
they're all individual and I mean it's
literally you just replace your for loop
and poof typically i get a improvement
of depending on all things of about a
spirit of x 5 okay and it's a very easy
speed up and of course copyright and
licensing issues and now i said i got a
last minute thing this was a very
interesting somebody
um well actually I sondra foreign oh
that's you oh okay I was about to say I
hadn't met him before but he so great
yeah i'm about to butcher his
presentation but he sent me a really
fascinating paper a really fascinating
paper in progress that uses custom they
came up with a custom clustering
algorithm it's implemented in hardware a
field programmable gate array and they
came with their own thing that's
blazingly fast because it's for kinect
has to be blazingly fast and i only had
a chance to read over but it looks very
interesting so I'll just say first of
all he gratefully said that if you're
interested you can contact them but the
paper looks great i'm going to read it
but this is the idea of practical uses i
think of clustering you know as opposed
to just clustering some sequel data
using it within a larger problem that
involves machine learning okay now
there's a very h there's many variations
on the k-means algorithm i think the
most important one to me is called the
fuzzy c-means algorithm and imagine this
data it's almost the same as the
original data but what's different now
we have these two ambiguous points here
so if you look at this screenshot what
happens is fuzzy c-means instead of
k-means which assigns you know cluster
membership or not this assigns a
fuzziness now i always think of
fuzziness as a probability which is
completely wrong okay if you're a
researcher out there yes I know fuzzy
fuzziness is not a probability it's
really a measure of ambiguity blah blah
blah okay but I still like to think
about it as roughly meaning probability
you can see that these two points clear
those and here these have high certainty
that these points belong together and
these have lower certainty so the fuzzy
i think is quite interesting and it
automatically gives you
sort of these outliers you know right
away you can identify outlier points
that there's something different about
these and I'll show you the I'll just
run the demo okay yeah sure I'll do that
right now I feel like the magician oh
yeah okay show me with my deck of cards
um so no does the silence to two
different cluster well actually in fact
I actually did that one it's very
interesting what happens when you do for
so here's the the call you can see it's
quite easy we set the number of clusters
here but you have to supply an
additional parameter they call the
fuzziness factor a fuzziness factor it
has to be greater than 1 but if you put
1 point 0 0 1 i'll show you what happens
the larger the number is the more fuzzy
the assignments become the less you know
that the numbers get smaller but here
let's do at rob's question first and
look what happens when we say okay four
clusters what happened here here's the
four clusters now look at here's the
assignments and it was which what was it
was six and seven so it put them in it's
hard to see without the data but what it
didn't do what you would expect I was
expecting it to create its own cluster
with those two points in but what it did
was it has sort of unexpected
consequences on some of the other data
okay so i guess the moral story here is
that there's a certain amount with
clustering a certain amount of
experimentation i'll demonstrate one
last thing here too and that's one of
the reasons why I kind of like fuzzy
clustering is let me do a fuzziness of 1
point 0 1 which is minimum if you go to
one there's going to be a division by
zero type thing but if you make a
fuzziness factor of one observe what
happens here essentially it I'm not sure
what the word evolves or converges or
collapses into the normal k-means okay
so in other words there's no fuzziness
in this step so you get served like a
freebie so to speak that makes sense
what was the distance well here i'm
going to show you now now the next thing
is sandra's you know where does that
come from it is a surprisingly so i
thought the same thing you know when i
first ran across this I go oh well we'll
just make it proportional to the
distance you know further distance more
fuzzy okay well it turns out there's
been a lot of research and the algorithm
that I implemented and have will post
it's called a bez Dax algorithm and it
turns out it's not so simple at all at
in fact at all here is the whenever I do
a algorithm that involves data
structures I carefully draw this out so
this is that the fuzzy c-means how it
works and i'm not going to go through
here but sadly there's two things here
it if you're just looking okay yeah I
see a bunch of a raise and matrices and
stuff but I think in your hearts you
know yeah I could figure this out once I
have this and I see the code I'm going
to match and I can see how I loop
through it but basically there's nothing
too fancy here you just do some matrix
operations the real problem in any of
these clustering things is I went
directly to the original source and this
is all you get okay okay this now I work
with you know some nations I'm an
applied mathematician or might in my
school days but even that I go now man
okay and you have to laborious Lee try
to figure out what does that actually
mean in terms of code here's the second
half of the fuzzy c-means where you have
to do fancy footwork and I saw is how
great you know but it becomes a little
challenge you have to figure out what
does this mean and how do i implement in
code well you can look at the code so I
hope that answers your question is that
it's not totally obvious I thought it
was just going to be a simple math
mapping from distance to fuzziness but
it's not quite sociable
yes it is it's it's based on a power
relationship yes if you have data are
I'm up outside the bone if somebody with
a wave of 0 well somebody even if I
don't eat me that's that stuff it
depends you mean with fuzzy or k-means
or either for k-means so what happens
the question is what happens if you have
a really extreme outlier well that is
going to because the Euclidean distance
is really heavily influenced by outliers
because the squared distance that's
going to really move the mean more than
it probably should and that's just going
to affect things in hard to describe
ways but with the implementation here
one outlier can have a unduly large
effect so one of the and I said one of
the alternatives is if you have data
that is subject to that you might want
to use instead the Euclidean distance
what's called the Manhattan distance
which is sort of the same thing but you
don't square it and it doesn't penalize
those huge outliers so there's really no
good answer it's just it just throws
things out of whack right I mean I still
come back to you oh yeah normalizing
does a lot to prevent that but it
doesn't totally eliminate that outlier
effect a lesson think the maximum
that's a there's a different form of
normalization call it's called max min
which deals better with those far
outliers now are you gonna say at this
point you know you might be think if I
was sitting in the audience I started
here I go oh wait a minute seem pretty
easy now there's this detail there's
that detail there's the next detail
don't get wrapped up with the details
clustering is really pretty simple and
there are tons of details but we'll run
into tons of details all the time and
it's not this is not one of those topics
were like neural networks to effectively
use a neural network you have to know
dozens of details to even make it work
but for clustering the information i'm
going to give you here today is going to
make you dangerous with clustering I
mean dangerously effective okay our last
a topic today is clustering categorical
data and here's a claim I'm hoping that
someone in the audience will email me
with a counter opinion if you have
numeric data I claim k-means as the
standard that everyone would try first
but with categorical data I don't know
of any standard way okay so in fact if
you search for this you don't find a
whole lot of solid information the
obvious approach of just converting
string data to numbers doesn't work
because there's no inherent distance the
k-means relies on distance well what's
the difference between red and green it
just doesn't work cases where some
string data like I'm thinking like
spirit colors representing they'll gb
values all right right now you can
probably do something with that with
clustering right that's direct
Technicolor you're absolutely right in
situations where the categorical data
does represent magnitude what you can do
is then you can assign values to them
and use a numeric algorithm so here I'm
talking about purely categorical where
there's just no mean there's labels of
some sort people's names would be a good
one or text processing you know natural
language processing type environments
well I stumbled across this thing called
category utility
so there it is your initial reaction too
many Greek letters but your second
reaction is ok I will now prove using
Zoltar Zulema the upper bound nah I'm
lying I'm not going to do all that all
from a developer's point of view this is
what you see what what development is no
is what does it mean and how do i
implement it so ain't I was going to try
to do a sound effect but I didn't have
one so category utility is a metric it's
a number that can be computed on a
particular clustering of categorical
data so you apply some code and it spits
out a number and larger numbers are
better and you'll just have to take my
word on it there's some deep research
papers that I can point you to and
basically you know just doing a hand
waving argument there's two main chunks
here this reads the probability that an
attribute like color takes on some value
like red given a particular clustering
so it's a probability and over here this
is the probability that an attribute
like color takes on the value without
any clustering so this thing represents
information gained in some way and the
larger the number the more information
you've gained by the cluster it's a very
clever thing so I saw that and it and I
co to this thing it's it's my own thing
but it seems to be very effective and
really the algorithm is right here this
is all you need so we've got some
measure of clustered goodness so what
you do is you start with no clusters
okay and you take the first data tuple
and figure out the category utility if
you would assign it to each of the three
clusters and you just assign it to the
one that gives you the highest number
now you take the next data boom next
next next so you run through it each
data once and it's called greedy because
you just assign it to the tube to the
cluster that gives you the best category
utility and it's called agglomerative
because you start out with nothing you
build up the clustering
okay this is a common thing these greedy
agglomerate things and it seems to work
really well there isn't a whole lot of
research on it but I did it for a
project and it works as far as I'm
concerned and this one I think I won't
do the demo because you just see this
but basically this is the data from the
beginning set the number of clusters the
three and poof it does like that here's
the actual clustering which is somewhat
surprising to me a lot of times
especially categorical data the
clustering I don't know this is what it
came out yes tuple that you decide to do
the assignment of clustering which
columns to do it uses the UM the
category utility takes an account all
the columns and it take not only that
takes both intra and Inter clustering
effects into account it it's a really
interesting metric and there's been a
lot of math done how this thing relates
to other types of statistical things but
anyway um what happens here is that you
can see well that's a pretty good
cluster and in fact it's not really
clustering by color well what does it
appear to be clustered by mostly yeah it
seems to be doing this here's the shorts
here is the long and that one's sort of
a it just just works okay it you know
this is the type of thing when you're
clustering categorical data is very
difficult to know what's going on here
you know it's just there's no obvious
you can't draw a picture there's no
obvious intuition at least to me because
of green yes it's a secondary
or true yeah what you got I mean see the
ideas you know if you put it here or
here you can see the clustering wouldn't
be as good in some weird sense
repeat that it picks the category
category had a number of a number
I don't follow exact the four colors
yeah yeah well then we have three
clusters in there is three
options and early we're there to watch
it um you know I don't know if that is
coincidence or not I don't know but
you'll have the code so you can play
around with its kind of fun kind of yeah
here we are kind of fun yeah bring us
home to your spouse or whatever hey I'm
gonna play around with a clustering yeah
yeah Microsoft people okay we're on the
home stretch here I have like I think
two slides left the this algorithm that
I showed you it turns out that in order
to make it scale to compute the category
utility you have to scan through the
entire data set so you're constantly
trying this you know trying this
tentative assignment and if you do that
you'd have an N squared essentially
algorithm so the solution is to store
things here and I'll need you to read
the code and it makes some of the data
structures pretty tricky but basically
if you can store the results of your
previous assignment then it becomes
completely linear yeah this would also
parallel lies parallel allies that's a
word okay we'll wrap things up here
there's a pointer to the demo code our
advanced development team server in
public you'll see three folders there
should be easy to figure out the next
reference is the source of the fuzzy
c-means clustering algorithm if you want
to know more information about that this
is the original reference of the
category utility metric if you want to
know more about that oh this one I put
in there was some while preparing for
this talk I got some email messages and
one of the the person I can't remember
his name he asked what about a
clustering on a MapReduce Hadoop cosmos
type environment that's one of the the
big research areas that's going on now
and I'll say that it turns out that
there's no new algorithms the approaches
are to take some of the things like you
can do sampling a stratified sampling
and things like this the best reference
i found was here it describes a
particular
technique but the introduction of the
paper gives a very good overview of the
approaches you can take to clustering
data in cosmos in our world and then
here's something I wrote for msdn
magazine and that's my email you're
welcome to send me a message and that's
all I'm done can ask questions or go
away or he said visualizing the data is
extremely powerful of course as you
pointed out as well if you can have 27
dimensional data usually can only boot
runs up to 33 so so which then brings
your answer you like with these examples
are no it's trivial for illustration
purposes you can see that the result is
working right with that notion of the
sensor does it do the right thing must
go away as the data gets more complex
and hard to visualize to even know that
it even work so you're absolutely right
you there's uh there's a certain amount
of uneasiness involved with data
clustering large amounts of data when
you're done it's very difficult to tell
is this data clustering makes sense at
all in fact one of our researchers here
rich karuna on the third floor is going
to give a talk I don't know if it's
happened yet but it addresses that topic
about data clustering and the
difficulties in assigning meaning to it
I just paraphrase it but I'm looking
forward to that talk to I think it's
going to be does anyone know about that
talk you know I think it's I'll try to
find out but well look look for riches
talk on cluster yes there was also this
was some work on visualization that's
the mission
and there was a talk here with a guy
gave so talk on large dimensionality
visualization this is one of our major
areas here ms research is an area
outside of my field but I've talked to
people like that especially when I was
working with cosmos data and that's a
whole different world but i'm not sure i
think visualizations always a good thing
but at some point with data clustering
it stops becoming effective you know i
think it's always use okay good talk out
it's always fun because they usually
have colors and stuff to look at you
know our talks always end up being a
console shell you know i can't we don't
have much pizzazz oh oh they're giving
it all happened remember that next time
yes they're calling really on the
traditionals gasp actually is it must be
very range things esa de cette clusters
with a reduced set of dimensions to a
sufficient level which means i don't
really have to look at
montessori
and finding some way I was saying look
at all the dimensions would find
effective pastoral users
a number of Iceland
right so I'll just sort of for the
benefit of the audio sort of paraphrase
that and in machine learning this is a
close cousin to feature selection where
we want to try to simplify our life by
identifying those columns in a high
dimensional problem that are the most
meaningful and in fact this paper
actually talks about that as one of the
approaches you can use on very large
amounts of data so you can try to weed
columns out or just do with them one at
a time so go high dimensional data do
you know of any good implementation so
sparse crossing using sparse data
clustering using sparse data now not
sure exactly what that means when I
think sparse data I translate to a
sparse matrix where we have so now
you're talking so how let me rephrase
the question what about and this may be
a completely different question but what
about data that has missing values
columns have missing values that could
be considered sparse data where the
missing values now that is one
interpretation but I don't think from
your face that's what you had in mind
they were this loss zeroes a lot of
zeros okay well I will say that that is
isomorphic to the missing data thing um
I don't know I haven't really worked
with that data or looked at it to tell
you the truth so I'll have to say I
don't know the question is data
clustering on sparse data or we can
rephrase that to say would this be the
same as saying clustering data where a
column has is dominated by one
particular value zero or otherwise I
don't know so I'll just say I don't know
okay I'm done that's all go away thank
you for coming I'll just see you next
time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>