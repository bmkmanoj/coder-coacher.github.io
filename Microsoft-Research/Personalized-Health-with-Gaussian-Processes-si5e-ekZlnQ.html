<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Personalized Health with Gaussian Processes | Coder Coacher - Coaching Coders</title><meta content="Personalized Health with Gaussian Processes - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Personalized Health with Gaussian Processes</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/si5e-ekZlnQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it's thanks hello and it's very true
I've had excellent students Niccolo
amongst them and one of the problems you
get when you have very good students is
you start knowing less and less about
the things you apparently are supposed
to do so what I thought I'd do today is
talk a bit about what is the main being
the main motivation for the group and
actually niccolo's a PhD project was a
key important part of being able to make
claims that we're doing this I don't
think we are doing this yet but it's
sort of speculation and ideas about what
the challenges are for personalized
health and when I say personalized
health I mean you know we're doing
little aspects trying to piece some
things together there'll be one example
that isn't really so much personalized
health but I'd like to sort of pull
everything together all the sort of data
we can get about people and use it to
predict health outcomes and I thought
this was an interesting idea but before
I sort of go into the background that
idea I just give you my perspective on
what it is I'm a
we do which is sort of machine learnings
I know not everyone has a machine
learning background so this is a sort of
broadly non-technical talk but I very
much view machine learning as combining
data with a model to make predictions
and I think without a model you can't do
anything so your model is assumptions or
based on previous experience so in some
fields your model may be driven by
physics and machine learning it's it's
based on old data and I particularly
like the idea is that it encodes
something about our beliefs of the
regularities of the universe which we
sometimes see as inductive bias things
like smoothness so one of the things I
should I should actually always test
doing this this used to work in an
interesting way and it probably doesn't
work in a very good way in the United
States but this would be a live test
when you google for have I got Internet
Oh nothing comes up certainly in the UK
when I have internet I think I've lost
the internet connection you see what
people search for and the big thing that
people are searching for in terms of the
suggested things is patient data
protection that's what tends to be the
sort of first suggested search job so
here it's patient that information
patient data analysis patient data
collection sheet so you tend to see
terms like this and it's sort of
something that's more on people's mind
is analyzing a patient data here
apparently protection is less on the
minds of people who are searching but
what that makes me what that brings to
mind for myself is whether I actually
even want to get involved in this domain
because I think it's a very complex area
so I had a while ago about eight years
ago I had a cycling analogy that made me
think that this was an interesting thing
to do in the in the world of anonymity
this is my picture of where we are at
the moment this is I think an 1880
picture of cyclists going down a street
and this is how you know this is how it
feels in terms of the protections we
have now and the lack of information
people know about us outside what's
going on for us is the invention of the
motor car so I cycle a lot and I really
don't like cars
I particularly us like cycle in the
United States it's a very
pleasant place to cycle and that's
because the things are weighted towards
the car the car over rules the cyclist
but there was a time when things were
weighted more towards cyclist and
cycling was more present we're going
through a similar sort of social change
with our data without anonymity at the
moment and it sort of worried me a while
ago that if I were to get into this area
I would have to deal with all these
things like privacy and so and so forth
and and this intimidated me and I
decided I wouldn't do it
as a result until I kind of thought of
this this is the UK history of
registration and in the early days they
had this thing called the the locomotive
Act 1861 limit the weight of steam
engines this was steam engines at the
time to twelve tons and the speed of ten
miles an hour limit in 1865 a speed
limit of four mile an hour two miles an
hour in towns and this provided for
something called the red flag Act so
this was provided for a man walking in
front of steam engines waving a red flag
to warn you that these steam engines
were coming is actually more for horses
than cyclists but it was to deal with
the effect that these objects had this
was kind of reasonably restrictive for
the development of the automobile and
you may know although at the time
Britain was the center of the industrial
world the modern automobile was invented
in Germany and in the end it was
overturned and it was replaced with
something like this the highway code in
the UK the highway code wasn't cool here
it's the UM there's an equivalent
document here that you have to read and
get examined on for your driving license
and this is the UK version it was
introduced in 1930 I kind of view us at
the moment that we're in this difficult
regime of developing legislation where
we don't even know the technology's
going the equivalent zone they were in
in 1860 and the development of the
highway code was actually about a debate
to allow us to move forward and sort of
balance the rights of the cyclists
people with horses people with cars and
it's an evolving document and so when I
thought about that I thought okay great
so that will happen some process will
occur like that and everyone's going to
be happy in the future so I'll start
working on machine learning personalized
medicine that was about six years ago
and I see very little progress
in this domain so one of the things I'll
try and bring through and as I've spoken
to people today I brought through is I
found myself increasingly being worried
about all the things that I didn't want
to get involved with and that's why I
didn't want to do personalized health
because problems of invading people's
privacy and what it meant to create
algorithms that could predict someone's
health futures okay
so there's issues of who owns our data
and and I mean it seems it's like
Finders Keepers like the kids rule you
know if I find the data then it's mine I
could do whatever I like um how
ownership proliferate some and what data
protection offers I think definitely I
would argue that UK and Europe well
ahead of the United States in
recognizing data protections right
privacy is a right and I think that that
has to come here as well um but I don't
know what the rights are and I don't
know that we've got the correct
frameworks in place or how those rights
should evolve so what does current data
protection offer who has the right to
share our data and can we withdraw the
right the last thing seems key that we
won't be given someone the right to own
our data we have no right of withdrawal
of that right and that has a lot of
implications so in the UK there was this
NHS care data information scheme so you
may know we have a National Health
Service so that means we can have like
national databases funnily enough we
don't have very good national databases
but this was a scheme to sign up to a
national health database which everyone
was gonna be signed up to and you had to
explicitly opt out if you didn't want to
be involved and it was I was really
looking forward to this because I sort
of thought well when this is imposed and
it's done in the right way because these
clever thinking people will work out how
to do this will suddenly start getting
access to these very large data sets
that will allow us to start predicting
health outcomes it was a disaster it was
utterly mishandled in terms of the press
basically I don't even know what its
state is currently it's one of those
things where it was just too depressing
to read the articles about how it
evolved and so you sort of ignore those
news items but this is one I dug out
it's so it's 14 18th of April 2014 so
it's just over 16 months ago and the
reason it was mishandled was because
they never closed off the option that
they were just going to be selling this
data that people were going to be giving
freely to healthy
shuras they wanted to make money out of
it is our government not the well it was
unclear what they wanted to do some
people say they're not going to do it
but then there was evidence they were
trying to do this extremely badly
handled so why one wonders if they've
learned the lessons but it's not clear
that they have and that's one reason why
instead of just talking about technical
machine learning stuff I've moved much
more in this direction because it's
depressing the extent to which things
are going wrong so this is my sort of
image of a country that sort of going
through this transition
I would love to a visited Beijing when
it was all bicycles but it isn't anymore
it's got Ferraris and stuff in it and
makes apparently car dominated so I like
this image of this juxtaposition of the
Ferrari and some bicycles this is a mod
Beijing in 2011 you know I don't know
what the right way of legislating for
data but I know at the moment we're
heading in the wrong sort of direction
that's Beijing today as well okay so um
probably many of you already know what's
changing for medical data what me got it
what got me into this and in fact
niccolo's project as a PhD student was
about genotyping and had the potential
to introduce epigenetic into um trying
to well most of my work up until then
had been on the transcriptome but I tend
to think of the transcriptome as a
detailed characterization of the
phenotype a way of stratifying patients
that I can do in a data-driven way and
nicolo's project we were trying to
associate Urdu eqtl study so associate
the transcriptome with different genetic
mutations but it was sort of the first
step in a direction where I was
interested in expanding that idea and we
still are and working towards those
methodologies we don't have them all in
place for doing what I think of as
massive unstructured data sources so
whatever data you're interested in what
in everything is phenotype all data
should be assimilated within this system
the reason I started moving more towards
the regulation type idea is I actually
think you know I we can't do this today
but I see a technological path forward
to believe make me believe that we'll be
able to do this people will be able not
last necessarily people will be able to
do this in the mere few
assimilate large unstructured data
sources some of which I'll briefly
mention as we go forward in order to
predict people's future health outcomes
now I realize that we might be able to
do it I kind of find the implications
quite frightening so I like I use this I
give this talk to clinicians sometimes
so I like this example a little bit this
is a there's an open street map in
Sheffield of Sheffield University of
where you can actually see right where
is it gone
Glossop Road I can't even find my own
University the Hicks building octagons
Center oh yeah this burger here is
John's van it's a mobile burger van I
don't know they don't have them so much
here you have very fashionable hip
street food this is a mobile burger van
of a guy who sort of makes bacon butties
and this is the sign outside his van
it's right outside our stats department
and it says you can get a bacon and
sausage butty that's the sandwich with
bacon or sausage in not the highest
quality bacon or sausage and a large -
your coffee for 2 pounds 10 now that is
about three dollars Sheffield it's got
good prices so it makes you wonder well
do the people who are in the stats
Department you know that this van exists
you can get this open data um are they
gonna have an increased risk of heart
attack as a result of this type of
information well that's obviously a
total speculation but this is John's van
and you can follow John's van on
Facebook and Twitter in fact it was
stolen and it was recovered via Twitter
there's a 10% discount also for NHS
staff which is hospital staff so you
might also worry about hospital people
now the question is can information like
this feed into such a model the answer
is of course well I don't really know if
someone follows John's van on Twitter if
they're retweeting his offers when he's
selling bacon butties at 2 pounds 10 you
know maybe you can but it's just part of
a picture it's the sort of information
that humans are very good at simulating
weighing up maybe not predicting in a
well calibrated way but we're happy to
assimilate the information and and say
something about what it means probably
if we're all well-trained statistically
not very much but what about other
information if this is just part of a
larger picture of how someone behaves
they follow John's van on Twitter
but we know something about their
musical tastes we know something about
their exercising you dream all of these
things can we pull them all together in
order to get a picture of what this sort
of person is like well partly we don't
really know the answer and but that's
the sort of aim in order to be able to
do that that's what we'd like to do in
the long term this is very difficult and
very invasive so we don't have access
certainly I don't at Microsoft you may
have more access to varieties of
information like that but for the moment
what we do is much sorry the previous
picture was supposed to be a picture
which was a bit larger is um we're
looking we're interested in looking at
one in the research group one of our
strategies is to look in countries where
infrastructure that does the sort of
things that we take for granted doesn't
exist and I'll try and give you an
example as I go on as to how this works
now I don't really want to spend time
looking at people's spotify accounts
people's who they follow on Twitter lots
of people are doing it it's a crowded
research area and I also don't
necessarily believe that I'll learn very
much about someone's health outcomes for
the moment what I'm very excited though
is the potential to go into countries
where health systems don't exist and
take things a little bit more seriously
and assimilate different types of data
together to make predictions about the
spread of disease or what likely health
outcomes are put things in place that
don't exist already also do it without
invading their privacy so we've become
very interested in Sheffield in applying
a date science in Africa and we're not
doing it in isolation we have
collaborators at un Gopal pulse and the
University McHenry in Kampala and the
idea is that we can sort of use our
apply our ideas more readily because
existing infrastructure isn't in place
so the particular thing that I'm very
interested in as a solution for some of
these challenges that we've talked about
is what we call user centric models of
data so I'm saying mobile phone here but
based the basic idea is that the data
should be within the control of the
individual so they're gatekeeping on
their own data the reason why that maps
to Africa
that the mobile phones infrastructure is
extraordinary really impressive in terms
of everywhere I trailed in Uganda I had
really good cell coverage not all
countries I'm sure the Central African
Republic is less good but countries like
Kenya
have already moved to mobile money so
there's mobile you can text people money
they're well ahead in some areas in what
they do on their mobile phones so the
sort of idea is well can you use that
infrastructure to perhaps do something a
little bit some beyond what we're
capable of doing or people are
interested in doing here
now I'm I'm also working with a startup
company that is sort of interested in
applying that on your mobile phones and
allowing people to do these sort of a
data processing challenges in a
distributed way but I'll sort of talk
less about that but what are the
challenges with this um with this data
so I like this article so we want to go
and we want to look at large-scale data
in countries like Africa and perhaps
pull it together to draw conclusions
about what's going on in terms of
people's health outcomes big data is a
big buzzword and I kind of use it less
and less because I think it's becoming
less and less meaningful over time but I
really like this article because it
captured something that I thought about
big data it's a July 2014 article and it
just said that big data proving to be a
real challenge for scientists not
because the size of the data but because
the diversity of the data so it's not
this article says and it's I agree with
it so that's why I'm showing it that the
real challenge is not the absolute scale
of a number of data points you have but
the different natures of data that you
want to assimilate so the question is
how do we do that well in machine
learning the thing that's really blown
up is deep learning so you presumably
have all heard about deep learning
scientists this is like an early article
like 2012 which comes after some early
nice results on image processing and and
predicting so categorizing data and
images I kind of think it's not where
things are going for the areas I'm
talking about and I want to give you a
hint about why Gaussian processes are
the sort of at the core of how we're
doing things so I you know even this is
I mean there's got so much more press
now but this is back 2012 New York Times
The New Yorker
geoff hinton going to Google which is
old news is all before deep mind and all
because they're getting very good
performance on image categorization
tasks and this is generated a large
amount of excitement but my own feeling
is that the models that they were using
for this I'm not going to be the
solution I think they feel the same
thing I don't think they're disagree
with me so yan laocoon going to Facebook
I even wrote an article about the change
that was my first popular article and
then this sort of phenomenon now this is
an article I wrote so there was the AIT
ting scandal makes machine learning
sounded like a sport it isn't this has
got this phenomena has got so scaled up
that actually Baidu
kind of slightly cheated on a just a
machine learning benchmark and
uninteresting machine learning benchmark
and it makes sort of international tech
news I mean that's a really depressing
setup maybe it wasn't what they did
wasn't even very interesting how did
they cheat um it was one of these you
you it was a benchmark where you submit
your algorithm you get your test result
and you were allowed to submit twice a
week so your test results so you can
kind of learn on the test data you can
actually explicitly try and do that and
so in order to prevent your doing that
you're only allowed to look at the
results twice a week I think
and they looked ten times a week or
something something of that order and
and the reason they justified it is
because they had three authors so each
author was looking twice a week I mean
it wasn't it wasn't my kicks it you know
it's nothing that it's just really dull
the article is about this is really dull
can we talk about something else you
know if I want to criticize Baidu for
doing things this won't be the first
thing I'll criticize them for I've got
lots of other stuff I could worry about
but really what's going on here
where I have a problem with all this
work is um it's actually not doing
intelligence the reason all these ideas
are working I mean there are other stuff
that those groups are doing that are
intelligence but the successes we're
hearing about the AI
success we're hearing from Baidu is not
generating intelligence it's what I'm
trying to call in order to categorize
how it's working emulated intelligence
so there's a common thread to what's
going on here
applications are falling over that we're
seeing big successes these are ones
where there's massive data and massive
compute and their perceptual tasks so if
you're that interested in the task if
it's translating English to French you
can get enormous amounts of corpora of
data where it's lined up and you can
start doing the learning that's not a
problem you can't do luganda to english
luganda is a native you go in the
language because there's not the scale
of corpora and whereas a human can learn
how to speak again what I probably
couldn't but humans can you can't train
computers to do it because we don't have
the size of data the scale of data we're
using to train these image recognition
systems is well over and above what
humans need to do this to the processing
so all we're doing is we're taking
humans and we with a big powerful
machine is we're sort of managing to
reproduce what they say so they're all
perceptual tasks the big successes
I mean I'm generalizing the some
exceptions but they're they're all
perceptual tasks so speech recognition
computer vision translation where we
have enormous amounts of data and we're
managing to build computers and I like
this idea because actually we do this
there's an area I'm peripherally
involved in which is say climate model
emulation so people do this with climate
models they have a very expensive to
compute climate model and they don't
want to compute it so they just learn a
mapping that recreates what the climate
model says it doesn't have any other
mechanism it just tries to recreate it
and then in effect that's sort of what
we're doing with these big successes
which is great because these really
difficult tasks and we want to solve
them but we're going to hit buffers
because we don't have the scale of data
to do this type of task for things like
human health in order to do the same
thing for human health we would have to
make force everyone to be sick all the
time in order to acquire the diversity
of data to understand the complexity of
what's going on I mean it's just not
going to happen
health is way more complex I believe I
mean if we consider all the ways in
which we can go wrong the complexity of
the human body then any of these tasks
and even if we have population scale
data sets I don't believe that we'll be
able to solve what's going on
with the current generation of
techniques so one Willis tration of that
is
what I call massive missing data so
again I'm trying to illustrate a point
about where current algorithms or week
so we kind of tend to think when we're
dealing with learning of missing data as
a sort of challenge that we've got some
Excel spreadsheet and the some Nan's in
it or some pandas array in the sum and
and maybe 20 or 30% of these things are
missing that's very frustrating and we
do a lot of work typically to impute
those missing values
that's not missing data because
someone's already carefully collected
all that information in a table row by
row column by column and presented it to
you that's actually statistics and
someone was explicit about trying to get
the data you aren't missing ten percent
of the data you are missing 99.99999
percent of the things you could know
about that individual I mean you're
missing everything
all you've to have collected is twenty
five things or you know in the case of a
genome however many thirty thousand a
million or whatever your snips now in
massive missing data things change the
typical approach in missing data is you
impute your missing values but in
massive missing data you can't do that
imputation is totally impractical um
that's because the algorithms we use
have this sort of iterative approach
eeehm is the sort of gold standard but
most impure algorithms are kind of like
that you are in eeehm what you do is you
make a guess for what you think your
missing value should be then you fit
your model to the data and then you
update your guest based on what your
model says and then you refit your model
that's something loosely speaking
there's a nice mathematical proof of why
that's a good thing to do but that's
basically what you do now that's all
right if you're missing 20% of the
things but if you're missing 99.999
percent of the things you fit your model
to pure rubbish because you have to
guess so many things so imputation
doesn't work I mean even at the scale of
50% missing data
you can't impute but my argument in so
that's just a review of em in in very
large sparse data so the e miss database
is a GP database in the UK it's got 39
million patients but they only have
thirty percent of the market so from
that infer that must be at least 17
million non-living patients
and thousands of tests that are done at
different GPS that are coded in
different ways these are actually
they're selling databases to individual
general practitioners I should explain
sorry
they're the frontline of clinical
interface so you're not allowed to go
and see a specialist straightaway you
have to see a general practitioner and
then he directs you to the specialist so
everyone has to see general
practitioners and they they store your
patient record so then but they're often
like a house with three doctors in it
like a big old Victorian house
it's my one and they buy you know they
run their runners independent companies
there's partnerships and so they buy in
their database system from a company
that says here's a database system now
these companies bought up other
companies until they've actually got a
large scale it wasn't planned
so there's only now two or three
providers of these systems so you have
quite large databases but of course the
data is massively sparse because we all
get ill in a massively different way and
not only is it massively smart but
within each practice they have different
standards of how they enter data how
they code the disease it's it's a bit of
a nightmare I'm so in in any case if you
try and do en for this know so it's a
medical test some electronic health
records yes precisely things yeah
precisely but but it not properly
standardized like done in a sort of ad
hoc way as it's needed so that they're
very interested in um in instead of
people coming and helping them improve
their systems so some I work with
probabilistic models and the way we
denote what um what we're trying to do
when we model these things is let's say
I have a patient and he's this patients
had to test outcomes well what I have to
do is I have to but there's many more
outcomes that are possible so there's up
to P potential measurements I can make
on a patient and P is bear in mind gonna
be in millions what I need to do is I
need to do this massively high
dimensional integral in order to find
out what I expect that patient's outcome
is which is a bit of a nightmare because
why three to YP contains every test that
hasn't yet been applied to that patient
and every test
that's not yet being invented so you
know Facebook I know
rerun their advertising ranking system
every night so that's how they deal with
non-stationarity and data they relearn
every night my assumption is if we're
going to do these sort of large-scale
population wide health monitoring
systems we will not be rerunning them
every night we will need to adapt them
in an online way so when someone invents
a new test we won't be able to say okay
train the whole thing all again we need
to work out how to assimilate this extra
piece of information in our data so this
is the interesting phenomenon something
I've been interested in for a while
technically is when P becomes n
everyone's used to P the dimensionality
of your data set being fixed you know so
there's n people in this room and you
have P characteristics and you're not
allowed to gain any more than P
characteristics at any one time it's
kind of absurd when you think about it
but that's how we model data of course P
isn't like that P is also like n you
could gain new features that we find
things out about we could measure you in
different ways so medical data has this
property for sure real data all has this
property it's only in our little
closeted world of how we're doing
learning at the moment that we don't
include for that but there's a way out
and the way out is that friendly
distribution the multivariate Gaussian I
mean it has problems which we've been
discussing today but it has a beautiful
characteristic high dimensional
integrals well you know one of the
things that held back the Manhattan
Project why they invented computers all
these sort of things but high
dimensional integrals in gaussians are
kind of trivial so if this is a 10
dimensional so this is correlated
Gaussian with a covariance in this form
so it's telling us how 10 different
variables are correlated one sample so
they're sort of these two are correlated
and this is anti correlated with this
one so this is saying hi sorry well this
is saying a high covariance correlation
would be high and but it's anti
correlated with this so this index here
is a color visualization of the
covariance so we're seeing high
correlation between these two or roughly
certainly an an anti correlation here
now this has got 10 measurements but the
beautiful thing in a Gaussian is if I
want to know the Gauss the density for
two measurements say these two here it's
trivial it simply
take those two elements of the
covariance matrix and use those two
elements instead that's why correlation
is like a widely used statistic it
wouldn't make any sense if this weren't
the case but you know when you see that
the first time you succeed I think okay
clearly but that's super unusual now
other well no other continuous
distribution I can think of has this
sort of property and this property
allows us to trivially handle missing
values like if it's not there it doesn't
affect the correlation we believe in the
existing things there are lots and lots
of problems with Gaussian densities and
a lot of our research in the group is
how to deal with that Niccolo had an
excellent Nature communications paper on
warping the outputs of Gaussian entities
to map the phenotype better to something
that's gal Sein and that worked really
well um but this property is why we kind
of use them and why when people say why
don't you do something else I can't get
my head round on the other thing because
I can't handle this challenge and I want
to be able to hand this hand trivially
the challenge yeah well that's the
further challenge you then have to you
have to build so what does this mean for
the challenge in practice if I invent a
new measurement if I did if the world
were all Gaussian well we wouldn't exist
but it would be really nice we could
model everything and then we had to
model a new thing all we would have to
define is its covariance the cross
covariance between things that already
exist and we would typically define that
functionally with a covariance function
and and how you would do that open
challenge
I'm not saying we've resolved that but
you can do it there's a way forward you
know the right step but typically we
don't use order n we we define some
similarity but to existing things which
gives us a function for killing for
filling in those things and the fact
that it's order n is also a problem
because it's a nonparametric models so
we need some way of compressing that
down to some number of things that we
can yeah this is these are all these
things we're working on and have ideas
about how to solve okay so um this is
this is one of the pieces of work also
Niccolo was um is something Niccolo at
the end of his PhD thesis we start
looking at and have never had a chance
to follow up but max Vizsla who's a PhD
student with me now we keep hoping to
build models that can kind of we do
recommender systems as a sort of so
recommender systems of sparse as well
and have these some of the properties
so rather than dive into medical data
all the time we tend to demonstrate
these sort of things on recommender
systems and that's sort of ongoing work
just briefly why is it that this is a
problem for most models if you're
interested in deep learning I can give
you a sort of quick you know if you're
not aware of what goes on in deep
learning there's sort of things people
do in deep learning I can give you a
quick illustration about you know some
people say well this doesn't sound a
very difficult thing to deal with so
this is a model I'm drawn here which is
known as a sort of restricted Boltzmann
machine it was like the first deep
learning model in the resurgence of deep
learning but it was shallow so what they
what they they did is they defined this
and then what geoff hinton did is he
stacked these things and his science
paper was about stacking and unrolling
these things and that was one of the
things that triggered interest in deep
learning again but it doesn't have the
spectacular results that some of these
other neural network algorithms have but
a deep in in this model what you have is
you have some number of latent variables
and then you have some number of
observed variable here there's ten now
just by colouring things I'm showing you
what the latent variable is a latent
variable is some characteristic we're
interested inferring so like a typical
one that people did back in the sort of
1920s or 30s would be like IQ or these
rather well-known factor analysis based
characteristics and the idea is somehow
the latent variable is responsible for
declaring water is in the output so PCA
is a good example of a latent variable
model this model is discrete actually
but what I'm gonna do is show you what
happens when one of these nodes is
unobserved so in the graph so in the
Gaussian it just disappears but in a
Boltzmann machine that node itself can
now be drawn up here so this node is
missing it needs to be some doubt the
problem with latent variables you have
to marginalize over them they're
discrete so it involves
some with now two to the five terms but
when we have this one as well it's two
to the six terms this model is actually
no longer in the same class as the
restricted Boltzmann machine model so
this is a deep this actually is a
variant of a deep model so these models
have a lot of trouble with handling
missing data on the outputs so not all
model classes this one is an interesting
model class that you know enjoy hearing
talks about but it can't handle this
sort of thing where most of these
variables are missing most of the time
okay so the question is for massive
missing data how many additional latent
variables well what you can show is that
when you're assuming a Gaussian process
you're assuming infinite additional
latent variables but they just integrate
out to have a Gaussian effect because
the various properties of gaussians
properties that have issues but are
useful in this case okay so I said I
mentioned some of our work in Africa and
I'd like to sort of just show you an
example of how how how we try and do
this in practice so John Quinn is he
said he was a student of Chris Williams
who's a well-known Gaussian process
researcher wrote the book on Gaussian
processes with Carl Rasmussen who about
eight years ago after he graduated his
PhD he thought he did try I'm lecturing
in Africa and he moved to Africa to
Meharry University and he's still there
eight years later just on a Ugandan
salary he's not in any way funded by
charity or anything he's just like
working as a Ugandan at the university
Meharry from recently he was also
appointed jointly to un global pulse so
un global pulse is an organization
founded by the UN to sort of do data
science because they felt they kept
getting caught out by events emerging in
the world that they were unaware of and
needing to move resources to handle the
event so they've got three labs one in
Jakarta one in Kampala and one in New
York they're actually advertised
recently for positions in the New York
lab and the idea is to try and predict
ongoing outcomes also predict when
troubles are occurring so it's a big
interesting data science project and the
aspect we got involved with them was
pretty
malaria incidents in Uganda why is this
difficult well normally if you're
interested in the incidence of disease
you send out large survey teams to sort
of measure to what extent the disease is
present in any given area but that's
expensive and and difficult to design
studies you actually don't have good
baseline values on what the population
is in different areas so actual
well-designed studies are a little bit
hard so our interest was can we take an
area like Uganda and use information
that is available such as the number of
registered cases at health centers
within little departments so each of
these is like a state and combine that
with other information sources like
weather rainfall vegetation index
altitude and try and improve the quality
with which we're making a malaria
predictions so these different types of
data combining them with the same model
and we use Gaussian processes to do that
and so this is the sort of the nature of
these different data so there are seven
Sentinel locations where there are
actually health centers that are
equipped enough to diagnose malaria why
is malaria difficult to diagnose well
you have to have people who are trained
to look down microscopes and see the
Plasmodium and say yes there there this
bigger way it's diagnosis people exhibit
a fever and they're just given malaria
medication if they can afford it and if
they don't get better they given typhoid
medications so there's a lot of
confounding between typhoid and malaria
and that causes deaths because people
can't afford two sets of medication so
this is H Mis data which is just this is
how much malaria drugs we've given out
but this data is difficult because the
the distribution of drugs is not done
necessarily in a clean way so you don't
actually know so there can be corruption
in terms of the distribution and
movement of drugs because they're worth
money so you don't know that the number
of drugs that were actually they may be
playing with the data in order to sort
of hide the fact that drugs are going
missing so and then a third form of data
is um satellite data on rainfall and the
beautiful thing is this sort of open
data now sorry that's what rainfall
there and and this is temperature and
what we're seeing here is a joint
Gaussian process model overall the
things and the idea is can we sort of
combine these things to improve to
improve the prediction for malaria sorry
the y-axis is varying according to I
haven't given any labels but each HMS
would be the number this is um this sort
of it's going to be a exponentially if
you exponentiate this you'll get a rate
of number of infections apparent
infections according to HMS and this is
time along the x-axis yeah yeah five yes
so you and and what you're interested in
doing is trying to cross correlate these
spatial outputs together so and it's one
of the things so we wrote a paper on the
different ways of doing that so this is
an arrow where expert in sort of like
cool we're gonna do that we're gonna
solve all your problems
sorry I know I got the axis run this is
the Sentinel this is the Sentinel
patients with all patients who go to the
Sentinel is the patients with malaria at
the central disease because they
actually look at them these are the HMS
so these are the ones from health
centers that are um being reported in
their different departments and these
are this is the rainfall and this is the
temperature no so I think it's that one
where we've no I can't remove that one
is so let me skip through it we did a
sort of school there but actually the
interesting thing about that result is
it didn't work we could not get
correlation between those different
things to the extent that we could make
any predictions because the HMS data was
so noisy it's difficult to model I mean
actually the main thing that Ricardo
Ricardo has got a couple of papers one
of his papers is so for political
reasons the President of Uganda often
splits these economy where they're not
departments but let's call them
departments or States split them in two
because every time you do that you
create a new set of administrative roles
and if you need to repay people with
favors you've got a new set of jobs you
can give so these splits happen on a
reasonably regular basis every sort of
few years
and so you've got data that's being
aggregated at one level is suddenly
being aggregated at a different level
and these are the sort of challenges you
sort of end up facing all the time like
how do you do that because you think
well that's great well they're just
going to sum but no because it's two
different reporting bodies and suddenly
these numbers change
totally so the actual some of the
reporting bodies from the split thing
there's not equal the number that are in
there before so large sections of
Ricardo's work we're just actually
solving these issues and at one point
and we're also trying to cross correlate
across departments so it's not just for
outputs we've got these outputs for
every department but we only get the HMS
statistics at a summary level yet we've
got vegetation at quite a low grain
level so a long-term idea was that we
were going to be able to take the
summary level but say perhaps how it
might be distributed across the state we
just do not have the level of data and
the quality of model to do this with
what we're doing at the moment so the
thing Ricardo did do which is actually
um he's going to be in Uganda
implementing this is something's
slightly different but related and I
this one I love about the way Ricardo
did his work because this is not my idea
that's his idea and it's actually
improved beyond this now this is a gif
he sent me for the workshop we did in
Kenya a few months back but Ricardo I
mean when we talk to John Quinn what he
sort of says is you know you can do all
the modding you like but what people
actually want is they want a
visualization of where there is malaria
at any given time they don't even have
that so there was a typhoid outbreak in
Kampala I guess at the beginning of the
year first three months and you want to
know where typhoid is distributed so you
know where to intervene to try and deal
with the outbreak so what you want is
maps like this so the illustration I'm
giving you here it contains two parts
this is a Ricardo's idea it's taken for
economics so to visualize what the state
of disease is at any time so what we see
on the plot is actually two curves so
the nice thing about Gaussian process is
we can be clever about combining trends
so we fitted or Ricardo fitted to this
data which i think is HMS data
it's a malaria cases log scale it's not
with a Poisson because then there's such
a large number that there's no point in
using Poisson you can just model it as
Gaussian
but Ricardo's modeling this in showing
the evolving model over time and he's
showing two components that model one is
a long-term trend that's the red and
then the other is a short-term trend
that's the blue now the short-term trend
is you know it's it's varying over
months and the long-term trend is
varying sort of over years and the nice
thing with the Gaussian process is we
can decompose it's quite noisy signal
into these two trends and then Ricardo's
idea was to use something that I think
people he got the idea from economics
and we're gonna even improve this type
of plot further which is to sort of
allow people to visualize what's going
on so you do this for each state in each
state and each time so Green here means
the the state of the disease is below
the average of the trend currently and
it's also going down so yes what yes
visualizing is a plot of the rate of
change the gradient and the vert and the
sort of absolute value red is its above
and it's going up and these two
different yellowish colors are either
above average but going down or below
average but going up so these are sort
of things that we feel there's people
who are doing the decision-making in the
UN can kind of grasp the idea we're
actually going to change this plot yeah
so we see these years here so you're
seeing different points coming in
probably a couple of weeks or every
month or so something like that yeah so
I think that we're more interested the
particular example when I was in Kenya
doing the state science workshop the
thing we got excited about was this
typhoid example because those maps were
evolving on an hourly basis type thing
or six hourly basis something like that
they actually use what I'm John does I
think I don't know all the details of
this they have access to telecoms data
now so they know people's movements in
Kampala from station to station and so
they can sort of try and exhibit where
use that combine that with existing data
to sort of say how things are evolving
meet what when you say discrete
derivative so it's continuous because
it's the one of the properties of GPS is
the derivative is jointly GP with it so
you know the derivative constantly all
times you know a distribution over it so
the set of things that we want to modify
with this plot is that actually we've
got drawn a spot here but it's not
really a spot it's a distribution of
places where we think we are so the
thing that we want to do with Ricardo
when he's in Kampala is start working
out how to visualize that type of
uncertainty and then how to convert it
to the map so the idea is then you have
some sort of spatial map so now we're
just because it doesn't work to
correlate everything all we're doing is
is viewing the map over time that gray
area we don't know whether that gray
area is the right size maybe we need to
play with that but what we're seeing in
these gray areas is is is um it's too
indistinct sending it's just around the
average we're seeing the radius sort of
high and going up green is low and going
down yellow is a common which way around
and the idea with that is to try and
extract this information that's in the
Gaussian process and show it to the
person you know when when so the idea of
doing these data science workshops in
Africa is to help people understand how
to process data but people aren't that
interested in the Gaussian processes and
they're clever properties they are much
more interested in visualizations that
are helpful in showing them how to do
things and it sort of strikes me that
you you you you um well or what Ricardo
did which I really liked you can't do
these things independently you have to
think about the visualization how you're
going to communicate that information
because you won't be able to sort of
make the decision
let's send drugs here based only on your
Gaussian process
using no no in this example because it's
sort of um this is it didn't work to use
the information in the way we wanted to
use it in in the typhoid example that
John's modeling he is using some of that
information
no sorry well we don't joint your model
over the covariance model over the
response only yeah no that's true I've
I've not highlighted yeah there's a bit
missing part of the story as you've
correctly identified yeah so if it's a
so I'm imputing over that it's and
actually the thing that I didn't really
show is that's how we do most of our
modeling so we tend to model the
responses directly we're not doing it in
this case because it just doesn't sort
of fit but I like this example of um I
don't know you know you have these big
visions about are you gonna save the
world and in the end you realize that
the most useful thing anyone can
actually have is a correct visualization
of what's actually going on in any
instance at the time because they don't
need your fancy Gaussian process model
yet so you have to do these things along
the way okay so but as you correctly
pointed out um though those types of
models yeah are actually not modeling
directly on the output so this is
actually the sort of type of model we're
interested in so this is a so-called
deep unsupervised learning model so what
we do is with these latent variables and
unlike the Boltzmann machine before we
pushed Gaussian processes between each
of these layers so in deep learning
normally people build parametric models
to map between each of these layers we
don't we build Gaussian process models
so this is work by um andreas Damiano
who was one of my PhD students he's now
post talking with me I mean I prefer
this diagram it looks quite simple I
actually got into machine learning cuz I
like drawing these things back in the
1990s but nowadays I prefer these things
and and the idea with these deep models
is that um you don't just do latent
variable more
but Jeff's very persuasive argument that
I enjoyed is that you have the idea of
you get some abstract features and then
as you drop down the model things get
more and more detailed until you observe
something in data space so this might be
an image or this might be anything this
is the area where we can do the the we
have the marginalization property we can
deal with massively missing data so you
know just to finish on is there's the
last thing that was sort of interesting
working on is how to these are some
samples from these deep models it's how
to take these type of models and use
them in health modeling so what am i
showing here I want to show you just an
illustration of this abstract each thing
so this is a deep Gaussian process model
being sampled from and to try and
illustrate the samples I'm showing you a
square at the top and then this square
is going through a Gaussian process so
it's been distorted and normal in your
way
it's gone through another Gaussian
process here which is twisted some
corner of that square now the argument
is that this is just the simulation of
the model not doing any data analysis
now but once that feature is in it will
stay in so the you know the rough
argument is that anything that is in
here like this sort of point here will
continue to exist all the way down you
can't get rid of it but as you drop down
the layers you get additional features
added the interesting question and this
was what really persuaded me like I
think Jeff actually at Microsoft
Research in Cambridge 2001 gave talks
about models like this and I thought
yeah I really understand now why you
might want this type of architecture the
reason you said want it is because you
don't quite know which level you're
interested in so is this the level here
the abstract features where the
clinician is interested in how disease
is actually split up so you know the
disease manifests in two different
categories such as you know responders
to treatment non responders to treatment
and then people who get worse that's a
typical thing that a clinician might
think about a disease where does that
exist in that model that's a some sort
of abstract thing that the clinician
thinks and that's the thing we're trying
to recreate so that the clinician can
get involved with the model
but actually the data we get is like an
x-ray or something totally departed from
that it's it's sort of removed away from
that so this led to this idea of a
yeah
so when I do this talk to statisticians
and I'm talking about the model I call
it process stochastic process
composition as a way of and then the
reason yeah yes and the reason we do it
is because it means that the properties
of the Gaussian we get a much more
general class of processes yes and and
the order of the number of those
parameters is small so we only have sort
of order so in this model here we would
have approximately order four parameters
four parameters six six not millions of
parameters like you do in normal deep
learning we integrate all those out yeah
yes precisely
so back to how we sort of composed these
Gaussian processes together compose
being right but this is the sort of
thing we'd like to do so this is what I
call deep health I've been calling it
that for years I mean like three years
now or something
and you know we can't do this but what
we can do is we can do subsets in this
graph so it's nice I can point out
people's PhDs so niccolo's PhD was to
combine that and that with that in one
model so it so each sort of subset you
take is is actually like a whole PhD
there's a guy called Alan Sol who's
who's who's working on GPS for survival
analysis now how are these coming in
here we want this to be a latent
representation of disease stratification
at this level here we need to include
the sort of things clinicians are
interested in so I thought is that
survival analysis should be directly
related to that area that level here so
that hopefully makes that level more
interpretable because we can look at
which dimensions are associated with
survival because getting these latent
variables interpreted is gonna be a big
challenge to integrate them clinicians
in practice when we've tried combining
gene expression with survival we get a
lot of problems we get those classic
problems that survival information is
super weak so the model is trying to
jointly model gene expression and
survival that worked well for niccolo's
project
but in Alan's project that doesn't work
well at all because the model spends all
its time modeling the gene expression
and ignores the survival a model so
you're better off trying to input as
covariance the survival data as we were
doing before but this is the sort of
thing where we're trying to model it
directly which is what we'd like to do
we'd like to do that because often that
data will be missing in a given patient
example so that's the sort of challenge
how you deal with that you know this is
weak information and this is you know
very detailed information and you need
to combine that together we don't have
good solutions but you know ideally we
then also like to include social network
information music information the
clinical notes so text data about what's
going on with the patient biopsies
x-rays you know and they should come in
some level on this hierarchy and the
plan is that you can potentially do this
you know population scale for all these
different modalities of data and when
there's a new modality of data you can
introduce the modality and try and
associate it learn which level it's
associated with so you can learn the
structure of these models as well that's
super ambitious and you know if we got
all the out we know we can do each of
those things individually we can't even
pull survival data together with gene
expression at the moment so there are
big challenges there we actually learn
that - yeah interestingly and David
Domino who's now here at Harvard did
some nice work on this it turns out that
this is closely related to the the
properties of the underlying stochastic
process you get so the number of nodes
you use at each level if you were to
expand the number of loads to infinity
the whole thing would be a Gaussian
process again with a particular
covariance function adding these nodes
in here leads to a sort of nonparametric
non Gaussian process capability which is
the sort of thing we're after I'm now
thinking more of the stochastic process
point of view which was something came
to later the geoff hinton perspective on
let's do deep with the original thing so
these things don't have to be very
important parameters in terms of
determining the properties of the
underlying process
so in nicolo's thesis we learn well
Nicola will removal now correct me if I
get his thesis wrong it's a Nikolai's
thesis we had the gene expression as an
output so of order 30,000 we had snips
as a genotype input so potentially order
a million and then we actually inferred
the environment as an additional well
this was directly connected we didn't
really have this latent layer here we
were connecting these things to this and
we did it linearly to get the
interpretability I mean if we done it
nonlinear lean I think no one would have
published you tried nonlinear right and
they didn't publish it yeah because it's
not interpretive all it's hard when you
go nonlinear with the Gaussian process
people you know don't know how to
interpret those things even if it
performs better yeah I mean maybe yes
and maybe no covering all my options yes
I think that that's a really important
choice ones why may not be as important
as you might think
is because if you think if you're going
to push one Gaussian process into
another you're effectively allowing
nonlinear warping on the inputs of the
Gaussian process so this gives you a
much larger class of functions you can
consider we don't include we don't do
these sort of translational invariant
things with these models they do in
image stuff at the moment ya know you
would
roughness also again I'm not too worried
about but I am sympathetic your point I
think it's true and the problem is we
can only do it tractable II efficiently
for certain covariance functions so the
approximations require us to use certain
covariance yeah it's driven by current
capabilities yeah
so this deep learning process as as very
abstract features and then building up
more and more interpretive any more
interpretive ones or getting that level
data the skills I mean the picture is
quite clear in the in the image
classification tasks or the square
example you showed us but now you're
talking about integrating all kinds of
different data into this picture and
you've restricted yourself to a vision
which is very linear it has one
parameter in this notion of abstraction
and it feels to me like if you want to
squash all this together there will not
be just one parameter it's almost a
manifold maybe one parameter in terms of
in terms of I mean okay new type of data
to not just fit at one depth but that's
allowed for that's a lot of port so you
you know I've drawn it like this because
then it would the idea of the hierarchy
features wouldn't come in but there's no
there's no algorithmic there's no I
don't wanna say theoretical there's no
issue with a trying to do that
architecture you could have an
architect's view and be careful my same
as one parameter because there are
actually infinite parameters will be
integrated them out so you know I think
it's dangerous to summarize it with with
like the depth because there's an awful
lot going on in terms there's an
enormous amount of uncertainty being
sustained in this model and that's why
and that's important if you're going to
do a spa you can do data that is my
belief is that you need to sustain that
uncertainty if you're going to be in
this zone where the system you're trying
to model is really really complicated
and the amount of data you have ease is
scarce relative for that complexity
which is the domain that the current
generation of deep learning algorithms
do not perform well and and we do
perform well in that domain when we do
little experiments okay and that'sthat's
kind of it this is more stuff if I'd
emphasize different things but um that's
it so the key the key lesson is well you
know I believe in models like this I
kind of think the technologies are in
place not that we can do it like
tomorrow but that we know that we can
fit models of
this scale we can do all the things we
just talked about learning the different
structure of those models scale them up
to millions of data there are you know
there are problems about like what you
know my belief is that if you get the
likelihoods right you can integrate
survival-type data but you still get
challenges in that sort of domain and
and we haven't sort of yet resolved them
and the broad work in my lab is most
people's work you can actually summarize
by trying to piece a couple of these
things together and do something useful
in the interim before we can solve such
a large scale</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>