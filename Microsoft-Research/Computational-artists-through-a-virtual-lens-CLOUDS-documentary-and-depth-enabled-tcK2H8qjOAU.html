<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Computational artists through a virtual lens: CLOUDS documentary and depth enabled | Coder Coacher - Coaching Coders</title><meta content="Computational artists through a virtual lens: CLOUDS documentary and depth enabled - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Computational artists through a virtual lens: CLOUDS documentary and depth enabled</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tcK2H8qjOAU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
my pleasure to welcome James George who
is I was tempted to introduce him as
computer scientist turned artist but I
suspect he was artist way before he was
a computer scientist so he's an artist
who happened to have gotten his degree
in computer science I says likely yeah
and his art pieces his installations
films his mobile applications have been
exhibited around the world and I think
he's going to show us some of that today
James is currently and artists in
residence at the i-beam art and
Technology Center he's also a fellow at
the CMU studio for creative inquiry as
well as a faculty member at NYU so yeah
take it away James cool thanks thanks
dezzy so the title of my talk and the
plan was to talk about computational
artists through a virtual lens and I
think that will clarify itself quickly
but then after all meeting you yesterday
i figured i might throw in a little more
personal data so the second title for
this talk has become a computational
artist with a virtual camera so it's
these two things together and this is
going to get pretty meta so be careful
so my name is James George I have a
Twitter and a website which I encourage
you to contact me through and I have a
hobby of playing with cameras in strange
ways and this is one of my favorite
pastimes
you're gone so this was the first test
of an application that i developed that
was called freefall high score and the
the concept of this application was who
can drop their phone or rather suspend
their phone and free fall for as long as
possible using the phone's accelerometer
I could detect the moment minute it left
the Earth's gravitational force or
rather joined with it and start start a
timer as well as recording video those
videos would then you could tell the
story and how long that you had they had
fallen and tell us and that would upload
to YouTube where it was secretly tagged
with that data and then aggregated onto
a website ranking the highest score the
highest score drops in this kind of a
high score page so the concept here is
to you know take something that we value
more than we should that maybe we don't
even realize the amount of that we've
attached that were become attached to
these devices and then take an
application through that language the
logic of writing applications and call
into question that attachment and put
someone in an uncomfortable situation
but still you know ticking the box of
competition and play and creativity and
maybe instead of looking at your phone
look at the high buildings around you in
New York City and think about how can I
use that building to break this high
score so I was really excited about this
application you know fervently developed
it submitted it to the App Store and
rich was returned with this and I mean
so Apple said in their policy that they
wanted people to make innovative
applications that make people think
about their lives in new ways I thought
I was doing that but they had a
different opinion so the applications
available on Android not as cleanly
implemented as Apple so but that's
that's how it goes but I thought that uh
to make up for this I would do an event
like an installation like how am I going
to get this out into the world how am I
going to find people to set the high
score so I started a contest and the
contest was modeled after
in high school you have an egg drop
contest where it's you to teach kids
physics how do you protect an egg well I
was going to be the same thing except i
would give people phones and they could
keep the phones if they could find a way
to drop them off a building and set a
high score and i would like to show you
the winner of that competition hi my
name is Tim this is my preserver it's
the I'm pot hole in the bottom the the
camera to pick out of and some daisies
on top
yes
so the phone obviously survived and he
won the high score is 2.7 seconds and
actually someone in Finland recently
broke the high score but they were
disqualified because they put the phone
inside of a container so you couldn't
see the video so one of the requirements
for the preserver was you couldn't
obscure the camera's lens so he had a
proof of the of the phone Falling so
that was an individual project but I
also work on a lot of collaborative
things and this is where this awkward
position of being a computer scientist
and an artist comes into play is that
oftentimes I'll work with other artists
to realize their ideas or merge my
skills that with though with them to
create larger scale installations and
things of greater context i'm going to
show two collaborations which i think
will resonate with people in this lab
the first is called sniff maybe we get
the lights down for just a second sorry
I sprang that on you so sniff sniff is
an interactive dog made for Street
projection and he he tries to understand
what you think of him and in doing that
he tries to make sure that you are
trying to think what is this person
think of me so it's a way of instilling
agency in a virtual creature injected
into public space in a way that makes
you question your environment and people
that aren't expecting to encounter art
well counter an artful experience and
you know wonder who put this there why
is this here why is this dog looking at
me and then slowly start to understand
through the power of gays and
interaction that yes this this this
creature this virtual dog is indeed
guessing what I'm trying to do if you
run at it quickly it'll jump back if you
hold out your hand it will wag its tail
and sit down if you move suddenly it
will bark at you if there's multiple
people it'll choose the most interesting
person based on their their gestures and
you know different a group of people
will vie for the attention of the dog
and the idea is even though we know
something is virtual if you if you
instill something with animated
qualities of a character we can suspend
the
belief and start to really imagine what
does this creature think of me and this
has been exhibited several times this
was the first installation on a brooklyn
street corner and just because I'm the
tech guy I'd like to show you the
behind-the-scenes of how this works and
this is begins to get you know at some
of the conversations that we were having
yesterday so this is unity 3d rendering
the dog which is choosing hundreds of
small animation clips that are less than
a second long trying to decide which one
is best based on the dogs desire where
it wants to stand what it wants at what
expression or emotion it wants to
express at the same time there's a
vision system that's feeding it
information about who's standing in
front of it and these applications were
never meant to communicate with each
other necessarily but they work over a
network protocol called OSC and we
design these these little communication
systems to to make experiences that are
more powerful than either of these
applications could achieve by themselves
so that project led to another
collaboration with a renowned video a
music video director and experimental
artist named Chris milk where he had a
concept for this this triptych of
installations that interacts with you
and I directed the software for this so
I'll show you a video of him explaining
how that works so it's a triptych where
the three individual panels represent
the process of creative
this is something that I struggle with
constantly
so the first panel where your body
disintegrates into the birds that
represents that initial moment of
conception it's the moment of
inspiration it's the lightning in a
bottle it's the purest moment of the
idea that there is so this essentially
represents birth now the second panel is
representative of the critical response
either by your own self-doubt outside
critical forces or just the
impossibilities presented through the
process of production this is what it
feels like to have your purest
expression picked apart by a thousand
angry beaks in other words this this
panel is back with our panel where you
sprout the giant wings represents that
feeling when somehow you and the idea
are able to transcend that death and the
panel before the idea so transforms
through the process of abstraction into
something that's larger ultimately than
its original so this panel is
Transfiguration so can anybody guess
what camera we used to make this it was
the Kinect and they said we actually
used the Kinect SDK because of its
amazing skeleton tracking abilities and
granted we ran into some trouble because
we were pointed at the back of people
when it was designed for the front and
their their hair or head would disappear
but all of these you know aesthetic
concerns that come up with using these
devices you you solve with design and
you solve with in person molding with
the data and trying to get it to express
something that maybe it wasn't intended
to express and those silhouettes are
derived from the Deaf map something that
was meant for computers to use but we
can use it to make it feel like
someone's shadow it's disappearing to
tell a story in a space and along along
these lines as have been doing this work
something has been boiling up in my mind
about about the means of production of
these of these projects and and I've
this curiosity has come from being so
deeply ingrained in working with this
these these cameras in this data in this
interaction and so it's boiled into
something I call aesthetic research or
critical aesthetics but
it doesn't really necessarily have a
name except when you're talking about
the specific projects which will show
you but I think it all sort of started
or what set this park was a science
fiction author by the name of Bruce
Sterling which maybe some of you guys
have read and he was giving the closing
keynote for the vimeo awards the website
vimeo were they and he had was speaking
on the vernacular video and it was a
long rambling talk if any of you guys
have seen him talk but at one moment he
stumbled upon this gem that really
clicked with me where he's talking about
the camera of the future i'm going to
play just a clip from that talk to see
the the next the remainder of my talk
except it's not a lens it's just a piece
of black fabric or glass it's an
absorptive surface it simply absorbs
every photon that touches it from any
angle and then in order to take up a pic
oh oh now hmm let's write it again it's
like it's not a foreshortened it's just
a piece of black fabric or glass it's an
absorptive surface that simply absorbs
every photon that touches it from any
angle and then in order to take up a pig
oh it's got short so what he says and
I've memorized the quote is in order to
take a picture I simply send the problem
out to the cloud wirelessly i compute
the photograph after the fact from a
computational data set from these
cameras that are perceiving the world
from every angle for and in every moment
of time and then I can photograph after
the fact and photography becomes a data
visualization problem of sorts a month
after the talk this camera came out and
you know there was a lot of buzz around
this camera and especially with its
interaction like how it opened up all
these new possibilities for people to
interact with computers and speak with
computers but the as the open-source
drivers became available and people
started to pull the images off of this
camera and those sort of streamed onto
the internet we saw pictures that looked
like this
and this is our friend Kyle he's a
member of the open framers community and
he was involved with a 3d scanning
research long before the Kinect came out
and so he was of course one of the
pioneers to be excited about you know
the opening of this technology for
creative projects and when I saw this
image Bruce's words were in my mind this
this surprised me and reference what he
was saying because this photograph was
taken from a different angle from where
that camera was and you can immediately
start to derive all the the implications
of that based on you know that his
science fiction prediction that more
sensors higher fidelity higher quality
you can start to think of what this new
computational photography paradigm that
this camera might be nascent Lee
ushering in and I wanted to explore that
even in its nascent form and one of the
things that I found the camera needed
augmentation for was the the resolution
of the color I felt like it wasn't the
photographic quality of the color camera
on board wasn't enough to satisfy my
needs of documenting things around so
working with a photographer Alexander
Porter we strapped an SLR to the top of
the camera and then I wrote software
that would capture it images depth
images at the same time that he would
take a photograph just by signaling at
each other and meet clicking a wireless
mouse and we took it to the subway
because we just heard a radio story of a
contract between lockheed martin and the
MTA where lockheed martin had promised a
massively advanced surveillance system
to be installed in the New York subway
system and the you know and as happens
in product and software design the
product team sold it before it existed
and then it never actually worked and so
resulted in massive lawsuits and also
thousands of unused surveillance cameras
just living dormant in the New York
subway system and we sort of use this as
a as an inspirational anecdote to
imagine what seeing through that
science-fiction surveillance system
would have looked like and so
we pulled this data together and we
started to combine it and we pulled the
images we polling started look like this
and they felt like this fractured
reality of seeing through the eyes of
the machine and it sparked imagination
all over the place and people that were
seeing it and thinking you know thinking
about this type of thing thinking about
since well what sensing technology means
and as we start to have this greater
collaboration with computers or
interaction with computers trying to
empathize and trying to understand what
it means to be seen and what the
limitations are so we were playing with
that aesthetic territory and could and
connotative that type of concern but we
were also still interested in this idea
of rifa tog raphy and in this image set
we would generate pairs where each image
was each pairs of image was the same
moment in time but visualized with
different data from different
perspectives may be easier to see if the
lights were down just a little bit but
this is you know getting at this notion
but we also realized a very interesting
problem then in our night are in our
naivety at that I couldn't just linearly
scoot these images on top of one another
I couldn't just take the SLR image and
just scoot it onto the Deaf map and have
it line up that there was actually a
greater problem going on and I was
really interested in continuing to
refine the aesthetics of this of this
process and combining these cameras
together so again the open source
community sort of came to my who came to
my survival and we we've refined this
process and we built this so this di
have these two to pass around actually
so you guys can check out
we started talking to Kyle who was
pictured earlier and Elliot woods and
various members who had done a lot of
like stuff that Andy's doing this
calibration combining different
perspectives and we open it up this
whole problem space of how do we take a
camera like this and combine it with
with this other camera to get at it at
to continue to excavate this idea of
depth cinema and so we designed these
mounts so that we could combine them
together and all these interesting
design problems start to come out of
this one so one notion and Pete other
people started to get involved and so we
eventually we created a the the rgd
toolkit which is an open-source platform
for depth filmmakers to begin
experimenting with this format and we're
also doing this while we're creating our
project so it's this it's a shared
notion of artists as tool makers and
making open opening a process to the
world as well as exploring it through
your through your ideas and I want to
pause briefly while you pass those
around and play for you the first pilot
film of the project that we're working
on now that I'm going to that I'm
ultimately presenting which is this idea
of computational of artists seeing
artists through a computational lens
yeah and I'll give a little more
background on this this project started
at the studio for creative inquiry the
the calibration system was that that
prototyping session was done as a artist
in residency program where Golan had
invited and the studio had invited many
different people from the community to
work together in person and that's where
we finally cracked the code on on
combining these cameras together and so
these in person intense sessions are
what create these sort of bursts of
creativity and ingenuity and I was so
privileged to be a part of that because
it's what led has led to all this
interesting research
conference in october 2011 which was
gathering to enable connect hacking into
teaching and hacking that was supported
by microsoft research yeah so gratefully
indebted and so my collaborator on the
film i'm going to show is also a fellow
at the studio Jonathon Bernard and this
is the pilot for the interactive
documentary that we're producing that
I'll explain later
if programming needs you can do whatever
you want but at some point you you I
wouldn't say biblical but it's like
there's nothing in the beginning or just
as you have in history it's just an
empty space and you can start throwing
things in there what I want to do I want
to program like I can paint so like 10
years ago I painted a lot and I want to
get the same feeling with code there's
so many things that you want to do that
you could do by hand and it would take
you a long time that if like programming
just increases your capabilities you
know almost exponentially i addicted
myself and I'm highly addicted to
programming you can't stop me sometimes
I forget to eat sometimes I forget to
sleep and I don't know can you turn a
pickle back into a cucumber probably not
so I'm here for life if you got me it's
terminal
this adding and taking away that's kind
of the design process now it's I still
feel in control but I like to be
surprised I think that's the beauty of
it the the thing when the system goes
beyond what you naturally understand it
to do and sometimes you have something
that surprises you now always look for
that that one moment basically
you get to a point in large
visualizations where it's just actually
too much data that look at this a flat
image and you have to do some sort of
zooming or you know flying through
navigation to get out of it to focus on
some small part of it because just it's
there's so many examples of this like a
huge hairball of connections and like it
means nothing at some point and he is
basic explaining how the database books
and he said that at one point here and
then you go around and there's another
autosampler there and another one there
and because this is closer this is the
one which is matched it's like you
explains everything like that is there
an experience that you experience the
desire to have
oh yes teletransportation yeah that's uh
this many things yeah okay yes I
shouldn't talk about but yet hotly
transportation is because you know there
was a time when they say why with video
conferences you won't need to meet and
but they don't works well you still need
to to be with the people to be discussed
and more compulsions you can do part of
it bye bye bye bye video conferences but
it has its limit so it means i have to
travel i'm sick of traveling and weight
of a negative one so if today
transportation is it was impossible is
it going to be picking family one that
we could start off with the point cloud
of this and then I don't know at some
point when I say talking about water or
physical systems all the points you
could just flush or turn into water or
something like that just to illustrate
the thing you've been talking about
let's just go
quick fortunate people and it's the
thing that comes after a video camera
it's the thing that comes after a
camcorder its connect with a digital
camera guess there's a sound microphone
and is this amazing blinding light
because this footage is rendered in a 3d
environment you have to look at it from
some perspective you can browse anywhere
in it you can see just edges of it or
the camera angle can change even though
this cameras might completely static
there's just a lot of room to play Italy
it's an experimentation you don't know
what will work there's no cinematic
language that is evolved yet so we're
it's sort of at a point to define that
in theory you could build the whole
universe inside your computer you just
don't have enough power and then you'll
never get there and it would be boring
to build the same thing all over again
but in theory you could that's something
I find kind of stunning so
that film is the protagonists our media
artists and programmers people who
consider themselves in this confusing
hybrid discipline but powerfully
creative discipline of being both
implementers and creators of systems but
as well as artists and for you know
cultural questioners and we continue to
build this database of conversations and
work working now in a way to present
them interactively and that's something
I'll explain in a second but I'd like to
pause and answer a lot of questions that
I received yesterday you give
demonstration of actually how this
calibration system works because i think
this audience is particularly receptive
to to that explanation so i'd like to
illustrate it so again this is the
problem space you have two powerful
pieces of commodity hardware and you
want them to work together in a new way
and the problem is that you have two
lenses which have different angles and
it's a classic calibration problem where
you want to be able to look to
understand the relationship between
these two lenses so that you can create
a mapping between the two different
types of data and understand where they
live in the world so the first the first
challenge is to find the lens intrinsic
so we need to know exactly the shape
empirically based on known data in the
world the field of view the the
principal point all the things that you
need to know about a lens to model it in
code and the distortion coefficients and
we do this for both the the connect as
well as the SLR image or whatever your
external camera is this is one of the
things that was really nice about the
lib free necked is it had in its first
release that had access to the raw I our
image so that we could actually see the
checkerboard through the lens of the
Kinect and get the intrinsics off of
that which now I hear that can now the
Kinect sdk support so we'll be able to
support our toolkit on that platform
which is really exciting and so the
second plaque part is a little more
tricky and that's finding the lens
extrinsic and this is a new system that
we actually just recently developed that
a different way of finding this mapping
so I'll explain it really quickly so the
extrinsic parameters again is a
translation and rotation between the two
lenses and the way we find this is we
collect a small data set of samples
after we have the the intrinsic so we
know roughly what the SLR looks like in
its own world and what the what the
Kinect looks like the depth camera looks
like in its own world and so we collect
a series of pairs off the off the Kinect
and this is actually the color image
that's been distorted to match the depth
image most of time you do it the other
way around but because we're really
interested in the cinematic quality and
the aesthetics the depth image we don't
want to touch that data the distortion
destroys it visually so we actually work
to pull the RGB pixels so that they
match the the depth image and this is
somewhat non-traditional this is
supported with a half to live free
necked but actually Kinect for Windows
supports this off as well and so given
this we actually get a really rich data
set of checkerboard positions floating
in 3d space just super powerful because
you can take there by inferring the
checkerboard the corners you can then
sample those points in the depth in the
depth image and so you get a known
floating data set with the of known
points at the same time you take coco
you know simultaneous coincident images
with the SLR and load those into the
into the software and so then you have
the same exact rich data set of 2d 2d
image points and four people have done
calibration this is a simple solve PMP
where you have object points and image
points and you can find a extrinsic
relationship rotation translation so
this is how we then understand the
mapping of those two lenses
and I'll actually demonstrate this
working in code and this is something I
hacked together last night so apologies
if it's a little um a little rough but
this is this is part of the RGB tool kit
SDK so this isn't the toolkit itself but
it's code written against this library
so it gives me a few simple things like
a like a timeline that lets me visualize
both the depth stream and the end the
color stream together so this isn't this
is an external video file this isn't
taken from the Kinect it's an SLR and I
can scrub through this you know finding
key phrases in these massive
conversations was a huge challenge we
need to have a visual timeline with like
Final Cut or Adobe Premiere based
editing system but built into our
environments that understand these
non-traditional data formats so this is
one of the nice things that we've been
building into the toolkit so the first
thing I'll do is just show the depth
projection matrix so this is a classic
this is the intrinsic parameters of the
Kinect visualized and it's you know the
classic pyramid 640 by 480 inside of
that we can render our projector points
and draw a wireframe of our subject
she's thinking critically she might even
be thinking about whether she wants to
teleport right now I think and we can
play this I would video conferences will
be wanting to meet and but they don't
works well you see need to be with the
people to really scarce and welcome
project you can do part of it back back
bye-bye video conferences so now we have
a big part of our data which is a really
nicely surface reconstruction we can do
some hole filling here which we have
stuff for but we have our form and we
have it mapped to this video but now we
need to actually combine our known
extrinsic and we do that using
projective texturing so here's a
visualization of the RGB camera in
relationship to the Kinect and this is
pretty good right there's our connect
there's our SLR you can kind of imagine
the lenses floating a little bit out in
front so that's about that looks about
right
if you look at where they land on the
figure it's about right so then if we
actually visualize our texture being
projected here you can see it coming off
of this frustum and then we can just
project that onto our form and now we
have free CLE texture have to order sick
of traveling is always so much time in
veins and in airport and waiting for
trains and I cannot flow so I wish to
the transportation is it possible is
going to be possible romantic and so you
start to see the camera language here
like I I have interactive control over
viewing this person talk and I want to
punch in at certain points i'm curious
about which angle looks more interesting
i pull off access to see the profile or
look for someone in the eye even if they
looked away from the camera and this
type of interaction has become extremely
fascinating and we really feel like
we're excavating a new type of cinematic
language so i hope that was elucidating
so that was a software demo didn't go so
bad though and actually I'll stop there
if anybody has any questions about that
technical stuff because I'm gonna get
back into the conceptual project so why
are you going back to the debating about
whether I should just ask and you have a
couple of questions by the calibration
procedure okay you'll see up here for
everybody else we left yesterday we were
joking about starting a calibrators
anonymous it's like a drinking Club but
people are like where's your fudge
factors like I totally hit it there was
like I've liked hard-coded it before the
demo but I definitely had a fudge factor
so yeah there's always you know it's a
it's a it's a voodoo it feels like it's
close to like electronics programming is
I've ever gotten writing software just
like never is right but um but yeah I'd
love to discuss more about how it works
and I feel like people in this room
probably are like you don't have to
deselect way too complicated so but now
I want to talk to you continue to talk
about clouds which is where we're taking
this this project and I want to really
introduce this idea of an infinite
conversation and this network that we
visualized here is an abstraction of
this idea where each one of these dots
is a snippet kind of like was in the
last video that you saw and we have 30
hours of these snippets even after it's
been condensed to things that are
potentially interesting to a lot of
different audiences who would have
intersection with this world of art and
code together and we're finding ways of
networking them of saying well at this
point in the conversation they bring up
this subject and at that point there's
all these digressions that could work as
follow ups and the same way when you're
having a conversation with someone and
you're excited you have a million things
you want to say and afterwards you
realize that I only said a small portion
of it well hopefully this system the
standard of system will present a way to
explore a conversation where you can
exhaust those possibilities or feel like
they're never exhaustible so this
traversal this it's sort of hard to see
here is the idea of one path through
this where you fall jump from know
node may be starting with a search query
saying I'm really interested in how
online sharing effects creativity or I'm
interested in the dangers of perpetual
novelty these sort of and then the
system constructs a story for you and
you're at will to follow you can watch
it or you can pick digressions so and at
the moment and this is where it gets a
piccie as that we have a Kickstarter
campaign that we launched maybe 48 hours
ago to help fund this this project and
you know I appeal to you on a personal
level if you think it's interesting we
would love your support and you know
there's some cool cool rewards that we
have that I'll show you but I want to
show to you for to further illustrate
the concept and to show you where the
the next generation of this this
research has gone I want to show you the
video from that from that Kickstarter so
we could really interested like the idea
that you know when I talked a lot about
the fact that our lives can be
documented through third data and they
are being documented through data I just
think it's so much fun to build projects
that people can bring their own stories
to in their own perspective a real
passion for code and impulse to share
their inventions unites this community
of open-source programmers who are the
21st century's pioneers of digital art
so you may be wondering what's up with
the way this video looks well it's
filmed using a format we've been
experimenting with called RGB d it's
video but with another dimension and it
offers the new potential for filmmaking
this is the first production of its kind
a film exploring creative technology
while using an emerging technology so
microsoft released the video is
controller code the xbox kinect and
immediately artists and designers
recognized as potential as a 3d scanner
the community work together to make it
available for creative projects we are
both filmmakers who work with technology
and we wanted to use this device to make
movies clouds will be the first
production to use this technique so as
we've been developing the film we've
been releasing our code open source
teaching workshops so that others can
get involved recently we begun to see
some amazing projects come out of this
so these are all projects and other
people made with our open source
software in the last few months
it's almost like people across the
planet are dreaming together there's
this imagination that's coming about by
people interacting with one another and
creating this totally different thought
space
we have over 20 hours of edited
conversations captured in RGB d and we
want to give you access to all of it but
in a way that can be explored
interactively like experiencing a
documentary in a video game environment
I'm interested in the in the underlying
aesthetic of the data interested in how
networks manifest themselves kind of
multiscale levels I'm interested in
emergent properties I'm interested in
our brains ability to to recognize
pattern
like break down the passing of time and
what the self means I think you could
just like disappear and reappear at
every in
in this phase of the project we're
building an application to present the
interviews as an infinite conversation
we imagined this working in real time
allowing the viewer to control the
camera flying through space choosing
what conceptual threads to follow or who
to watch
so I'll pause it there
because after that we just big please so
really oh so this is again this
visualization of this conversation where
we want to present these interviews in
this completely navigable world and we
we know it we can do it and we just need
the time and space and X you know
platform to show it so we plan on
distributing this on a mac and windows
application on these customized USB
drives that we're having laser etched so
there's also an art collector sort of
attitude to it or a collectible object
that well these little capsules will
contain this precious data you can also
download it but something that we're
excited about this new new ideas of
transmedia storytelling and film
distribution in with new media and yeah
so that concludes my presentation thank
you for listening I have a couple
questions actually first one is one of
the most interesting aspects of this RGB
video are actually completely novel ways
of thinking about transitions that I see
here your work you're basically the
actual shots of people in there and they
have a certain novelty factor right now
I don't know exactly how well it given
this given this kind of a quality that
you have right now in bed like we can
all that but i think is really
fascinating is that the ending
introduced a completely new language of
transitions so far leah crossfades and
you know yeah feeling things but you're
introducing all sorts of dissolves party
calls and all the stuff um people call
it co-located insane right right and
everything nice together right merge
particles from one person to the other
and yet also definitely yeah so my
question is what kind of tools and what
would it what are you actually building
totius important stuff because one of
the things is the pipeline that you're
showing is really you know processing
the existing software that but
transitions are purely animation the
right thought that you haven't recorded
right it's it's kind of animation
scripting and that so there's a complete
separate part to this that I'm missing
in your prison yet I think it's because
it's still the part that's niacin and to
be explored like when we do these
transitions again I like we work in two
modes we have this toolkit this this
user facing application that's
essentially a hack turned GUI turn
public turn stuck with it you know
there's no release cycle system in place
we're just like publishing this stuff
and that's you know that's actually sort
of a condemning middle ground because
you know you're stuck with the default
aesthetics have been programmed in that
interface so we're there with the
Kickstarter in the next round and what
we've learned from the community is
we're taking it two ways the first way
is building a more robust API so people
who are programmers can actually you
know get at them get at the vbo
structure get at the projective
texturing shader really document how
those work so that people that are
working in a high level can start to
script these things and make their own
things I grant green tea that'll be a
smaller subset of people and for
actually disseminating this into culture
and seeing really unique things we're
working on data format so we've already
done a little bit of this actually when
you saw the the thing with Aaron Koblin
II was like a triangulated form and he
was drawing in space that was done in
cinema 4d and we exported a bunch of
data as obj files in sequences and
actually with Maya we've been able to
bake the texture the projective
texturing into the obj file and then
export a series of an image sequence
along with an obj sequence so actually
our process is just becomes a data
capture
preprocessor system to let people who
you know filmmakers and animators that
are more expressive in these
environments really make this stuff come
to life and that's going to be the next
the next year is going to be full of
that as soon as we release these tools
there's so many people chomping at the
bit to get access to this data in there
you know native native habitat basically
so that's where I think we'll see more
of these interesting transitions I don't
I don't necessarily agree that it's just
the transitions that are interesting i
think yeah i'm saying is another thing
you haven't slept less time talking
about right i agree the other stuff is
interesting as well i'm wondering are
you thinking of doing the opposite so
like right now you're saying the you're
trying to export as much of the data and
put them into tools that people are
using but those are not technically
cinematography tools that are mostly
modeling animation to write measuring
which is interesting so the opposite
approach might be that you actually have
pre script animation sequences like
transitions right now in movie editing
tools that are applicable to our GBE
right that you can apply so you can have
a cross dissolve that's a point cloud
process all that you just apply to your
movie sequence in which case you don't
have to go and invest time to do the it
seems like the rights and opportunity
they're not not to learn maya and
writing you know a year getting getting
good about it but actually just focus on
anything movies in this new media right
i think you know like the what we've
really learned is that you have to play
ball with the ecosystem that exists and
i kind of had this bullheaded attitude
of like i'm gonna make my own final cut
for depth data and we got pretty far but
making that time line system was
insanely complicated and I felt like I
was losing my like sharpness as like
making art project because I spent you
know three months building this timeline
system and granted that also has
community benefits and we've been using
it in the film it has interactive
implications so I think as far as the
research strain of publishing this idea
and pioneering the thought space of our
gbd as a film format making it available
for creative uses and existing 3d
applications is going to be really where
the interesting work happens James
you're you're three months spent
developing via the RGB d
I'm lightheaded or make possible a
residency right hand right right the
yamaguchi-san Ferrari there's actually
specifically the timeline yeah I spent
three months in japan this summer at a
arts organization as a technician so
it's actually the opposite and but they
have a its contractual a contractual
agreement that everything you produced
there will be published open-source
giving credit to the to the organization
in fact I wasn't allowed to push code to
anybody's github but there's but it had
to be public so it was really
interesting restriction because they're
interested in building credibility
within the open source community as an
arts organization that funds tools and
so I spent three months they're kind of
in like as a like Buddhist programmer
and South Japan eating sushi in doing
really delicate gooey code which was
yeah and I did a few little art projects
too but so that's yeah but yeah I you
know I do agree with you that this
transition idea is one of the most and
you know interesting aspects of it
can you speak a little bit more on the
interactive aspects of who you're
talking about just because I think I got
a little confused as we're talking looks
turn to hear what we like in subway
right so the film the Kickstarter that's
for finishing up this playback linear
sequence with the effects being
generated mr. know this is the kicks
arse for the inner production of the
interactive documentary so I'd be able
to sell plug the camera mm-hmm factors
repulsors audio fitting in and out there
some of course exactly particle effects
is oh yeah is that something that work
you know we're considering i think the
camera control will be work within a
constraint like it's kind of like these
WebGL sort of systems where you're still
you're influencing the direction but
there's through narrative tension but
then you still feel like you're you have
autonomy with head movement but I think
the most interesting interaction comes
from how it influences the story engine
that you're able to steer it on a
trajectory so it continues to be
interesting so that's more like at each
moment these digressions will be
presented to you in the world that the
people exist in and again these
transitions when you make a transition
there's a spatial metaphor that happens
where you actually feel the camera
moving through from that node to the
next node and drawing the conceptual
relationship mapped into space
so one of the things that sounds
interesting about these project is that
the connect data you're showing and
people see it as near because the data
looks kind of here yeah so do you think
that if there instead of having to
connect data that is pretty bad in 0 you
have perfect reconstruction maybe people
could not be as interested I think if we
had perfect reconstruction to be so new
that it would be way more mindless but I
here's my response to that is really
interesting I've thought about this a
lot because in the way it's week we have
this term embraced the glitch which
means like we're kind of actually
celebrating the the nascent aesthetic of
this and soul are great so so did we
also embrace your others too is like it
feels like here great the you
haven't you haven't been working on your
working out the aesthetics of the are
TBD camera but then you did this other
thing where you added this SLR words now
like perfect video right what if you can
talk about that feels like you you went
back bar we head well it embrace the
glitch but you don't want to look bad
it's I mean it every moment you're
making in person aesthetic judgments
about it about it and I think the medium
raised you let's try and make it been to
your will and look good yeah I think
maybe it I would always expected slight
difference or something more like well
it has to be at least as as far as the
old media is concerned up to straight up
to d video it has some decent
look like that yeah that is definitely
good incidentally sort of like people to
walk away crap so so we're playing in
this dangerous territory of aesthetic
novelty and even in our own research we
look back at what we made three months
ago and we go oh that's terrible yet
remembering when we had that
breakthrough or like this is amazing so
we're constantly like exhausting
ourselves with this because it's this
research and aesthetic novelty and I
think what's important to realize and
how to sort of be safe in that territory
is frame your work in the fact that
you're documenting right now and so this
these conversations happened in the
course of a year at this at this pivotal
juncture of this explosion of the art
and technology seen a lot of it having
to do with the the public publicity that
came out of their role in the Kinect
hacking movement which is something
that's become a household almost a
household known movement which has broad
exposure to this community so
documenting these people in this format
right now hearing them talk at this at
their age and at this community it's
your you're making a document of the
moment and so we're hoping and the the
conceptual answer to this is that when
we look back on this it will feel like a
home movie it'll feel it'll have it'll
be like a super 8 film where you love
the inherent imperfections of the media
because it was so appropriate for that
time and it really captures that time so
rather than trying to make really slick
motion graphics something that's you
know only successful because of its
aesthetic perfection we're instead
saying this is the medium of this time
and let's let's capture the voice of
right now in that medium and that will
give it a timelessness time to create
the future Instagram that takes the 50
year from now and makes it look like the
RGB yeah I'm totally going to be up you
go so on your cell phone
in the in you know 20 years there'll be
a connect style filter which takes your
depth data tatters up the edges puts in
artifacts it puts a few holes in
people's glasses you know and yeah
exactly so yeah there's also a bit more
reviews um we've tried it I have one
aesthetic problem with that is the joint
and it I've been able to fuse the joints
like doing really hard it's really
computationally intensive it's quickly
makes me think that I should learn pcl
and then I go I'm just gonna go like
like I you know I'm not like there's
still an art artists side of me which
like is a fearfully inept computer
programmer that I like hide my code from
people sometimes but so there's a
computational problem with that that I
haven't been able to really overcome but
also aesthetically those joints I've
never been happy with because as you get
further away from the camera you get
these that the data is like nature
changes and when you fuse those I
haven't gotten it to look good and so
and also the way we're doing these
shoots it's so like setting up is so
instantaneous and kind of on the go that
we really can barely handle the data
flow of having two cameras let alone
trying to figure out a situation for
having more I think just very definitely
yeah our we've already I've done a few
experiments that I haven't published
because they didn't look nice but it's a
direction that I want to go like it's
it's like I imagine Bruce Sterling
vision where you actually just have this
full reconstruction and you can just
choose these camera angles and every
camera angle looks like a photograph and
that's that's fascinated me what granted
when it gets that perfect it won't be
interesting anymore but that's this like
this is asymptote that we're working
towards so it definitely involves more
sensors
have you thought about distribution it
seems that in this age of everything has
to be online and everything has to be on
YouTube probably the big problem is
these these are super data intensive and
I admire your way what's tributing than
one USB sticks I think it's a in some
sense it's kind of a little bit of retro
right now but it's completely necessary
because there's no real good ways of
distributing so have you thought about
like dissemination beyond kind of though
yeah let me I want to show you a demo
that we collaborated with a WebGL
programmer and we actually came up with
a really hacked little format to slip an
RGB addy stream into a web M video and
we made a zoetrope out of our friend let
me find this really quick so their early
steps in showing rtvg video in a browser
yeah I mean ideally in theory in
principle would be great if the clouds
generative documentary different browser
be great I think yeah it's it's the fact
that like you know are the WebGL is
still nascent enough that it
freaks me out if we're going to like
actually go forward and be like hey
everybody we're fun this documentary
they were going to publish in WebGL
whereas like we have the tools and we
have the know-how to make it work as a
rich you know desktop application so i
think we will do a follow-up project to
this to this film and it will have a
website i imagine mixing video content
with some WebGL snippets but that would
be an ideal format for distributions
this is why you would do it in real
kills because you want to make it
interactive and you want to distribute
the 3 dimensional thing easily through
the internet you want yeah it doesn't
matter what city unless you're changing
your game 1 yeah exactly it's
interactive yeah but then like so so now
you're there's this line you're crossing
between sort of like the what the what
your director wants the grand director
on this this this
sequence and what the user wants to do
and I was wondering how you I taught
about like this is the bait we're having
actually able to come over later project
don't know there's there's some effort
to say I'll just render the thing to a
regular movie you know trailer to
sequence and slap it on YouTube be done
with it right but that they were totally
totally taking away the interactive
component and then the question as well
as a creative director you might not
want any interactions so for clouds in
particular what do you admitted I mean
thing I think the important thing is
like as a creative director you're an
interaction designer and you you present
a system that's constrained but has
freedom within it and so that the person
feels like they're completing the system
by being inside of it so what that means
is like as it's an intuitive interaction
so moving the mouse will cause the
camera to move in an intuitive way to
like highlight different things so that
people can sort of investigate this
curiosity or just even the minor
parallax of a shifting camera gives you
this feeling of fluidity and openness
and so that that kind of interaction
that's constrained that you can still
move your head it makes you feel like
you're in a world in cloud you're going
to have like moments where the camera is
completely not under the user control
other moments were there it is under the
users control definitely back forth how
do yet I mean it's like a cut sequence
in a video game to like video game has a
lot of develop length you know language
for this like visual cues that show that
you're passing you're taking control
away from the user and showing them this
richer experience versus like now
they're right is the letterbox that's
right great cutter box exactly yeah and
he's like cinematic humor yes no movie
because this is so this is aaron koblin
in WebGL and he has all these little
snippets of his interview first
experimentation with collaboration
online was a project called the sheet
market is basically using Amazon's
Mechanical Turk which is like the eBay
for brains you can pay people and you
can get them to do tasks online so for
the project I thought hey I'll pay
people to sense and I'll get them to
this ridiculous activity i'll say draw
cheap facing to the left and i'll pay
you your two sons and I started
collecting thousands and thousands of
this is interactive yet I can click yeah
I can go down to guys different things
now and they're all detectives are
little outtakes basically so the second
project that I worked on with me that's
definitely one of the attributes of
storytelling and so this has a very
slight camera shifting across her black
perspective we're able to communicate
across cultural Bell Oh step getting
this to a surface app that shits out
hdmi 3d to your six hundred dollar off
projector like 3d active shutter the
chapter that you mean like stereo 3d
yeah yeah because you have all the dead
and yeah if you have this is this oh
just do it this I know this line I can
grab this it's yes cuz cuz URLs right
there we're talking a few ends of the
spectrum sunquest array or well you have
a creative director who has a linear
flow for the documentary style that goes
in here oh and then you're talking fully
interactive right right but she's
totally I'm going to be both we're
talking jobber miss we're making an
assumption on standard 2d projection
technology
technologies or far will replace right
at a very and dropping it postponed so I
haven't seen it yet I'd love to see it
in many people brought it out if there's
one pocket free I could be happy yeah I
would let I because these these figures
was really pop in stereo they would just
ended up an older guy 70 recent person
we just meet yet honda shake hands if in
fact this opened this openframeworks
directx maybe the the kinect for windows
our DVD toolkits release will have
directx supportive 3d which could be
amazing yeah it's something that it's
like there's i feel like i need 10 of me
and I think my it's about opening my
process more I'm trying to be as open as
possible but it's still like people want
to get involved and I don't have the
bandwidth to like do knowledge transfer
to them through email and then they're
busy to get you know this community I
feel like the communities there and
they're boiling and I just like every
morning I get up and answer two or
three-hour gbd emails about people that
are trying to get involved you're having
problems with it and so it's just a
matter of finding you know it takes time
to build that knowledge base and do
these new experiments but there's a
there's a whole log of things that we
want to try if we do talk later to be
awesome dis in the office I nasir stuff
yeah yeah great cool
i don't know how i'm doing a time a
little bit over we should probably peel
out of here at eleven fifty or so okay
no questions lunch thankfully yeah
thanks everybody</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>