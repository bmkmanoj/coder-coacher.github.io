<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning and Efficiency in Games with Dynamic Population | Coder Coacher - Coaching Coders</title><meta content="Learning and Efficiency in Games with Dynamic Population - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning and Efficiency in Games with Dynamic Population</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bAyMZkaty1A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
we're very pleased today to have avatar
Doge visiting us she's a professor at
Cornell University and has was you know
a leader in algorithmic game theory does
a lot of wonderful work on smoothness
and understanding the welfare of various
auction designs she won the Goethe prize
in 2012 for one of her papers on selfish
routing which was hugely influential and
the Fulkerson prize in 1988 so thanks
and so thank you so I guess this talk is
about games in general and I'm going to
somehow alternate between a sort of more
game oriented thing versus a more
action-oriented saying
whichever for each of you whichever you
prefer is fine um but I'm more focusing
on or one definitely focusing on is this
dynamic population at the end of the end
of this slide that this is a game that's
going to get played repeatedly by a
bunch of people and dynamic meaning that
bunch of who exactly the round is going
to be changing like it's not study
that's sort of the main focus so
anything that you do that is a repeated
interaction between sinks or people
where those things and people are not
completely stable that's the application
you should think of one application and
that's sort of the more game oriented
application is traffic routing of any
forum and I guess this to picture is
supposed to make you think of cars as
the form of tacit and packy does the
form of traffic that's definitely one of
those situations if you're a router or
if you think about it rather rather is
sending packets it's doing this over and
over time the packet destination origin
destination things change you know in
the mornings different evenings today
it's a little different than yesterday
that's definitely something that repeats
over and over
and the population which in this case
these sort of packet destiny origin
destination pairs are not completely
steady they also not completely worst
case it's not like you know I think a
minute from now it's completely
different than it was right right now
but something of that form so we got
going to think of my main focus and that
has been maybe the main focus of almost
everything I've done in in in this game
game theory area is what usually I've
been dubbed is the price of energy that
is the the quality of solution due to
selfish behavior of players actually
going a step further because this is
going to be a sort of dynamic set up was
the selfish behavior of the player and
they learning loss that this is the
population is changing and they're gonna
have to adjust to this changing
situation they're gonna run a learning
algorithm I'm going to assume they're
running some sort of learning algorithm
so lost were coming to two flavors one
they not do such a good job learning and
second they still trying to learn to
selfishly optimize for themselves and
that comes with a loss and these are the
two losses that you're combining here
it's a repeated game setting that we're
assuming that the same game is slightly
different population or players is
played over and over again and again you
can think of traffic routing which is
one application but if you prefer
auctions especially ad auctions a great
example of a similar not this kind of
option but adoptions are a different
similar situation where the advertisers
are over and over are trying to buy at
space like they need their money
tomorrow and they have turned day after
or every ten minutes or every second
they repeatedly would like to buy and
here it might be the players that are
somewhat changing there are other
aspects of the environment that there
may be more honestly changing then then
then the player population but things
are little bit changing underneath them
as they as they move so what do I mean
by price of energy so if I go
back to the last 15 years in me in game
theory and thinking about games and
outcome of games and social effort this
price of energy has been an important
concept and what we're trying to say is
compare what we used to try to say is
compared a Nash equilibrium that is the
stable outcome of a game compared to
socially optimal design that maybe it's
officially not sustainable bond theorem
of this form this tim roughgarden is is
in traffic writing that says that in
concrete this version of the theorem
says that if you compare the traffic for
socially designed solution in which I
tell every piece of traffic which way to
go
versus one that people choose the
shortest path by themselves then the
socially optimum can unfortunately
compare quite badly that is the mesh
could be a lot worse but if I give the
social optimum double the traffic rate
which is what's written here then
apparently it's at least as Nash's at
least as good as the social optimum is
double traffic rate which would say that
if you design your network to twist and
drop double traffic rate you're okay
more generally this is what price of
energy was so cost of the some Nash
equilibrium against compared against the
social optimal solution and you should
think of this as cost of the worst Nash
equilibrium that I don't want to be
predictive of which Nash equilibrium
people they chose as long as they chose
a Nash equilibrium the cost is and and
the spice of energy is not too bad then
the cost is not too bad okay so this is
the basic setup and question oh yeah
definitely
so one form of refinement that's fair to
ask I can limit myself to pure strategy
Nash equilibria and that might be better
than mixed strategy Nash equilibria
stable that's another saying yes yes you
can try to improve this by selecting the
right Nash equilibria I wasn't going
down that path because just about
second from now I'm going to try to make
it worse not better so okay but as I
told you oh yeah this is the traffic rat
thing there's also a lot of
auction-based kind of game sin and
anyway forgot those lines that's a
mistake it's a bunch of auctions that
you can analyze this way
unlike the sort of classical auction
design which is truthful trying to get
people to reveal their valuation here
the principle is that you're running
something quick and dirty and hope that
things work out okay and I put like you
can selling multiple things you can sell
them at simultaneous first price auction
sign within a second price auction
generalized second price for the
auctions and actually maybe won the
paper of arts that I like is you can mix
different kind of options you can buy
some items on first price and some one
second price or some on high production
and you can try to prove things about
them none of these are truthful because
there are all kinds of issues between no
multiple items but you can analyze it
with the price of any kind of setup and
there are lots and lots of papers by
many people some in the audience and
some not in the audience and I maybe
should point out vassilisa pennies as an
author in many of them who is one of the
lead contributors here and a co-author
on the but I'm presenting and hopefully
you know you guys don't think highly of
him as one of your colleagues here or
almost here okay but I'm going to think
about repeated games and repeated games
set up which is why I'm not going in the
that's the extension direction I'm going
so I'm going to think of a setup where
we repeatedly play the same game and
this is a simple chart to tell you what
I mean by a repeated game so again think
of check of your traffic I think think
of auction or sink at an upstart game if
you want so this is minute number one
where there are players and they age
each shows the strategy the upper index
is the minute
or they step number one it shows the
strategy and something happened and then
same thing happened the second day and
something shows the strategy something
happened every day they chose a strategy
something happened I'm going to assume
that they go on the exact same game or
might be quoting some variational game
but the simple way to think about they
playing the exact big game every single
period and the players have editing
values over periods so for example if
the traffic wrapping if you're sending
sending packets you know over and over
again on time you care about the average
delay that would be a you care about the
sum or if there's some cost you adding
up the cost or in the auction setup you
you want as many impressions as you can
possibly get a little bit evaluation so
that's the two basic assumption be
playing the same game the same setup
this additive value is important that
isn't what you've on previously doesn't
change your desires tomorrow you still
want another item so that's what's
happening and I guess what I'm trying
tomorrow is learning is that here when
you start you really die rather clueless
and you don't know what's going on but
over there are many many steps later
hopefully but smarter like you somehow
see what the other people are doing and
and react to it in some way now what
could I mobile as reacting to it and
something that's not on my slide and I
could do it but it would be very very
complicated I can think of this whole
thing over the whole time horizon is a
single game is very complicated
strategies and analyze and try to
analyze it and that's classically as
being believed to be very very
complicated and hard to think about
exactly what might be right thing to do
or even worse there is the folk theorem
that every idiotic thing is a Nash
equilibrium here so - equilibrium is not
going to be a powerful enough concept
here's another thing I can do I can
think of the classical one-shot game
Nash equilibrium as my solution concept
so what would
mien is a belief that somehow whichever
way these players will figure out how to
play something stable so they do
something upfront they know what to do
but when they know what to do they
settle into a Nash equilibrium and I
mean the Nash equilibrium of the single
shot game so what that means is that
they repeatedly play the same strategies
over and over again and what's better
they happy visit and they happy visit
this is the notation I'm going to use
and I have a dual version whether you're
gaining something or losing something in
a cost version it says that they playing
this strategy vector F and this is the
cost of player I in the strategy vector
and it cost in when they play that
special strategy vector that smash is no
better is it's less than or equal to any
other cost they had had they deviated to
some other strategy or similarly
mattingly utility backwards that is
utility is bigger than utility for any
other strategy that you're going to be
talking about the relationship across
things yeah games or evolutionary games
or something like this explicitly to
like try not to think about the links
across different stages and and the
second thing is I think in this context
because you're usually thinking about
very large populations one very good way
to justify this sort of thing that
you're doing is to say that nobody
believes they have much of an influence
on the aggregate state that people are
then going to react to and so it's sort
of reasonable to think about these more
one-shot notions rather than thinking
about the long term evolution
ok so it's a great point and I certainly
have an issue of is what you warned me I
will have an issue with priming the
audience to think about across perhaps
and maybe I should use better words but
what I really intend to do is go halfway
between the two but I don't like this
because it's very stable and there's
this magic of how they found the nage
and I don't like the other one because
that's very complicated and I what I'm
proposing is a halfway solution between
the two I'm happy to take your advice to
avoid the word repeated games for those
who maybe one will still see what I mean
by learning in game so and I don't like
the stability assumption because it
assume that they're reaching a stable
outcome for those who more primed in the
computer science literature computing
that thing might be computationally hard
so that might not be very rational you
vote they can't do it because it's
computationally hard to get to that but
here is something we could do instead
that's very similar to this condition
without erasing all that green up there
and here is what that looks like so
they're doing something which is not
going to repeat they're not going to be
stable but instead it has this Nash
equilibrium property which I'm going to
call the recruit no regret property that
is if I compare it what they cost they
got and now this is sum of cost over all
the time compared to a single strategy
with hindsight they should do at least
as well that would be a really no regret
so that says if there is a super good
strategy acts a stable strategy please
wake up to it and do it now maybe this
could easily be a strict inequality
it could be strict for every single acts
it could be that somebody's player is
really good at this and does X when X is
a good thing to do and the survive and
why was a good thing to do but if there
is a stable X that's good please do at
least as well now I didn't actually
really mean this because there's the
learning error in the beginning so
there's a lower learning
this so what I really mean I'm going to
say that the regret over time T is the
sum of the cost that the guy achieved
compared to the fixed strategy behind
side accent I want to assume that this
regret error goes to 0 or or doesn't go
as fast as time T or regret over time
goes to 0 and in particular I want it to
be little lofty that is go less less
fast and T they're very very simple and
natural algorithms indeed alive like
literature of them that basically get
growing at the rate of routine yeah yes
ok that's a good point and I in fact it
should maybe should have been on my
slide it's not in order to get this
learning great I'm assuming they get
feedback about what would have happened
had they done the others all the other
strategies so for example if they know
what everyone else is playing then
that's full feedback and then they can
learn at this rate you can achieve the
little o of T rate even if you have to
do an explore explore exploit trade-off
that is you have to try other things to
learn what's up is what would happen in
these other strategies so there are two
things on the slide Bonnie just says
regret which I can define independent of
the feedback and then I like algorithms
that have little loyalty feedback and
then I make a concrete claim of how
small I can push that dam how small I
can make the regret get that second
statement depends on having full
information if you have to explore and
try to figure out what these other
strategies are that will hurt you and if
there will be a little off-key regret
but not quite as small as what I'm
giving you now so the assumption is the
strategies they know the game definition
thought you might the strategies are
like you know you know you know you know
the network you know which way you can
drive you know the
if you have a map but you don't know
it's very bad traffic is and there are
two options up to here it doesn't matter
which option you're in one option I mean
ignoring the last part one option is
that your only experience the things you
you drive someplace you know how long it
took or you bid on something you either
won or you didn't you know how much you
paid and you know whether you won or not
but you don't give you feedback of the
form of what happened in the other rats
that you didn't try what's easier
mathematically if I give you full
feedback that is I tell you that value
didn't win because you bid was seven on
the winning bid is eight and someone
else with that or it's a radio who
announces well how about the traffic
jumps were everywhere this for feedback
that is you know how bad but when you
would have gotten had you went someplace
else or did some other strategy you can
push the regret down to Ruthie you
actually you only need to know you you
know upfront your strategy space you
know but you don't it doesn't matter
what there's no and you'll get feedback
of your cost you don't know the cost
function you don't know you don't have
to necessarily know how the cost
function depends on anything you just
experience the cost in a full feedback
version and you also get feedback of
what you cost would have been had you
done something else in the limited
feedback version you have to experiment
to figure that I question
a different strategy would have changed
individual time steps on how to have my
correct so again the assumption is and
it's a great question so maybe stop me
without any other sort of clarifying
questions because these are important
what's the feedback structure here I on
on if I want the best algorithm the
simplest algorithm both simplest and
best performance for learning then
what's in the bottom here that is their
growth only with square root of the time
then I assuming full feedback by which I
mean not only do you know what happened
to you but you also know what you would
have happened to you had you done
something else so in the wrapping
context is the assumption is you drove
it took seven minutes but you also
listen to the radio and you heard that
you know the Massachusetts Turnpike is
really congested people are standing
there for two hours so you know how long
it would have taken had you've gone on
the Massachusetts Turnpike that's the
full feedback there is no assumption any
place and this will actually be
important that you know what the
opponent's experiencing you don't know
that the appoint but what action of the
opponent causes you to have this sort of
pain in your life
in fact the learning deserts are not
only applicable in games but applicable
in a much broader context where you have
some set of actions that you can choose
between which we can call strategies and
you get some arbitrary worst-case
feedback of which strategy is costing
how much at every time step which can
vary by time steps which we know why
because your opponent might be doing
something else yeah yeah oh I see
I guess just saying I have algorithms a
guarantee I have no regret and I imagine
you know I'm into this environment where
people keep doing different things
you know maybe it'll be really hard for
me to do well against environment there
was something of a cell block the way
people were playing going into the
future when I start me with you I'm
learning I'm learning from the I'm
learning for my outcomes people are
doing different things and people
behaving and maybe in this I don't know
what this regret is about exactly I know
that like I should be able to so it's a
great questions and I don't know whether
this is maybe I'll let me respond to it
because you asked even though I was
planning to think about it a little bit
later so I guess one important fact here
is they're very simple and very natural
logarithms that that guarantee this no
regret in fact if you're running an
algorithmic tool to adjust your bidding
it's definitely helped to avoid doing
now the one one important or doing that
enough for this the one important thing
that you have to do that's sort of one
way to fail one classical way to fail is
that the algorithm has to have some
randomness in it
so in averse case setup because I made
no assumptions of I didn't this this
these algorithms don't make assumptions
that the other guys are learning they
can do actually especially screw you
that's the plan so if you're running a
verse kiss at deterministic algorithm
and the other guys really want to screw
you then they can do like whatever route
you choose they are choose it too and
then you guys are all like on top of
each other it happens all the time
so in order to achieve the no regret you
have to randomize to so that this kind
of opposition doesn't work but very
natural very simple algorithms achieve
it one but I also as something I
mentioned you when you talked earlier
this is not the people I'm talking about
but it's a great question of whether
people actually do this so I'm not going
to make the assumption there anyone is
using this algorithm or that algorithm
or any of the or or a you know
fictitious play or any algorithm that
all achieved this I'm not making that
assumption I'm making the assumption
that they achieve this whichever method
I don't care whether one of this on the
list or not on the list so much supposed
to look at data and figure out do they
achieve this and I guess for a concrete
better I don't know about the proper
time here is but how much regret that
all is there and we have been looking
with Vasily and Danny snake a pull-off
at Microsoft data actually and they're
interesting they're very interesting
facts in the data we have a NEC paper
explaining our current progress and many
more questions that we are trying to
address that we have an address but we
do know right now for that particular so
Microsoft data looked at and adoptions
it's an older data set where ads were
more simplistic so you didn't have to
worry about the fatness or size of the
ad it was one slot one ad which makes
life easier and we looked at only at
bidders who frequently changed a bit
that made made us think that those guys
are using some sort of algorithmic
adjustment tool because otherwise humans
don't change that frequently so there
must be algorithmic on the plus side a
good fraction of them 60s and what
percent have no regret interestingly
some of them have negative regret that's
a form of no regret I told you you can
do better than this no one tells you to
use a fixed bit I'm only comparing you
with a fixed bid it's physically
possible to get negative regret and some
of them do have negative regret that is
they do better with any fixed fixed bit
designs right but about 30 to 40 percent
I forgot the number actually do a
positive regret and quite high compared
to their value some random number
between like essentially uniformly
distributed between 30 40 % of the value
so there's a large fraction in the death
of your looking at that have that but I
don't know actually
so this is where every curve this is
where we
currently and it would be interested yes
no I don't know the answer so what I
really want to know is only look at
looking at so were they trying to be
smart where they just frustrated at
being random so one thing I would love
to know is whether if you look at the
sequence of this events the people or
the listings that have high regret value
what happens to the how do they change
their bid and do they go towards to
think I think they ought to be reading I
have an idea what they should have done
are they going towards that thing a and
B does they regret go down started doing
actually yes so we have some correlation
of how your current regret value
corresponds to the amount you're shaving
your bed in our estimation of your value
which of course is also questionable
this is an ongoing project we put the
current version in an easy paper and
which is up on all of our web pages and
the DC web page but there are still many
questions here I think it's in line with
people are learning it feels in line
that the standard might be hard for some
people because some listings have have
have do have regret there's also the
issue of you know do we have the
objective function right that they are
optimizing we are assuming that they are
optimizing number of clicks times some
number of value and some people might be
optimizing something else but it's a
good question one thing to notice here
and it's in part why I put in the the
fixed strategy Nash equilibrium into
this presentation is that this
assumption is weaker than than the study
Nash equilibrium assumption like Nash
actually if I went back to the version
with all the green on it that of course
has no regret one particular way to
reach no regret is
that you to find a Nash equilibrium but
there are other ways algorithmically
more reachable that are reaching the no
regret
yeah because even though I agree with
what you just said
any reasonable process of learning will
only reach stable equilibria they won't
reach unstable equilibria so there are
some Nash equilibria which any
reasonable learning dynamic will rule
out and yet which isn't ruled out by
exactly the definition you just gave
excellent point so I guess the
definition I gave and this is why I said
I'm going in the other direction from
from finding particular equilibria
actually so ok the point you're making
is that there are some stable equilibria
that you there dynamics that might find
it and then they're unstable equilibria
there is no I said a decent dynamic has
to randomized or any hopeful dynamic
must randomize and no randomized
equilibria can settle on them because
they are unstable so this learning will
never find those on the positive side
good point yeah yeah excellent point on
the other hand there are dynamics that
this thing can go to there are not
equilibria so there's no guarantee that
even if I said this this guarantees
about the single player loan I can
guarantee no regret for myself
independent of what the rest of you are
doing this by the way a very attractive
property of learning because just in
case the opponents playing stupidly I
shouldn't play Nash I should take
advantage of the mistake the other guys
are making and learning does that for
you thanks perhaps you take advantage of
them
but even if everyone does no regret
learning and even if I take the order
one error or dirty air or at and take
the limiters as time goes to infinity
but they converge to as what's usually
called correlated equilibria more
putting it the other way they don't
correct no not necessarily converge to
anything stable but they might sort of
cycle around like for a while on meaning
and
and Christian mix-up to the fact that
that you know I'm doing something and
beats me and then I said I wake up to
the fact there were Christians doing
that I guess I should change my strategy
and we can cycle around forever never
never ever settling on anything yet on
average we have no regret so if we're
not doing anything well so I guess this
assuming things are somehow normalized
to say the maximum cost minimum maximum
utility is born you're making some
amount of utility I think a better
measure is the amount on average like on
average how much you cost how much cost
you're losing every day you're making a
dollar on T days you make T dollars and
relative to that you lost to T but
relative to what you made
you're losing less and less I mean the
relative error goes to 0 remember the
values additive there are T time steps
today 60% of that I lost two things so
it's it's an it's an off like little off
compared to what you're winning okay so
that's yeah okay so in a beautiful paper
or pair of papers really of a problem
and and quarters including
Katrina Liggett and Aaron Ross they
introduced this phrase called the total
price of energy which is trying to live
the price of energy to learning outcomes
so now I took away the stability I want
to know how bad our learning outcomes
and I guess going back
and the framework I'm actually also
going to use astre is is this smoothness
that that Nicole mentioned in her
introduction is analyzing price of
energy via a particular technique that
has the beautiful advantage that it
works on learning outcomes it doesn't
have to go to Nash and here is what it
does and this is the schematic picture
of what I'm sure what what what these
people suggested so price of energy
bands for Nash equilibrium so this is
this repeated game I now be very clear
reminding you that it's the same
population playing at every single time
step and they they claim that for most
of the price offender here almost all
the price of energy band for you it has
the nice property that in this setup
when you repeat the players over and
over every time step then even if
they're just learning it may never
converge to Nash but the same price of
energy bands work and this I really
really like this piece of working as as
Nicole pointed out I've been working on
consequences or applications of this
because I really really like it but here
is a handicap which is today's talk is
about what do you mean the study
population who forced these people to
keep playing in any game something's
change and in my model whatever changes
people will come and go they will
sometimes vanish and our main theorem
wants to do that this corollary applies
even if I take this last line off of
here so that's the main thing that we
want to do so what I want to do today if
it's convinced you that the that I can
take the stability issue assumption
error so I take a game that's repeated
but I'm going to make the player
population change and how am I what's my
current model of pay population changes
is a mix between worst-case and some
amount of randomness if I first get
changed the pair population from step
one to step two like well there was a
different game so I don't want to do
that
but if I make a sort of Bayesian
randomness replacement assumption
the hosting is very steady and they
don't have to learn so much so what
we're doing is halfway between the two
and here is it what we're doing every
step is affixed every player with some
fixed probability P the guide will
vanish and the replacement is an
arbitrary worst-case other guy now what
does this do
first of all and I'm going to play this
haha I can push that probability and
still have the serum so my main sort of
game dot or whatever that's wrong word
here main thing I'm playing is how high
can I make this probability PB and still
make a positive statement here so if P
is really really big s in like one then
there was the case everyone vanished and
horn you guys showed up if I make P
virtually zero then kind of no one ever
vanishes and I'm one will try to play
this but peace can I make reasonable
statements for this is the individual
probability remember we have n players
so in expectation n times P players will
vanish and I would like to make P sort
of constant that's my goal and we're not
quite up there but we can basically
we'll make P roughly one in Logan so a
very large fraction of the population
will get replaced every every stop oh
definitely definitely
so if I pushed it all the way you're
right all constants are low to high I
can't go that high because then they
they can only they're going to have
money Ruth P money one over it Ruth P
like that for sure and worse than that
because they're playing in this changing
environment okay so this is the kind of
I didn't invent a new word and I'm not a
big fan of new words so I adopted this
total price of energy to also mean this
maybe it should have been this so this
is the cost of a game where I change the
notation to have the player types
because of my simple model we always
have an player
man isn't changing this is just because
the math is easier that way
so one guy vanishes another guy shows up
but I guess two examples just to sort of
get you the sense of what's going on
here
one in action and one in one in in
traffic I think so this is the simple
Network here is a guy that we go on to
watch maybe he's around for these stops
maybe even he showed up this was the
case there are tons and tons of packets
going on the upper right and so he's to
going to choose the lower out because
that's good for him
he learned that but you know as things
happen in the middle of his still alive
those guys all vanish for whatever
reason and replace it with these guys
and then he's going to have to change
his strategy so what is the example try
to illustrate then you going to have to
adopt because the refreshment is worst
case so I could strategically even
though I'm randomly deleting people make
it one kind of bad people show up in the
worst case and if you're not adopting to
it you're gonna be hosed this happens in
auctions just as much here is there a
guy that sort of thinking whether say
you need demand kind of buyer he likes
either the orange or they read whatever
balls there are many shows up or these
other guys are all buying that upper
ball in this right congestion and it's
very high price but the yellow or
whatever that color is is kind of cheap
he buys that well it turns out after a
while those guys Phoenicians replaced by
this guy's here to switch so whatever
context you're in the game matters and
if the population contrasts on one side
you don't have to go over to the other
side so you have to learn you have to
adopt so Christian is asking a good
point connecting the two things I've
said so far which actually don't connect
connecting yourself is the fixed-fixed
strategy besides that's a really big
benchmark you can't get anywhere on that
one so I'm gonna have to do better than
this big strategy with benchmark in
either of these Kings is this horrendous
because half the time you're doing
something really bad that's bad half the
time doing bad things that's bad so
the sunset is not strong enough no good
I don't think it does but I guess I know
okay so what Jason is asking my goal
wasn't to have people learn but instead
my goal was to have a price if any
result it's conceivable I don't believe
it's true but I certainly don't have a
counter example on my slides to prove to
you that you can't do that you won't get
a prize if you don't if if they doing
this week learning that's we can they
won't be able to get the present
if you're assuming that if regrets
bounds are not very meaningful that
makes the result
how do you yeah sure it would be harder
to prove in at the moment I don't have a
proof so what Jason proposed is that
guess could be true I don't believe it's
true I do not have a counter example it
definitely would be hundred and my
theorem what am I going to do is assume
that people are little more clever than
this and I will tell you in a second
what format of cleverness am i planning
to assume yeah are you going to say
formally what what is the game here and
what does it mean to replace a player or
should I just think of example example
like will this work for any game where
you just delete a row from the matrix
oh um let me good question and let me
actually I just say something a bit more
formal than what I did so far and can we
come back to it right at that slide of
what do I mean and I think you probably
bestop thinking of this too and these
kinds of examples where it's easy to
understand but it's a general game I let
me come back to this question okay so
I need this last line I need to learn I
claim I need it I guess I should have a
counterexample that the learners need to
adopt it's also natural to adopt if
you're one of these players it just
feels that like you need to do this like
otherwise you're not playing very
reasonably so what do I mean by adaptive
learning so I guess the good news is as
you guys may be many all of you know
among other reasons because Shawn caca
there was or was here for a long time
there's a large community of people
thinking about how to prove learning
bands and what kind of learning bands
one can prove and I guess the version of
the learning one that I would really
like for like for I like thinking of as
a form of recency bias that is if the
population is changing if you know back
10 years ago
you guys always used to have the
coffee-room back in this other room
I accidentally showed up in that room
first that's why I remember this I need
to learn that now you don't and if you
if I give another color film tomorrow I
should be smart enough to come here this
time because I have recency bias I
experienced that yes in the past it was
over there but nowadays it's in this
room so learners should have English
intuitively speaking recency bias recent
experience should matter to them more
and in the mall of this makes sense
because people are vanishingly small
probabilities so things that happened a
million steps ago a lot of those people
are gone but things that happened
yesterday now that's pretty relevant
very few people left so recency bias
makes it make sense a format of learning
that the learning community and likes
and it's a very nice extension in the
current version Boston in in Boston in
the algorithm itself and in the regret
they do is the following form of recency
bias if Sun fixed strategy if a strategy
X is good for a pretty long period a
period from may be calling tavern to
tattoo that's a long think of it as a
long period then you should make up to
this and during that period
play it if the period is short I don't
care so I said that you regret should be
little low of the period length when a
pair of this and I really mean as a
period growth as a function of the
length of the period so in particular my
favorite I think all of our favorite
algorithm is the Lucia Pyrrha Arboretum
which is a very nice and very simple
adaptation of the classical hedge
algorithm but their earlier algorithms
here the history goes back to it or
three Learner paper and mom woman
masseur has a more complicated algorithm
maturing the same regret rate so it's
still the case that multiple algorithms
can achieve this that is learning rate
or error rate or regret rate
proportional to the root of the error
root of the lengths of the period so now
I don't have a fixed fit strategy over
the whole period I would like you to do
well the fixed strategy against long
enough periods and I what I mean here is
your error is supposed to grow
proportionally to the root of the length
of the period okay and and again this is
doable and the algorithms are very
natural and then and there multiple
algorithms out there that achieve this
band which are the greens here in fact
the learning paper for most three is
offering an even more sophisticated
version of what you should not regret
but this nice this simple one is natural
for us and what they really do is that
is I think fella described as a form of
recency bias so but but the algorithm is
due to get no regret is remember how
much regret they have and trying to act
on this that is if you regret something
then I'll do it maybe that's a good
thing to do and what these algorithms do
is every now and then wipe out the
history just forget what you had so far
just start again that's a form of
recency bias no no I mean recency bias
seems natural and the algorithms that
achieve this do have this do seem to
have this property but for example
didn't formally define what I mean by
recency bias and if I were to define it
I would more go for the incan style
definition from what these guys do but
if you want this sort of a band which is
this top line again because the big
little law is what I really want and you
think of your last period last hundred
step last thousand steps you do need to
have some sort of recency bias you
should do okay on the last dozen step
even if your previous history is a
million step is telling you something
else because that doesn't matter now you
should react to the last time and so the
evolution of the in order to I honestly
I should I should check I certainly must
bear of a dual literature on learning on
the the classical learning without this
business abayas version that there's a
CS econ dual literature with similar
bands but I guess this particular and
and also the learner paper is an econ
publication like it's an e company I
forgot which journal it was published in
but the rest of them are CSP person I
guess I wasn't aware that this
particular form of regret against the
small interval I know I wasn't aware of
any follow up of this learner paper or I
should check I sorry my mistake I should
know about this so like I said last time
I guess we I'm again using this as a
behavioral model and not as an algorithm
I'm not telling you how to do it I'm
telling you that these guys know
algorithms had to do it may be many more
guys know algorithms have to do it there
is actually there is a
a recent film very paper that I do know
that is using the more econ flavored
format efficiency by exclaiming similar
bands which is very recent so at least I
know some piece of this literature this
has a better convergence band but the
error bump here is better but anyway so
again I'm using this as a behavioral
model that is unassuming that whichever
method they use whether they write the
feelin Berg paper whether they like
blonde monster or who Shapira they're
doing something that achieves this
regret and the main thing we're proving
and I'm going to try to at least give
you a hint of a report give you an idea
of how we prove it is that the following
basic flavored serum that we try to
prove results on the price of energy in
this changing environment with a
probability that's roughly proportional
to the log of the population so very
high turnover and these are the two main
points I want to make I'm doing worst
case replacements of people so I believe
that causes that it to be really
necessary to to for them to do adaptive
learning but I guess Jason wants to
contradict like proof to me that I'm
wrong but at the same time I'm doing a
slow replacement process that is every
player independently vanishes with some
probability roughly 1 in Logan that is
the fact that they all vanish all of a
sudden and some other guys show up is
incredibly unlikely and in this case I'm
going to do it badly and that's ok
because that's an unlikely event so I'm
using this moose game framework of that
was invent suggested by tim roughgarden
who basically suggested this framework
to unite or unify a whole bunch of price
of energy proofs and because i'm running
out of time here i'm going to maybe do a
little bit less business than I
originally intended and I'm going to
instead switch to the auction version in
which maybe this is a little easier to
understand so here is a version of as
business proof and because I skipped
through the smoothness defining slides
I didn't define what smoothness was but
the thing that's happening here real
business here's an auction the setup
here is there's a unit demand buyers
they would like one of those balloons
and might have preferences between the
colors okay and they have different
values if you think of the optimal
solution the Optima is a matching
problem every buyer is assigned to an
item or most of them say that buyer is
assigned to that item that's reactant or
solution one thing they can do this is
the spacer strategy or action that I'm
going to have them think about is to bid
ignore all the other items because
magically they not know which one is the
optimum which one they should get in
optimum and in this I just bid on that
item and bid harmed a value on the item
this is a first price auction maybe the
shaded value of they'd be of their value
on that item this is something they
could do I don't know how much they
would know to do this but they could do
it and if they do this we get this very
nice inequality which turns out to be
super helpful and this is maybe worse
reading off it says that if they play
this special bid have the value on that
item then whatever everyone else did
doesn't matter they either get half the
value that's because they beat half the
price if they win the item then have the
value is home or the price was high if
the price was high whichever your claim
is a good thing because auction are made
a lot of money the price was high so
either they get half the value or the
price was high and in particular this
inequality was so they can there's a
special action again there is no way for
them to know but there is one that if
they did that there's some sort of
guarantee and the guarantee basically
says I can get a good fraction of my
optimal value that's the first half or
the price is high and high is there is
no multiplier here that's why it's well
if I do this and just end it up over all
the players and use this
they're the Nash equilibria you don't
have regret against this particular
action your home that is and I maybe did
that in this slide I said okay so if X
is a Nash equilibrium then you don't
have any regret against some other
action so in particular don't have
regret against this particular action
this action has an inequality which we
can use from there and using that
property we get that the utility plus
the price is at least the value which is
have the optimal so very simple in my
party basically just riding on a single
circle I squared here that is if this
kind of property holds then then you can
you're not regretting that particular
action that gives you a very fair
guarantee and what's nice here and
that's again in blue they don't have to
know what the magic action was because I
assume they're learning they're not
regretting anything this is the action
I'm using in the proof they're not
forgetting other actions either but this
is one that they should not be
regretting so this is a classical
smoothness proof and as the kind of
proof I'm going to use now what do I
want to do in my remaining one minute
what do I want to do to combine this
proof there's this dynamic population so
we have a lot of pieces that I really
want to put together in one last way of
how to combine this so what I would wish
to have is this inequality I would wish
to have the property that these players
as they're playing all kinds of sayings
and they have some type they're not
regretting whatever the optimum is but
they knew every time or at least on the
average the bad news in my life is that
this optimum the special action which
remember depending on the optimum is a
very changing action every time
something else happens the optimum
changes all the time so for example is
the last demonstration here is again a
unit demand action kids these people are
getting the optimal item but one guy
vanishes what happens the optimum join
them cut all the changes
everyone everyone should get something
else not because you want a change
there's a chain reaction everyone
changes in the optimum so there is one
piece on missing and this is the last
piece I want to tell you about that I
want to do this I just don't want to do
it with the real optimum I want to play
this game and this is sort of the high
level serum and I can come back to your
question of what I mean by a game so if
I have a game that satisfies the
roughgarden smoothness property that
it's in the class of games on which we
have a smoothness base price of energy
band and that's a pretty restrictive
game so like you know I can do it with
replacing rows but they're not going to
satisfy that property so I think you're
better off thinking of examples and the
second point the welfare optimization
problem the optimum solution of which is
this magic action came from has a stable
opt cost to optimal solution has a no
other solution which is with the
property that the the people's
strategies or people's actions don't
change much it's very very steady as I
change people
unlike the matching but it was super non
study something very steady then you
have a price of energy band okay and why
because what they needed to learn here
going back in this slide just just had
to learn not to regret the optimum if I
can plug in an optimum here
that's very stable they can do that they
can learn studies things they can't
learn very changing things so I need a
very stable optimal solution in order to
do this and the one piece that I that
now you're supposed to imagine and
that's really sort of orthogonal to
virtually everything I've told you so
far then I have to claim that in many
games in maybe in almost all games I
could think of there we can actually get
such a stable solution approximately
efficient over point good point this is
not an algorithmic statement
it's an existence statement I don't need
an algorithm that computes this
this is inside the proof or if you want
and this is something that I philosophy
Catholic to discuss if you had an
algorithm and you want to help these
people you can make suggestions for them
or what they should bid if you have an
algorithm and then they should only not
regret your suggestion they don't have
to they don't have to run such a
sophisticated learning algorithm so
because I because you guys were a great
audience and asked many good questions
there is a flight first I just passed
through which I definitely don't have
time for in the unit demand auction case
one can actually show you that one can
do this stable solution without any
machinery like there's a pretty simple
greedy algorithm based I can I can just
do it directly it can give you a pretty
stable solution no assumptions no
nothing it just basically the idea is to
to don't use the optimum we use a greedy
greedy algorithm but very much more
generally the I wanna end with the
connection that was just made
there is another notion out there is a
large literature that we can cash in on
how to get stable solutions to two
problems and that's because of privacy
causes if you're playing a game and if
you're worried that because you guys see
what happening in the game my my
properties are my desires will leak to
all of you we might want a solution is
the property that you guys solution is
not very sensitive to whatever I told
the mechanism designer that's called
differential privacy and what how do you
achieve this that your solution is not
sensitive to my input is precisely what
I want it to your solution should not
even be sensitive to whether I'm there
or not that's what I wanted and that's
precisely what differential privacy does
so using the differential privacy
machinery there are two classes of games
to be proving positive sinks and this
has the details but I'm not sure right
it definitely not worth living we can do
like this on different shop on general
congestion games because they're nice
could different papers don't have to get
the optimum solution that may
differentially privately and literature
might largely championed by aaron ross
and many other people working with him
and similarly also aaron ross and
co-authors large communitarian markets
with gross substitutes here he is making
a large nasa sumption he is making the
assumption there enough enough items
around to cause this to happen okay and
i'm going to stop here and thank you
very much for your attention
yeah you know any negative examples of
welfare exposition problems that don't
admit an approximate solution that
varies very little when you change
good question right uh no I don't I
this there are two sort of big big
classes of games here in this category I
don't believe it's possible but I might
be wrong
this the the Aaron company paper biz
ghost substitutes does make a very large
game assumption and they need it in the
proofs but our version which I admit is
unit demand only doesn't need any
assumption so I'm kind of thinking we
just need to bridge the gap here we're
just doing like there is an algorithmic
issue like like this is a particular
tool that we can't use because this uses
the large game assumption and this is a
simpler setup between active is I think
that's bridgeable in the game version
I'm sure it's not like certainly I thank
you it's a great gates edition it's not
clear it's so different because for what
like when I think about games that you
don't need the shared prior assumption
but the classical patient games are just
learning it doesn't matter what they
know but in particular it's reasonable
to assume that they don't actually know
other players they're playing against
like there's some random variation on
players and they don't know really
what's going on so there is some
informational issue here that we're not
explicitly exposing because we is making
the learning assumption which makes it
go away but they certainly they're in
the background yeah
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>