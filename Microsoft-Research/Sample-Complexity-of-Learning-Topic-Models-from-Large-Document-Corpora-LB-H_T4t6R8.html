<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sample Complexity of Learning Topic Models from Large Document Corpora | Coder Coacher - Coaching Coders</title><meta content="Sample Complexity of Learning Topic Models from Large Document Corpora - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sample Complexity of Learning Topic Models from Large Document Corpora</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LB-H_T4t6R8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
the second term is x qi luo from IRC and
he asked me to introduce him as just
chiru from is good morning everybody so
first i will be thank the organizers for
inviting me to speak so pardon me for
the title I told me as boring as what I
it seems so we're going to talk about
text analytics and the big data setting
and I think the previous talk clearly
summed up why in nutshell startups will
come upon another takes on big data etc
right so so this talk we are going to
talk about interesting problem in text
analytics that of extracting topics from
a large document collection and was
happy to hear the words like
unsupervised learning etc so this would
be an example one service learning so so
this is a picture of what anybody any
guess let's not I just tried to make
sure that you're not asleep nervous this
so I jokingly you can say this is the
picture of papers on my desk after the
examination no joking I scraped it from
internet somewhere so imagine this is a
common sight in most of our desks
especially p unorganized people like me
so one of us often one things right if
if there was a tool which could help
organize these things you know in
unsupervised fashion won't it be the
code no so so the main challenge is that
the main challenges okay one good thing
to organize it okay higher you get ten
guys and each of you look at the paper
and put it in this bucket that bucket
that bucket right so that's not going to
that's not possible you want your
automatic scheme and that's hard the N
and so there are two reasons or other
two dimensions of this problem first of
all what are the methodologies you want
to deploy it for such a problem that
also know very clear its emerging theme
and the methodologies which we currently
have
a severely challenged if a number of
documents are quite large so what can we
do and this is being and I mean this is
one of I would say if text is if not
data is in text and this that meant
today's challenge for sure so topic
models is a very interesting attempt at
this problem so these models attempt to
discover hidden themes in document
collections in a minute we will try to
make this more clear now if you have
some hidden if you understand the things
in a document you can use them to label
them automatically tag them right I once
you have tagged them you know then you
can then you can use the Stags to do you
know browsing in a searching and and
storing them efficiently etcetera right
so so so on so on and these models do
not require supervision so given so many
advantages that is why this is almost I
mean my colleagues in Google and Yahoo
tell me that this goes into the back end
I have except the Android eleventh I
can't tell you what's going on there who
doesn't work there but this has been now
industry standard in in organizing large
text collections so so just to set some
example legs just to set some
definitions right sort of topics so
suppose I tell you I show you a document
on which you see this following words AG
run innings it season game any idea what
the document would be sports baseball
and cricket so now if I say the document
of I came out a new york times or since
so basically this is on as a baseball or
cricket if it was time so it could as
well good times of india then it will
quicken or whatever break so so the main
idea is that just knowing this few words
if it is ok this could be about cricket
or baseball so that's what we are
talking about here
right so so for example I mean you know
if you run such models in topic models
over you know say some data set maybe in
this case you're running on New York
Times data set so so obviously you can
see the first five words of this run
inning hit season game we talked about
it that's more like baseball the second
one could be you know I mean about
cooking and then there is this patient
drug dr. cancer medical of course it was
not Sonya healthcare right so can you
automatically extract them that is the
main idea sort of right so now as you
see these are very interesting and
useful things so people have built some
topic models says one great example
which I find advertisement for topic
models I mean I'd not sure how much of
it is used so imagine Wikipedia right I
want to browse Wikipedia so what you can
do is you can take the we can tell you
it a dump right through topic models so
you can see I don't understand can read
those words carry the same household
population female family median right so
this is this is a like these are the
words I'm finding right there's another
set of words let's say war pours army
attack military something with measuring
things now so for example in this
particular case if you so these are
basically some topics you are defining
publisher digital now as I saying
collection of words now which are which
all appear in co-occur heavily right so
if you go and click there you know you
will see the following documents these
are document titles coming up so this is
great now you want to say ok I am coming
to pull your document i wanna see
related topics irken to township
household population that's given
another topic so this gives you a very
easy way of browsing large document
category just to show one use but you
know but now effectively all back and
search is now driven by this topic
models that viewed from industrial
search companies can tell us better so
having convinced you that these are you
know great tools
now let us see how can I learn how can i
find so this is the bill this the
subject of this talk so we briefly
talked about what are topic models now
we'll introduce you to a very you
classic technique or latent semantic
indexing where this is started and then
we will talk then if you get time we
will actually talk about what i mean by
learning talk some finite sample number
of sample complexities this is our
recent work with power avi canal and
drop it I said that's the outline so so
basically all they started in 1960s and
when the first shoot first notion of
topics sorry document started emerging
in collections so at that point people
said okay I have a collection of
documents and documents relational words
etc I have this model now the main thing
is can I find information what is
information now L said I'll ask maybe
ask a question and you have to answer
this they're right I'll give some
keywords and you to find me the nearest
documents so at that point of
formulation was I give you a document
finally the closest documents use 1970s
you know so then so so then people then
came up with this model which are called
vector space model so it's what is it
would this is the levelized with the
following thing let us encode the image
document the presence or absence of a
word for your account that i can
represent by a vector and then you look
at your query that also represent by
another vector then try to see which
which documents are closer to that query
and return it back to you this is this
is what i will call as keyword search
now very soon people found that this
works sometime but not always so if I'm
looking for example is my for example is
you are looking for something like
Jaguar j gu er a Jaguar now it can you
check word the car or Jaguar danimal is
not very clear I just went from that
word right then it was a Jaguar
adventure and say
were fast speed you know so the
squatters helps right and then so
sometimes these are not so apparent just
by words 1 / 1 can have okay people can
write the same word there are words with
say many different words that can always
happen so all kinds of things came into
play right so the point people came up
with this interesting idea that vector
space models are may not be that good so
the king of interesting available latent
semantic indexing and that completely
outperformed this keyword search so
let's look at it very quickly to set the
tone for this talk so we'll as we
discussed for us you know the data is da
cunha collection will input the data as
a matrix and here is what Matt comes and
that's what we know so so so here let us
assume that each document is a column I
am sorry to do this in the morning see
each document is a column and each row
in the column is a word now what you
fill in the row they can be depending on
what models you have for example the
vector space model can fill in the
absence or presence of a world or you
can count the number of words if you
wish all right all right so so for
example let's say that I have counted
number of words and and divided them a
total number of words so total number of
words is 100 and there are nine words
the first world 8 sep 10 night and all
that right so some of them should be one
something like we can do other things
also right so finally you're given a
document matrix so the purpose and put
it in a matrix a right so then people
found is that ok there is this
interesting tool called from mathematics
linear algebra or singular value
decomposition what you do is you write
this matrix into a factorization MDS the
three matrices right and now so remember
each column of a is a document so you
can see each column written has
something like AI into some matrix into
a sign but SI and now I think of it is a
new representation of this document
okay so now what I do is I now a
curative a query good q is given to me
yeah so what I do is I project the query
into this using cmd by this formula and
then I can put a similarity score
between Q &amp;amp; A I Q is a query document
and I is that the document in my data
sets so now what I can do is if this
number is high then I is close to Q this
is very similar I will take this
argument now I can use this code for
each grade L I can use this code for all
the documents in the corpus and return
to you the closest ones now this
completely outperform the keyword search
so now by keyword search a wind does Q
transpose a idea that is the main thing
if you are mathematically so i said as i
said right so people are wanting this
working where but why why should work
well Evan what did i do when there is no
linear algebra i knew the two light on
it why so this was saying I so maybe
then facade maybe a matrix M is encoding
something to semantics maybe do it was
projecting wanna say by projecting the
documents those columns of em see
there's some where the words so it is
not really aligned along each word it's
a direction of which is mixing up to
three words right so maybe this is
catching semantic this is what they are
thinking yet see this is 1998 so people
but applying it using it no one
understand now as you said 30 years
right these two ten years ten years
later there's a very interesting paper
came up a classic paper which that me
explain this the widest work so now so
then said okay maybe what you should do
that each topic sorry each document has
a low single topic will define it in a
moment right so and if each topic has
some very few words that is you take the
matrix the m
matrix write a matrix you have this
words all the speakers what are you
showed no bad inning season game etc and
you see there's a large fraction of the
probability mass is sitting on those
words that is this normal are very high
okay in such a case we can prove really
show that the keywords are outperform by
this and that is a great explanation and
of course there's a mini math here right
essentially this gives a pretty
interesting idea so then topic is
nothing but the priority distribution
over words the first time since came out
right so sore the document now so
document is this is nothing but maybe I
am rolling a dice with D number of
phases where D is a vocabulary em throw
em drawers yeah and each time a word
shows up is put in document so under
that model and all this was like all
this is true so I have eight minutes so
let me just speed up so sorry now
another question is this is 2000 right
we will understood bread but what is the
assumption they made they said each
document has only one topic there are
assumption right will flash down
slightly he's talking about one topic
that is that is each draw is from one
distribution now the question became is
that ok so now here is a paper you know
the sword art and computer science
obviously they say this is not going to
work here we will have as we were to you
know as we are trying to apply less idea
quickly found what okay this is breaking
down in these places so we need to have
models which can go beyond that which
can handle multiple topics in a single
document so so for example you know
let's not get political here but let's
say let's say there's a document we are
talking orange utley we are talking
about chidambaram we are talking about
BCCI see they are both on the BCCI board
but open corruption here so this
document even this
this comes out with this out BCCI
scandal see but here they are not in
opposing sides correctly sir cricket is
what politics it's also about corruption
right so unless I will not be able to
handle this correct so now what do we do
so this is a very interesting open
question and now income of a very firm a
fantastic answer in 2002 where they
actually took the idea of probabilistic
that topic is a probability and netted
to fit actually a full generative model
of a document I'm in a very complex
model I will not have time because I am
racing against that it's five minutes so
so we'll just say that a DA is this
great tool but it's a brave prob listing
ok so now so here is a tool and it has
well deadlier can do so then build a
model of science that is they taken the
science magazine or last hundred years
this took this on the data set they ran
this model so it's discovering you know
so they tracked how this topics will
change so for example you see 1960 you
to see water fish Mary notions what is
it so in the marine science right
somebody marine you know McMahon in
biology so how that topic has evolved
you know just to give you another
example how can visualize you know large
government election so I spend much time
on this so essentially the idea is this
so I so the main idea was that ok LSI
grid tool but it can only handle one
topic one document and you save a new
tool which can do multiple things so
what does it do it says that ok for
example assume they're only two words
your topic this to have a visualization
so now so let us put some weights on
these so this is you can find out there
are three topics so you can put it as a
triangle and now what it is doing is
what is LD a new topic is doing is is
putting some weights here putting some
weight there including someone who ate
there and turn that linear combination
of that that is is trying to find out a
point inside the triangle under the new
distribution and now they're trying to
drawing the cones according to that
distribution okay so this is that that
the util gives them the idea or the
flexibility to handle multiple topics
what document okay seriously on the flip
side great it would explain but from a
computer science is nightmare everything
is np-hard everything the sense that ok
I give you a document tell me which to
which topics are there and be hard how
will I find topics for large oven
corporal np-hard okay but but these guys
are designed it you know so they give em
same city at knees markov chain monte
carlo techniques which have which are
which does not guarantee but works
fabulously well and this is no by far
now is the industry standard and this is
news heavily they would score so so now
so often it is that so given the
importance of this so often does not
interest in understanding that okay can
we do better can we do can I develop
algorithms we're can guarantee that if
there's a topic I'll find it and will
not be stuck by this and we are de
sumption right so so that is this recent
interest and that's where our results
were so so so so basically what what I
mean what is the recent last 23 years
this is happening so basically as weas
now saw that M is a matrix each column
is a topic and you're putting weights on
them right and it's a random so given IM
matrix you randomly putting weights on
them you generate a new doc new
distribution our distribution will
generate a document this you do it many
many times remember each time the
weights are changing all right but if i
show enough documents m is not changing
right so probably I can still be able to
guess m so that's the main idea right so
now open this is not always possible on
Tennessee make assumptions on them so i
will quickly point out so this is a
break should pay for a phenomenal
breakthrough what they said was that
okay if you assume that an M matrix
right each column of em column means
each is this a word right in there is a
word
which occurs in no other topic but of
course only on that topic for example if
you see there is one bad happening and
that can only mean of course not little
forget forget the forget animal but say
bat only happens is i am looking at any
sports let's say so if i see the word
bat it must mean cricket so the
probability of occurring a bat occurring
any aerobics is 0 right so i can so you
can use that to anchor the this is like
a signature what if moment i see this i
know cricket i saw that particular topic
is of occurring so using that and some
clear mathematical tools you know well
Rhoda and all that give a fabulous
algorithm which has this kind of sample
complexity now thing to notice so they
conjectured that you have to go beyond
SVD right because everybody LSA was as
video it can only do one topic right
remember lsi was SVD based you can only
to one topic right so these are you dead
okay we have to go go to do other things
and all that boy band SVD now I have
exactly one minute so i'll tell you our
result so if you hear smart okay I not
sure the audience will like it but okay
so let's see let's put a result and
we'll see we'll take some questions so
our result is we make several options
will not tell you what what is I'm sorry
weaker than the previous ones what do
you suggest is do some threshold e do
SVD and i can meet your sample
complexity and thus our result of and so
if you want to hear that we can be happy
to talk about for another 20 20 30
minutes and i think i'll stop here and
take questions yes
hi I'm english from to Blighty delhi
sure so I've been studying this topic
models and it's interesting that we can
use co-occurrence and try to figure out
some semantic value out of the documents
and you as you say it's quite reasonable
that it helps in search and other kind
of applications but i wonder how
interesting are these topics and how do
you make these topics interesting
ultimately there is a manual
intervention for labeling these topics
right so so you have these five words
and from that you said it's either
baseball or cricket or whatever right so
systems may not come up with such
interesting topics it might just find
another term which is not so interesting
for labeling that topic what is ooh so
your question is that if I run it on a
general corpora I may not get good
topics per se so talk marine has issues
one of the issue is that currently that
if I do not set the number of topics
correct I make it topics which are
garbled words okay so some amount of
hand tuning is needed to that effect so
we are not there the full full you know
the whole things and so this is clearly
a very evolving research area but the
interesting point is the example which I
showed two examples I showed Wikipedia
and science proven examples on large
document collection it is giving good
results right so open so which basically
means that if you randomly take some you
know collection of documents I'm never
be able to find out the tongues may or
may not become coherent okay but but
probably if the documents have some link
there indeed some topics right del dia
model you know does discover some things
and for example on the new results or
were a result or our result you know now
are trying to get can tell you more
precisely that if the document has this
characteristics we can we'll find them
so that that's happening so it's clearly
this is going to be dominating the
the field for some time now Thanks yeah
I can hear you just a small question you
said about your result that you use some
kind of thresholding followed by SVD
yeah is it do you mean threshold on the
columns of that matrix the sunset and so
what I do is he data is a matrix right
data will given to us as a matrix from
that ought to do whatever I have to do
node will give me em right so what we do
is I take the a matrix and do some fish
holding operation remember the lsi or
chinless I was you factorize the m8 is
bias video and that took them where they
took them right so now we're you're
suggesting under some assumptions that
you threshold the a matrix then apply is
VD then I'll then also brought to do
something to K means clustering and it
gives me the M matrix this is any
intuition why this objects the intuition
is another 10 minutes so basically okay
the ideas of the little spend that since
this said I can take some more time so
we make this interesting assumption
which we defer so first thing to notice
so this I'm okay i can i take a few more
minutes so let me bore you with some
details so you see so here is the
interesting parameter but p naught p
naught is what this set is that the word
bat the probability of the word bat
occurring and the word bad occurs only
in that particular topic and anywhere
else right just preserve it was six is
Dominator that's what I Rose result is s
is number of samples I need to recover
and M hat which is very excellent close
to the original topic matrix right so
this is horribly it says horrible thing
right so okay so now we make a sort of a
different assumption so the assumption
we make is as follows in it and it is in
light in
line with lsi assumption what we assume
is maybe the bcc example the good topic
good our document to think about it may
have politics it may have corruption it
may have privet administration but
fundamentally it must be what cricket
and this topic should dominate it right
so we understand let's think about those
document called versus where the be
multiple themes and so the user thing
theme is basically you appalling as
topics so da cunha have multiple themes
but one theme will dominate okay this is
something I make on the daughters that
is the weight on that particular theme
to be very high okay now if you see if
it is if i push it to the extreme I
recovered lsi right okay lsi was it the
only one topic at walkers are they would
say which addresses 0 isn't it so I make
it high I make it high not what not to
one since i since i want to make it 2-1
well now the math is very difficult and
you know but the essential idea is that
that is where the thresholding helps me
in discovering those dominant topics you
know so that's the intuition if that ok
ok so let's thank you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>