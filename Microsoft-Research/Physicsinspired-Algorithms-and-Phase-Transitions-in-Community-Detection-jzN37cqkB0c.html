<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Physics-inspired Algorithms and Phase Transitions in Community Detection | Coder Coacher - Coaching Coders</title><meta content="Physics-inspired Algorithms and Phase Transitions in Community Detection - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Physics-inspired Algorithms and Phase Transitions in Community Detection</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jzN37cqkB0c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
look welcome everybody to today's ms our
colloquium we're delighted to have Chris
Moore previously of University of New
Mexico and for the last few years at the
Santa Fe Institute Chris wears many hats
mathematician physicist theoretical
computer scientist and today's talk will
be sort of a fusion of all those persona
he's also written a very engaging and
popular book the nature of computation
so getting the title right and we're
very delighted to have him here if you
are not going to the retreat Wednesday
and Thursday and you want to see more of
Chris he's going to be coming in
tomorrow morning I will turn it over to
you Chris thank you very much so I've
had the honor of working with lots of
great collaborators and people who've
taught me a lot so I know that not
everyone here is a professional they're
improving theoretical computer scientist
for those who aren't then I hope
there'll be some nice lessons here about
how we can infer things from data and
when we cannot infer things from data
and for those who are there will be lots
of good conjectures here for you to
prove all right so I want to push the a
certain analogy between statistical
inference and statistical physics and
you know physics and computer science
are both about lots of things that are
interacting with each other at least
that's what statistical physics is about
and about the global behaviors that
emerge from them whether those are
natural systems that we try to
understand in physics or engineered
systems that we're trying to design to
get an algorithm to work so I think the
two fields as many people here know have
a lot to say to each other so let me
first philosophize a little bit since
it's in the it's the afternoon
we're all looking for structure in data
what does that mean how do we know when
we found it so one thing is that
structure is what makes the data
different from noise so if you have some
simple null model some structureless
background model it's what makes the
data different from that so in a graph
you could have a really simple random
graph model and you could ask to what
extent does the network in front of you
differ from that another point of view
is that structure is what lets you
compress the data so if I have a huge
network and you want me to send it to
you when you think it's noise there
actually a structure but anyway that's
true yes so you can articulate structure
and we'll talk about that but if
structure that you can't avoid is not
structure
I mean well structure you can't avoid
structure you can't avoid you can
exploit for algorithmic purposes which
is great because then it's there even in
the worst case but it doesn't if you see
it it doesn't mean that you've
discovered some pattern in the data
because it would be there no matter what
the data was good all right all right
another attitude about structure is it
will let's let's let you compress what
lets you compress the data you don't
want me to send you the raw adjacency
matrix of a million by million of a
graph with a million nodes you want me
to give you some succinctly
or inexact you know informally you want
to know what are the important
structures and of course that's
application dependent so one very good
tester whether you've actually found
something in the data is whether you can
extend what you know about the data to
what you don't know so based on a past
time series can you say anything about
its future based on some of the links of
in a network can you say anything about
the ones you don't know about here's
what I wanted to throw
so my friend Jennifer done at the Santa
Fe Institute is an ecologist and she
studied food webs including fossil food
webs that are hundreds of millions of
years old and reconstructed from I guess
teeth marks and fossils I'm not sure how
it works exactly she wants to know
whether evolution has merely changed the
individual species that live in the
ocean or whether the overall structure
of the food web has changed in some way
it's you know nodes or species and an
edge is this seats that that might be a
very successful structural finding would
be terrible well so I mean for instance
have the number ecologist try to figure
out why do food webs have a certain
number of trophic levels from plants to
herbivores to carnivores to super
carnivores or whatever why aren't there
why why aren't there seventeen layers
for instance you know what does that
have to do with the amount of resources
and I I don't know but I like putting
this up here because it's not a math
question and I'm not sure how to turn it
into a math question so so far I have
not been able to help her very much with
this question so it's a challenge
bridging between the science and the
math um finally we're interested in a
lot of dynamical systems on networks
oscillating voltages in a power grid
epidemics going through various
communities perhaps one thing structure
can do for us is give us a useful coarse
graining of a dynamical system like when
epidemiologists say I don't want to
think about a seven billion dimensional
system to model my academic and I don't
have that much information anyway I'll
just assume there are twenty kinds of
people like 70 year-old Estonian women
and african-american college students
and there's a twenty by twenty matrix of
transmission rates so I'll treated as a
twenty dimensional system so if there's
something going on on a network and we
can divide the nodes into
communities then maybe it gives us that
kind of coarse graining not lower
dimensional approximation all right so I
have fallen
III have the Zell tree of the recent
convert for this approach to all this
broadly called statistical inference
that many fields have rediscovered the
idea is you have some data in front of
you in particular a network for most of
the talk a network will be a graph but
of course networks are much much much
more than graphs but let's pretend
they're just graphs sets of nodes with
links between some pairs so we're going
to assume that it's generated by some
underlying model which has some
randomness but also some parameters and
we'll try to fit the parameters to the
data and one reason I like this is it's
very happy to deal with partial or noisy
information so for instance maybe
there's some metadata some attributes
demographics or locations or something
that you know about some of the nodes in
your network but not all maybe you don't
know all the links so I think because we
often think about online networks where
someone at least Facebook knows all the
links we forget that in a lot of real
networks discovering the links takes
real work in the laboratory or the field
one of the networks we've worked on is a
food web of about 500 species in the
Weddell Sea in the Antarctic so to
discover those nodes in link someone had
to sit in a boat and be really cold for
a long time also some links might be
false positives right so just because
two proteins stick together in a test
tube does not mean they work together in
the cell just because two genes have
activity levels that are correlated does
not mean they have a causal link in the
genetic regulatory networks so the data
we have is going to be incomplete and
partly wrong anyway you give whatever
data you have to some procedure that
fits the model to it and then you can
take that inferred model and make
guesses about what you don't have and
hopefully you get some of that right all
right so I'm going to introduce you to
an old and extremely naive model
but it already has a fair amount of
personality and some interesting things
to say about it I think it started in
the sociology community um we're gonna
assume there are K kinds of nodes in the
world there are like three red ones blue
ones and green ones and now you if
you're familiar with the erdos-renyi
random graph model that's the most naive
model in the world it just says that
every pair of nodes is probably is
connected with the same probability P in
the stochastic block model we generalize
that there'll be a little K by K matrix
so that if I'm of type R and you're of
type s there's a link from me to you
with probability P sub R s now we can
model directed links predators eat prey
and not the other way round so this knit
this matrix does not have to be
symmetric we also are not going to
necessarily assume that it's bigger on
the diagonal so in social networks were
used to the idea that people are more
likely to be connected if they're
similar but again consider the example
of predators and prey or or like words
in an adjacency adjacent words and text
right adjectives come before nouns in
English and sometimes next to other
adjectives but nouns don't usually come
next to other nouns all right so there's
lots of games we can play yes uh yes
that's true
now some people write block model
without a space but I I resent the
gratuitous formation of compound words
in English I think you know in German
that's perfectly appropriate behavior
but in English I think there should be a
space all right yes yes the whole thing
yeah all right so there are lots of
games we can play I mentioned before
that we know some of it maybe we know
some of the links and not others but at
the moment let's say that we do know all
the links we know the topology but let's
say that these types are hidden and you
want to find them and maybe you know
this matrix P but maybe you want to
learn that as well maybe you don't
initially
the likelihood of with which group 2
connects to group 7 um I'm going to
mostly assume that we already know how
many types of nodes there are but of
course that's an important question
right because if I give every node its
own type I can model the whole network
perfectly but I've learned absolutely
nothing about it
so that would be an egregious example of
overfitting the model to the data anyway
so choosing the number of groups is an
important model selection problem all
right here's a little cartoon there are
three groups this group doesn't like to
connect to themselves these guys really
do these two groups like to connect to
each other and so you can imagine a
little 3x3 matrix of these edge
probabilities on food webs so I like
this because in most work on community
detection in networks we assume that a
community is a clump that it's more
densely connected inside than to the
rest of the world and lots of people
there's a huge literature on that so
here we're defining community in a
somewhat more general way I would say
that two predators are similar not
because they link to each other because
they don't eat each other
but because they eat similar prey so
here you you belong to a community with
each other if you connect to the rest of
the world in similar ways you know this
could happen in economics you could have
a roughly bipartite network of suppliers
and customers maybe the suppliers don't
do so much business with each other but
they compete for the same customers we
mentioned weird adjacencies even in a
social network you can imagine competing
leaders who are trying to connect to the
same followers but they don't like to
acknowledge each other's existence maybe
you know people like that ok so here is
a little picture this is getting old I
think I need to change it at some point
but these these are the 60 most common
nouns and adjectives in the novel david
copperfield and there's a link if they
occur next to each other in the text
somewhere I or rather my student told
the algorithm that it had three groups
to play with
what there's a total of 60 nouns and 60
adjectives so did a pretty good job of
putting adjectives here and nouns here
we did not tell it anything about the
structure of the English language um we
gave it three groups to play with and it
used the third group for things that it
had trouble trouble classifying and this
include the word light which appears as
both a noun and an adjective in the text
so anyway it's kind of fun to see what
it does the black and white are the
nouns and the adjectives there's there's
some of these yeah it's by no means
perfect obviously anyone who works
seriously on part of speech tagging
would just laugh at this but I think
it's a kind of nice little example all
right so of course fitting a block model
to a graph is np-hard so for example
graph coloring where you are only
allowed to connect other people if
they're a different type than you is one
of the classic np-complete problems so
this is np-hard on the other hand um
don't kill me but I think this is one of
those cases where we don't care so I
mean if you are trying to fit a model to
data and you find you're finding it
incredibly hard computationally to fit
the model to your data maybe it is the
wrong model for your data so maybe if
the stuff you're trying to find is
really hard to find maybe it's not
really there so we can talk about that
some more um so in practice there are
several methods that work quite well um
I will be talked talking about belief
propagation because it has this nice
analogy with physics and because it is
also good there are other methods that
can work quite quickly even when you
have rather complicated models there are
some variants of the block model that
have easier algorithms we'll talk about
spectral methods at the end but belief
propagation is the coolest because it
builds this analogy with physics which
is always a good thing to do and it will
also give us unlike many other
approaches it will give us natural
measures of whether the structure we
found is really there so in other words
maybe you could even I'll give you the
magical ability to find the best
possible fit of a block model to your
network well I don't just want to know
the optimum I want to know in some sense
how robust this optimum is and so we'll
find that there are some measures of
that and also analyzing the performance
of belief propagation will reveal these
phase transitions in our ability to find
communities and it turns out that in
some cases we just cannot find them all
right so um let's just write down this
model this part is easy so if you knew
all the types of all the vertices then
the probability of getting the graph in
front of you would be really easy to
write down it's just the product over
all the edges that exist of the
probability that they do and all the
edges that don't exist of the
probability that they don't okay so one
thing is you can try to imagine
maximizing this function as by by
altering the types of the vertices
because again you're trying to figure
out the types of the vertices I've shown
you the graph now of course in in
statistical mechanics and physics Ludwig
Boltzmann taught us that if you're at a
particular temperature then the
probability that you're in a given state
depends it decreases exponentially with
the energy of your state rocks fall so
we prefer lower energy states but when
things are warm where they are
occasionally found in higher energy
states so because of this equation and
I'll take the temperature to be 1
although it's useful to tinker with it a
little bit then the first thing a
physicists would do is take the
logarithm of everything and change this
probability into an energy
so when you do that you get a sum over
all neighbors of now this log of the
probability and to the connoisseur this
looks like a generalized Ising model or
if you are a real connoisseur of pots
model where so between each pair of
nodes that are connected there's some
energy associated with their combination
of types in a classic magnet like a
block of iron we would prefer to be the
same as our neighbors because we're
conformists and so then this would have
a higher energy than that so here
there's just some matrix of energies
depending on the types there's one twist
though which is there are also
interactions between the non neighbors
right so I mean if I didn't know all
these people
there had to be some explanation for
that so maybe I've been living under a
rock or something so the fact that edges
don't exist does tell you something
about the types of their endpoints now
in a sparse Network where P is small
anyway these terms are small on the
other hand there are a lot of them there
are about N squared non edges and about
n edges in a really sparse network so we
can't ignore these terms completely as
it turns out all right well yeah you can
do that too right but then you're yes
you can do that too but well but it
depends on the types right right it does
depend on the types
okay so here's a little two-way glossary
the probability of our graph given these
underlying labels becomes this beta
there is one over the temperature
becomes this Boltzmann distribution so
it's minus the log probability becomes
the energy now the most likely labeling
which you know if you've hung out with
people who do machine learning or
inference you've noticed that there is a
constant hail of three-letter acronyms
or TL A's so you know I I find this very
unpleasant but there's a lot of it so
you know anyway this is the maximum a
posteriori estimate and what mode has to
real work and also also mode has only
mode as the disadvantage of only having
one syllable or as if you say maximum
a-posteriori that is kind of partly
Latin and it sounds very fancy although
then people say the map estimate and
anyway never mind it's it's horrible the
way this field works but okay so so the
most likely labeling is the one with the
lowest energy and that's what physicists
call the ground state so we can also
call this the ground state so it can
also be worthwhile to sum over all
combinations of possible types and look
at the total probability that we could
get the graph out of the model and this
is called a partition function in
physics and if it's also the
normalization factor so you know if we
actually want the problem the posterior
probability I'm ignoring the prior here
the the posterior probability of the
labeling given the graph we'll use Bayes
rule normalize this gives the gibbs
distribution which physicists call the
gibbs distribution finally minus the log
of this is the free energy which I will
argue as a kind of good measure of the
overall ability of this model to fit the
data
I think they call it the free energy if
they've heard of it maybe there's
something else the marginalized maxim
alized normalized regularized I don't
know something something all right we're
getting punchy here that's good all
right so now your friend the biologist
or your friend the sociologist has a
network and she wants you to tell her
what the labels of the nodes ought to be
so what's the best one well one thing
you could do is take that maximum a
posteriori estimate that maximum a
posteriori estimate the one the labeling
which maximizes the probability of the
graph but this is wrong it's wrong it's
a terrible idea
bad why because of illusory communities
that are not really there so for
instance here is a graph I have labeled
the vertices I put the blue ones on one
side the red ones on the other and only
about 11% of the edges cross from one of
the other so clearly these are two
different communities in a social
network that much prefer to link to
others the same kind than to the other
ah but here is another such partitioning
which is just as good and completely
uncorrelated with the first one so the
problem is that simply by chance out of
all the two to the N ways to color these
nodes there are some that looks
surprisingly convincing as community
structures so this is one of these areas
where we really need to think about are
we overfitting our data are we finding
things that are not actually there I
could imagine that one of these is
slightly better than the other in from a
computer science point of view that one
of these solves a min bisection problems
slightly better than the other but again
that would you know yeah who cares I
mean if it there's always going to be
such a thing and even if it looks good
if there are many others that are nearly
as good that share no structure with it
then you don't want to solve that
optimization problem all right
especially is it yeah even if it is so
we don't want the quote-unquote best fit
there are illusory communities even in
random graphs and of course you know if
I show you this data and I fit it with
an elaborate model like a tenth order
polynomial I can do a really good job
and completely fail to understand that
the data or generalize to the data I
don't know what I want is a simpler
model that is willing to have a noisy
fit and that will actually generalize
better to the rest of the data you know
humans our favorite are famous for my
favorite quote is finding black cats and
dark rooms even when they're not there
right we're we're excellent pattern
matters we will find patterns even when
there are no patterns to be found so and
so will our algorithms if we're not
careful okay
so here's a better way to choose the
best labeling so for each vertex you
want to sample all possible labelings
I'm not gonna tell you yet how to do
that maybe that's hard but if you could
you would want to know the the
probability averaged over all labelings
of that this particular node belongs to
this group that group or that group
that's called its marginal distribution
the probability that that one node is of
each site and you should assign each
node if your friend wants you to bet on
types you should actually assign each
node to its own most likely label this
is called the maximum marginal
estimation okay anyway so this actually
optimizes the fraction of nodes that you
will get right and that's the theorem so
of course we have to be careful if
you're gonna give me a billion dollars
if I got if I get every node correctly
labeled and nothing if I get any of them
wrong I should do the ground-state the
thing I said before was bad but if
you're gonna pay me $1 for each node
that I label correctly I should do this
all right so the I part of the idea is
that if there really are communities
then these marginals the
the fact that most of the time jennifer
is in this community the blue community
means that there are actually many
labelings in which she's in that
community so it represents a whole
cluster of solutions that have something
in common
misunderstanding something about the
maximum marginal estimate if your walk
model has symmetry to it for example
there are only two labels yes yes you're
absolutely right so right so simply by
permutation these things would fuzz out
so I'm sweeping some so okay oh you
could guarantee all right well we'll
lift the rug back up and let this thing
crawl out so yes we need to break the
symmetry somehow if there's that kind of
permutation symmetry by setting the
types of a handful of vertices for
instance were yeah yes but let me tell
you my my my my slogan here's the slogan
so you don't want the best solution you
want to know what all the good solutions
have in common with each other and I
think the statisticians have known this
for a long time but I don't think it's
really been absorbed by the vast and
rapidly growing Network literature yes
right link then presumably in the rabbit
case if you jiggle the parameters
correctly the best would be just one
community English
they're actually I agree with you in
principle so so so what you're referring
to is the fact that we want to penalize
more elaborate models in particular
models that think there are communities
there or think there are 17 communities
as opposed to only 2 or whatever the
question is what that penalty term
should be and as I'm sure you know
everybody has their favorite so there
are the there there's the minimum
description language people the one
there yes there's the MDL people there's
the AIC people which stands for the Aki
ek information criterion which is just
plain wrong in this case because it's
designed for iid data there's the B I
see the Bayesian information criterion
and so on and so on and so on
so each of these will give a slightly
different penalty to more elaborate
models they all have they all have you
know the bit our best interests at heart
but it's not actually clear which one is
correct in some sense so all right all
right well so so let me let me proceed
so the thing is that I haven't told you
oh one other thing um I actually kind of
said this all right so the total
probability of getting the graph forget
this theta thing all right
the the total probability is the log of
that is a free energy and if you
remember your thermodynamics class it
has an energy term but also an entropy
term so part of our attitude is that
rather than minimizing the energy you
should minimize the free energy because
what that means is that you have a model
with a low energy that means high
probability it has some good fits but
also a large entropy so it has many ways
again many good fits to the data which
hopefully is something in common with
each other all right but how are we
going to compute these marginals and the
free energy so like everybody else
I used to use Monte Carlo sampling for
this sort of thing which
a lot of people call Gibbs sampling but
it's too slow for our purposes it's
really rather slow and it cannot in in
scale - you know networks with millions
of nodes thousands okay but not millions
and now I'm sure there are some super
sexy ways to make it run faster but the
other problem with it is that in order
to obtain marginals you need many
independent samples from your Monte
Carlo Carlo algorithms so however long
it takes to reach equilibrium you need
many multiples of that and also
computing this free energy in general
computing entropy is is hard with Monte
Carlo computing energies is easy but
computing n trapeze you typically need
to do Monte Carlo at multiple
temperatures and then some kind of
integral and never mind
so what are we going to compute these
things this brings us to belief
propagation so if you're like me you
spent several years hearing about belief
belief propagation hearing that it was
really cool but every time you tried to
learn it you were so snowed under by the
massive density of superscripts and
subscripts that it just gave you a huge
headache and you decided to do something
else that day so uhm actually for me the
so I learned about this first in the
 in the context of random
satisfiability and colouring problems I
I like the block model because when we
wrote this down we were able to get away
with hardly any double sub double
subscripts or double superscripts
alright so here's the idea how many of
you already know the idea of belief
propagation alright good well let's tell
everyone so every node is going to send
its neighbors an estimate of its
district it's marginal distribution the
probability that it belongs to each type
but with an interesting twist when I
tell Bobby my estimate I base that on
what I'm receiving from everyone but him
similarly he gives me an estimate based
on his interactions with everyone but me
now there are nice principled
mathematical reasons for doing this it
turns out to be equivalent to a certain
variational method but I also like this
this there's a nice intuition behind it
right which is that if I tell Bobby hey
I just heard that there's a scientist
out there who is totally unconvinced
that there's gonna be climate change and
he tells me hey I just heard that
there's a scientist who's totally
unconvinced and I tell him hey I just
heard that again from another one and he
tells me me too then this is an echo
chamber like certain news stations where
information is just being meaninglessly
amplified back and forth what we need is
to bring fresh information into the
conversation from outside this pair of
people all right once when I gave that
example someone stood up and left the
room in a huff so I hope I haven't
offended anyone or at least not too much
yes well there are a few all right so we
passed these messages back and forth
until we reach a fixed point we might
fail to reach a fixed point it might
never converge if we reach a fixed point
it might be a terrible estimate of the
marginals
there is a big warning label here saying
exact only for trees and then there's an
asterisk saying may be asymptotically
exact some of the time if the network is
locally tree like but actually that's a
major area of active research so but if
it works this fixed point return returns
these marginals and something called the
beta free energy named after hans bethe
who also figured out what makes the
sunshine which is you know which is when
it works a good estimate of the free
energy all right um
so let me just show you this because
it's not five o'clock yet I think you
can parse it with a little help this is
just a normalization okay so this is
what I tells Jay about the probability
that I is a group is in group s this is
just a prior probability that you're in
group s that I didn't tell you about
before
and there's a product over all of eyes
other neighbors that for instance if
they tell I that their group are well
because they are connected then we're
gonna have a wait here piece of RS which
is the probability that they're
connected then remember there are also
these messages coming in from your non
neighbors because you do interact with
them alright but okay the fact that this
is a product of a sum and not a sum over
a complicated Joint Distribution is this
thing called conditional independence so
we're assuming that conditioned on eyes
state that its neighbors are independent
from each other that they're only
correlated to the extent that they're
communicating through eye and this is
why this is exact only on trees now
maybe these guys are connected by a long
path and maybe correlations decay
quickly as a function of distance but
then again in lots of real networks
there are a ton of triangles so they
could be connected directly in which
case this seems like a completely loony
assumption it works surprisingly well in
practice alright now there's a problem
here though which is that everybody is
talking to everybody else whether
they're connected or not and that's not
scalable
N squared today in this context is not
scalable enough because maybe n is a
million and a million squared is too big
so happily if we use the sparsity of the
network if we assume the network is
sparse so that these peas are small we
can kind of wrap these terms up one way
to view this is to say I will carefully
tailor my message to each of my
neighbors but to all of my non neighbors
I will broadcast the same message ok
good now the number of messages we need
to compute an update is only then
proportional to the number of nodes and
edges which makes it scalable now it's
scalable ok for some people even this
isn't scalable but for me this is I'm
going to stop here all right now of
course there is a question of how long
this will take
to work so but let me just show you a
few pictures here are some common
examples that people like of the block
model here's one where you have one
probability see in of connecting to
others in your group and another outside
your group here's a nice example where
maybe there are core nodes that connect
to each other with high density and then
with load probability two nodes out in
the periphery who connect to each other
with even lower probability and then
there's graph coloring where you only
connect to those with a different color
from yourself all right so here's our
hydrogen atom our fruit fly of network
science the karate club has anyone not
heard of it ok ok universe are you
serious Madhu give me a break
you're gonna make me tell this story
University karate club 1977 a young
anthropologist named Zachary interviewed
everybody to find out who they were
friends with by the way this is back
when friend was a noun and then then
there was a big argument between the
president and the instructor which I
suspect Zachary fomented this argument
in order to have a good thesis to write
they broke up into two factions and then
he said gee can I retro actively guess
what the factions would be based on the
friendships he used max-flow min-cut and
got everybody right except for one guy
who was friends with the president but
he stuck with the instructor because he
had a black belt exam coming up in three
weeks and there's a whole page in the
thesis about this right now of course we
shouldn't view this just as a failure of
the algorithm it's a oh ok I well I
don't is that true I don't know so and
we should remember that whenever we see
a network there's lots of idiosyncratic
real-world facts about the nodes and
edges that are not drawn in this graph
so anyway we shouldn't expect to get
everybody right so here's our two
factions but the funny thing is the law
model the vanilla block model that I've
told you about actually prefers this
structure you at you tell it okay
there's two kinds of nodes go and find
them and it says oh okay and it comes
back and it says hey hey I found there
are two kinds of nodes there are high
degree nodes in the middle and low
degree nodes on the outside did I get it
right huh like well that's not what I
wanted but it's not wrong it is also
true both of these types of structure
are there and so one thing we can do
which is kind of fun is take a slice
through the parameter space the model
space of all these block models and
actually look at the beta free energy
except I've turned it upside down so
that up is good and we can really see
that both these structures are there is
kind of a local optima in the space of
models I think that's kind of a nice
thing to be able to do all right
um we can also see as the model learns
more how it switches from one to the
other so suppose you're the algorithm
and for a dollar you can point to any
node you want and I will tell you it's
true type but you have to figure out
which node to point to so you want to
learn about the nodes that will teach
you the most about the rest of the
network so we did actually kind of slow
thing with Monte Carlo and we maximized
we figure out which node had the biggest
mutual information between it and the
rest of the network so which would
literally give us the most bits on
average of information so it starts out
with this high load degree but like any
of us would it says oh you know what's
what's the type of that node we tell it
it's red and it says oh oh okay all
right I see that there must be two
groups here but I'm not sure where the
boundary between the two lies so tell me
about this one and now I kind of like
this we did not tell it to first ask
about the hubs and then about the
boundaries we just said maximize the
mutual information and so after asking
about just a few nodes it labels most of
the nodes correctly all right so
belief propagation has this nice I think
double role to play on the one hand it's
an algorithm that you can run on real
networks and it's pretty fast but you
can also try to analyze it using methods
from physics or rigorous methods from
computer science and probability theory
and try to do things like compute
analytically well what fix points is it
going to have and are these fixed points
stable or unstable um and this will
reveal these phase transitions that I
want to tell you about so here is a
graph let me explain the graph remember
that C in C out thing so this axis is
their ratio at this end you only connect
two others in the same group here you
connect everybody equally so it's
completely it's a completely random
graph this axis is the number of
iterations that belief propagation took
to converge so two now two things to
notice one is that for each value of
this ratio the number of iterations we
needed for 10,000 nodes and for a
hundred thousand nodes was about the
same now you can prove that you need
about law that in some cases you need
the number of iterations to grow at
least logarithmic ly l Conan and Alan
taught me that because I was going
around claiming it was a constant and
it's not but it doesn't numerically we
don't see it very strongly here the
other thing to notice so I mean that's
good then it converges quickly but the
other thing to notice is that the
convergence time explodes at a
particular value of this ratio so to a
physicist this is a sign of a phase
transition whenever things get close to
their boiling point or their melting
point or any other sort of transition
times to equilibrium diverge you get
bigger fluctuations over larger scales
and it takes longer for things to settle
down so indeed here is that transition
the x axis is the same now the y axis is
the accuracy the fraction of No
foods that we label correctly with what
a random coin flip would do subtract it
off and the interesting thing is that as
you move towards this structureless
graph you might expect that this would
go to zero gently but instead it crashes
down to zero so here when you're still
50% likely to connect others in your
group than to others outside your group
the algorithm isn't labeling the nodes
any better than a coin flip so um what's
going on so in a in a physics paper we
non rigorously said you know we'll bet
that not only can belief propagation not
work here well then open can work no
kind of algorithm can work and it took
me a long time to sew this you know the
idea of this kind of transition I got
was taught to me by lanka's Deborah runs
around critical oh because they have
enormous amounts of experience in this
sort of system and at first I thought
that they meant it's exponentially hard
but it turns out that's not the issue at
all
so let me explain what happens here
let's assume that there's this symmetry
you know that uh let's see I mean if I
tell Bobby hey I'm equally likely to be
in the red group or the blue group and
if everybody is telling Bobby that he
compiles the results and says hey I'm
also equally likely to be in the red
group or the blue group so there is a
there is a trivial fixed point where all
these marginals are uniform the question
is when is this fixed point stable right
so if it's globally stable then no
matter what initial guesses we start
with we get sucked into it and then
we're just doing no better than chance
on the other hand if it's unstable then
hopefully even if we start near by it
belief propagation will move away from
it and create accurate biases so this
point here is exactly the point where it
becomes unstable so this was proved by
Marcel Neiman and sly and then they and
independently
Soulier proved the positive result as
well that on this side you can label the
nodes better than chance and but it's
only been proved in the case of two
groups for more than two groups you
might think oh well it's just more
technical but it gets more complicated
yes that's what they proved I'm sorry
I'm I'm I'm starting to speak more
quickly but they that's what they proved
so think of the graph as a kind of a
communication channel right I have these
underlying labels for the nodes then I
probabilistically generate this graph
then I show you the graph and I hide the
labels from you so they showed that its
information theoretically impossible to
recover the labels from the graph so
they actually showed two things they
showed that the real marginals are
asymptotically close to half a half but
they also showed that the distribution
of graphs you get from this model in
this regime is contiguous with the
distribution you would get from an
erdos-renyi graph of the same average
degree it's not that the total variation
distance is small it's not but
contiguous means that any high
probability event that's true and one is
true it is true in the other so there
isn't even an algorithm that will turn
on a green light saying there are
communities that is right was probably
close to well I would I would say that
this was
Dimitra I mean who first in that context
pointed out that the planted ensemble
can be indistinguishable from the the
random ensemble yeah
alright ok so here's something that
badly needs to be proved about which
almost nothing is known rigorously so
when we have more groups it turns out
that there isn't just this detectable
versus undetectable there's another
transition so here this is a hidden five
coloring so I have five colors of nodes
and I've added random edges but only
between nodes of different colors now
and this is the average degree of the
graph when there's a lot of edges you
can find the coloring quite accurately
when there are few enough edges then you
can't do anything but in between there's
an interesting regime in this regime
there are well from here to here there
are at least two fixed points the silly
one where everybody is equally likely to
be in every group and an accurate one
okay so the trivial one is locally
stable but not globally stable the
problem is that the basin of Attraction
the set of initial messages which will
lead you to the accurate fixed point
appears to be exponentially small so you
have to be exponentially lucky to find
it or have exponential time to explore
the whole space so in this regime we
believe that finding the communities and
you get similar things in other in other
areas of the model we believe that
finding the communities is information
theoretically possible but exponentially
hard now of course we don't know that P
and NP are different but we could still
hope to prove that certain classes of
algorithms like belief propagation have
exponential difficulty with this like we
could try to prove what I just said
about the basis of Attraction there is
yet another transition that happens
there we're at the in this region we
believe there are exponentially many
fixed points and on all of them are
locally stable and you would have really
no reason to choose one over any other
okay now um this is one of these places
where we start late but end on time
right
I hate that all right so now of course I
can find this point if I have some
initial hint like some metadata okay so
I don't know if you heard that quote
okay I don't know if you've heard that
quote from the former 50,000 groups
colors colors I think that regime would
get really big there because like in the
coloring problem right you have a huge
can you actually distinguish in that
regime this does start to bleed into
some work by I mean Coghlan and his
collaborators on a on a hard but
colorable region yes so you can you
could prove some kind of you could
probably prove like a clustering result
or as they like to call it a shattering
result I don't think people have done
that in this so-called
positive temperature regime but I expect
I expect you could alright so what if I
give you the correct labels of a certain
fraction of nodes that should help you
label things right and indeed you have
some nice additional phase transitions
phase transitions everywhere where your
accuracy suddenly jumps when I give you
a certain amount of information and I
think of this as a kind of percolation
if I tell you about only a tiny fraction
of nodes well you know them you can make
good guesses about their neighbors maybe
their next nearest neighbors but it
decays but if I tell you about enough
then your knowledge of the network
percolates and becomes global and you
can make a good a good prediction about
everybody and here's a picture of that
here's this discontinuity which
decreases until it reaches a second
order phase transition here on beyond
which everything is smooth and none of
this has been made rigorous
so all of these are these cavity method
calculations that we can do numerically
I think that they're exact but I don't
know right with only two groups this
doesn't happen so with only two groups
when I give you a little bit of a hint
the transition just smooths out it goes
from this to a smooth curve this
phenomenon only happens when you have
more groups in you if that hard but
detectable regime so yeah okay you can
add more data you can throw in text
classification using classic models of
of text like LD a and then you can try
it out on a network of patents and you
can get this cool-looking division of
patents into the arithmetic instructions
and the debugging stuff and the power
supply stuff and as you can imagine you
get a better classification than if you
use only the links or if you use only
the text by combining the two types of
information we do a better job all right
finally here's a very pretty picture
which I just had to show you so we just
got this just got published this is work
of my postdoc panjang who is looking for
a job and it just got published in PNAS
or accepted so give him a job so what we
do here is we start with the network and
we're going to recursively subdivide it
but only as long as belief propagation
is telling us that there is
statistically significant structure as
soon as it tells us that there isn't we
stop subdividing and we can do this very
quickly this is another classic network
of political blogs there's a thousand
nodes and again we're not trying to
optimize the structure we're trying to
find out what many competing what many
good community structures have in common
with each other now I'm very sorry to
tell you that these blogs which were
about the 2004 political election it's a
tragedy but what they were actually
saying
has apparently all been lost in the
mists of time but I like to think that
if we could find it we would find that
these are the you know labor Democrats
these are the anti-abortion Republicans
and so on that these are divided out by
so these are the Democrats these are the
Republicans but that they're divided out
by topic in interesting ways
okay um I'm of two minds about whether I
should do this I can do it in five
minutes
want me to do it in five minutes okay so
let me tell you about what it first
appears to be a completely different
approach but is deeply deeply related
yes sir oh these were just uh these were
just links I think that you know whether
this blog link to that blog
it's just whether they reblogged or
whatever you do yeah there's some way
for blogs to log each other I'm a
theorist so I not sure how this works
actually okay okay so you you you you
guys you guys work with computers right
you know about some of this stuff so you
you know about all this bloggers okay
right the hyperlinks yes I've heard
about those too right if you if you
click on it with your mouse see I'm hip
there's all right okay
so let me tell you about what it first
appears to be a totally different family
of algorithms but is actually connected
so another very popular method because
it's very fast because sparse linear
algebra is very fast is to take your
network or your graph and to associate a
matrix with it like the adjacency matrix
where you have a 1 or a 0 depending on
whether there's an edge there or if you
prefer you can take this thing called
the laplacian or you can take the
adjacency matrix and normalize it so
it's like the probabilities of a random
walk everybody has their favorite people
argue about which is best and then um
you take the nodes and each one
corresponds to a component of for
instance an eigenvector and so here is
our karate club network again where I've
labeled nodes according to whether the
second eigenvector is positive or
negative there and you can see it does a
nice job um and if you have K groups you
look at the first K eigenvectors and
then use your favorite clustering
algorithm in k dimensional space till to
give a picture of these where the nodes
live ok so when does this work
when does this succeed so in a lovely
paper in physics which again I think is
not quite rigorous but almost certainly
correct um Raj Nautica DC and Mark
Newman computed the typical spectrum of
a graph generated by this stochastic
Bloch model and so as some of you know
random matrices often if you look at the
density of their eigenvalues you could
you get this beautiful thing called the
Vigna semi circle drop everything and
learn about it there's eight different
ways to prove it it's very beautiful um
but there's also these eigenvalues
outside this semi circle so we call this
the bulk because in here there's lots
and lots of eigenvectors that look
pretty much random what you want is to
find this eigenvector which is actually
correlated with the communities so as
you vary the densities of within group
and between groups
a certain point this crosses into the
bulk and at that point if you ask your
if you ask MATLAB to tell you the second
eigenvector it gives you one of these
and you look at it as totally random and
you don't learn about the communities
and happily this happens at the same
detectability transition that we had
earlier computed using belief
propagation which I think drives home
the fact that that transition is an
algorithm independent phenomenon so I
really like that but there's a wrench in
the works or as my parents would say a
squirrel in the junction box
they eat the wires the problem is that
in this first sparse networks this semi
circle doesn't hold anymore
um and one reason is that in sparse
networks imagine that you have somewhere
a high degree vertex so if you apply the
adjacency matrix twice if this thing has
degree D there are D ways to leave and
come back which if you think about it
means that there's an eigenvalue there
which is at least square root D so if
there's a couple of high degree hubs of
degree a hundred then there's going to
be an eigenvalue at like ten way out
here and so when you ask your spectral
algorithm to tell you about the
communities it tells you about these
high degree hubs instead and that's not
what you wanted to know so in the sparse
case we have these big deviations from
the semi circle law we have these things
bleeding outside here and swamp in the
eigenvalues we care about we must fix
this how can we prevent this nasty
backtracking well it sounds a bit like
that echo chamber before right but now
it's between one newscaster and all
their twitter followers so we will
prevent backtracking so let's just take
a walk on the directed edges of the
network and let's not allow you me any
of us to turn right around and go back
the way we came no cuz we can go here
and then go there and then go there if
we get stuck at the at the leaves stay
away from the leaves
all right now this matrix is not
symmetric so its eigenvalues now live in
the complex plane but now notice how
this is the same matrix as I showed you
before now it's very nicely confined in
this disk whose radius is square root
the average degree so um and I learned
about this from alcone on an Allen who
had seen this type of so they and their
student Joe Neiman and I and my other
co-authors we got together on this and
published this paper conjecturing that
that the that the spectrum is
concentrated it sort of confined in this
disk and indeed this then crosses the
disk and gets lost at exactly the
detectability transition variance
yes right so this is known for D regular
graphs we're arguing that is true based
on average degree in the stochastic
block model or in GNP that's the
conjecture here is a much better picture
it looks like a scary pumpkin or the Eye
of Sauron which I had to rotate 90
degrees I okay all right so indeed all
the classic methods of spectral
clustering fail some distance away from
the detectability transition spectral
clustering based on the none
backtracking matrix goes all the way
down and indeed does almost as well as
belief propagation so this is
interesting there are multiple ways for
them to fill so the the the stochastic
version of the GSM see matrix it's
problem is not high degree vertices its
problem is rattling around in the little
trees that dangle off the giant
component and sticking around in them
for a long time or getting stuck
wandering back and forth on the
occasional long path of degree 2
vertices now there are other fixes so so
arash emini and lisa Lavina
and I can't remember the other authors
they looked at another type of
regularization which kind of adds a
teleport term so it lets you get away
from these localized structures so it
has a tunable parameter it's not clear
how to set it but that seems to help in
back when PageRank was what we tell our
students to do but that you know it's
yes right okay or it's like a lazy it's
it's something that'll get smooth not a
random walk for teleportation
teleportation all right so the title is
spectral Redemption
I'll can unpin up with that all right
okay so what's the connection well the
point is that remember the detectability
transition happened because of the
trivial fixed point becoming stable well
you look at the matrix of its
derivatives around there to tell whether
it's stable or not
so you linearize you expand the first
order the belief propagation update
equations around that fixed point and
those derivatives are essentially the
non backtracking matrix it's actually
that with a 10 surd with a little matrix
which we can ignore um so that's indeed
that is how we stumbled upon the non
backtracking matrix just by saying well
let's use that matrix that showed up in
the question about whether or not the
trivial fixed point is stable and it has
it had appeared in the literature before
um all right morals so these generative
models like the stochastic block model
they're really nice I'm a kind of
moderate Bayesian any any frequentists
okay so I like it because you know
you're required to write down a guess
about how the presence or absence of
links depends on hidden attributes of
the nodes
I think it's good to try to write down
such guesses all right not that they'll
be right I mean but hopefully they'll
help us some of the time but the most
likely labeling the ground state often
overfits producing illusory communities
where there are none so instead you
should seek the consensus of many good
solutions rather than the quote unquote
single best one you to do that you need
to compute marginals and belief
propagation we'll do that at least some
of the time quickly and it also gives
you this thing called the beta free
energy which lets you explore the
landscape of possible models and it is
also our favorite way to choose the
number of groups so we can talk more
about that basically as you add more
groups the beta free energy improves to
a certain point but then it levels off
on we can also study belief propagation
analytically and look for these phase
transitions in our ability to find
communities we can extend it to fancier
models with different kinds of data on
the nodes and edges and sometimes it
even inspires new operators for spectral
clustering connecting these two families
of algorithms I'm really over time but I
just want to say there are lots of
interesting cultural gaps here some of
my best friends are machine learning
people and often when you read those
papers the models are extremely
elaborate with lots of parameters and
lots of priors and hyper priors and
hyper hyper priors and so on and it
seems rather difficult to fit them to
the data although then people come up
with various high-powered clever clever
ways to fit complicated models to data
quickly I would rather fit a really
simple model to the data and I would
rather do this even if even if their
model gets 86 percent accuracy and mine
only got 83 percent accuracy because at
least I have some sense of what my model
is doing and when it works and when it
doesn't and I might even be able to
prove things about its strengths and
weaknesses which i think is valuable of
course it's also valuable to push to 86
but I like this I like these simple
elegant models we're running out of time
alright so but finally a cautionary tale
which I'm obligated to say ok networks
as a field is I think as we all know a
bit too popular for its own good
and in particular the naive topological
modeling of networks is a bit too
popular for its own good
here is an actual blackout after big
blackouts people do a careful analysis
as it like with airplane crashes ok the
network literature is filled with papers
saying well blackouts travel from nodes
to their neighbors right like an
epidemic well actually that's not how
the power grid works at all so this
power line went down and then that one
and then that one and then this one
hundreds of miles away and not even
topologically close was the next to go
because of the way the flow of
electricity through the network
reorganized itself with the speed of
light it's a very non-local process so I
say this because it's important to
remember that this topological picture
of networks often ignores a lot of
important real-world dynamics every
metaphor has to pay its own way right so
in the 19th century everything was a
steam engine and then everything was a
computer and then in the 90s everything
was an economy like even the brain even
like neurons in the brain or an economy
unless you're kind of more earthy
crunchy than it's everything as an
ecology because then there's symbiosis
and mutualism and that's much nicer and
nowadays everything is a network so each
of these metaphors make certain aspects
of the real world easier to see but may
obscure others thank you very much
you'd say that thing using marginals
just likely to be more robust so do we
have any evidence of that or is it just
well we certainly things we can point to
where where the ground state is a bad
estimate and this marginal approach is a
good estimate but there is also a
theorem from Bayesian inference that if
you want to maximize the the number of
variables that you get right you want to
use the marginals and not the ground
state you know on the other hand
computing them is can be difficult so
because it requires some kind of
sampling or exploration of the space but
sometimes belief propagation does that
very well when it doesn't you can always
fall back on the Gibbs sampling which
will always work if you give it enough
time but that might take a long time so
let's suppose you have a community of
people and I know there's some
controversy about whether we mean
so-called annotated communities or
structural communities but let's say you
have two groups of people um and they
really do link to others in their same
group with probability twice as big as
you know outside their group but if the
network is too sparse then indeed if
you're not given any of the metadata if
you're really just trying to cluster
them based on the topology alone there's
a regime in which you just can't which
you know put that way maybe that's not
so surprising but you know this hat this
hadn't been observed or discussed before
and I think people knew that well if I
connect with those in my community with
probability one point zero zero one
time's the problem I connect outside it
fine but actually this can happen when
that ratio is three halves or two I mean
one way to view it is that in a really
sparse Network if people don't have that
many links and I know on Facebook people
tend to a lot of links but um a lot of
friends but in a sparse network if
everybody only has a small number of
Link's than bite by chance some people
might have more links outside their
community than inside their community
and that is one reason the algorithm
gets confused so you can view it as a
theoretical result saying sometimes you
really need the metadata and then I
think this gets into interesting
questions like what if obtaining the
correct metadata is costly what if
people have some demographic attribute
that they don't want to tell you or some
of them left at blank or some of them
want to lie about it and maybe you can
discover its true value by offering them
a coupon or getting a warrant from a
judge or something and you know then the
question is if that information is
costly which nodes or which links should
you really examine closely which ones
will teach you the most which gets into
this nice active learning question
and that might be a little premature
process that might be a lot easier to
learn about I mean if you give me the
real labels and the links then yeah from
that I can very quickly so the link I'm
thinking about the linking to this we
sort of know that it's quite hard to
classify every point to being what the
other right I can somehow still learn
about the mixture process yeah although
there are there interest you know if you
take if you have a mixture of two
gaussians that are too close together
it's really hard to tell them from yes I
agree well and so is the first
detectability transition I showed you so
the case for two groups is really an
informational to your attic question I
mean going back to your question about
what does this have to do with Facebook
if i zoom out a little farther I would
say that there are a lot of algorithms
out there that claim to find communities
of various sorts and when you hook them
up to nice visualization tools it always
looks really beautiful
well you where in some of these some of
the things that these algorithms are
showing you are not actually there and
just as statistical significance is a
huge crisis in biomedicine right now
where we're discovering that many of the
effects that we thought were significant
aren't because of publication bias and
so on
well I think there's a similar issue
with any sort of data analysis including
network analysis</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>