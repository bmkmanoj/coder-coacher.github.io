<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MSR-INRIA Workshop On Computer Vision and Machine Learning | Coder Coacher - Coaching Coders</title><meta content="MSR-INRIA Workshop On Computer Vision and Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MSR-INRIA Workshop On Computer Vision and Machine Learning</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dv2KTCigQbw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay it's good so here we are so just
given the short time I just present to
20 person chapters and computer since
and external condition for videos so
what's external condition and videos so
either we have a clip which is already
extracted we want to do just perform
action classification so we have short
clips in which we have actions you want
to classify them for example this would
be the track for example very want to
classify things in making sandwich
feeding animal and so forth or more
difficult you want to localize and
search the actions in an entire video so
of several hours of videos and you want
to see were in the video the action is
actually performed and so obviously this
is much more difficult because you have
to have a much more discriminative
descriptor and so in this talk I present
two recent contributions one is on low
level descriptors how to improve the
low-level descriptors for absenting
videos and the second one is a high
level model I've been similar to it even
and oseph their students presented by
modeling the interaction between humans
and objects which can be done either and
still images or here in this context and
videos okay so first what is the idea
behind the low-level descriptors so here
the idea is that we kind of war ideas
from image descriptions I want to do
then sampling we also want to do the
sampling it's also described that every
frame individually but to track points
over over the video so to use future
trajectories a workaround where they use
for example coyote or sift try to inject
area so here we develop or introduce an
improved trajectory and then obviously
what you want you to characterize you
want to characterize the space domain
the time than weight differently and so
here the approach is perform then
sampling we have attracted points over
the frames using optical flow so what
you want is a good optical flow
detective scripter which then allows us
to track points for a short range of
frames very accurately and then the
descriptors they're based on some
Underland alignment with the
trajectories so we don't just take rigid
cuboids but we really align so we have
shown that this is important as well can
be evaluated align the descriptor along
the trajectory so then we can either
classical descriptors based on hog sift
like features optical flow histograms or
here in addition we introduce an
additional descriptor based motion
boundary histograms again here just to
give you an idea about the different
trajectories it's kind of obvious but
it's kind of nice to see as well as you
look at kelty tracks which still many
people use action nowadays you can
really see that there many outliers
there the noisy it works well if you
have large objects with screw texture
but if you have small objects and no
text to the latest fail completely don't
see this also in the context of our
object drugs then similar actually the
syph tracks there it's a nice idea but
again they have a lot of outliers a lot
of stories mismatches you can see we
really see if you live the future tracks
there's a lot of noise in there and so
here this is our dance tracks and
obviously you can say it's based on the
optical flow so here's the optical flow
or fan back which is an open CV and it's
clear obviously it depends on the
quality of the optical flow but if the
optical flow here is good can we do see
that the tracks are very good okay and
then basically mount motion boundary
descriptor again it's a very simple idea
but the idea is basically in many cases
you have my camera motion you want to be
robust up to some point obvious is not
completely invariant but you won't have
some robustness so if you have
background motion for example of the
camera which is a translation of the
camera you have all the optical flow
which completely Lutz your optical flow
histogram to just take the derivatives
of the optical flow vector you get rid
of that and similarly if you have
emotions in one plane and you just take
the derivatives you have the relative
dynamics of these regions obviously have
your complex camera motions it doesn't
hold but here it's really between two
very close frames so it's a good
approximation okay and then you can
obviously use these trajectories and
descriptors in many contexts so here
it's evaluated for the standard bag of
features framework you can obviously
also use it you recently be elusive
officially no representations where the
results further improve or you can use
these trajectories for clustering and
segmenting out objects it's like it's
not just related to useful for the bag
of futures but you can use it in
different context so here if you look at
the results so most important you can
see that actually this mode
boundary instagrams improved
significantly of over using optical so
so in both cases you have camera motion
in the data set or this is the Hollywood
data set where you have some small
asteroid action clips which you want to
characterize you had to use your sports
data set we also have background motion
and then it's interesting to see is also
that if you look here this case optical
flow helps much more while here actually
for the sports videos for the background
is very dominant so the actions are
always say for example diving is in the
swimming pool and so for the background
information is actually also very
relevant so that's interesting and then
what you can see here it's maybe the
most interesting thing is you can really
see that by using these dents or our
dance collector e you improve
significantly over using kelty or
syphilis trajectories but i think it's
we have seen it right it's like it's
more it's more noisy for visits at the
main reason i think if the tracks were
perfect why the kelty protects were
perfect and then basically could
propagate them around so it's both
basically it's the denseness which gives
you more information as you have seen
basically for images if you use interest
points for category confusion you lose
information given to person and then the
kelsey tracks there so i'll come back to
that in a moment they are really
intrinsic the noisy so they have i mean
basically if you want to track small
objects or things like that you really
want to have the optical flow which is
actually more more robust to this kind
of noisy outliers so basically in this
case matching interest points gives you
one clue but you want to also load
houston low-level signal so basically
the best approaches or one boot approach
for optical flow is the prox algorithm
where you combine the lower level
measures with the high level interest
point measure so that you want to have
both together and then do some
optimization over the whole thing and
have it locally integrated right into in
response they only look at two days and
frames and they don't take into account
all this information to kind of losing
out on the information which you could
get
move tax is the same for up to the same
rule 3 worry no that's it's not exactly
the same sir that's something which we
are now experimenting if you take the
oars away that's good to be twice yeah
but then settings why so few times more
so that for dense have removed the
slides because I have not much time but
then so you also do some pre-filtering
right if their points where this
actually you know information they're
sitting in completely homogeneous
regions will just remove them right
because they're you cannot track right
so you have also and you have a great
step so basically can evaluate the
performance over the grids that which is
then the same thing can just remove the
grid step to get the same number of
points I don't have the numbers exactly
my mind but we did the experiment so I
can actually dig them out because it can
such subsample I mean it's just it's not
dense it's just a mess up something on
the grid right so you can just remove it
further till you get the same number of
points for Kelsey Kelsey you can bury
them a bit as well so yes you still get
improving your sisters okay and then
basically also the results compare
playable through state-of-the-art so for
all the state of the art book data sets
of your outperform existing approaches
have I'm not showing this light now and
then basically we have seen so they
really outperform any of the existing
approaches which around today included
so there was his favorite CPR there this
year where they learn this density based
in place of dancing sectors and and
depth so this approach well then you
learn the hierarchy automatically to the
support our approach outperforms said as
well the motion bounded descriptors
perform very well they were boys to come
emotion and the algorithm is very simple
it's available online and you can just
write it and try it out yeah we haven't
so we also didn't use the blocks tracks
so that's something we try to do and I
plated it so they're so look slow all
these algorithms they're so slow that
right now I kind of trying to speed
those at algorithms up I think they
would give you a further boost right so
if you have a good optical flow the
Bellevue optical flow is I think I get
the feeling but really the results we
did on a very very small subset for for
proxy exits takes about a minute to run
per frame so if you want to run it on a
real data set is just too slow so I
don't really know your country's looking
at that if you can speed it up a bit
have some compromise otherwise if you
could test it so we did with comparison
the very small data set whether it's
obviously even better if you have a
better optical flow algorithm so he can
still improve a few percent but then you
really i mean really slow down your
complexity i mean one frame per minute
if you look at the video it's just more
possible rights we have to do something
about speeding that up further or I
don't know something we're currently
investigating okay and then the second
thing so for obviously what you really
want to do is you want to model the
interaction between the person and the
object so again i'm showing coffee
drinking but I have other data sets as
well you'll see the end chopping a
banana so hold on till the end and
you'll see and anyway you can see the
here is done automatically right on the
test set you detect the humans
automatically and then we also get the
cup and really see that the guy is
drinking that's really think it could be
to you yes yes no it's actually nice is
if you look at the beginning if you even
if you drink out of this mag is still
detected correctly and I guess that's
you because what our descript off the
object is captured some part of the hand
right so it's important next to that
it's not just the object but the hand
holding the object actually that's why
you're also having this big teapot which
is kind of nice friends that you run
dekap detector it is still to take part
and so what
you know they can detect I may still
find something else I say it's good to
find a girl is this email it's a sim
okay it's a similar shape so okay wait
till I explain Nick Foles but basically
what it does we have a very loose
detector and tracker so basically it
detects many objects because a cups in
these images for example or whatever you
see you see telephones they're very
small alright so if you want when you
have an end reply here with the cup so
the cap is very small and we all know up
to tech sectors if you object how small
work not as well right I mean this is
you must admit this as well so what we
do is we detect many of the cups so
these have many cap hypothesis so you
like I don't know 100 capacitances for
image we tracked them until we keep what
you do is with people many of these
potential cup tracks right and so then
you have the humans the human affection
I get like this a moment is pretty
reliable again there might be some force
policies with your diaper and then we
have a taped temper model which allows
to filter out all the cup tracks which
are not behaving like this okay so
that's probably why even if this cup
this bad cop gets around if you low
score you're still able to get this
motion information and filter it out
okay and also it's not for performing
perfectly hundred percent of the time
right so here's some errors anyway so
what works well here is our human
attackers of you have a good human
detector which we combine with for
example Prague tracks for the humans is
not so important the tracker is not so
important because you have generalized
surfers and then basically integrate
this over time we get really include
human tracks for the object tracks as I
said previously it's much harder so in
the beginning we use alt tracking and it
doesn't track at all because it just
loses these small cups all the time
they're not enough points in them so
what do you use now is this box optical
flow which really allow us to follow the
cops all you do is have a combined
detection and tracking framework we
detect cups and track them over the time
with the proc strikes those together and
we get still they're not perfect the far
from perfect will reduce but I said
before extract large number and filter
filter them out so we have a large
number of these pair
then we have some spatial temporal and
relative location descriptors which
allowed to filter out incorrect tracks
that's given so the the human detector
is learned because we have a yummy
detector which we have trained for the
cops we take so in the training you have
an elated their cups in the training
videos we have penetrated one cup /
training video and then we track them
automatically and use that as additional
training examples so and it's not that
automatically we have developed those
static images we have developed an
algorithm which actually selects the
object automatically so if you have menu
so that's something you could do as an
extension if you have many videos for
the human drinks from a cup you could
say that basically what you want is
basically what you're doing some kind of
course I plantation we have done that
for static images here the cups they're
so small that it was it's really
difficult to track them and to hear and
automatically so for now we're still in
this week leasable a scenario where we
annotate one cup / cleaning they're not
that many training examples so there may
be had no event it was better I think
they're like around 30 training videos
for me to annotate the cups for 3 30
boxes so it's not very very difficult
and okay and then we also combine this
interaction descriptor with some
previous work actually work with for
sandwich where's Andrew he's gone which
is which was done with Andrews over here
we have this track and they have a set
of low-level features within the track
based on 3d Hawk descriptors which
characterize the motion some kind of
optical flow inside the track the threat
follows the person in their psyche
inside the tech we have the small cells
very characterized emotion so it's more
we did descriptor right this is Amorites
descriptor while this is really flexible
descriptor which can model very very
many variations of the devotion and then
not to show results on the coffee and
cigarettes dataset have not showing
results on the Gupta later said it's one
of the state of the our data sets for
interactions and Kennison here on this
data set the interaction classifier
performs extremely well if you then
combine it with our
tracted classifier you have your 93%
performance which is the same as Gupta
well Gupta uses a much more complex
model uses for example effectiveness
aesthetic background and all that which
we don't so really it was as much more
training data so as like annotations for
all the human poses for the object so in
any way what if you don't get the
results we have only two misclassified
these examples of your hand of said
rating or display that's it and then we
also tried it on the register daily
activities data sets or their hundred
fifty videos of five persons what they
do is they leave standard scenarios
leave one person our test scenario and
so you have can see here again the
objects are very small again it's pretty
difficult to detect them and you can see
here that our interaction classifier
outperforms approaches which i use only
low-level features and then if you use
the full model using the two descriptors
together outperform the state of the art
and what's really nice if you look at
this chopping banana you can see here
that you can really follow the knife and
they really learn how to chop the banana
to basically our approach if you look at
the results here if you're better but
what's measured here it's only
classification show it so they don't
measure it all if the object is
localized correctly because what you
really want to would want to do here for
a good elevation is really to say do we
detect the objects correctly right
because that's something you really
would like to know if you really detect
the object because if you take the
object here then basically your model is
just doing something completely
different and it's wrong right so that's
not what they put a true what the
current approaches are evaluating so
okay so in conclusion we have seen that
basically our human object into X
descriptors outperforms the state of the
art its complementary to the CD hawk try
to descriptor and it obtains excellent
performance and like as a message the
tracking algorithm is really important
so something which I didn't expect the
beginning but something which we found
for both approaches actually go to
optical flow a good tracking is
important okay thank you I think I
didn't run over any time
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>