<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>eScience in the cloud | Coder Coacher - Coaching Coders</title><meta content="eScience in the cloud - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>eScience in the cloud</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XCMx3Wddo6k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so good afternoon I'm Dennis Gannon part
of the Microsoft team here i have a
question actually i'm curious of the
group here how many of you are PhD
students are students how many students
oh good good good group of them I was
very impressed by the way by the student
presentations that we saw just before
lunch or doing some very good projects
and how many are like university
professors or researchers that's what
about the other half this talk thank you
good this talk I'm going to go basically
give you an overview of what we have
been doing with the azure cloud at first
of all tell you about what the azure
cloud really is how it's built and then
i'm going to give you an overview of
just a few of the science projects that
we have done around the world it just so
give you a sense of what's possible so
first thing is I need to make this thing
doc there we go all right Tony hate
already gave you this slide the data
explosion is transforming everything
every field of research is now a data
science field and and the other thing
that he already told you is this that
all this is causing this fundamental
change in research and that fundamental
change is is what we call the fourth
paradigm which I think you've probably
heard 10 times already at this meeting
only 30 d3 okay I'll say up to three or
four more times before I for this all
right now what I want to do is to get
into something that Tony did not talk
about and that is what is the azure
cloud because that number of you use it
or I hope a number of you will want to
use it first of all it's based on these
data centers so we have these are very
very big facilities they use enormous
amounts of power and each one of these
facility
these the really big facilities that
I've listed here those are the current
ones there are more coming in various
places and we hope to see some come in
the places where you don't see them
right now we hope that will happen so
each one of these big facilities is on
the order of say half a million servers
so they're very very big plus we have a
lot of other smaller facilities that
maybe a hundred other what we call edge
facilities or smaller facilities around
around the globe now how are these
things constructed let me show you so we
go through these generations there's a
history of how to build these large data
centers that goes back 10 15 years
actually there's a the early ones the
very earliest big data centers were just
traditional servers you buy just
traditional boxes of desktop machines
and you throw away the keyboard and the
monitor and you stack them up and your
wire them together and that that was
really the early days of doing this
stuff that was those are the first
generation of cloud facilities of data
centers you know there's an enormous
problem with that approach because after
all you buy these big desktop machines
you get a very large pile of styrofoam
after a while and what do you do with
that that's a problem so the next thing
was to basically build them in large
racks to put these to increase the
density so just just put them in racks
by racks get get small servers flat
servers and stack them up then another
thing happened as we were populating
racks and and our competitors at Google
and Amazon and these other interesting
places building large data centers
started to ask the question how you know
we buy the racks we buy the servers
where you put them in can't we just buy
the racks with the servers already in
them and and I've come as a big package
and someone at one of the companies that
was supplying these
racks and servers said well yeah we
could do that we could put them in a big
package and the idea was that the
question was asked how big a package can
we actually built and someone said well
whatever it is it has to fit into a
shipping container and that gave birth
to a whole new industry which were giant
basically standard shipping containers
filled with lots and lots of servers now
at each step and so that so then you can
take a shipping container move it into
your giant building set it down and it
would be already configured and have the
network in it I would have you know all
set ready to go the buildings that these
things were coming in we're getting to
be a very big problem because they were
getting bigger and bigger and bigger
they're the size of you know ten major
football fields in size and their two
stories usually they're very big they
require massive amounts of air
conditioning they're not right and also
backup power is a huge problem for these
facilities so the last generation are
what we call our current generation of
generation 4 we took a look at these big
buildings and said do we really need the
building our servers are already coming
in containers do we have to put them
into building can't we figure out a
better way to do it and that created
something new that we call an IT pack
now what this is and I've got another
picture is a very specialized type of
container that we designed at Microsoft
and what it is is you have inside the
container you have racks with several
thousand servers inside one container
and it's designed those of you that how
many people here know about hot aisle
cold aisle data yes some people do yeah
and so basically you have the cold aisle
here the hot aisle here air comes in the
one whole wall of the container allows
air to come in this is all ambient air
cooled so there's there's no cooling
apparatus there is a bit of water
cooling but instead of a big pipes that
we had before it's a small what we call
in the u.s.
garden hose something that you used to
water your Tomatoes you know it's very
simple and that's that's the level of
water cooling required otherwise and it
only comes on in really hot environments
because this is to be placed with no
building in in any environment it has so
it has three connections it has a little
place for your water hose it has the
network connection and the power and
that's all and so this thing is very
self-contained its senses by changing
these these luver connections on the
side you could change the different
amounts of air flow inside you monitor
all the systems are constantly monitored
in the system so it can actually work so
to show you that this is really real
there is a picture of of one of them
arriving on a flatbed truck ready to be
so they just simply truck rolls in pick
up this thing with a you know several
thousand servers in it and then you put
it down and then you assemble these into
these frameworks here so this shows you
an actual data center there is no
building anymore the the IT pack is this
thing underneath to be honest I don't
know what the thing on top is it is not
air conditioning but it's some sort of
thing they have to have I don't know
what it is but this isn't one of the new
data centers so that's all there is to
it no big buildings anymore now we're
not alone in doing that but it is
something that that we're pretty excited
about it allows us to reduce the power
consumption because these things are
very power hungry and one of the
advantages of building a data center
that has millions of servers in it is
that you could do you could exploit
economy of scale that means lots of
servers where you manage the power
across the entire collection of servers
in the most efficient way possible you
have you have a cooling system that is
incredibly efficient it's ambient air it
doesn't require big generators we don't
we get away from backup batteries and
fancy generating systems by software now
which means that if something fails
we can automatically transfer load from
one thing to another so it's it's more
decoupled and this is very important
capability so that's how these these
data centers are built like I say the
other you know we're looking at a
variety of different ways of providing
the electrical power geothermal is one
thing that we're looking at there's one
experiment that's been reported on in
the press so it's no longer a secret I
don't think it ever was actually of
using putting the data centers next to
very large garbage places where there's
lots of generated heat and gas that you
can then recycle into in the electricity
for the the system all right now I'm
going to change gears and I'm going to
talk now about what we've done for
science in the cloud and so I'm just
going to give a series of examples now
so we started this project about two and
a half years ago when Azure was still
young and we started giving away pieces
of azure to the research community and i
was working mostly with the science
funding agencies so i went in the u.s.
to the National Science Foundation and I
said hey if you have researchers that
want to do cloud stuff I'll give you a
big chunk of my cloud and then you could
give it to your favorite researchers and
and then they said yeah it's great it
will also give him some money to
actually do something with and then we
went to the European Commission had a
good project there that around the world
doing this so now I got 90 projects
altogether actually that's growing
probably hit a hundred soon and and and
of those projects some of them were
really really turned out very very well
some didn't you know this was the sort
of thing you're experimenting with you
don't know what sort of science will
work in the cloud one thing it does not
work very well in most cloud
environments are the applications that
are designed for very high end tightly
coupled supercomputers there's a big
architectural difference between these
data centers and
a fancy super computer system and I can
go into that in a question and answer if
you want me to so for example some of
the projects we started one does is one
around protein folding that we did with
the University of Washington where we
used two thousand servers over a
five-day period to do some protein
folding having to do with a particular
structure of protein that salmonella
uses to inject DNA into its its victim
and and that was actually really nice it
generated some nice articles in science
another one I'll talk about it is the
fire protection thing that Tony already
mentioned we'll talk about that later a
project in Australia was looking at
using a lot of azure cycles to do
computations having to do with
magnetotail Urich's which is a type of
it has to do with discovering minerals
underground which is something in
Australia that they actually care about
in France the Institute inria which is
the major computer science research
institute did a really cool thing
involving looking at brain scans fMRI
images and genetics as a big data
challenge lots of MapReduce computations
to understand the connection between
certain genetic snips and and and
occurrence of brain tumors and that's
work is still going on in Japan we had a
really great project on looking at
napping Japanese natural language
analysis and structure so that's that's
those are a few let me give you some
more and in in the Europe we started
with seven pilot projects and these
these were very very interesting and so
I would go through some of these one was
to really look at basically looking at
large-scale structures such as
architectures would design so there's a
professional architectures and civil
engineers built some tools for tree
modeling they're sort of
three-dimensional test cases of what a
physical structure would look like
another one was this
company this was very interesting he's
company called green PFF in Italy is a
startup company starting the University
I was fighting university projects they
got going and then it became a company
and they were looking at ways to create
architectural renderings for buildings
for green construction let's see that oh
yeah another project was looking at the
distributed the distribution of species
in a variety of places occurrences is
relation to environmental conditions
then we have the Greek fire one which is
also good for now bioinformatics to a we
have a lot of success with
bioinformatics tools I notice that
there's a number of the project here or
related to bioinformatics another one
having to do with systems biology us
distinct from say genomics and another
project having to do with drugs
discovery so these these were our seven
pilots in Europe after those seven
pilots we wanted to another I think
about 20 or so additional projects after
we you know we got these first pilots
that so that was that was very
satisfying now the one again I don't
have to talk about it because Tony hey
already mentioned this this was this one
in Greece and it was really interesting
because it started at a university
project and and they really were looking
at the idea of dealing with crisis
dealing with us getting data together
from a lot of different sources I mean
they have sensors on all the fire trucks
I think they have sensors and all the
fire it's finding personnel like they've
got sensors round in the forest on the
island that are measuring things like
humidity and heat they've got the the
weather prediction stuff bring all that
data into the cloud and integrate it and
then what the cool thing really is they
did is in a way in which it could be
used by the actual firefighters and so
they deployed this in in in places where
the firefighters could actually get at
it they could communicate they could
look you know I don't know if they got
as far i know they're working on this so
that the firefighters in the field could
actually look on a phone if they've got
phone connects
they could sort of see the map of what's
going on as queries now this was an
example firefighters using this result
of cloud science that was very very
significant to me because I very
strongly believe that something that we
have to do is to let scientists be
scientists the people in this room
you're all adventurous your adventurers
are willing to try crazy things and do
programming and complicated scientific
endeavors most scientists don't want to
do that they don't want to program
supercomputers most scientists don't
want to program a cloud what they want
to do is get their science done and it's
just like a fire fighters they need to
fight a fire they don't want to have to
stop it and write a bunch of Java code
to in the middle of a fighting a fire to
get something done they need to fight
the fire scientists need to get their
work done and what if scientists do they
use standard tools they use spreadsheets
statistical packages if they do any
programming at all it's usually
modifying some parameters in an hour
package and so a lot of the tools that
Microsoft people have been showing you
are really about helping this long tail
of science this this community that
needs are they need to be able to do
their science better but they're going
to leave it to people like you to do the
hard part to figure out how to make it
better and so you know there's there's
another view of this we want to bring
the cloud to the desktop that's
something that we feel pretty strongly
about five minutes perfect now one of
the things that we've been doing on this
is well let you have this let me just
say through this so that we need to
heard this message already several times
data sharing the cloud they need to
sample and manipulate data tap data
analysis algorithms invoke analytic
models do machine learning and publish
data and do visualizations so we've been
working on a number of things and you've
seen some of these so I can skip to them
one that we've been working or that I've
been working on with some other people
is just created a lot of the scientists
I know build really incredible tools
using Python any Python programmers here
there
is a few yeah so that gets so what we've
done is we have put first class support
for Python on the azure cloud so if your
Python program we can do Python really
well there's something called the Python
notebook which is a really great way
interactive way of running python
applications on the cloud it gives you a
way to type in your code your Python
code and execute it and get the
visualizations directly so you're
sitting on your laptop but the
computation is running in the cloud to
do your Python work now we have other
applications that you've already heard
about in the last session you've heard
about world wide web and layer scape
chromosome and vets climate so these are
the types of tools that we're trying to
get out and these are tools connected to
the cloud they depend upon the cloud but
they're designed to help the scientist
do their science so you know in the
future I think there's a number of
interesting new horizons for science and
ways places we can go I'm going to skip
two three some slides here and get to
this last slide which I think the most
important thing we can do is to help to
democratize research today the majority
of researchers don't have access to big
computing but tomorrow we think that
they they will and they will have access
through these applications in the cloud
so I mean oh I'm out of time there and
we've got two more really good talks
coming up okay thank you very much
Dennis and i'll be around after the
thing and around tomorrow in case a
watch stopped well we do have a couple
we can we have time for a little bit a
couple of questions if folks have some
questions I'd like to ask hi my name is
veneer scarf I'm from the University of
campinas and I am a theoretical chemists
I do molecular dynamic simulations of
proteins just one of the applications
that you've shown in my group we have
built this the first stage we put up you
know a few tens of
desktops and we built our own clusters
and then we drop that and it purchase
the Iraq but now my chairman is
complaining that i'm using too much
energy as too much air conditioning so i
want to drop that and go to the next
stage but i want to be a scientist and I
don't want to build one data center so
um can i buy time at the Microsoft cloud
can I get time for free that's we're in
the business and that's a very nice
thing to hear when someone says can i
buy some yes you certainly can but we
could maybe work together to do
something more interesting so we could
talk about that all right thank you
actually I have a question for you
Dennis you mentioned that some of the
projects were more successful than
others and I won't ask you what the
statistic was even though mr. Brito gave
us their percentages were but I'm
curious what actually do you use to
measure success is it that because the
problems are embarrassingly parallel or
is it some other factor so actually
measuring success is really well I could
tell you the statistics actually I
consider about thirty percent of them to
be very successful and the way I measure
success is that finally is there a
result that is something running in the
cloud that the researcher is actually
able to in most cases these were
scientists that wanted to do more
science and if they are begging me to
use more and more cloud because they're
getting closer to their scientific goals
and their students are all using the
same platform if they're you if they
built something that they can reuse and
they're using it that was my sense of
success nicely ok well let's thank
Dennis one more time
our next speaker is market matos oh and
she's gonna actually be talking to us on
a particular look here users users
during my work clothes oh ok hello
everyone I'm very happy to be here and
part of this exciting workshop we also
have a Brazilian a science workshop that
I started with a chef Zhang from here
from wisp and it's very nice to have
this opportunity here so we've seen many
topics very interesting in science
challenges I'm going to talk to one
specific that involves cloud but also
high performance computing workflows
which is related to the participation of
the user during the workflow execution
long time running workflows so this is
the agenda talk a little bit about the
life cycle of the workflow and then the
challenges on user steering so this is a
generic I actually it's a real workflow
of one of the users that we work with
also related to the Trypanosoma cruzi
problem that Christina mentioned earlier
with the leash mania and drug
investigation the idea here is to show
that initially many work phones work
this way but you know you collect data
you do some analysis on your desktop and
then you evaluate long-running programs
in the cloud and high-performance
computing center and then you analyze
data if you have a workflow management
system behind this it actually can
follows the execution from all these
areas and collect provenance my point
here is related to the time that the
workflow spent here or the the programs
that are executing here and rub already
mention the problem of a batch
experience you know waiting for a long
time for it finish here and only do the
analysis later and you realize that it's
wrong so we are working on issues of
improving this part of the steering the
long running tasks not just following
what has been going on but also being
able to interfere on this execution and
save some time so the workflow execution
lifecycle actually has well many phases
but in a general way it could be seen as
a data planning and execution and
analysis we've seen a lot of very nice
tools on the tutorials and on the talks
about how do you select the data due to
the data planning and also to use
visualization and provenance to do a
very nice analysis here so I'm going to
talk some more of this part let's see an
example of a long-running workflow where
you have to combine a lot of parameters
different values analyze different
algorithms if you have to work in a in a
batch mode such a way of speaking just
follow rods term you have to plan in
advance all the evaluations you want to
do and then you write a script and then
the workflow execution engine will
follow that and give you all the nice
provenance that you can see during the
workflow execution but what if you would
like to change some of that during the
execution not just interrupt so these
are there are some issues there so here
are a few of some nice tools some of
that some of them are very familiar with
and some of them are very good in high
performance scalability and they all
share the good thing that is collecting
provenance but what happens in most we
in all of the seasons as far as I could
see is that provenance is only available
for querying after the workflow has
finished you can follow what's going on
you can see each step of the execution
but you cannot really submit a query and
search for a specific date or a specific
part of what's been going on there so
following that batch mode kind of
working the workflow spends a long time
here executing and then when you realize
it it's not exactly the way you wanted
then you go back and change the
parameters change the combinations and
go all over again and so our users would
like to to do some changes there what
happens is once they start using the
workflow systems they get very excited
about the provenance about the way it's
doing very fast about the way they don't
have to buy a supercomputer they can run
it on the cloud and then they start
worked wanting for more and so what our
users have asked it is was to do some
interference I would like to query
inside data during the execution and
interfere on the execution so I was very
lucky to see the exhibition on matey's
called pairs and series so he works in
parallel he does the same painting
parallel using different views and the
nice thing one of the things that
reminded me about the scientists I work
with was the script you know he does a
script following determining all the
colors that were there is a sketch and
all the colors that need to be used the
texture the styles and all of that and
of course during the course of the
painting several changes are made so
it's not like you know you follow that
and then you get
to this very nice picture as in fact
several stages are going on but these
stages are not fixed it it's not like he
is looking at each stage and deciding
the and waiting to the end just
following the script what happened is
that he changes the color in the middle
of the execution there's other changes
and that's actually what is kind of
happening you know being able to see
follow step by step of the workflow
execution is very nice but an
interesting thing would be to change the
script if the script of the workflow is
hard-coded and it's very hard to change
it and do interference and continue to
have a consistence is a consistent
execution so these are one of the
challenges of doing this runtime changes
and dynamic workflows so basically we
can divide the workflow systems like the
knopf line and the online so the offline
is the batch mode where you run and wait
for the result and the best you can do
is if you see something is not going
wrong if you open your files and do an
interference then you can interrupt and
start all over again and an online
dynamic workflow execution would be
having access to query provenance having
access to evaluate partial data find
some parameters occasionally visualize
the partial results that are being
generated and this very long running and
execute and also skip some parameter
combinations is also something that we
have been asking for so we are not
inventing this problem in 2006 many
scientists gathered to discuss carol
global was one present at this meeting
and they come up with a very nice paper
on this challenge and several of them
have been become true so scheduling high
performance this
yeah I mean challenges are taken care of
but this part of the dynamic workflows
is still kind of open so there are many
interesting problems to work with in
this area so here are some examples of
real users real applications that our
group has been working with using
high-performance scientific workflow
support so on the bio informatics the
evolutionary studies and drug research
they would they like to query the
provenance and see if the results are
having a bad quality if the the filter
they value a similarity parameter needs
to be adjusted due to the
characteristics of the data that has
been analyzed in the oil and gas they do
large parameter exploration for example
to analyze the structural properties of
these risers that connect the platform
to the bottom of the ocean they have
2,000 meters long so they use extreme
conditions and analyze its behavior to
predict how it's going to behave but
sometimes you know the the combination
of parameters could benefit of a
branch-and-bound tech technique you know
based on this parameter this is fine I
don't need to evaluate all the rest and
they would like to have access to that
slice of the parameter space and maybe
do a visualization of that slice of
these characteristics so combining these
things is very important in one of some
of the tools we saw in the tutorials
they can do that but as far as I could
see the user would have to wait for the
workflow to finish and do that so doing
this in runtime is just a matter of
having provenance available and
related for example but jumping the you
know pruning a slice of the parameter
space that's kind of more complicated
another group that we work with also in
the oil and gas area is the the
structural engineers Mechanical
Engineers where they run solvers and do
numerical simulations also to analyze
the relational fluid structures and down
in the ocean and they are applying
uncertainty quantification models to
improve the numerical analysis and
predict the behavior of this interaction
and once they started using our workflow
high performance solutions they realize
that by acquiring provenance they could
interfere and adapt the certainty
quantification progress you know the way
it's going to arch the best result of
the convergence or the error sometimes
before it reaches what the original
algorithm would do Pike wiring problems
they know it I'm fine here and then
improve the result so these are as like
a summary of some challenges and the
potential of things that you could do
and some of the issues involved so the
first part is somehow a bit easier
because users steering you just have to
make data available during the execution
so that the user can analyze but it has
to be a nicely related so that you can
browse and go to wherever you would like
to and having access to the slice
specific slide runtime steering and
interfering having a dynamic workflow
there you have more challenges but it's
still many things that are possible to
to be done so fine tuning these sobres
and being able to take advantage of the
first part that has been
executed because usually they run
several steps several programs so having
that contacts kind of in advance and
just changing the following is something
that they are proved they are finding
very useful so changing the tolerance
some silver options and and the stopping
criteria why why it's hard kind of find
this this type of interactive and user
steering and while I mentioned most of
the time Providence is being collected
in logs and only available for query at
the end of the the workflow so one way
of dealing with it is to already store
provenance in aquaria Bowl format or a
database system in our case and then you
could store not only the definition of
the the workflow that's going to be
executed but the files it's going to act
as if this is all structure you can kind
of relate things another difficulties
that sometimes most of the times the
user submits the workflow from the
desktop sometimes the initial running
and executions are done locally and the
other goes there so the provenance
gatherer has to keep following what has
been going on to collect program
problems but this is not a problem as
long as it's all interacted and
integrated in not necessarily
centralized database so this is a query
and in here I am going to show an
example of a kind of interference away
you could represent this change on the
iteration and and all that so if you
have a flexible script to run your
workflow parameterize it where you can
replace some things maybe you could at
the beginning there we have the data
files that are going to be used some
some programs that are going to be
executed to run the workflow and then
you check the error and go back again
and so on so if during the course of
this execution the user here is able to
submit a query to the provenance
database and then decide to change or
interference because I she couldn't plan
in advance all the possible situations
all the combinations it's not just
looking at the error they actually
analyze other data and other
characteristics to interfere there so by
having some checkpoints and some notices
that you can receive during the
execution you can take a look and decide
to change you know these workflows are
long-running they take hours days
sometimes weeks the user is not going to
sit in the desktop and wait until the
end or we had some difficulties in some
tools that the client of the workflow
management system had to be on the
server on for the long duration you know
you have to do in a kind of remote way
but still connected and on some specific
parts so having a checkpoint is not a
difficult thing sending a notice through
Twitter or wherever is also possible and
then submitting queries to the
provenance database at the cloud and I
now having an honor and analysis of what
is possible to do in that way you can do
the changes submit I would like to
change this for example so if we want to
classify kind of the issues that are
related and involved in this kind of
support we could say web workflow
execution monitoring data analysis at
runtime and dynamic interference in the
execution in each one of the steps has
its own challenges and
necessary tools to combine with we are
going to present a paper in the cig mod
workshop next month where we analyzed
current solutions in details and so I'm
not going to discuss on all of them you
could have the look at the paper but the
important thing is there are a lot of
open issues most part hasn't been we
haven't found enough information for
this support kind of so the dynamic
monitoring's the problems we have been
working we are working now and we have
been working of last year's and they
could be divided into simple in Terry
interaction interventions and more
complex interventions so these are some
approaches 0 so here is our approach I
just like to point a one issue that we
think it's an interesting part of the
approach is the to have an algebraic
approach I'm from the database area so I
love relational algebra and structuring
data and having organized things in data
view so we have this algebraic engine
and all the the script that's going to
be executed is structured in data like
the relay in relations and operations
that work in the break we don't put the
code of the user inside the database we
don't change the binary date and the
local code into a text or whatever but
we have with put metadata in the
database style and we define the
workflow in an algebraic way and in this
way since it's all structure it's like
we're running an interpretive workflow
execution engine and it's not that hard
to change things during the workflow
execution and in addition to that the
user can quite
re and we can use Providence to improve
our scheduling algorithms our fault
tolerance algorithms because we have not
only access to run time but the kind of
behavior that this systems have so we
haven't been using that on the closer on
the cloud and we are taking a lot of
advantage of this very special knowledge
that providence can give you at from
time to do adaptive executions and this
kind of thing so we have experimented
for it this kind of parallelism is quite
simple it's like they had to the mat
inhale to do you know you just have to
run the same workflow for that different
combination of parameters the problem is
balancing and taking care of all that
and if you take a Hadoop a plain Hadoop
approach we what we have seen that in
this kind of application it doesn't have
adaptability to predefine I'm going to
use a large scale machines and i'm going
to use 11 machines and that's it it's
going to run until the end no provenance
whereas in this kind of approach if
Hadoop had this outer break approach
then you know the pig algebra when it's
method to Hadoop they forget about that
and they follow that script so maybe
using an algebraic approach using a
provenance would do a lot to this kind
of so do I still have time ok so this is
just a nice picture of one of the things
we have been doing with our by
informatique users represented a poster
last year in the supercomputing
conference so the idea is that the in
this lightning here and you know the the
workflow passes by a checkpoint and the
user gets an SMS and wherever and then
start submitting a query and then with
this table she analyzes what's going on
and say I'm going to stage out this
partial result and see how my flow
genetic tree is behaving
and then brings it back uses
visualization tools and tie wall display
and all these many high-tech things and
our engines are there controlling what's
going on and heavily based on problems
that and that was I really wanted to
talk about this but unfortunately
because drew talking so much about about
inverse problems about certainty and
predictions and these users that use
uncertainty quantification on Sobers
they are very excited with they're using
of workflows and all that so we have
some papers okay so in this case they
are they are trying to I've put a
picture of a submarine which is easier
to see but in fact they are interested
in the deep oil platforms and in the are
fluid dynamic and structure so here they
are analyzing the turbulence and of this
moving object that is also happens in
real life and so they have a parallels
over with the parallels ovaries hear
that a lot of steps they generate
billions of meshes to solve with finite
elements and and all of this is part of
the uncertainty quantification process
where they want to predict what's going
to happen and how the resistance of the
materials and this kind of thing so they
have some samples and in each sample
they run this small workflow and what
happens is that our provenance gather
follows the wherever they are going and
collects what's happening now the meshes
and all the things and when the
uncertainty quantification process they
have whoops several samples and to do
the turret of process so they don't know
in advance how many samples they are
going to use like Drew mentioned if they
use a Monte Carlo simulation
they have to prefix let's say 1,000
samples and wait four hours whereas in
this case in the uncertainty
quantification you already do lesser
samples but in the adaptive uncertainty
quantification by querying provenance
they are aware of the behavior and they
can finish earlier they can see in a
better way so there are details I am not
familiar with uncertainty quantification
but our specialists are very happy with
what they have been getting from in this
case we are not really changing the the
workflow but we are finishing earlier
and with a lot of provenance so that
they can play afterwards with it there
are some workflows in bioinformatics too
and so just to conclude there is a lot
of room to work there I believe that by
having provenance been standardized by
doubletree see we have a lot of
opportunities to have interoperability
between what's being collected by the
several tools with we'd like to work
with other tools we just have this
high-performance engine but no nice
tools so that the user can interact we
have to help them a lot so these are
some fun dng agencies and our code is
available if you like to try or ask us
and no nice interfaces but a lot of real
examples
okay um we actually do have time for a
couple of questions so well actually
I'll before these guys come up with
theirs I do have one that i was thinking
of as well so I always I always think of
the difference between dynamic workflows
and being able to query the provenance
at the end as a trade off of one another
so when you're making it reproducible
that's really what it is or its
reproducibility which what do you save
out if you were actually allowing a
collaborator to reproduce your results
using your method well the Providence
that's stored it started in the way of
all other solutions so in order to do
the reproduction you mean when we change
the workflow and yeah so you start with
one workflow now you change it to
another workflow which it's the second
one that gets published out is that
actually the whole history the it's
available and it's and it's there
because if the the person that wants to
not reproduce but reuse that idea if
they get just the last part it's not
going to be exactly like this but they
have access to what happened in the
beginning in the middle and in the end
okay we don't throw any provenance away
no big that you were okay well thank you
very much our last speaker is Dan pay
he's with the microsoft research
connections team and his he's synonymous
with science at Microsoft because that's
the blog that he has out there in
addition to running another blog I don't
know actually how he finds all the time
but so he's going to speak to us on
science in the club
cool good so again for those who are
here yesterday not I get the unluckily
unlucky bit to be the last one right
before the cocktails and the the end of
the day right so I'll try to go as
quickly as i can make sure that we're
out there on time but as Chris had
mentioned and I'm danfa I head up this
area that we do in our science group my
timer here I want to talk about some of
the stuff that we do with a science and
what we've been doing especially around
some of the the work and Randy science
and Azure in the cloud and taking
advantage of some of the functionality
there so so one of the things that we
think about a lot when we look at not
only the cloud but any of the
technologies that we have and you've
seen some of them from the different
groups the stuff that Drew was showing
earlier this morning some of the
visualizations that Rob was talking
about all across the board here all
these visualizations and tools and
techniques that we have as a company as
well as information how do we use that
and helping scientists actually do new
ways of not only processing analyzing
visualizing but another key thing is
communicating the information in the
data that others so it's kind of a theme
through some of the things that we we
end up doing wanted to highlight this
yose article that Jeff Dozier who's the
snow hydrologist had written around
that's in the theme of the fourth
paradigm but really taking advantage of
how do you take satellite imagery
information from that as well as a
remote sensing data and local on the
grounds data combine those together an
analysis that you can actually get some
more interesting information out on it
and it's a really good kind of synopsis
of what we think about when we think and
we talk about things like the the fourth
paradigm just in general combining you
together many different types of data
sets
one of the other things that we think
about a lot and this is a slide from
some of our collaborators and it really
does hit what why not only processing
information what's valuable is that as
you go up as you process more of the
data sets and you make it more usable
and more scientifically ready for others
to consume there's a value in there all
of a sudden you're not having your grad
students or those postdocs or the poor
students here doing a bunch of grunt
work just getting data pulling it all
together and actually trying to actually
get to the science so if you think back
if some of the work you've done how much
of it was actually not even science
related but getting the data ready to do
some sort of scientific work and so this
is one of the things that we look at and
it helps with the overall insight into
the information and so there's an
interesting value in there and how can
you provide that to others you know as a
service or make it available not just as
purely the data but it's something that
they could process against and so when
we look at that we see that's where some
of the benefit of cloud computing comes
in not the fact that you're now just
moving your computation from your server
in your office to somewhere remotely
it's yet you're actually giving somebody
access or processing that information
that they don't all have to do right why
should I actually have to rewrite a new
algorithm when you actually have the
best and then the other thing we think
about though we don't know exactly the
way to solve it if anyone here has a
good idea would be how do you reward
folks that are great at the work that
they do they're great at processing
information and algert and writing
algorithms to process the data that you
know you trust it everyone in the end
their domain trust it how can they be
valued and actually given credit for
doing that that's a really key thing
because why have people multiple people
doing the same exact thing if somebody
actually does a really good job on it
and we see this a lot with data sets
like people pulling it from NASA down
reprocessing the modis data sets in the
exact same way every time so that's one
of the things we think about you can
kind of tell you get to look site about
that
this is some of the stuff hopefully most
of the folks here know we and we kind of
talked about it also der Drew's talk
about the you know the overall issues
with data addict and especially the
spaces of ecology but gets interesting
because you start pulling together data
sets of many different types and how do
you end different scales and how do you
pull those together and actually compute
those and also makes get some sort of
meaning at it everything from the model
information the output the any if you
have any satellite information coming
down or even sensor based data but even
going back to historical information
which will never be as accurate as the
current information you're getting
because of measurement techniques and
other calibration techniques that we
currently use now so you know you have
to put all those in there and that's one
of the things drew was getting across
this morning was how do you bring across
the uncertainty in the calculations so
that they propagate the whole work and
not just one piece of it again different
types of data sets how do you pull them
together it's this is really where the
especially in ecology and environmental
space these are the challenges that we
kind of been working with and and some
of the projects that we've been working
on so but why why kind of go through all
this and tell you about that well it's
really because you know there's a whole
bunch of things we heard about the
provenance of it but there's problems
really differently in the different
domains how they're being done there's
obviously data sharing concerns
environmental data is really hard to get
people actually have to go out in the
field and actually go and get it and
collect it get dirty other things you
don't just sit back and have it flow in
so there's a real ownership to the
information and data and so how do you
actually make sure to share it in a good
way and used in a good way but also the
person gets credit for it and then
there's a whole bunch of different types
of computational tools but the part that
I actually really think that it's
interesting is this what we put down
here on the bottom which is we actually
think now science is really happening
when you start
bringing all these different data sets
together and looking at them from a
different kind of lenses across the
scales of them that's where you start
finding out some of the more interesting
information so a lot of this is some
background into one of the projects that
we've done over the couple years ago
which was called a sure motifs which was
a process and this was originally done
on Azure as it first was which was
doesn't have all the functionality we
currently do but it was a service that
we put up to help a researcher at UC
Berkeley to do some evapotranspiration
analysis over the whole globe at a think
it was a five-year ten-year continental
scale at a fine more fine-grained that
had been done before previously he been
trying to do this on one machine would
have taken forever to do it so we were
working with him on actually how you
could pull this data set in pull their
tiles from NASA from ftp sites actually
reprocess it because it comes in swass
and so if you want to get to lat lon
latitude longitude and actually do it on
an overlay it you actually need to
reprocess it and reprojection on the
values to get the evapotranspiration and
so there's some of the numbers here that
we had previously done and then get the
result back to the scientists so make
this as seamless as possible the
challenge always is that there's ftp
stop sometimes the files aren't there
you know so there's a lot aegis issues
just in general when you're trying to
pull some of these data sets across and
pulled make them available this actually
we're continuing on with a new project
actually with the young rell because now
and wasn't UC Berkeley it's now at in
Korea and so will continue forward on
this looking at how we can actually
process more of this and in a way that
makes it available to others so when I
look back on how we did this it's what
we call platform-as-a-service when we
think of cloud computing and just as a
quick
background for folks that may not know
the difference between some of them
there's the idea of infrastructure as a
service so where you essentially are
hosting the machinery there's the
platform as a service we're actually
building a lot more the functionality
directly on there but you let in this
case cloud or in this case a sure take
advantage of everything you don't worry
about the OS you don't worry about
updates you don't worry about anything
in there or software as a service where
you actually have the service available
for others to consume and so these are
kind of the three pieces that you'll
hear in kind of the standard discussions
around cloud computing and then I want
to talk about a couple of those today as
well as the describing a couple of
projects we've been involved with a
quick kind of marketing slide wanted to
throw that in there a shiur really is
about all these different services and
pulling them together so I'll cover a
couple of those you know three main ones
that I think are really applicable the
folks here is the ability to use virtual
machines and I'll kind of go into that a
little bit more which is it doesn't
matter what types of virtual machines
there's a whole list of them that you
can do the ability to pull up a get a
website up and running really quickly
we're again you don't have to worry
about managing the website except for
your content and you can actually have a
scale up and then things like cloud
services that platform as a service how
do i write bigger applications that run
across the whole network take advantage
more the infrastructure and this this
kind of exemplifies it a little bit
better where in the case of the VMS you
can have multiple virtual machines
pulling from a gallery one of them is we
already have predefined ones in azure
where you can just pull those get them
up and running and install your software
and get going so there's I'll show that
in a minute there's a you know windows
server there's Linux ones there's all
over the board there there's also the
ability to have a your own user-supplied
VMS and so make those available not only
for yourself but for others to actually
utilize and i'll show something in a
minute where there's a gallery we have
out there as well where you can actually
post yours think of these as
applications on the cloud right all of a
sudden it's up and running I click it
and I'm running I have the environment i
need i have everything running I don't
need to actually install much because
hopefully somebody who's working who had
created it has all the connections and
the links that you already need and the
software the websites can have multiple
of those running off the same server or
singles and then the cloud service is
where we for folks that are familiar
with Azure there's things considered
worker roles as well as the web roles
and these are pieces that work on behalf
of the the service so a lot of the
pieces that we saw the azure motifs are
the are the code that actually runs as
both the web roles and also as the
worker role so they're the part that
actually you know crunch way for you so
just some of the pieces that are all
involved one of the things I people
don't when I talk to people about what
cloud is or about Azure they don't
realize the amount of services that are
actually built on top think of it almost
as if you had an operating system what's
also on there so there's the compute
pieces where there's the the cloud
services the VMS and the website there's
a whole bunch of other parts in there
from everything from a full sequel
server to blob storage table storages up
there and the ability to have virtual
networking so you can actually have
connections from your local on-premises
machines and work with the remote
machines as if they're local so you can
actually burst stuff up and back you can
host data locally and actually have all
the computations up remotely a whole
bunch of different types of
functionality also there but also then
support for different languages so one
of the projects it's also been going on
for a about a year or so is this project
one of our guys actually it's in our
team now wen Ming has done which is
what's called weather in the cloud it's
essentially bringing the wharf model and
some of the other functionality and
implementing it natively just right on
to on to Azure so make the weather
computing
like a service and so you can actually
go if you have your laptop open and are
doing email or want to take you notes
you can go up to the weather service doc
cloudapp net the URL up there and you
can actually see some of the the pieces
that have already been done but it's a
it ends up using a lot of the platform
as a service and some other
functionality as well to me bring that
up to allow you to run this as a service
so so hopefully this one actually
finished so I actually started this one
earlier ok so it's starting the
animation step I started one right now
to go through what the weather forecast
would be for sale polo during this time
frame because you have three day whether
one you can pick the area that you want
and they actually have it run because
we're running on a small node right now
it takes about probably about five hours
to execute because it actually goes out
pulls in the data execute sit and then
does the animations on it but we also
then have all the information on there
actually let's go to a smaller songs
okay which has all the log files what's
actually happened during the the
computations and all the whole kind of
the data on there you can also pull down
the logs themselves what will occur yeah
that's not done yet I'll take a look at
this one we'll come back to this second
if we go to the gallery you can see a
whole bunch of other ones that have
already been computed and so we have the
imagery up there will it'll do the most
California one we can go in and actually
see the animation of this case it's the
heat and play it through as well so the
temperature during that time frame and
it will start rotating around poops but
what
interesting about this is all of a
sudden we've we've taken a wharf model
running and actually put it up as a
service that you can actually just run
query get actually execute it started
running and we know about how long any
of these should take from pulling the
data sets down executing the wharf model
and then performing the animations on
hopefully there's not gonna be any snow
oh that's good for this time of the year
but also the yukata the you can also go
through the precipitation on it if they
were sending going on and if I wanted to
create a new one it's pretty easy to
actually to just select location and
then just have it go off and execute so
part of the idea was Helen can make it
easy and but also the real key piece in
this whole application was how could we
take off the shelf code and actually
very easily get it running in the cloud
as a service and make it a bit and kind
of make it available so let me just
switch good this one since it's using
wharf model uses just standard MP I
didn't change anything in there and it
does the detailed forecasting there's a
whole bunch of also data sets on there
but again feel free to go try it out
it's a we just have it up there running
and running either my eyesight's going
bad or this is very blurry but that kind
of shows a snapshot before of a lot all
the ones that have been done in those
areas and so then you can actually go in
and see what's already happened look at
a typical kind of website interactivity
so what's actually kind of happened in
this area so we actually use what's our
HPC scheduler for our high-performance
computing on Azure we do the have the
scheduler kick off the computer ob's
those are written as both as worker
roles within a sure start they at start
the data sorry to start the processing
which goes out to know a pull
download the latest information data
sets for that area actually does the
computations and stores out into the
blob storage within within a sure and
then you can keep doing that for more
more data sets so to even increase it
you can actually create user management
pieces that are already in there to
increase the size of the the vm you want
to use or the the roles that you want to
use so it makes it a very nice interface
to deal with to do some of this work
what we ended up doing was using another
feature within a sure which is the
ability to use multiple languages
against it so not only can use some of
the standard languages that microsoft
supports under nazi sharp some of the
other ones but also there's a bunch of
other support for you know node there's
the Python Java and so within this we
actually use some of the support for
Python and Dennis asked earlier you know
folks use it if Python one of the things
that's really useful within a sure and
within the product team that they've
done is have native support within
visual studio so have a real ide for
actually creating python applications
and so you get advantage of a lot of the
ide and the processing that you have in
there to take advantage of for doing not
only intellisense and some of the other
work that you most people enjoy in a in
a ide but also debugging and some of the
other analysis in there so we end up
with in there there's also the support
for all the traditional technical and
scientific computing support so if you
do Python type computing take a look at
the PT BS and that's actually it will
install within visual studio very useful
the other piece at den Dennis also
mentioned was kind of the ipython
notebook so that's an extension there
you can create these and they run within
a virtual machine and why that's kind of
useful is that obviously you can get
this up and running have it running in
your own vm and do and control that
however you want
so like as I mentioned earlier the
virtual machines support not only
windows but also Linux and you can do it
very have a virtual networking to
actually interact with it directly as
well from a local site so you can have
these running remotely and they look
like they're on your own network so it's
a real kind of benefit in there there's
two different type of depos you can pull
virtual machines from so the one that's
built into Asher allows you to pull in a
bunch of standard ones there's windows
server in there there's a whole bunch of
Linux distributions sequel server
distributions and then there's a
community-based one from one of our
subsidiaries called open tech and they
have a whole bunch of also virtual
machines that people have done from the
community packages them up and make
those available so if you had a specific
image or that you wanted to actually the
community to use so they could just get
up and running or your own application
this would be a place you could put it
get it up running very quickly so
somebody could use it rather than having
to do all the installation so there we
see couple of these that are on there
already
and then the also the azure sdk for
python is available too so we have
native support within there so one of
the things like I mentioned Asher has a
whole bunch of these building blocks in
there I only showed in the the weather
one us using a few of those but there's
a whole bunch more both on logging in
the identity piece mobile services the
load balancing storage and on goes down
the line and a lot of it's also
available like through the SDK and all
the tools and libraries are available as
open source on github so you can go out
there and actually see what's going on
within the code so let's keep over there
so I just wanted to kind of cover some
of the projects we've been kind of
working on there this from where we
started with the motifs Azure to where
we are now it's so much easier to even
get up and running we can deploy an
application in no time you can create a
website copy it up and be up and running
within minutes and so I have a whole
bunch of links here to some of the
resources both for students there's the
research resources up for how to get
some of the cycles and some of those the
information on Python the depot's prayer
for faculty if you're interested in
using any of this in your courses
there's grants that are available from
another part of our organization that
will give grants for six months to a
year for you and your students to
actually execute code on and to run so
think of it as a virtual lab you don't
actually have to worry about those
machines locally so very kind of useful
the other thing you may be may be
interested in as well is put in at the
bottom as well there's we have the
flyers out there if you're interested in
from a faculty point of view there's a
kind of our RSVP it's down there if you
send it to Azure you at Microsoft with
your information they're collecting a
community of folks on the on the faculty
side and so folks that want to share
information and then there's another
plan flip that has a whole bunch of the
information on an azure as well how to
get up and running
and what resources are there for both
the students and faculty so just you
know as Tony had on his slide earlier
today the future is you know coming type
1 it's actually now the clouds are out
there from us from other vendors and in
our case asier it's available we'd
really love to see people test it out
and use it especially on the science
side and myself Dennis and the rest of
my team are always willing to try to
help and see where we can help on the
science side to help you solve some of
those problems that you know people run
into not only using technology but to
solve the science because that's I think
which is really key so thank you and I
finished early so you guys will be able
to get out there for cocktails early
enough thanks it sounds like you're
making a lot of work for Dennis and I of
course do we have other questions from
the room yeah no no they're not believe
I told with cocktails and demo fest yes
I have a question maybe it's not just
for you is maybe for marketing approach
to big data in fourth paradigm by Jim
Gray is clearly stated that in big data
cases is the process that should go to
the data are not data that go to process
but I'm seeing that the almost all
products are fetching data not going to
data you're right one of the challenges
well I should say yes and now there's
two pieces kind of in there so it's very
hard to break people's habits about
compute and data and pulling the
computing the data to the computer so
Jim's whole thinking on this was you
need to move the compute to the data
because at a certain point you can't
move enough data to actually get it
there right the bandwidth just isn't
there yes we can keep increasing
bandwidth problem is the data just keeps
increasing right so it's a you need to
balance it so the whole idea was you
moving
the computations to their as I mentioned
it's very hard to break people's mindset
and how they usually go about processing
it so everyone is used to this model and
so that's why you see it reflected in a
lot of the functionality that what the
cloud does enable you to do is actually
have a pot a data that is actually more
local than you going and pulling it all
the way across the world so you are
essentially a lot of times in this case
moving it at least into the same
building to do the processing and not
moving it moving the big bit of data
down locally and processing sitting on a
smaller machine there are some other
technologies though they do some of the
more than data on the and the
computation together one of them didn't
talk about here today but we've done
when some other project is some stream
processing one whereas the streams
coming in you're actually continually
processing on it as it's essentially
going by and so there's a sequel stream
insight project product that we have we
use that at a project in USC looking at
traffic data and so it allows you to do
the analysis more instantly because
you're looking at this data one of the
key things and I've been looking for
some collaborators to try this out
because I actually do believe this is
would be useful it doesn't need to just
be on real-time streaming data it can be
on historical information and data so
you can actually have it process put it
put the the box the node as close to the
disk as possible and actually have a
process on that much the way say at an
advanced grep might work and you're
executing it but dealing with into the
spatial and time constraint type way and
so your latency is from the disk to the
computation which could be right next
week and not across the network or
anything so it could actually
essentially be the front end there
because in your painting the close-up
application let's say if there are some
multiple process leading same data so
multiple are you talking about in that
case and the stream insight or in the
case of just
Oh so so in that case what's the benefit
is it most of the time everyone's
caching anyways on that in the cloud
side so you're actually speeding up the
for the next person yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>