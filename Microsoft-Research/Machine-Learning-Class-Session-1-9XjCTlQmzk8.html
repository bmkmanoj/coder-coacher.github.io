<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning Class (Session #1) | Coder Coacher - Coaching Coders</title><meta content="Machine Learning Class (Session #1) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning Class (Session #1)</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9XjCTlQmzk8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay let's begin this is the machine
learning class the plan for the week is
to teach everybody a machine learning
and the first question of course is what
is machine learning
interesting
Newt okay okay so if you go and you look
at what a lot of modern machines look
like when they're actually running you
look at what's in their RAM you see that
there's some portion that's compiled
interpreted code this importunate that
is actually just derived from data this
is really what machine learning is about
it's about trying to use data to make
useful predictions okay
the amount of RAM being used for data
derived predictions versus compiled or
interpreted varies greatly from machine
to machine but there are some machines
where a large fraction is is in fact
data derived okay so now it's useful to
think about examples of machine learning
and there's quite a few examples many of
them at Microsoft so a search engine
results are driven by machine learning
to a large extent interesting thing is I
understand that Google search results
are not quite as driven as Bing is
there's a guy who's pulling a Paul
Bunyan
and and eventually I expect he will
break connect uses machine learning to
recognize poses of people that's pretty
cool spam prediction is the example of
machine learning that you can tell
everybody including your mother
automated arbitrage aging when you when
you see what's happening on Wall Street
there's a lot of machine learning going
on there and then I have a friend who's
actually working for the Obama campaign
he's tried to figure out how to best
persuade people to vote for Obama with
machine learning
ok so these are just examples of machine
learning it's helpful to understand sort
of what are the characteristics of
machine learning problems and there's
several sort of basic paradigms of
machine learning which exist right now
the one that we're going to focus for
most of the class on is supervised
machine learning okay so in supervised
the machine learning what happens is you
pick a problem where where you can't
program the solution very easily so
where the you fail doo-doo case
complexity so an example of this would
be spam prediction right so in spam
prediction you could of course program
something which says that if the email
has the word viagra then then reject it
and turn it into spam but sometimes
emails with the word viagra actually are
not spam and sometimes there's TLS
instead of viagra and if you start
thinking about this carefully what
happens is you come up with exceptions
and then you come up with exceptions to
the exceptions and the exceptions to the
exceptions the exceptions and you just
you go crazy trying to program these
things so machine learning is what can
help in these kind of situations
if there are more straightforward
solutions then of course you should use
them but but when you start getting this
incredible case complexity and no
obvious program can actually do what you
want to do then it's time to start
thinking about a data-driven solution so
once you have this problem you need to
in in classical supervised machine
learning you hire people to label input
and output example pairs so we're gonna
be trying to learn a function which
predicts given the email is this spam or
not right and the classical solution
would be that you hire people to label
things as spam or not and then you use
your learning algorithm to find this
function which you have to input to the
output
okay and there's lots of different
learning algorithms so we'll be
discussing several of them but in
general all they're doing is trying to
find this mapping from input to output
and then of course you apply your learn
function to solve a problem wherever it
happens to exist are there questions how
can you assure the quality of labeling
that's a tricky question in many
situations often people often think it
labeled multiple times and you get a
sense of the quality of the laborer this
comes up to a much greater degree the
lower the expertise to the laborer right
so in particular if you're using
Mechanical Turk to label things which is
fairly common nowadays there's a lot of
tricks people use to try to weed out the
bad labelers and keep the good labelers
and and so forth okay so this is
classical supervised machine learning
this is sort of what existed even 20
years ago it's been slowly improving
over time and and it gets routinely
applied in many situations it's also
interactive machine learning so
interactive machine learning the
learning algorithm starts controlling
which inputs it learns from and that
introduces another level of complexity
however it can do some very cool things
so for example when you start hiring
labelers it starts costing money to
label things and then if you can if
you're learning album can reduce the
number of things you need to label by a
factor of two or a factor of ten that
can greatly improve your budget right so
there are these systems for automated
labeling of various tasks
so you HR s is the internal Microsoft
one Mechanical Turk is from Amazon both
in Seattle interesting and then the
other thing which is happened is there's
a large number of recorded computer
mediated actions so what I mean by this
is trades on an exchange or search
sessions or here I displayed a news
story and somebody clicked on it and
read it and so this is this is
essentially free data free labels as
long as you can actually store the data
this is not quite labeling the correct
output given the input and this is this
is a tricky process here compared to the
standard supervised learning but this is
cheaper and potentially more powerful
and this is much cheaper and potentially
much more powerful because because you
can express a lot more a lot more
complex function with the data that you
gather from all of these recorded
computer mediated actions another
example of this which comes up and which
is very relevant to OSD is trying to
find the ad which most interest people
if you find the AdWords ad which most
interests people then then you know a
lot of money gets made okay so an
interactive machine learning this is a
newer area it's been developing over the
last I would say decade maybe decade and
a half is typically much more data
involved particularly for number two but
there's a more tenuous connection
between the data and the solution it's
not like for this input I want that
output for this input I want that output
it's more like for this input that
output kind of worked
okay so that's interactive machine
learning and there's another branch of
machine learning which is model-based
machine learning right so the goal here
is typically to model how the world
behaves so the last day is the modeling
day and this is this is a huge subject
in general this is very broad the basics
will be covered in this class is
particularly useful I think for
hypothesis testing so if you have a
belief about how the world behaves not
necessarily a complete belief but you
believe there's some structure to how
the world behaves this is dependent upon
that and it's independent of this given
that then you can you can create
graphical models you can test these and
if you are correcting your beliefs about
the world you can get very good
predictive ability right this is also
the tool which gets used when you don't
know what to do right so if you if you
have some data and you're like hmm
what's the input what's the output I
have no idea then often these modeling
based approaches are used to try to
summarize the data in some manner which
is human understandable or which can
then be fed into a supervised learning
algorithm for later predictions okay so
these are sort of three branches of
machine learning now I will I'll discuss
the class itself so Vijay right after me
it's going to talk about statistics for
machine learning the first thing that
you need to know with machine learning
is that you have succeeded all right so
you need to be able to say I have
succeeded right and that's essentially
what Vijay's talk is about so an
understanding of basic statistics is
necessary to know that you have
succeeded because machine learning is
kind of a so with constraint programming
you have succeeded if you have found the
Optima which is you can prove
when you you're you're at some corner of
some optimization space with machine
learning you have succeeded when you
have a good test error performance when
you can predict things well and defining
well is something which is which is
inherently noisy because some problems
are just impossible to solve all the
time perfectly right is this email spam
or not well maybe it depends upon the
person who's actually receiving it and
maybe you don't have information about
that person which would allow the
algorithm to determine whether it's
going to be flagged as spam and that
means that that there is some
fundamental noise you can only achieve a
small area up to some some limit which
is going to be dependent upon the
problem that means it's noisy and that
means that you need to understand
statistics so that you can understand
how confident to be in your solution
results the simplest version of learning
the simplest representation is linear
learning so mirror is going to cover
this in great detail and then we're
going to finish up with two tools later
in the day so math is going to talk
about linear learning in TLC and I'm
going to talk about linear learning with
VW okay so at the end of the day you
should understand how you've succeeded
understanding what success looks like in
machine learning and you should get some
familiarity with basic tools so you
should be able to just apply machine
learning this very simple versions of
machine learning on your own
okay the second day is about a new
representation decision trees decision
trees get used in the core search engine
and Microsoft and in general Microsoft
has a very highly optimized decision
tree package which makes it an
attractive algorithm to actually use so
rich is going to talk about decision
trees and then Vijay and I are going to
follow up with some more details when
you are doing machine learning it's
often very helpful to have a lot of
different metrics on your learning
algorithm what the learning algorithm
does
is often a little bit difficult to
understand and so there's a lot of
different ways to evaluate what it's
doing to help you to debug if something
is going wrong and then I'm going to
talk about different learning problem
types when you run into machine learning
problems almost always they're not
exactly the simplest machine they don't
fit the exact setting that you that is a
simplest and so it's very helpful to
understand sort of the known variations
on the basic setting and how to address
them and then Misha and Matt are going
to do a full tutorial on TLC right so at
this point at this point you should know
how to do machine learning fairly well
okay so then the next three days are
sort of on an advanced topics the third
day Wednesday is about making things go
fast in general a lot of machine
learning is about optimizing stuff and
there's a lot of tricks to optimization
which can make an enormous difference in
your life I mean it can be like I wait a
day for this out of them to finish or I
wait ten seconds so understanding
various tricks to make things go really
fast is very helpful for you so Alec
will talk about online learning I'll
talk about feature hashing so this is a
technique to improve speed of
optimization this is a technique to
compress the size of your learned
predictor is the Alec will then talk
about parallel learning this is again to
speed things up then we have Jian Feng
and Misha and Matt talk about learning
algorithms that can run directly on
cosmos cosmos is OSDs
internal combined compute and data
cluster and so it's very desirable to
have learning algorithms which run
directly on cosmos and then the last
talk will be about clustering this
doesn't quite fit that rest of the day
but it was it was what we could fit in
so clustering is a handy technique it
actually fits into the modeling category
typically you just you find clusters in
the data and then that helps you
understand what the data how that is
useful and also helps you extract
features which is actually the cluster
belong to individual things that you can
then feed into other learning algorithms
okay so the fourth day is about
interactive machine learning Miro
is going to talk about learning and
evaluation in this setting
this is settings where you're
interacting with a user and you're just
getting feedback from the data traces
that you actually observe from these
interactions then I'll talk about
gathering exploration data and then
Daniel will talk about active learning
Daniel's thesis was an active learning
and it's quite a good one okay
the last day is about modeling Chris
Bishop and John Braun skill well
introduce the basic modeling setting and
approach John is going to talk about in
Fernet which is one of the core machine
learning tools at Microsoft and then
lead dange is going to talk about deep
learning which is a very fun topic it
lets you do things which we don't know
how to do with other approaches like
some of the really advanced speech
recognition applications okay so this is
a typical class schedule there's a few
variations from day to day the thing
which is okay so on our break we have
some refreshments
assuming we didn't run out of them this
morning and for lunch the Commons is
recommended it's nearby there's also a
cafeteria right over there so the
Commons is that way about two buildings
over maybe three buildings over and the
cafeteria is right inside of this
building and then we finish up at 4:30
all right so this is a picture we're
here I'm pointing that way okay so I
want to give you a sense of other
machine learning resources at Microsoft
in general Microsoft Research is one of
the great centers of machine learning
and there's a large number of experts in
machine learning at Microsoft Research
so if you have questions about specific
things you should post them on the
machine learning mailing list and that
can help you connect with the people at
Microsoft Research there's many
applications that we've seen learning
OSD has many people who use machine
learning all the time
STB and ie B are both using machine
learning and increasing the month
they're using it and general I think
it's happening in the rest of company as
well but these are the areas that I'm
most aware of okay
there's also some previous course
material which is on the SharePoint site
so there's pointers to it on the
SharePoint site which you can use to
fill in additional things everybody this
is it this is an academic field so
everybody has their own view on sort of
what is the most important and and so
forth so for different views you can go
here and each of these are are good
talks good good courses well where you
will learn some more okay last thing is
tools so TLC is a tool which is
developed here in Redmond by Matt and
Misha BW is a tool which I worked on at
yeah-oh research it's an open source
project and so it is quite possible to
use it inside of Microsoft Owl queuing
is a linear learner in cosmos and
fernette is a compiler from model based
learning so each of these are these are
sort of that what I think of as the core
tools at Microsoft there may be more
which I'll learn about but
these are the ones that are going to be
covered in this class okay so this is a
class not all of us are teaching all the
time so I think the most important thing
is that you should ask questions if you
can try out the tools in real time and
these these lectures are being recorded
on ResNet so if you if it can be hard to
go do things in real time but you can
easily pause the reson it and continue
and and check things out more thoroughly
yourself later distractions are
definitely interesting if you have
questions and it's more convenient to
ask offline then use the ML October
milling list I'll be watching it and I'm
sure many other people as well so your
questions can be answered
other questions right now
okay let's begin
all right thanks Abby thanks John
so in this session will primarily focus
on understanding some basic concepts and
probability and statistics especially as
they relate to machine learning the the
goal here is not introduce anything very
deep or mathematical right of the bat
but hopefully to gain an intuitive
understanding of some of the basic
concepts there are lots of rigorous
definitions that will follow in
subsequent classes but in this session
what I really want us to go where
there's some appreciation or some deep
concepts some some very basic concepts
which are taken for granted in many of
the other sessions so some of these are
really really cool results to know
irrespective of whether your learning
machine learning or not and I'm sure you
must have seen them in a lot of your
earlier classes basic classes in
statistics or probability earlier as
well in which case think of this as a
refresher and more importantly there are
two aspects in machine learning one as
you know is a lot of theory behind
machine learning right I mean how do
humans learn what is the basic structure
of the learning patterns how does
learning happen in real life all those
nice cool things but also very often
when you start applying these techniques
to real problems in the in the industry
you are dealing with practical data you
are often dealing with noisy data you're
dealing with incomplete data you are
dealing with partially label data and a
lot of host of other problems in these
cases it's very very important to
understand what are some of the
statistics of the data how does the data
look like right before you start
applying the techniques it's very
important to analyze and explore and
gain an intuitive understanding of the
data some of the basic properties of the
data before you start throwing in
algorithms so this class will primarily
hopefully give you an appreciation of
what are some of the basic tools and
techniques that you can look at for
understanding for gaining an intuitive
understanding
of the data so the the basic outline is
here we'll cover some basic probability
concepts go into some statistics and
then I will take you to through a gentle
introduction to machine learning how
does the typical machine learning
process look like what to look out for
etc this is not about teaching you to
drive but this is hopefully to teach you
not to create an accident so it's less
about the machine learning techniques
per se but more about the guardrails and
some of the places where you need to be
aware of when you apply machine learning
to real-world problems all right so
let's start with reviewing some basic
concepts and probability
we'll keep hearing the mention of the
term random variables so we'll quickly
go over what random variables are how
they are different from other
deterministic variables cover some basic
concepts of probability and probability
distributions joint probability
conditional probability Bayes theorem
Exeter all right so what is the random
variable so it's a very simple
definition basically it's a result of
any experiment that can eat one out of
many possible outcomes if you have a
variable that says x equals five then
you know exactly what is the value of x
right but if I tell you hey I'm going to
flip a coin and the outcome of that is
my variable guess what now it can take
two values it could be a head so it
could be a pain right so now if I want
to quantify what is the outcome of a
coin toss I call that a random variable
because now it can take one out of many
possible values and when you do this
experiment repeatedly you're going to
get some or all of these values so it's
any variable whose value varies due to
chance another very simple example of a
random variable is the outcome of
rolling a die right so if you just roll
a very fair die
that has six phases in it you're going
to get one of the following outcomes
rate it could be any number from one
through six
the the face of the diet that is showing
up could be any one of these six numbers
now if it is a fair die you would expect
each of these outcomes to occur equally
likely so this is a very simple random
variable now let's go to there now
remember suppose if I ask another
question suppose if I roll a pair of die
then what is the total what is the value
of the total now remember here again
each die by itself is a very simple
random variable and your Sun is the
combination of these two random
variables which is again another random
variable but here the sum can only take
on eleven possible values right so it
can it can take on values from 2 through
12 however notice here that not all the
values are going to be equally likely
some of the values for example 6 is much
more likely to occur the extreme values
like 2 or 12 occur much less frequently
so unlike in the first example where if
you just roll a single die where all the
six possible outcomes are equally likely
when you look at the total that you get
from rolling a pair of die the outcomes
are not all equally likely there is a
certain variation in how often they like
they they they show up does that give a
given a reasonable idea of what a random
is what a random variable is okay so
there are basically two different types
of random variables one is a very
discrete random variable where X just
takes values from a very countable set
right so the possible values are all
countable you can start with you can
start rolling up your fingers and start
counting them for example in the case of
a coin toss or when you roll a die you
can actually enumerate all the different
possible outcomes note however here that
just because it is countable doesn't
mean that you can finish counting
so the size of this countable set can be
finite which in the case of the coin
toss are the die roll are all things
that you can finish within a few seconds
you can keep animating all the possible
outcomes in a few seconds they could
also be countably infinite for example
if I turn if I ask you a question how
many tosses how many coin tosses do you
need till the first tail occurs guess
what you could flip a coin maybe the
first time it occurs good for you you
flip it two times and it occurs the
second time maybe the first time it fell
heads on the second time it fell tails
but it's equally likely that both of the
coin tosses came up as heads so you can
keep toying I mean as you can see here
you can keep potentially tossing the
coin infinite number of times and you
might never get a tail now the
probability of such an event is very
very small but potentially you can keep
tossing the coin in finite number of
times and you will never end up and you
will never see a tail so this is an
example of a set that is countably
infinite you can keep counting how many
times it falls but you will never you're
the probability that it's going to end
is is very small
it just a syntactically goes to be yeah
so the question is if you toss it in
finite number of times the probability
goes to zero yes I syntactically yes but
there is still a very very very small
finite non-vanishing probability when it
is it is definitely decreasing but it
only hits zero theoretically at infinity
so this is a discrete random variable
and as you can see here it can be either
finite or countably infinite now every
finite random variable is almost by
definition discrete right because you
can just enumerate them in you can just
enumerate all the possible outcomes you
can just count them up and it is finite
so all the possible outcomes can be
discretized so this is the discrete
random variable the other type of random
variable it's not that it is in discrete
it is just that it is continuous so here
the random variable takes values from an
uncountable set so basically for example
if I if I go and ask okay what is the
distribution I mean what are all the all
the heights of some animals in a given
region say what are the lengths of all
the fish in in Lake Washington for
example or if I ask a question like what
is the life time of a light bulb it's
going to be a real number it's going to
be some number between 0 and potentially
in finite but you can take on any
possible real value between these two
extremes things like time to failure of
a hard disk for example all these sorts
of variables are continuous random
variables and these continuous random
variables take on an infinite number of
values now these values can all be
bounded within a given range so for
example all possible values between the
number 0 &amp;amp; 1 there are going to be
infinite such numbers between 0 &amp;amp; 1 but
the extremes are still bounded
or it could be it could take values over
an unbounded range for example all the
numbers greater than one right so it
could just be all the real numbers
between that is the type of there should
be greater than zero so between zero and
and infinity for example something like
the time to failure of a hard disk is an
example of unbounded is an example of a
continuous random variable that has an
unbounded range so now with the
definition so I mean the having some
rough idea about what these random
variables are discrete continuous finite
countably infinite etcetera let's go
let's move on to the next possible
question you might ask out of these
random variables which is how likely is
it to see now that you have said that a
random variable takes values from a
given range your given set of possible
values you can then go back and ask okay
how likely is it that this random
variable takes one value as opposed to
another value how likely is it that this
random variable takes the value for
example if I if I if I toss a coin how
likely that is it that it takes the
value heads go ahead
in southern Africa
theoretically it could be
multi-dimensional I believe so yeah so
the question is can random variables be
composite the composite is say if it's a
sum of two random variables or fits a
function of many random variables than
that by itself is a is another random
variable yeah it could be a vector sure
higher order variations are possible and
I'll actually touch a little bit about
multivariate distributions later on yes
it's certainly possible okay so
probability is nothing but a
quantitative definition of this question
how I a quantitative answer to this
question how likely is it to see one
value from among all the other from
among all the possible values of a given
random variable so you can also turn it
and and ask a related question so you
can also frame it in a in a different
manner now you mentioned that the random
variable is an outcome of an experiment
so the question is if I if I keep
repeating the experiment then what
fraction of all the experiments will
have a given outcome now this is what is
addressed by by probability so what a
probability says is every outcome every
possible outcome in the set of possible
outcomes is associated with a given
number a number between zero and one and
for example when you when you roll a die
each of the possible outcomes from 1
through 6 is associated with a number
between 0 and 1 in a fair die each of
the possible outcomes can occur
one-sixth of your time right so if you
if you if you do the experiment say 6
million times if you keep flipping a
coin 6 million times then 1 million
times you will expect to see hey to see
one sorry it's not a coin it's a die if
you roll a die six million times then 1
million times you will expect to see 1 1
million times you'll expect to see 2
approximately so probability is just the
quantitative definition is just a
quantitative answer to the
question how likely is a given value
going to occur for a for a random video
so there are a couple of very basic
axioms I'm sure you must have all seen
these multiple times over they say the
the probability it's a it's a bounded
number between 0 &amp;amp; 1 right and the sum
of all the probabilities of all possible
outcomes within this set is 1 this is
just a way of saying that if you do an
experiment then one of the possible
outcomes has to occur right and the set
of all possible outcomes is what you
have enumerated as the possible range of
values of this random variable now for
discrete random variables each of the
possible outcomes has an Associated
probability because it is a countable
set so if you go through say a dice roll
then each of the possible six outcomes
has one real number associated with it
in a fair die it's a it's about 1/6
however what does it mean by mean to
define a define a probability for a
random variable that takes continuous
values suppose if a random variable can
take any value between 0 &amp;amp; 1
what does it mean by saying that hit
this random variable has a probability
point 1 of taking the value of point 2 3
4 5 6 7 8 9 3 2 X doesn't really make
much sense so in this case for
continuous random variables you define
probability over a range of the possible
outcomes over a range of the continuous
values that this random variable can
take so you define hey now I can take
this I can turn the same problem around
and say between what is the probability
that this random variable takes the
value between 1 point 1 and 1 point 1 5
then you can start defining the notion
of how likely that if I do an experiment
my outcome is going to lie within these
two ranges 1.1 and 1.15 right so this is
the
the slight difference between defining
probability for discrete random
variables and continuous random
variables whereas for discrete random
variables every possible outcome has an
Associated probability value for
continuous random variables you only
define probability over a range of
values of the continuous variable okay
now let's go into probability
distributions the thing to just keep in
mind here is there are different types
of distributions that at a very high
level will just be called out as
probability distributions and exactly
which of the probability distributions
that I will mention here I mean is that
I will describe here is referred to
depends on the context so we will
loosely talk about the term probability
distributions but there are actually
different types of distributions that
you need to be aware of and it's
important to know that depending on the
context different definition mean
different types referred to now a
probability distribution is just a
function that describes how the
probability of a random variable takes a
given value now for discrete variables I
say as we saw earlier you can count all
the possible occurrences of the random
variable so what is referred to as a
distribution is really what is called as
a probability mass function here it's
just the probability that this random
variable takes one of the possible
discrete values that that it is it is
allowed to take so let's just take a
simple function like this right so here
X is X is a discrete random variable in
this instance it takes five possible
values 1 3 4 5 &amp;amp; 7 and in each of these
points in each of these possible
discrete outcomes there is a probability
associated that that tells you if you if
you do an experiment then maybe 40% of
the time you're likely to get the value
120 percent of the time you are likely
to get the value 3
10% of the time you can get the value
for and 10% of the time you get the
value 5 the value 6 can never occur for
this random variable so that's why it's
a zero and twenty percent of the time
it's likely to be seven so here you have
just defined your math for each possible
your probability mass for each possible
outcome of this discrete random variable
the sorry did you have a question there
okay the so another type of distribution
yes this is something you keep hearing
often is this notion called probability
density function so when people also
call this as the probability
distribution functions very often but
it's the probability density function
this is what is referred when people
typically talk about continuous random
variables so here the if you want to
know what the probability is then you
only can define it over a range of
possible values of the continuous random
variable but for a given point you can
still associate a function which when
integrated over this range or when
summed over all possible values within
this range gives you the probability
within that range of this random
variable falling within that range and
unlike probability which can only take
on values between 0 &amp;amp; 1
the probability density function can
take on any non-negative value so if you
want to say here is the probability that
it takes I mean what is the probability
that this continuous random variable
takes values between X 1 and X 2 then
you integrate the PDF it's also called
it's it's commonly called as the PDF the
probability density function over the
range X 1 through X 2 now it's important
to note here is that I mean this is an
example of a PDF of a very commonly
occurring PDF where X here is a
continuous random variable that takes on
all possible values here in this case
it's all non-negative values
and this function which has a peak and
dies down very smoothly is an example of
a probability density function now to
know what is the probability that this
random variable takes on values between
say 5 and 10 you integrate this function
between the values 5 through 10 and that
tells you what is the probability that
this continuous random variable takes a
value between 5 and 10 between this
range of values 5 and 10
this is that is if we want to come do
the expectation of f of X over this
range so here you are just looking at
the probability not the expected value
of the function we have not even come to
expectations so you can really define
this strictly you can only define this
if the integral is still finite then you
can still go to infinite but this
integral of P of X DX can still be fine
so you can you typically normalize the
PDF right and and you will also
normalize this by the integral from 0 to
infinity to keep that within ya
PDF values or no PDF values can be
between 0 to infinity but you normalize
this from 0 to infinity you divided by 0
to infinity to bring it to below 2 to 0
the total value of probability contained
under this curve is the value that you
normalize this body and typically that
that that gets absorbed within this p of
x and most the most common distribution
so that's why I didn't really call that
I mean those are all Tweedles that I
really didn't want to get into right so
so another notion of a distribution that
that frequently comes about when you
talk about probability distributions is
this cumulative distribution so remember
till now we talked about what is the
probability that something takes the
value
I mean takes a given specific value in
the case of a discrete random variable
or a value within a given range in the
case of a continuous random variable the
cumulative distribution is the
probability that the random variable
takes some value that is either less
than or equal to a given value so if I
go back to this previous PDF here and I
ask you what is the cumulative
distribution at five then it is the
probability of this entire curve between
minus infinity to five now in this a in
this example there is no extension of
this curve to non-negative values but
technically there's nothing that
prevents you from having a probability
distribution where the random variable
takes on negative values so for the same
distribution here the red curve shows
the cumulative distribution function in
the case of a normalized PDF the red
curve as you can see here starts off at
zero at very low values of the random
variable and nicely settles down to
around 1 as you go to higher and higher
values of the random variable so all
that the CDF tells you is that what is
the probability contained when the value
of the random variable is either at a
given value or anything lower than that
value so it is just the the integral of
the of the of the probability function
till from from minus infinity to X note
here that the CD of itself is a function
of X so the CDF is actually a function
and sometimes you can even if the CDF is
a closed analytical form it might be
easier to work with the CDF instead of
the of the PDF because especially if the
PDF has some funny behavior like going
to infinity over a small at some point
etc then the CDF might be a more
analytically tractable function okay so
before I go to multivariate probability
just a quick point so as John mentioned
we typically take a break at 10:30 but
today I believe we will go till 10:45
right so this session will go until
about 10:45 will take a half an hour
break come back at 11:15 and go till
12:30 now so far we all talked about
just the distributions the probability
probability distributions continuous
sorry random variables of just one
dimension but technically you can extend
it to all these concepts can be easily
explained it to just multiple random
variables so for example you can ask the
question what is the probability that
the random variable X takes a value
between x1 and x2 and another random
variable Y takes the value between y1
and y2 for example in the case of a coin
toss you can ask what is the probability
that the first die takes a value between
2 &amp;amp; 4 and the second value and the
second die takes the value between say 4
&amp;amp; 6 so you can extend this concept of
random variables probability all the
distributions to multivariate cases as
well so in that case the the integrals
basically become higher dimensional
integrals so in this bivariate
distributions where you have two random
variables x and y the probability that x
y that the variable x is between
one and X two and the probability that
the variable Y is between y 1 and y 2 is
just this integral of the joint
probability density here when you go to
higher order very good higher
dimensional distributions they typically
refer to them as joint distributions so
P of X comma Y DX dy where X now goes
over X 1 and X 2 and Y ranges from y1 to
y2 okay so the first few slides are just
to get some of the concepts out of the
way and hopefully we'll do a lot more
examples as we go forward now when you
are given a higher-order distributions
when you have a multivariate
distribution you can still ask okay but
you have given me this joint
distribution of two variables x and y
how can i compute just univariate
distribution just there with the
distribution of P of X itself so you can
easily do that by basically integrating
out all the possible values of your
other other dimensions of your other
random variables for example if you want
to look at P of X if you want to compute
P of X from the joint distribution P of
X comma Y you integrate over all
possible values of Y the the joint
distribution and when you do this
typically P of X you keep hearing the
term marginal distribution so here the
marginal is the joint integrated over
all the other dimensions all possible
values of your other dimensions now for
just a point to note here I have shown
some integrals that are relevant for
continuous distributions but for
discrete variables basically replace all
these integrals by by sums right by sum
over all the possible values of the
other random variables so you can even
have a distribution where X is a
continuous random variable and Y is a
discrete random variable in which case
if you want to do this marginal
probability density of X you will
replace this integral over Y by a sum
over all possible values all possible
discrete values or
why okay now it starts to get a little
bit more interesting so far we've just
looked at individual distributions
probabilities etcetera but when you
bring in more than one random variable
into the picture we can start asking
questions like what is the probability
that one random variable takes a given
value or or a continuous or within a
within a range of values in the case of
a continuous variable when the other
random variable has given has taken a
specific value right the types of
questions that you can show so this is
referred to as a conditional probability
so in this example you are asking what
is the probability distribution what is
the probability of x given that another
random variable Y has taken a specific
value this is called the conditional
probability of x given Y and what why
this gets very interesting is your lord
of the machine learning problems boil
down to asking the questions suppose
here is the state of the universe here
is the state of the system I observe
tell me what will be the outcome many of
the ML types of especially the
applications of ML boil down to asking
the questions here is my state of the
system for example in the case of say if
you want to predict if a given
transaction is say fraudulent or not
right you say you have done a credit
card transaction and you want to say if
it's if it's a fraud if somebody wants
to predict if it is fraudulent or not
they're going to ask the question look
here is the behavior of this credit card
now tell me if it is fraudulent or not
so the questions become very conditional
you're you're asking given that this has
happened now tell me what is the
probability that this is my outcome
right can you learn these these these
relationships so this conditional
probability is typically used to
represent these given that type of
questions for example one one simple
question you can ask is okay given that
the lawn is wet what is the probability
that it had rained last night
well the lawn could be wet because maybe
the the sprinkler was was on last night
or maybe it really rained I don't know
but you can still ask the question right
what is the probability that it had
rained last night
given that the lawn is wet so the fact
that your lawn is wet is an observed
variable you have fixed the value of
that random variable there is no more
uncertainty there the lawn could have
been dry it could have been wet but you
have gone out and seen that the lawn is
wet now you're turning around and asking
the question given that I have seen that
the lawn is wet what is the probability
that it had rained last night
another simple question is given that it
had rained earlier what is the
probability that there will be a rainbow
today right so all these sorts of
questions like the given that questions
where you are saying one random variable
has taken a value has has a specific
value I have done an experiment and I
have read and I have seen that this
random variable has taken a specific
value or a specific outcome now tell me
what is the distribution of the
variables of another random variable so
this is called conditional probability
and the reason this guy so so just
quickly cover over this concept called
the Bayes theorem it I won't go to too
deeply into it but I guess this will be
covered in a lot more detail on the last
day when when Chris Bishop and John
Brown scale talked about modeling and an
inference so if you take two random
variables D and H and a later come down
to why I chose these this this notation
then suppose say this red circle tells
you all the possible values that the
variable D can take and the green circle
tells you all the possible values that H
can take and Brown here the intersection
is basically where it's a it's a place
where D takes on a specific value and H
also takes on is some value within that
that that Brown circle
so red is basically all possible values
of D H is all possible values of green
Yas sorry Green is all possible values
of H and brown is the intersection
region so now you can ask what is the
probability that D will occur given that
H has already occurred so as you can see
here it's just the ratio of the brown
area over the green area right so
probability of DN h divided by the
probability of h happening by itself now
you can also ask the other question what
is the probability that of H given that
B has occurred so here it's again the
overlap area the brown area but now you
are asking what is the probability the
array of H will occur given that D has
already occurred right so it is you are
your total possible values of D is in
the red area P of D
they are possible outcomes that that
that red area is the set of all possible
outcomes of the variable D so you can
just combine these two and say the
probability thereof the H given D is the
probability of D given H times the
probability of H given the probability
of D now this is a little bit abstract
right now but really this what this this
equation is is called Bayes theorem and
I'll just tell you why it's very
important
h-here think about them as yeah your
hypothesis suppose somebody has given
you some data and you want to see is my
hypothesis so my my my hypothesis is
that this card is this this transaction
this credit card transaction is a
fraudulent right so what you have is the
data which is all the transaction
streams the history of all the
transactions that has occurred on this
on this credit card for example now you
are asking given this data what is the
probability that my hypothesis which in
this case is that this car this card has
gone fraudulent is correct so
probability of hypothesis given data is
just you're just turning it around and
say probability of data given the
hypothesis times the probability of the
hypothesis itself divided by the
probability of data occurring by itself
okay I know this seems to be a little
bit confusing you see a lot of people
just staring yeah so you can you can
replace that with the sum over all
possible hypothesis right of this data
being generated by all possible
hypotheses so this is also frequently
put as suppose so what you have is your
data and you try to ask what is the
probability of this hypothesis being
correct so what you have is the data and
now you are asking what is the
probability of this hypothesis being
correct that's frequently refer to as
the posterior probability its posterior
because you are asking these questions
after you have had the data after you
have collected the data and now you are
asking I mean you first before I collect
the data I don't have any intuitions
about which hypothesis could be right or
wrong I might have some expectations
saying here look frauds are inherently a
lot less likely to occur
so maybe my probability of this card
being fraudulent is is not very high but
then you actually collect the data and
then you can ask the same question again
now tell me how has my hypothesis has
changed may how is the probability of my
hypothesis has changed given that I have
observed this data so that's called the
posterior probability and the
probability of data given the hypothesis
is frequently referred to as likelihood
so here how likely are you to have
generated this data given that your
underlying hypothesis was correct the
probability of hypothesis is also called
the the prior the prior probability
which for example in this credit card
fraudulent transaction example is what
is my expected rate of fraud before I
had any data
all right so at some level that's been a
very broad overview of just some basic
concepts and probability any questions
now what if my hypothesis space is not
easily yeah so if you absolutely no clue
about the hypothesis space then I
suggest you get some domain
understanding that's fine so in that
case your modeling is going to be as
good as your as to how well your
hypothesis space is complete if you are
operating in a region and and we will
actually come down to this concept a
little bit later this shows up again as
to how rich is your hypothesis space
what if you don't have a rich enough
hypothesis space or what if we have to
reach of a hypothesis space right both
of them are bad you want to have some
reasonable assumptions about what are
possible hypotheses to test so there are
some ways to constrain I mean there are
some some ways to measure if your
hypothesis is reasonably good or not but
you need to have some understanding of
even what is possible like what
hypothesis or even possible at the same
time we don't want to make it overly
complicated either previous slide I
really like the concrete examples again
for what prior probability our
likelihood and posterior probability
about the prior probability he said it's
expected rate of fraud before you had
any data what was the likelihood and the
cluster
like we do this suppose if your
hypothesis is correct then how likely is
this data how likely is this hypothesis
led to the data that you actually
observed so you are basically turning
this around and say probability if
hypothesis given data if look what I
have done here is initially I know
nothing about which hypothesis is
correct I have some vague notion saying
they look frauds are inherently less
likely to to to occur so it's probably
not a 50% probability that this card is
fraudulent probably maybe about 1% but
you can then ask the question look here
is the series of transactions I have
seen now what is the probability that
this card is fraudulent given that I see
this level of activity this pattern of
activity on this card so that is your
posterior probability likelihood is
given that the card is actually
fraudulent what is the probability that
you see this this type of transaction
stream right your your your observed
transaction string so that's your
likelihood okay any question so I think
we will stop at probability at this
level and take a look at some some
statistics any questions so far all
right so let me start with statistics
for maybe or should we just pass here
take a break and then come back at 11
John take a break ok so we'll cover
statistics and machine learning after we
come back maybe at 11
all right thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>