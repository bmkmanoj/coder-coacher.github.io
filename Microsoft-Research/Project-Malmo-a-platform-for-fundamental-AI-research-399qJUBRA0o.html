<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Project Malmo – a platform for fundamental AI research | Coder Coacher - Coaching Coders</title><meta content="Project Malmo – a platform for fundamental AI research - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Project Malmo – a platform for fundamental AI research</b></h2><h5 class="post__date">2016-07-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/399qJUBRA0o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so thanks a lot welcome back and I'd
like to introduce my colleague Katya
Hoffmann who is a researcher and a
Mattia machine intelligence and
perception group here and actually the
project she's going to talk about
project Malmo Chris Bishop already
mentioned in his in his talk yesterday
it's the minecraft AI platform that
we've developed in the course of the
last one and a half years so Katya is
gonna give you an introduction we'll
also have a demo this afternoon and the
demo fest okay there you go
thank you very much started welcome to
my talk I hope you can hear me all right
the project I will talk about is project
memo as scarlet mentioned and I'm the
research lead of the project but there's
a large group of people already doing
research on this platform as well as our
core development team Matt Johnson
tomaten and David Wagner who is here in
the audience today we also have an
amazing set of interns our last year's
project interns are listed here and
didn't oppose the current intern is here
in the audience as well
as you probably know research and
artificial intelligence has made
enormous progress over the past few
years in fact the progress was so rapid
that even for people in the field of
came as a surprise how quickly the
technology has developed and how quickly
we have moved to very exciting
applications of this new technology and
specifically I'm talking about deep
learning and the progress that has been
made there some of this progress would
have been unthinkable even five six
years ago but today we have the deep
learning approaches that do
state-of-the-art speech recognition that
is at the level of human accuracy and
technology like this powers products
today already such as the Skype
translator that allows people to have
conversations in across language
barriers while simultaneously
translating language from one person to
the other so this is one very exciting
application but is already helping break
down barriers between people and
allowing people to communicate
each other another example is the
progress in the planning that's been
around um computer vision there's now
first applications that for example help
visually impaired people see maybe you
have seen the seen AI project where a
blind person can walk around with
glasses they can take a picture of a
scene around them and get image
captioning to help them interpret the
science that they see interpret the
world around them so this is very much
about augmenting human capabilities and
giving them more power more control over
their environment and better ways of
interacting with their environment with
all this dramatic advance in AI some
people have started to speculate how
close we are to actually solving AI
seeing all this rapid progress
it seems that well maybe there's just a
small step to get to the level of human
level intelligence but what does that
actually mean and where are we along
this quest I'm arguing here that we're
actually very far away from achieving
anything that looks like the kind of
learning ability that we see in humans
or animals in general all the examples
that I've mentioned and all AI
applications at the moment in fact are
examples of narrow AI it takes a large
team of experts a large amount of
resources to develop each individual
application so for example in the scene
AI or in the image captioning work it
would require a number of machine
learning experts we would need large
amounts of people who annotate images to
provide labels for those images and then
we can train models and each individual
model is only good at one individual
task so it's a very narrow kind of
approach that is very specialized in
doing one specific task very very well
if we compare that to the kind of
learning that we that we see in
biological learning we see very a large
amount of flexibility and the greatest
amount of flexibility we see in fact in
human learning if you compare a newborn
child to the current state of the art an
AI newborn child cannot translate
English to French
it can't recognize speech and they
cannot tell you that the person there is
a guy on a skateboard but over the
course of our lives we learn to deal
with all the tasks all the challenges
that our environment throws at us so a
child growing up will learn the language
of their parents they will learn the
skills the social skills that allow them
to deal effectively with the people
around them and to achieve what they
want in their life now is there a way to
actually move towards that towards the
kind of flexibility we see in biological
learning one important line of work
there is reinforcement learning which is
an approach that works towards systems
that learn interactively from
interaction with their environment so in
the next part of the talk I want to
spend a few minutes in highlighting some
of the ideas and some of the insights
that have been developed there just to
give me a brief idea of your background
who in the audience is familiar with
bended approaches that's roughly 5%
who's familiar with let's say cue
learning okay also roughly 5% okay so I
will spend some time explaining those
core ideas because I think they're very
important and they will be very
important in the future starting with
bended learning blended learning is a
type of reinforcement learning so it's a
type of learning approach where systems
learn from trial and error to adapt
interactively to their environment in
contrast to the full reinforcement
learning problem bandits make some
simplifying assumptions and particularly
they assume that whenever an agent takes
an actions an action that action does
not affect the state that it will see in
the future so if we look at learning as
an interactive loop as you see over
there we have an agent so that could be
some system for example some character
in a computer game that could be a
search engine an arbitrary system that
makes some decision that agent observes
something about the environment so we
can call that context which is this
according to some context distribution
acts it observes a context it decides to
take an action based on the context that
it has observed that action is then
taken in the environment and in response
the agent observes some kind of reward
signal the name bandit comes from the
comparison to what happens in casinos so
if you see the one-armed bandits the
slot machines there you can imagine that
you come to a casino and you're told
that one of those machines is biased so
one of them has a higher payoff than the
other machines now how do you deal with
that scenario you don't know of you know
when you start which machine is the one
that gives you higher payoff so you need
to try all the machines a certain number
of times to understand what the what's
the likely payoff function of each of
those machines is but once you have
figured out that oh this one is likely
the one that has a higher pair than the
other ones then you want to keep playing
that machine so that you make a lot of
money this trade-off between trying the
different machines or the different arms
to gather more information versus
playing the one that you think is best
to get a likely high payoff it's called
the exploration exploitation trade-off
and bandit learning effect formalizes
this challenge to find a good trade-off
between exploration and exploitation in
all of these scenarios the goal of the
agent is to maximize some cumulative
rewards so in this scenario here you
could imagine that you want to maximize
the long-term payoff that you get from
those machines when we look at blended
learning there are a couple of
approaches that are typical for doing
this exploration exploitation trade-off
a very common and very simpler type of
approach is some called epsilon greedy
the idea there is that with some small
probability epsilon you take a random
action so you pull one of the machines
arms and just uniformly at random but
the rest of the time you take the action
or you pull the arm that you think has
the highest payoff
and in practice that simple approach
actually works quite well the problem is
that you need to tune that epsilon
parameter so you need to know how
exploratory you want to be in order to
not spend too much time exploring when
you could already be repaying the
benefits of what you have learned
another type of approach is called UCB
and it's the whole class of approaches
that are based on the principle of
optimism in the face of uncertainty so
the idea there is that when you are
uncertain about a machine's payoff then
you would be quite likely then you would
prefer to take that particular action to
learn more about its consequences the
idea there is that you give an optimism
bonus to each of the actions that you
have available and you can see that
there that this optimism bonus is based
on the number of times that you have
already played that particular arm so
the more often you play it the more
confident you get that you have a good
estimate of the consequence of taking
that action a third approach that I
would like to highlight as Thompson
sampling and this is a type of approach
that has become very popular over the
past few years the idea there is that
instead of just maintaining a point
estimate an estimate of how high the
pair of that particular action does you
maintain a full distribution the
posterior over the the reward for a
given action and then using the
posterior you can actually sample from
that posterior and then once you take
that sample you take the action that is
optimal according to your current sample
the effect of that is that you're very
naturally trade-off between exploration
exploitation depending on the
uncertainty in your estimates and
roughly speaking you will take an action
according to its probability of being
optimal given your current model given
what you already know about the world
and this type of approach is actually
quite powerful in practice in practice
and it already allows us to develop
systems that can learn interactively in
interaction with their environment there
is an extension to MIT unbend it's very
take context into account but I will
skip over the details of this I just
point to some
the resources there in case you want to
read up on some of those details now how
can we use how a bandit approach is used
in practice one case study that I would
like to highlight as restaurant
recommendation and this is work that
Philip read Minsky and I did with our
intern Konstantin a last summer
the idea there is when you come to a new
city you want to figure out where to eat
in Cambridge tonight if you were to ask
another person they would first gather
some information they were try to learn
to know a little bit and maybe see what
your preferences are so for example you
could start a conversation and say hey
do you know a good restaurant or it
could go for dinner tonight
and the person the other person could
say well what kind of food do you like
do you like Italian
you're like a seafood restaurant do you
say Italian and then based on the
information that they gather they can
give you better more targeted
recommendations we address this problem
or remodel this kind of problem using a
contextual bandit approach if you are
familiar with graphical models you will
see that here we have remodeled a user
and an item and the affinity between
users and items if you're not familiar
with this don't worry I just talked
through the high level ideas behind this
model so the model so the idea is that
we keep track of the model parameters so
in particular we want to model latent
factors that could explain what types of
users like what kinds of restaurants and
we infer that from the data from the
observations that we get and we maintain
the full posterior distribution over the
the user item affinities so over the
probability that a user will like a
particular restaurant now when I knew
when a user comes in at every point in
time we will have some model of their
preferences the model of how likely they
are to like certain restaurants so we
can use Thompson sampling to generate
new recommendations for them so we can
propose restaurants
according to the posterior probability
that that restaurant will actually be
liked by the user that every point in
time we sample from the model we present
the recommendations to the user we get
some feedback and then we can
immediately update the model so this
allows a recommender that can
immediately respond to the new
preferences that you provide them just
to give some idea of the results here we
were able to show that first of all this
model allows you to very naturally
incorporate offline information so if
you have any initial expertise and the
initial data you can start using that to
initialize the model and then from that
starting point onwards you can keep
learning as you go along so you can see
on the graph over there that already
given some initial information given
some offline data we get to roughly 60%
average precision when we recommend
restaurants but then with each
individual question with each individual
point of feedback the recommendation
quality very rapidly increases so we can
very rapidly learn to pick up new
preferences as they become available
as they become available to us another
quick example that I wanted to highlight
is that we can use this kind of
technique to optimize online radio in
this particular work here that I did
with Eric Parque and Wayne and Chang
again last year we looked at how you
could optimize a listening stream for
radio listeners so if you come to an
online radio system you can provide an
initial artist and then the idea is that
the radio will play the songs that are
likely to fit well with that particular
artist we can use two sources of
information here one is offline data on
the kind of information that people
provided through generating playlists
and we would assume that the kinds of
songs that people like are the ones that
typically co-occur in playlists' but in
addition to that we want to very
specifically optimized for online
listening performance we incorporated
that such that we would minimize the
number of skipped
so when he used her fully listens to the
song that is interpreted as a positive
signal when they skip a song they would
that would be interpreted as a negative
signal so we want to minimize the number
of skips that we expect in the users
listening behavior in this particular
paper here we showed that you can
optimize that particular objective
staying close to the offline playlists
but also optimizing for minimum number
of skips simultaneously and just to give
an idea of the results here we show that
we with quite a large margin improve
over just using either one of the data
sources so we can very effectively
optimize for online listening behavior
and we also get some very nice
embeddings of artists in the naiton
space where we can see that popular
artists tend to be grouped together so
their people tend to you know people who
listen to one popular artists tend to
listen to other popular artists as well
but as the taste of a listener becomes
more specialized they tend to be in a
very particular section of that space
and not listen to other artists as much
now I want to move towards reinforcement
learning so as I mentioned earlier when
data purchase make this crucial
assumption that whatever the system
shows whatever action the agent takes
does not affect the future stage and
this is a reasonable assumption when we
assume that a new user that wants a
recommendation is independent of other
users that the system has seen in the
past but if you might want to model
longer term conversations where the
state has maintained through the
conversation then that model would not
be appropriate and in that case we need
to move towards full reinforcement
learning where the future states are
dependent on the current system actions
one example where this is the case as I
mentioned as in conversations so let's
say the system takes a turn may be a
response to a query with response to a
request for a recommendation
the user may refine the query and at
some point in the conversation we can
detect whether either the user is
abandoning the system in frustration or
they have found what they're looking for
and they're delighted with the results
now this again can be modeled as an
interactive learning program as I show
over there and now as I mentioned in
contrast to the bandit program we're now
in the full reinforcement learning
problem where the state the the next
state that we observe depends on the
previous state and the previous action
these kinds of reinforcement learning
problems can actually be very nicely or
very nicely correspond to things like
learning to navigate a maze so if you
have an agent that wants to find some
ore that needs to find some goal and
environment we can model this
equivalently as a reinforcement learning
problem where the agent has to move
through that maze and would only observe
feedback when it gets all the all the
way to the end of this maze in terms of
the probe formulation this would be
equivalent to having a conversation with
a very late reward in the conversation
and the popular type of approach to
solving this is some so-called cue
learning the idea in cue learning is
that you look at temporal differences
between your estimates of how high a
reward is going to be given that you
take a certain action in a certain state
and you can see that over here that we
use an iterative approach where your
value the Qt plus one so this is the
value of taking an action T in some
state s T expect up to the old value
plus some learning rate the rewards that
we observed for taking as the action in
that particular state plus the maximum
of the maximum value that we can get in
the state that we see afterwards now
this is fairly abstract I just want to
illustrate this in a small demo now I
was told it's a bad idea to do life
demos we just see how this goes here
alright that actually seems to be
working so what you see here is a
standard reinforcement learning approach
called pure learning that we have
implemented in the memo platform for AI
experimentation that I will talk about
in just a few minutes what you can see
here all the way on your left is the
state of the world that the agent has
seen so this is the model that the agent
is maintaining and you can see these red
areas are the ones where head as where
it has already learned that taking that
particular action in that particular
state is a bad idea so in this case it
resides in burning in lava for the more
neutral States it has learned that there
are no particular bad consequences
associated with those particular States
now the only observation that the agent
gets here is the number of the square
that it's on so it's only observing hey
I'm Square 53 now if I take action
forward and 53 that's generally a bad
idea so you can see that the
generalization ability sorry of a
tabular approach like this it's not very
high but it very nicely illustrates this
idea of backing up your estimates of the
value of taking taking an action in a
particular state and as the agent
explores the environment more it'll be
able to back propagate to earlier in
earlier States that hey this is the
right path towards to go so eventually
it's pulled towards the goal and then it
learns to back up those states and
within roughly a hundred and fifty
iterations it manages to get to the goal
very reliably so it keeps running in the
background
and we can look back in a few minutes to
see whether it's actually managed to
find the gold
how do you measure the generalization of
this kind of I mean for example I mean
do you change them by roaming after you
learn something I mean after it's really
successfully in this kind of environment
I mean you were changing the and you are
even waiting if it's really powerful
when the environment is also like
adversarial in general if you use these
classic types of approaches like a
tabular q-learning the generalization
ability is not very high if you have
some distribution over environments then
you then your Q values could approximate
the the Joint Distribution over all
those environments but as you probably
know there's a lot of very interesting
work at the moment that automatically
learns a good representation for a given
environment so for example if you use
visual features and combined
reinforcement learning with deep
learning then you can automatically
extract a good representation for
generalizing across many different
environments that have for example lava
or that have similar visual
characteristics it's very much an
ongoing area to understand how to how to
get both very robust learning and this
good generalization ability at the
moment the protests typically do either
one so for example tabular pure learning
had good has good learning guaranteed so
we know that it will converge to the
right or to the optimal solution using
quite mild assumptions and using deep
learning we can get good generalization
ability but it's much harder to get
robust performance there and it's not
very well understood what kinds of
guarantees we can get out there but this
integration is very much an interesting
research area oops we can see whether
okay I know you can actually see that
it's fun to go once they see a green a
very positive value over there but the
policy has not fully converged yet so
what what's happening now is that as it
keeps
keeps exploring and getting to the goal
more and more often it would more and
more balaiah P and get to the goal until
it converges to that solution were there
any other questions at this point
so as was already highlighted here there
are so many interesting questions for
example around how we can generalize
effectively how we can learn effectively
in complex scenarios and the two
examples that I'm showing over here are
two agents that we trained to imitate an
expert so we find that if an agent just
has to explore randomly it's actually
very slow to learn it takes a long time
before it encounters the goal and then
it takes a very long time before it can
actually learn to reliably get to the
goal if instead we provide just a
handful of examples of an expert
navigating to the goal or demonstrating
to the agent what it has to do then
within a very small amount of data we
can actually train the agent to imitate
the experts behavior and in the examples
here we're actually using the visual
feature so the agent is only observing
the pixel screen and it tries to or the
learning approach tries to minimize the
KL divergence between the agents
behavior and the demonstrated expert
behavior so it tries to find a policy
that is as close as possible to the
experts behavior and this works very
very rapidly however there are many open
questions on how to actually learn
robustly in those kinds of complex
environments that we see hopefully
eventually in the real world but
initially in something like computer
games just to point up a few point out a
few open challenges a key challenge is
to learn very general robust
representations that support learning
across tasks found for example in the
multitask learning scenario or that
support transfer of knowledge between
tasks and so-called transfer learning
and eventually long lifelong learning so
when we think about learning ourselves
these are all very obvious when we learn
one
tasks when we learn to ride a bicycle we
don't forget how to brush our teeth but
for the current day I approaches that is
very non obvious usually you train an
agent to achieve one particular goal to
do one particular task but it's not yet
clear how this can be robustly
maintained so that a new skill can be
learned and then skills can be retrieved
whenever they become relevant in a new
scenario so that's very much an ongoing
line of work ideally we would like to be
able for the agents to learn
generalizable skills so for example when
you know how to walk around then you can
do a more complex you can do something
more complex for example walk to a table
and pick something up and you need to
put together different kinds of skills
in order to achieve more and more
complex goals again that is something
that we don't know how to do yet and
it's very much an open challenge of how
to approach that and finally lifelong
learning if the challenge of
understanding how an agent can continue
to learn new things over the whole
course of its lifetime again without
forgetting and with becoming better and
better at a task as its as its
practicing it it is not clear how to
learn in environments that change over
time so if you imagine an AI agent that
we want to let's say we want to deploy
an AI agent that can help their users
achieve certain goals well as you go
through your life you will have
different goals so how do you
communicate with that agent that it
should be doing something else that it
should be learning something new and
that it should use a new skill to
actually help you achieve your goals
better and finally there's a very
important question around feedback so
how penny I agents be taught effectively
how can they learn from human feedback
or a combination for example of having a
user demonstrate a particular task and
then providing feedback as the agent
goes along and learns to get better and
better over time so this last set of
this last part of the talk it's very
much about finding a way to start to
answer those questions and I would like
to introduce the memo platform which we
have designed to be a platform for a
fermentation to make these kinds of
experiments as easy as possible to allow
us to push towards answering those very
fundamental questions that are still
very much open an analyst before I move
on who is familiar with Minecraft let's
say who has heard about Minecraft
everyone who has played for more than
two hours okay that's about half so if
you if you're not familiar with
Minecraft it's a computer game that is
based on voxel graphics as you see over
here so pretty much everything and there
is a block
it's very open-ended so you can play the
game in many many different ways you can
go in adventures you can play it
socially to discover the world with
friends and people build amazing
structures there's a concept that works
like electricity called redstone with
that people have constructed huge cities
with complex transport systems they have
implemented computers one transistor at
a time that actually works so you can
build a calculator within the game so
it's very fascinating for all those
different reasons that it's and in
particular because it's so open-ended
and so flexible and exactly this
flexibility and this open endedness
makes Minecraft such an exciting
platform at such an exciting basis for
AI experimentation as I mentioned
earlier in AI research the current open
challenges are very much around how we
can train agents that are flexible that
can solve different tasks and continue
to learn as they go along and exactly
coupling the technology with a platform
that is flexible they can post very very
different problems and try to achieve
very different tasks it's very promising
now what project memo provides is a very
intuitive API for making AI
experimentation in Minecraft as easy as
possible the platform provides
embodiment so an agent is put into this
3d world and experiences it like a human
player getting for example visual
observations or other observations
about the world and they have to
interact with the world and experience
the consequences of their actions so
they learn from trial and error they can
learn socially for example from other
players or other agents you know with
the platform we also have full
experimental control over the
environment so we can start with
creating tasks for the AI agents that
are easy enough so that current
technologies can solve them but as we go
along and the technology becomes more
powerful we can start to create more and
more challenging tasks to really push AI
technology towards something that will
eventually approach the kind of
flexibility that we see in biological
learning to give you a very brief
overview over the platform essentially
what it provides is a mod that already
hooks into the game minecraft and it
exposes the state of the world um to the
agent it provides a reward signal then
the platform would be used to implement
an AI agent and would choose actions to
send back to the platforms so this very
much implements this interactive
learning cycle that I mentioned earlier
the way this looks is roughly like this
what's what's important is that all the
data all the interactions with the
environment are recorded and are
available for learning so here we can
just see an agent walking around an
environment we can see all the data that
is recorded here for example the
position of the agent the the visual
input that feted perceives as I
mentioned this platform can be used to
experiment with very different types of
tasks it also allows us to work with
very different AI approaches so it
allows us to put on the same or roughly
equivalent level something like the
classic tabular q-learning approach that
I showed earlier and something that
works in a very different way for
example using the visual input and deep
learning to make sense of that
particular scene and we can start to
vary systematically compare those
approaches to read it further the state
of the art in this research area
just as an example here the tasks in the
platform are specified in a simple XML
format where the experimenter can choose
what kinds of observations the agent
should perceive and how it interacts how
what kinds of commands that can send to
the environment just to give you an
example this is a complete agent written
in Python in this case so you would set
up a particular mission or a particular
task put the agent in the environment
and then run it just to take this apart
the agent usually has this interactive
cycle this very much implements again
the cycle that I showed earlier where as
long as the mission is running you get
some observation of the road state for
example the reward the the current
frames or some other form of observation
you interpret this in some way and send
a command back to the environment so
this is the simplest possible agent that
you can start running so it was just to
illustrate that it's very very easy to
get started with this and explore new
approaches to addressing some of those
challenges just to show how
cross-platform the the MIMO platform is
this is the code that I showed in Python
that's the same code in torch that's the
code in C++ Java and c-sharp and don't
worry if you haven't been able to read
this as quickly the point is that we
have made the memo platform as cross
language and cross-platform as possible
so that experimenters can go in and
couple this with their favorite approach
with systems or agents that may already
exist in different programming languages
and get up and running very very quickly
and start experimenting with the
platform from day one we're also very
excited that the platform supports
mighty agent interactions so for example
you can imagine having an AI agent that
has to solve a task collaboratively
collaboratively with another agent or
where we have agents compete against
each other or you could have
interactions between a human and an AI
agent and for example learn from each
other or
understand how the agent can learn to
effectively collaborate with the human
player so in summary I talked about the
current state of AI and highlighted that
while there was dramatic progress over
the past years
all current AI systems are still very
much narrow they are trained to do one
particular task very very well but they
have very little flexibility they're not
very robust to changes in the
environment and they cannot learn new
tasks as you go along
they cannot generally become better as
they practice a task over time the
challenge that we currently set out to
solve is to develop flexible AI systems
that can learn across tasks and learn
new things as they go along
project mo is providing the platform for
that particular research so it allows us
to very easily set up new experiments
try out new technology and hopefully
work towards this very ambitious goal of
flexibility AI learning I'm also very
excited to announce that project mammal
will be open sourced in the coming days
and you can already try the platform
today in the demo that David pigna will
be giving in the afternoon in the coming
days there will be an announcement about
the open sourcing of the platform so I
invite you all to go and to try it out
and give us feedback thank you very much
for your attention
I organize competitions and search for
design of the vests a we've had a lot of
discussions around that and actually
we've already worked with academic
collaborators to get an idea of what
would be best for the research community
I'm a bit worried that competitions
usually don't bring out the best advance
in in from the research side so you
usually get people to guard their
particular implementation there
particularly aristocrats actually
solving a very ambitious goal we are
thinking about ways of for example
crowdsourcing or collaboratively
developing tasks and trying to move
towards a more collaborative scenario
where people can really collaborate on
solving those very ambitious challenges
and we have a few ideas around this but
if you have others please come and talk
to us during the during the demo session
after the talk hi would you say that the
problem with there are two different
tasks is more effective because
q-learning is like learning and
developing policy instead of learning
something like a model of the world that
you were doing in other approaches
that's a that's a very good question um
part of it is related to simply the
representation that tabular q-learning
kind of you know learn learn new
representations and some of this is
already alleviated with deep learning
approaches to reinforcement learning but
I think you're right a lot of the
flexibility and generality and
biological learning comes from water
based learning we're we haven't seen a
lot of work in that particular area but
it's something we're actively working on
it's a very exciting direction
absolutely
it doesn't seems like it's like I mean
more general motor inference for the
environments on the environment it's
also like our doing because it seems I
mean sometimes it seems for me like I
mean they it's doing something
and they and in vitamin it's already set
up Andrea on the I mean change they
change the state on the environment but
the Byram by itself is not I mean like
learning itself but we are not like
learning something new about environment
seems like it's like more study dynamic
right right so so if I understand you're
right you're worried about if we
evaluate an AI approach or reinforcement
learning approach in an environment that
aesthetic we are not really modeling the
real world the real world changes all
the time and in the real world we have
to deal with that with that change
because the environments and evaluation
are typically static we are not really
addressing the full program is that the
term you're mentioning I think that's
exactly right and I think the only way
to overcome this problem fundamentally
is to have humans in the loop so
initially we can still push a lot we can
still do a lot of research and improving
AI approaches in a reasonably static
environment because there's still so
many open questions around
generalization and adapting to new tasks
but eventually the real test will be
when an AI agent has to interact with
with the human with the person whose
tastes change over time who may give
different feedback very different
instructions on different days who may
be you know in a different mood etc etc
so I think a lot of dynamicism will come
from interaction with human with humans
and I think the the core AI challenge is
to actually learn effectively in
studying that is that dynamic and that
has humans in it
okay we'll have one more question
examples and things like that do you
think what potential do you see for
example for an unsupervised learning
because like actually geoff hinton once
said that actually people don't learn to
recognize object the way continents do
it so they are not provided so it's like
thousands of examples of birds and
saying this is the bird and this is the
car and this time they all didn't just
met maybe told once and they just they
try to generalize and reveal the program
model by themselves absolutely I think
there there are many very interesting
threats that you mentioned there one one
point that I would like to counter a
little bit is this lack of experience or
lack of data that is typically mentioned
so an actual child growing up it's seen
millions and millions of images as
herring a million words you know by the
time they're three so there is actually
a lot of data that they can learn from
but I very much agree a lot of that it's
not in this very nice format of this is
a bird this is an acid etcetera etc but
there is a lot of unsupervised learning
potentially model learning as you
mentioned up there that and and
eventually a learning step is really
just attaching for something neighbors
to a particular concept that intuitively
an agent or or a person already
understands it's not quite clear yet how
to achieve that kind of model in an
artificial agent but I think that again
that's a very interesting direction for
research okay I think we'll wrap up here
and because there'll be more opportunity
to ask questions in in the demo session
this afternoon</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>