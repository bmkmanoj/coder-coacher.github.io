<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>IMS-Microsoft Research Workshop: Foundations of Data Science - Opening Remarks and Morning Session I | Coder Coacher - Coaching Coders</title><meta content="IMS-Microsoft Research Workshop: Foundations of Data Science - Opening Remarks and Morning Session I - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>IMS-Microsoft Research Workshop: Foundations of Data Science - Opening Remarks and Morning Session I</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/he4RcmPtjsU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so good morning everybody
this was a wonderful workshop up to now
thanks for coming to this session
imaging is a science a big data
challenge we have three distinguished
speaker in that session and they will
all talk on different aspects of at the
cutting edge of statistical inference
efficient statistical inference large
scale or large data optimization and
certain aspects of physical science in
that case the main applications are
actually in biophysics so to start with
the first speaker who is Timo is Paul
Meier he's going to explain a little bit
of very exciting new statistical
challenging so rising actually from what
is called nowadays nano scope your nano
scale for our sense mask microscopy as
you maybe know until the early 19th of
the last century is a so-called a big
extent for roughly about a hundred years
and told you tells you that you cannot
discern object with visible light
actually beyond 1/2 of the wavelengths
you use and then in the early 90s
essentially Stefan hell from the Max
Planck Institute I made a break a
groundbreaking observation how to how it
is possible to break that limit and from
that time on the rapid development
actually has been initiated and nowadays
a lot of activities is going on in
understanding and observing living cell
and transport communication processes
for example if you look to a recent
issue of Nature Medicine you find
actually that about 80% of all papers
are based on nanoscopic techniques so
consequently the last year's Nobel Prize
in Chemistry has been awarded to these
three gentlemen Eric patsak Stephan
he'll and William learner for their
fundamental work interestingly by
education they are all three physicists
but they got the Nobel Prize in
Chemistry Tim oh well now
here is an example from what is called
stochastic marker switching microscopy
and actually the movie should work but
it doesn't let me see ya now it works so
what you observe is and we will learn
more about it in teemo's talk timely
highly resolved and spatially highly
resolved response or photon counts and
from that you finally have to aggregate
an image which is in this case a part of
the alpha 2 billion network in the cell
skeleton as this is what classical white
field microscopy gives you and this is
what nano scale resolution microscopy
gives you and there are certain
statistical and computational images we
will learn about then Victor Penner ATIS
is going to talk about a different
aspect again his applications are from
biophysics these are so-called DNA mini
circles and he will talk about also
spatial spacial temporal issues that is
functional data analysis of spatial
temporal time series and here methods
which come in are in particular certain
kinds of spectral expansions in order to
model that time dynamics finally good
novelty is addressing a detection issue
we learned about that already in the
talk of who has knowledge esterday in a
sense detection we know that of course
statistically is simpler than estimation
then recovery of the whole function or
image or whatever and and he will give
as theory for precise detection
boundaries in particular in a multi
scale setup and fascism so I'm looking
forward to all the three speakers and
our first speaker is Timo as Paul Maier
from University of göttingen and I will
just good morning so I would like to
tell you in this talk about some
statistical issues and challenges that
arise in super resolution microscopy and
this is joint work with Alexander ignore
from the laser lab or laser lab in
cutting and axial moon and first of all
I'd like to introduce you to this
sort of it's not complete at all it's a
list of today's microscopy methods and
how far you can get in resolution with
them so why field microscopy is sort of
standard microscopy which you probably
all know and then there are several
methods and some of them
stet and SMS are going down or are
improving the resolution of a standard
microscope by at least an order of
magnitude and that's of course an
incredible gain because then you can get
down to scales of 1020 nanometers where
you can see sort of individual proteins
and I'm going to focus on just these
lower three of these methods and I'll
show them how they work and so you can
understand where the statistical issues
come in so before I start I would like
to introduce you to sort of standard
microscope because there you can see
where the problem actually comes from
the sort of resolution boundary the
diffraction limit as it's called has its
origin in this picture so on the left
you see some object and that's being
magnified by this microscope and you
look in from the right and in fact you
can achieve very large magnification
factors that's not a problem but
magnification is not the same as
resolution so the main problem that
appears is if you imagine a light
towards a point source over here it
emits a spherical wave and this wave
enters the microscope and is transmitted
to the other side and it's focused onto
one point but it's not the entire
spherical wave that enters the
microscope it's only a small part of it
the small part that goes into this
opening angle there and this is a
fundamental loss of information you
cannot recover the rest of the spherical
wave it's lost and that leads to the
effect that on the other side what you
get is not actually
it is not focused on a point but instead
you get a blurred image like this so you
started with the point source and what
you get is a blurred image of that point
source so this is the point spread
function as it's called this is this
blob over here and I've shown a cut
through this point spread function and
you can actually calculate what it looks
like that's this green curve and it's
actually pretty complicated it's got a
single peak in the middle and then some
small peaks and there are infinitely
many to the sides as you go further out
and this point spread function has a
certain characteristic width and
effectively you cannot see any details
which are smaller than this width of
this point spread function and this is
the fundamental diffraction limit and it
was thought for more than 100 years that
it cannot be broken because it's a
fundamental thing you lose information
right at the start and you cannot get it
back before I'm going to tell how it can
be broken anyway I must tell you
something about fluorescence molecules
because in today's biological
applications this is what is being done
and it's actually crucial for these
super resolution techniques so what's a
fluorescent molecule it's a big molecule
like this and it has several electronic
states one ground state and a higher
energy state and many more um and it can
happen that if you use laser light of
the correct wavelength you can cause the
electronic state to change from the
ground state to the first excited state
and then after a short time it relaxes
and in then it falls down into the
ground state again and it emits a photon
but
a photon of a slightly different
different wavelengths than the one that
you're excited it with so it's shifted
to longer wavelength mm-hm
so this is very exciting because you can
now attach these molecules to basically
whatever you want you can target a
specific protein of interest for example
say I want to know something about that
and how it works in cell and then you
can make that you can arrange for that
protein to be labeled with these
fluorescent markers and then what you do
is you shine the blue laser light on
them onto them excite them and they emit
a green light so you make them light up
by themselves and that's a very exciting
thing these molecules have several other
characteristics one of them is
unavoidable basically which is that they
can break so eventually molecule may
sort of do this circle here many many
times emit many photons in the process
but eventually and that's a stochastic
process it will break and then it's dead
and it can can't emit any photons
anymore
the other thing that's very important is
read these you can have molecules that
have a dark state so there's a molecule
in this configuration and it doesn't
emit any light if it's in this
configuration but you can make it switch
from there to the active configuration
again by stimulating it with a laser
with laser light so you can turn these
things on or off as the cases and that
will turn out to be crucial as well for
what I'm going to talk about so now we
have an optical microscope and we have
fluorescent molecules and this is what a
fluorescence microscope looks like you
have a sample down here and it consists
of a set of markers fluorescent markers
you excite them with the
blue laser light and they start emitting
photons these photons are transmitted
through the microscope you have observed
them on a camera here and what you get
is this image it's still very blurred
because you haven't done anything about
the diffraction limit yet so you observe
this green cloud of light ok so here's
an example of that this is a an image of
a hippocampal neuron and here's an
announcement and what you see is first
thing you see is it's very blurred
that's the point spread function that's
also a lot of image noise and the noise
comes from various sources actually so
the primary source is that we are
actually counting single photons here
because the fluorescence molecules emit
single photons and that's what we're
seeing and that's what the camera is
recording so this usually it's rise to
post or noise but not always and there
can be very much more complicated forms
of noise here then has the camera at
work and that converts the photons into
electrons and then you have electronic
multiplication noise it can be modeled
by a gamma distribution for example then
you have some analog circuitry in the
camera so that add some gaussian perhaps
noise as well so all these images have a
very complicated noise structure ok but
in principle you would hope they are
static
so I can the picnic well if this is also
noise I mean it's depends on the number
of photons you count well this is my
spot so it's really noisy so how do you
actually break this resolution limit mm
and the first idea was as accent
mentioned by Stefan hell and what he did
was he had this excitation laser and the
excitation laser is now focused focused
on a little spot this is the spot here
the the whole thing is the blue
excitation spot and it's diffraction
limited because we can't break it you
can't break this limit hmm but then on
top of that he applied a second laser
this red one and he also focuses that
but in a special way and so the the red
excitation field has actually a doughnut
shape so it's a ring with a hole in the
middle and it's got a different
frequency and what it does is it's
working like in a laser in fact it
stimulates the emission of photons so if
a fluorescent molecule is hit by this
red laser it thinks okay I must emit my
photon and it does so very quickly and
it does so with a different wavelength
the red one but that means this molecule
is effectively switched off because it
cannot emit the green photon anymore and
that's called stimulated emission
depletion microscopy and so the effect
is that this doughnut shaped ring it's
also diffraction limited so it doesn't
have any sharp edges or anything
but it's got this hole in the middle and
if you superpose that with this blue
excitation light there is only a small
spot in the middle from which the
photons that you observe and actually
originate so whatever you whatever
photons you see here on the other side
you know they must be coming from this
little spot in the middle and that's the
gain in the resolution that you can get
and then you do this for one pixel and
you scan the whole image and then you
get
your total image and that's a really a
very ingenious idea I think in 94 1994
this is a technique that's an
alternative if you like it's much newer
it's from 2006 from Eric patsak from
Howard and the principle here is again
that you have your fluorescent markers
but this time you exploit the
possibility that they can be turned off
on or off so what you do you excite your
whole thing as before with a blue laser
but then you have this switching laser
and the switching laser you have a you
put you use a very low intensity and
that means that you have a very low
probability for each purif or to be
switched on
so in effect most fluorophores stay in
their off state only very few enter the
on state and what you see is in this
case two of these fluorophores have have
been switched on and you see two green
blobs over here but then you know under
each blob there is with very high
probability only exactly one through a
four and you can localize that very
precisely and then you do that many
times you a great the data and then you
get an image super resolution
super resolution image okay and a third
method I would like to introduce is
based on the fact that each molecule has
also an orientation so it's got a deep
hole vector and light also has an
orientation it's called the polarization
and the photon yield of this floor four
is maximal if the dipole moment and the
the polarization are aligned and it's of
a continuous function of the angle
between the two so if we rotate the
polarization of the light we can make
them
these molecules blink in time if you
like and if each molecule has a random
orientation that means each molecule has
a random blinking pattern so this is
what you do you use your excitation
laser but this time with a polarization
rotator in the middle and then what you
observe is this such a sequence of
images one may look like this another
one like this and one another one like
this and the task is then to infer the
distribution of of molecules underlying
that so these are three methods that can
be used for super resolution and a basic
principle that unites them is sparsity
you can the principle by which you can
break the diffraction limit is that you
caused by whatever means you have to
light up molecules not all at the same
time but at different times or with
different phases so in this example here
if you have two molecules close by and
they light up at the same time you get
something like this and you have no idea
what's going on is it one is it two is
three
where are they you don't know but if you
can separate them so at the one time you
only see this and you know okay it'll be
there in the middle and the other one it
will be there in the middle so you can
achieve super resolution now how do we
go about and model this what we want to
know is this function f concentration of
the fluorescent dye but we don't want or
we can't we observe that directly what
we observe is the function G which is
the concentration of excitable embodied
molecules at the time T so we are
changing so that's the specification
that I was talking about and then we've
got this point spread function and we
are involved in this function G with the
point version and that is the average of
what we would have observed
and that enters this for example Poisson
process as an inhomogeneous parameter
and that's then is our image because you
don't see the proteins you only see the
present marker which is attached to them
so you have no means of seeing the
structure of proteins one method is to
modify the that genetically so it just
is expressed that way there are other
methods I think but I'm not an expert on
that so for this procedure what you do
is you have your concentration F and you
multiply it with HR PP s which is
centered around zero but then you shift
it along a path it's this scanning path
X note there and you scan the whole
image and then that's what you observe
and for single marker switching it's
different but in this formalism it looks
quite similar you have your
concentration F and you multiply it with
random process which is sparse so you've
only very few locations where you can
actually see anything and so because
this G is now sparse in X at every point
in time this statistical inverse problem
to get from your observed image image to
the actual G is a well posed problem and
once you solve that and that's fairly
straightforward you can just add up all
points in time that you have and that
gives you an estimate of your underlying
density yes you know s you know X naught
you don't know e but it doesn't matter
because
in maybe I'll show in a minute and now
for the third method it's again quite
similar but this time your concentration
is multiplied with a time dependent
function it's so D is a time dependent
function it's periodic because you're
rotating your polarization periodically
and each molecule has a random
orientation and therefore a random face
sorry a random phase shift tongue so if
you put all this in you can write your
function G as an convolution of a
density in the phase shift domain with
your modulation function D and the point
now is that age doesn't have to be
sparked in X but by construction it's
passed in the phase shift domain it
starts in theta so again you have
created some sparsity and you can use
that to break the resolution limit okay
now I'd like to share what kind of
methods we can use and what challenges
arise in this context so here's an
example at first of a steady image so
this is a sort of conventional
microscopy and this is the same thing
exactly the same example but with dead
and I think you can see that the
resolution increase is really
substantial it's a lot better this is of
a protein complement in in itself but
this image although it's very much
better than this one still isn't the
real thing it's not the real
concentration of the fluorescent dye
because there's still noise on it quite
a lot of noise in fact because we are
observing very few photons and there's
also some still some sort of blur on it
and it's still a point spread function
at work but it's much smaller of course
but still so we can try to improve that
and one way to do that is to use multi
scale constraint optimization
so what do you do there you take the set
of subsets of the image for example set
of all rectangles or of all squares or
whatever seems appropriate and you
choose a prior functional that encodes
your prior knowledge about your density
and it can be the TV norm and one norm
and a tool to of gradient or many other
things and then on each of your chosen
rectangles for example you perform a
test a statistical test whether the
image you observe is compatible with a
candidate function f that you choose so
if that's if this test is fulfilled and
your candidate function is in the
feasible set and from all those
functions in the feasible set you choose
that one which minimizes your prior
functional and this is a way of
constructing an estimate of your
underlying density F hat and it's some
advantages because what you need is the
statistical tests but that's usually
easy to construct for Gaussian
statistics or poor song or many other
things and what you need to know is the
1 minus alpha quantile of this statistic
but that can be calculated in many cases
or even if it cannot be calculated can
be simulated and test images so you can
do this very easily it's testing and
this quantile this alpha here is the
only parameter in this whole scheme and
that's very attractive because that's
one only overall parameter and that has
a very clear statistical interpretation
because it gives us a smoothness
guarantee the smoothness guarantee is
that the estimator is as least as smooth
as the real function in terms of J with
the probability at least one minus alpha
so that's if there's some statistical
guarantees about our estimator
here's some examples how do you mean
example I mean that's that you're trying
to choose from
Oh for example for the next image here I
chose the set of all rectangles of size
one by one up to 20 by 20 so but you can
make many other choices and I have a big
influence or they can have a big
influence
yeah that's certainly true so this is
now this little region up there and this
is the original very blurred very noisy
this is a reconstructed version and you
can see some details in this region for
example which you cannot see here at all
and this is for the confocal so the
standard microscopic image but we can do
the same for the sted image it's the
same region and here you can verify that
the details that are found in the
previous image are actually compatible
with a real thing that's going on
underneath and again we can do this same
thing for this sted images and get a
statistically valid estimate of our
function f ok now let's switch to this
other kind of super resolution method
the single marker switching where you
switch on the molecules one by one and
here's an example image so I think this
is refers to your question this is the
sum of forty frames in fact so each
individual frame is much sparser than
this one but in you don't need to know
this function e because you can't
identify these these little spots in
each image you can just see what it
looks like I don't know if making myself
clear
but what you see whenever you see as a
spot of light there you know there must
be one molecule underneath of that and
you can automate this and localize the
molecules and that is what this picture
then looks like this is for the first 40
frames up here and then this is for if
you add all
frames together so this gives a very
blurry image sort of a standard
microscopic image but if you do the
localization first and then add you get
a very sharp image of the whole thing
and that's quite an ingenious method but
um as I think you remarked it may very
well be the case that over the course of
time the whole image moves because it
takes of the order of minutes I think
for such an image to be taken so and
these yellow markers here they show an
actual drift over this time so um in
fact we would like to correct for that
and one way of doing that is to put in
these reduce your markers so you can
track them you know the drift and you
can correct for that but even nicer
would be if you could do that from the
data itself without any markers and
that's this drift estimation while you
have a parametric drift model and you
put that in to your guru for density
here as a time-dependent drift and then
you can formulate a certain contrast
functional and if you minimize that
functional with respect to your drift
parameters then you get an estimated
drift and you can use that to correct
your images and that gives very accurate
results as you can see in this image so
here on the top row is the the
uncorrupted image and corrected with be
producing markers and corrected just
from the data alone and you can see that
it using markers and data alone is give
both very good results and this is the
original if you don't do the correction
okay now I'll almost finished now for
spot I said that we have sparsity in the
phase shift domain so what we do here
for example is we write down likely
for our image sequence that we caught
and we put in a sparsity enforcing
penalty there at the end and then we can
try to minimize this and that gives us
an estimator of our density in in phase
shift space and this is in fact a convex
optimization problem with non smooth
terms we can see we've got the l1 on
there so it can be done with a number of
methods you've got this software
available which is based on the back to
Bruhl algorithm but you can also use
jumbo pork or ad mm or other methods and
with that you can then reconstruct your
image and this is on the left the
original this is the resolution enhanced
image and here on the right you see if I
switch through it face slices so at each
sorry each face slice you get a
different image and if you superpose all
of them you get the image in the middle
and in fact what I said in the beginning
there's a lot of blur going on here but
in this resolution enhanced image you
can actually see that this is there's
some structure here at least at this at
the end of this spine head ok and with
that I'm finished with my overview over
the statistical methods and problems in
super resolution microscopy so what I
would like to say is that all of this
requires you to take into account the
single molecules and the single
individual photons that you are
observing and to do that you need a
detailed physical chemical statistical
description of the underlying processes
that are going on and you need to put
all that into your methods and then on
top of that you have to apply advanced
statistical methods in order to get the
optimal image from out of there
including possibly confident statements
and
all these problems can be formulated as
non smooth convex optimization problems
on a large scale so it's really a tough
problem to do new Mori numerically okay
thank you very much so it's really
interesting so even if you get this very
very clear got a point estimate of this
this function or protein or whatever
you're doing is it that is that the end
goal of the analysis or do you actually
want to do some sort of analysis of that
image I think that would be very very
challenging I think we'll try to give
them a nice picture for their paper or
you have many of these pictures and you
want to ideally yeah I mean ideally you
would like to have an estimate of your
underlying distribution concentration
and have some confident statements about
that so you can make whatever biological
statements you would like to make and I
think that's the biological statements I
can't do those for the biological
biologists but I would like to be able
to make sort of confident statements
about how good is the image that I am
giving you this is a very core issue
here which should we talk about living
cell my
it's nice each year Microsoft Research
helps hundreds of influential speakers
from around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>