<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Compositional Proof Automation for Multi-level Abstractions | Coder Coacher - Coaching Coders</title><meta content="Compositional Proof Automation for Multi-level Abstractions - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Compositional Proof Automation for Multi-level Abstractions</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/75S8dk8J3NE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
ok good buddy everyone so it's my very
great pleasure to welcome Gregory Malika
from Harvard where he's a student of
Greg Morissette and of a dreary parlor
at the same time which is a very fine
combination to have supervising you and
he's going to talk to us today about
some proof automation in thanks all
right thanks for having me because as
Nick said I'm Gregory and talk to you
about proof automation in so you
know maybe I'm talking to a an audience
that already believes this formal
verification is kind of nice thing there
are lots of systems that we rely on all
the time things like cars and bitcoins
and iPhones or windows phones and we
kind of expect them to work and we
expect them to not have security
problems and we don't get this
necessarily from all current software so
the rooster up there says what this talk
is going to be a little bit about
but from what I do I've been looking at
program verification for a while I
started out looking within this system
called why not using horror type theory
we built things like databases and web
servers sense started working with
add-ons Apollo I started working on a
system called bedrock which is about
lower level program verification and and
all these things are built on and
I'm going to talk to you in particular
about the automation that goes into
bedrock here so bedrock is a language
kind of think of it like assembly
language programming and we want to
verify large programs with it so things
that we verified with it our compilers
garbage collectors even kind of the
entire stack up to a simple web server
accepting kind of only the POSIX very
simple net API said and things like that
the the works that I'm going to talk to
you about today has been used in bedrock
and is being picked up by and repelled
using his verified supper tulle train
for verifying see programs and also this
charge project that yes verb angstrom
has been working on so all of these
things
kind of fit into this similar
application domain of verifying
imperative programs here's what an
imperative program might look like for
those of you who don't know what
imperative programs look like imagine
that's not very many of you a simple a
simple finite map data structure we're
going to assume it's implemented as a
linked list and we might write some code
that looks up some key inside of it now
the code here isn't particularly
important all of you probably could have
written this the specification just says
I'm going to take it a map m here the
parameter and a a mathematical version
of that map st here so we can think of
that as what the map actually represents
and here i'm going to give some result
that corresponds to an unchanged map
plus some property about the result of
this function the details aren't are
particularly important what I'd like to
get across to you is it in reasoning
about this code I have to reason about
all these different domains so let's
take a look at one verification
condition that might come up if you
start writing this function and you get
a new you just immediately run through
here and you return at this point return
carrero value so you'd start with the
precondition you've gets you our
sequence of instructions that are
running and you'd have to justify the
post condition and what you'd
essentially do is you'd start by
unfolding this map predicate exposing
some stuff the head pointer and things
like that then you start symbolically
executing each of these instructions
learning the assertions moving them into
the precondition and until you finally
get to the end of the instruction stream
and you just start manipulating the post
condition and then you try to solve this
entailment so you know when when we
looked at this we had to reason about
logic we've got lots of Ann's these are
really stars but separation logic is not
terribly important for this talk we've
got arithmetic reasoning about pointers
and pointer aliasing we've got things
like finite maps we've got quantifiers
and we've got kind of other predicates
things like sort of
of arrays so all of you I think are
familiar with but for anyone in the
room or who's listening who isn't
automation kind of works like this so to
prove a program will take a very simple
program we'll just think about a
proposition we want to prove that p and
Q entails Q and P right so the standard
way to do this is with tactics so you
might run a bunch of tactics cross
introduce the the P and Q fact destruct
it split it performance of assumptions
in this builds this proof term and
in a schematic way and we could make it
faster by by doing something like this
but regardless we'd still build the
proof term so the important thing is is
if I do it manually tlie manually then
it's a lot of work if I do it
automatically using an LTAC program I'm
still building the same proof term I'm
not getting any compressed proof term or
any better proof from exo usually end up
with the worst proof term because I've
made my code more generic so when we
think about applying this type of
verification to our kind of toy example
up here we realize that each one of
these steps is going to result in a very
large proof we're gonna have to
manipulate lots of quantifiers where you
have to manipulate maps and various
reasoning pieces so if each one of these
little things represents a big proof
then when we compose them all together
we'll end up with big proofs if we want
to verify large programs then these big
proofs will start to overwhelm us so for
example in the bedrock project we've
been looking at proofs where you can to
even kind of do them in a reasonable
amount of time on an eight gigabyte
machine and so thinking about things
like taking 30 minutes to verify a
program a relatively simple program all
right so the idea of this talk is to use
this notion of computational reflection
right so computation reflection says
let's shortcut all these all these
little tiny steps by writing a verified
procedure here
and the way the way computational
reflection works is we take our goal and
we're going to try to represent its and
tactically we do this because we can't
write a program that matches on terms in
prop because we don't know what they are
exactly it's an extensible type so we
build we carve out some subset of our
types here we're just going to talk
about this X here think of that as the
injection of some property so here P
here would be injected as X sub P the
same thing with Q we have um
conjunctions and we have things like
true so very simple language we connect
this we connect the syntactic
representation to the semantic meaning
using a denotation function your prop D
what we do is we prove that if we give a
particular term to this proof function
and it returns a new term then we prove
that prove is sound and we say that if
you assume this then you can prove this
right so we're doing kind of backwards
reasoning so we're simplifying you can
think of it as simplifying the goal from
this you have to prove this to to we
only have to prove true and the
denotation of true is true and that's
pretty simple to prove oh you guys are
free to interrupt me or stop me at any
point here we build this proof term
which is you know equally large to the
previous term but the nice thing is as
this formula gets larger this proof term
stays essentially the same size the only
thing that changes is this is this term
here in this term here which tell you
essentially which are the the syntactic
representation of the goal so if we add
a hundred more con junks here at 100
more con junks here this term will get a
hundred larger and 100 larger and 100
larger 100 larger but we won't get the
kind of exponential blow-up that we saw
in the previous in the previous example
so let's take a dive a little bit deeper
into this idea so um
here we've defined our language props we
start by defining that syntax we give a
denotation function we describe the
meaning of what our terms mean so here
we're saying true is true property of
this of this conjunction is just the
conjunction this is kind of like a
standard denotational semantics then we
write this procedure so procedures yes
sure think of this as the injection of
something so p for example and the sharp
represents we're going to represent it
as a number and that's because when we
go to find this assumption here so here
we'll have something like p you want to
prove p and you'll have an assumption
that says oh i know p is true but we
can't check to see if two propositions
are equal we'll only be able to check
for example that two numbers are equal
so we're going to use the fact that two
numbers are equal to infer that these
were actually the same proposition this
X here that's just a constructor so
that's the name of the constructor all
use a pencil terms what you're saying is
that you said that setting up a formal
framework to be able to say that once I
prove that laboratory assertion P and Q
so actually a P and Q implies Q and P
basically this lemma can be reduced all
over the place by plugging in concrete p
and q yes but we'll be applying it
usually to ground terms so we'll apply
this in our actual proof fair because we
won't build build it we won't build
specialized lemons for them because and
discussion is an instructor which
constructor which constructor you can
think of this as a three constructor
type true between constructor yeah isn't
that type yeah yeah is it something
things like lynnwood ultimate using high
reunification there's nothing
necessarily higher order about this you
can just think of this as an
uninterpreted predicate so just ready
goods sorry okay so let me look through
it okay yes you can pattern match on
this type and you'll be able to know
essentially if you get this you'll say I
don't know anything more it's just some
term that I don't understand I really
view you go XL hash and then over here
you go XL little p okay so what you're
saying since we can serve two terms so
that third guy which is it can start to
the hash of the X the X is the
constructor the hash I'm just saying
this is a number so yes x of it sorry
yes okay my apologies for that yeah when
we go to prove our procedure our theorem
looks something like this so here we saw
that goal above or we're using the proof
procedure and we're applying it to go
we're getting out some value true and we
says if we get out true then if we know
all of the assumptions so we're saying
here if all the assumptions are provable
then the goal is provable this make
sense
alright so how does this pan out we've
said okay we can build better proof
terms if we use computation reflection
so where can we apply computational
reflection it turns out that we can only
apply it between individual proof goals
here so we can compress this large proof
into a small proof but we can't compress
this large proof into a small proof and
the reason is because we took and we
carved out the language of propositions
into a little set here this this this
type here when we needed to do
arithmetic reasoning we needed to carve
out a different set right and these two
sets are in some sense incompatible when
we get to an equality term in the logic
framework this will be used we'll use
this with X but in in the e framework
for reasoning about arithmetic will want
to actually represent it as this
equality and vice versa when we need to
know about a conjunction effects in E
will won't be able to represent it will
have to represent it using X this makes
sense to people so the problem is is
that I've carved out to different
languages and they don't know about each
other so the goal of the next few slides
and the talk is to show you how we can
actually put them together to do this
we're going to take our two two
representations of syntax here are
propositional language in our arithmetic
language and we're going to factor them
through simple language do you all might
not recognize something like the lambda
calculus so here the same thing on the
same thing holds so underlined blue
terms are constructor names here this at
sign means application so this is the
application of e1 to e2 we've got lambda
terms we've got bound variables all
right now we can take the common or the
specialized parts of each of these
languages so here logic is talking about
the type prop and it's using these these
expressions and in true
and the arithmetic language is talking
about natural numbers and prop and it's
using these constructors plus and equals
and we can kind of use those so these
bees will will be used as the values in
this ich instructor right and the and
the tea is up here will be used as the
values to these constructors so if we
want to represent a hybrid term true and
E 1 equals e to we can use the injection
of and from this language the injection
of true from this language applied to
the injection of equals from this
language and then e 1 and E 2 however
they might be represented who have I
lost yep I would like to introduce a
different background theory say i would
like to throw in bit vectors yeah so so
so there's some schematic approach to
throw in your theory so like very
laborious for every new addition um it
is not trivial but it is much easier
than it used to be and i'll tell you how
that works instument what two theories
can you not put together there are no
two theories that you can't put together
but there are slight limitations on how
you phrase these things which is what
I'm about to talk about all right I mean
those two shared st. Paul yep who says
that's really the same problem and I
mean you could have a type that the two
different theories really treated as
something different even though it ended
up with the same name sure you so max
seems wipro on one side and some other
actions with each of them are by
themselves consistent but not altogether
yeah so then you'd have to use two
different types and the key thing to
notice is that we're not going to when
you bake in those axioms you'll write
those axioms about prop for example and
then
the notion of property in both of these
cases will be the same if you say oh I
quantify over another type T and I have
assume these axioms tea and prop aren't
the same the restriction to know that
I've seen in herrin through the what
you're doing yeah we can't handle
non-dependent types sorry we can't
handle the pendant types of the moment
that's a limitation of the system there
is there have been several reflections
of cockin of Galena &amp;amp; khakhra yes push
handle both and so yeah so I think this
framework is a little bit is geared more
towards computational reflection as
opposed to other other ideas where
you're talking about kind of just
reasoning about in a meta theory
web system was for internalizing of
NT does using a computational reflection
so some pretty much on the ground on the
other side of cranky proves that ok well
I I'll chat with you offline about that
thanks all right so we talked about the
syntax let's talk a little bit about the
denotation function so some people were
scared when I used to when I showed this
to them so think about the prop D
function this is what we saw before it
was pretty simple it turns out that the
type language here is essentially
exactly the same language so here we'll
have an environment of types you can
ignore the 0 but that's a universe you
have an environment of types and you get
a type a syntactic type and you've
returned a type so this is a denotation
function for type when we want to do the
denotation for the the expression of the
expression language we can think of it
like this really what's happened is we
put another language on top this
language of sorts s
and sort has this very trivial
denotation function because there's only
one constructor and it just always
returns type 0 so if we wanted to expand
this we could say Oh type D takes an
environment a type and then a sort and
returns a value of type sort d so here
we're using dependent types to say the
type the return type of type D depends
on its argument s sort okay so using the
same idea we can talk about expanding
this down and talk about the denotation
of the term language so we have our
environment of types which will need to
feed to type D here we have an
environment of values that's going to
represent these X constructors we have
an environment of variables for these
bound variables will have our expression
and then we'll have this dependent type
here's this is the type I expect out of
this so I can ask for the denotation of
an expression at prop or I can ask for
it at NAT or I can ask for it something
else and here i use this maybe to note
that not all terms of all types right so
i might not get a value out that's fine
alright so we've built a new syntax
that's extensible we've built a
denotation function for that how can we
start reasoning about it so here we have
our old proof function and here we've
got our new proof function where we've
just basically said oh you know we used
to match on true now we need to match on
this opaque symbol and we need to figure
out what index we want to say is going
to be true so there's going to be some
index in our environment that means true
and there'll be some index in our
environment that means and then we'll
stick that there this proved err is only
going to hold true for some environments
it's only going to hold when this list ?
say it's 0 actually represents true in
the environment so I can't universally
quantify overt ES and FS here right in
particular this term isn't even well
typed won't even allow you to state
this because here the denotation
function is taking this x ? and the
denotation function applied to that
isn't the same as prop they're not
definitionally equal and this is a
problem that will have to solve because
we want to talk about our programs so
the idea is kind of simple if you were
going to do this the most the simplest
solution would just be let's talk about
what constraints the logic language
needs so let's focus on logic for a
minute let's say we require that Prop is
at position 0 in our in our environment
so we can change our theorem to say oh
you know TC logic here this guide the
constraints in that satisfy or TS
satisfies those constraints so this is a
way to say i don't quantify overall
environments they only quantify over
environments that satisfy these
constraints now this works great and you
would think it would be wonderful we can
substitute these guys in but we still
don't get a definitional equality here
we need to inject a cast that tells us
using this fact I can prove that the
denotation of x 0 is actually prop and
it turns out that reasoning about this
cast and an intentional type theory is a
very difficult thing right so how do we
solve this I take a step back well think
about an alternative formulation so
we're going to talk about this function
apply see which is going to take an
arbitrary environment here tau 1 tau 2
just represent arbitrary types or when
we apply some constraints to it we're
going to get out of a new environment
that's the same as the old except it
must agree everywhere that logic makes
that this constraint makes a requirement
here prop this will say the first index
of this environment must be prop so
instead of stating that equation alee
using an equation we're going to state
it by updating essentially you can think
of this as updating the the quantified
environment so here instead of talking
about TS
and giving a constraint on it we're
going to construct a new environment TS
updating this old TS prime here and
we're going to apply these constraints
now when you take the denotation
function of x 0 at the type level you
get prop definitionally because our
environment is actually prop cons done
with some other things so we don't have
to worry about any cast we don't don't
worry about reasoning about any casts we
can do the exact same thing at the
function level so previously we would
have had to specify all of these terms
with casts in them but now this term
which is a dependent pair the syntactic
type and the value of that syntactic
type this is well typed now because the
denotation of x0 is actually prop and
that's important because true has type
prop and we need those things to match
up we don't have to cast anymore so when
we do this in the function side we get
the same property here it looks exactly
the same and we end up being able to
prove this theorem as opposed to not
being able to prove this theorem which
will be important because we want to use
it obviously all right so semantic
composition how do you compose these
theorems if I wrote one theorem and then
I wrote another theorem and I want to
glue them together how can i use both of
them at the same time so let's go back
and think about our logic language right
prop here was required to be a position
0 in our arithmetic language we have
required profit position 0 and numbers
at position 1 the prompt case is a very
special public engaging your cherry
sounds there you could have hardwired
and I hate some other way but when you
want to combine various then you need to
violence to match up that's really why
you have yes this extension
mechanism to make things work yeah in
particular over here if we fix these
environments in any way than what we
won't be able to use it in an extensible
way right so the key thing really is
this slide where we're going to talk
about how do we take this this proof
that this logic prove our is sound and
this proof that this arithmetic a look
then this arithmetic prover is sounding
how do we glue them together and use
them both at the same time right the key
thing to note is if we know we have
constraints we can talk about the
composition of constraints so just think
of this operator here Circle Plus that
computes a new set of constraints given
an old set of constraints or two old
sets of sets of constraints and we can
you might notice this circle plus is
actually associated associative and
commutative all these nice properties
hold of it and that is going to be
important for us in particular see say
this so here here we're on the left we
were talking about apply see of this
logic of the constraints of this logic
right so we can think about that here
apply see of the constraints of that
logic if we sorry think about it here
apply see the constraints of this logic
so TC 2 is logic here TC 1 is arithmetic
so if we take apply see t see a riff and
stick it in 40 s prime here and we do
the reduction in will note that we
actually get out this term prop and and
then this tale of the tale of TS right
and that's exactly the same term we get
out if we flip these two things so when
you partially apply apply see to some
constraints you do that and you compose
that with another partial application if
those constraints match up then those
two terms are definitionally equal in
 that means you don't have to reason
about casts and it means everything kind
of falls out naturally so if I stick
apply c of t see a rhythm here and if i
stick apply c of t see logic in here
then these two terms have exactly the
same types modulo the fact that here i'm
talking
logic and here I'm talking about a riff
and so I can conclude that logic equals
true and the wrinkles true implies that
both of these functions are true all
right so i can use either one whenever I
need it the same holds for function
environments and the equivalent thing
happens there when I compose the type
constraints and when I can compose the
function constraints I get the
specification of both of these and all
of these things are typed without casts
so what I've shown you so far is what
lets you take these each individual
provers and glue them together
reflectively so instead of justifying
going into and out of each logic each
prove herb now I get one step in a
particular I can also have my logic
prove ER call my arithmetic prove ER and
my arithmetic and my he proved or call
my arithmetic proved ER in any way I
want so what I've building is something
like an smt solver where you've got all
these communicating theories that can
talk to each other and the reason I can
do that is because I don't have to go
back down to El sorry go back down to
witnessing proof terms that's important
so you know there's a few more things on
this that i'm still doing with old
proofs it turns out that we can
represent quantifiers it's not too
difficult to see why so I've got general
purpose binders here and i can convert
general purpose binders into quantifiers
just by using a function here like XCX
for the existential quantifier this has
interpret this next lambda abstraction
as an existential as an excellent
existentially quantified variable all
right so there I can represent this term
there exists an X subtype NAT such that
x equals 0 and in syntax it's
essentially represented like this right
so now I've been able to do reasoning
about quantifiers and reasoning about
heaps and logics and arithmetic sand
anything you might want in there but
there's one kind of nasty little bit in
here these maps now I could write
approver
maps but then I'd have to also write
approver about lists I'd have to write
approver about every potential data
structure that I might want to reason
about all my abstract data types I'd
have to write my own procedure to reason
about them that's really annoying
because we have a lot of them yeah so
what we're going to do is going to
derive these things automatically I'm
going to show you how you can build
reflective procedures like auto or auto
rewrite which will allow you to build
these guys from standard lemmas
right so if you've used before you
may think of okay i could write these
theorems about about objects like
sets or maps right and I'd prove them
and I wouldn't know anything about
reflection and then I might want to use
them in my reflected procedure so it
turns out that reflected procedures
can't call LTAC so you'd have to write a
reflective procedure here like prove
that would do a bunch of matching here
so in Union here would be implemented
kind of by this first rule you can prove
this if you can prove this and then you
have to go and prove this theorem and
that would suck because every single
collection of lemmas you'd have to dump
into this proved ER and you have two
bright a proof about it we don't like
writing proofs despite the fact that
we'd formal verification so what can we
do we've got a very generic
representation it turns out that we can
take this step we can take these
theorems and we can construct hint
databases so just like you would use in
something like auto or auto rewrite
those are based on hint databases
collections of lemmas along with their
proofs and we can build those objects
syntactically and pair them with their
proofs all right so now we've got this
galena term which represents all of
these llamas and so now you can get
someone like me in my thesis to write
this really annoying function auto proof
which is the implementation of auto
reflectively using this and so as a user
you write these three theorems
check them in a hint database and then
you say Greg were you worried about this
I'm going to call your auto prove and
whenever you want to use that you use
the auto sound the soundness of that
theorem and you get generic reflective
search for free so you don't have to
write your new your new decision
procedure improve its sound because I
built a generic one and you can just
instantiate it with the facts you care
about so all of this is Galina terms so
if you think of this as the syntactic
representation paired with its actual
proof object what's driving the search
proof search so is there some procedure
that actually integrates candidates yes
this auto proof is does exactly what the
auto tactic doesn't which
essentially looks at the goal checks to
see if it unifies with this term and if
it does then it applies this lemon tries
to prove this search so smart disease
does it use backtracking or the
exhaustive blind doesn't learn something
is from the failed attempts it is
relatively simple at the moment it could
be smarter and it could be faster so
improving the performance here is it is
feasible there's a famous failure of
this type of automation in a project was
called R Omega reflective Omega and
which ran much slower than actual Omega
yep because essentially interpreting
interpreted inside the theorem prover
something that could run natively in the
camel implementation a very good idea
yeah so nowadays we use things like vm
compute and when 8.5 comes out we'll use
native compute so that will essentially
be compiling this term to assembly
and running it on in x86
call theta revelations which aren't the
greatest yes this will be a this will be
a question of how well can you do native
compute how well can aid of compute work
I think is the question there yeah this
search procedure is something that lean
is implement using this whole
unification engine yes so we have to do
unification to do this right just like
we've just been saying in order to know
when this theorem implies we need to
know when the goal unifies with this
term right and so that's what we're
talking about right here so we have to
solve this unification problem if we
think of the boxes here as representing
? or meta variables solving this
unification says what is the
instantiation of those they'll make
these two terms equal the difference or
as you point it out there's lots of
things that we can do in this but now
unification is a term it's a
function that we can extend kind of an
arbitrary ways so has a built-in
notion of unification modulo beta ADA
etc we can make ours programmable it
allow us to do things like unify X plus
y with y plus X now people have pointed
out that this quickly becomes a very
undecidable problem and you notice a lot
of the things we've talked about thus
far undecidable problems in general so
you might say well maybe we don't want
to reflected procedures to try to solve
undecidable problems but whenever you
try to verify a program by dumping out a
smt formula and passing it off to
something like xiii you're not really
probably guaranteeing that you're in a
decidable theory you might be but often
people will just say well i got this
verification condition let me give it to
xiii maybe xiii will solve it and often
it does right so we're using the fact
that xiii is better than its able to
solve things in undecidable domains but
not everything obviously so
we're going to try to get lucky and we
and we can use things like facts from
the context or knowledge about things
like plus to unify these things and this
gives us a nicer cleaner notion of
semantic unification that we that we
would like to use when we reason about
kind of higher abstractions the
doesn't know about natively further our
unification is type directed it turns
out you need types in order to write
this theorem or in order to write this
and prove it nicely but you can also add
extra facts that depend on types so we
can unify any term of type net of type
unit with TT or any two terms of type
unit with each other just knowing the
fact that the only value of type unit is
TT so using all of these things we can
put all of these things together
reflectively we've gone from the
beginning to the ended one reflected
procedure that we're going to call one
time and get the whole proof out we all
to prove an entire goal like this just
using that one procedure all right so
various questions have arisen throughout
the talk how do you handle the pendant
types and things like that so we sit the
talk presented something like the lamp
like the simply typed lambda calculus
down here when you want to do various
extensions of this like polymorphism you
can do that in the framework right now
we don't do it in the completely generic
way but you can basically add type terms
to each and say that each X sub ? is
actually like an ml typed term a type
scheme it has a type scheme and you're
required to instantiate the type ski and
each time you want to use it right so
this isn't general polymorphism but it's
it's it's nice for what for what you get
in something like ml you can also think
about type functions things like list
currently you mon amour Phi's them with
things like universe polymorphism things
like that then you can
you can build those into the system as
well alright so that's enriching these
different languages the type language in
the term language with new constructors
dependent types are a lot harder because
you might say like well what I need is
my term language to be indexed by my
type language my denotation function
here needs to mention the denotation
function above it and so truly dependent
types would require a cycle here we can
approximate that by building additional
layers down right so we can say you can
depend on things which don't depend on
term you can depend on types whose types
don't depend on terms but you can't do
three levels or four levels if we can
make this generic then we can build this
arbitrarily arbitrary levels down and
use that as a kind of an approximation
you think of this as the different
levels of the universe hierarchy in some
sense so those are the three primary
axes completing the lambda cube
obviously depends on all of those things
and putting them all together in the
same system so related work there's been
a lot of work we've people have
mentioned some of them so we've got
things like our Omega and AC tactics
things like ring in field these are
different reflected procedures which
work on subsets of domains so for
example our Omega works on numbers field
and ring work on theories of fields and
rings their specialized to work on those
procedures they're pretty good but they
don't compose well so I can't take my
reasoning in can't combine these two
things in nice ways if I want to reason
about fields and rings of two different
types and how they influence each other
I can't do that in one reflected
procedure I'll use ltac to glue them
together we've got things like posterior
or simulation which is about improving
the performance by recording traces
through reflective procedures and that's
complementary work to our own other
systems like very ml new pearl and
logical frameworks like LF have
different
approaches to this type of automation
computational reflection was I believe
first done in New Perrault a while ago
it's a little bit easier in that system
and in these other systems are kind of
built for reflection because for example
new pearl is extensional we never have
to reason about casts casts are always
free or cast by quality proofs are
always free but they pay the price of
undecidable type checking or carrying
around proof terms and also things like
restrictions of reduction to you can't
reduce underneath binders or in open
terms recent work MTAC is a new tactic
language for it's still a tactic
language is still building proofs and it
still suffers from the same problems you
can an LTAC though it is a much nicer
kind of formulation than l tac SS
reflect is a similar notion both of
these are extra logical so they're
building the proof terms but there are a
lot cleaner than kind of what you might
think of as the ltac solution now or you
might do things very manually and so
I'll conclude with with kind of how I
started the system is all implemented in
something called mirror core it's
available online all in it has
these three applications at the moment
and we're looking at extending it a
little bit to address some of the
complicated issues with the charge
program logic and and the sea program
logics that people are working on that's
all I have thank you very much
is your phone working if you say
formalize some fragments efficient
intuitionistic logic you've shown how it
can be applied by interpreting your
lowercase propers imma cake topper
what if I want to interpret any other
different players so he flips or
something and I use the same clueless or
for those yes so I guess the charges
yeah so with charge we've built
essentially a tato solver at the moment
so tato is just tautologies we've built
it over an arbitrary logic so you give
it an instantiation of Esper's I logic
statement or an axiomatization of
intuitionistic logic and our ver will
work over that interpretations of the
props I in the same system and some
places you want to just use to balto
with with an interpretation in top and
then some other places you want to
interpret it in a different yeah so
there would be two different instances
of of that proved ER so applied for
example at the prop type or applied at
the nat arrow proc type of the he para
prop type and so because you get kind of
the same because you internalize all of
that you can apply it at different
levels you can we can worry about
injections of prop into for example your
he pero prop logic and things like that
so all yes all all that works out
I'm sorry paradise rats a quark thing
from UCSD so the sections coke verified
corn then they prove cookie secured
secrecy integrity address bar knows all
this stuff so in terms of whether this
simply question if you're not familiar
with quark so what does your brothers
are doing what have you proved about it
um so I haven't built a browser we built
a web server um so in that sense we do
the server and they do the client so
what can I tell you about the web server
sub the question ok so the so the web
search so we actually built to web
servers what one was in why not and one
was in in bedrock the bedrock one is
just a kind of a standard ground up web
server it uses cooperative
multithreading and features like that to
serve essentially static web pages
essentially it's just proving memory
safety the web server is written in
bedrock oh whoa I end up there the web
server is written in bedrock so
essentially assembly language all the
way down it has a few axioms for send
and receive network communication but
that's it the top of my head I don't
know it's built on quite a few it takes
about 30 35 minutes to compile so pretty
good feel free or
subtypes of predicates what type of
thing you mean there yes so do you mean
like propositional and implication so
what you mean by subtypes here yeah so
you know if you want to prove p is even
and you have a proof that p is a
multiple of four for example right there
p as even as a subtype oh it's already
ya say p is even p is for implies PSP is
even or p is a multiple of four implies
P as even you can reason about that
within the system there's there's no
limitations in that regard but you have
to code it yourself you don't get
anything for free specifically yeah
successful for free thursday you're kind
of like doing like hall-style reasoning
and I'm got any I'm taking the whole
theorem prover we haven't gotten prove
terms yeah you're just trying to get
away from the cost of that what are you
doing in addition case you're kind of
justifying the actual tactics yeah so in
a sense yea hall hall is built on this
notion of will kind of use the type
system of the metal language to justify
like and use parama tricity to say that
we're not going to build any bad proof
terms this is saying kind of something
similar will justify this this proved ER
and then we won't have to build the
proof term we are technically building
the proof term because if you took that
soundness theorem and you said actually
compute it down there would be a proof
term there we're just saying you don't
have to look inside of it so if you
didn't want to give away your fancy
prove er you could imagine just opening
up that proof and saying compute it all
down get rid of my terms it should go
away but be ugly sometimes people
interested in counter examples two
theorems that they should do not hold
this something that you could produce a
counselor as a framework
that something like yeah say this
framework can do a lot of things you
have to do it yourself though right so
this framework could generate
counter-examples you could use this
framework to build a representation of
different theories and you could ask
instead of for auto prove to say return
true if it's too if it's if it's
provable you could say return a model in
which this is untrue right and so you
could do it but because you're not
really caring about the proof so much
it's probably not the best application
for it so I think that you know if
you're looking at debugging theorem
statements and things like that things
like SMT solvers much better at that
it'll it'll work nicer but work very
easily like out out out of the box but
if you're really interested in having
that final piece of assurance all the
way down right then something like that
either having something like XIII
produced proofs or building something
like this that's verified and so you can
get that final foundational proof that's
where something like this is really
useful so I guess your motivational it's
not off with was to continue to move
move the work out of I guess L tacked
into these these and provers you know
for efficiency to make things go faster
so can you come up with any evidence to
doubt that that worked you know the
proofs actually faster than if you
didn't otherwise yeah so I don't have a
slide on performance unfortunately so
when we did when we did bedrock the
first version of bedrock was a
simplified assembly language and it was
kind of fast enough in LTAC when we
moved to the new version of bedrock
which is more realistic it uses bit
vectors and kind of more interesting
instructions things kind of fell apart
instantly so we were able to
symbolically
acute like up to three instructions at a
time right this system will symbolically
execute as many instructions as I've
been able to give it instantly right so
that's certainly a win bedrock is
currently architected in a way that it
does reflective reasoning followed by
some LTAC reasoning followed by more
reflective reasoning and going into and
out of is where we pay most of the cost
now so the reason why the web server
takes 30 minutes is because we actually
go into and out of reflective procedures
multiple times you know 15 30 times
every proof obligation and so when you
have a lot of proof obligations it's a
lot of going in and out it's a lot of
these theorems kind of stacked on top of
each other so using something like the
work here and moving more of those
things reflectively we think will
drastically improve that using M seconds
to the veil sack for doing that
transition help house um I don't believe
that it would help you're still building
the proof term huh dude atrocious
performance of unification in Concord
probably killing you it's having
variable assignment beaker graphic in
the size of the context yeah so um more
like a better proof engineering of going
into and out of reflective procedures
would be helpful yeah and it would cut
down on some of that time the full
incision is there any reason why we
shouldn't just throw away also and torso
and be placing with your reflective
versions she is that future of cop I
think it could be a future of auto
and aunt ro and things like that we'll
handle the pendant types using higher
higher order unification there are other
issues in that when you're doing things
reflectively you have to be very very
ice about everything that you use so for
example unification in is modulo
Delta reduction so unfolding terms I
don't get that for free in my system
because I'll just have this opaque term
plus and I'll have to know what plus
equals so if you define your own
function you almost have to hint to the
system that says my function actually
has this body right and that is
something that you're not going to get
reflectively it's something that you
could ask yourself how could we do it
reflectively I think there are there
might be ways to do it but you're still
talking here about kind of increasing
this trust a little bit so yeah you
don't get things like Delta reduction
but you do get you know plus reasoning
higher level plus reasoning it's
programmable and if you equality amongst
different series yes could you run into
a few so the different theories are
somewhat user-defined I guess yeah they
could have different notions of equality
I guess to be handled they do have
different they could have different
notions of equality it will it will show
up in your proofs for example if your
reasoning about a settled so your reason
about to settle it in sets right the
quality on sets is the set notion of
equality not liveness notion of equality
so Sara Lee there your prover would have
to talk about I I like your the proof of
your prove herb would state I prove
settled equality for sets I don't prove
liveness equality and it's up to you as
the client for example to say here i
only need Seto inequality on sets so I
might need to say the function that I'm
passing the set to respects settled
equality and therefore i can use this
procedure to reason about it but if it
doesn't respect so it's settled equality
then I can't use that and so
that's where you're doing things like
when your reasoning about said aways or
things like that you have to reason
about whether functions respect those
settings the quality is another
predicate it's it has the same you know
it's as specialized as is in some
sense you could replace all of the
notions of equality here with a type
directed equality that says like oh you
know functions preserve equality's and
things like that but it's not done in
the system all right thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>