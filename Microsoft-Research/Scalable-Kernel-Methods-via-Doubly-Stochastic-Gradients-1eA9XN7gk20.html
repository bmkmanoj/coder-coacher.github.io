<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scalable Kernel Methods via Doubly Stochastic Gradients | Coder Coacher - Coaching Coders</title><meta content="Scalable Kernel Methods via Doubly Stochastic Gradients - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scalable Kernel Methods via Doubly Stochastic Gradients</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1eA9XN7gk20" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
is my great pleasure to have a professor
la song Lou is currently an assistant
professor at a department of computer
science and engineering school of
computing georgia institute of
technology he obtained his PhD in
computer science from university of
sydney in 2008 and then contacted his a
puzzle research at a squirrel of
computer science konsi mellon university
from 2008 to 2011 I'm it's basically
research interests include and
nonparametric and current methods
probabilistic graphical models and
dynamics over a natural process and also
its locations he received and there's a
career award in 2014 please okay thanks
for coming ok and I'm going to talk
about kernel methods then how to make it
scalable to millions of data point ends
of millions of data points so our
motivation for these work is we try to
catch up deep learning where the colonel
Methos the kind of motivating problem is
this gigantic image classification
problem you have well millions of the
labeled examples and then the number of
classes is also very big so 1000 classes
is just a subset of a classes there's
even more and then what we want to do is
to take this image and predict its
labels the label can be just mushroom
things like this and then what is the
show here are the labels particularly
some algorithms you might predict the
top of five labels and some labor has
high probability ok so you see this
prediction is actually provided by New
or nets and it's doing a pretty good job
sometimes when the the correct label is
not in the top places but it might be in
a second place for instance like this so
the best-performing algorithm I
mentioned is the deep neural Nets so
it's a very complicated model it has
multiple layers but
the layer of structure in such a way
that you see patterns in the in the
first five players it's this convolution
pulling a highly structured process to
uni I'm going to explain a little bit
more and then in the next two layer is
going to be these fully connected layers
and then trying to perform some kind of
nonlinear transformations in the last
layer is going to be these multi class
logistic regressions so what is in this
convolution amex pulling layer is
performing somewhere its importance so
essentially the convolution layer is
just taking some common lucien corner
that's different from their positive
semi-definite color i'm going to talk
about but it's also the colonel so you
take these more templates and you had
some number to it essentially you
perform weighted sum or over your
original image and move this window
across the entire image you'll produce
another image and then we're often new
and that's you also apply this operation
max after you finish the convolution
that's called rectified linear units you
also performed only near fresh or type
of thing okay after you perform this
convolution so essentially the first
five layer of performing this type of
operations you come off the image with
some templates and you perform that
nonlinear operation and also do max
pooling I'm going to explain the max
pooling is something even simpler it's
essentially reducing the resolution
image and you just take a small patch
and look at the value in this small
patch and you just return the maximum
value in that small patch that's the
operation by their operation typically
you reduce resolution image you
interleave between it's convolution and
max polling for many layers five layers
it is the best performing model and then
after that you have two layers of these
fully connected layers so it is just
taking the input image your vector and
perform the weighted combination of
everything in that image and then
produce an output pixel so you also
apply these the max ratify linear units
this operation they do it twice in a
hierarchical fashion so it's a highly
mysterious right and then afterwards you
are put the logistic regression on top
of it essentially you try to learn this
weights w
so you do a weighted combination of the
input vectors and then and you push it
to an exponential function that gives
you something related to the probability
of particular labor why okay so this is
the model and then um yeah so can you
explain just again this max pudding
operation with rats so next fooling
operations just just take a small patch
in your original image and then just
return the maximum value in that small
patch and then filling in as a new pixel
and so you were saying that I'm doing
convolution and then max pooley yeah you
might interleave between them so when
you say interleave you mean that one
layer is doing so the mark max putting
together supposed yeah so one day I mean
that the convolution and next pulling
convolution started followed by next
point yeah sometimes they may skip this
step so by model selection whatever I
don't know why so it's such a highly
structure more doing lots of max max
okay and then in the end you are pushing
through these multi layers of this multi
class what used to be a question it's
highly structured okay and differently
it has different function and but the
goal of this model is to do this
gigantic non linear classification task
and this is just a detail about the
model the perimeter so when you have a
convolution you need to specify these
filters kernels right and this is the
way to sort of specify those parameters
using convolution max pulling things
like this and you have this original
image which is 224 by 224 and three
color channels ok so the size the condo
sites are going to be seven by seven so
you slide across this image and then you
might jump you know rather than sliding
window but do you jump to ok and then
you use the 96 of them you get an image
of 110 x 110 296 after the first layer
convolution and you do max fooling
reduce the spatial resolution you get
something like a few
d 5 x 55 but putting in a three by three
window so that's how they specify this
model and then you do convolution again
five by five window just read to and use
256 or ten you get a smaller image you
do max pulling convolution again
convolution convolution okay here they
skip this maximum pulling operation and
here is another convolution and max
pulling and pushes fully connected layer
and fully connected layer and the
multi-class a logistic regression okay
so if you look at these very complicated
model deconvolution actually are you
know in consists of five layers but
total more parameter in that convolution
layer is only 3.7 million parameters
majority of the parameter actually lies
in the last three layer for this fully
connected layer and the multi-class
logistic regression and it's 58 million
parameters so essentially given is the
jagged image data that you want to learn
all these at 60 million parameters okay
okay that's the problem yeah how they
came up with this magical number is why
actually they have a nice package you
can just specify the configuration for
these layers and they came up i think by
seven and not a model selection i think
model selection over 10 years or maybe
20 years that's my my impression yeah
you can experiment with these
configuration you know by specified
smoke configuration file they just the
hinges search for this architecture yeah
yeah I think trying but this convolution
makes sense in some sense with training
image feature and I don't kind of stand
a fully connect earlier understand these
multi class the logical question sir so
we all try to understand when we put all
these things together what's going to
happen right why is it doing so well
yeah so which I don't stay yeah that's
the number of you know the number of you
yeah different about futures 96 view
that that's why you get something image
like 96 colors yeah okay basically you
produce sign language but
you increase the number of yes that's
right that's right that's right this
place yeah so when you do convolution
voices in this layer 5 by 5 filter you
can walk across different color channels
so you some attention look across 96
dimension you know different color
channels so it's a it's something like
this yeah so they managed to get is
working okay and then working so nicely
when you try to learn these parameters
the algorithm that has been used is very
simple the driving force behind it is
really these that's all care to creating
this end you take a smell batch of data
and computer gradient and you are you
know use the chain rule or derivative
and you get update for all these
parameters yeah you have a gigantic non
convex optimization problem you just
take the gradient of the objective we
respect the primary and use the chain
rule to update it but when you try and
change it you also join in something
called the virtual images you are not
just using the original data sets of 1.3
million data points you actually
generate some image by randomly crop the
image or mirror image something like
this do some transformation on image
generate something like the origin of
data set but slightly different okay so
along the way the original data says 1 /
3 minute you actually generate up to 100
million data points and it takes a week
and GPU to change its model you get this
performance if you predict the top one
they've used the top one label and then
compared to the the actual label and
then the error is like forty two percent
ok that's the top one classification
error yes is a solid chocolate yeah so
I've explaining and this point the menu
adjustment or running right the way they
do it is they said this learning rate to
be some constant and then after some
point and CDs capture a curfew flattened
said you drop the learning rate and then
have a sudden decrease in the performer
and then after some time flattens you
decrease until after three of this
adjustment you don't see an improvement
more okay so this is some
think that if you don't try it you won't
realize it how they actually trade okay
um yeah it's Holly manually adjust so
you just watch this cover you can
basically asked the man you saw the
change I guess okay if you want to do it
automatically I chain chaining error is
going to be like a 30 or 20 yeah there's
a huge over fit in their top five
nowadays that the best platform model
can be below ten percent it's very
accurate it's close to human judgment
already okay so this is the state of
these model and then actually for this
model you create a lot of fun I'm soft
question as well of course you perform
so well in many applications speech
image the questions of this very special
kind of architecture maybe there's some
special characteristic the data which is
particularly suitable for this type of
architecture it's not clear what what
kind of assumption you can make it about
the data such that this frozen pipe
light is really good so I haven't seen
any principle if your reticle work for
this question so and also Wi-Fi player
convolution max bullying can we use
other architecture operations to extract
these features it's not clear yeah
whether there's any alternative and
simple operation which you you can also
use and then of course why it's relay
are fully connected non linear units
yeah why do you need is nonlinear
transformations why not for and is there
a way to do it in a shadow of fashion
and can use alternative nonlinear
classifiers if you just wanted an all
new classification so in this talk I'm
going to essentially explore these
crutches I'm also we're interested in
the top two but now I don't have
anything concrete for a top too so I'm
going to use color Martha trying to
explore these two questions at a better
i'm going to try to replace these three
layers fully connecting all units and
the multi-class logistic equation
bicolor methods and see what I in able
to achieve a comparable result if I able
to do that maybe this really is not
necessary okay you can just replace it
by a traditional nonlinear nonparametric
method and and what may be really useful
is convolution max pouring layers okay
yeah okay so essentially the
Commonwealth I me is this type of color
positive semi definite kernel when you
have it's essentially a function taking
two arguments so if you have a data cell
fixed size m you make these color matrix
a matrix has to be positive
semi-definite okay it's this special
type of color is different from this
morning colonel from Statistics Stephen
from this convolution color it's this
type of function kind of function so
essentially i'm going to estimate some
you know are non linear classification
functions right and trying to do multi
class known in classification i will i'm
going to restrict my classification
function to be in this space of function
the space of functions spanned by these
kernels okay i fixed one argument i
chase out the class of function using
these data points from our particular
space so when I have these positive semi
on internal are the nice properties that
the kernel function itself is going to
lie in that space and them are the inner
product between two kernel function is
going to give you this kind of value and
then you favor function in that space
and then you can evaluate that function
by just performing in their products
their dead space has some nice property
of it for instance some familiar kind of
functional polynomial kernels so this
color is definitely not in a smoothing
colonel family and a Gaussian kernel is
also a smoothing colonel in the inner
statistics literature so essentially for
this particular kernel function if you
have a function that space which is
linear combination of this kind of you
will get a function of these checkpoints
as you can represent highly nonlinear
functions just by linear combination of
kernel or sits on each individual data
points okay so this is just a
introduction of this type of color
functional space of function chased out
by these kernels and intuity you can
think about is kilometres transforming
the data for instance here is a good
binary classification problem you have
negative class and positive class you
want to do non linear classification
this kind of function is going to
transform this data to a new space
potentially infinite dimensional space
in that space you try to find in your
relationship in that space okay so many
colonel martha has this intuition behind
and many karma actually can formulate is
an optimization problem try to find some
function f in that space and you
minimize some kind of expected loss
error is the loss function and you have
some data x and y y can be the label
generative on some distribution you try
to minimize this expected loss function
and subject to some constraint that the
Narada function dark edges is found it
so an equivalent formulation of this top
optimization problem is that you move
the constraints to the objective
function and add these mu is something
like regularization parameter okay the
beta mu is related so you can choose
where different loss function you get
different algorithms kind of logistic
regression if you choose these loss
function you get support vector machine
you get a lot just two equations here
okay so this is D so you typically for
kind of mother you solve it in the duel
so essentially you use these so-called
represented fury you have n data points
you replace the expected loss by these
empirical laws and then you just the it
turns out the solution of the
optimization problem it's going to have
a phone like this it's just waiting
combination over the kernel function
apply on the chain of data points or
they are optimizing over some function
and functional space could be infinite
dimensional okay so if you have that
represented here and you can influence
if i plug in that into the original
optimization problem and then just
optimize over this offer instead okay
Alfre is in some RM sorry it's not f
it's just some finite number of offer
okay so is the you solve these
optimization problem but the problem is
if you observe it carefully you have
this kind of with some in the objective
function that means for each pair of
data points you have to evaluate its
kind of function okay and that creates
lots of the problems okay think about M
is one medium so if you want to evaluate
pairwise kind of function and
essentially you need to viewing the
entries in this big matrix and then one
minute the moment you cross this memory
yet tend to the cough and the
commutation is generally and square d DS
the dimension or the original data so
it's a huge so you had come up with the
way to scale this kind of F up so there
has a lot of after all ready for
instance some approach based on
lower-end decomposition of these color
matrix so you take these color matrix
you don't you don't compute this entry
you have a way to incrementally
approximate these color matrix by some
Lauren factors so this is not either
nice jobs nothing complete joy seeking
organization any other method usually
the computation of this method is going
to be linear in a data point t square in
the rank you choose and D the dimension
of data and the storage is going to just
mg so but if you look at the
generalization ability after you do this
after you do this the lower end
proximate and plug into your
optimization algorithm and you get your
function you look at the generalization
ability the best you can prove without
any further assumptions the
generalization ability comparing the
expected loss are produced by these best
function in the family and the function
produced by this low-rent decomposition
and then the generalization ability is
the difference is going to be 1 over
square root T the rank plus 1 square
number data points if you want to get
best out of your data you have to match
these two tins essentially and that
means that the G has to be the order of
em so you get an cubic
kind of computation here and then you
hear n square memory consumption again
okay so essentially if you actually in
practice you also find this is the
problem so you do some low-rent
incumbents you can fix to sell small
rank you find that that you learn a
classifier will always lose to some
performance comparing to the case where
you optimize directly with respect to
these food color matrix that's what
you're observed because in theory you
need to match these two turns up okay
recently people are more are looking
into this so called random feature
approximation of the kernel function so
essentially there's some interesting
relationship between this kernel
function positive semi-definite color
function and some random processes
essentially if you have these kind of
function you can always express it as
some kind of integral form okay you will
have some random verbal Omega it follows
on distribution P and then this kind of
function can be written as some random
function which is indexed by this Omega
apply on your ex and the random function
apply ex parte you this type of our to
in a prototype look thing with respect
to this Omega okay so you can also write
it right here as it is expected form so
for some of the kind of function you can
find this distribution P Omega in closed
form you can find is the Phi Omega and
closed form but you can also go from
this direction to that direction if you
you can pick whatever nonlinear function
Phi Omega ready for you you want it you
can pick some distribution want it and
then you just define kind of this way
it's going to give you a world where it
positive semi definite kernel okay so
you can go both directions but the
people has work out some coastal
expression for somewhere no kernel
functions for instance for Gaussian RBF
kernel suppose I at the forest like this
Delta is the difference between x and x
pi the the random function Phi Omega is
just going to be something cosine Omega
transpose times X plus tau and the Omega
follows
Gaussian distribution and tau follows
uniform distribution thank you actually
uniform distribution between I think 0
to PI sometimes yes okay and if you have
another Colonel Laplace distribution any
family of translation color you have
some nice solution yeah that correspond
distribution over Omega is going to be a
costly distribution okay if you have
cost aconitum correspondent distribution
is a Laplace distribution this some nice
relationship between them so comparing
these type of random feature
approximation approach to these lower
matrix approximation approach there's
not an advantage already okay
essentially what you do is you just on
Brendan parameter Omega right random
feature preventive and then you're going
to approximate this kind of function by
the average of these random features
instead of expectation just George's
Omega randomly from P Omega and
approximated by silence sample average
and the competition is a simpler so just
ignore the log P for the moment suppose
you have a deed there okay essentially
you just need to apply each one is
random feature function on each one of
your data point so you have T random
features it's going to t times D for
each data point and you have n data
point that's the operation so Nocti
there is because we're going to express
on tricks for efficient matrix vector
multiplication and get lochte kind of
scouting the memory is still other of m
times t okay so essentially you can also
view this matrix instead of using lorem
matrix factorization you just compute
you apply this random function directly
on your data point you get a number for
each one is random feature so again once
you have these the lower n factor a you
solve this two-dimensional problem and
again if you want to prove something you
will find that the generalization
ability is going to be 1 over squared to
t plus 1 over square m you again you
want to buy the balance the two things
yeah
I'm missing something on the commutation
but yeah given if I just want to write
this approximate matrix yeah I'm not
doing the true brand naturally i'm doing
the formation since it has size M by M
then its quality and squirt yeah that's
right so you don't explicitly
approximate entry in this matrix you
just keep these lower factor a and work
with this lower factor as if your data
from final and dimensional space of T
dimension so just you just you're going
to work with this yeah did this matrix
but my algorithm is going to access for
the entries right so your algorithm is
some terms of the optimization algorithm
it's going to spend an square on the
godless so before you run optimizer you
do this pre-processing and that
preprocessor actually don't need to
assess the entry in the matrix you
directly work with this data point the
same thing for our this nice Jamel
foreign company typically held you down
you don't actually go through every
entry in these metrics yeah that's how
you get MV square otherwise your you
can't know why n square operations ok
just use a few entries in these matrix
to come up with approximation so in this
case you directly apply this random
feature on your data points to get this
representation right ok so here you need
to match the truth so so how high is
this generalization valve I imagine
every day today like Nemo dementia
patient is so in that case you're lucky
this Laura approximation will work
really nicely and in that case if you
you incorporate that low rank knowledge
you might get better bounce in here i'm
not making an assumption about data if
you want to do something fun in
nonparametric that's the kind of bone
you will get yeah so yeah
with roland sanction possible you will
get better result ok better than
theoretical guarantee now with that
that's the problem and then why I'm
going to do is I'm going to look into
some scalable algorithm which has been
applied in many other places the
stochastic gradient descent ok and but
i'm going to add another layers of
readiness to this algorithm to make it
scalable for this kind of case so first
i'm going to show you is why traditional
stochastic gradient descent is not not
good for kind of another i'm going to
directly optimize this function in the
arc HS i'm going to use something called
the functional gradient instead of a
grating over some finite dimensional
vector ok it's just a generalization of
that so essentially you perturb this
function by some epsilon in the
direction with G and you look at a
change you can express their changes on
inner products and these this thing in
front of this G is going to be your
functional gradient for instance if your
function FX is here you take a gradient
with respect to these function them
using the reproducing property it is in
this space you can express the function
like this then when you take the
gradient cycle linea in the linear
function here you get you stun if you
have square nor in the RHS you get the
function Grady like this two times F
it's just you think about it as a vector
that everything seems to be where
natural so essentially for these
expected loss ok if you take the
gradient you applying chain rule once
you take the gradient of these loss
function with respect to these FX and
take waiting with respect to F again you
get is ton here you get this additional
ton here and this comes from the square
nor okay that's that's the if and then
expectation can be exchanged with the
operation of taking the gradient so
that's what you get so essentially for
many of these kanomata formulated as
convex optimization you can take
gradient
has phone like this and then what you
can do is you take a subset of data
points you take one day to find its
better case but you can take a mini
batch and then update your function
using these stochastic gradient computed
using individual data points okay so
essentially in the end you will find
your function is the weighted
combination of the data point you have
seen so far okay and then the number of
turns in these summations going to be
equal to the number of data points so if
you apply some standard answer for
stochastic gradient descent or mirror
descent and then you will get this type
of rate one on the square root e in this
case the number iteration is t its match
up with the number data point you'll see
so you get is this rate okay and but the
problem of these approaches you need to
remember all these trading points okay
so you need to be able to if you want to
evaluate this function in a new test
point you had to keep all training
points yeah you plug in your new test
point of this function and the colonel
value and then do a way to some of them
you cannot show away those training
points in general yeah this is t the
same as a team before the random sample
of keyboards it's not exact I try to
make them the same so this G you can
think about it as the wreck okay it's
not the same same same parameter but
they're comparable in some sense so the
rank there at the G he had a duration
here play the same row as rank in the
previous night slice yeah the comment
you need to remember all points just
introverted team points yeah yeah that's
right you need to remember T points so
suppose you want to get 1 over square
root M a generalization ability then you
need two essential remember endpoints
yeah yeah you should remember that
points you have seen so far yeah
so we're where is functionally banco
popular yes optional reading wishes her
vision directly cheap yeah it's more
like yeah where did you use here so
essentially i can actually plug in this
into the buff equation and then you can
check derive this way so for instance
you plug in this f here into these are
right and then your plus epsilon G and
then you put it you just look at the
difference and then divided by x y you
will get is it's just like you can think
about it just vector it's a back this
year so this one is the G is geez just
just here but your your direction of the
derivative it's just a part in front of
G it doesn't move the g yeah so for here
I just invited that's the party really I
or in here I just gives you the final
results I didn't go through that step I
didn't go through that step I just give
you the final result of techniques that
direction of derivative yeah in the
direction yeah in some some g direction
but it doesn't matter usually these for
this simple function that that direction
doesn't matter yeah i'm just trying to
send the red the red side again the next
slide yeah so you're saying that in
order for I need to chose choose t to be
on the order of em yeah the generals a
shy one yeah but then the number of
points that I need to remember is just
the support it's not going to be every
year so far it was the perfect to
mention that's nice you might get fast
solution solid off at zero and then you
you can yeah you you will show you
choose the colonel correctly only most
of the will be 0 so far for a simple
vector machine that may happen and then
hopefully the orders of the number of
superb actor is orders of magnitude
smaller than the the agitator sir but
the Ferrari regression for instance
usually the solution is dense and for
other other you know many many loss
function that this solution maybe dance
okay so and also for support vector
machine generally this
no guarantee that the number of simple
vector is orders of magnitude smaller
than the actual training points yeah
especially for this highly nonlinear
case you will find that that you
actually need lots of support vectors
yeah that's happened in typical in
practice throws in a linear right so
then if it's growing linearly then them
instead saying order so maybe the fact
is small but it's killing you right so
this is the key idea okay how do we do
it is the problem of not remembering all
these chaining points so just make a
second stochastic approximation so in
stochastic great anything you
approximately graded using data points
now I know this duality between kernel
function and its random feature I'm
going to simple some random features in
this particular just simple one but you
can send / mini batch as well you're
going to sample these random features
and approximate its kind of function by
these product of random random functions
okay so the advantage of doing this is
of course after you get the W stochastic
gradient you just move your function
after your function using these this up
w stochastic gradient and what you will
find that the final functioning gear is
a weighted combination of these random
functions you know exactly what phone
this function is and then you only need
to plug in your text X and evaluate that
xx on this random function then and do a
weighted combination you will get your
prediction okay you don't need to
remember all these points anymore the
reason why is you just need to
incorporate the evaluation of this
random function or these training points
into these weights alpha okay so in the
end is chained in NY or maybe it's
1,000,000 dimension but you just need to
evaluate in this random feature and it
will enhance a single number is
incorporated into this single number so
if you go through 1 million data points
you just need to remember 1,000,000
numbers offer
okay and then this random function you
know the form of it you know which
distribution are sampling from where
typically with some preference and
pseudo random number generator if I know
the seed I can always be in sin she ate
that ran the number okay I don't need to
actually store this Omega I draw off on
my distribution I just remember the seed
next time I would need to use that
particular random a feature I'm going to
redraw it okay using the same seed so
the algorithm is actually very simple
it's just keep updating join some data
points join some random features and
keep updating is alfre okay so the
algorithm can be summarized in this one
slice in the end the form of the
function is going to be weighted
combination of this random function you
have jaw so essentially your samples on
data point sometimes I ran the future
using particular seed corresponding to
the iterations okay then you can
reinitiate this random feature easily
when in test time in the car rental
raishin you already get a bunch of alpha
you already get a bunch of random
feature if you want to reevaluate a new
test point on this function half you
just need a passing the currents that
are offer you get so the evaluation of
this function is going to reinitiate a
random feature and apply the random
future in this new test point and
weighted by this offer you learned
before and then just accumulate this
okay and because the presence of
regularization parameter you see some
modification of all these other j4j
small equal to i it's just because
regularization you try to forget a
little bit about the otha you have run
before for the car now how you're going
to update you just use these w
stochastic gradient okay yeah vision
generation process and signal processing
something like matchups it was not me
yes so socially is doing more naive than
that confess to you are not doing
smartly intensive sample in this random
feature you're just doing it joining
from some random distribution you might
choose some Omega maybe most
monthly you will get the better maybe
convergence but yeah I don't know how to
how to do that you realize an alpha I is
very inspirational zero meaning that in
random direction is orthogonal to the
objective function then maybe you can
ignore that I reserve the sound great
yeah it's possible I mean yeah it's just
we try to just explore simplest worship
and this one don't do any big
bookkeeping and then you just keep
averaging all these random functions but
you can think about the extension of
this but join these random feature more
intelligently do bookkeeping more
intelligently potentially can improve
the convergence of this equation yeah
maybe I'm not fun but so are you
describing to us the original technique
of of the ratification scenes of wrapped
around me or is this an easy this is the
new thing so you don't generate this
because to me it's exactly what they say
so they what they did is they first the
generators run the feature and then they
optimizing the finite dimensional space
right but you can do that optimization
stochastically which is what people will
do in practice is that with yourself so
people don't do that in practice people
do that really generate then maybe just
one thousand random features you're
optimizing this one thousand dimensional
space and that's how people make it
scalable for these color methods and
then if they want to try a more random
features what would they do is they
regenerate another maybe 10,000 features
and we optimizing it so this one is
essentially generating this rena fish on
the fly so you're seeing using more
basil in some sensible wear a flag on
how to make sure from the same Omega if
you generate video-thumb like it was you
gotta go because usually when we
generally use a program except for this
and then you can always apply the seed
that to the random a generator so that's
why I need to keep checked off the seat
to make sure that every time is dumped
exactly the right the question is if you
do something like this is going to
converge now you have an awesome for
stochastic gradient descent now you
added the second randomness to your
circus equated the question is whether
it's going to come watch come wet
weather is converging to the same rate
with the same rate or not ok so we also
have some answers whether it is is
convergent and essentially if you do
this w stochastic gradient you will find
that the rate of convergence is the same
under some conditions so the next thing
you don't actually need to remember this
renifer what you actually remember in
the algorithm adjust these are frost if
you have one minute that Empire you just
remember 1,000,000 numbers that's it
okay education so you can apply this
technique convenient colonel right so if
i apply this to me to the linear kernel
problem with a lot of whiskey mr. Darcy
with a lot of small speakers so the
processes are going to assign a friend
earlier example densely related of Egypt
building a future but in that case many
people video because
that if I do a spaz be to their just
friending people feature every 30 then
that the chance I giving up it will be
very low yeah yeah if you do it there is
no advantage for doing it for linear
features and but the mapping of this i
went to linear case exactly your sample
these random these dimensions mini folio
random because if a linear kernel you
just do inner product right it's a sum
over the the product type 1 dimensions
and then the second dimension for divers
you can you need for example no problem
full of energy linear features in a
linear case you do not need to use the
cupcake trick because if you opted for
any features of the partial derivative
any feature you need to leave edit the
inner product anyway which is here it
was none right if you have another one
they sing the italian how for every
feature is correct do not be forgetting
it so these are just friends gonna treat
yeah there's better than me for games as
we've already you know in some case
maybe you need to single a lot of times
to give up but they're for linear kernel
i don't suggest you to miss Entropia
that this is really designed for non in
case yeah exactly yep the understand me
so the monopoly trick so in every
iteration you know you pick a new
feature and you take a single sample
yeah so if you if you generate thousand
features basically you observe only
thousand points you can make these
separate so you can take a mini batch of
data points that mini batch can be
different from the mini batch sizes
random features yeah okay so just make
sure that you don't recommend this for
years before another but it should sync
between Indian Union are not very
different say you could say so for your
original data you can just
yep that's right exactly so polynomial
Atkinson phone is already nonlinear so
actually these when the future
approximation for Parliament kind of
recently one very convenient equities is
tensile sketching exactly you do some
kind of a hash Rendon tension and then
here you can think about these Omega is
performance i ran the hessian of your
data okay um again you want to get the
generalization bound you will need them
you know the number of running Hashem
will have to match the number of data
point you'll see otherwise you're losing
some performance without assuming any
distribution for any any other without
any assumption of distribution of data
so the day always is just just destined
for looking am I mentioned about these
random future evaluation you get lucky
instead of D because for instance the
forties translation wherein Colonel you
have this random feature essentially
draw some Omega from some Gaussian
distribution you do it inner product
between Omega X if you have a bunch of
random future you have many many of
these Omega giaffone distribution and
you want to evaluate sensually these you
put is omega in the matrix you
essentially want to perform in a product
between this Doppler matrix and each one
of your data point okay so it's a mega
is a random matrix okay the call is
going to the role in this case the
column okay the corn is going to be some
random number jaw from some distribution
for instant Gaussian distribution and
you want the performance in a product so
there's some way to perform is inner
product in an efficient way so
essentially this is the some technique
called fast-food emotionally so you can
approximate is this matrix Omega w okay
by several highly structured matrix so
you can actually have a product hg PI H
B matrix the B matrix is currently
diagonal matrix where they uniformly
distributed
do it just entry- 11 on the diagonal and
then the probability of minus 11 are the
same okay and then actually is the hard
on my matrix where is structured and
this is a if you're your DS 2 is
something like this for is like this so
you don't actually need to compute these
matrix but the nice thing about this
matrix it allows you to very fast matrix
vector products and then the pie is the
random permutation matrix and then G is
just a diagonal of calcium and then H is
the harder my matrix again so
essentially you don't actually need your
these are d times T random numbers from
the distribution so what you can do is
that you just Joe this diagonal suppose
T equal to D ok diagonal of gaussians
and you use these the matrix this is
where your structure matrix to mimic a
huge rendon Gaussian matrix you can do
that and then these matrix allows you to
do fast matrix multiplication and you
can actually even from a guarantee that
the after you perform this approximation
and do the measuring vector product you
get pretty much the same results so this
is the same result from random matrix
multiplication and then you can actually
use it here to speed up the evaluation
of these random features on the data
points ok so this is just some speed up
for the speed up you can do this
transition very well so translation
wearing color is this Ford for some
other kind of for instance there's some
rotation invariant cargo you can also
derive the similar thing but it's not
not for arakata ok so you have to be a
kind of where you draw this Omega from
some distribution maybe stop cows in and
then multiply by these input data points
and apply some nonlinear function
afterwards then you will have these type
of results ok and then in terms of the
convergence we also provide some
analysis for convergence out essentially
what we try to do is to find some
function that we're producing
Hilbert space and without making these w
stochastic gradient proximate in the
function we get it's going to be
something like this weight is some of
the colonel apply on each team into
training points but we have make some
approximation and then we get something
like this okay so essentially suppose
you use this translation in work or no
then you're going to get a weighted sum
of sine cosine functions this function
themselves may not actually be in the
arc edges you are actually using
function in a lot of space approximate
functional catches and you try to show
the commercial in here we will ensure we
just show that when you have that
function f t + 1 X the function obtained
t + 1 iteration and evaluate some point
X and compared to this F star function
which is in the RHS the best optimal
function evaluates the difference is
going to be small so essentially you can
decompose these difference in 22 x 1 by
introducing these singular stochastic
condemnation ok just approximately the
stochastic a grading coming from the
data ok so you essentially can be
composite air in the Teutons that this
decomposition is on expectation by the
way ok one source of error is due to
this random feature and then second
source of error is due to this random
data so essentially for the analysis of
it we're going to analyze this to attend
separately so you turned out the second
turn you can just use standard analysis
from this mirror descend you just
generalize it to function in our HS you
can get is one number ki kind of
convergence ok for the first time we did
something special to this problem we use
basically the concentration in quality
for martingale differences essentially
if you compare these function in our
cages edge and this function outside
catches the difference is this ok the
difference is this ok it is some a bunch
of ECG is just the the function obtained
in each iteration so somehow this
sequence of v's consists form a
martingale different secrets and you
just
find some concentration in quality it
says that you also get one with you read
together you get this kind of rate 1
over T ray for the function square the
square difference of the function and
the four you can also get a
generalization ability which is one of
the square root tea okay so essentially
that's how you are getting analysis
working and essentially get the best
possible generalization ability for this
nonparametric estimation problem and and
the other is very simple okay the
question is whether he works or not so
we tried this way yeah so H is what you
get without applying this random feature
around features yeah this is what the
that is what the rectum is equal to H at
she is not H is H is this guy which you
don't apply any approximation to this
wonderful yeah at rakhana itself DSF is
applying stochastic witness and using
random data and also the random features
simultaneously you never revisit those
random features you just keep generating
new ones see so what's so the martingale
is just just the defense in the one
where is doing the one by one yeah the
difference is is in exploitation dear oh
okay if you look at a difference between
your choice so VI is that this term you
have these one using one render feature
you try to approximate this corner okay
conditioning or the previous randomness
the expectation of this guy compared to
this guy is zero because these kind of
function on expectation this random
future approximately on expectations
equal to that so conditional or previous
randomness because we have the previous
function here these expectations 0 and
essentially if you look at the
difference is the sum of a bunch of this
okay so wait why is the difference
between ft also we just easy data so a
difference and then take the expectation
for it
so these decomposition is in terms of
the expectation yeah we just look at a
squared difference between the function
value it's easier to analyze you can you
look at the absolute difference as well
yeah but we just look at the square
difference okay and then the second part
is just near edison okay and if you are
you had some property for loss function
then you get this kind of rate for the
for the risk expected loss this cave is
that thing over this cause harm or so k
is to k is k then up stepped up on the
top there on copper in some family
related to the kind of function is like
upper bound for the tunnel function for
instance you have a BF Colonel then that
one essentially there is using the
reproducing property you can express the
evaluation of a functional catches as in
the product between for the h and k xt
and then you use koshish wats that's how
you get the copper yeah so we're going
to apply this algorithm to this very
challenging data set you need to go to
maybe 10 million or even more data
points in order to get a state of our
performance so we compared three models
okay 11 model we call jointly chain new
net is the original neural net so we're
going to use stochastic gradient descent
and manually adjust the learning
registry times to get the best
performance and then we're going to have
the second model called the fixed new
owner it's a it's basically a sanity
check so we're going to take these
convolution later learned by in the
first model and fix it here I'm just
going to retain again just is a fully
connected normally units a multi-class
classification I can OC okay and in some
sense this model is the closest our
border we are also going to reuse
exactly the convolution layer learned by
the neural Nets but we are going to
replace the top part by this kind of
machine okay
so it seems that the this convolution
pulling is really doing something
amazing okay so we are going to replace
that and use the established casting
kind of machine okay yeah it was in
joint it's not a comics problem anymore
if you want to also optimize like I'm
convex program ah it's not like the
first one is a convex program either
yeah that's right so actually I we have
child I would show you the result okay
but at the moment I'm just focusing on
like a replacing this part but it's kind
of a ship which you have we're not
guarantee and see what I perform well in
practice or not okay so this is the
previous figure you have seen that's for
chaining these dipping your nuts and
then if you trade is the fixed neural
Nets you you have already learned this
convolution layer right you just try to
optimize the topper fully connected
layer and then you get it somehow you
don't get the best performance comparing
two DS yeah using using all the same
tricks of manual intervention yeah this
one's not so you just have added yeah
yes that's right this one yes you have
adjusted somehow at this point it's even
you adjusted it's not decreasing anymore
somehow there's some co adaptation of
the convolution layer and then the fully
connect lights now you Cheney jointly
you get better model but if you fix one
punch and the other one seems that
you're not getting that's good result
it's a it's a steal here your skills try
to optimize some kind non-convex
objective function we don't understand
where why why this is the case yeah but
if you use the established can see what
kind of machine that's what you get you
have faster convergence and then you
just use one over key type of learning
rates okay that's what you get yeah this
fluff in terms of time the time is that
this is one week and the times are
comparable to these neural Nets so the
iteration time is yeah comparable we
also implement our hours in GP us far
it's comparable yeah so actually most of
time is spent on loading the data
and then performing transformation and
convolution so although this conversion
only has three medium parameters they
actually perform the most of the
computation and then these 60 medium
parameters are later it's only taking
fractional time to jury goes once no no
once yeah for for us we can also go to a
data point many times because every time
you don't see exactly the same example
and then you you actually make a random
cropping of this image and then run the
mirror imaging and some transformations
in the Sun says you never see the same
data point yeah where's the red line in
the sooner so we didn't uh it's not been
proving any much more it's not
improvement but that's why we we just
know that you go so we go to some I
think the way we call some longer thing
and it doesn't so it stops at 10 times
less data than the green line yeah so
the green takes a week and you are the
same speed notice also that I mean at
this points are week the students not
patient enough to wait for are you using
the results of the all the guerrilla
generated for the yeah that the
convolution we can see so how much does
just your training of the so a few days
a few days as well it takes a few days
as well so I don't remember exact time
but it takes a few days so it's it's
that most of time is actually loading
the data and doing all this convolution
so because our mother is something new
the student is not sure whether this
method will work on lock he tried many
different things of what is some
constant into learning rate and he
didn't go wait for one week what each
one is experiment infosys the dimension
of these input essentially be this one
right this layer it's like 6,000
dimension around 6,000 dimension yeah
you silly operation give you my solemn
questions
like many of you just use the
traditional colony hmm justjust to train
it I think he'll also be Danny probably
several days possible hopefully someone
can try it and that means that we really
haven't try hard enough Kalamata okay
actually in this case we need the
chameleons are more okay together settle
upon it's completely kidding okay you
need to go to that kind of scale in
order to gather state of our performance
you know what idiot yeah so how it was
really icy isis so so so the original
image is one part 3 media but as i
mentioned that you generate this virtual
image right you take the original image
you crop it and then flip it then then
do some transformation yeah and you join
a slightly variation of the original
image yeah yeah that's right it's harder
maybe you can wait longer or use a
larger machine but for a typical you
know the desktop it's difficult it's
difficult so you get is we also try this
on other data set that the story are
pretty much the same Pacific can you
also general watch your image that's why
data point can go up to 10 10 million
okay so this is a color machine can
watch much faster and then you get about
the same performance in this case the
data says simple it's only 10 classes so
we actually get the same performance as
newer nets so you can also look at it
even easier data step through google so
here you get a different picture from
before which is the 51 net actually
perform better than the John its frame
right right so so could it be that is is
a difference statistically significant
dude can you analyze something like
there are we just so taking quite a long
time we just look at these cassia and
that's if you run several times probably
we can average a curve and it's maybe
not not significantly different the
datasets simpler and then what I can say
is they pretty much get the same
performance okay and in this case I mean
if we didn't adjust the run your race
yeah
yeah may give me something early so
what's is the loss function that using
the w/e stochastic agree it's a
multi-class largest regression one class
was just the same thing for me on that
thing is yeah right and an institute
that's even simpler right and you can
actually also know make some small
transformation of the image in general
up to a mere des des moines and in this
case aromatic gets where is more error
in this case below one percent okay so
for instance our almost slightly faster
but I cannot say statistical whether
it's a second or not this one is faster
because the image is more and then the
newer Nazi is only actually two layers
so it's not every model is these semi
layers this is for image net for the
simpler data set you actually use fuel
is this is two or three layers and then
the image are also much smaller and this
one is within the day we're very fast
you get similar type of results and we
also have child regression problem not
just classification problem in this case
we tried something like trying to take
some three-dimensional structure this
molecule and predict some property of
this molecule for instance how efficient
these molecules can run in sunlight to
energy things like this and then you
want to do that you had the first
represent its kind of three dimension
structure some feature representation
okay so people have come already come up
with some limitation in wherein fit
because in this case it's a
three-dimensional structure coordinates
you rotate it you get even college but
it's the same molecule in its some way
to represent this thing they come up
with something called the column matrix
looking at the pairwise charge and then
divided by the distance and then you get
its matrix you're permitted as your data
points it's a regression problem the
output is between the range of the 02 12
and it's just a regression problem you
don't use this convolution putting any
more okay in this case is really just
fully connecting your nets sauce three
layers and then we just
do these cannot reach regression okay so
this is just comparing the fully
connected network to Connor machine
thank you so in this case we go through
this 23 million won and look at is
basically the mean absolute error okay
in a test set and in this case you see
fewer points actually the reason why
this curve is so smooth we have a few
point for the evaluation and then
naturally just interpolate feels like
smoother okay message to the traditional
yes in the paper yes we have we compared
to is a dual coordinate descent things
like this from UCI data set where we can
actually run the competitor so in that
case essentially what we show is the
convergence rate is about the same okay
we are not or clearly better than not
enough in terms of rate of convergence
and then the final classification
accuracy it's just that in our methods
more scalable allowing us to you know
child this type of data set okay so what
we find is is fully connected layer and
the new Nets is really not useful ok you
can replace it by other kind of math and
nonparametric method you get about the
same performance what seems to be really
useful are these convolution and pulling
layers so what we tried is exactly we're
also going to learn two things together
we're going to learn this column as a
classifier and also seven times adapting
is convolution layers so just by chain
rule or derivative okay and what you get
is your can also get also get close to
the performance neural nets we just try
it recently we get 46 for new nuts you
can 42 okay there's still gap but this
is much better than run against running
as in 99.9 something percent error okay
this better day
separating the information in tow it's
about a thing again yeah separating you
you already learned is the futures right
that's much easier probably just want to
classify here you some kind of learn the
filter that we don't have any guarantee
for this but somehow works okay you
don't need to use this fully connected
that rectified linear units you will put
conversion here you adjust this it's our
works yeah we didn't join it extensively
we could possibly get even better
results here you still have the
convolution layer yes you just learn you
do the back propagation yeah from the
kind of gun right here right right ok so
this is a fixed convolution goes so far
it's like a 44 oh yeah and then we just
tried it in a few months and that that's
the result it can improve ok ok
essentially with this I'd like to
summarize a little bit DD method that we
you know scoured is we used to scour
these color machines really just this
one slice this one slice instead of
using the training data random batch of
chain later you also use a random batch
of random features simultaneously to
approximate this gradient and using this
double useful crazy equated to update
your function in the end of your
function is going to be waiting sound is
running it that's the same
representation as this kitchen sink but
the way we use these random features
different ok we generate this run in the
future online in and we never used
revisit them ok but in practice you can
release it and again you can blend
between the two so the advantage of
these approaches you can actually use it
to handle streaming data suppose your
data is just keep streaming in right you
can just keep increasing a random
feature to accommodate these increasing
complexity ordered it of course there's
also interesting problem of why if
you're some data or classes you will
never see in future you want to forget
about it how do you do it
nicely such that you keep the overall
representation still manageable
functions so you can be applied to of
course this convex objective function
but we're also recently trying it for
Colonel PCA okay for principal component
analysis you can solve some kind of non
convex problem you try to maximize and
convex function instead of minimizing it
you can also use these stochastic
gradient to do it and then it doesn't
prevent you use the W stochastic way and
you can actually also provide the
guarantee for these convergence if your
initialization is close enough to this
basing of that chapter okay gaussian
process you want to estimate the
predictive meaning and comments it can
be set up as some comics optimization
problem you can also use this approach
so I memorized we just need to remember
this Alfre the one for each data point
so that's why it's OT if you run T
iterations and in terms of estimating
the function we get its rate in this is
of the generalization ability we get 1
over square root tea okay that's the
best possible for this kind of
nonparametric method without for the
assumption in practice he also works
nicely okay that's the most important
thing in some sense so there are many
other questions that hasn't been
resolved so mom may also look into these
two questions on the top so what is the
maximum pooling and convolution is
actually doing okay why can't we use a
lark attaches yeah and to get about a
same performance so people have go to
extreme that you're not just having five
layer of this you might have a 15 may or
may be 29 of this and where is more
common Russian colonel and you get even
better model so it's it's it's amazing
how you can do this and you get a model
across the human judgment and then if
this architecture works it must be some
characteristic of the data which is
suitable for that type of operations if
you apply this architecture to other
domain maybe language it's not going to
work okay there must be some character
is your data what what kind of
characteristic is there yeah to make
this heart attack work it's not clear um
we're studying this hopefully we can get
some result in future if you I come back
I'll give this talk
ok ok that's everything any question
yeah further question ok previous slide
yeah sister studies from the actual
function that Miss has no you see or
petitions perspective you get the same
covered in trees as the singles hungry
yeah so it is a question is impractical
observe more various brands yes you have
more branch concession divided up in a
bun and also even from the referees are
you've got the same green in terms of
heat but in constant that can be
partially yeah it's going to be larger
exactly you have the two sorts of burns
one source from the data another source
from the random feature exactly so it is
convex optimization remember in practice
you will try to grab as large mini batch
as possible and German as large a batch
of rena features past as much as your
memory can hold and do this you don't
actually just use one data plan it's
actually interesting that it's
completely it's different for neural
Nets somehow in order to make this
neural nets way work you have to use a
mini batch that is small enough not too
small or too large a few hundred points
or something in order to see the best
convergence phone you're nuts it's a non
current problem somehow there's the best
stochasticity that allows you to get a
better result but here is a chronic
optimization you really want to have as
little renders as possible yeah so we
will grab a large batch mini batch yeah
any other question that would be here
for a few days if you guys are free I'd
be love to talk to you guys more ok</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>