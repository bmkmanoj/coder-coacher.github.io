<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Rethinking the Interface to Solid-State Storage | Coder Coacher - Coaching Coders</title><meta content="Rethinking the Interface to Solid-State Storage - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Rethinking the Interface to Solid-State Storage</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/56LeI0Wcerw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
all right it is my pleasure to introduce
Mike Swift Mike and I have known each
other since Mike was a graduate student
at the U dub and I used to go admire his
nooks papers and then go up to him
afterwards and say wait a minute it's
got to be a lot harder than this what
are you gonna do about iocked dolls and
things like that which point Mike would
when I would have the discussion of the
difference between product code and
research code things like that and we 0
NZ excellent you're a good shape Ed's
even shown up and let's see and I guess
the other thing so we actually did try
to hire Mike performance but he had a
lifelong goal of being a professor
that's true my father's a professor and
I had to live up to yes I remember the
point is the point in the selling the
selling conversation where Mike said
look I saw my dad teach when I was 12
and I've had a dream to be a professor
every since then ever since then so he
had to go try you know at this point I
can't remember if that's really true or
not how soon that since I told you it
was cool it was whether it was a dream
or just an assumption it would be doing
it made me feel better and letting him
go to Wisconsin anyone but we're glad to
have him back to tell us about the work
he's been doing okay thanks this out of
the way anyway thanks y'all for coming
I'm going to be talking about the work
that I've been doing on looking at the
interface to solid-state storage devices
like flash or phase change memory so to
put this work in context I think I'm
probably most known for work on device
drivers and I've sort of expanded since
then and my real interest is really kind
of the bottom half the operating system
where you're really focusing on
interactions with the hardware and what
that looks like so with the vice drivers
I first look at reliability and then
I've been doing some work on sort of
different models of how you program
drivers or where the code runs and
running it in languages other writing
and languages other than see in terms of
memory I spent a lot of time looking at
transactional memory and how that
relates the operating system how do you
virtualize it a handle page faults and
context switching recently I have some
work on dynamic processors where the
processor can reconfigure and again what
does this mean to the operating system
and most recently I've been looking at
solid-state storage and my interest here
really is saying if we have these new
devices what does it mean to the
operating system you know are they
really just faster smaller lower power
discs
are they something different that we
should treat differently and think about
new ways of interacting with them so if
you look at storage trends over the last
10 years i think is all of you know
there's been this huge trend from sort
of spinning magnetic disks to solid
state disks and then hopefully this is
the will get to this this is the onyx
prototype from UCSD which is built on
phase change memory which is sort of
again an order of magnitude or two
faster than flash storage and so what's
interesting about this is that as you
look at these devices these newer
devices are not just smaller faster
disks but they have dramatically
different properties and they behave
very differently and I think that's what
makes it interesting to look at how we
interact with them so first looking at
flash drives the key property of flash
drives as I look at it is that fun and
play this storage medium is something
you can't directly over right you have
to erase large blocks of flash which is
pretty slow in the order of milliseconds
before you can write to it and what this
means is that inside most flash drives
there's a piece of software called the
flash translation layer that will take
incoming addresses and then route it
through some kind of mapping table to
find where the data actually lives
within the device in the device data is
written usually in some kind of log
structured format where whenever you
write new data it writes it to a new
location and then you can asynchronously
garbage collect the old data so this is
very different than traditional discs
where you just write things where that
you know when you just overwrite data
directly in addition a key feature flash
is this limited endurance where you can
only overwrite flash a limited number of
times which means that you have to spend
a lot of effort in terms of where you do
these things to kind of where let where
to sort of level your rights out across
the whole device and the net result is
you've got this fairly sophisticated
piece of software on a processor inside
your flash device doing all these
interesting all this interesting work so
the second new piece of storage
technology is storage class memory which
I know there's been a lot of work added
at Microsoft and the key features that
I'm interested in are first that you can
sort of attach it to your processor in a
way that makes it sort of act a lot like
DRAM and this means that instead of
using a device driver with a block
interface you can potentially just use
normal load and store instructions
within the processor to access memory
furthermore it's very fast
it's much faster than flash and this
means that software overhead matters a
lot because all that software overhead
really kind of hides the short axis time
so the things that that this could
potentially enable are things like
having very fine-grained data structures
that we make persistent because we're
not limited to just writing out data for
K at a time we could write out four
bytes at a time if we wanted to and
furthermore it allows perhaps direct
access from user mode programs because
if we can just make it look like memory
programs can do load and store
instructions and they could potentially
just do load and store instructions
against this storage class memory to
access persistent data so again this is
very different than disks where it's
very slow and you need a device driver
so if we look within the operating
system though and we see how does linux
for example abstract these kinds of
devices what we see is that really not a
lot has changed that if you use flash as
a file system which is what most people
do you still have your standard read and
write for reading and writing data and
then sync for forcing data out and
making it persistent you can do em map &amp;amp;
M sync if you want but it still is more
or less just like a file on a file
system lower level within the operating
system there's still a standard block
interface which has been around for a
while which is basically reading a block
with be read and then you can submit
asynchronous i/o request using submit
vio to read or write a block but it's
still just transferring a block of data
in or out at a given address no
acknowledgement whatsoever that these
things are more sophisticated so the
reason this is important is that these
devices really are fundamentally
different and they have really
interesting features that using the old
interface conceals so if we look at
flash SSDs they have address translation
built it and I think every operating
system person knows that address
translation is a great feature that
we've been using for all kinds of nifty
things like copy-on-write and shared
memory and protection and things like
that and this this great features built
into devices you can buy but we can't
get at it to do anything interesting
with it so maybe there's garbage
collection which again is a very
powerful general-purpose technique for
managing data structures that is being
done inside the device but it's not
accessible outside the device to do
anything interesting and it would be
nice that there is a way to make these
features available to applications so
one thing you can look at is anyone
is building high-performance
applications that runs on flash
typically tries to do their own garbage
collection on top of it where they'll
sort of write things out sequentially
and then they'll because sequential
performance is so much better and then
they'll do their own user level garbage
collection where they compact and copy
data and that's the same thing the
device is doing internally and there's
sort of a duplication of effort here
assembly for storage class memory we've
got this really low latency access we've
got bite addressability but if you go
through a filesystem you lose all that
until applications don't get sort of the
raw latency that this can access that
this can offer you a jump so to that end
what I've been trying to figure out is
how can we expose these features to the
operating system and applications better
by looking at the interface that they
present instead of saying everything
must be a file you know is there
something else that we should be doing
here because files may not be the
end-all be-all of data access so
specifically for any kind of device that
has internal management I'd like to
expose those internal management
features to applications or to system
software so they can take advantage of
them at least coordinate their activity
and for anything that has sort of really
low latency I would like as much as
possible to sort of bypass all the
software layers on top of that and
really get that low latency right in
front of applications so they can use it
so with this in mind I'd like to talk
today about two projects we're doing one
is a paper we presented at euro cissus
you're called flashed here and it's one
approach at sort of rethinking the
interface to solid state disks by
looking at what would we do if we were
building a solid state cash instead if
we knew we were building something just
for caching and then second I want to
talk about a paper we had a task class
last year on up system called nemesi
knee which is how do you abstract
storage class memory and make it
available directly to user mode
applications so the reason I'm
interested in flash is a cache is that
it's a really natural use for flash so
first of all flash is a lot more
expensive than disk so if you've got a
lot of data it's probably too it could
be too expensive to move everything into
flash it's maybe ten times more
expensive but the same time it's so much
faster maybe a hundred times faster or
more that you really want to get the
advantage of that performance and so a
natural thing to do is to basically
stage all your hot data into a flash
device and then
you know hopefully most of what you need
to access whoops I'm jumping ahead of
myself most of what you need to access
is on the flash device and you can get
to it very quickly so this is such an
obviously good idea that pretty much
every operating system except maybe Mac
OS has this built-in at this point may
pretty much every storage vendor has a
product that does something like this
fusion-io sells a lot of caches I think
ocz has some caches that are sort of
come with hard drives yes this is jae's
gauge model or directions a CST to
actually build using as so there is a
there is a block-level filter driver
that acts as a cache called be cash that
sits underneath any file system or
database and will then transparently
send blocks to either the flash device
or the disk so it's a lot i think turbo
boost is the mic is that the microsoft
caching product it's already boost i
forget which one so I think it's similar
to turbo boost which does transparent
caching the block level in Windows Vista
beyond I think so furthermore big
service providers yes so today most
people who are doing this don't take
advantage of the non volatility they
actually well in a way they work they
only write out clean data they'll do
right through caching and then most of
these systems will basically delete the
cache when you reboot the system so they
don't actually use the non volatility
and I'll talk a little bit about how we
can exploit that you could do it but you
know in a lot of cases people don't
trust the flash device as much as they
trust the disk despite the fact that
that I know there's data from Intel that
shows at least there I says these are
much more reliable than discs intel how
to study where they replaced all their
laptop hard drives with this and the
failure rate for laptops went down by
factor of five so they actually saved a
lot of money putting in SSDs anyway so a
natural way to do this as I said is to
do it transparently as a block device
because you can do this basically
underneath the filesystem you don't need
to rewrite all your code to take
advantage of this you don't need to
modify ntfs or EXT 22
do this basically the file system talks
to a generic block layer and then in the
block layer you put a filter driver
which acts as a cache manager and the
cache manager has metadata about all the
blocks that are being cached and when a
request comes in it considered decide is
this block located on an SSD or is it
located on disk and where should I read
it so on a read it will you know consult
either one on a right if it's right
through it will write to both or it can
do right back and sort of write the data
to the SSD in which case the SSD acts
like a really large write buffer
absorbing new data as it gets written
and then you can later D stage it back
to disk so there's a lot of advantages
to this one is that it's transparent to
most software in the system because it's
at the block layer below everything else
and two you're using a commodity piece
and SSD so you don't need to go build a
special-purpose device for this so you
can buy any SSD and plug it into your
system and take advantage of this and so
for example some of the hydrogue hybrid
drives out there just have a stock SSD
built into the hybrid drive with sort of
additional technology on top of it to do
this caching stuff so you can also do
this sort of inside the device so the
downside with doing this though is that
it turned that that in my opinion maybe
it's not just my opinion is that caching
is actually a really different workload
than a storage workload where you're
actually trying to store data
persistently so one thing that's
different is the address space when you
are having a caching device the address
that every block that you're caching has
its own address on the disk already so
which means that when you write it out
to the SSD you need to remember what was
the original address that this block was
stored at so that's why the cache
manager has to have this table of every
block that it's caching that says for
this disk address this is the
corresponding address on the SSD I mean
furthermore if you want to make the disk
non-volatile you need to write that
mapping table out and that's expensive
enough that a lot of people don't bother
writing it out and that's a big reason
for sort of recreating the cash after
failure second is the issue of free
space management so cash is already have
to manage free space you know they want
to make is use as much capacity as
possible and they think about what we
vicked when they get full so similarly
SSDs are doing free space management
because they're absorbing new rights
they have to garbage collect old data
they to figure out what to garbage
collect and how to sort of manage the
free space and so
and there's things going on at two
layers that could be coordinated for
example might be beneficial at the SSD
instead of garbage collecting data that
was about to be evicted would just
delete the data instead you know if
you're going to evict it why bother the
cost of moving it around what you're not
going to use it again and then finally
there's this problem of cash consistency
and durability that we might like the
cash to be durable across reboots which
would allow us to use it as a write
buffer also because it's not sort of
take advantage of non volatility but
because of the cost and complexity of
persisting this mapping this translation
of disk Idris is the flash addresses a
lot of flash caching products don't do
that for example i don't think the
microsoft version or the solaris version
of the caches are persistent across
system crashes so the key point here is
that a caching workload looks very
different than a storage workload and as
a result we might want something
different out of the device yes me right
we're closed for obviousness easy cash
actually it's a hurry
is what other read white right workloads
in your experience where you have the
SSD for the discounted harming
throughput arming performance likes is
actually get get away on it's strictly
do no harm we haven't seen that yet but
i will say their experience is limited
to a small number of traces from the
snia website which you know may or may
not be representative but I could
definitely see if you have something
that is just doing a lot of right and
you're basically you know you're not
reading anything then writing it out to
the SSD really doesn't buy anything
because you're just going to fill up the
SSD and then write it out to disk and
it's just sort of added expense and
latency in the system yeah so I think
that would be the number one case please
just go up parallel well the over i mean
the overhead goes up because you're
spending more software time managing the
cash so you can do the things in
parallel so to address this we build a
system called flashed here which
basically starts with the normal caching
architecture the first thing we do is we
take this SSD at the bottom and replace
it with what we call a solid-state cache
which again is designed to be a
commodity system that anybody can build
with a standard interface but it's built
to be a cache so the first thing we do
is we have a unified address space which
means that you can store data at its
disk address directly instead of having
to translate disk addresses into flash
addresses second it has cash away or
free space management so it knows about
which data might be cold data that you
can evict instead of garbage collecting
when you want free space in third it has
an interface where it has commands that
are relevant to caching for example the
ability to mark data is clean or dirty
or to forcibly evict data from a cash
for consistency purposes and finally and
something I don't want to spend a lot of
time on is it also has strict
consistency guarantees that allow you to
reason about which data in the cache is
usable following a system crash because
you can know for sure that for example
anything that you breed from the cash is
always the most recent version if you
follow the protocol on top of this we
also modify the cache manager to
basically use this new interface and
we've implemented both the right back
and write through policy yes so you're
replacing the flesh itself here as far
as what you get
architect when you when you do this we
are modifying the flash translation
layer effectively that interprets like
sada commands or scuzzy commands and
then figures out what to do the MTL do
not say you don't have within your
design ability say making your cache
line or making your blocks a little
bigger we have not done that at this
point we're sort of trying to trying to
adjust sort of focus on that translation
layer and the commands that come in over
the SATA interface something that can be
modified on a 16 classroom it cannot be
done as I'll talk about there's a
prototyping board where you can write
your own flash translation layer and
we're in the process of implementing the
system on this prototyping board that's
our practical you so practically using a
practice our model as we would like ocz
and Intel to actually build devices that
have this interface into it it's sort of
that would be our product ization goal
it wouldn't be something that Microsoft
could then do purely in software you
know unless unless there were vendors
that let you rewrite their firmware and
I think that's unlikely yeah
okay so I now want to stay a little bit
more about the design and I'll focus on
the three biggest issues which are the
sort of the address mapping address
translation managing free space and then
the new interface sort of the commands
in the interface so here a set of anak
rocks the problem with address space
that we basically have multiple address
spaces going on we've got disc addresses
so the host operating system has this
translation from disk addresses to flash
addresses and then inside the FTL we
have another mapping table that
translates flash addresses into the
physical location and so we've got
double translation going on it really
doesn't buy us anything yes translation
right now you have to be mounted centers
so they have a they don't map every
single block they have a small
translation table let more like a look a
side list that says if you're on this
list if you yeah yeah so it sort of fits
very much fixed size there's a limited
number of things they can remap actually
if you look at disk technology those
there's this technology called shingle
rights that allows you to basically
double or triple your density by
overlapping data which allows you to
write that you can't override eight
anymore because you write a track this
wide but then you you sort of overlap
tracks on top of each other like
shingles and so you can read this narrow
strip of it but you write something
three times wider right so you can't do
random rights and so disks are likely
going to get sort of have the same
problem of not being able to do random
rights anymore without what so in
talking to people at netapp they think
that shingles are going to happen
because the reason to use a disk is lots
and lots of storage if you need
performance you'll get an SSD and so
their take is you know the disks will
get you basically get really really big
and dense and then anything you really
need random write performance from will
use flash and sort of sort of further
distinguish the two layers from each
other but i don't know if i don't know
on my own dice is just what i hear okay
so the approach that we take is to say
well let's get rid of the translation
table in the cache let's do the
translation table directly in the device
so here you can send a dick disk address
right to the device and we can translate
it from the disk address right to the
physical location in flash the second
thing we do is we change the data
structure here instead of kind of a page
table data structure which is good if
you have a really dense address space
because you
have a big page that has lots of
translations and everything is filled in
in a disk this makes sense because the
number of addresses you translate is the
same as the side the number of physical
locations you have whereas we have a
system where we have the number of
addresses we translate could be ten or a
hundred times larger so we could have
like a ten terabyte disk system that
were then caching with a 10 gigabyte
cash and so we have potentially a
thousand times more addresses so we use
a hash map data structure instead which
is optimized much more for a sparse
address space instead and we have some
evidence that about that sort of caching
workloads are likely to be much sparser
than a normal storage workload just
because the hot data is going to be sort
of more widely distributed okay so the
second thing we do is reason it is sort
of change how we do free space
management so the normal way that free
space management happens in an SSD is
through garbage collection so you might
have multiple blocks here and remember
the rule is that you you can write to
sort of small pieces of a block that are
called pages but when you erase you have
to erase a much larger block which may
be 256 kilobytes or bigger so when you
do garbage collection typically the
process is like in the log structured
file system where you find multiple
blocks that have some valid data you
then sort of coalesce this valid data
into another block and then you can
erase a block and you get a free block
out of it and so the problem with this
is that we basically have had to read a
whole block write a whole block we have
to erase two blocks because we've erased
both of these we have to start with one
empty block to start this whole thing
off so it's a pretty expensive process
furthermore every time you copy data
you're writing to the disk and this sort
of it uses up its right capacity because
it has limited endurance there's a
second problem which is perhaps even
worse is that the performance of garbage
collection goes way down when you have a
full device because you have a lot the
chance of these of these blocks being
sort of filled with stale things is much
lower when your device is almost full so
this means that you have to read all you
have to sort of copy a lot more data to
make free space when you're almost full
it's sort of like when you defragment
your disk and it's almost full it's a
lot harder because you have to move a
lot more data around so there's data
from Intel that shows that at least for
their SSDs if you use them to their full
capacity then write performance goes
down by eighty-three percent
compared to if you reserve thirty
percent of the capacity as basically
spare yes perfusion what are the small
boxes in the big boxes so these small
sorry about that so the big boxes are
what is called an erase block and this
is the unit of data you can erase so you
can only do do erases in a large block
size of like 250 6k you can do rights
and much smaller granularity which are
like about 4k or so sometimes larger and
so the idea is you can write to each of
these individually but you can only
erase the whole thing sorry about that
it's a good question that makes more
sense now okay so the other problem is
that endurance because we're doing more
garbage collection we're doing a lot
more rights to move data around our
endurance goes down by eighty percent
also so this means that if you use a
normal SSD for caching and you actually
use the full capacity it what may not
last very long because you're doing all
this garbage collection and finally you
know because we're doing caches we
actually want to have the device full we
want to get a good hit rate by having as
much data as possible within the device
so the approach that we take is to say
well you know what this is just a cache
we don't actually need the data so it
might be beneficial in some time to
actually just delete the data instead of
moving it around and this is actually an
idea that I have to sort of credit Jason
Flynn for we were talking about flash
devices and he said you know why bother
actually storing data let's just delete
these delete data instead and things
would be a lot easier and so that's what
we do so to do this we have to make sure
the device knows which data is dirty and
which is clean and once we know which
data is dirty and clean we can find a
block here that has only clean data and
when we need some free space we can just
delete the data instead of copying it
around and the nice thing is we have now
made an erase block without any extra
reads or writes we just have to erase
the one block and we delete the data and
if the data is indeed fairly cold then
it has very little cost of doing this
and so it can be much more efficient
than normal garbage collection so we've
implemented to very very simplistic
policies here that really don't don't
even look at usage patterns at all but
we basically segment the disk the drive
or the flash into two regions one is
called the log region which is where new
rights come in and they're basically
sort of randomly accessed and things are
just written in log order so this is all
sort of the hot data that's being
written
here we still do garbage collection
because this data was recently written
it's likely to be accessed again soon we
find and so deleting that data really
hurts performance because we get a lot
of read mrs. this other portion is
called the data region and this is
colder data that tends to be stored more
sequentially and here we will just evict
this cold data when we need space in
this model we actually just recycled it
when we evict data we recycle the the
blocks become data blocks so here we
have a fixed size log in a fixed size
data region an alternative is that sort
of a variable size log where if we're
having a lot of rights we can make the
log much larger than normal and this
means we have a larger region to sort of
coalesce data from when we do garbage
collection and then we still will do
eviction from the data region but will
then recycle the block to make a new log
block and we can also take a log block
where if you write all the data
sequentially will sort of convert it
into a data block the reason this is
important is that we can also do the
address translation for these regions a
different granularity so this is sort of
randomly written new data we translate
this at 4k and this is sort of much more
sequential data so we translate this at
an erase block granularity of 256k and
so this saves on the memory space needed
for translation by partitioning things
this way and it's a fairly common
approach from what I understand in real
solid state drives to yes any education
policy for drinking the blood flow
string into the cache is it like ladies
plus taxes so we have to you know get to
that on the next in a couple of slides
yeah when I talk about the cache manager
the cache manager implements the policy
of when to put data into the cache ok so
the next thing is the caching interface
which is what do we need to add to it a
solid state drive to make it a better
cash so what we need is first some way
to sort of do cash management to
identify which blocks are clean in which
are dirty so we know what we can evict
and we also need some way to ensure
consistency so for example if you write
data directly to the disk and bypass the
cash you might want to forcibly evict
data from the cash so that you're sure
that it never stores a stale copy of
data you also might want to met change
the state of a block from being dirty to
being clean when you do write it
so to address this we basically take the
existing read and write block interfaces
and sort of tweak a little bit first of
all we have two variations on right
where you can write data back and say
that it's dirty meaning that you
shouldn't erase it on purpose or you can
write it back and say that it's clean
which means there is a copy somewhere
else in the system and if you need free
space you can erase it for read the only
thing that we do differently is we now
return an error if you read an address
that isn't there so normally when if you
have an SSD and you read an address that
has never been written to it'll just
make up some data and give it back to
you typically zeros or something like
that but it could return whatever was
written last time at that physical
location there's no specification for
what cat what happens when you read on
written data I'm here you know for sure
that something isn't there so the nice
thing about this is that if you want to
access the cache it's always safe to
just read a disk address off the cash
and if it's there you'll get the data if
it's not they'll get an error you'll
never get invalid data and then finally
for cash management we have two
operations one is evict which
invalidates a block and the second is
clean which basically marks a dirty
block is clean it so it's just a little
bit we have a trade-off in the read
interface Music imagine issue reads and
if you miss you just messed up this
killed it is important
it just means it so it's a big you know
ten ways so in this system we assume
that the this is sort of a standalone
drive and it cannot go to the disc
itself so it's not kind of a stacked
interface the cache manager will first
go to the SSC and if it's a Miss will
then go to the drive so that peace is
all in the software and the reason we
want that is because it allows you to
sort of plug this into any drive system
you want you could put it in front of a
raid array you could put it in front of
a network file system it can work with
any form of block storage it seems like
you keep it just a small bit of State in
memory their map to know whether God
exists yes yes so so the nice thing is
that's a that's a great point is that
this is always correct which means you
can have em precise information in
memory is sort of like a bloom filter or
something else an approximate data
structure that says is it worth checking
the cash and if it is you can go and
you're guaranteed to get the latest data
or nothing if it says it's not there you
can go right to the disk and avoid the
latency of going into the cache cache
we'd want to speculate go to disk in
cancer the i/o you can't ahhh you
certainly could we haven't looked into
that I know from talking to some people
in net up that they really do worry
about the cost of cache misses and so
they spend a lot of effort in not adding
latency in that case so that would make
sense yes this is implemented inside
this theorem right this is in this is
all implemented inside the SSD so SSE so
these are the commands that are being
sent over the SATA interface into the
SSC instead of the normal read block
right block yeah so poor memory on the
SSD diria flash
would that change we we certainly
wouldn't go with this mechanism of
having the sort of separate I think
having the the separate data region
mapped of course or granularity we would
not do because there's a lot of overhead
and sort of translate translating
between 4k blocks into 36 k blocks but I
don't think we necessarily change it we
do as you'll see we do expand a little
bit the amount of memory in the device
because we now have a hash table instead
of a page table which is a little bit
less than efficient and we also have to
store metadata about the block so we
have to store you know a bit that says
is this block dirty or clean if we had
more data I think I'd add space for
things like last accessed I'm also but
you know we all read the already we
already we keep all of the metadata
about all the blocks in memory so that
like a read miss is really just a memory
operation on the SSC and it doesn't
actually have to go to flash so that's
why I wouldn't change too much yes don't
need the FTM you could alternate did you
have your mapping table in post memory
and that could be a hash filter- yes so
we thought about that and in talking to
various people in particularly fusion-io
one of the things they said is that they
really find it's important to have some
management software on the device itself
and the reason is that the flash chips
themselves are so crazy and hard to use
that things like what is the frequency
which which you reread a block or
handling where management is really
important and they find that by having
this on the device they can actually
improve their their write endurance by a
factor of two or three just by doing a
better job on the device itself and so
for that reason we wanted to have at
least some management code on the device
and our other goal was you know I think
there's a huge benefit to having a
standard interface that you can just
plug it if ice into instead of having to
write a device driver for every
operating system and so we tried to come
at it with the approach of let's not
require a lot of OS code much as I love
drivers and writing them but i think you
know most storage vendors would rather
have a device you can just plug in with
no required kernel level software
particularly for linux we have to
rewrite it every
three months for each new version of the
coronal interface so I think that'd be a
great design but we sort of chose a
different set of constraints and so
finally we have an exist interface so
there's a problem that if you do crash
and you come back again you need to know
which blocks are dirty so you notice
that you have to clean them again and so
we have a command that allows you to
probe for the existence of dirty blocks
so on top of all this we have a cache
manager and we've implemented to caching
policies so far the first is right
through where the cache manager stores
no state whatsoever and basically every
right is written both to the the SSC and
of the disk so we cash all rights and
then on reads every read goes to the SSC
and if we miss we go to the device and
we populate it so we have sort of the
simplest possible caching strategy here
yes too but in terms of the dirty blocks
I assumed you making some assumptions on
atomic about the atomicity of rights or
about the illusion tank torn right sir
so we do we actually this is the piece
that I said I wasn't going to talk about
the consistency so we actually do
internally use sort of atomic right
strategies like the atomic rights from
fusion-io where they use sort of we
don't use a super cap we basically we
don't assume we have a super cat but we
actually do synchronous logging of
metadata updates when it's important and
so that's part of our performance model
and for rights we do assume that the
device fusion-io is this idea where you
can basically write a bit that says this
is because you're writing everything to
new data you can actually detect a torn
right because you can see have you ever
overwritten this or is it cleanly erased
and so we assume we can do atomic block
right using that that's a great question
for right back we it's a very similar
reads are exactly the same where we we
try both on rights we will write it back
the device and we populate a table in
the cache manager that says this is a
dirty block and we have set of a fixed
size table here so we have too many
dirty blocks then we'll start cleaning
blocks and writing them back and in our
experiments we set this to twenty
percent of the total size of the cache
and then finally when the system crashes
we use the exists primitive to sort of
go and reread to find out what all the
dirty blocks are
okay so that is sort of the essence of
the design I now want to talk about our
evaluation and our three goals for this
are you know performance is obviously a
good one reliability and then is there
some efficiency savings in term of
memory consumption so we implemented
this we took the facebooks flash cache
manager which they use in production to
cache data in front of their my sequel
for their my sequel databases we took a
flash timing simulator that was
published by Kim at all we do traced a
simulation on a number of different
traces from the storage network industry
association and here's sort of the
parameters for our device we more or
less try to set the parameters about
like an Intel 300 series SSD which is
sort of a medium grade latest generation
SSD so here is the traces we have two
traces at the top that are very right
intensive there eighty to ninety percent
rights and we have two traces that are a
read intensive that are sort of eighty
to ninety percent reads and so this sort
of covers you know different aspects of
the performance yes plus unique blocks
yes the size of a megabyte with your
thank you these are the blocks or 4k
blocks so you can see this is a really
small workload and so what actually I
should mention this that we sighs the
cash to be able to hold one quarter of
the total number of unique blocks and so
we assume we're going to cache the top
25% of things and so you know the kind
of things to be honest we could set this
any size and get any performance we
wanted you know at one point a student
made it so big that everything was in
the cache and performance look really
good and I had to say you know that
doesn't actually prove anything when
everything is in the cache and you're
not running any of our code so we
figured twenty-five percent is
reasonable we get a Miss rate that's
measurable so that we can see the impact
on mrs. so here is the performance
numbers in red is the SSD we then have
the two variations so to the two garbage
collection or silent eviction policies
SSC and SSC dash V with right through in
blue and then right back in green so
here are the right intensive workloads
and this is where performance really
gets helped because of the silent
eviction policy so what we can see is
that particularly for right back we get
about 200 about one hundred sixty-eight
percent performance improvement I think
or 200
268 168 percent performance improvement
over the baseline system and this is
because we're doing a lot less garbage
collection so we're getting a lot of
rights here and then once this be clean
be clean we can just delete the data
instead of moving it around using this
is an egg would catch result in campus
or something new right it's the baseline
system that is currently in production
at usage at Facebook yes yes so we're
still using Cashin this is not compared
to a disk this is still with an SSD the
sort of the same performance model of
SSD as a cash and you can also see if I
they for a right heavy workload the
right back definitely outperforms right
through because we're basically using it
as a write buffer so whenever you
overwrite something instead of one less
right that has to go to disk which is
pretty much what you'd expect for Reed
performance you know performance really
I say this is unchanged and you know the
read tread the read path is basically
the same we're not going to make flash
act any faster we're still translating
addresses so that doesn't help so what
this really measures is what is the
impact of our really simple eviction
policy where we're willing to evict any
clean block that we're storing so it's
basically a random eviction policy and
we can see I think we have about a ten
percent miss rate that even then we
still get a very good hit right on on
the read heavy workloads even though
it's so simple yep II thought the genius
question that had because flashes or
whether the media in probably want to
have an admissions policy where you
entered the block in the flash cash only
after it proves itself to be hot enough
right I think that would be a great idea
because we could definitely bring it
into the page cache in the operating
system and then if it's referenced the
number of times then write it back into
the flash cash instead of just putting
it there on the first access so I think
that's a great idea
double summation be double we do get rid
of the double translation but that's not
a big expense I mean in the host memory
it's a you know a simple hash table
lookup that takes you know 100 like 20
nanoseconds or something and our
translation we time the the speed of
using a hash table instead of a a page
table and we see that it adds like what
was it like five nanoseconds onto the
time to do a translation Andrew knew
that you had room like this all the
footwear yes who's this they have yet
they disability their arms trade also
things like looking up page table
business look kind of hash tables right
so we we are currently implementing this
there and I agree that a low-end arm
process I think that the device we have
as a low-end ARM processor and so I do
think there might be a bigger difference
but you know even if we were ten times
slower the the cost of that translation
is so much less than the time to
actually access flash but i don't think
that allowed a noticeable metal agency
it might mean though that we do need a
slightly beef your processor to keep up
with the translation this is also yeah
the actual production where is what it
is a lowly words basically also it's a
this is a commercial SSD controller so
it's the indy links controller from i
forget i think india leaks is a company
that makes controllers that that that
ocz and intel using their products and
so this is a commercial controller but
we can provide the firmware for it did
you ever how many these
Ashley's so this is so this is a trace
replay and so this is basically taken
beneath the buffer cache and so this is
just the things the traces are just the
things that already went to disk so when
we have the real prototype working we'll
have a lot more information about how
this interacts with the buffer cache yes
also one of the pasta virally fillings
in the same places MSR camera is used we
are using some of those I believe so one
issue with those princesses those are
unmodified applications and so they
already have large Ram caches right as
part of the application logic so the
accesses guitar hitting artists don't
have much temporal locality right that
is totally true and you know I don't
think using a flash cash would
necessarily change the fact that they're
using a large Ram cash because there is
still you know two orders of magnitude
better performance i'm using a ram cash
instead but it is true that if we did
reduce the amount of cashing in those
applications we would see we would see a
different rate of here definitely
applications have to use that cash
because they are only adult in it in his
heart is but right and a flash cash
maybe they don't need that big of a man
yes i think that's like that's a great
point but it's not something we've
investigated yet so i do think the the
our other work we looked at a work on
use on using flash or virtual memory and
we found that when you use when you're
swapping to flash instead of disk you
could dramatically reduce your memory
usage by in some cases about eighty
percent for that exact reason because
the cost of a page fault was so much
less that you could keep things you
could basically swap a lot of your data
out instead of keeping in a dram and i
think definitely the same thing would
apply here yeah 15 from flashed upon a
spoon right and take advantage of a
sequential dice two hard days in terms
of choosing which love swimming so we do
not at this point so well i mean
basically we do whatever we're using
weirding facebook splash cash software
to do to choose which blocks to be big
and so my guess is they already tried to
keep it sequential but we didn't modify
that in our performance model we
basically assume that all disk accesses
take half a millisec
and so it's a very simple performance
model we're not we're not sort of
looking at the locality of the disk
rights here but that would definitely be
an extension okay so the second thing we
look at is endurance does sound eviction
help with endurance because we're not
doing as much garbage collection so here
we show what we measure is the life time
in years of the device so this is a
device size the whole twenty-five
percent of the unique blocks we look at
the duration of the trace to calculate
the frequency of rights and then
therefore the frequency of erases and we
figure out at this level of workload how
long would the device work assuming that
you could do 10,000 over rights / you
know bit sort in flash and so what we
see is that there is a reasonably good
improvement in lifetime here because of
silent eviction because we're not doing
garbage collection and in the mail case
I think it goes from a place where
really you know one and a half years for
device is probably too short from a
management perspective to want to use
caching up to 2.8 years where maybe it
would be worthwhile because you wouldn't
be replacing the device and I will note
that this is a very small workload and
so you take with a grain of salt in the
case of read caching we see that here
the first most important thing is
because this is a really big workload
endurance isn't a problem there's just a
lot of bits to spread rights over and
there's not that many rights so
endurance is very high 200 years or
something like that and the endurance
does go down a little bit and the reason
for that is that we because we're a
victim data that gets a reference we do
have to read in data and do extra rights
we wouldn't have had to do otherwise but
because it's so rare relative to the
size the device it doesn't impact
lifetime that much do you take a peek on
the pitch ratio
because yes so we do garbage collection
sometimes but if we go back here this
performance drop here is because of the
hit ratio so we're losing about 2% of
performance because we're not hitting in
the cache as much and I think if we had
a you know a fiction policy that was
more sensitive to usage that actually
used recency of access instead of random
we would improve this and so the final
thing we look at is memory usage in here
for the right intensive workloads we
have in blue the amount of memory on the
device used to hold the translation
table and then the amount of memory used
in the host to hold its whatever data it
needs and here we only look at
right-back because it's right through
the cache manager didn't store anything
and so the red bar would be zero so the
ratio of these things is the same across
all workloads so we can just take one
for example and what you can see is that
with the normal SSD there's a small
amount of data in the device this is
very compact it's basically a page table
containing every block on the device but
the host has a lot of memory because it
has to store a hash table that keeps
track of every block stored there in the
SSC case the device memory goes up
because we've gone from a page table
like structure to a hash table which is
slightly less dense and so we increase
memory usage in the device by about ten
percent but the data usage on the host
has gone down by eighty percent because
we're only keeping track of the dirty
blocks and not all the clean not all the
clean blocks also and that holds across
all of the different workloads Andrew
so you know they're going to buy wonder
why not to trade all full flash will be
rare so what you would say is when you
when you build the device you know how
many hashed how many blocks it has
physically and you need a hash table
that can hold that it can hold it
mapping for that many blocks and that's
how you its eyes the memory on the
device so in the host similarly in the
host you'd want to reserve enough memory
for whatever your fraction of dirty
blocks is so so remember what is this
relate so so we're boy doesn't use right
so the reason is we sighs the cache
differently for each workload according
to the size of the workload so this is
sort of you know one gigabyte cash and a
five gigabyte cash and a 100 gigabyte
cash or something like that and so
that's why the amount of memory you need
is different because the number of
blocks you're caching is different for
each workload it's a good question okay
so as I mentioned this was all done in
simulation this is the board that we're
currently using it's the open SSD
prototyping board and it has this any
links barefoot SSD controller it has
some memory some flash and you can
basically write your own FTL to go into
here it comes with three FTL's and it
performs relatively well so that I think
it actually provide useful results and
so my student has implemented the stuff
and he says it'll all work but he hasn't
actually tried it yet so we'll see what
happens but it's nice because I think
until this year was very difficult to
actually pro you know actually build an
FTL and do anything I know Microsoft
Research had a big effort to build
something on the bboard the b3 board or
something and this is a lot easier to
just use a commercial controller and I
think it's probably more representative
is limited to what you can do because
everything are sort of commodity parts
but for at least the FTL research this
works pretty well ok so in summary your
mother take of this research is that
sort of making flash look like a disc is
great for compatibility great for
getting devices out there but there's a
lot of neat things about flash that get
hidden when you do this and by you know
one thing we can look at is are there
better interfaces so we've looked at one
interface which is really designed for
caching which makes it easier to write a
cache and perform you know better in
some cases reduces memory consumption
and you know I think there's other
opportunities to look at other other
interfaces to flash sort of tailored to
specific applications
or even interface the flash that is sort
of subsumes this and handles other
applications as well yes is the only
point you use the non-military profit
too late fast recovery like instant on
cash yes we do so this is doing we have
a right back and it right through so in
all cases actually the cash is
non-volatile and so as soon as you turn
it on everything is accessible in cash
and I have I have a separate slide I
don't know if I have it with me that
shows the recovery time is a like half a
second or something like that whereas
the Facebook flash cash which had a
non-volatile option took about 30
seconds to a minute to recover because
it had to reload all the metadata into
memory so we do have that capability yes
earlier you talked about limitations of
flash as you plant lead 20 or 30 person
mg to get high end or the other so have
you done experiments were or you think
with this system you can keep the flash
now your simple and not take the big
permit or such as footnote so I think I
think it depends a lot on the workload
but what we can do i think is more
interesting as we don't we no longer
have a fixed capacity device we can
decide for a workload how much of it can
we actually use for live data if we have
a very static read intensive workload
where we're not doing any rights we
could use a hundred percent of the
capacity because we don't need to
reserve anything for incoming right so
every there's one copy of every block so
you can use all of it if we have every
right intensive workload what we want to
do is absorb rights as quickly as
possible like a write buffer and
coalesce over things that are
overwritten and then we might use only
thirty percent of the device or fifty
percent of device so really what we want
to do is size the device to optimize
performance rather than hit rate or
something like that
well the devices internally actually
reserved seven percent for logging
purposes and things like that yeah so
you can do so but you can definitely
statically I mean Intel recommends if
you're doing a cash you should
statically reserve you know create a
partition that's thirty percent of your
device and not use it more or less to
get optimal performance and this would
allow you to dynamically react to a
workload okay so I have I think I've
about 10 more 5 or 10 more minutes or so
so I'd like to set of go on I will I
will try to not go through this fast but
i'll try to cover the highlights in the
time that i have remaining so the second
piece is looking at what do we do with
storage class memory it's this really
great technology that can be you know
the dream of system architects of having
persistent memory that's really fast
perhaps and the question is how should
we expose it to applications so if we
look at what we did with disks with this
we typically have a pretty deep stack of
software that we have to go through
before we access the disk and if you
think about it there's actually a really
good reason for this so at the bottom we
have devices that have no protection
mechanism so a desk can't tell which
processes are allowed to access which
blocks so we need to go through the
operating system which actually
implements protection through filesystem
ackles similarly we use DMA to access
dma operates on physical addresses you
don't want user mode code doing DMA it's
not protected again we need the kernel
to handle the data transfer we have a
device with incredibly variable latency
and so having a global scheduler that
can reorder things can really optimize
performance and get you you know ten
times better performance if you do
things the right way simply because it's
very slow having a global cache where we
cache commonly used data across
processes makes a big difference so you
know for normal four disks this is a
great architecture and I think it's
worked really well over a lot of years
if we look at using storage class memory
things don't you know we can see that
things change a little bit so at the
bottom if we map it as memory we have
hardware protection so we can use
virtual memory protection bits so we no
longer need to go into the kernel to
implement protection because we're
accessing things directly with load and
store instructions we don't need a
device driver that mediates all access
and so we can get rid of the need for
sort of a global
Vice driver because we have more or less
constant latency we probably don't need
an io scheduler to reorder requests and
so we don't need that global another
piece of global thing and then because
the latency could be as good as deer and
we may not need as much sort of shared
caching across processes because it's
just not that expensive to go fetch
things directly from SCM and so what
this means is that we think that you
know all these things that said file
systems have to go into the kernel to
access data may not be true with storage
memory anymore and so what we set out to
see was could we build a way that you
could expose SCM directly to
applications as memory using memory
protection to control access so to
address this we build a system called
demo CD the student who did this is
Greek and so we were told to not use
this name because it was too hard to
pronounce but it turns out that nemesi d
is the Greek the Latin board for nemesi
news moneda and Steve Swanson's group
had a hit there there there PCM
prototype disk was called minetta and so
that was already taken clearly we were
consulting the same source of names okay
yeah well yeah but it's all in all
stories right right whenever I never
seen is the the the personification of
memory which is you know a great we
tried indian words also but we you know
we couldn't find one that we were
comfortable with anyway so the goal here
is at first we have a persistent region
in advert in your address space where
you can put data that survives crashes
and then we also have a safety
mechanisms that if you're doing an
update in that region of memory and you
crash you don't get corruption kind of
like file systems do journaling and
shadow updates to prevent corruption so
the basic system model we have is the
picture I showed before we've got drm
and SCM both attached to your system in
your address space we now have regions
that are volatile regions that are non
volatile you can put data structures in
them if there's a crash and something
bad happens the volatile data goes away
but your non-volatile data survives so
this is the basic mechanism we want to
give programmers so the key chat one of
the key challenges here is what if
you're in the middle of doing an update
like inserting into a linked list and
you've got your update partially done
and something crashes before you fix the
data structure when you restart the
system you'll find your data structures
inconsistent and you eat
need to walk your data structure to
figure this out or you may suffer with
incorrect data so this is what we would
like to help programmers with as part of
this yes it is a little bit slower than
he ran and it's getting slower than gear
am every year it's getting slower faster
than derezzed DRM is getting faster
faster than it is getting faster I guess
now there's no difference in the
vacation song could be like some you
know the cash the way the way with CP
you know casual stuff with you he's
longer yes so actually I talked so I I
agree they're definitely some system
integration issues that i've heard that
processor pipelines do really poorly
with variable latent long variable
latency events so if you have a right
that takes a micro second you know your
your pipeline is not built to have
things that are outstanding for a
thousand cycles or two thousand cycles
and so integrating this into the system
is not you know when we started this
work PCM looked a lot more promising and
the speeds we're going to get fat we're
going to be you know twice you know half
the half the performance of DRAM instead
of a quart of their performance and so
it made more sense but we still you know
the read performance i still think looks
pretty promising it's the right
performance that really would be an
issue and the way we handle rights we
actually sort of encapsulate the right
separately and so we can do that we
actually do the rights off the processor
pipeline we do non temple i'll show you
how we do that to sort of it not affect
other instructions but i agree that it's
definitely an issue what is there what
the reads are there the reads are
projected to be a lot closer to DRAM
reads the rights are projected you know
compared to what the rights are the
currently projection and rights are like
a mic or like a microsecond and reads
are like 200 nanoseconds i think ed
could probably tell you more about it
i'm if you have questions ok so our goal
is basically to make this consistent so
the first thing we have is a program
abstraction which is basically a
persistent region where you can declare
variables global variables to be static
p static variables which means they're
like global they're like static
variables that survived multiple calls
into a function these variables survived
multiple implications of the program and
this is really designed for sort of
single instant programs like internet
explorer or word where you're only
allowed to run one copy of at a time and
if you try to start a second copy it
will sort of kill itself and redirect
you to the first copy so this is a
pretty standard model for applications
that manage their own persistent state
to begin with we then also have a
dynamic
way to create this using P Malik where
you can basically do heap allocation and
our goal here is that you can allocate
some data with P Malik hook it into some
kind of data structure and then the
global variables the pset a global
variables or how you name this data and
find it again after a reboot so that's
sort of your name how this is how you
name persistent data and get access to
it again so the key challenge here is
how do you update things without risking
corruption after a failure and so here's
an example of the kind of thing that can
happen so suppose you are writing you
have a data structure that has two
fields a value field in a valid field
and the invariant is that you know the
the new value isn't the value shouldn't
be used unless the valid bit is set so
suppose you first set the new value the
old Bell bit was 0 so we know this for
example is invalid data we then set the
valid bit so now in the cache we see
that we have a new value in its valid
the challenge we have is that the data
only survives a failure if it actually
gets all the way back to SCM if we crash
and the data is in the cache it gets
lost so this is a lot like you know your
buffer cache of disk pages if they're in
memory you don't get to keep them if you
make it write them to disk you're good
the challenge here though is that we
don't get the control when things get
written back the cash itself decides
when things get written back and it can
write them back in any order so the
processor cash may evict this valid bit
before they fix the value bit when we
have a crash failure now what we see is
that we have inconsistent memory and SCM
and are invariant is broken Andrew it is
can you have faces yes yes I mean it's
the same basic ordering problem that you
have to solve it's an except it's set up
being visible to other processes it's
basically being you know how to memory
exactly so the basic thing that we need
is a way to order rights because
ordering rights allows you to commit a
change if you have a log you can say the
log record is done if you have a
transaction you can say the transaction
is committed a shadow up that you can
say here's the new pointer so we need a
way in existing to sort of order rights
and our goal is to do this as much as
possible out modifying the processor
architecture you know there's things you
could do for example there's some work
that I think ed was part of that look
that can we sort of introduce epics into
writes the cash
say this is the order in which things
was me evicted to preserve this ordering
on our approach instead is to say well
fundamentally what we need is a flush
operation which says for something out
to PCM and then offense operation that
says wait for everything that's been
forced out to memory to actually
complete so it's a lot like asynchronous
i/o where you're doing an F sin async
you know an F sync and then you're
waiting for it to complete and with this
we can sort of order any pair of
operations and make sure that they
actually become persistent in the right
order so if we do this we sort of can
make sure that we forced this this value
out before we we actually force the
valid bit out so if you're not from
Lucy's instructions move antq is a non
temporal store it's meant for streaming
large amounts of data out and it does it
isn't ordered with respect to normal
stores so it doesn't obey TSO or
sequential consistency and so it works
better i believe for long latency
operations and it basically it's also
right combining so you can sort of push
it's the best way to push a lot of data
out to memory and then CL flush just
flushes a cache line so this is useful
if you're kind of updating a data
structure in the cache and then you want
to sort of committed at the end this is
good if you know that you're sort of
writing a log and you want to force your
log out and you're not sort of writing
it in pieces or something like that then
we go three okay so we have these
primitives that allow us to do ordering
and on top of this you know we think
this this is useful but it's not what
programmers want to use so the first
thing we built was a set of data
structures basically we have a
persistent heap we use for p malik that
allows you to allocate and free memory
it keeps track persistently of what's
been allocated what's been freed we also
have a log structure here that handles
things like torn rights so if you're in
the middle of writing to the log and you
crash we can tell which log records are
complete and which ones are not complete
with very high speed and then on top of
this we have a general-purpose
transaction mechanism where you can put
arbitrary code within a transaction and
whatever within that code is references
persistent memory well actually be sort
of made atomic and consistent and
isolated and durable if the transaction
commits so I'll show a little bit about
how this works so here we actually just
reuse existing software transactional
memory technology we use the Intel
software turns
memory compiler so you can put atomic
keyword into your program here we can
put our two updates in there the
compiler will generate instrumented code
with calls out to a transactional
runtime system to begin and commit the
transaction and then every store will
also be instrumented with the fact that
this data was updated and what value was
written out there and there's a runtime
mechanism that implements a right ahead
redo log so everything that gets stored
is written to a redo log and then it
commits we sort of forced the log and
then we can lazily actually sort of
force out the data itself because we
have this log around and so by doing
this we get sort of full acid
transactions for memory and the nice
thing is this also because we're using
transactional memory there's a standard
problem in persistent memory is what if
you have a lock in persistent memory and
you you know you write out the fact that
something was locked and then you crash
how do you figure out to sort of clear
all those lock bits afterwards so the
recover virtual memory work from satya
had a special mechanism to kind of fix
all this but transactions avoid the need
to actually have any sort of state about
what's locked within a data structure
itself so here is sort of a high level
picture how you might use this this or a
hash table we have a static global
variable that we can use P Malik to
allocate space for one thing to note is
that P Malik takes the address that
you're allocating memory to as an out
parameter and this is important because
there's a race condition where you might
allocate memory and then crash before
you assign it to your static global
variable and making an out parameter
means that internally the allocator uses
a little transaction to allocate the
memory and atomically assign it to the
pointer so we can guarantee you won't
lose you one sort of leak data if you
crash in the middle of this to insert
into the hash table you basically begin
a transaction you can allocate some data
float with the key and value insert into
the hash and if you crash at any point
within this the transaction system will
sort of wipe away everything and make
sure that when you restart all of these
changes don't show up because you never
committed the transaction so that's kind
of the big picture of what we would like
to enable so our moment ation i think is
pretty obvious what you might expect
so evaluation we did not have any SCM
available or PCM to do this so we have a
very simple performance model we're
really focused on the latency of rights
does this make it faster to write data
and so we basically because all the
rights are going through a transaction
mechanism or through our primitives our
our data structures we basically
instrument all of the rights to SCM with
a delay a configural delay and we
default to a very aggressive 150
nanoseconds added latency on top of
accessing deram and then for comparison
we compare against running ext2 on top
of a ramdisk ext2 is pretty lightweight
so it performs pretty well in this
configuration I mean also here we're
largely looking at data within a single
file and not metadata operation so it's
really just moving date instead of doing
mem copies back and forth it's not
doesn't really stress the metadata which
is where a better file system might have
an impact so our micro benchmark is just
looking a hash table this is a
comparison of a hash table my student
downloaded off the internet and made
persistent using transactions against
Berkeley DB which is the thing that
everybody in the open source community
uses for comparison and what we compare
here is we have berkeley DB and we look
at three different granularities of data
and eight byte value 256 and a 2 k value
and we have three different latencies
for PCM 150 nanoseconds one microsecond
and two microseconds and what you can
see overall is that for small data a
small data structures or small updates
and for fast memory we do really well
and it's because we really get you know
very low overhead access to memory as we
get to longer Layton sees all the
optimizations that Berkeley DB does for
disk about sequential writes actually
help because PCM looks a lot more like a
really fast disk at this point similarly
for small data structures the
transaction mekkin m is pretty fast
we're copying small amounts of memory
but for larger data structures sort of
copying things a bite at a time through
the software transactional memory is
less efficient than doing sort of big
mmmm copies like berkeley DB and so we
don't perform as well at that large case
so we're really optimized for sort of
fast memory and very fine-grained data
structures so i will just sort of show
the highlights of an applique
in workload and the net result is for
applications that are really sort of
storage bound we can be a lot faster
this is a program that uses em sink to
force an in-memory data structure out to
disk and we replace em Singh calls with
transactions and it's a whole lot faster
this is a lightweight directory server
kind of like the active directory and
here we have some performance benefit
but not as much and mostly because the
disk is so fast or the the PCM disk is
so fast that there's not that much to
speed up in this case but it still does
make a difference so any questions I
nemesi need okay so I want to say a
little bit where we're going with this
so nemesi knee is really focused on a
single application with a single or a
couple regions of memory and we want to
see can we extend this the whole file
system so ed did some great work
building a kernel mode file system
optimized for storage class memory and
we want to say well if we can take
storage class memory map it into your
address space can you actually sort of
access the file system without calling
into the kernel can you just sort of
have user mode programs directly re
filesystem metadata out of memory and
find data on their own so we'd like to
be able to support sharing where you
have multiple programs sharing data
pretend perhaps at the same time we want
protection so we can control who can do
this and we want to rely on memory
protection to implement this and we also
want naming where you can have a normal
hierarchical namespace to actually find
things and the benefits we see is one is
performance because we can avoid calling
into the kernel which has some overhead
and second flexibility where it could
hopefully be a lot easier to design an
application specific file system format
because you could potentially just
writing code it user mode where
hopefully it's an easier environment you
can sort of customize the layout or the
interface for your application so we're
working on a prototype and this is what
it looks like and there colonel we have
what we call the SM manager and all this
does really is allocate memory pages of
memory or regions of memory it handles
protection so that it keeps track of
which processes can access which data
and it handles addressing by mapping
this data into user mode processes
address space and that's all it does
there's nothing specific to a file
system here we then have a trusted
server which implements
synchronization and sort of a lease
manager this is a lot like Franz upon
you if you're aware of it yes yes yes
I'll get to that in a bit good so we
have a lease manager that implements
sharing so this is a distributed lock
manager pretty straightforward we have a
trusted server here which handles
metadata updates and we this is the only
thing we really centralized because we
can't trust individual applications to
recur ectly right metadata because they
could corrupt the metadata and affect
everybody else so we centralize those
things the applications though have a
shared namespace here and they access it
by basically figuring out which page of
memory holds the superblock more or less
and then reading and parsing the
filesystem metadata structures right out
of memory and finding whatever data they
want and so far we have no cash in
whatsoever here we're just every time
you open a file we go and more or less
reread memory to figure out where to
find it and the nice thing is that
basically if you're doing a read-only
workload the application can run
completely within its address space so
that any communication at all except to
acquire some locks initially to prevent
anybody from modifying that data so it
has the potential to be a lot faster and
a reduced communication in a multi-core
system so we have some preliminary
results using file bench and a file
server protocol profiler about eighty
percent faster than calling into a
kernel mode file system we also
implemented a customized key value file
system where the interface instead of
open read/write close is really just get
input so we're bypassing creating any
transient in-memory data structures and
we also sort of support fixed sized
files like supposed to storing image
thumbnails or something like that and
here we're about four hundred percent
faster on a web proxy profile so this is
very preliminary and of course we're not
doing really complicated access control
lists like NTFS but it kind of gives us
a sense that there is something to be
had here so in conclusion you know I
think these new storage devices really
have a lot of new capabilities and
making them look just like block devices
conceals a lot of that interesting
features and the goal of my work is to
really figure out can we find new ways
of accessing these devices or making
them available where programs can
actually get sort of direct access to
their features and benefits so with that
I will stop and take any remaining
questions and thank you all for coming
anything left nope here's not okay great
well thank you all</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>