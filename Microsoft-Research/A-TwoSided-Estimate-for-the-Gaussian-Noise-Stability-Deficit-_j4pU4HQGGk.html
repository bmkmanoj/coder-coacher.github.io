<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Two-Sided Estimate for the Gaussian Noise Stability Deficit | Coder Coacher - Coaching Coders</title><meta content="A Two-Sided Estimate for the Gaussian Noise Stability Deficit - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Two-Sided Estimate for the Gaussian Noise Stability Deficit</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_j4pU4HQGGk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
ah so happy to have the third talk of
the day running Alden will tell us about
the Gaussian noise debility deficit
thank you ah so far I really enjoyed
talks in the center alright so we're
talking about the Gaussian noise
stability deficit let's stop let's try
to understand what Gaussian noise
debility mins and our starting point is
that actually actually the Gaussian
isoparametric inequality let's see what
that is so our setting in this whole
talk is just our n equipped with the
standard Gaussian measure this is its
density and the Gaussian surface area of
a subset of RN will just be defined as
the integral over the over the boundary
of the set of the Gaussian density with
respect to the N minus 1 dimensional
hausdorff measure so this is roughly how
much the Gaussian measure increases in
first order when we take an epsilon
extension of the set now the Gaussian
isoparametric and equality proved him
initially by Leyland to the coffee
filters on somewhere in the 70s says
roughly that isoparametric minimizer's
are half spaces in other words out of
all set whose volume is some prescribed
number the set which minimizes the
surface area is just a half space now
what will consider is some extension of
this isoparametric inequality which is
an inequality concerning to noise
stability so let's first try to
understand what we mean by Gaussian
noise so we say that x and y are jointly
standard Gaussian
with some correlation row which is the
parameter between 0 and 1 if either so
one way to define it is the coordinates
of x and y are all normal variables and
the covariance matrix is just such that
each one is just a standard Gaussian and
the corresponding X 1 and 1 Y once they
have correlation row between them so and
that's true for each coordinate
separately another way to define it
equivalently is the following we take
three independent star standard
gaussians and we say that x and y have
this common component root rho times z1
and then we add an independent component
to each so two x where this and to why
we had an independent copy of this so we
can just think about this as being the
actual thing we want to measure and this
is being the noise so x and y are the
same thing with if rho is closed one we
have some small noise which distinct
between x and y and we define the noise
stability of a subset a of RN as just
the probability that both x and y are in
a so we should maybe it would have been
natural to divide this by the product of
probabilities that x and y are in a in
some sense it measures how stable the
set is to know is given that x was
already an a why is some noise version
of x what's the probability how likely
is it that Y is also in a so that's the
Gaussian noise stability now a theorem
of crystal bellaire from them from mid
eighties says that well half spaces are
not only isoparametric minimizer's
they also maximize the noise stability
so for all sets with a given Gaussian
measure if I want to maximize the
correlation between x and x and y both
being in a i want to take this my set to
be a half space why is this an extension
of the isoparametric inequality so it's
not hard to see that when Rho is very
close to one when the noise is very
small the probability that X will be in
a and y will be in a in the complement
of a is is just more or less
proportional to the surface area right
because x and y have to be kind of close
to each other and this is just a
calculation that the second the first
order of the of change with respect row
of this as stability is just
proportional to the surface area so we
so this extends there is a parametric
inequality and I there ok this result
has many applications it kind of
connects many areas of mathematics it's
relevant in approximation theory in
rearrangement inequalities in
concentration in high dimension related
inequalities I just want to mention one
discrete application to this to the so
called majorities stay blessed theorem
so this is due to a elkin and muscle
o'donnell and a leash gavage and that
that's kind of a discrete version of the
same thing which states the follows so
as the following so if we have a
function defined on the discrete cube so
we can think about this function as an
election system it takes the votes of n
different people and the outcome is just
zero or one who won the election say and
we're thinking
out this point in the cube is just a
uniform point in the cube and then we
can consider noise so we can imagine for
example that the people counting the
votes sometimes make mistakes so there
is a probability epsilon for each vote
being counted that they regenerate the
vote randomly and now let's say that we
want to maximize the noise stability so
we don't want this this noise these
errors to affect the final outcome and
what the theorem says roughly is that
the best way to avoid this to get a
stable thing under under a condition of
low influences so this roughly means
that each voter does not have a big
effect on the outcome I don't want to
precisely define what that means but it
turns out that the best the most stable
thing is just the majority function so
just tell to sum them all up and check
whether they're bigger than some
constant my so the best real life
application that I could come up with to
Borel theorem is the following so say we
are collecting Street cats so we're
wandering in the street and we see cats
which have different properties like
their size and height and how loudly
they meow so these are all properties in
the real line that have Gaussian well
it's I guess we could expect them to
have gal a Gaussian distribution and
let's say that our goal is to we don't
want to break up families of cats so if
to cut our siblings I want to kind of
get a high correlation between the
events that I collect them both so I
want to maximize the expected number of
cats which are family members that I
collect and let's say we have
so our space as two parameters say
what's the weight of the cat is it light
or heavy and what the complex argument
of the cat is it an imaginary cat or a
real cat and I'm starting to collect
more and more cats and in the end I want
to kind of decide how I well what's my
criteria for keeping the cat or not
keeping the cat and it turns out that I
want to choose criteria based on some
half space like this if the properties
are not correlated it's easy to see that
it will be a coordinate half space
otherwise it will be some I have to do
some PCA probably and it will be some
kind of half space all right so we have
we know that half spaces are the most
stable sets now we can ask ourselves is
this fact robust namely if we know that
asset is almost as stable as as its
corresponding half space as the half
space which has the same measure is this
set in some sense does it look like a
half space or we could ask the same
thing about that is a parametric
question if a set is this if a surface
area of a set is almost like that / of
the corresponding graph space does the
set in some sense look like a half space
so more formally we would like to say
something like given the deficit between
the noise stability of the set a and the
noise stability of the of a half space
with the same measure is small is true
that the distance between the two sets
is small with respect to some metric and
what metrics would one consider so a
natural one at your own metric to
consider is just
a total variation distance oh just the
measure of the symmetric difference and
another measure one could consider is
just the master Stein distance between
the restrictions of the Gaussian measure
on to the sets and the first result we
have in this direction is by Ellen and
muscle and Joan M Neiman which says the
following so we define Delta of a to be
the minimum among all half spaces of the
Gaussian measure of the symmetric
difference between a and the South space
on of course sorry I I also want the
measure of this half space to be equal
to the measure of a so this is some kind
of total variation distance between a
and the set of all possible half spaces
and the result says that this quantity
can be controlled by the deficit so if
the deficit is very very small in some
sense the set is close to a half space
and this is true up to a constant which
depends only on the measure of my set
and on this parameter row in particular
it implies that the quality case can
only I mean we could only have equality
if our set is a half space up to some
probability zero change now ok so this
robust in quality admits numerous
applications basically wherever we we
have wherever broyles theorem is used
almost we also get some robust version
in particular for the majority's stay
blessed theorem we know a robust
versions
basically mature the majority function
is essentially the only function which
minimizes which maximizes the stability
and this this also implies for example
so this could be seen as a quantitative
version of arrows theorem for those of
you know what that is so it implies for
example so so this implies that the only
way to minimize the probability of
non-rational outcome in an election is
by taking the majority function under
some low influence assumptions by taking
growth one in some cases we get also a
robot robust isoparametric and equality
and okay hey I don't want to get give
details about this they conjectured that
this exponent for over here it could be
replaced by two and I just want to
comment that there is a slightly older
robust robust the robustness result for
the isoparametric inequality by a chunky
Fusco Maggie and telly I guess I said
that okay from 2011 chunky all right ok
now let's let's go back to this metric
and I want to try to understand maybe
there's a better way to capture the
distance between a in and H I want to
try to convince you that at least one
we're talking about noise stability this
metric might miss something and to do
that I want to construct a very simple
example and the example looks like this
so we're going to construct two sets
which are slight
perturbations of just the measure one
half half space on the line so let's
consider the real line and I and let's
say that this this is zero so the
measure of all of this is one half and I
want to take here in an interval of
measure epsilon and call it I to and
i'll call this thing i won so the half
space is just i 1 and i 2 now i want to
take this interval and just move it
slightly to the right and call it I 3
and so okay so my the set age would just
be these to think that the original half
space and a perturbation of H which I
call a will be just I 1 and I three
instead of I to but now I want to
consider another perturbation instead of
taking a 3 to be here I take this eps
epsilon mass and move it a constant
distance so I put it here so let's say
that this point is the inverse gaussian
cumulative distribution function of 3
over 4 so this is one half and i put it
in 3 over 4 in it called this I for and
the set B will be just I 1 and I for now
it's pretty clear that the distance well
the total variation distance between
both these sets and the half space is
just epsilon so Delta of this set is the
same but on the other hand let's try to
understand what the noise stability of a
and B are maybe I'll just so a is the
blue set
and b will be the black set
ok so to know what the noise ability of
a is I have to consider the probability
that both x and y are in a so that's the
probability that both x and y are in I 1
plus the probability that X is in I 3
and y is an i1 conditioning on expanding
I three I have factored to hear because
they can also replace X by Y and I have
an o of epsilon square which is the
probability that both x and y are in the
small interval now I have exactly the
same thing for be so if I want to
compare the deficit between these two
guys and the stability of age will okay
this suggests we have to look at the
difference between these two terms now
it's not so hard to realize that given
that X is in I three the probability
that Y is in I won the noised version of
X is well it's not so different than the
same probability but given that X was in
I to I didn't move I to so much to get I
three and if you calculate this you you
will see that actually the differences
of order epsilon on the other hand if
Rho is not very close to zero it's also
easy to say that given that X is in our
for when I say that x is here this
diminishes the probability that Y will
be here by by a lot I mean well at least
by some constant factor and if i plug in
these two facts to the previous formulae
what i get is that the stability of a is
the stability of age but well- something
of the order epsilon square while the
stability of bill
is much smaller because I move this
interval over here I get something of
order epsilon and well this suggests
that this metric doesn't capture what's
going on so well somehow I want to
capture not only how much mass I moved
but how far I moved it so this gets us
to the main theorem I want to introduce
and it's the following so let's try to
define a different metric namely what we
do is is this we take our set a we look
with all possible have spaces whose
measures the same measure as that of a
and we look we measure the distance
between the centroid of h and the
centroid of a so it's pretty clear okay
so if this is the origin if a is
somewhere here H will probably look like
this and it's not hard to see that this
measures how far I moved the mass and
not only how much mass I moved and I
guess this result could convince you
that this metric is somewhat more
natural what we get with this metric is
that again up to constant that depend on
the measure of a and on row this deficit
can actually be bounded from both sides
by the same quantity well up to some
logarithmic factor so in some sense if
we only care about knowing the deficit
up to Constance it's actually enough we
don't we don't have to calculate what
the noise stability of the set is we
just have to calculate this quantity
which is I I'm sure you'll agree with me
that this is simpler to calculate it's
basically just the one dimension
think it depends only on the marginal of
a onto a certain direction right now
this theorem has a few corollaries so
first of all the conjecture I mentioned
is verified since delta square is
controlled by this metric epsilon it
gives an improved robust Gaussian
isoparametric inequality because well by
taking growth one it turns out that you
can also get the limit case this is
another example of what you could get by
this inequality so for example if you
know that asset has a pretty good
surface area measure then well when Roy
is close to 1 this deficit will be
smaller which will imply that epsilon is
rather small and now you could use this
fact with a larger value of Rho and plug
your estimate on epsilon here and this
will give you some estimate on the noise
stability in terms of the surface area
so sama we know that the noise stability
cannot get much worse as we increase row
by using this two-sided thing any
questions so far because at this point I
think I'll move to some ideas from the
proof
ok so did this is uh well I I haven't
explained why but but basically the the
extremal example in this case and it's
not so hard to prove it is is the set a
defined here if you take the mass and
move it very closely you'll see that ok
for a delta square is of the order
epsilon and it's not hard to see that
this is the worst case you can be you
just project it onto one dimension and
somehow play with it ok this is a very
easy fact but not not it maybe not
immediate ok glad to help ok so let's
talk about some ideas of the proof so
what I'll do is mainly I'll prove
Burrell's result this is an Avelle proof
of Burrell's result based on stochastic
calculus and somehow in this proof we'll
see how the centroid of the set comes up
so I'm not going to really prove the
robustness friend but hopefully I'll
give an idea about how to do it alright
so we're interested in this quantity the
stability of a which is that just the
probability that x and y are in a if we
plug in the definition of x and y it's
the probability that route rose at 1
plus root 1 minus Rosie to an A and the
same for why we're z2 and z3 well that
one's at when Z 3 I remind you or just
independent standard gaussians what we
can do is definitely we can take
expectation over that one and inside the
expectation we can condition on that one
right we we did nothing here
and well when we conditioned on that one
it's clear that this guy and this guy
will be independent right so we can
instead of just checking that they're
both in a we'll just check that the
first one is an A and they take the
square of the probability at this point
what we do is the following let wtg be
just the standard winner process or
Brownian motion it's clear that w time
bro the Joint Distribution of W tomorrow
and time one is the Joint Distribution
of these two guys right so what i can do
is like i can replace all of this
expression by w1 and instead of
conditioning on that one adjust
condition on whatever happened until
time row so what we get is that the
stability is just the probability that a
Brownian motion time one is in a
conditioned on the filtration time Rho
squared until now we didn't really do
anything this encourages me to take this
probability to look at the dube
martingale the probability the w1 is an
a conditioned on ft this thing and give
it a name let's call it empty so we're
actually interested in the expectation
of em r 0 square now since empty is a
martingale by definition right to do
martingale this expectation by Ito's
formula is just the expectation of the
quadratic variation of the martingale
between time zero and time roll so all
we're interested is in is how much this
martingale really varies and in order to
know what the quadratic variation is
what we want to do is try to calculate
just its ito different
to do this what we do is okay so empty
is just this probability let's this
probability is just the integral of some
measure on a and this measure is just
the measure of w1 conditioned on WT now
w1 conditioned on WT is just the
Gaussian measure centered a WT we
already went we already used T of our
time interval 0 1 which leaves us 1
minus T seconds to go right so it's a
Gaussian was variance is 1 minus T and
empty will just be the integral of this
density ft / our set a so now we have a
process of measures ft which begins with
a standard Gaussian the Gaussian its its
center moves according to a Brownian
motion as the actual gaussians shrinks
right the variance shrinks and a time
one we end up with some Delta measure
and we want D of empty which encourages
us to calculate D of ft right so if we
calculate idea of ft ft well we have a
formula for it so we can just use Ito's
formula to calculate the differential it
turns out we get the following thing I
don't want to bother you with actual
calculation but i do want to give you
some intuition about what we get which
is pretty simple we get that the this so
this process measures varies in
infinitesimal time what happens is
really we take our measure ft and we
multiply it by a linear function which
is equal to 0 on the center if x is
equal to WTS it
well 20 and has a random gradient so
basically our process is we start with a
growl Gaussian measure and we keep
multiplying by linear functions with
random slopes I mean randomly
distributed directions and this kind of
makes sense because well if we think
about it in one dimension we multiply by
many functions which look like this 1
plus epsilon X and many functions which
look like 1 minus epsilon X with many
constellations each cancellation looks
like 1 minus epsilon square X square and
if we take this to some high-power we
get something like e to the minus some
constant X square which is a Gaussian
density right but not all of them cancel
some of them I mean we're in the end
we're still left with some terms which
don't cancel out and this gives us an
exponential which actually moves the
center of the Gaussian right so this is
I mean this is a very simple fact but it
turns out to be very useful and the
reason is it's useful is the following
so well if we wanna know what d of empty
is now we take we just integrate d of ft
and this is a linear function if and if
we integrate a linear function over the
set a well all we care about is where
the centroid of a is located right if
the centroid of a is far from the origin
this will change the mass of a a lot and
if it's at the origin we're multiplying
by a linear function we'll just do
nothing so the center of mass actually
appears here but the center of mass with
respect to what well with respect to
some random measure fft but well it's
not so hard to just change variables to
get
well ft is some is some gaussian and of
course i can make it a standard Gaussian
by just moving the center and dividing
by the standard deviation and if we do
this we just get the actual Gaussian
center of mass over set but the set is
not exactly the satays it's just the
settee I were which I moved a bit and
well I shrinked actually I inflated a
bit right so in order to know I remind
you that we're interested in the
quadratic variation of this process well
it'll be big if those vectors are big
right at any given time I'm taking this
vector and a multiplying it by an
infinitesimal Gaussian right so we
finally get that the quadratic variation
difference is just the norm of the
center of mass of of the Gaussian center
of mass of some translate of my original
set a and if we use the same change of
variables we actually find that the
measure of this set with respect to
which I'm integrating is just my
martingale Mt so it's each point in time
I have I moved my set a somewhere so
that it's Gaussian measure is exactly
empty and the quadratic variation is
just how far the center of mass is from
the origin I have five more minutes I
think right we started five minutes late
all right so what we want to do now is
to compare the quadratic variation of
the process on a which was an arbitrary
set to the quadratic variation of the
same process on a half space whose
measure is equal to the measure of a so
let's take a half space a
which satisfies this and define exactly
the same process let's call it empty
instead of empty and I want to come I
want to see what the quadratic variation
of NT is here we make the simple
observation that if we started from a
half space the set will always be a half
space right if we move a spacious and
shrink it it remains a half space so
that the analogous expression to this
would just be the same thing but note
that in case of a half space all of this
thing will only depend on the value of
the dube martingale namely we have the
martingale empty and the quadratic
variation of NT is just some quantity is
just some fun functional of NT what is
this function we take a half space whose
measure is empty and we look at its
centroid and measure how far it is from
the origin and now we have we now well
we just observe one very simple fact and
the fact is that if I have two sets
which have the same measure so I have
the set a and a half space was measure
is the same as that of a then the
centroid of H let's say the origin is
somewhere here then the centroid of H
will always be more far away from the
origin than the centroid of a right
because to get from a to H I have to
take this mass and put it here and well
it's just the monotone one-dimensional
thing this is a pretty obvious fact and
this is actually the only point in the
proof where we have some inequality so
but you
in this inequality what we see is that
ok given that empty and empty are the
same we know that this quantity must be
bigger than this quantity so the
quadratic variation of Mt is always
smaller then well the same thing we get
for empty so we have to diffusion
processes and we know that well when
they are equal one of them moves faster
than the other is that I well that
doesn't exactly tell us that the
quadratic variation of aunty will be
bigger than this that of empty we'd
still have some work to do what we can
do is well one thing we can do is we
couple empty an empty by saying okay I'm
up to some time change they are both
Brownian motions let's make them live on
the same probability space by just
saying that they are the same Brownian
motions and then the inequality we just
saw just means that given some given the
time of the Brownian motion the inner
clock of empty move slower than the
inner inner clock of empty and this
world with this coupling it's easy to
see that the quadratic variation of
empty will be dominated by then that of
empty which finishes the proof right if
we take expectation here we have an
inequality and actually this gives us a
stronger fact we have a stochastic
domination between these things which
gives us information about higher
moments which in itself it has some more
applications I just have to mention that
well at least integer moment were
already known the inequality was already
known by in a paper by muscle and
o'donnell but okay this is a new proof
of this so this just gives us the
inequality and let me just for one
minute in one minute try to give you
brief ideas about how to prove the
robustness so to do the
we have to say okay we know that the
process empty is the head of NT of empty
red of empty but by how much so we know
that whenever the barycenter is that
those distances are quite different
empty kind of accumulates the four empty
becomes lagged after NT but well this
this metric epsilon just tells us
something about this difference at time
zero and we want to say that somehow
well we want to say that given that its
large time zero it kind of remains large
for quite some time and to do that what
we well we kind of take the second
derivative of what's going on we take
the ito derivative if it'll differential
of this process epsilon T which turns
out to be dictated by the behavior of
some random matrix which this process is
related to and we can analyze this
random matrix well it's kind of a
stochastic random matrix we can analyze
it with some spectral tools and
telegrams transportation entropy and
equality is kind of a central turk
tooling the analysis and in the half a
minute I have left I just want to
advertise that okay this kind of
stochastic equation well it was pretty
simple for us to derive we just took a
very natural process and differentiated
it but we can actually given some
initial measure mu we can actually
define a new process using these
stochastic measures so if the initial
measure is not a Gaussian but but
something else we can still somehow
followed the same method and give and
get some kind of a stochastic evolution
on the space of measure
is and for example if we start with the
uniform measure on the cube in bed on
the discrete q But embedded in RN this
gives a new direct proof of the majority
stay blessed theorem with a slightly
stronger version the conditions we need
this is joint work with Elsa mimosa the
conditions we need are slightly weaker
and it turns out these equations turn
out to be a pretty useful tool in high
dimensional convex geometry yeah I guess
I'll finish here
and yet it went to talk
go so there is</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>