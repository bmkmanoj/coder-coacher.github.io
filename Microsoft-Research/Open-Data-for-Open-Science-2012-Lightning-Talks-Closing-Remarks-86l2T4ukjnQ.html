<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Open Data for Open Science 2012 Lightning Talks / Closing Remarks | Coder Coacher - Coaching Coders</title><meta content="Open Data for Open Science 2012 Lightning Talks / Closing Remarks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Open Data for Open Science 2012 Lightning Talks / Closing Remarks</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/86l2T4ukjnQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay we're gonna start again and like I
was saying the following presentations
is gonna go really quick each one takes
about 15 minutes so I'm gonna let each
speaker to introduce themselves and just
go that quickly go ahead Evelyn all
right she isn't there pronounce my name
because I'm French and so you know like
just in case so I am Thank You antha
evening gigas can you hear me sorry well
you know I can switch to French right
away right i mean if you're that tough
with me so so I work with Jana I'm
calligraphy on in the mattress of
research connections team and i am the
director of semantic computing at a very
high level it's looking at how we can
make sense of data at a detailed level I
really don't have time because i was
given like 15 minutes but having said
that what I'm going to do here I think
my goal is to have you go and try a shot
and actually not just that I want you to
go and try try shop my talk is done ok
so now if I reach that goal you know
record here so sorry you to see so
actually if you put me here just to say
that so i'm going to talk i'm going to
talk about information-rich programming
so it may be slightly different from the
talks you've heard earlier or in the
other days but i think everybody will
come back to the theme here because i'm
going to focus on the data part so i
think that's not controversial we live
in an Information Society we can all
agree with that and if I put that into
axioms I would say well proposition one
remember i'm going to talk about you
know some programming languages here so
let's have some fun proposition 1 the
word is information rich and i'm sure
you would agree with that proposition to
modern up
occasions our information reached right
like more and more it's not so when you
talk about data there is more like
should I say semantic information out
there right and I'm sure in your head
too however if we look at the
programming so here languages i should
say programming languages because then I
know that crystal was at the end there
she will be talking about natural
languages so here by languages I mean
programming languages there are
information sparse we don't bring all
that richness of the information into
those languages so that means that there
is a bit of a problem for the developer
who now wants to develop applications
where there is so much information
reaching you reach information out there
so what we're going to do we're going to
fix that with F sharp here I was told to
do a plug on this book I did not write
it just in case you were wondering
actually if some of you have read it
then used it you can teach me a thing or
two here but this is an extra book done
for the data scientist eyebrows it's
really really good by the way and the
inventor of F sharp was on that one so
we want actually to fix this problem
about bringing information into the
language using something which is called
type providers and which is actually
already today i will speak within the
visual studio with in your studio so
type providers the way to look at them
is language integrated data and services
but of course some of you might say well
but can you just tell us just a little
bit but you know before you go to type
provider what extra right so in just a
few minutes and why should I care you
care I can just tell you that I don't
have time to die but you do care so in
short f sharp is a functional language
practical and I don't have time to get
into all the details which means that is
not purely functional there is a little
bit of other paradigms going on in there
petite anel enables you to write simple
call code on to solve complex problems
actually so f sharp is open source and
not only that what I wanted when I
mentioned at the beginning I said I want
you to learn about that sharpen actually
I want to you to try try a sharp what
we've done Microsoft Research connection
because remember F sharp it's a language
which is developed which is part of the
visual studio suite but what we've done
microsoft research connections we have
developed this shrine f sharp which is
well what if i don't have visual studio
right what do i do can i still get to
flavor sense of what this extra piece
and that's exactly what we've done there
so let me see if I can I should have ok
so what exists today we have this site
which is where I would like you to try
and go and try there right this is the
track sharp website and as it says here
we are providing tutorials resources and
tools when you can start even if you
have never ever program you can start
playing around and there are you know
four people depending on your level know
this expert or just enthusiast you can
have different experiences even actually
right now F sharp is targeting the
developer the researcher the enthusiast
we are also here target target in the
educator so that's the side i would
encourage you to go there just to find
information just try it out so that's
what we have today so let me go back to
what I started with which is well okay
we have programming languages out there
but we also have a word where we have
more and more information richer and
richer information and we have not yet
reached the gaps between on one side the
developer community and on the other
side people who are producing the super
rich data and I said well one way to do
that is going to bring in this new tool
which is so to speak which are those
type providers
so the way to think about that is like
well now we're going to design languages
where are we going to bring this
information out there into the language
and the solution as i mentioned art I
providers there are reasons why ads I
didn't mention all the details about
that sharks but when we are very
interested in are also like the type
inference because when we start talking
about which data different types of how
can I use them right directly as I
program so to make it slightly more real
I'm going to go through a demo which has
been done for the word Bank here and if
you think of what this type provider is
going to bring is really what it says
here is dis intentions for data you know
when you type commands actually think
search to you type a word and now you
start getting like some recommendations
that all you men this or that so now
what these type provider the idea behind
the type providers one of the main ideas
is to bring this intellisense not just
for comment but specifically for data so
we are going to bring in some types into
as we program our applications so let me
give you an example so here this is a
site of the word bank there are
different data set and you could write
go and look at the data by countries by
topics data catalog Micro data so there
are different categories how can we do
more in a much more interactive way so
let me now so this is a screenshot of F
sharp and the type provider in action so
let me go through that so this one can
you see my cursor here yet so this one
we're going to look at the data word
bank door which is decided which has all
the data and let me just go through the
demo and what we're going to do is like
we want to access the data of this word
bank yeah and now because I have a type
provider who knows about those types
during the data set so now I can as i
type i said well you know I'm
interesting the countries will actually
which countries are my interested in and
I
do that as I program i'm bringing the
data set so to speak into my programming
environment as i type right so he in
this case i'm interested in greece and
actually the debt what which was very
timely at the time of this demo being
done and and then i can do typical
typical things we can do when we program
i can sort things i think we're going to
sort we're going to do this one here
we're going to sort some of the data and
later and we will also be able to
visualize as we go what's happening
there right so that's really the idea so
the intelligence which is the
visualization from 94 to whatever 2010
this is what happened with the GDP of
greece and now well you know i'm
interested in other country so here i'm
selecting greece and denmark and all
that comes because with my type provider
now have access to those types and this
is one of the functionality of what you
can do when you have a type provider
okay and here we just then you can
visualize and show different the
different gdp of all those different
country so that's what we mean by
bringing into the language this rich
information like now I'm bringing those
types I have all this data which is out
there and there are other things i'm not
showing sheer like on which are really
powerful to into like you can check on
the lat/long and things like that but i
think I'm kind of running a bit out of
time so there was another demo with skip
it but again try try f sharp and then
you'll be able to see more so what is
interesting about F sharp so this is not
about you know going back to programming
and same going back to program in
principle and see no we're going to mix
languages and data this is not what it
is right see this is done via those type
provider and there is still at repeat
step doesn't have take a right either
did this separation and actually you can
also write your own type provider when
you go and find out about try try up
shop
so some of the application so we already
have some applications actually out
there some examples but not a lot of not
on all of them we have some on web data
the data markets also with the azure
data market there are some out there
right now I'm working I don't know if
it's here well some semantic data so we
are trying like different type providers
and we will encourage the community to
try yours right i mean like if you want
to look at were talking earlier i was
talking with you but you know this
neural network well you know you have
your graph can you do something with it
so anyway so the ids like you can try
and beat your own so and these are the
properties again of the language they're
strongly type which is what we're
interested in here so of course if your
data is it not some thematic Ali ridge
that would be an overkill right so here
we're talking about data which is really
semantically rich so in summary the word
is information rich that means that well
how can we bring it as we program so we
need to be information risk to and i put
the type providers at the rescue if you
use F sharp I recommend that you go back
to this tribe sharp so by the way right
now on the tribe sharp dot org we sit on
top the type provider there they do
exist if you if you have a visual to the
environment you can you can actually try
out tile providers but if you don't we
are working on it and so come back to
this side and you be able to find more
information all those high providers
within a few months also the blog so
done time and let me actually go and to
this slide because I really want to
acknowledge so don't sign with actually
he did the forward for he basically the
person who his driving the inventor of F
sharp and he's also working on the tribe
providers he's a principal researcher
and I also want to acknowledge Judas
bishop who is the one who started this
try a shop project working with
dongsaeng which
like well what if you don't have a
visual studio environment I can try
something and then if I really like it I
can go and get the full experience
better functionality in the visual
environment right and also Kenji Takeda
with a colleague in the UK who prepared
some of those demos and with that i'm
done and i don't if you have questions
try try off sharp great my name is Alex
Wade I am what am I I'm director of
scholarly communication within Microsoft
Research and how many people here are
familiar with microsoft academic search
how many people have seen it before okay
so twenty percent may be excellent
somebody said a couple years ago in a
slightly different context that trying
to trying to get semantic information
out of PDFs was like trying to make a
cow out of hamburger and that's sort of
how this project started it started as
an effort to take some unstructured data
to take PDFs that were found out on the
web and to try and recreate some some
semantics recreate some structure out of
that really focusing on some general
things that exist in research papers
first of all identifying which of the
PDFs that were out on the web were
research articles and then once we
discovered that made that decision try
to pull out the things like the author's
the author affiliations the references
of information out of that and for a
couple of years this grew up around the
computer science domain and over the
past year we've sort of moved away from
just finding research that's available
out on the open web and have made a
concerted effort to reach out to the
publishers and then try to start growing
this up to all of the published
literature right now we've indexed about
38 million publications we have a lot
more publications in the queue but
really what this is is trying to get
beyond this idea of just having a simple
search box that returns papers and
instead for us to create a graph of
information that is about the
relationships between the papers and the
authors the papers and the papers the
authors and the institutions and then to
be able to analyze that data in a number
of different interesting ways so in
addition to the search box we also have
a browse interface you can drive right
down into each one of these areas and
I'm actually going to jump into demo
mode if the demo God's will will
participate here and what you can do is
jump into a domain we have a very simple
to level deep subject domain here and
within each one of these domains without
even having to do a keyword query yet
and maybe a little bit slow you will get
ranked lists of all of the the top
authors the top publications the top
journals and the the top conferences in
that domain and there's a number of
different ways that you can start
slicing this information you can look
just in the recent recent years if
you're looking at organizations you can
slice that by geographical region etc it
does also work as a search interface as
well so if I'm searching something like
hydrology you do get a set of of search
results and you can see here at the top
where we've actually done a poor job of
extracting author's names out of the
document but if I do search for an
author let's pick somebody on our team
like Robert here's a case where Robert I
think he's published some some
publications under rob some under Dennis
some under Dennis are and the the
challenge here for us is to find the
publication's under the different
variants of his name and to aggregate
them together into a single author
profile page so one of the popular
features of the site here is as and
something that Google Scholar is started
to to copy from us in the in the past
few months is to create an author
profile page that gives a sense of the
publication history of Rob the citation
history of Rob's publications either an
accumulative or in an annual format and
over on the left-hand navigation a
number of different
pivots around Rob as an entity here so
you can see a list of his co-authors
conferences that he's published at
journals that he's published within and
the keywords that have been extracted
from the full text of his publications
if I go into a publication record let me
dive into his list of publications and
sort these by citation count within each
publication itself we also offer a
similar profile so here is a entity page
a profile page for a publication that
will show how this publication has been
sighted over time as well as extracting
from citing publications some citation
context so this particular article cited
Rob's article here and this was the sort
of bounding text within the full text of
that document that the the author of
that the sighting document is referring
to if you scroll down the page you'll
also see things like the references that
were included within this article and
these are things that we pulled out of
the PDF itself and then links to this
citing articles themselves and we do the
same thing for a few other entities in
the system as well so here I'll pivot on
to the journal page and you get the same
idea this is a page it aggregates
together all of the information about
the journal Geophysical Research Letters
the top authors within that journal etc
okay so that's the general idea of of
Microsoft academic search back into the
deck here just to round out a couple of
other the entities in our system we
provide that same sort of information
around keywords where we have extracted
a keyword or key phrase out of the out
of the literature and try and marry
together occurrences of that phrase in
either the initialism in this case
there's sometimes it shows up as g-csf
and sometimes it shows up as not going
to try and pronounce it the long version
of this and we'll pull definitions then
out of the literature so if you're
actually doing research in it in a
particular area you can get a sense of
how that particular phrase so you can
you can
research here for things like SARS and
you can see definitely when when SARS
literature peek in about two thousand
five and how that's tapered off over
time and then lastly on the talk about
journal pages we provide an
organizational roll up as well so will a
great together all of the authors
institutional affiliations and provide a
timeline view of an organization and
then as well be able to do things you
can see up in the upper corner here we
have this comparison button that allows
you to put in any two organizations
yikes I made made my presentation
unhappy so if I jump into a live demo
gone bad here there you go so if you if
you click on that organizational
comparison button you can enter in a
second organization and pick the domain
that you want to compare here either at
the top level domain or the sub level
domain and what you'll get is a
comparison of the publications in those
two institutions over time tag clouds
here that show these in this case are
the extracted keywords from the
literature that are more unique to MIT
in this case the ones that are more
unique to Stanford in this case and then
the areas of that Venn diagram where
they overlap and then down below you'll
get a list of the top authors within
that domain so it's another way of sort
of sorting through and comparing
institutions within a certain domain
area we provide a number of other
visualization features if you go to the
home page you'll see a little left
carrot and right carrot that allows you
to pivot between the domain based browse
view of the site and a number of our
visualization tools but I can show you
here that we provide a number of
visualizations for things like citations
citation paths co-author graph a
genealogy graph that shows a
relationship between researchers and
their advisors if this is displayed all
right so that is the site that's the
sort of end user portion of the site but
because this is an ongoing research area
for us in ways that we can do this
entity extraction do a certain level of
deduplication of author records of
research article records but also
merging these things back together and
aggregating them at the proper level
we've opened this up we're crowdsourcing
the actual editing of the site as well
so we allow anybody to come in and to
make edits to the records you can edit
the author metadata you can do things
like adding profile pictures and adding
institutional affiliations you can
change the data in there if we've gotten
that wrong you can merge authors
together if there's multiple forms of
the same author and you can add a new
content in the site either by uploading
a bib tech file or by providing us links
to PDF records and this is something as
I mentioned the beginning that we're
constantly growing right now we're only
at about 40 million publications and we
expect to double that over the next six
to eight months as we consume more
information that we're acquiring from
the from the publishers there's a number
of ways you can get data out of the site
as well so we provide a couple of
widgets that you can take these are the
Silverlight controls that show the
co-author graph and the co-author path
visualizations and if you navigate to
those pages you'll see a little embed
link on that site that that allows you
to grab a little bit of JavaScript and
put it into your own site that will then
make a runtime call back to our database
and display that that Silverlight
control on your own page as well as
things like publication lists in
addition to that everyone of the
entities as well as the queries is
accessible via an RSS view as well so
you can go in and subscribe at the
journal level or subscribe at the author
level and the RSS feeds you
get any updates for new up new
publications or updated publications
that have come in for for that
particular entity the author or the
publication or the journal but if you
want to go deeper and you really want to
interact with the data that we have in
the system we do have a public API it is
a limited to non-commercial academic
purposes only but if you want to get an
app ID key you can go to the site and
there's a link that's a not a URL that
you're probably going to write down
right now but at the bottom of the page
there's a help link and click through
that and there say how do i request an
application ID we'll send you an
application ID and there's lots of great
things that you can do with the data
just to give you a very simple example
here I have improper use of Excel here
this is my excel spreadsheet as string
concatenate ER and what you can do here
is you can request any of the entities
that i mentioned out of the system so if
i want to do things like i want to get
authors out of the system and I want to
get authors from let's say Microsoft and
I want a query term to be databases
though authors who've written things
that have something to do with databases
and we return data in a number of
different ways the the easiest thing to
work with is the the JSON service that
will return JSON records we also have a
soap interface or return XML but I'm
going to cheat and use something just
for demo purposes which is piping that
JSON data into a an HTML table just grab
that URL that stuck my application ID
into it stick into my browser and one of
the limitations on the API right now is
that we only allow you to request a
hundred records back at a at a call and
you can only make 200 calls into the
system per minute and we try to throttle
that just because we don't have this
running on a
huge number of servers at the moment as
you can tell by the performance but what
this will now do is it will fetch a
hundred rows of author data out of the
system and give you a next identifier if
you wanted to fetch the next records
from us you will also get the author IDs
if you want to then turn around and make
a secondary call and get information
from the author that will then give you
all that authors publications as well as
ways of pulling citing publications from
them and with my final demo gone
horribly wrong the last thing I wanted
to mention was my information our
Twitter hashtag follow us on Twitter or
feel free to subscribe to our Facebook
page it's called scholarly communication
at Microsoft if you want to keep
up-to-date on on new releases of
academic search and new ways of
interacting with the underlying data any
questions
say that again we allow searching on do
eyes in the UI one of the things that I
want to add right now so we have a very
minimal advanced search here so you have
to put in a DOI colon and then the DOI
that you're looking for we don't allow
right now wildcard voi searches we just
have an implemented that yet you can't
do a prefix if you want to get
everything back for a journal and we
don't yet support queering by do is in
the API so we just need to add that
anything else yeah
I'm sorry can you say that again what's
a paper and what's not a paper
yeah yeah so so it's a good question and
the short answer is we don't we want to
be from a philosophical point of view we
want this to be as as inclusive as
possible we are focusing on our efforts
right now on getting in the journal
literature and because we also then are
doing this entity extraction around the
citations within the literature we have
the sort of secondary effect where we
end up having a lot of books in some
cases web pages in our index because
that article that we've indexed is
citing a book or a chapter in a book and
so we will have other artifacts we're
not yet focusing on going after the
monographic literature we may attack
that in the future there's a few other
areas that we'd like to explore like
bringing data sets in bringing software
as entities into the system so we're not
going to prevent things from coming in
and we are going to focus on getting
comprehensive in a few other areas as we
cue those up okay so um in keeping with
exactly the what Alex said there at the
end about hooking up data to these
publications I'm going to talk a little
bit about the data curation for Excel
that Lee mentioned on the first day
breathe in brief and then I am going to
go on and talk a little bit more about
what's on the agenda which is Microsoft
translator hub first of all before I get
into this space what this tool is
targeted at is the long tail of science
so for those of you read the fourth
paradigm you'll know that this these are
pokes that are still using just the
desktop to do most of their compute
power and also gathering data and
typically saving it within Excel this
project could not be possible without
partnership we're actually currently
working with the merit repository at the
at the University of California creation
Center we're also working in conjunction
with Gordon and Betty Moore who's
co-funding this project
with us and data one is also one of our
partners we're trying to be compliant
with them and the repositories that are
under the data one umbrella so what the
vision of it is is to aid scientific
discovery by ensuring that data
management and repository compliance is
done for the data that's been captured
inside of Excel which is a lot of data
right now particularly in the
environmental scientists and that's
actually our original focus for we're
going to focus starting with the e3 area
where people are doing things on climate
change and environmental science
oceanography and atmospheric data we are
partnering with with data one we want to
be a part of the solution while we
ourselves are not in the business of
doing storage search and curation
collaboration they are what we want to
do is facilitate them to achieve their
objectives that have been set out by the
National Science Foundation we want to
be able to make it very easy for people
who are maybe a little bit less
comfortable with interacting with
repositories to upload them very easily
from within Excel or to do a
drag-and-drop if they want to use our
web service we do have an ad in solution
that we're going to be providing in
addition to providing a web service both
of these applications that we're
developing that are fitting under the
data curation for Excel are going to be
made available open source as Apache
chief window we're thinking about not
only putting that in outer curb but also
in the Apache outers open source
repository so people can take these
technologies and build on them as I said
before we're going to start with III
data but one of the objectives of this
and we just added a milestone to the
project to to do was to allow this
particular application be very
extensible so while today it's compliant
with data one that means in the future
could being compliant with some of the
other types of repository activities
that are happening in particularly in
Europe right now so the whole way along
since the beginning of this project has
been to engage with the community and
we've literally been collecting
requirements for nearly a year now and
as was mentioned on the first day all
the the documentation has been collected
by a person who's our community program
manager and there's a lot of links here
if you want to actually go out and take
a look at the data that she's been
collecting and what we've done is we've
boiled that ant down into a very
a developable requirements spec that's
currently underway we are just in the
early phases of development we have a
force milestones development cycle
that's going on right now and we're just
in our first milestone but I want what I
want you to take away from this really
is that we're not building this in a
vacuum we very much engage with the
community and we are building what their
they've specifically been asking us to
build once we have a beta solution that
will happen between our third and fourth
milestones we're going to put this back
out into the community let them use it
tell us where they think improvement see
to happen and they will we'll have a
final milestone to address any of the
changes that they may have recommended
we are looking for additional partners
particularly for people who want to pick
up the code or work in conjunction with
this to deploy it on other types of
repositories and our plan ship date is
this summer so it's a very aggressive
schedule our plant scenarios really are
the first one that the that we got hit
with was I'm out in the field I'm
sitting by the campfire I collect my
data but the minute I get back I want to
be able to upload stuff to the
repository get a unique document
identifier so we want to be able to have
that happen and have it happen
seamlessly for them that they can
actually get this unique document
identifier that they can later use for
citation in publications so for instance
if they were to publish something in
PLoS you would be able to see the
particular paper that they've written
but then also be able to access it
directly from acts as the data in the
repository from the unique document
identifier they can upload the new data
at any time so for instance if the data
changes they can add to this without
changing their do I and then we want to
make sure that whatever David goes up
there is repository compliant and this
refers to both the data that's in the
document itself within this Excel
spreadsheet or in the Commerce separated
value document itself and also we want
to make sure that it's repository
compliant with regards to metadata this
is how we enable the search and
collaboration to take places by speaking
you know handshaking with the repository
finding out what the appropriate
metadata it is bringing that into Excel
and prompting the user to fill in that
information we also extract any document
or file properties
and immediately populate that into the
metadata if repositories cannot take a
native Excel document we check to make
sure that it is comma separated value
compliant and suggest any corrections
before they upload it so that they don't
upload it and then they lose their
charts that lose their tables they lose
all kinds of information we actually
allow them to convert tables to texts
and things like that all right strengths
our stretch goal is that in that fourth
milestone if we've done our job right
gathering all the requirements we may
have some time to actually implement
some of the extensibility to other types
of repositories so now for something
really truly completely different I
tried to stay on track so as I Valene
foreshadowed I'm going to talk a little
bit about natural language processing as
well this is where my background is I'm
actually a machine learning person I've
got a lot of work in in NLP and one of
the things that should be of interest to
this group and also you got to see a
little bit of a demo earlier from our
jomon is being able to translate between
two languages so that mobile application
he was showing well it was built on top
of Microsoft translator what we're doing
is taking that a step further so the way
in which we've developed translation
systems in the past have predominantly
been very customized so we gather
amateur data we we tune it we train it
and then we ship a language out on being
but that isn't something that's a very
extensible wow that isn't something that
that can scale up to a large number of
users so the purpose of the hub was to
actually create a platform where we're
by anybody could create a translation
system between any two languages so one
of the things that that enables it
allows you to take advantage of some of
the linguistic properties are similar
between languages and build your own
custom translators but really when you
think about some of the scientific
communication barriers that exist it's
the try and dresses what as well so
scientific publishing is starting to
take place more and more in a variety of
different languages and that creates the
communication barrier for your students
human translation is slow it's not
specialized and it's extremely expensive
so being able to automate that process
will be valuable and each
maine has its own lingua franca so even
though a word might mean one thing I
remember when I was working as a
research fellow at the National Library
of Medicine one of the computational
linguists there said if only the
chemists would speak the same language
as the doctors well that's just never
going to happen it's what I told them I
said you can just forget about it so
it's ways to be able to map over the top
of those or create very targeted domain
specific models would be very valuable
and there is there's a high lot of high
amount of ambiguity even within domains
and so if you're using a general-purpose
automated system like being translator
it's probably not going to do very well
on very highly technical text so what we
wanted to do is create a solution that
would bridge the gap between translating
online materials through automatic
translation wanted to create a system
that would take very targeted very
specific inputs and create a custom
system that you could access through our
standard api's and so those are HTTP
Ajax soap and odata so literally you can
tap into our api's create a very
customized system and then be able to
translate that so for instance a use
case for this would be able to translate
online course materials from books that
have already been translated that are in
a particular domain such as chemistry or
oceanography you can actually create a
custom system and that creates a private
model so now you're not accessing
actually being what you're accessing is
a private model that's your own custom
model and yet your your website can be
translated automatically using this very
specific domain model and then and it is
it's perfectly accessible so what i
thought i would do is this is the system
and i was hoping to have time and i do I
have a little bit of time to give you a
live demonstration so I'll go ahead and
go into the system
well it would help like type
you
so I have a lot of projects that I've
got running right now several of them
some of these actually haven't ship mom
is actually our most recent language
this is a completely community developed
language so Microsoft didn't have to go
out and custom tune it we gave them the
hub they were our alpha testers they
uploaded their data they created their
own system what I'd like to do right now
is to show you how you could create a
project on your own so let's just call
this oceanography and two languages
throw them out at me Italian okay oops
another language mom okay do that but I
think what's really interesting here is
I can go ahead and select it but I don't
know if you've seen what's available in
Microsoft translator today but it's
actually 38 languages now that we've
added Hmong this list is considerably
longer there are over twelve hundred
languages that we presently support now
there are more than 7,000 languages in
the world but many of those are not
actually written languages so we only
support a few of the click languages in
fact this very first one I mean I don't
know how to pronounce that but that's an
actual click language that's spoken in
Africa but I can also just type in
Italian here and get it right away
because i'm lazy and I don't want to
search through that system and then even
different dialects so we have green and
we have white the white is the one that
we're shipping today that's monga here's
what here's the trick here's the thing I
really wanted to show you so if you want
to sculpt this to something like
chemicals or do something like you know
science you can do that and then further
classify it down here so I can go ahead
and now create a project and this is my
own private project I'm given a
application ID that allows me to access
it
I just go ahead and I can manage my
members here this is where I go out and
invite community members to help me
improve the system review the system
upload new data we have two different
types of relationships so we oftentimes
meet with people who want to create that
custom model but then they have worker
bees who want to actually manage the
project so we have co-owners and then we
have reviewers these are people who you
trust to review and then there's also
communities which are in a kind of an
untrusted relationship but they're
allowed to submit as many translations
or corrections to the system as possible
okay so I'm going to go ahead and stop
there because I'm out of time but if you
guys have any questions we all take
notes now 22 tots and one extra value
yes
oh so actually it's interesting that you
mentioned that it's it it's because it's
a controlled language it's actually
possible to do it what we would working
with our Microsoft Research age a team
to do is to try and translate pseudocode
in and out of sequel sure right yeah
um you can you as a project owner can
narrow it down so that's actually for
you so that means you'll know to upload
data that's in that specific domain so
yet so I think the answer it is metadata
right it is metadata right now however
we are working on domain specific models
within languages which would enable the
feature you're talking about uh-huh okay
sure ah I'm Giglio I'm a researcher in
the sensing and energy research group at
MSR I'm going to talk about a project
that's less of a product is more like a
research project in collaboration with
ms or connections team in particular eng
and den phase team a bunch of people
also work on this project we have our
collaborators a durometer actually
sitting over there and changwon from
china is helping us building some
hardware platform so the goal of this
this particular project is really try to
cut the we're call it cutting the long
tails of environmental observations
meaning we want to move beyond the
typical infrastructure dwai of
monitoring and do environmental sensing
you can see a couple of examples here
some of them we did together with
scientists in a Swiss and some are
collaborating with people in Brazil are
putting these very high towers and
environmental fencing equipments into
the the forest and this is a project
that if I Berkeley have done to actual
crime up to these redwood trees and put
sensors on those trees to monitor on the
humidity distribution these are what i
would like to cut infrastructure
environmental sensing these are you
actually plan it you did provisioning
and carefully install the sensors
carefully and try to understand what
variables you want to sense and
carefully manage the data that come out
of these in the last several years in
particular with the proliferation of
embedded devices and personal consumer
devices another paradigm sort of emerged
as what people call the participatory
sensing where you have a goal you're
right you want to derive some
collaborative behavior or information
out of sensors but you don't really have
a way of provisioning right for example
let's say want to build the traffic out
of users mobile phones gps data you
don't really have control to say this
person should go on that street and so
on but instead you rely on crowdsourcing
a lot of people having your applications
are driving on the road to provide your
data and you merge them you take
advantage of the coverage and the
opportunities are the so-called
opportunistic sensing and immersive data
to to create what you want and people a
lot of projects or systems has been
built around this concept for example a
being has a photo physicist application
that you can take pictures from a
different person different people and
different locations figure out where
these pictures come from and actually
stick together and to give you a view of
a paramedic view of the either the
indoor or the order environments people
have done this for the signal strands
for mobile phones for a noise level
sensing and so on so I really become a
new way of collecting data with the this
in mind right we look at what people do
in the first generation of mobile
sensing what people what kind of devices
people do in either sensing well
wildlife animals or the participatory
sensing for environmental sensing and
other places we see this rather bulky
self-contained
and usually not very efficient devices
this is early work done at Princeton cut
so called the zipper net and actually
put a big color on zebra or try to track
their locations that's a device that
Intel built to do environmental sensing
actually it's a it's basically a cell
phone that you a clip on your backpack
and you carry them around of course you
can use your phones and the sensors on
the phones to do a bunch of sensing work
but usually those are not enough for our
environmental science applications you
want to extend them and that's exactly
what we're trying to do we're try to
extend this to the next year of embedded
devices with the sensors that serve as
the accessory of these mobile phones and
then rely on the powerful cloud and the
accessibility of the the platform to get
these data are available to the end
customers and users and in particular
this project is looking at the sensing
platform is a sensing platform that
portable provide useful information and
can be easily connected to mobile phones
and this is the project Clio in
particular start with actually building
a card where platform that has several
interesting properties on it one of them
is well if you think about a mobile a
sensor platform that will prevent early
provide your data you really care about
two most important things or time and
location right you want to know where
this particular data point is collected
and for that we have we look at a couple
of interesting ways of getting those
information and in particular we take
advantage of the so-called WWV be this
is a radius protocol that actually a
single station covers entire United
States and there are other similar
systems available in different countries
that broadcast a atomic clock signal and
you can receive it using very low power
to to synchronize your device crop there
are
cheese and wall clocks that build this
way and we incorporate that into this
device the other is our own research
results that later actually spend a lot
of time work on a cut leap which is low
energy assistant positioning it's a way
of Apple offloading GPS processing to
the cloud rather than do it on the
device and you know as a result we can
actually save orders of magnitude energy
consumption on the GPS sensing site and
the third one is the communication to
the phone this we borrow idea from our
colleagues at University of Michigan
they it's actually old idea to use audio
ports use modem like communication to
the phone so you don't have to rely on
the particular model of the phone and
particular connectors or wireless
protocols because the audio port is
pretty much consistent and uniform
across all mobile devices and from there
once the data got into the mobile
platform we can upload it to the cloud
we support Oh data for the data
management and accessibility on in the
cloud and WT as the way of visualizing
and mash up different kinds of data so
just to go a little bit detail into the
technologies WWV be in united states
there are these towers or antennas in
colorado they use a long wave radio to
broadcast these signals and we obtain
these receivers that actually consumes
like a sub milliamp power consumption to
receive these signals and in fact you
can you only need to receive maybe a
couple of these signals a day that's
enough to synchronize the cloud to
provide good enough information for the
location sensing because GPS also need
clock another interesting technology is
the the low-energy GPS technology
instead of doing the GPS sensing on the
device which takes a lot of signal
processing and CPU cycles the device
just buffer the raw data and we have a
way of using only two milliseconds of
raw GPA
data together with the timestamp that's
collected by the WWE be radio and and
put that together with the actual sensor
data and send that to the car we're
actually archive it on the device and
upload it to the cloud later on at night
when you have a connectivity and power
and then the cloud user software-defined
GPS technology to actually extract the
information everything you need from
that to millisecond of law data in
comparison terms of energy efficiency
compared to typical mobile phone-based
just black box GPS sensors on the phone
we're like three orders of magnitude
more efficient so we're talking about
just give you a sense of analogy with a
pair of double a battery we can run this
GPS this way of sampling GPS for one
sample a minute and the device can last
for three years that's the kind of
energy efficiency where we were shooting
it but in comparison if you turn on your
GPS on your phone the phone died it's
probably in six hours audiojack
communication again this is a technology
wise it's not that surprising but it's a
clever way of adopting to all different
kind of phones so you can get the data
into the film the way it works is pretty
much like your old age modem over
telephone wire um put that all together
we've built a hardware platform that has
a solar panel that do I need scavenging
and has a thin film battery it doesn't a
house a point seven million hour of this
is probably to one of two thousands of
what your phone battery without right
it's a very thin point two millimeter
thick thin film battery that's charged
by the solar panel and then we have the
GPS and the rate and the clock
synchronization radio for this prototype
will have light and temperature humidity
sensor on it and this but it's
extensible you can add more sensors into
it
we really put this into the mode is to
put it into a open source a design so
when we're ready we're going to release
the design you can take it it can build
your own offense or platform you can add
your own sensors and we provide the
firmware and the hardware so you can
communicate out that it upload the data
to the phone and we have web we have web
services under construction that would
take take these raw GPS signals and
convert it and turn them into a lab to
your laundry locations if everything
goes well and hopefully we're going to
release this in the fall this year
hopefully by the fact that is some it
time when you if you're interested in
coming and from there hopefully we can
build a community of people using using
these technologies to do environmental
science
their audio thank you for patients
during setup here so right now I'm
playing a tour I created where we're
bringing in orbital elements for the
International Space Station putting a 3d
model here with the that's actually
following the orbit of the International
Space Station along with a shuttle here
and then we're animating the shuttle
doing a decoupling from the ISS it's a
sort of a little bit of a fantasy
because it would never spin that close
to the ISS in real life so Alberto you
don't have to correct the thing but
we're we're trying to tell a story here
not necessarily show exactly real real
time data at any rate but this
environment here let's go ahead and and
I'm going to go ahead and pop in and
find a specific spot on the tour here
and going and close the tour keep the
layers intact and I'm going to run a
connect control for roll by telescope
and this right here we started demoing
this publicly it mixed last year but as
we've got the new
pardon me here is an ID to track down
the correct
wish wish we could add a break before
him because this takes a little bit to
set up all the hardware and all the
software running okay here we go so
let's make sure
I'm going to make sure that it frames me
it recognizes me it needs to be tilted
down a bit okay perfect
oh great
okay here we go now so now we can
actually come here and fly around the
space station here basically I'm just
using gestures here like if I'm on a
multi-touch wall so i can zoom in i can
zoom out and right here one of the
things is i just sit here and we have
time going let me go ahead and uh oh why
bother going back to my computer to
change time time slow time slow I'm fast
well but we have a if the voice commands
are implemented let's see move up well
I'm not sure why the voice is not
working right now but so we can come in
here let's go visit the shuttle itself
back a little bit and bring that in here
and in this orbit that we're tracking if
we speed time up we'd actually be
watching from the view of the ISS orbit
which is kind of a new function we have
now we're kind of zooming back out solar
system overview okay oh there we go did
recognize me
so now we're going to take an overview
here and look across the solar system so
we have our planets we have the asteroid
belt here we have all our
trans-neptunian objects and orbits go
all the way out here to beyond our own
solar system into the rest of the stars
see right there that's Orion
constellation Orion but as we zoom back
out see how it is in 3d and so we're now
out to see the overview of the galaxy
and can zoom out even further and take a
overview of the entire Sloan Digital Sky
Survey and so we can come over here and
look and each of these is an individual
galaxies and we'll look at the coma
cluster right there in the center
so let's go back to go to Saturn go to
Saturn I've got to be careful that it
doesn't it when you give a dialogue you
can't go oh man and you can't like just
throw it in right after the end of
another sentence as a run on and
continue to talk like that go to Saturn
just not going to pick that up you need
to have it as a command that's
punctuated by a little bit of white
space so that it knows that it's not
just words in the context of a sentence
turn orbits on or sorry orbits on orbitz
le there we go so time fast time really
fast time really really fast I'm super
fast okay time really fast I guess it's
just a little hit oh there it is it's
actually moving yeah there you go yeah
some of the inner moons time really
really fast
well sorry it's a little hit and miss on
this it might be because of the echo of
my microphone here we go
okay well orbits off
you know they say never give a speech
demo and I can understand why because it
like it works you know perfectly when
I'm in a you know in an office
environment but then somehow it you know
always you know I don't know if it's the
speaker and then you have all this
echoes so it's hearing another internal
Milky Way go to Jupiter solar system
mode
earth mode
the the windows sdk for Kinect which was
just publicly released as the the final
version at the end of februari or
beginning of februari allows you to
basically bring in the Kinect camera and
the microphone array and the other
resources and things like that and
fairly easily create an application to
get the depth buffer do skeletal
tracking and be able to process the
microphone array connect it up to speech
and everything and and truly I mean this
generally is not anywhere near this uh
you know in the I'm not sure exactly
environmentally I didn't get to you know
come in and set this up ahead of time so
I'm not sure what sort of environmental
issues might be interfering but
generally it's pretty reliable in my
office it's like about ninety-eight
percent recognition rate when I'm not in
the middle of a sense and I'm and I say
a phrase or whatever it's it's the the
microphone arrays is pretty good the
recognition skeletal track you know i'll
give you a quick demo here of some of
the applications that are available in
the SDK samples so we can bring in like
a skeletal viewer here and go ahead and
okay hold on one second let's kill my
kill the preview client and run this and
this is going to show you an example of
what you can get for it so we have the
color camera view we have a depth view
right here's our depth view our color
camera view and then when it recognizes
the skeleton you have your skeleton so
you deal with the individual points that
it's recognizing from the deaf and it's
mapping a 3d skeleton with my arms so if
you dance or something like that you
know it's actually capturing the frames
and it's giving you an array of 3d
positions for all of the components of
your hand so it can track your waving
your hand like this um it can track your
hands it can track gestures so you can
do like a menu command or other things
like that and I use this to go ahead and
pretend there's like a 3d touch screen
in front of me and then I connect and as
I touch the points I'm essentially doing
like the typical pinch gestures for
rotation and then you know you touch and
allows me to move like that and this is
a really easy to get going with if you
know any c-sharp programs programming
skills you can just install it or C
sharp or C++ plug it in and go and the
equipment that used to be required to
make this work was phenomenally
expensive so it's now for you know a
couple hundred bucks you've got
something that even to an awful lot of
really cool stuff with and and at every
level from the full skeletal track and
all the way down to just getting raw
buffers off of it any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>