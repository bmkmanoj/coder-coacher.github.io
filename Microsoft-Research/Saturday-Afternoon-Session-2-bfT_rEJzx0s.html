<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Saturday Afternoon Session 2 | Coder Coacher - Coaching Coders</title><meta content="Saturday Afternoon Session 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Saturday Afternoon Session 2</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bfT_rEJzx0s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
ok so i guess ii then can get started
for the second afternoon session today
the session going to be about
computation and illumination actually so
we right away start with our diffuse
structured light by sri nya and mood
gupta and mode will actually talk thanks
Michael thanks Henrik hello everybody my
name is mohit gupta and i'm going to
start by talking about extended light
sources extended light sources have a
finite length or even a finite area one
of the most common extended sources
especially in seattle is a cloudy day
now extended sources have a long history
in computer vision about 30 years ago
called sushi acuity showed that extended
sources can be used to perform
photometric stereo for perfectly
specular surfaces the idea is that with
an extended source it is possible to
sample multiple normal directions at the
same time while also reducing the scenes
dynamic range now it was possible to
measure 3d shape of specular surfaces
with very few images as few as two
before this photometric stereo could be
applied to nearly diffuse surfaces later
the technique was extended for glossy
surfaces which have a specular lobe
instead of a specular spike now here the
main observation was that if the light
source is moved then this then the
specular loop is symmetric about the
specular direction using this
observation and linear light sources
glossy surfaces could be reconstructed
going further hybrid objects were
considered which have both a diffuse
component and the specular component
here by using a diffuse by using a
spherical light source and changing
illumination gradients but using
changing illumination gradients hybrid
objects could be reconstructed now more
saintly might i'll use polarisers and
spherical light sources to first
separate out the diffuse and specular
components and then measure shape using
both the components the common thread in
this line of research is that extended
light sources can be used to reduce
specularity 'he's now extended sources
have also been used in the context of
shadows here are two images of a scene
in Manhattan the left image was taken on
a clear day we can see strong shadows
here the right image was taken under an
overcast day the shadows are
considerably reduced in fact if the sky
is heavily overcast the intensity of the
sky is nearly uniform such large nearly
uniform light sources are called
ganzfeld sources and they have
interesting implications in psychology
now coming back to vision longer in
zuker showed that the intensity of a
scene point under an overcast sky is a
function of its sky aperture that is the
amount of sky visible at that point
using this observation they proposed a
nice shape from shading approach which
works on a cloudy day the main point
here is that extended sources reduce
shadows so extended sources are a
win-win if the goal is to reduce
specularity and shadows now let's talk
about structured light structured light
is one of the most widely used
techniques in computer vision it is used
for 3d scanning pose estimation canet
and also defect inspection in fact
currently it forms the workhorse of a
very large multi billion dollar
semiconductor industry now there are two
problems which nearly every structure
like technique faces first its
peculiarities consider this metal
hemisphere and suppose we want to
reconstruct its 3d shape we project
phase shifting sinusoidal patterns on
the scene now look at this point p it
falls on a highlight if you look at the
radiance profile that tempura radiance
profile of P over time ideally it should
follow a sinusoid but because of the
limited dynamic range of the sensor the
intensities are clipped obviously this
is going to result in big errors in the
reconstructed
consider this cube here and suppose we
illuminate it from an overhead projector
now there are parts of the cube here
which do not receive any light also the
cube casts a shadow on the ground
resulting in shadows here now again of
it's very hard to recover any meaningful
information from this shadowed region
now fortunately these two problems sound
familiar we just talked about them why
don't we use extended sources to deal
with them well here is a simple
structured light setup we have a
projector projecting a pattern on the
scene one of the easiest ways to to make
an extended light source is to use a
diffuser so let's place a diffuser
between the projector and the scene we
have an extended light source but we
also have a problem the pattern is
completely blurred it is going to be
nearly impossible to recover to recover
back the coding information that was
lost here so let's get back to a message
here extended sources are a win-win
provided the coding information is
preserved so how can we use the power of
extended sources how can we use the
power of extended sources in the context
of structured light our main observation
here is that several structured light
patterns have translational symmetry
that is they have constant instant
intensities along straight lines if we
use one of these patterns and use a
linear diffuser then the coding
information is preserved by linear
diffuser I mean a diffuser which
scatters light only along one dimension
it has a 1d scattering function here is
an example on the right side I'm shining
a laser pointer through a linear
diffuser the scattered light forms only
a straight line if it were an isotropic
diffuser we will see a 2d scattering
function yeah ok so we call this linear
need a few structured light now we have
an extended light source and the coding
information is preserved as well this is
nice let's look at this a bit more
formally let us look at the image
formation model under linearly diffuse
structured light suppose we have a
collimated light sheet meaning all the
light rays in the sheet
the same intensity and we have a linear
defuser place so that the all the
scattering happens within the plane of
the collimated light sheet now let's
look at the radiance of this point P due
to a small diffuser element right here
the radiance is a function of the source
radiance the scattering function the
foreshortening term and the brdf now the
scattering function gives the angular
profile of the scattered light now to
calculate the complete radiance we just
take an integral over the entire strip
note that the source radiance term has
come out the source radiance term is
outside the integral this is because all
the light received at P is due to a
single collimated light shape okay
because of this the integral is
independent of the source radiance and
we can replace it with a single constant
now this constant is going to be
different for different scene points but
it is going to be independent of the
illumination of the projected pattern
now we are going to compare this
expression with the radiance expression
in the case of conventional structured
light the expressions are nearly the
same only the constants are different
what this means is that if we project a
series of patterns on the scene the
radiance with the diffuser is going to
be the same as the radiance without the
diffuser modulo a constant which means
that the coding information is preserved
now let's see how this this let's see
how the few structured light helps us
reducing shadows consider this point p
which is highly speculative use constant
k diff in this case is significantly
smaller than the conventional constant
because of this the diffuse radiance is
going to be significantly smaller than
the conventional radiance if point p is
glossy then k def is smaller than K conf
if point P is diffused then the two
constants are nearly the same
intuitively the few structured light
reduces the radiance of specular points
and maintains the brightness of diffuse
points this has the effect of reducing
the effective dynamic
the scene the red curve here shows the
intensity distribution without the
diffuser you can see that it's nearly
flat now with the the diffuser
compresses the dynamic range of the
scene so that it falls nearly within the
sensor dynamic range let's look at the
case of Shadows here this is a simple
seen a sphere lying on a plane there are
two points here q and are both q and r
are in shadow this is convention
structured light now if you place a
diffuser both these points now receive
light they are no longer in shadow the
amount of light received at Q on earth
might be different it will depend on the
relative geometry and the scalp cooling
function of the diffuser but the
important point is that both of them do
receive light so just to recap the few
structured light reduces specularity and
shadows and since the coding information
is preserved we do not need to change
the decoding algorithm the decoding
algorithm remains the same all right
let's look at a few applications first
is 3d scanning here is our simple setup
we have a projector a camera and we
place a linear diffuser between the
projector on the object these kind of
diffusers are relatively inactive lee
inexpensive and very easily available
this one was made as a lenticular array
the first example is a glossy a metal
hemisphere we saw this before and these
are the input images without the
diffuser going from left to right the
exposure is reduced on the left we can
see strong specularity going right the
specularity is reduced but the overall
brightness of the scene has decreased as
well these are the corresponding 3d
reconstructions here we can see hole
through the specularity going right that
hole is filled but the overall quality
has decreased because of low SNR now we
are going to compare the best exposure
here with the best exposure with the
diffuser now immediately we can see that
the specularity is weaker and it's more
spread out as a result we get a much
cleaner reconstruction here
here's another example this is a coin
it's made of metal so it's shiny here
are the reconstructions using the few
structured light we can get the fine
level details this is a profile view as
expected it should be flat and if we
compare it with conventional structured
light you can see big errors due to
specularity is here and low snr here
let's look at a couple of examples for
shadows this is the cube which we saw
before and these are the input images
lift without diffuser right with the
diffuser now with the diffuser it is as
if the pattern has wrapped itself around
the whole object and now the whole
object is nearly covered these are the
3d constructions again as expected we
see LA we get large holes here and here
nearly the full object is reconstructed
this thin strip here which is not
reconstructed this is because the
normals here are perpendicular to the
diffuser direction these four only these
normals we cannot get a good
reconstruction this is the fingertip a
fingerprint we wanted to scan the
fingerprint lift again without the
diffuser with the diffuser but because
the fingertip curves away parts of it
are in shadow but here most of it is
covered these are the reconstructions
again we see errors due to shadows and
here we can get very fine structures now
I should point out that in all the
comparisons the number of images or both
the conventional and a few structured
light was the same right we are going to
look at another application this is
direct and group this is the separation
of direct and global components of light
light transport now see with these
colleagues a while ago showed that this
separation can be done very efficiently
by using high-frequency illumination for
example high frequency binary stripes
these are input sequences on the left
without the diffuser with the diffuser
now the first thing to note here is that
with the diffuser even with the diffuser
the high frequency nature of the pattern
is preserved
these are the separation results on the
top are the direct components on the
bottom or the global are the global
components and left without diffuser
right with the diffuser now
qualitatively they look the same but
there are some interesting differences
for example the specularity is here are
more concentrated and with the diffuser
they are more spread out in fact they
take the shape of a linear stripe okay
just a few remarks now so far we have
not considered the effects of projected
d focus now nearly every light source
has a finite depth of field because of
that the projected pattern is blurred
with a key focus Colonel if we add a
diffuser there is an additional
diffusion Colonel but the diffusion
kernel is linear it does not scattered
light in the orthogonal direction now
because of this linearly diffuse
structured light can also be used with
techniques which rely on measuring the
projected d focus for example shape from
projected d focus now you might wonder
what is the class of patterns that this
technique is applicable to so far we
have talked about linear patterns well
it can also be applied to radial
patterns patterns which have radial
symmetry along with radial diffusers in
general we can apply to patterns where
the ice low intensity contours are
linear in those cases the coding will be
preserved we cannot apply to patterns
where the ISO intensity contours are not
linear for example if you use a circular
pattern and the circular diffuser the
coding information will not be preserved
now in summary we have presented a
technique to handle specularity and
shadows in structured light it's very
simple and cheap it can be applied to
patterns which are linearly invariant we
have shown two applications 3d scanning
and direct global separation and we
believe that it can be applied to
several other structured light settings
thank you very much for listening and
I'll be happy to take any questions
the surface can be parallel to the
packages that's a very good question so
the amount of gain that you will get is
dependent on the surface normals and the
distribution of surface normals I
actually have a quick small video for
that so one very good way to look at the
the quantitative game is to look at a
sphere because it has all the normals so
here we have mapped the sphere to a
circle and the red portion is the
corresponds to the part which is not
covered which is still very dark and the
white portion and the grey portion is
the covered part now with the diffuser
we get this coverage and now as we
increase the diffuser scattering angle
you can see that the coverage changes
there will still be some parts of the
sphere which will not be covered and
that depends again on the material of
the sphere and as you said we can also
take more images by changing the
orientation of the diffuser that will
help
not that much actually I i was expecting
these problems but while doing the
experiments the thing especially if you
are using sinusoidal patterns for
example because there's already a
gradation even if there is small
scattering orthogonal e you are going to
be fine but if you absolutely want sharp
edges then you probably have to be more
careful in doing the alignment but in my
case rarely took more than five minutes
to do this okay so there are no further
question let's send boy it again and
with that we come to the second talk
which is about eliminating through snow
and rain action hi my name is Asha
navasana Simon I'm a faculty member at
Carnegie Mellon and this is joint work
with my students and colleagues so
imagine you're driving at night and it's
snowing pretty hard right you see all
these flickering streaks coming straight
at you if there's a rainstorm you see
all these flickering streaks as well and
you see all these splashes it's really
hard to drive at night during a
rainstorm right the world meteorological
organization shows how often does it
rain in major cities across the world
and the only thing I you know I am happy
about is that pittsburgh has fewer
number of rainy nights than Seattle but
you know given the number of
distractions we have while we drive
nowadays we want to try to minimize the
distraction due to rain storms and
snowstorms okay and one possible
approach is to do post processing of a
video that is captured by a camera that
is placed on a car okay this
the result from our work of a few years
ago and you know what we are doing here
is we are separating out spatiotemporal
frequencies of rain and snow versus
everything else in the scene now these
things produce reasonable results but
you still have to put that result on a
display in the car what that means is
that you're diverting the attention of
the driver at a critical juncture right
so he has to he or she has to look away
and so it would it's sort of much better
if we could increase the visibility
directly write to the naked eye and so
in this work we're going to be proposing
a novel headlight that can allow you to
see through rain and snow okay the idea
is very simple if you know where the
raindrops are if you know where the rain
you know snowflakes are you can sort of
streamlight in between them okay so you
know if you do this if you if you're
successful in this illuminating enough
number of drops you can reduce the
visibility of the snowstorm of course as
you all laughed about it's easier said
than done okay there are two issues here
one is you have light throughput from
the headlight that needs to go out right
how much light you're sending out and
the second issue is the accuracy how
many drops are snowflakes you can
successfully dis illuminate now if it's
a ordinary headlight you have one
hundred percent light throughput but
zero percent accuracy right or if you
switch off your headlight its hundred
percent accuracy but 0% light throughput
so really the goal is to be able to
achieve high accuracy while maintaining
highlight throughput okay as you can
imagine this is a challenging problem
and there was one pattern by Ken Berlin
who proposed in his patent a scanning
laser beam that lights up dust particles
so it's similar in spirit to that idea
ok so you know you might think it's
crazy and in fact the number of days I
just woke up and I thought it was crazy
was a lot more so but there are a couple
of observations that gave us hope first
one is when we actually look at rain or
snow we are actually seeing big
streaks right this temporal integration
going on and what we are imagining is
that these streaks are occupying this
volume very densely but if you actually
capture an image with instantaneous
snapshot of rain it's very sparse so you
have bright specks of drops and
snowflakes and this is true even in the
case of a thunderstorm okay and you can
actually if you think about just a
sparse number of bright spots you can
think now about three distributing your
light in between these these things now
a second observation is that we are more
stressed about the drops that are
closest to it the brightest streets that
are flickering and air coming towards us
and our closest to the light source so
what we did was we did several
experiments and this is with a prius a
dlp projector and halogen lamp of
comparable brightness and we sort of
illuminated drops at different distances
of different sizes we use sprinklers we
also watch them in real rain and so on
and what we found was the most important
things or streaks to take care of our
very short distance away from the light
source okay so typically all the action
is happening about 3 to 4 meters and
beyond 5 to 6 meters you don't have to
worry about it okay so with these two
observations we can now sort of describe
our system which consists of a
co-located projector camera system and
the projector of course is a spatial
light modulator in the beginning it's
going to just light up the entire field
of view and the camera is going to
quickly capture whatever it can see at
the top of the field of view right so
that's the top of the field of you going
down okay now once you have these images
then a processor needs to detect where
the drops are and then try to estimate
the velocities predict where they're
going to be in the future time locations
right and so all this while we're still
illuminating the world so this is our
system latency right but once we are
ready then we can start dis illuminating
these drops and this is the reactive
control part that I'm talking about now
obviously if you just you know transfer
the image using a USB
open it up in MATLAB load the image the
drop is gone right say ghost has to be
really fast so you know one a good thing
is we can sort of pipeline this system
and so there's three separate stages
capture asus and projection right d X
denotes transfer of data now since these
things can be run in parallel the
subsequent stages we sort of stagger
them like this and in the system that we
have actually built and I will talk
about it in a little bit we achieved
these kinds of timing delays or
everything is in milliseconds actually
so it's 5 milliseconds for capture 5
milliseconds for processing and so on
now the time that elapsed from the end
of capture to the beginning of
projection is our latency and in our
current system it's about 13
milliseconds okay so even before I tell
you how we built this thing we wanted to
see whether we are crazy or not so we
wanted to actually build a comprehensive
simulator right so this simulator will
take as input all the parameters of this
system so it can generate different
kinds of rain different kinds of hail
snow and so on it can take as input the
parameters of the projector camera it
can take as input uncertainties in the
algorithm and so on okay and here i am
just going to show you a video oops
sorry
on the rated video that's just
highlighting rain five millimeters per
hour with the system latency of 13
milliseconds our adaptive projection
system runs at 120 Hertz this first
example is a simulation with a
stationary camera black streaks are the
drops successfully not illuminated the
red streaks of the top are drops that
the system is not yet recognized if we
add forward velocity the drops enter the
field of view from the top and the sides
which makes the test challenging though
not impossible at 30 kilometres per hour
the accuracy is seventy-three percent at
very high speeds our system is still
capable of not illuminating many of the
drops while still maintaining highlight
throughput ok so you know the in the
paper we have presented a lot of plots
lot of detailed analysis and I encourage
you to take a look at that to see this
is that this is truly feasible and so
what we did was we you know we try to
think about how to build this system and
of course this was in my lab and you
can't put it on a car and drive around
it 30 kilometers so this is actually a
stationary system that we have and here
it is a rain generator this has a bunch
of these valves and they open and shut
and you release the drops right so that
you can control these valves at about 60
drops per second and so you can generate
rain all the way from a small drizzle to
thunderstorm that has never been seen in
the United States right so you can
generate all of these things in the lab
and now of course the imaging and
illumination system is something that
you all know this is a dlp projector a
point camera it's about I think 120
Hertz or 200 Hertz can go up to 200
Hertz okay so let me show you a video
that talks about how to detect these
drops estimate the velocities and
predict their future locations and we'll
start with one single drop one time you
drop this video shows that the system
operates on us
single drop the camera input is fed into
our system which predicts the future
location of the drop and directs the
projector to not luminate it for the
predicted location video the color red
indicates the parts of the drop that are
incorrectly lumen ated green indicates
the parts of the drop that are correctly
not illuminated and blue indicates
pixels that were incorrectly turned off
this video shows multiple drops released
from different Droppa meters the system
is able to track and predict the future
locations of each drop at 120 Hertz okay
so you know this is just a sort of fish
that shows what the camera input dates
how we predict things how we project
right because we can control the drops
precisely we can redo the experiments we
know the ground truth we we can analyze
things now I just want to show you an
example where what happens if we do this
very very slowly if we use the 30 Hertz
camera for example all right and add 30
Hertz you're actually measuring not just
drops of specs but actually Longstreet's
okay and here's a bunch of these streaks
and I have placed a background behind it
and what we can do is of course we can
detect these streaks amazingly well we
can predict them we but what we have to
do is we can block them out as well
right so instead of these white
flickering streets you have black
flickering
okay so it's not so smart a headlight
right so you know what even if you have
something slow one thing we figured out
is to actually use instead of an 8-bit
or 24-bit projector if you use a
high-speed bit plain projection such as
that from the mule dlp projector for
instance alright so what you can do is
if you send it 24 bits at 60 Hertz but
those 24 bits are sent out bit by bit
remember what we are trying to do is
just binary projection we don't want to
project color images right so here is
the video on the left that shows how the
bit planes are sent out really quickly
and this is actually observed by a
high-speed camera that's at two thousand
Hertz okay now of course if I actually
watch this with my naked eye or actually
record it with you know 30 Hertz
Canberra you can barely see anything
right this slide flickr there and that's
you know you you've sort of got around
the problem of projecting big black
streaks instead of rejecting a whole
streak you just sort of following it as
the bit plane goes down right so here's
a photograph of our system working on
the left is a photograph it's a long
exposure photograph it's about two
seconds or so and you see all the rain
that has accumulated within a couple of
seconds and the headlight is on all the
time but on the right is our smart
headlight where you can see or rather
you hopefully you cannot see that as
brightly as the rail here right now this
top part if you're wondering what it is
it's actually the system latency right
so it's about this high before the drop
comes there we are ready to predict and
track these drops okay now let me show
you a video of this on the left you have
a standard headlight this is by the way
this is generating about 32 drops per
second and it's in a small volume it's
not in a large volume i granted but if
you collect the amount of water for an
hour it's about nine
millimeters so it's a severe
thunderstorm right any stronger it's a
shower okay so on the right you can see
that we are sort of predicting
everything we're detecting in the top we
are predicting correctly and we sort of
create you know distributing light in
between these drops now the challenge of
course is how to show something that is
not there right but you can see a big
difference between the two now how are
we doing quantitatively and if you
actually dinner remember we have this
simulator that takes in all these
parameters so if we took all our actual
system parameters and you plugged it
into the simulator and you simulated
different kinds of rain this is the kind
of light through put that you would get
for different fall rates okay now if you
think about rain we are achieving about
seventy percent accuracy that means we
are reducing the visibility of rain by
about seventy percent while losing only
about five to six percent of light and
that's not so bad right and now if you
think about snow snow is a little bit
harder because snowflakes tend to fall
slower and so they stay in the field of
view longer that means you have to
switch off more light rays so naturally
snow will need lower light throughput
for the same kind of accuracy so in this
case snow we achieve about 62 63 percent
accuracy while losing about fifteen
percent light through but now you might
be wondering why this snow graph is
stopping there this snow thing is you
know it corresponds to about ten
millimeter per hour this actually the
snow melting volume right so this is
snow water equivalent so this is the
amount of water if we melt the snow on
the ground that's the height you would
achieve and if you assume about thirty
percent density of water this ten
millimeter per hour corresponds to about
point eight meters of snow accumulated
in one day or if people here you know
understand in just better it's 32 inches
of snow so it's
water snow okay so you know we might be
able to simulate more but really we
didn't see the point now of course how
does it compare to a system that's ideal
that's the ideal system what do we call
ideal system so we have no system
latency perfect detection perfect
tracking and all of that what we have
said is ok the there has to be some
finite imaging exposure let's call it
you know one millisecond exposure and if
you look at rain we obviously get one
hundred percent accuracy because we can
detect and track everything but the
light loss is about only two to three
percent that means even if you take the
strongest rain there's about two to
three percent of this entire volume
instantaneously occupied by the
raindrops okay so let's just go back and
forth between the two things so you know
we are behind but not totally behind
right okay so let me summarize by saying
that we have introduced an idea of a
head light that can allow us to see
through rain hopefully and simulation
suggests that the idea is feasible while
I didn't talk about all the simulations
here please look at the paper for this
and the initial prototype is encouraging
now of course this still a very long way
to go and we have to think about when
the turbulence around the car the
vibrations of the car we want to make
the device compact put it on a
fast-moving car and so on but you know a
lot of people here would like to have
codes so I would just like to say that
is hope thank
yeah oh you're projecting his light on
the road that it would have a pattern of
black dots on the road there would be
coming at you because of the missing
life yes yes by having multiple
headlight they were all intersecting yes
absolutely we thought it was it would be
possible just with two headlights for
example I don't know we have to do a
user study really but the other related
thing is this bit plain projection will
basically it will you know wipe out all
of these black things if we do that yeah
you know I showed you that video right
if you just integrated it's just black
dots but you know that's at 14 hundred
Hertz or so so you will not see anything
but adding more headlights is a good
idea yes changes in speed or turning
track to a certain extent or come in
sideways
absolutely I mean this is a very
challenging problem so if you are moving
let's say at 100 kilometers per hour and
if it's a severe thunderstorm I would
say don't drive at hundred kilometers
per hour because what we what we had in
the simulator was about we could get
about fourteen percent benefit okay in
the worst case but to answer your
question more precisely i mean this this
tracking thing we are assuming that
because of the colocation we have to do
everything in the image space right so
we don't have to do it in 3d but as you
moving then perspective comes into play
right so the streaks actually come at
you like this and so what you do is
instead of just blacking out exactly
that row you're just going to have a
cone of uncertainty so it can take into
account small changes in perspective
provided your system is fast enough
that's that's perfectly fine that's
perfectly fine but remember we you know
we have to think about milliseconds
rather than you know the just one or two
milliseconds so that's what we are
talking about right and so the wind
changes or the steering changes very
less in that amount of time right so it
is definitely possible at 30
milliseconds you will start seeing a big
change so that's why you should not use
this with a 30 Hertz camera for example
yeah all they do is put the lights up to
the side right right no absolutely
absolutely absolutely I think this is
something you know similar to what Mark
mentioned you know we've done a lot of
work apart from this sort of flickering
streak business we have done work in fog
and haze and underwater image visibility
enhancement and we showed that you know
placement of the light sources are very
important underwater imaging people say
that you have to place it as far away as
possible but that's not the best thing
because this attenuation of light right
and we actually showed one of in fact by
Stewart
mahuva ex-student rather he came up with
the idea of what is the optimal position
of you know where do you place your fog
lamp depending on how much fog there is
so 4 low-frequency effects like fog and
haze these are solutions that are
definitely possible yeah yeah if you
need a dlp processing would this be very
expensive for ah well the DLP by the way
the everybody knows dlp pico projector
right so the pico projector is worth
three hundred dollars but of course it's
very dim but it can absolutely do
exactly what I was talking about the
plane projection so it's it's not very
expensive anymore of course it's a
question of marketing and how you can
convince those people in texas
instruments to do this right cheaply
right i have a couple of students there
now so maybe we can make some inroads
yeah i think we have time for one more
question yeah
special light I mean let's say it is a
light modulator but it's a DMD based or
DLP projector right so the actual system
that you saw was just off the shelf I
mean we didn't do anything okay so thank
you Sweeney once so we're going to move
on to the last talk in that session
which gonna be about our desk coded
shape from focus so hello everybody can
you hear me my name is Martin lens and I
did the work called depth coded shape
from focus with my colleagues david
fassold yasuda and Trust Bischoff at the
Institute of computer graphics and
vision in Graz in Austria so as the name
already insists this has these deals
with sorry these deals with shape from
focus or is related to shape from focus
which we heard yesterday already to the
talk of Sri nya who actually invented
this so the principle of shape from
focus is getting some depth information
especially in for example microscopic
environments where you have little depth
of field so if you take an image it is
very blurry and has only certain regions
in focus so you move your scene along
the optical axis of the camera or you
change camera parameters so you get a
stack of images with the varying varying
locations in and out of focus as a next
step you just take a local neighborhood
or local volume and you can compute the
so-called focus measure there are
thousands of many different focus
measures available next step is to find
the maximum along your optical axis and
then you have a result text image and an
all in focus image so what is the
problem of this approach or a drawback
you
eat this large image stack so for
example if you have only a small volume
of one millimeter x 1 millimeter x 250
microns and you have a depth sampling or
a sampling in each direction of 1 micron
you already have 250 megapixel image
data and actually it's a lot more
because you have better less resolution
and you have hundreds or even thousands
of images in your image stack so this is
maybe one drawback another drawback is
the following if we look at the
traditional shape from focus you just
have to move your scenes little depth of
field then you for some exposure time T
you take your image under a constant
elimination and this takes of course
quite a long time because you only have
cameras that work at the frame rate of
let's say 100 or 200 Hertz so what we
propose instead of that is that instead
of using a constant illumination we
propose to use some binary veterans
local panera to deep patterns like 3
times 3 pattern as can be here and we
project them side by side over the whole
scene and while we are moving our scene
and we are exposing our image sensor we
protecting a sequence of veterans and
this result you get like an image like
this which might be very bad we can of
course the convolve it and the result
you see only that for a pattern that was
in focus at a certain time and you can
directly read your depth information
from a single image so short outline of
my talk first I want to talk about
integral imaging very briefly it's the
focal sweep let's explain to yesterday
already and extending that fulfilled to
get a sharp image out of your D focused
image and then I'll briefly explain the
our approach and give some experiments
first about the integral imaging shortly
will explain the image formation so you
have some senior radians P so this is
your scene s
this some location x and y and you have
a Dirac Delta function selecting your
pixel centers for your camera for
example and at the traditional image
formation here you have a convolution
with a rectangular function which could
be your pixel with pixel width in pixels
height so this is your one pixel and you
also have a point spread function this
is this theorem here or build box
function or portion filter or whatever
and this has some play radius R which
depends on your scene depth or to be
more precise with the distance of your
focal blinded to your scene point so
this is the traditional approach now
once again explanations and the
difference here if you do the integral
imaging or the focal sweep is that you
have a moving scene so this term will
also be they can employee count so this
is just a translation of your constant
object speed and you're integrating over
time from the 0 to t 1 these are your
integration boundaries and this is then
your final image formation for the
integral imaging and as we heard already
yesterday you can use a special method
to get a sharp image out of a blurry
image because in Nagahama showed in 2008
or observed that the integral of your
point spread function is constant and
insensitive to the scene death and image
location so you can rewrite this thing
here to just a convolution of your
integral point spread function here and
your Sharp zeen texture and if you know
your interpret point spread function due
to some calibration process you can find
your sharp seemed extra from your
observation here with some deconvolution
algorithm there are many of them okay so
far we don't have any depth information
yet but we didn't be dull dunno the
sharp seen for example so what we
propose now is to project during the
image acquisition we to protect a gray
coat pattern or many great code patterns
beside each other
for example like this and additionally
do your scene you also you now have a
modulation of your scene with some
patterns and the integral of the
patterns could be seen as one static
pattern be this here which only consists
of those codes that vary in focus at a
certain time and the final image
formation now looks like this so we have
this term here this is now our shop seen
illuminated power static pattern with
the depth information that is the code
and we say that this is not completely
true here because you might have some
transitions but these are really really
small because you have a d focusing that
happens twice once during the projection
once during the acquisition of your
pattern so when you get to this constant
error here in the back and we rewrite
this and have now a sharp illuminated
seen that we can receive through the
convolution the problem now is that it
might be difficult to robustly determine
what is seen and what is pattern so we
propose to take another image under
constant elimination for example like
this Oh hard to see sorry so this is one
the scene with the bad on this is a
scene without the pattern and if you
take the relative intensities you can
easily determine your code patterns here
and the final thing to do now is just
for each image pixel this is for example
our input for our algorithm you just
take the local neighborhood you and in
this neighborhood you perform simple
thresholding by taking the minimum in
the maximum intensity this works if you
for example project the greatcoat
starting at a single number one so you
have always at least one bit set to 1 so
as a result you get your two dimensional
pattern and with the knowledge of the
sequence you projected you can determine
your depth map your final tax map as a
synthetic experiment I just chose a cone
like this the optical axis of the camera
is the set axis
and this is the resulting image that is
used and for the reading of the depth
information so you can see some
transitions here from one pattern three
other pattern and the result looks
something like this so you see this
steps here because I did not do any
smoothing here the final reconstruction
a real eager to experiment we also did
so I just have some prototype run here
of a microscope so this is our optical
axis of the camera and usually here
light is coming into it and co actually
illuminating the scene and we have
attached an L cos display to modulate my
light locally and both devices share the
same focal plane I did two experiments
both on planar surfaces one with the
extras and one with an textured surfaces
this is mirror and as you can see here
in both reconstruction results you can
clearly see like a plane and with some
steps of course or like here which are
the transitions between patterns a
qualitative experiment is a coin with a
star on it and this is again the input
for the coded seen depth and final
reconstruction with a Nolan focus
texture on it since you already have
that from the algorithm as well so it
comes to some limitations of the
approach at first it's as a kind of
trade-off between lateral resolution and
depth resolution of course because if
you're using a 2 x 2 pattern you only
have a limited number of depth
resolutions you can sample but you have
to find some trade-off here depending on
your scene depth you want to or volume
you want to measure another thing might
be a problem that radiometric limits of
projector and camera you it might not be
easy to
we'll choose the correct exposure time
for example but you could overcome this
problem by simply putting your camera
into continuous mode and taking the
discrete integral of your final image
stack which we will be let's say fewer
images then using the standard approach
and it's very very fast again to take
your images so to sum it up we have
presented in all three reconstruction
method for microscopic environments
which works on textured as well as on
textured scenes and it's in theory
possible to do the reconstruction from
two images only or if you have an on
textured scene from a single image even
the disadvantages might be that you have
an additional hardware namely the
projector that there is a trade-off
between lateral and depth resolution and
you might face summary idiomatic limits
but on the other hand you have reduced
acquisition time a very reduced memory
consumption since you only need one or
two images the processing time is very
short as well so you really fast because
you only need to do with the convolution
and fresh holding and it works on and
extra teens as well here are some
references and thank you for your
attention
and that's why it's limited by the size
of your banner of course but you also
have these limitations in shape from
focus since you have neighborhoods where
you have to compute your focus measure
example so somehow lateral resolution
decreases yes please
sorry yeah
the question is I think if I have a 2 x
2 pattern and I only need one picture i
project the sequence of veterans so they
change during image as a different image
acquisitions so the balance size has
nothing to do actually with the numbers
of image is required so it's the
patterns are used for the depth sampling
yeah in theory of course maybe you have
video magic limits then you need more
okay so there the questions then our
let's thank Marty again and this
concludes the session I just want to
remark that your imagine buddy that the
next break is rather short so we
reconvene here at four-thirty for the
fast forward on the poster presentation
and every poster presenter please come
up down here so that we can already
order everyone into the ride water thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>