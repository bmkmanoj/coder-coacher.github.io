<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Computational Limits in Statistical Inference: Hidden Cliques and Sum of Squares | Coder Coacher - Coaching Coders</title><meta content="Computational Limits in Statistical Inference: Hidden Cliques and Sum of Squares - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Computational Limits in Statistical Inference: Hidden Cliques and Sum of Squares</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_Ayuckh1BpU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
great so we're thrilled to have Yash
deshpande here um he was an intern with
us so some of you will know him from
there and he will be talking about
computational limits in statistical
inference hidden clicks and sum of
squares okay thanks everybody for coming
thanks for the introduction and it's
great to be back after a few years I
always liked visiting Boston so I'm
going to be talking about computational
limits and statistical inference and in
a talk like this it's almost a cliche to
start with something of the form that we
live in a world of big data so I'm not
going to start that way instead what I
would like to do is you know start I
just explained a few examples which
illustrate what I think are the
important trends so what you see here is
an MRI machine the first example is
about medical imaging and you know some
of you might have had the chance to be
inside one of these machines
unfortunately if you haven't the
likelihood is really high that at some
point in the future you will but the
point is the machine is basically to
take medical diagnostic images of the
kind that you see on the left now we
won't go into the physics of how the
machine really works but it suffices to
say that each measurement of the machine
gives you one point on the Fourier
transform that you see on the right so
now these machines if you have been in
them you know it can take a lot of time
to collect all of the points in the
Fourier transform so you have to stay
extremely still for about 10 minutes or
so this is hard enough for adults it's
completely impossible when you start
thinking about infants or rambunctious
little toddlers who will want to wander
about all the time there are all her
pets right ah ah exactly exactly there
is other applications like like taking
the MRI of a beating heart and
if your heart is beating then you it's
impossible to image this the the
breakthrough that you know that happened
about a decade ago was to realize that
you can get away which is taking a few
points in the Fourier transform so
instead of actually collecting all of
the points which will require your long
time to collect you just subsample some
of the Fourier coefficients and collect
them say you collect these red dots over
here this takes a much shorter period of
time because the machine has doesn't
have to run as much but then the
question is where you now only have a
few of the measurements can you still
get medical quality images and really
the breakthrough work by Donahoe Candace
and others was the realization that you
can use l1 based methods or convex
relaxations to obtain very good quality
reconstructions even from very few
measurements my second example is is
recommendation systems so a few years
ago Netflix run one of the early machine
learning challenges which nowadays are
very popular if you've seen Carol etc
they released a data set of movie
ratings that people had given so for
instance this is a cartoon of the data
set every column corresponds to a movie
and every row corresponds to a user so
maybe the first user is Jennifer who I
don't know maybe happens to be a sci-fi
buff so she really likes Martian and and
the new Star Wars movie and perhaps the
second user is me who doesn't happen to
watch a lot of movies and maybe I didn't
even enjoy the Godfather very much so so
the question is you're obviously given
just a small set of the ratings because
not everybody is going to go and watch
every movie so given just a small set of
the ratings can you fill in the blanks
can you use can you infer the rest of
the ratings and use this for downstream
applications like recommendations and
during the price there was a lot because
of the netflix play there was a lot of
work that was done on on on problems of
this kind and more generalization
and the realization is that convex
relaxations of one or the other form are
extremely useful and informed in fact
formed part of the solution for even the
winning Netflix or price challenges I
believe now it is used also in in the ER
in the company's recommendation system
my third example is instead from
genomics so what you see here is a
representation of the results of a study
that was done about more than a decade
of good was reported in pnas what they
did was they acid gene expression levels
from a number of patients and basically
what you have your is a jeans times
subjects matrix every row corresponds to
a specific gene and every columns
corresponds to a subject and the
subjects are of two types you know the
blue ones are actually lupus patients if
you've watched house m.d. lupus is an
autoimmune disorder I don't watch house
m.d. so I had to look it up on Wikipedia
the green ones the green columns are
actually healthy control patients which
they also assayed what they found was
that there were a certain subset of
genes these were related to a certain
immune pathway called the interferon
immune pathway which had a abnormally
high expressions in in a small subset of
the lupus patients you know so within
this red splotch that you see over here
there is actually hidden an even darker
red splotch and these patients which had
these are normally high expressions they
were actually the more critical patients
they were the ones with the more severe
versions of the disease the disease had
affected that liver kidneys etc this is
obviously findings of this kind of
important because you can use it for
targeted medication it tells you a
little more about the mechanism of the
disease because lupus is an immensely
complicated disorder we don't know a lot
about this so the question is given this
large messy data set can you figure out
you know patterns of this kind
so there is really an important
difference between between the first two
examples which is medical imaging and
recommendation systems and the third one
which is that you know in the first two
cases we have good estimators these are
often based on convex programming but
not necessarily and these are good in
two senses they have good statistical
guarantees they are statistically
efficient they take advantage of all of
all possible statistical information
that is present in the data set and
because for instance if they are based
on convex programming you can actually
compute them there are fast algorithms
to compute these things for the genomics
example you do not have this in this
scenario and in many other examples of
this kind what you actually have is that
there are either estimators that are
statistically very efficient but we do
not know how to compute these quickly or
the other way around in the sense that
you have something that you can actually
program on your computer but it is not
as it is not as good in statistical
performance so you know this is really
the main question that i would like to
think about in this talk which is that
how do we characterize computational
complexity in statistical estimation
settings this is actually an extremely
broad question and a reality because
people are extremely clever in coming up
with variety of algorithms and doing
computation in very very interesting
ways this is one of the probably the
most difficult way to earn a million
dollars that you can think about so
instead i won't want I won't try and do
that but I will try and do something a
little bit simpler which is just talk
about one example which is the hidden
clique problem because it ain't in
reality is a prototypical example of
really this phenomenon where
computational constraints are more on
eros for a statistical estimation
problem so okay so this is a plan for
the talk I'll set up the problem in the
beginning and then for a while we will
talk about what are the things that you
can actually do what are the good
algorithms you know really there are
spectral algorithms and a little bit of
twists on that and i'll mention a result
there after that will change gears and
move to instead trying to think about
computational hardness so this is kind
of the other side of the story and I
will talk about the sum of squares
relaxations just as a tool to
characterize the complexity and in the
end I'll just mention some other things
that I have been interested in the past
and what I think would be good to think
about if anybody has any bright ideas I
would be very happy to hear okay so
let's go back to the genomics example
remember that there is you know hidden
within these rows there are the Black
Rose which correspond to genes that have
these normal high abnormally high
expressions and let's just do something
very simple let's try and construct a
graph that summarizes this data okay
maybe we can see something in the graph
so the graph will contain your two kinds
of vertices the vertices on the Left
correspond to genes they correspond to
each row and the vertices on the right
corresponds to the patients I will
connect a gene with a patient if I see
that you know I see that there is a very
red dot in this matrix so if there is in
principle yes but for now let's just
think that there are no weights okay
this do a very simple thing okay so you
like you pick a threshold and if it's
below the threshold you you don't put an
edge if it's about the threshold you put
an edge okay so maybe the first gene is
not an interferon G and so it doesn't
have too many edges the second one has a
few more edges and so on and now you
build the graph and really what happens
is that the gene signature starts
becoming visible as a dense sub graph of
this graph okay and what you would want
to do is given the unlabeled graph the
graph without all of the colors you want
to find that dense sub cross maybe that
tells you something that tells you you
know which of the gene
in which of the patients are in so let
us take this a little bit seriously I
will start with a mathematical model so
instead of having two types of vertices
i will just scimitar eyes the problem
and have one type of vertex really the
results are we'll probably go through it
just as is but it is easier to talk on
in this setting so the null model or the
the null hypothesis is that the graph is
generally is a graph on n vertices and
it is a purely random order any graph in
the sense that between every pair of
vertices energy exists with probability
one-half independent of all of the rest
okay this is similar to the it's it's
just a simpler version of the Bible
tight one we had earlier and instead the
alternative hypothesis contains a k
vertices out of n which are colored here
in red and there they are all forced to
form a click so they're all of the edges
within those K vertices are connected
but the rest of the graph is generated
as random as before okay there's two
kinds of questions one can ask the first
is just detection can you distinguish
between these two hypotheses you are
given a graph can you tell me whether it
is generated according to this model or
the other k depends k might depend on n
in general yes yes indeed indeed think
of the K points chosen at random
beforehand before the gas so they are
not dependent on the graph structure or
think of generating the graph
conditional on a certain subset of K
vertices being a clique is two are the
same distributions the second problem is
the estimation problem which is
obviously harder than the detection
problem which is that given the graph
can you find me if it if the cliq exists
can you find it for me ok who can find
the clique in this graph yeah
okay the real problem is of course you
know things are never ordered and
they're never colored no but the edges
word out at the edges were fatter so
maybe I don't could even there so Adam
can still find the cliffs early in this
one so maybe we should just stop here
but okay so this this is how you will
receive the data set and now you want to
do the distinction there is a useful
interpretation in terms of the adjacency
matrix what I will do is I will include
the presence of an edge by a plus 1 and
the absence of an edge by a minus 1 okay
so under the null hypothesis my
adjacency matrix or the adjacency matrix
of the graph is a purely random it is a
symmetric matrix with purely random
independent signs so it's filled with
rademacher independent random variables
under the alternative hypothesis instead
there is a principle sub matrix here the
sub matrix shown in blue which is all
plus once and then the rest of the stuff
is plus minus 10 4 okay and then the
detection problems is the same as what
we had before so as boas mentioned
earlier you what is what is what what is
the thing that you need to at least
solve this in principle is there any
algorithm that will solve this for
certain values of k NN and you know if
you think about this for a couple of
moments you realize well if the clique
that you put inside the graph is bigger
than the largest naturally occurring
click the largest click that occur at
random and basically you can just find
it using enumeration you just enumerate
all the clicks of the right size and
then you're done okay this boils down to
a second moment calculation you can
compute the largest click is of size 2
log n I think this is the earliest
reference is due to grim it but there
are all sorts of generalizations for
different models different values of
parameters that you can do but they all
boil down to use it using the second
moment method to estimate the size of
the largest leak again it's it's simple
find the cliq just using exhaustive
enumeration let us move to what is
actually possible one thing really that
is probably the first non-trivial idea
that people had was to use spectral
methods and the spectral method is based
on the following simple observation that
if you take the adjacency matrix and
split it into two parts first is its
expectation conditional on the click and
then plus something else then the
expectation conditional on the click is
really a rank one matrix it is the
indicator vector on the cliq times that
indicator transpose this indicator
vector is very simple it is once where
the CLE on the vertices which contain
the cliq and 0 otherwise so it really
looks like a matrix of this form and now
the noise is whatever it is and when you
look at this it becomes very natural
that you might want to do principal
component analysis so you just use the
principal eigenvector of a normalized
appropriately to as an estimator for for
the cliq the way the efficiency of this
method really depends on the
signal-to-noise ratio in this regime in
in in this case the signal-to-noise
ratio is the ratio of spectral not so
it's the spectral arm of the signal /
the spectral norm of the noise the
spectral norm of the signal the one line
calculation is proportional to the size
of the creek and the spectral norm of
the noise is of order square root of n
by very standard methods in random
matrix theory so you know intuitively if
K which is the size of the clique is
much bigger than square root of n then
your principal eigenvector gives you a
good estimator which you can clean up
and this was the work that was done by a
long crib elevation pseudo cough in 1998
there is a more refined version of this
phenomenon really this is a much more
recent work in random matrix theory and
what happens is a certain phase
transition of the following kind if K is
a little bit smaller than 1 times square
root of n then if you plot a histogram
of the eigenvalues of the adjacency
matrix it looks like this semicircle on
the left so it looks exactly like the
wigner semicircle and in fact what you
can prove is that the principal
eigenvector is as in protic
be uncorrelated with the field in fact
the you can do even more the correlation
is as though it is random effectively
okay on the other hand if K is bigger
than one times square root of n a little
bit bigger then one eigenvalue pops out
of the bulk of the spectrum and the
eigenvector that corresponds to this
eigen value which is the principal
eigenvector is indeed non-trivial e
correlated you can at least hope to get
an estimator that might work okay there
are many many versions of this resulting
they're called spiked Wigner models in
probability theory the most advanced
result that I know of is remarkable
paper by knowles and gene but there are
many results by sonic of bike been a
ruse pêche a and others in the field
okay so here is the first result that i
would like to talk about which is
something that really breaks beyond the
spectral barrier it is that you know the
result is that there exists an algorithm
that will work if K is bigger than
square root of n over e so strictly
improves over the spectral barrier of
one times square root of M and moreover
you on this this really works in linear
time essentially linear time in the data
because the data is in an n-by-n matrix
there are n square sum of bits of
information so you know what is what is
the key observation that makes this
possible which is that you know there is
a little bit more structure that you can
take advantage of and the structure is
well the signal was of course Rank 1 but
it is in fact also sparse so you can try
and take advantage of this extra
information and indeed PCA is completely
agnostic of this posture in PCO does not
care whether the rank one component is
partial not so this observation was also
used by Phillip and others in doing
reductions to the hidden clique problem
and there is various these are just a
small sample of the references that do
reductions from other machine learning
and estimation problems to the hidden
click settings so you can set up a
complexity theoretically
in the usual sense okay so how do we
take advantage of this let us just let
us consider the first you know the
simple way to compute the principal
eigenvector is this the power method
okay so i will start with an estimate v0
and i iterate VT plus 1 is a times V T
times a normalization factor that makes
my life a little bit easier okay since
VT is supposed to be your estimate for
the cliq maybe you want it to be sparse
because you know a priori that this is
part so let us try and do the following
instead of using the power iteration as
before I will do a nonlinear version and
the nonlinear version is that instead of
applying it directly to VT I will apply
it to a function of V T and this
function for simplicity I will just take
it to be a coordinate Y separable
function so just think of the function
as I take some thresholding operator for
instance and applied to each entry of VT
and then hit it with the matrix again
and then I keep doing this this is a
simple idea and what obviously you would
want to do is to choose ft which is some
some some kind of thresholding function
that X that price and enforces the
sparsity in some way okay so for the
next few slides I will do a wrong
analysis but we will see why it's wrong
let us write out the is coordinate of VT
plus 1 the VT plus 1 is 1 over square
root of n that was my normalization
factor times the sum over j AI j ftv TJ
this is just a matrix multiplication
written out sorry likely I don't know
what was the last amount honestly I
wouldn't recommend it there is a reason
that is a wrong in the title
I prefer one good show it's done
correctly so yeah so I think this is
something so this this is something that
I think that physicists say that that
wrong calculation is better than no
calculation so let's go with this one
for a bit okay okay so you now when you
look at this sum and you remember that a
IJ s are you know these random signs
when I look at that some I feel like
applying the central limit theorem okay
so maybe let's just try and apply the
central limit theorem for VT plus 1 I
this tells me you know the normalization
is basically correct so VT plus one of I
should be approximately Gaussian since
all of the edges are random signs if I
is not inside the creek this should be
Gaussian with me in about zero and
variance on basically the sum of squares
of the coefficients okay this
prescription also works for the previous
iteration so since vti is in also
Gaussian with mean 0 and some variance
you can write down what the variance
recursion is you can write down this
variance at the next iteration based on
the variance at the first iteration so
you get some recursion for Sigma T it's
a simple recursion that that is based on
the function that they function ft that
you use instead if I is inside the cliq
something almost similar happens except
that there is a small tweak since all of
the edges are not random signs some of
them will be all plus one the ones
because this neighbor this vertex is
going to be connected to all of the ones
in the inside the clique so those ones
do not have random signs they are all
plus ones so that accumulates into a
mean term but then the rest of the stuff
is the same as before okay so you can
similarly have a recursion for the mean
and this depends is proportional to the
ratio of K over square root of n times
some similar expectation that depends on
the mean variance parameters as before
this is okay does anybody know why this
analysis is wrong
okay men are you worried about is
exactly the you cannot apply the central
limit theorem right the the EI j is the
after at least the first iteration the
VT j's are also complicated functions of
the ages and they are not independent of
the aaj so you cannot apply the central
limit theorem it's just wrong let's
persist for a bit we will see we will
see a little bit later how one
eventually fixes this so in pictures
what you have is you know this following
history if I plot the histogram of the
vti is what you get is that the ones
been not belonging to the cliq form this
big ocean of a certain variants and the
mean is centered at zero and instead the
ones that are inside the clicker a
similar Gaussian but much smaller in
number and those are situated at a mean
that is away from zero when you look at
this what you want to do is they push
them in as far away as possible from
zero and make it as far in comparison
with the variance so that you can just
maybe cut in the middle and remove the
clicker okay so these are the equations
or the recursions that i will call state
evolution but it basically describes the
state of this algorithm and you know
what is nice about this is that it tells
you what the optimal function is this is
a simple khushi Schwarz inequality you
can normalize the variance to one and
then optimize which is the function to
use and what you get is a recursion for
the mean parameter okay the recursion
for the mean parameter is nice because
now this tells you what really happens
depending upon the ratio of k k over
square root of n with in case over
square root of n is bigger than one over
square root of e you don't have any
fixed point so if you start at the you
always start at the origin this is mu t
plus 1 in terms of U of T you start here
you go up to the curve you come back you
go to the curve you come back etc if
there is no fixed point you will you
will essentially go all the way to
infinity on the other hand if case small
K over square root of n is smaller than
1 over square root of V you have
non-trivial fixed points and then this
you will just saturate at the fixed
point you will never be able to recover
ok so the analysis is wrong but the
theorem is right and what really makes
the theorem possible is that you have to
change the algorithm a little bit
instead of using using the
usual power iteration what you have to
do is or non-linear power iteration you
have to do it with what you would call a
cavity removal and this is inspired from
the belief propagation heuristics that
has been used in machine learning and in
statistical physics do you need is just
to analyze it though the do you need
these things to actually make it so the
original without the cavity removal the
state evolution predictions are false so
you cannot so that the the recursion
does not describe the original algorithm
without the cavity removal which the
cavity removal the dependence problem
that we had that you mentioned before
actually gets resolved at least as in
Tata CLE okay so there is also a lot of
recently there was some follow-up work
by Bruce Hayek and others which applied
this and this stuff to some other sparse
graph estimation problems okay so at
this point I will just sort of summarize
the picture that we kind of know which
is that statistically you just need k to
be of order log in and you can do
exhaustive enumeration which is quasi
polynomial time here on the other hand
for any most of the computation all of
the computationally efficient ones
require k to be at least a constant time
square root of and including our
algorithm it did not really improve over
the spectral algorithm by more than a
constant so you can actually reduce this
constant as much as you want if you
sacrifice in the exponent of n that is a
there is a nice trick that is actually
even there in Ilan's original paper but
the fact is that in a reality will never
want to apply this to any algorithm any
any data set because it what what it is
asking you to do is to look at pairs or
trifles of further all pairs and try
pools of vertices and this is never a
good idea to do on it
the other direction is n square better
than n square there is an algorithm that
is order N squared which is due to
evolve Paris I think yelled aquel and
gory good al gharafa i don't know i
think they are constant is something
like 1.6 I don't know whether it is
sharp or something to do is open i'm not
sure i think the n square usually just
comes from doing some sort of I compact
like some sort of power it usually all
of them are I turret of algorithms which
look very similar to the power iteration
with a few extra weeks of some kind of
the other I'm sorry what assumption
square they do instead of n Square log
in yeah so the log and we require for
four ok so the I did not talk about this
but in reality we can do the analysis
for the cavity modified power iteration
only for a constant number of iterations
what you need to do is to do a spectral
clean up after that so the log n comes
from doing the power it at the usual
power iteration for finding a but it's
just on a much smaller matrix here
official strong the constant is weaker
and there is a log in the constant is
weaker but the log n is a bit better
yeah in practice and reality i think
this result goes i mean if you actually
run this thing 20 iterations is fine
yeah you can run this thing for 20
iterations and it first ferences the
simulations in the paper i did not do
the spectral clean up we just just did
the usual algorithm run it for a few
iterations it it's done
so okay so this brings us to the
question it really is this regime hard
and for what algorithms can be hoped to
prove it so I I will spend a little bit
of time talking about the sum of squares
approach which is really a powerful idea
for general polynomial optimization this
is become extremely popular since the
work of para lo and lahser in in the
early 2000s but was also present in the
literature in in since the late eighties
what is the sum of squares there really
a principled way of constructing a
sequence of nested convex approximations
you have a difficult convex set and you
construct outer approximations in a
nested fashion and they seem to unify
many different other ad hoc versions of
two in convex relaxations or other
versions that were used in combinatorial
optimization zit is the lava shriver and
the cherylee adams hierarchy and they
have very interesting consistency
properties which we won't go into and
there they are also very being convex
relaxations you know there are also some
more naturally robust you know this that
all statistical models are not always
correct there is always something that
is left unexplained and you don't want
your algorithms to be extremely
dependent on the model because that
means when you applied to some data it
will just give you some nonsense result
so convex relaxations are a nice way to
make these robustness properties happen
and the sum of squares relaxations have
some natural robust person monotonicity
properties that make them a very good
class of algorithms to consider and
really the third is that if you prove
something about the sum of squares even
a cstr estate would be interested some
of them are obviously here but really it
has been extremely well studied in over
the last 10 or 15 years in the cs theory
community so if you want to prove
unconditional results if you want to
prove a hardness result about a specific
class of algorithms
I would wager this is a decent want to
think about okay so what is the
polynomial optimization problem for each
vertex in the cliq you attach a boolean
label which is 0 or 1 so I'm sorry for
each vertex in the graph you attach a
boolean label that 0 or 1 1 indicates
membership inside the clique and 0 is
number 0 means it is not inside so you
want to maximize the size just want to
find the largest click subject to well
the boolean constraint and the second
constraint that the support of X must
determine a click and the way you can
enforce this is that for every pair of
vertices that is not connected you
enforce the product of the variables to
be 0 so it cannot be that both of them
are one this is natural the key idea of
the sum of squares is you know in store
one way to look at it is that you don't
want to optimize over points on the
feasible set of X instead you optimize
over probability distributions that are
supported on these points moreover and
this is key you will represent these
probability distributions using just a
small number of moments and you enforce
only a few of the consistency conditions
that these moments are supposed to have
ok so this will be a bit of notation
that I really need to set up but n in
braces is the first n integers n choose
D will be the set of subsets of size
exactly d n choose less than D is the
set of subsets of size at most D and the
disc the decision variable is something
that takes a subset of size at most D
and spits out a number between 0 and 1
ok and the interpretation that I want
you to keep in the back of your mind is
that this this number is is kind of a
probability or or a moment of some under
some appropriate distribution this is
not exactly right but it is a useful
mental model to have so what is the SOS
program you want to maximize the mass
that is assigned to the synchronous this
is analogous to the size subject to a
few consist
the first is just normalization
corresponds to probability distributions
integrating to one the second is the
constraint that enforces the cliq part
which is that if X if a subset of
vertices in the graph does not induce a
click then X must assign it zero
probability or zero value or zero mass
and the third is really the most
important constraint which is that a
certain moment matrix that is a linear
function of X this must be PST and what
is this moment matrix it is a more it as
a matrix where every row and column
corresponds to a subset of size at most
D or ok a subset of size at most D over
2 and the entry corresponding to the row
s 1 and the column s2 is simply X
applied to the union of s1 and s2 okay
so this is a bit of a this is a little
much but let's just look at a picture so
let us look at the case when T equals
two convenient to think of dsr even
integer so if you look at d equals to
you have the moment matrix which where
every row and column corresponds to a
subset of size at most one so you have
this null set and then you have all of
the singletons right so you must assign
something to the null set you must
assign something to all of the
singletons and you must assign something
to all of the pairs
so let us talk about the specific test
so i will call SOS GD the optimum of the
SOS program when it is run on T on the
graph G and what i will do is i will
just declare that the alternative
hypothesis is true if this SS
programming bigger than k otherwise i
will declare the null hypothesis to be
true the rationale for this test is very
is very simple imagine that instead of
the SOS program i had the Hydran oracle
that computed the true optimum of the
program if I had that Oracle then this
would be the right test what I am doing
here is just replacing that Oracle with
a sum of squares relaxation of the arc
which is computable in n to the order d
times ok so this is the main result it
is a result for D equals 4 and this is
the first some non-trivial level of the
relaxation which goes beyond the
spectral methods somehow which is that
this particular test fails when K is
smaller than n to the 1 over 3 and the
twiddle means that I am ignoring a few
log factors because I think boys might
agree that what are a few logs between
friends and not even sure how many log
factors and hearing here so
independently there was work by rock
Omega R and protection and RV big person
and they proved a slightly more general
result or a more general reserve which
said that the same test but for
generality failed if K is smaller than n
to the 1 over d if you compare it there
is obviously more general because it
goes beyond the equals 4 but for D
equals 4 it is a bit weaker again I'm
ignoring log log factors here
okay so some comments that I would like
to make the a reality if you believe
that the problem is hard you would want
to prove that for any constant D the T
some of the sum of squares test really
fails at the threshold of square root of
n times maybe something that might
depend on d there was a very nice paper
by raghu maker and avi victors and in
2013 that claimed this result really
they set up a lot of machinery that the
wall of all of us used later on but
unfortunately the matrix concentration
methods that they used failed for very
subtle reasons it took a while also to
find compare examples to their proof or
it took of a while to find even counter
examples to the bow so is it was but
it's still a very beautiful paper and
what what got us interested in this
problem but you know just a couple of
months after we wrote our paper there
was a very nice observation by two
independent groups one is prasad
raghavendran and students a lil SRAM in
berkeley and hopkins kothari and
protection who are at MIT and ms are
respectively and they improved our
result to essentially the optimal
exponent of square root of n it requires
you know really one one or two important
observations beyond what we did it's a
very nice set of results indeed how the
hopkins paper has a little bit more ins
and they prove something also for the
generality
so if you have any questions about this
yeah particular tests that all these
things are analyzing is provably optimal
amongst Atul tests that use the degree D
SOS relaxation it's a good question
there is nothing okay there is really
nothing of this sort that's exactly in
this setting there is there is some work
by David story and the students on a
related problem on that is related to
tens for PCA and what what they show in
that paper is that you know if you have
if you have at one level a second order
effect or a lower order effect of that
is present in the relaxation and you do
one more step of the relaxation this
becomes a leading order effect so I am
not sure whether one can prove that this
particular test is optimal maybe what
one can hope to prove is that if the
test is suboptimal perhaps increasing
the order by one or two or a small
constant number of by a small constant
actually removes this sub optimality
it's very subtle how to find this
question because in particular you could
say that the SOS solution also gives you
the graph itself you could recover from
that all the edges of the graph so
information theoretical it also
determines exactly whether there is a
click or not so you have to be a little
careful in how you define what it means
for a test the you know what kind of
information do you allow a test to use
is it just do it the actual value is it
is it the full moments
I mean even in the first part of you
taught just using the bulk of the
distribution groups of the actual I get
vectors non-linearity was already very
different right I think there is a
result that shows that no test based
only on the eigenvalues can work I
respect of whether it's polynomial time
on all right but that is not for this
model that's for a slightly different
model for the Gaussian version of this
point okay so what is the proof strategy
you know it's it's fairly intuitive it's
the following observation is that if you
think that the relaxation is good you
would think that well the SOS value on
under the null model would be pretty
small if it and under the alternative
model would be pretty large if the
relaxation were good in the sense that
it is faithful to the original
polynomial optimization if you think
that the relaxation is bad then all this
means that you have to do is that you
have to ensure that the value of the SOS
program is large under h0 or under the
null model ok so this is a maximization
problem one way to show a lower bound on
the maximization or the value of the
maximization problem is just to find us
guess a solution and compute its value
so that is what we are going to do we're
just going to guess a solution will
prove it is feasible and then we'll
compute its value minus 100 ok so before
going to the D equals 4 case I'll just
spend some time on the d equals 2 case
and we'll actually do a proof here so i
think i've heard there was a quote that
i heard only a part of the first part of
which is you know every talk must have a
proof and the joke there is a second
part to this quoted that they must not
be the same thing so so this is the
proof part so ok recall that we have
this moment matrix this is for d equals
to some something that has rows and
columns that are indexed by subsets of
size at most one so just an ulcer
and the singletons so recall what are
our constraints you want to set X fee
equals one so there is no there is no
nothing to guess there you if X of S is
not if X s is not a creek then you must
set XFS 20 so for instance if a pair of
vertices is not connected you must put 0
over there anybody have any guesses for
what you want to put on the singletons
and on the pair's ok I will take this
simplest possible guess right you want
to put something for the singletons and
something for the pairs if the pair is
not connected you must put 0 so you know
there is some multiplier alpha that
depends on the pair and that depends on
the singleton let us just take it
independent of which one it is so I'll
just put one value if it is a vertex and
one value if it is a pair if it is an
edge and for the non edges i just put 0
right so basically what i get is alpha 1
times 1 on on the outer product and
inside i have alpha 1 times identity and
then there is alpha 2 times the 01
adjacency of the matrix which i will
call script EG ok a question is now the
linear constraints are by the way
automatically satisfied so you do not
have to worry about them at all all you
now need to do is to show that is is to
show that the moment matrix is PSD so
when I look at this I wanted to do the
shore complement immediately so let us
do it the first part is the alpha 1
times identity plus alpha 2 times 0 1
adjacency matrix and then you have to
subtract the shore complement which is
the product of outer product or the
factors you get a lot of value alpha 1
squared times 1 1 transpose you want to
show that this matrix is PST let us look
at it in expectation in expectation the
the graph just becomes on alpha 2 times
1 1 transpose because each vertex is
connected with pro each edge is
connected with probability 1 half so you
just get alpha 2 over 2 times 1 1
transpose and since you want this to be
PST it makes sense to use the 1 1
transpose factors to cancel each other
out let's just so we
put alpha 2 over 2 minus alpha 1 squared
we bigger than 0 this is one condition
the second condition comes from the fact
that there is a deviation indeed so
we'll use the identity parts that was
left over in the earlier equation to
cancel out the deviation the deviation
is just alpha 2 times G minus its
expectation okay so if you set also one
bigger than the spectral norm of G minus
its expect if you are done this again we
can prove is about of order square root
of n using very standard results in
random matrix theory so you have alpha
one is bigger than alpha 2 times square
root of n okay so this is the second
condition you can put them together
because you know you have an upper bound
on alpha one in terms of alpha 2 and the
corresponding lower bound these must be
consistent so if you solve it you get
with a one-winged calculation that alpha
1 is supposed to be at most 1 over
square root of n under h0 or under the
null hypothesis the value is or the
value of the maximization problem is at
least the value of this solution the
value of the solution recall is just the
sum of them or the mass on the
singletons so I have n vertices and for
all of them I put alpha 1 so i get about
square root of n okay so what did we
really prove we're approved a result by
phi grand crowd kemer in the early 2000s
which is that this test fails to
distinguish between h0 and h1 when k is
about of order square root of ok before
going to d equals 4 i think it makes we
would like to just go over the key
ingredients for d equals 2 which is that
you know firstly we had a very simple
choice for x we analyzed its expectation
you know there was some some of
orthogonal projector there was something
along 1 1 transpose something orthogonal
etc and then for the deviation we could
use standard results that were already
known you can use the matrix
concentration or results you can use the
moment method that goes back to Wigner
you can use epsilon net arguments which
are I think first
used by Friedman content samurai D in in
four random graphs you can use all of
these things the thing that changes when
T equals four is that really each of
these steps becomes a little bit more
complicated so the guess is kind of
similar in the sense that what you want
to do is to take X that is proportional
to the indicator that a particular
vertex is a click and then the
proportionality constant you know we
just make the simplest possible
assumption of choosing it to be just
dependent on the size so it is something
if it is a vertex some things it's a
edge something if it is a triangle and
something if it is a four click ok again
linear constraints are automatic all
that matters is the PSD constraint that
gives us conditions on these alphas okay
so this is the moment matrix for D
equals four it's in it has about 10 to
the four entries in it but instead of
looking at you know this this large
messy matrix i will just concentrate on
the sub-block that is indexed by the
edge the by the by the pairs and and you
know see what happens to each of our
steps for this sub block it illustrates
basically the main difficulty the
expectation you can instead of compute
instead of having just two orthogonal
projector ins you have actually three
orthogonal projections and they have
actually very widely differing
eigenvalues really the important the
point is that these the eigenspace is
corresponding to these eigenvalues
relates to irreducible sub
representations of the symmetric group
this is because this expectation is
invariant to changing the leg relabeling
the vertices so that is why this kind of
naturally concept but the eigenvalues
are very differing so you really need to
understand these symmetries and take
advantage of this fact especially when
you consider the deviation part okay
this way when you look at the deviation
you know it is an N squared times N
squared matrix so it has n to the four
entries but is dependent only on N
squared random bits okay so all of these
entries are highly dependent
okay and because of that the behavior is
very different from classical random
matrix ensembles in the sense that some
of this matrix actually has an unusually
high spectral norm base to the compared
to the prediction that you would make
without taking the dependency into
account so how do we make it work really
you just decompose the deviation into
pieces and you control each piece
separately and you try and align the
pieces which have large icon values or
large spectral norm with the eigen
spaces that are large and this requires
you to understand the symmetries of what
is happening will not go into this
detail but I'll just end with a couple
of before I finish this section I just
want to mention a few open problems so
the question remains whether we can
generalize this to other models if K is
smaller than square root of n proving
that this is hard for d bigger than 4 is
still open the last time I said this I
had less bigger equals 4 and then like
two weeks later the bigger equals became
wrong maybe this time maybe this time
they will do it for you be careful you
can of course it remains whether you can
come up with stronger hardness proofs
okay I'll just spend a couple of minutes
talking about a few other things that
I've been thinking about over the past
few years the the hidden clicks problem
is also very interesting because it's
related to you know structured versions
of principal component analysis
particularly sparse pc so we had there
was work with andreea in which we
analyzed covariance thresholding which
is an algorithm and what you can do is
really go beyond this standard stp that
people usually use first patch pc which
emil and Dondria we had another paper
which considered even more general
versions of this where the factors of
the principal components were
constrained to be in a cone in a convex
cone so for instance non-negative PCA is
is an example of this
more recently there is been a lot of
work on stochastic blocker models from a
variety of different communities so this
was a paper in which which we proved a
couple of things but I think the most
interesting part about this result is
firstly a universality portion where you
prove really that the asymptotic mutual
information is invariant is essentially
independent of the specific random model
that you use this is similar to
universality results and random matrix
theory where say the spectrum does not
depend on whether you have Gaussian
entries or plus minus one and trees etc
and the second part of the result is a
rigorous verification of what in physics
is called the replica symmetric concerts
I won't go into that in detail here
lastly i just want to mention some
things that are that i'm thinking about
now or the racing could be very cool too
cool to are cool to tackle the first is
you know it is still related with with
pca and structured versions is that if
you if you see the use of these things
especially in nowadays it's popular in
medical literature to combine data from
many different past studies and then do
a joint analysis unsupervised methods
like pc a clustering all these kind of
things are used a lot in in such studies
for a variety of reason to do pre
processing of the data you want to
extract features you can use the
principal components as certain features
of the clusters as features I and really
they also formulate different hypotheses
based on this so I think there are a
couple of challenges that I find that
would be cool to think about the first
is obviously for clustering and for
structured versions of PCA we don't
necessarily have great algorithms so can
we do this in polynomial time or can we
come up with fast and reliable
algorithms the second you know is
important for particularly scientific
applications is that can you do
uncertainty SS can you build confidence
intervals can you assign p values to the
estimates that you get because
important when you try and use these
things for for medical studies and the
second is related to the sum of squares
results when I learnt this stuff I by
the way I strongly recommend boys is
lecture notes they are fantastic when I
read those nodes and i learnt started
learning about this i found it really
cool but a reality if you try and
program these sum of squares algorithms
they are very difficult and this is
because it gives you a humongous stp
which is not easy to solve so but there
is there are some ways in which you can
make think about using these in
practices well which is a lot less done
in the community then say for instance
lower bounds using SOS are the cs three
community and one example i think that
would be nice to do is use this in
graphical models graphical models are
widely used for a variety of different
things like image segmentation error
correcting codes etc this is a book by
Daphne Koller who is a professor at
Stanford I think it is about 1,200 pages
of examples of graphical models so very
nice book so there's about three
important things that you want to do
with a graphical model but only you want
to sample from the model you want to
marginalize variables based on the other
so say you are given some diagnostic
information you want you are given
symptoms you want to know what disease
it is you can think about
marginalization and the third is
computing partition functions all these
are roughly equivalent and one can think
about computing partition functions as a
general version of all of these now
really there are two popular methods the
people use in practice there's
variational methods where you come up
with versions of the partitions
approximations to the partition function
and try and computed belief propagation
and other versions are similar
unfortunately they yield only
approximation so for instance you do not
get rigorous upper and lower bounds
moreover these are notoriously difficult
to analyze especially for general
graphical models I think the sum of
squares methods can provide a way to
actually computing these or
with rigorous upper and lower bounds for
these things you give you a knob it is a
rough knob but at least they give you a
knob to trade off between accuracy and
computational requirements and the
consistency properties of these sum of
squares are I think closely related to
what are called kikuchi approximations
in physics so I think there is a lot of
work that is that is that can be done in
in in this area so thanks for your
attention and I'm happy to take
questions yes 1 over square root the
threshold in the first part of your talk
is optimal for that class of algorithms
excellent question ah I suspect it is
but I'm not sure yet how to prove it one
version of the result that we have is
that instead you consider the same
problem on sparse graphs instead of
dense graphs and for that we can prove
that it is optimal for its optimal for
local algorithms if you consider the
problem on a sparse graph there is a
sense of locality in the usual sense
locality of so for all local algorithms
we're in the neighborhood where the node
makes decisions just based on a bounded
size neighborhood around it this one
over square root of V is an optimal
threshold beyond that I'm not because we
don't have regular tools for
understanding and belief propagation
other iterative algorithms and dense
regimes is that the reason why some of
the limitations for local algorithms
don't translate the dense settings but
there is no good sense of locality on a
dense craft right I mean a reality
though okay the graph in a reality the
graph that you must think about in the
dense case is is actually the complete
graph so every vertex has all of the
neighbors in its in its in its
neighborhood now the sum of squares was
actually one other reason that I'm
interested in summer squash is that it
gives you the right
a notion of locality which is really
based to the moments we're using only a
few moments so it may be that is one
notion that one can think about the
other thing is that this trick of a lawn
where you can reduce the constant by by
improving the by trading of the exponent
so clearly it can only be done say for
some linear time algorithms of some sort
this seems to be an even harder problem
that then anything else so i'm not sure
but yeah it's a it's a very interesting
question to think about yes it imagine a
variant of the planted cleek problem in
which someone gives you maybe a hint as
to which nodes are correct or not maybe
you where biochemist and they flag
certainly notice is being more likely to
occur are you aware of work in this kind
of hint model visit obviously easier or
is there is anything known ah it's a
good question this really this this
constant reduction trick is very closely
related to what you said we really you
just guess which pair of vertices and
then reduce the graph to all of the
common neighbors of this vert of
vertices that that improves the
signal-to-noise ratio that you have
there the thing is that in real applique
this is a very neat trick but I really
don't like this trick very much before
for a number of reasons first that you
cannot run it secondly on real data sets
you know it is hard to decide what is
what is the right common neighborhood
because that depends on the threshold
reality you would just want to do
something that takes advantage of all of
the weights so for instance if it was
not a completely connected so if the
cliq was not completely connected but
say was additional rainy with some
probability P that is strictly less than
1 it's not clear how to do exactly like
versions of this stuff so I would
suspect that I don't know if any any
work that really goes beyond this into
the sum squares favor but just saying
the key
you just measure the objective function
differently photos vertices if you had
scores if you have if you are the score
for every vertex you do my to maximize
the score evident maximize the cliq size
or something right right so those alpha
the seer so for ya it's that's an ID as
an excellent point for this is one way
to do it is that instead of having the
sum over all of all of the singletons
you had a weighted sum and perhaps you
put large weights on on the ones that
you think are more likely maybe this
would work I'm not sure if anybody who's
actually abused something with this kind
but it's a wonderful thing to think of
it you can get more questions from there
with you we're supposed to be on a phone
call let's turn in it sure sure sure
today and Henry is taking over and
hosting so there</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>