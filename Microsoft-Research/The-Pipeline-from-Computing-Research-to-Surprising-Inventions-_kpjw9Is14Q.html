<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Pipeline from Computing Research to Surprising Inventions | Coder Coacher - Coaching Coders</title><meta content="The Pipeline from Computing Research to Surprising Inventions - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Pipeline from Computing Research to Surprising Inventions</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_kpjw9Is14Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">that's uh that's really such a generous
introduction I I'm just so honored and
thrilled to be here I considered just a
great gift from all of you to me and so
I hope you'll accept my small gift of
this lecture what I'm going to talk
about today is the subject of basic
research in computing and of course that
initially begs the question what is
basic research and it's a little bit
embarrassing but it's actually hard to
answer that question now and it's a
question that's been hard to answer
actually for very long time around 80
years ago there was the famous rocket
scientist Verner von Braun and later in
his career he once stated that research
is what I do when I don't know what I'm
doing and I think he was inspired by
Albert Einstein who said something very
similar I think a little bit earlier
which was the line if I knew what I was
doing it wouldn't be research and so let
allow me please to put myself in the
company of Albert Einstein and Verna von
Braun and admit that oftentimes I don't
know what I'm doing and that's in fact
part of what I'm going to talk about
today one of the exciting things about
research in computer science is that we
oftentimes set out to do one thing set
out to solve one problem perhaps we
solve it perhaps we don't but
surprisingly often the results of our
research in computing produce surprising
results and that idea of surprise or
sometimes refer to as serendipity the
feeling that somehow we just became
lucky in the application of an idea or
in the usefulness of some result that
concept is something that I find just
completely intoxicating and valuable in
our field of computing research so what
I'd like to do today is go through three
examples drawn from the experience of
Microsoft
research of how basic research leads to
surprises in innovations in technology
now before I do that I am very honored
here to be in front of so many very good
students and so I was a professor for
very long time and so let me start this
lecture with a small test and so I'm
going to ask all of you to participate
in this test and what i will do is i
will show you a picture of some multi
colored dots and i will give you three
seconds to complete the problem and what
is the problem i want you to look at the
picture and tell me within three seconds
how many green dots there are everybody
ready okay here we go three two one okay
how many blue dots anyone
if if it's any comfort to you when I
lecture to students in the United States
and in Europe they don't pass the test
either so it's okay now of course it's a
silly test it's a trick but there's a
point here for example in Microsoft
Research we oftentimes work with product
groups and these product groups are
extremely disciplined they are just
incredibly focused on understanding the
customers needs on defining a product
and a user experience on writing down a
plan very carefully and then executing
that plane that plan almost with
military efficiency and so what you were
all doing when you were trying to count
the green dots is you are also executing
a task almost with military efficiency
you are very focused because you're all
very good students on completing the
task of cutting the green dots so one
thing about research is that research is
more about an exploratory search for
truth and understanding sometimes a
search for beauty and so in research yes
we want to know how many green dots
there are but we also have other
curiosities we want to know are there
other colors are there other shapes how
many other colors and how many other
shapes how many blue dots and it's in
those other questions that we oftentimes
find surprising innovations and it's in
those surprising innovations that
oftentimes in fact more often than not
we get the real value of research so let
me try to go right into some concrete
examples so the first example I want to
talk about has to do with what's called
audio processing and so this is a scene
from the New York Stock Exchange and if
you look carefully at the picture you
just see pandemonium lots of noise
people shouting waving at each other and
a traitor who's recording trades on the
floor is looking at this scene and is
listening to each person's voice
for important trade information and is
noting this down sometimes in the
notebook sometimes in the tablet
computer what's amazing is that we as
human beings with our audio array of two
ears are we are able to focus our
hearing even in a noisy environment on a
single person's voice in a crowded room
like this and so one question is can we
have computing systems with similar
kinds of capabilities and this is a
problem that of course many people have
worked on maybe some of you have worked
on and so the problem is as follows
someone is speaking to us and then we
have a microphone and we want to hear
that person's voice using this
microphone and of course we have the
complications that that person might be
in a crowded room with many people
talking and so somehow we have to focus
on that one person and in fact it might
not just people talking but there might
be many sources of noise and sounded
music all throughout the room and so the
question is can we have a system with a
microphone that's able to hear that one
person's voice that were interested in
now as many of you know with human
hearing we have two ears that makes an
audio ray and so we could also imagine
having an audio array on our machine and
then building an audio pipeline that
gives us the audio output of the
person's voice so that we're interested
in as well as the direction information
of where that person is speaking from
and this is a classic problem in signal
processing and computer science now in
Microsoft Research as long as the more
than ten years ago we have been engaged
in research on exactly this type of
problem and we're very proud of the fact
that we've developed technologies such
as the mvd our adaptive beamformer that
is actually able to do sound processing
of an audio ray in order to have this
kind of focus and this is an example of
what i call basic blue sky research
we're trying to solve a problem trying
to achieve some understanding about the
properties of sound
and about the properties of adaptive
beamforming and understand and write
down the theoretical properties of this
now at some point one wants to move past
the theory and achieve an actual working
system and so at Microsoft Research we
of course engage in that type of work as
well and so what you see pictured here
is a prototype of a 9 microphone audio
ray system that was used extensively in
a number of experiments to try to
implement and make real this concept of
an MV dr adaptive beamformer and so this
9 microphone array was attached to a
machine the computer with an audio
pipeline that would give audio output
and direction information and just to
give you an idea of how well it works
let me play two audio files the first
audio file that you'll hear is the sound
of several people talking in a noisy
room and this sound was captured with a
single microphone one of these
microphones so listen very carefully I I
think we need a little more volume
and so I don't know if anyone could hear
but in fact there was a man talking
amongst all in all that noise and you
hear the problem that computers tend to
have in a crowded room a room like this
or even in a living room where you're
having a party with a bunch of friends a
computer has a very hard time hearing
person trying to give commands to the
computer and so now with this device and
with an M vdr adaptive theme former that
very same audio source comes out
sounding like this and so now you can
start to see the same kind of ability to
focus the hearing of an audio ray
attached to a computer very similar to
what a human can do in a crowded room
and this ends up being extremely
important in a number of applications
such as video and audio conferencing
systems now I call this a type of
research that's directed to disruptive
innovation or disruptive technology we
take some basic research ideas
oftentimes very theoretical ideas and
actually try to reduce it to practice
try to build a device it actually
realizes the theoretical principles in a
way that one can imagine could be
practical and so the they're going from
basic research disruptive technology ins
up being a very important step in the
whole path or pipeline to innovation now
one thing i should say about microsoft
research is that and it's something I'm
very proud of that MSR is that we
publish openly most of what we do almost
everything that we do and in fact here
is a picture of a research paper that
two of our researchers Yvonne Tasha and
Alex acero along with their intern at
the time nilesh m'dear wrote on
that prototype system and this is
important because by engaging with the
external community by engaging with
universities we can tap into the best
minds in the world we can test our ideas
to make sure that they're solid and also
influence the direction of the field in
two directions that are hopefully
beneficial to Microsoft long-term future
now one reason also to show you this
paper is that this paper was submitted
instantly I should also mention before I
go to the next step that Alex acero is
actually here in the audience and I want
to mention that specifically that he's
here in fact he's sitting right there
because this paper while it was
submitted for publication was rejected
and I actually took the time to read the
paper and and the referees reports and
the referees reports were very unkind to
this paper and so just to give you some
excerpts one reviewer said that the that
the solution doesn't solve the problem
at all there's no real stereo echo
cancellation there's no improvements and
the last review report which I thought
was the most damaging was the reviewer
thought that there were serious
drawbacks for realistic scenarios when
these are actually comments from the
reviewers and so this is something else
on the path to disruptions in the path
to real advances in technology that
oftentimes it's hard for the scientific
community to really understand and come
to grips with with something that's very
new of course this technology is a
technology that has been incorporated
into the microphone array in the Xbox
Kinect and so another aspect of the
pipeline from basic research too
disruptive technology is that there can
come a point when as researchers we
engage in what are called mission
focused research research that is in
partnership in this case
is with the Microsoft product team where
we work in close collaboration close
partnership in order to bring a brand
new disruptive technology to market and
this type of mission focused research is
for many of our researchers one of the
Holy Grails of being at Microsoft and
Microsoft Research the idea that we can
use the fact that Microsoft has
literally hundreds of millions of
customers as a very as Eric Horvitz my
good colleague would say as a long lever
to really get the most out of our
research efforts and so despite the fact
that the paper was rejected what we see
here in fact we can see in this
rejection that there's a very thin line
between a visionary product concept and
science fiction the reviewers of our
paper felt that what we had proposed was
actually science fiction not signs fact
and in fact oftentimes researchers try
to stay in a very safe zone staying
within the well-known technology
envelopes and so when you move from
basic research too disruptive research
to mission focused research there's the
potential for magic to happen the magic
that happens when the visionary product
concept and visionary project developers
work hand in hand with world-class
researchers you can take researchers out
of the comfort zone you can add
technical grounding to the product
developers and make something really
remarkable happen and that's in a
nutshell I think one of the lessons that
we see from the development of Xbox and
the Kinect device and so to summarize
this we see a pipeline an innovation
pipeline where we go from basic research
to attempt to disrupt and do something
for the first time to mission focused
research to actually bring something in
collaboration of product teams to market
and then after that continuous in
sustaining improvements to keep adding
on keep building on the successes that
we have and that sequence is an
innovation pipe
line and that innovation pipeline is
something that we work very hard to make
sure we keep very full all four stage of
this stages of this pipeline we strive
in our research to always have good
things happening in fact within the lab
we think very hard about the investments
that we make and so in this graph if we
imagine this graph is the breadth of all
research investments we're on the x-axis
we go from very short term research and
projects and as we go out along the
x-axis we get into longer and longer
term research and on the y-axis we have
choices of problems where we have what i
call reactive problems problems that are
given to us by product teams by society
at large by our academic collaborators
and as we go up the y-axis we get to
research problems that are more
open-ended just the open-ended search
for truth and understanding within this
whole space we see the pipeline fills
the four quadrants of the space where we
have blue sky research in the upper
right quadrant mission focused research
and the lower left quadrant sustaining
research in the lower right and in the
upper left this very interesting
quadrant of disruptive research and so
for me it's as a manager of a lab it's
incredibly important to make sure that
we have very good activities and very
good people operating in all four
quadrants and this in fact is a
management principle for Microsoft
Research in in the last couple of years
and so this quadrant model is something
that will see a couple more times in
this talk all right there's another
point about the quadrants and that is a
point about diversity sometimes when you
talk to people maybe they're businessmen
perhaps they're writers
the in various aspects of technology
sometimes they are financial people
economists there's a tendency to think
of research as being only mission
focused or only blue sky but in fact as
you see in the innovation pipeline and
in the quadrants there's wonderful
diversity in computing research and in
that diversity if we embrace it properly
we get this innovation pipeline if we
can get all four quadrants filled with
very good activities we can really go
and hope to strive towards a really
great feedback loop that keeps new
innovations coming over and over again
all right so that's the first example
now let me give a second example and I
chose the second example looking at Rick
ratchets talk keynote from this morning
where he talked about program synthesis
and that's another area that has some
wonderful surprises and so let's start
from the very beginning on this now in
this research area we start from the
problem of program verification now what
is program verification well the problem
is we take some code and we want to
apply algorithms for program
verification and those program
verification algorithms are supposed to
inspect the code and answer just a very
simple question and that question is if
we were to run this program if we were
to execute this code can something bad
happen and as a practical matter to help
programmers it's also useful to extend
the program verification paradigm so
that if something bad can happen please
generate some examples that illustrate
or exhibit the the bad behavior the bug
and that's useful because that way
programmers can use those input outputs
to chase down the exact nature of the
bug and fix the problem now a program
verification is a very old research area
in computer science it's
very important one Jeanette wing alluded
to it in the first part of her talk and
it's something that Microsoft and
Microsoft Research invests in very
heavily because we write very large
amounts of software and is very
important for us to know that the
software is reliable and secure we again
conduct very basic research in this area
and in fact recently two years ago
there's a very important paper that we
published in popl 2010 on the idea of
compositional may must program analysis
and so I know quite a few of you study
software engineering and software
program analysis program verification
and this paper is really fundamental in
a lot of the tools that Microsoft uses
today in really analyzing our software
and making sure that the software is as
free as we can make it of certain types
of bugs particularly security bugs in
data parsers and so this type of
technology even though it is oftentimes
very theoretical is also very practical
for us and in fact from that basic
research the type of research that you
would read about in that paper in pople
2010 we also take the next step and try
to do something disruptive so we again
go from basic research and take the next
step in the innovation pipeline to
disruptive technology and one of the
disruptive technologies is a system
called sage which runs on a very large
computing system that's pictured above
there and that system called sage does
testing using program verification and
analysis technologies and in particular
using compositional may must analysis
and automatic they're improving to find
security bugs in data parsers in some of
our most important software products and
in fact today sage finds more high
priority security bugs in those software
products than any other testing
technology used at Microsoft today and
so that's again for us highly disruptive
and it's another great example of moving
from very basic research into disruptive
technology but there's more here
and there's more that's actually quite
surprising it turns out that this idea
of program verification can be run
essentially in Reverse and so while we
have this program verification idea we
can also run program verification in
reverse and when we do that that is the
problem of program synthesis that
requested alluded to this morning and so
program synthesis roughly speaking is
the problem of taking some example
inputs and outputs and then asking the
question or answering the question does
a program exists that produces those
inputs and outputs or can we generate
all possible programs that are
consistent with the example input
outputs and if we can can be then
generate the code or all the codes that
are consistent with that input output
and this is the problem of program
synthesis and so the key insight here
and the key surprise is that in fact it
is possible to take program verification
technology which goes typically in one
direction like this and in a flash of
insight run it in reverse in order to
take example input outputs and generate
programs automatically and so this
morning at ricks keynote Rick showed you
a demonstration video of program
synthesis in action in our new version
of microsoft office in the Excel 2013
we're just typing in a couple of
examples allows XL to figure out
automatically using program synthesis
using program verification in reverse
just on the basis of in this case two
examples or one and a half examples
generate some code and automatically
produce all the other results that you
want and so that's already quite
surprising but this is only three stages
of the innovation pipeline where we go
from very basic research to the
disruption of very large scale program
verification
to an application of program
verification in Reverse in Excel what
can come next and what we are seeing now
today with the advent of practical
program synthesis technology is just
huge areas of application possibilities
for program synthesis and just to give
you a little taste of the kinds of
things that we're seeing today let's
think about online education now if you
take a course online you'll listen to
some lectures and then you'll be given
some quiz problems and so let's say
you're taking a quiz problem in in
algebra or in in calculus the problem
with online education if there are
thousands of students taking the same
online course they will all see the same
problems and when they all see the same
problems they will very quickly share
the answers and it will be very hard to
know if people are really learning the
material but again surprisingly using
program synthesis technology we can
actually reduce the idea of producing
unique new problems into a program the
synthesis problem because it is possible
actually to cast the generation of
unique new calculus problems as a
programming language and then ask the
question based on one example problem
can we generate many new sample problems
that are different but similar in
difficulty and so this is technology
that we've been developing very
aggressively and this scales too many
different domains of mathematics we're
even working collaboratively between
this technology and program verification
and in natural language processing to
even have automatic generation of word
problems and English problems and so the
future I think is really very bright for
this idea of program synthesis and so
this idea again illustrates the power
this innovation pipeline going from
basic research the disruptive technology
development to mission focused research
and now to sustaining research where we
just
for even more ways to exploit the
research results that we've generated
and this again helps to populate the
whole space of the four quadrants in our
investment map for research and so it's
amazing to me that we can go from
program verification to spreadsheets to
online textbooks over and over again we
see basic research in computing
producing surprising outcomes so now let
me try to give one last example and in
this example the outcome is perhaps not
quite as surprising but the fact that
there's something new that's possible I
think you'll agree is surprising and so
in this last example I'm going to talk
about coding theory and in particular
erasure codes and since I know that
sometimes the terminology and the
concepts and principles are taught
slightly differently in the US Europe
and in Asia let me just give a very
basic grounding here so that you know
what I'm talking about so let's imagine
that we want to have a data store where
we're storing two pieces of data in this
case the piece of data a with value to
and the piece of data be with value
three so now we can worry about how
reliable is our data store and in
particular if we were to store data
value a on one disk and data value be on
a second disk what would happen if one
of the disks failed and so in that case
we would lose data and so how can we
avoid or make very unlikely the loss of
data now of course a very simple idea
and this is the idea of just backups is
to use replication we can just replicate
in this case have a single backup copy
for each of our data copies so we'll
have a backup copy of a and a backup
copy of B and so in that situation if
one of our data stores should go bad we
still can extract the value of a from
our backup and of course while this is
is a very simple idea it's also quite
expensive in terms of the storage
overhead in fact we've doubled the
amount of storage we need for for this
data so that's not too good on the other
hand is not all bad because while the
storage overhead is a factor of two the
what's called the reconstruction
overhead the cost of extracting our data
in case of a failure is very low in fact
it's no higher in this case then than
the original now in order to improve
this of course there's quite a bit of
work in coding theory and in particular
in erasure codes and I think many of you
have probably studied this in your own
investigations into computer science but
let me just give a very quick example of
erasure coding where if we want to store
a and B but with higher reliability we
can also then have a third data store
that has a code in this case the code
being a plus B equals five and in this
situation if we were say to lose our
first copy of a well it's okay because
by reading be and reading our erasure
code we're able to reconstruct the fact
that a is equal to two and so we still
have our ability to do reconstruction
and we're able to do that with much
lower in fact half the storage overhead
of the simple replication approach
although it we have double the cost of
reconstruction because we have to access
to pieces of data to do reconstruction
and do a little bit of computation and
so these are kind of the trade-offs and
many of you I know have studied
reed-solomon codes and understand the
principles here are quite well now
talking about reed-solomon codes
resettlement codes have been around for
more than 50 years and the so-called
conventional reed-solomon 63 codes in
fact are quite practical and used
extensively by Google and so here you
can see that we have some spreading of
the data that we're storing
over six stores plus having three
additional stores to support
reconstruction and so we have
essentially a 50-percent overhead for
our reliable store now we can ask
question can we do better and of course
over the past 50 or 60 years people have
worked very hard to understand racial
codes and to really investigate whether
we can do better and there are of course
refinements of reed-solomon codes one is
a 12-4 code and this is similar to
what's used by Facebook and this of
course has only thirty-three percent
overhead on the other hand if we do have
a failure for example in datastore d0
then we have a very large number of
reads in order to reconstruct the code
and so the network traffic and the read
traffic that we have during
Reconstruction can be quite expensive
and so we have this trade-off in erasure
codes between again between storage
overhead and reconstruction costs now
the surprising thing here is that while
this idea has been studied for more than
50 years you would think after 50 years
that everything possible that can be
known about racial codes has already
been discovered and if I could just
refer back to John hop Crofts lecture
this morning which I found really quite
inspirational if there's any one lesson
that I would like you to take out of his
lecture it is that it is never the case
that we know everything and even in
something as direct as a ratio codes as
as recently as March of this year in
even in this kind of well trodden area
new breakthroughs are being made and I'm
very proud to say that in fact in
microsoft research involving researchers
in product teams in a
and as well as in redmond and in the
Silicon Valley new erasure coding
techniques in this case called local
reconstruction codes were developed that
allow you to tune the availability of
storage for much lower storage overhead
and much lower reconstruction costs and
in fact this was in March that the
usenix technical symposium the best
paper of that conference now this
matters a lot in fact Theory matters a
lot because we are seeing today a very
significant growth sixty percent
year-on-year file based storage growth
and so the costs for maintaining cloud
services with high availability ends up
being very expensive and growing rapidly
and on top of that as that storage
growth happens in the data centers the
number of machines starts to expand and
the mean time to failure starts to
plummet and so being able to cope in a
cost-efficient way with this kind of
growth is something that's very
important and so in the paper if you're
very interested on your racial coding
Windows Azure storage we showed how
local reconstruction codes can do much
better and I would like to point out
that Jin Lee who is one of the leaders
of this effort is also here and
fortunately for him unlike with Alex
Aceros paper this paper was not rejected
it was accepted in the actually given
the best paper so sometimes the
scientific community gets it right so
what are the consequences of this well
the consequences are in this graph we
see graph comparing storage overhead on
the x-axis with reconstruction costs in
the case of failure on the y-axis and
the red line is the line that we've been
living on for several decades using
reed-solomon codes and now our new
technique of local reconstruction costs
is shown in the blue and just to give a
few examples of
of where the industry is in this space
we see the points on the storage or head
versus reconstruction cost trade-off for
Google and Facebook and for us in
Windows Azure we now have a tunable
system and in fact when we operate our
cloud systems we can tune according to
our own internal cost calculations and
when we deliver this technology on
premise to customers they can also
choose and so it's possible for example
to have a 12 plus four plus two local
reconstruction code that has very rapid
and low-cost reconstruction at at
storage overhead that's similar to
classic reed-solomon or if you want to
have much lower storage overhead you can
do that while maintaining a
reconstruction cost that's on par with
classic reed-solomon and so this ends up
actually for us in Windows Azure
producing savings that are in the
hundreds of millions of dollars in
operating costs every year and so again
another wonderful example of surprise
where such a well-trodden part of
computer science theory something that
theoreticians been pounding on literally
for 50 years suddenly can have new
possibilities and those new
possibilities leans new disruptions and
new mission focused opportunities in in
this case in the cloud alright and so
let me just make some concluding remarks
I talked a lot about the innovation
pipeline and this innovation pipeline is
really depicted in these four quadrants
of research for all of you is new
researchers new people in the computing
field embrace this diversity whether
your heart is in mission focused
research or in blue sky explorations or
in trying to do something surprising and
first in the disruptive quadrant or
whether you're just trying to build a
better mousetrap and just make something
better and better and keep sustaining
the world's capabilities
all of that ends up being necessary and
if you see this as a pipeline you can
see that we infect need computer science
researchers that are doing very good and
important research in all four of these
quadrants and so to conclude let me say
that computing research has blue sky
disruptive mission focused sustaining
possibilities and my wish for all of you
is that you will embrace this diversity
and help all of us as a field create a
pipeline to new innovations thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>