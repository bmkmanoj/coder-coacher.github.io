<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Security Games: Key Algorithmic Principles, Deployed Applications and Research Challenges | Coder Coacher - Coaching Coders</title><meta content="Security Games: Key Algorithmic Principles, Deployed Applications and Research Challenges - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Security Games: Key Algorithmic Principles, Deployed Applications and Research Challenges</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9x2w-Qi6ENk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi so it's with great pleasure that I
introduce Lynn tombe who's here to lead
the MSR conversational colloquium which
we encourage heavy audience
participation so please think of this as
a conversation more than a talk and feel
free to address the points that he
raises liberally throughout the talk um
Melinda is a professor at the University
of Southern California I first became
aware of his work when I was thinking
about reading about the applications of
game theory to security in especially in
airport security in LAX so Mullen has a
lot of very applied projects and using
game theory to develop security for LAX
so that he's now extended to a wide
variety of other applications and even
spun off a startup company that you can
call up to get your security needs met
oh so I guess with that I'll leave it
someone to describe their research to us
thank you thank you very much for
inviting me it's been two really great
days lots of really fantastic
conversations and I thank you all for
your very warm hospitality thank you
very much and so I'm going to talk to
you today about the work we've been
doing with security games and I'm very
happy to take questions as we go along
so this is a joint work with a lot of
people a lot of current and former
students and postdocs as well as lots of
other collaborators from around the
world and so that's our group and since
there are so many collaborators to
highlight the role of key style keep
highlighting the role of key students
and postdocs as we go along so let's
just jump right into the topic
Security's a global concern we have to
protect our ports and
reports interdict the illegal flow of
drugs weapons and money protect forests
Fish and Wildlife suppress urban crime
and in all of these cases we have
limited security resources a large
number of targets to protect and a
watchful adversary who can monitor our
defenses and exploit any patterns so the
question then becomes how do you
schedule or plan or allocate these
limited resources taking into account
this watchful adversary and we appeal to
game theory and in particular a model of
a stack of security games called the
Stackelberg security games now some of
you may be very familiar with the sacker
berg model but for the benefit of those
who may not be I'm going to use a simple
two-by-two game to try to illustrate
these concepts so here we have the US
Coast Guard trying to protect a toy port
of two targets targets one and two the
target one happens to be more important
here than target too now if as a result
the coast guards say we will always try
to protect target one then an adversary
conducting surveillance will attack
target too because the Coast Guard are
at target one so the adversary gets a
payoff of one the Coast Guard get a
negative way of minus one now the first
number is always the payoff to the
defender the second day after the
adversary now for a result the Coast
Guard were to say okay we will always
try to protect target two then an
adversary conducting surveillance will
switch as well they'll attack target one
adversary again gets a positive reward
of five the Coast Guard get a negative
reward minus five so any deterministic
strategy the adversary could be feet if
the Coast Guard word I use a mixed
strategy a randomized strategy so sixty
percent of the time they're here forty
percent of the time they are there an
adversary conducting surveillance will
only know that they are here sixty
percent there forty percent but what
they will do tomorrow remains
unpredictable the goal here is to
increase the cost and uncertainty to an
address
coming up with a plan of attack we are
not guaranteeing a hundred percent
security because there is no such thing
we are optimizing the use of our limited
security resources now these kinds of
games are called stackelberg games
because the defender moves first they're
out in the open they're patrolling for
example the adversary can observe that
and then do our best response now the
challenge here of course is that this is
a toy example with to target in the real
world when you have trillions and
trillions of possible are patrolling
strategies and very very large numbers
of target solving these games by hand
becomes very difficult and for a human
planner even if you say okay here's all
the trillion possible pass you know rays
of patrolling come up with a randomized
method it's extremely difficult and so
this is the problem that we solve and so
in the initial part of our work so we've
been working in this area of security
games it's not only use of game theory
and optimization but also planning under
uncertainty integrated into it and some
learning so the first part of our work
we solve massive scale games and
reasoning is uncertainty were combined
together in building applications that
we now call infrastructure security
games so our work is in use by the Coast
Guard in generating patrols in ports
like Boston and New York and Los Angeles
so this is generating randomized patrols
in the poor taking into account values
of different argit's or patrols around
the Staten Island Ferry in New York the
TSA use our work in assigning air
marshals to flights on randomized basis
and i'll talk more about that all of
this work started by early work that
began at the election airport which a
Nicole referred to in generating
checkpoints on roads and patrols around
the airport all of these games are
solved as
hackleburg game and I'll go into that a
little bit more throughout the talk we
take our own medicine and so at USC the
patrols are also done using the same
model yeah yeah interpretation of
Stackelberg is usually a little bit
ambiguous because sometimes it's viewed
that the outcome of the randomization is
observed by downstream player but you're
assuming you can commit but then the
probabilities of mixing were observed
but the outcome of the missing is not
answer exactly right exactly right so
that interpretation here is that you're
committing to a mixed strategy and
that's what's observed by the adversary
in long-term surveillance that's exactly
right and so it's spreading to other
places so our colleagues at University
of you know Cyrus have started to use it
for generating patrols in Argentina or
going on Chile and so forth so we are
very thrilled with all the work that's
that's going on in this area where we
are going with this is what we've been
calling green security games this is
protection of forest Fish and Wildlife
and in rivers and so on so we're working
with organisations world World Wildlife
Fund panthera WCS yes please yeah
how it works like a guard patrolling ten
percent of the time we have to-- ten
percent see significantly less safe um
or am i thinking about that in kind of
their own I'm not sure I understood
doesn't really scale with the level at
which you're waiting these different
options so if you're just like not there
ninety-nine percent of the time I'm not
one percent of safe I'm basically not
safe um so I guess I mean the there's a
one if you are there say ten percent of
the time there's a ten percent chance
that you are going to capture now what
you might be referring to I don't know
if that's the direction which you are
going is the perception of the adversary
and that's a whole that's a whole area
that we've been investigating hopefully
I'll come to that point a little bit
later on there's some very fascinating
things about what we found seemed to be
quite contrary to what prospect theory
says for example where we help
probabilities get perceived by a human
being but but I'll hopefully be able to
come to that point in the health related
to Glenn's question is the idea that if
the Coast Guard is there one percent of
the time they adversary doesn't cannot
predict which is the ten percent so they
might because they don't they only know
that it's ten percent but they don't
know which there is that yeah but
there's there's also the probability of
you know the perception and percent
probability weighting so I'll come to
that a little bit later on hopefully say
that again so I guess there's all this
work on audit games which is a whole
derivative on you know deciding the
level of punishment and things like that
but that's not that's not something
we've done you're very familiar with
that yeah we oughta games includes a
punishment parameter as part of the
defenders action space right right but
we haven't dealt with that so this group
has so no more
so what's different here when we are
looking at wildlife protection of
fishery protection and so forth or for
example work with IBM on river pollution
prevention is the fact that there is a
here we have data unfortunately there's
a lot of poaching going on and so
there's lots and lots of data on
poaching which means that you can
generate predictive models about how
poachers behave and therefore it now
becomes a reputed game where you
generate a strategy adversaries react
you can collect data and then use those
models to play the next set of
strategies so here now we start
collaborating with people from
conservation biology and conservation
criminology and other disciplines that I
didn't know existed beyond this though
there's another data set when we start
thinking about urban crime and so this
is work that we've recently started
focusing on with data from LA
unfortunately not no shortage of data on
urban crime here and so based on that
again we can do predictive models but
given that I'm expecting a
conversational seminar here this and any
work on cybersecurity and so forth I'm
not going to talk about so hopefully I
will just talk about this yes not
adversary friends here no adversary so
all of this work in 2013 led to our
startup armor way so this was started
with the competition at USC where they
after winning the competition they gave
a seed money we started with two people
and who's going to who's going to be in
the company and then against my advice
three PhD students quit their jobs and
are running the company and its it's
growing so now we have 12 people in the
company and well it's a very exciting
time I hope they've made the right
decision in these are good jobs I'm
really hopeful that this all works out
but it's a very exciting time in terms
of where this is going so USC also did a
nice magazine article about our work
pointing at all the different places
where the work was deployed and
particularly can see here work in
Argentina and Chile with our
collaborators and colleagues and work
outside with the non-governmental
organizations all of this work is
possible i should say because of the
interdisciplinary collaborations and are
embedding with security agencies and
this is really important and in the
process some of our students have had to
learn new kinds of PhD survival skills
that they didn't anticipate that they
would learn doing the PhDs so in the
rest of this presentation I'm going to
walk you through what we've done
starting from 2007 and all of the work
has been published in conferences like
the normal AI conference is triple H
guayama etc etc so if you want to look
at you know detailed experimental
results like when we talk about scale-up
does it really scale up when we talk
about handling and certainty does it
handle uncertainty all of those results
I here wouldn't have time to go into
that but I do want to talk about since
we are fortunate enough that some of
this work is deployed to understand what
impact has it had on the due to these
deployments I ask for your patience
though because this I'll do right at the
end of the talk and so please just hold
on till that time and I will I promise I
will come to that at the end so let's
start with the air travel which was our
first work that we did so this this is
our basic Stackelberg security game
model I present this because it's the
first it's also the simplest and
therefore easiest to talk about and this
work started because we were doing our
normal multi-agent research when eros
others was the assistant chief of
airport police at the time had this
question to us which is how can we
improve security at LAX the threat here
was something like this
a truck bomb in this case attempted to
be launched at the Glasgow Airport and
one of the terminals so his invited a
doc doctor at all so this was also later
President Obama's first nominee to be
the head of the TSA and so every
fortunately decided stay at USC stay in
LA not go to DC his challenge here was
something like this there's a 18 bound
roads into LAX but where and when do set
up checkpoints at terminals where and
when do you put dogs we solve the
problem as a Stackelberg game and
sometimes this schedule leads to arrests
of people carrying large numbers of
weapons into the airport but I'll do a
more thorough evaluation a little bit
later on so the way this whole thing
works is we start with the game matrix
all come to Harvey generate the payoffs
this game is then fed into a
mixed-integer program the mixed integer
program generates a mixed strategy which
is a probability distribution for
example it's saying at 8am it removes to
five and six there's a probability of
point 17 to have a k9 patrol at
terminals three five and seven
probability of 0 point 33 to have a
patrol and so forth and then we sample
from this distribution to generate an
actual schedule so he's saying they damn
team one should go to terminal 2 team 3
should go to terminal 5 team five to go
should go to terminal six and a nine
name something else should happen at
their name something else should happen
and this is an actual schedule that is
then given to the officers and they
execute that
what the teams are all moving around 59
right right right right right right
right I I mean so that's a that's a good
point about you know where the sort of
the movement times and so forth but this
is all this is all a sample schedule for
illustration purposes not the exact
thing that is matrix with entries that
came from somewhere yeah I'll come to
that in a second where I got that yeah
just on the next slide and in the next
couple of slides yeah that's a very very
important point so let's go to the mixed
integer program just to sort of give you
a overview of how this might work so we
are trying to maximize the offender
expected utility rij is the reward to
the defender if the defender took a
strategy i and the adversary took a
strategy J X I is the probability with
which a particular strategy is played so
X 1 is the probability there's a dog on
terminal 1 and a dog in terminal 2 X 2
is the probability is a dog and terminal
2 and dog on terminal 3 so we're
eliminating every possible combination
of assignment of our defender resources
to target and assigning a probability
variable to that this is okay for
small-scale problems it doesn't scale up
and i will come to how we can scale up a
little bit later on QJ is the
adversaries response and this is going
to be the adversary's best response so
we are assuming here that the adversary
gets to observe our mixed strategy and
then choose a particular terminal to
launch a truck bomb at for example so
I'm not going to be able to explain why
this is a best response but it is and
now the question is where did we get
these r IJ values from which are which
and you can't quite run this exactly you
have to do some tricks the get get it to
run faster and all that but again those
can be done so now where did you get the
r IJ value so in this particular case
the threat as explained to us was a
truck bomb launching into one of the
terminals and therefore we took as
payoffs the lives that could be lost if
such an event were to happen there's a
detailed surveillance
detailed information available about how
many people are there at different
terminals of the airport at different
times of the day and so we are using
that using that input enough payoff
matrix later on salt weapon I mean I
guess I mean there's a there's a class
of there's a class of threats that would
be captured here but you know I don't
know if there's some chemical something
threat I don't know so then that would
be a different that would be a different
threat model and that's I mean there's a
certain threat model that's captured
here where your and and they're certain
you know it doesn't capture everything
basically which is all I completely
agree with you I so I mean I think the
main point it's very important to take
away from here is that all we are trying
to do is to increase the cost and
uncertainty there is no such thing as a
hundred percent security with limited
resources that's what we are going to be
able to do and the other point I forgot
to mention earlier which I want to make
is that you might have seen earlier I
said ongoing big red letters and that
was to flag the point that this is
ongoing research and so there's going to
be questions which I hope there will be
that will be new research topics new PhD
topics here that I hope I can take back
with me as well so there's a lot of
questions that are unresolved lots of
things are standard here as you'll see
later is really having improved the
state of the art compared to what was
being done by human schedulers and so
forth before and the answer there will
be yes but could it be improved yes I
already agree
for multiple types of attackers yes if
they have different payoffs we can do
bayesian games and all that kind of
stuff and actually this in the end was a
bayesian game that's how we modeled it I
just didn't don't have time to go
through it right now I'll sort of
briefly touch upon it a little bit later
so it seems like the attackers might
value so actually many of our games are
zero sum games and I mean this is really
a you know I mean for a zero-sum game
Stackelberg model doesn't matter I mean
it the answer is the same the strategy
is the same and you're right once it's a
zero-sum if the adversary doesn't
respond optimally it's to our benefit
the reason this didn't go that way that
nonzero-sum ness comes in is that the
thought is that even if we are able to
capture the adversary the lost adversary
is not exactly our game because from
their perspective according to the
modelers here the people we were talking
to is that they still gain publicity and
that is not you know that's still a
positive outcome potentially and
therefore it was earned model as a
zero-sum you but it's quite close to
zero sum game and as a security game
there's always this property of the game
nomad Evan if it's nonzero sum that any
time you place resources we assume it's
always better for the defender and worse
for the adversary which is now been
erased to make room to the picture yes
he was using the column values CIJ not
the RO values so mathematical program
that he wrote down interpreted the
adversaries best response not as
inflicting maximum damage on the road
player but gaining maximum
yeah yeah my question is why is it
because it's a but I mean if you make it
as zeros yep so example if I understand
correctly suppose you have two terminals
one which has one person and one which
has hundred-person the new loss would be
hundred persons versus one but the
adversary might think that they gain
that even if they attack just a single
person they still gain so much publicity
so something like 50 they get like 50
one versus hundred and one so so that I
guess where i was going was if they get
captured then in some cases we would say
we've say wouldn't you know we've got
100 people that we saved something like
that but from their perspective the loss
was not 100 their perspective their loss
was may be 0 because hey they got you
know yes if therefore attack was foiled
but they got publicity you you can still
model it as a sua son but big I mean
that it gets into sort of very detailed
things if you get captured over here
does it matter does it kept you know
things like that so but suffice to say
in most of the other games in many of
the other games you'll see it's a
zero-sum game and with respect to the
payoffs later on you'll also see games
done with the US Coast Guard they're
incredibly there's a steam of
researchers has gone to all of the
different major ports for all of the
different major targets for all of the
different methods of attack that they
could possibly think of how many people
would die or would be the economic
consequences and they multiply the
number of deaths with some number to get
a total value of a loss for each attack
and that's the number that that we use
in our payoffs and so there's this lots
of lots of data that's available but
it's not exact to just there and so we
have to handle uncertainty and I'll come
to that a little bit later or gauntlet
value judgments that go into making the
entries of yes yes yes yes so for
example if the
defender is utility is interpreted to
the number of lives saved then it could
be that a defense strategy that says
stop every car whose driver has a beard
is expected utility undo that notion of
utility but okay we may have other
reasons not reflected in the payoff
matrix why we think the societal
consequences that's right that's right
that's right that's right with you know
like undermine the benefits so I'm
curious you know in the institutions
that you've interacted with to design
these games do they incorporate those
kinds of value judgments into their
reasoning when thinking about its own so
I guess there's two maybe three answers
I want to give one is that they like
these tools because precisely they move
away from human judgment at least in in
some of these respects because now you
have the program saying okay you know
this is where you're going to be said as
it's not going to be based on
potentially potentially their biases
right right I thought the program did
that's and and and I mean that's not one
of the rows of the matrix right right
and yes I mean that that's that goes to
some very very interesting questions I'm
on this AI ethics report panel where all
these I mean we are we aren't there yet
in these things I I understand the
potential warnings you're giving that if
you have you know you're doing predict I
mean in this case it is not but later on
when I talk about predictive crime that
this type of thing might happen you sort
of focus on ok that's the neighborhood
you know and that's that's a danger that
we really need to worry about in the
future but at least with respect to
these things these are valuing things at
a much higher granularity and so they
aren't really coming the program itself
is sort of saying things at a much
higher level of granularity like you
know BM this road at this time it's not
sort of saying this particular person
now this also means that there's a
certain interface
is between the program and the human
being who's working this and so this is
carefully manufactured so that there's a
certain level at which the program stops
recommending anything so it doesn't say
this car it just says you know at that
point it's not saying anything that
judgment is ultimately left to the human
so the the other thing is the fact that
this program in that sense is open you
know it's not I mean everything is
published and so forth is seen as a as a
good thing because they view it as a
value no it's like we can publicize this
it may act as a deterrent but you know
there's there's nothing secret in the
program that's that's be you know so
it's not based on sort of secret
guarding secret so anyway so after this
program was working Wow at this rate
it's a very conversational seminar he
had told me this but I realize how
conversational latest so I love all sort
of speed things up quite a bit as we
want but it doesn't matter it's fun I
will get through how whatever we get
through so in 2008 after having this
program work at LAX for about you know
year or so and other improvements Errol
had made he was called before a
congressional subcommittee hearing him
I'll just play like 50 seconds of his
testimony callahan safer today and it
was 18 months ago the team research led
by dr. Mallin tombe worked with our
department without armor and then he
goes on to explain how armor works and
then around the same time we had some
media coverage like this so you know at
the next Armas conference our colleagues
were asking us however cloaking device
research at USC was coming along
but it got us an invitation from the TSA
to go visit the TSA Freedom Center and
this is in front of this is a thing made
from parts of the airplane that crashed
into the World Trade Center the beam
from the World Trade Center rubble and
so on so at this point the standing in
front of it our armor program if they
ask us to make it work here really must
work here so and then they presented the
problem to us and the problem was
something like this how do you assign
air marshals two flights on the
international sector as thousand flights
a day we had worked on armor 100 action
game and here the game if you looked at
it was 10 to the power 41 and the reason
is you think about thousand flights a
day 20-year marshals and forget all the
food constraints and everything else the
problem becomes very very large and
essentially if you just feed the program
into our input into armor it quietly
dies running out of memory so we clearly
needed a different approach and here the
idea is incremental strategy generation
that is to not enumerate all the 10 to
the power 41 different actions so if you
write the game in the normal form what I
understand why scale up might be
difficult there's 10 to the power 41
different rows in this game matrix
assigning there's thousand flies 20 air
marshals that's thousand choose 20 might
assign air marshals to flight 123 124
135 so for this 10 to the power 41
different rosier the adversary could
attack flight one flight to up to flight
1001 the 10 to the power 41 different X
I variables one for each possible row
this program cannot run but if we could
make it run what we would find is that
many of these X I variables are zero in
fact most of them are zero in fact we
can prove as you might imagine that
there exists a solution with a very
small support set size and therefore if
we can eliminate all these X I variables
magically beforehand itself we would
have a very small game matrix which if
we solve we would get the exact same
solution as we would for the large game
matrix which means that we can now take
relative support we start with the
master program that has a small set of
pure strategies and then use a slave
that using LP duality theory gives us
the next best pure strategy to add its
column generation if it is very familiar
with mixed integer programming basically
there's a branch and price at work and
then at the end what you get is a global
optimal solution but here we have a very
small game matrix maybe with just 500
drills that achieves the same global
optimal as the massive program before
and this is what is in work at in iris
which is the program that's used to
assign air marshals on an international
sector it led to significant change in
the fams operations if you've been on an
international flight by US air carrier
whether there was an Air Marshal or not
on your flight was potentially possibly
decided by this program gauntlet sort of
like quasi additively separable across
the different like planes to which you
allocate the air marshals so it doesn't
seem like it actually requires you to is
that what's being put into this like
slave that's reducing the yeah it's it's
I mean the slave is doing some kind of
flow probably knows it's doing a max
flow problem I mean a network flow
problem is kind of assigning air
marshals two flights and then it comes
up with the next recipe I mean it's the
next best pure strategy to add I mean
there's a little bit more complicated
thing going on here because you have to
think about this app or and it's not
just one set of lights that are going
and so there's a little bit more so it's
not it's not a very very easy easily
structured problem there are
complications that arise if you try to
solve it using you know assuming that
there is no good and things like that
square separable like if you try to
solve it by say first assigning one air
marshal we delete to the best flight and
then the second one to the second best
etc so it will not it will not work it
hard work and I think part the part of
the reason there is the tours that kind
of make things complicated so anyway so
this this is just the another
congressional subcommittee hearing 2011
the Mars Society selected a University
of Southern California project it fams
on randomizing flight schedules for the
prestigious or is the world so we are
happy to hear that now let me give them
the amount of time left try to walk
through some of these things a little
bit faster so I'm going to talk about a
new way of scaling up this is the use of
marginals this work we did with the US
Coast Guard this is a system called
protect protect is an acronym this is to
tell you we spend enormous amounts of
time coming up with good acronyms
usually two hours before the deadline
and so this is generating patrols around
in different port patrols around the
Staten Island Ferry the threat here is
somebody launching an attack either on
the ferry or in one of the targets of
the port this program start in 2011
generating patrols he started with the
port of Boston 2012 the Coast Guard were
happy enough and I'll talk about some of
the metrics they used a little bit later
on so they created a small video I'll
play a few seconds of that video 50
seconds of that video the result was
invigorated boat cruise more focused
upon providing effective presence
reducing predictability and enhancing
the safety and security within our force
protect guided patrols became a source
of pride for station Boston cream and it
doesn't get much better than that
protectors also had other positive side
effects such as the development of
better tactics techniques and procedures
results in been exceptional and
following that we started working with
the staten island ferry carrying sixty
thousand passengers a day the threat is
somebody launching a suicide boat into
the face so here is the Coast Guard
patrols that run around the PHA this
these are our algorithms at
work and it's a game that solved in
continuous time I'll talk about that
next right so wait for the next ferry on
the next hour right right right so I
guess the idea here you are absolutely
right so there is this sense that you
know here the the whole thing in all of
these models is the following assumption
that you have a mixed strategy that's
going to be played and it's just going
to complicate adversaries planning with
respect to plan of attack now if you if
you do allow for this extra thing that
they're going to wait you know and allow
for that uncertainty in their plans and
they can observe and then react then
that leads to a more complicated model
there's that there's papers on that as
well that we work and I'm not presenting
them here that's not what's implemented
but but you're right so the assumption
always here is that we are saying that
the adversary would aboard that
uncertainty and therefore we want to
create these randomized patrols around
yes yes very visible security cameras or
grief or a sense that people are there
so so this this is still mapping on like
if you have one body you can put one
body in one place how do you factor is
there a way to think about their kind of
techniques about making those things
visible and not just like you can put up
cameras but for someone to believe those
cameras are effective they have to know
that when an incident happens people do
show up so then there is a using that
power that right right so it's sort of a
more of a I mean it could be even
extended to deke wise to improve
security effects and there's the I'm not
going to talk about there is some work
that's gone on into that and it's a
fascinating question lots of questions
but one thing here that I'll not get too
is a you know evaluation later on where
one of the things that weird the Coast
Guard hear a word is the fact that
operation hours were cut down but people
asking them have you purchased more
boats after the pretentious thing was
deployed and in fact you know normal
wardrobe purchase but exactly this
effective there's just more presence
seen on water off of the video you know
people might how proud they were to be
using this new system and I would've
thought actually that you know that I
might be unhappy as a work of being told
where to go on which all go to run
losing the sense of agency and and so
I'm curious like smc isn't very festive
it's all service yeah yeah what you do
find that experience you didn't really
know know so this is this is the
interface that you know we there's
always a negotiation so we start out
sometimes going too far so we will tell
I mean it start for example here in
Boston go to this exact coordinate then
go to this exact coordinate and then
they'll be like no no that's too much
you know we know our port so give us
this area then you know we can but
there's a sense of I mean it's it's it's
been this thing starting from armor that
being involved with this whole software
so forth it just seems like it it works
well for the officers and excites
excites them just as much at the excites
us and it is just sort of a very
synergistic interaction and it's been
fun that way so we haven't so there is
the bed but there is that negotiation
and this technology adoption issue we
were just talking earlier i mean it's
it's it's so fascinating I'm sort of
debating whether to discuss that or
discuss go through my talk but let me
let me just go through my talk is that's
a whole very very interesting thing to
discuss so let me just let me just push
ahead here and then well if time remains
how I'll chat more with you so I just
want to I just want to very quickly show
you the discrete space time
representation there's a continuous
I'm improvement of it in the Journal of
AI research 2013 I'll just use this so
that we can go through as quickly so
this is a you know ABC are three
locations three different time points a
ferry here can jump from C to B to a so
it's jumping at these three locations
you can have a patrol boat doing a
patrol here starting from be going to
see going to be this green patrol boat
and a ferry patrol boat protects the
ferry right next to it and so we can
solve this game as in armor and we get a
probability distribution over the
different routes the green route has a
probability of point two three brown
route probability of point four seven
and so forth we could do this but this
is very expensive because there's
exponential number of patrols each
patrol here is available there's n to
the power t variables but instead if we
now take probability flow over
individual segments as variable so we
combine the green and the brown segments
and call this one probability variable
to blue segments called this one
probability variable now we have N
squared multiplied by P variables we can
solve the problem and then sample from
this and get what we ultimately wanted
just thinking about each flight is
marginal right right right right and in
fact in this paper we say exactly
additive separable utilities so you're
exactly right but in the air marshals
the fact that you have these tours just
make things really complicated so that's
and we have a theorem that if the air
marshals don't have to return to the
United States then we can really speed
things up but of course that's not
acceptable so you know sometimes you'll
see this I feel say if it makes us stuff
nice and then this is the you this is
the u.s. Coast Guard in the country
we're working with the University
Southern
fornia to utilize game theory as a way
of optimizing and scheduling our patrol
it goes and talks about more about game
theory but for those who work on game
theory hopefully this gives you some
value to you know see in game theory in
use so i will now switch and talk a
little bit about use of trains here the
main issue is one of uncertainty so
checking tickets on cricket less
travelers on train was the next
application we looked at we said we
could apply a transition graph
representation send schedules to police
officers 915 a.m. start at this station
start checking people 925 get on the
next train and start checking people and
they have a paper schedule everything is
going smoothly my student is with the
police officer things are going fine and
924 just before boarding the Train the
officer says I have to take a bathroom
break now we can tell officer there is
no bathroom break in a Stackelberg
equilibrium we have to somehow handle
this uncertainty the more serious issue
there is the fact that they may arrest
people sometimes and that means all our
optimization is all out the window so we
need some way to handle this uncertainty
so we want the probability flow of point
30 here there is small probability of
point 05 to remain here itself this can
be done by integrating mark of decision
problems into security game framework
for this means is now each a policy in
MBP is a is a pure strategy in the game
so when we solve the game and so forth
we can load a solution on a smartphone
so they always have an action to take
even if there is interruption so but
this is only one kind of uncertainty
action uncertainty there is uncertainty
in Paris uncertainty in how much
surveillance is going on uncertainty in
the mono rationality and adversary may
bring to the table so we were we have
generated algorithms on each of these
frontiers lots of PhD thesis in this
area have been done more need to be done
more recent work has started to unify
these into one single algorithm so let
me now go on to what I think may it
seems to be of interest based on my
conversations here
to a lot of people which is protection
of forest Fish and Wildlife and so I'll
skip the part here which is talking
about sort of things generally about
protection of forests and fish and get
wildlife protection so this is much as
in Falls National Park in Uganda some of
you may have been there but if you have
an awesome place i was there in fall of
two thousand fourteen to establish our
collaboration and you can see amazing a
wildlife other stretch their stretch to
the wildlife so for example these are
snares the way the snare works is that
they buried in the ground they open the
jaws if an animal steps on it it closes
and then and poacher comes and kills the
animal these are thousands of these
snares found just in 2014 alone so this
is me with the head of security of
Uganda wildlife Authority in the park
and the way he explained they work is
they'll send out patrollers in the
forest patrollers will collect data
collection airs they bring them back to
headquarters they'll analyze going to
figure out what to do next and send
patrollers back again so this will go on
month after month and so forth so we can
model this as a repeated game so we have
here a the forest divided into a grid
each grid cell is a target cells where
there's a lot of water have high value
more animals and we are going to
generate our strategy execute it when
poachers attack targets we can get crime
data from that crime data we can
calculate our mixed strategy and
continue to improve things so we need to
learn bounded rationality models for our
poachers so I'm going to quickly run
through this part and then come to the
final part about evaluation so where as
one way to do it would be to learn from
data and we have a lot of data another
is to play a game on em Turk as what
we've done a simulation game where
people act as poachers our algorithms
act as defenders and so people can go to
different sites see what's the
probability of capture was a reward
what's the penalty and it plays a snare
and they succeed they get real money if
the place is near and they fail they
lose money and we've been playing these
games for the past several years trying
to collect data on how humans play these
games and of course if we assume people
are perfectly rational they would
calculate their expected utility capture
probability x penalty etc and everybody
would go and attack the one cell which
has the highest expected utility but in
reality attacks are spread all over and
the model that seems to capture better
for at first is this quantity response
modular sarcastic choice model which
means that if agents around so I
actually don't want to all attack at the
same place but there is no oh I see but
in this in this particular game there is
no competition so they if we both go to
the same place right in this the way we
modeled it they both win I mean if there
is no patroller there so right so they
have some but I mean the areas are very
large areas here killer I mean you know
what we're modeling are over kilometers
of areas I mean there is that effect
that's just not I guess let me answer it
in two ways one we haven't modeled that
and secondly I think the impact of that
competition even if it's there the areas
are really fairly large so I'm not
exactly sure how severe that impact
would be but we can we can discuss that
point because you will have different
drug gangs which actually sort of don't
want to encroach on each other's
territory right right right no I agree I
agree let me just push on here I mean we
should let me discuss this later on so
if we assume that the adversaries have
this model if a target has higher
expected utility that most likely to
attack lower expected duty less likely
to attack and then play games against
human adversaries these are four
different pairs of many other games y
axis is defended expected payoff lower
is worse higher is better if we assume
the adversaries are playing perfectly
rational that's blue
is the worst if they're playing green
it's a excellent optimal it's slightly
better but the best is assuming that
diverse edges are playing according to
this quantum response model we can
improve upon the quantou response model
by using what we call subjective utility
cuanto response model this is taking the
saying that the capture probability
reward and penalty are three separate
features of the game and people are sort
of looking at capture probability as
independent of the reward and penalty
and then we learn these weights from
previous data and now using the same
kind of a model we generate the
subjective expected utility and so this
turns out to be a better predictor of
how people are playing I'm going to skip
a lot of things now because I do not
want to finish on time and i'll just
show you some data with 12 years of
patrols 125 thousand observation yeah
yeah this is this one we were not not so
good at I think thi Nguyen who created
this name she should be told this that
this isn't this not good improve it or
no PhD yeah so so in this model what
what she's done is taken this data and
look at all of the different features of
each target but now we also have to be
concerned about the fact that even if
there's a crime you may not actually be
able to see it so using this we are able
to show that thermospas model so this is
protection assistant for wildlife
security pause so we've been improving
their that our paws model here for
predicting torture activity is superior
to other machine Latino machine svm
larger than the other models that we
have and so this is for dry season june
to august 2008 we have this for every
season since 2008 to 2014 and showing
that for each of those we do better job
predicting than all these other models
that were out there and so there's heat
maps so
based on this we generated patrols they
were first tried out in Uganda then
tried out in Malaysia that you can see
here portrush cam found one of the true
in one of the patrols but not all was
going smoothly they would keep saying
that you're not paying attention to
geography we were in LA there in
Malaysia they keep telling a shortest
distance between two points is not a
straight line and we are how's that
possible so we went to Malaysia
ourselves to patrol in the forest you
can see this is the beginning of the
patrol how happy we are that's me my
students a former a student bow and we
started these patrols hacking through
the forest there's a poacher scamp and
that's at the end of the patrol eight
hours thoroughly exhausted and the guide
saying there's 500 more meters to go
that way but at the end of the patrol we
understood the shortest distance between
two points is not a straight line if you
look at it on a Google map basically
what we can see here is that really
there are ridge lines and there are
riverbeds and to really walk in a way to
avoid energy consumption you have to
follow this highway map that exists
underlying in the forest you can't just
go anywhere and so we took into account
this highway map that is hidden in the
forest and that leads to a better patrol
patrolling strategy so this is more
planning rather than let's say game
theory but it's an important component
of planning so so we have the system
called pause which takes into account
species location uncertainty all that
generates patrols deployed in Malaysia I
mean try it out in Malaysia we are
working with WCS to get it used in
Uganda
so this is under evaluation we can show
we have data in one of our recent papers
that shows that compared to their patrol
routes if you look at animal science
scene and human science seen at our
patrol routes were better but this is a
small sample at this point so we are you
know compared to so we are always
comparing with the human experts and
that small data shows that but where we
are better where we will be better at is
more the randomization the fact that we
will be able to more generate couple
let's say that I don't have access to
your program yeah I want to sort of have
some rules of thumb for my you know for
my cars to sort of replicate it maybe
not perfectly you know but sort of
should do pretty well I mean have you
checked whether they're like you know
some sort of heuristics which they could
improve on what they did before but we
could get close to what you're doing
with your program because you know maybe
not everybody can do that so many takes
time to write right maybe maybe them so
simple things you can do to make things
so really excellent question but since I
have five more minutes left let me walk
through the last few of my slides and
then if you have time we will chat later
so just five more minutes I wanted to
talk to you a little bit about the
evaluation that we've done and obviously
this is trying to understand how well
have this games performed in practice
and what we are trying to show here is
have you done a better job evaluating
the use of so have you done a better job
using the limited resources compared to
how that was being done by human
schedulers before that's the test and we
can certainly do a lot of simulations
and human subjects in the lab but we've
also gone out in the field and looked at
patrol quality before and after our
software was deployed and also field
evaluation testing against adversaries
looking at capture rates of real
addresses and what we can show here is
you know indeed in all in all of these
measures we've done better than how
human scheduler used to do this before
I'm going to have time just to show you
some limit
set of results this is from the port of
Boston day one through day seven on the
y-axis is how frequently did a boat go
and visit a particular target so how
frequently did about go and visit this
blue target this is over many weeks
somebody went out and looked before
protect was deployed how frequently this
was done you can see two main points
here first there's very few patrols on
day two and secondly all these lines
crisscross meaning sometimes a target
was more important some some day the
same target became less important the
targets actually don't change their
value day after day this is after
protect was deployed the more important
targets are visited more the less
important targets are visited less on
any given day where the patrol go boat
will go remains unpredictable but the
more important targets are visited more
frequently certainly anything like a dip
on day two all goes away if you look at
it from a perspective of defender
expected utility there's a three hundred
and fifty percent improvement in
defender expected utility I wanted to
very quickly tell you about for example
for the fans there's a six-month
competition human versus machine at the
end of which there was a conclusion that
the machine had done a better job
scheduling this is not surprising if you
think about a human being looking at
thousands of flights trying to schedule
with all the different rules covering
high-risk flights and being random we've
done human versus machine competitions
in you can look at how predictable is
the deployment for example and the human
human deployment is completely
predictable you can look at the coverage
which flights were covered and human
deployment is a more focused on
particular and this is not me saying if
there's a government accountability
office report that points out these
weaknesses right so let me that's right
that's right so the one I mean there's
many other things let me focus on this
one study and then after that I'll stop
and take further work so we went out on
the trains in LA trying to check who can
catch more fare evaders this is an
experiment we can do so on the one side
was
the human the officers got this their
schedules on a handheld device one was
done using uniform random one was done
using game theory they just didn't know
which one was which they were asked if
you don't like the schedule you can go
and interrupt it and go somewhere else
to catch as many fare evaders as you
want and we did 21 days of patrol
identical conditions or we found this
that when we are given uniform random
the officers often didn't like it and
they went to some other place but when
they were given a game-theoretic
schedule they always stuck to the
schedule and at the end of this period
we looked at how many captures were
there / 30 minutes and we found that the
game theoretic one led to higher capture
rate than this you know combination of
Ray uniform random plus humans saying no
I don't like it I want to go somewhere
else to capture no no how many captures
were there / 30 minutes yes right I mean
we weren't doing like a very very large
number of officers but so there's 15
roughly 15 fare evaders captured you
know / 30 minutes basically and so this
is showing some I mean so where there is
low cost low consequences we can
certainly do these kinds of studies
we've done other studies but since it's
five o'clock i'll stop here and take any
questions you may have Redick randomness
vs I guess when when we've accidentally
tried uniform random in some other
places there's a uniform random means
it's really trying to send people
everywhere and so people will be going
to in airport setting terminal so
there's nobody there and and there's
immediate sort of feedback that this
program is do I mean it happens once
okay but if it just happens again and
again people can sense this and
similarly here you know you're being
sent to place it there's no you know
there's not many passengers there they
know that they're going to catch many
fare evaders and so this is a real test
they're really trying to catch one thing
is like a no Q F before and after that
really so convincing because it
sometimes you know the parks that are
you no no no III mean on the porch
inside we the work is relatively new
compared to the other work that's been
going on for more years and so we
haven't had an opportunity to really
thoroughly evaluate things like the way
you're suggesting and that's you know
that would be kind of ideal world
situation where we can compare areas of
the park as partly what we are trying to
do with WCS in uganda is to say okay can
we focus on an area of the park where we
can do this and then potentially compare
the results and so forth but we haven't
done that so the results that I sure I
mean it's just for example you know so
there are other things we have done
where we can compare things so this is
one comparison the other thing we did
was you know trying to have 90 officers
be scheduled on it on the train lines in
LA and so this is that again there's
like here's a game theory schedule it's
a human schedule however outsiders who
don't know who's scheduled which trying
to evaluate who did a better job
patrolling or generating patrols and on
the 12 questions here again theory
patrol according to the humans who were
judging this did a better job than what
the human schedulers had done so
consistently human beings take a lot
longer because they you know spend time
generating schedules for 90 police
officers on trains it's not easy and
they generate worst schedules and so
this is this so if you look at it from
the perspective of you have a human
being who's doing this very complicated
job could there be just out there doing
what they would like to do rather than
scheduling and the
and you know the quality of the output
that came out the program saves time and
produces a better output so I don't know
which way to go comparing the examples
is that poaching may be a case of you
have a lot of coordination that's
happening among the groups who are doing
the poaching and it's repetitive so it's
a different dynamic than fare evaders
who are mostly individual agents who
aren't trying to optimize in relation to
each other so as you're thinking about
the poaching problem I'm just seeing all
layers of things that have to anticipate
a deep amount of coordination that's
even different than the cases where
you've got a you know a bomb going in
that that might be a lot of intense
coordination but there's no sense that I
have to repeat that over time right so
just just a thought when you're thinking
about it right and I guess you know so
there's this netherlands crime institute
where our collaborators is located he's
the one and so they think a lot about
these you know methodological issues and
how you know the all these you know how
do you do careful experiment and i
completely give it its it's a you know
if you use different areas of forest
could it be you know is there some
leakage effect going on if you put
pressure on one side you know maybe
you're just pushing people on the other
side so it's not really zero so so maybe
that that kind of effect so i agree it's
a really interesting and complicated
question and lots and lots of research
issues as i mentioned you know ongoing
that there's tons and tons of research
problems and i hope i guess one of my
secret agendas is that i can get more
people to not work on ad auctions and
work on this
what because since that's been running
the longest what what kind of endow
issues so I guess there we have I mean
we have done lots of simulations and all
of those kinds of things the only data
that we haven't done a sort of a
side-by-side study the only data we have
so far is this this is not this is not a
controlled experiment so this is before
our software was deployed numbers of
arrests of drugs and firearms this is
after our software was deployed you can
see the numbers of RS going up then at
this point newspaper article start to
come out about the arrest and then the
numbers of RS start going down now
that's what we have this is not a
controlled experiment because we
couldn't you know we couldn't do this
kind of a test there because you know
you have the system whole opera you know
operating what we should you know it's
like some days you should do this
someday you should do that but it gets
really complicated with with these
checkpoints but but that's the data we
have and that's what I wanted to show
thank you thank you for inviting me
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>