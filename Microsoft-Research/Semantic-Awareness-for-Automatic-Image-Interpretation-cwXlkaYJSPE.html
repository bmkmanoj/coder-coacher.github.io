<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Semantic Awareness for Automatic Image Interpretation | Coder Coacher - Coaching Coders</title><meta content="Semantic Awareness for Automatic Image Interpretation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Semantic Awareness for Automatic Image Interpretation</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cwXlkaYJSPE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay well um good morning thanks for
being here is my pleasure to introduce
Albrect lint Linda his PhD student at
epfl under the supervision of Sabine
Seuss trunk it's actually defending next
month so it's be finishing his PhD very
soon his interest is in statistical
methods for seen understanding and seen
enhancement and the talks about the give
is based on his thesis so thank you for
introduction and thanks for being here
despite the upcoming CPR deadline I'm
glad you made some time so before stop
might talk I maybe give a short
introduction on my academic background
so I started Master somatic engineering
in ship garden Germany and then after
that there was a possibility to do
double masters program with University
in Paris where I focused on image and
signal processing and right now i'm
doing a PhD at UK facets island and
singing just said i will defend soon and
so my professor is Sabine Zeus trunk and
the industrial sponsor is OC cannon and
these are the ones that enable me to do
this work and before start of the
academic part I wanted may also make
quick note so I make I'm having a kind
of one-person business for a bit more
than 10 years now so I'm doing this a
bit in parallel and okay a bit less than
the last time but still so I'm building
props for magicians and so these things
are maybe a bit unknown domain for you
so I give a quick intro so he started
for illusion designed so the magician
comes to you and tells you what kind of
effect he wants to realize and then you
check out how you could make it possible
that the audience gets the illusion of
some some effect
then I built these devices of course
you'd use what a metal for the physical
structure but then also depending on
their a magician's need you also maybe
have to build printed circuit boards
program act rollers or do some power
electronics in order to steer different
flashes or motor other engines to make
something happen on the scene and so
this is not only a toy so we a bit
serious about that so the D gadgets that
I built we won and different
international prices and so i won't go
deeper to that of course i would like to
share all the secrets of all the
magicians but the interest of time I
will now go to the academic part so the
outline of my talk is I will pretend you
two different things the first one is
somatic image enhancement this is what's
presented last week at ACM multimedia
and the other one is automatic color
naming this will be presented next week
at cic in Los Angeles and both of them
together they work on Days use the same
statistical framework at the bottom
they're just two different applications
for the same mathematics mathematical
concepts so we started a semantic image
enhancement and the first thing i would
like to pause is a question and the
question is which image is better so do
you have an opinion which reverse ivann
the left
you won okay so and the others prefer
they want the right I guess huh okay so
the thing is now the question was a bit
unfair because I didn't give you all
information addition information could
have been who prefers you want the image
for the concept dark and which to image
we prefer for the concept of snow and in
any case if I ask you for the phone
image for the concept of dark you would
have said lower left image and for the
concepts know you would have voted for
the right image the same is true here
you have this again it's the very same
image these are just two different
versions of it and the one has been
enhanced for our sandy beach and the
other for a sunset so the point what I
want to give here is yes she looks
natural regardless of what you say right
it looks a natural because you don't
naturally see you know the appearance
does sound like that when everything's
so bright
that's you so it's processing quite
strong here in order to show the
difference between two images you can of
course just make the processing a bit
less strong and do something in the
intermediary this is just to demonstrate
you the difference between the two
concepts so the main point here is that
you can't decide which image is better
if you don't have the semetic context
but only the pixel values and this is
the reason why any kind of auto adjust
of contrasting colors that can do a
decent job but you will never get to a
position where you can actually enhance
an image for semantic concept the only
reason I dude the only way to do this is
to do manual editing so use Photoshop or
some other tool and if you give such an
image to an artist and say like make it
look like a sunset he will do something
like the one on the right for instance
but of course like many editing is good
but it will be nice to have something
automatic so the goal is to f an
automatic enhancement with semantics and
so now let's look at what are the
solutions are provided today so one
possibility is to have modes so you have
cameras that have a portray or night
nature or firework mode and depending on
the mode that you're setting your camera
will apply a specific processing to it
or you have printers they have modes
such as draft representation and then
again each mode then it voakes a
different processing that is adapted to
the contact or there are other methods
that do a classification plus an
enhancement step so there are different
publications and they try to classify
skin or sky regions or some other
classes and then for each reach in the
image of a class you apply specific
processing so if you detect in regions
you just make sure that this skin tones
are correct or not to reddish or
something like that so the problem of
all these methods is that they are
difficult to stay scale to large
vocabularies so you can have you can
imagine to have 10 to 20 modes or maybe
20 or 30 classes and then
implement a processing for each class
but what is if you want to do a thousand
or ten thousand s so this is just not
practical you cannot implement 1000
different processings for 1000 different
classes you cannot do it so you need
some other mean to to take semantic
context information and process the
image recording me so and this one point
I want to make clear is that so we are
not doing a classification task so a
classification task would be to go from
an image to the keywords however in our
case we have a keyword and we want to
apply this keyword to the image so we
going the other way around then a
classification task and what we need is
we need something that tells us a
keyword significance for an image
characteristic and we need a real
characteristic that makes sense to
change an image that's why we can't use
sift descriptors or bag of words or
these kind of descriptors these won't
work because we need something that we
can actually change an image so such as
lightness color or depth of field for
frequency distributions and so the way
we correct measure the significance for
keyword is the following so we started
for big database so we have 1 million
images plus keywords there's a database
that is public and Lee available that is
from Flickr and now for example let's
look at the keyword night and the
characteristics you just look at some
grey levels so what you see here is on
the x-axis is a histogram with different
incredible bins that go from dark to
white and on the y-axis you see how many
percent of the pixels in an image have
fallen to this pin and you see here that
for night for instance is you have on
average a bit more than twenty percent
of the pixel actually fall in the
darkest bin and this is quite more than
four images that are not annotated with
night and you see on the other hand he
on the other side here is that night
images they have
less pixels that fall in the bright bins
whereas the images that are not untied
with night they are higher so the
question is now is can we assess the
difference between these two
distributions so so each bin is a
distributions so this is a distribution
here the median and the 25 and 20 75%
percentiles and can we assess the
difference between these two
distributions for every single bill and
the thing is we don't really know the
distribution a priori the thing is we
want this thing to be very versatile so
we don't make any assumptions of how the
distribution might look like that's why
we using a nonparametric significance
tests because we don't want to assume
this is a Gaussian or whatever
distribution we just wanted to be
general so we're using a nonparametric
significance test and so there are a
couple of significance tests that do
that that compared to distributions no
matter what they're just what the actual
distribution is so free of the
well-known ones are the wilcoxon
rank-sum test the coma course nerve and
the chi-square method and i have here
free example inputs to demonstrate the
difference between these methods so the
vertex rank-sum test just measures a
difference in median so you see here
when you have two probability
distributions that just shifted your
test statistic will be this part here
the difference median however if you
just have two decisions have the same
median but they just have different
shape you will not measure any
difference so 0 and the same for the
bottom case the comic or smirnoff just
measures the maximum difference between
the two computed cumulative tribution
functions along the x-axis and that's
why you measure here also difference in
shape but you don't measure anything
here and last year the chi-square much
that because you make here the
difference in each single bin you will
measure actually also a difference
between this these two dispersion at the
bottom so the thing is in our case when
we want to do image enhancement we just
want to in or decrease an image
characteristic
so we do not care about the shape we
just want to increase or decrease so in
our case we using the wilcoxon method
because we don't want sensitivity to
shape changes but of course if you had
some other application mind you might
want to use a different test with
different properties shape distribution
very difficult contrast when you sort of
you know reduce your contrast it gets
sort of gray and Buddy if you really
push the contrast and things move out to
the edges sure but you can still do that
I control this the next slide so this is
not is the same plot you see again the
dissolution from images of night and an
attractive not with night so if you then
compute the Z value this is just a test
statistic minus the expected meeting /
the variants which gives you the Z value
is you actually see that so it is
positive here in the dark bin so we need
means they are significantly more pixels
with dark gray value and here is
negative that means night images have
significantly less pixels with these
bright grey values and now for instance
if you want to enhance the contrast you
would actually shift according to this
thing here you would say aya I need more
pixels here so you shift pixels to this
side and you need less pixels here so
you move the mod of this reaching and
then you actually have and if you do
contrast stretching you would have a
similar distribution that will tell you
you need more here more here and less
here and then you just move out of the
middle and you go to the sides and so
this knowledge example for great other
characteristics but you cannot post to
other characteristics so here you see
correct in Sealab space here's the key
word Ferrari and you see here a
three-dimensional histogram and the z
values in there with these free heat
maps and you see that the maximum here
at the crossing actually is in a red
region as expected for Ferrari and you
can do more for instance here we have C
values for red green blue and flour and
here at the top you see the you angle so
you actually see that red has
significantly more pixels in the indie
red bins and green more on the green
bins flower has not it's a bit less
significant but also has some peaks in
the red regions and
look at linear binary patterns you see
that right winning blue don't have very
significant patterns however flower has
and this just means that flowers they
have less angles that are very pointy
and they have more angles they're flat
or obtuse just kind of means flowers
have just round shapes okay so so you
see all these statistics and you see
these patterns as exists expected in the
data or you can do a special layout so
here's the keyboard light and you see
that in the center of an image so this
is just a spatial layout so we just take
an 8-bit grid that we superpose of the
image no matter what the aspect ratio of
the sizes and then you see here in the
middle you have slightly positive value
so it means the center of an image and
of light tends to be a bit lighter in
the middle but it's definitely a lot
darker on these surroundings and here's
a stippling example that shows you why
the reason for that or here we did
spatial chromel IOT the keyboard is barn
and you see here that in the center
image where you often have a bond that
is made of wood which actually is a low
chroma object you have significantly
less chroma pixels and especially the
bottom where you have like grass or
nature scenes you have a high chroma and
that's why the values are positive here
at the bottom or you have for instance
fireworks here you see so these are
gamma filter layouts that kind of tell
you how much structure is and you see
that here the but the top is a blob that
is from the fireworks in the bottom is
mainly an illuminated city delete their
behalf either
no what I tell you could but that would
be cuz that would be automatic image
classification that would be to go from
the image to the keywords okay what i'm
doing here is the other way around so
what i'm doing here is tells me if you
have an image of fireworks there
probably is a lot of structure at the
top in the middle and here at the bottom
okay you could potentially use it also
to make an automatic image orientation
but this is not the goal for this work
here and there's one more point is this
is very efficient so the this wilcoxon
rank-sum test what you need to do is you
need to sort all these values in a
sorted list and then you have to compute
the ranks of well of all the elements
that belong to the one set so i won't go
into details but the thing is you only
have to sort this list once so every
additional keyword that you want to
measure this their significance it's
just it's just a simple some so you can
do this a fraction of a second so you
can easily compute for thousands of
keywords and like in my case I just
stopped it around 3000 we can easily
compute for a large set of keywords you
can compute automatically their
significant values at almost no cost
because once the sorting is done like
they all go like one of the other very
fast okay so now I gonna stop this part
and I will go on to the application how
we used it which is the first one year
somatic image enhancement and we
implemented three different methods so
we did tone mapping we did our color
enhancement and we did a depth of field
adaptation so he for this one I have to
add that so the input images and the
method to estimate the defocus map they
come from these authors however the
algorithm and the output of the and the
output image are going to show you the
off course arms so on to show you the
principle I just focus on the color
enhancement the other two they're just
similar so the enhancement pipeline the
people posing is the following so you
have two independent inputs you have an
image and
key word and then you extract
characteristics and you have only one
hand you have some of them are no the
one part is image dependent in the other
is depending on the semantics of the
keyword and then you fuse them together
to get an output and in this case the
image looks a bit more golden than the
input so let me first go to the semantic
component so what we're doing here is
what is you take the significance values
in this case we just do it for the red
green and blue channels it's like the
gray levels that you saw before but here
just for rectly and blue channels and
you see that for the keyword gold you
have positive values he for the blue
curve that means you need more pixels
that have a low blue content and you
have high Z values here positive that
means you need more pixels that are
never right high read content and the
green is something in the middle and
basically this processing just adds a
bit more gold to your image yes based on
many images yes we took this database an
online images and then that's how you
got back this is how I get this fun
and from that now we can immediately
derive a tone mapping operator so so we
want something so here's the equation
but but what we want something is we
want something that accumulates the blue
channel in the lower flow values so we
need to for blue at or mapping function
that goes flat the beginning and then go
straight up and for right we need the
opposite we need something that pushes
the pixels away from the low red values
and accumulates them where they're red
hi red values and consequently have a
processing curve that goes up like this
sure so for any image that is that I'd
say its goal yes and I want you to
enhancer
you always apply that yes gorgeous okay
so even within go there might be some
you know differences in the way people
might
right I mean you're basically saying any
case go I just apply a single
transformation ok there's the second
component that adapts to the image but
the semantic component will always be
the same so if if some people have a
different imagination of the concept of
gold then I need a database from these
people to learn it but then I can do it
so it's just I I need data to learn what
people mean with the concept of gold and
if they don't match the taste in this
case of the flicka data sets then of
course it won't work but so here's also
if I say go and sunset yes what will it
do I gotta talk about it and so and the
only thing what we having you have like
just a single parameter it just tells
how extreme the processing is so if if s
is 0 we will just have the identity
transform and the larger s is the more
extreme the processing will be ok so we
have now the somatic on component and
now let's go to the image component
because if you just apply this tone
mapping curve to the entire image you
will affect everything also this guy and
that won't look good so we need
something at adapt to the image so for
this input image in gold we built a
weight map that tells how likely is it
that this pixel is part of the concept
of gold ok in this case what we're doing
is run a simple method we just take the
color value at the pixel and we ask how
high is your significance for this
romantic concept and if it's high you
will have a bright pixel here and if
it's low it's like a dark pixel here on
this
okay we just take these significant
values so how do you compute I mean I me
up or out of the subset of images that's
in can't go yes exactly so we have these
significant values for gold that we
computed from this 1,000,000 take images
database okay and this tells me how much
each single color is related to gold
this is this is a high Z this is a high
value in the yellowish region and like a
negative values somewhere in the red and
green regions yeah I mean well this is
not tagging so because the thing is I
know the keyboard anyway I have to if I
I have two inputs I have an image and
the keyword so I know the tag of the of
this image or understand so you say the
problem Linda is actually relevant go
exactly right for the question is have
you learned that well I learned it from
the cat database of 1 million images
that I had at the beginning yes the
thing is I have these the significance
tribution that are shortly before for
gold and barn and Ferrari and like any
any concept okay and so this is pre
computed and you can just store it and
now if you have this concept of gold you
just look out I ok what is the
significance distribution for gold you
just take it out of your database it's
pre computed and then you can just
immediately take every pixel here and
look up how significant this is for gold
and you just look look at all of up from
the distribution ok you could of course
to other methods to actually do to find
the regions that are relevant to gold
you could do like you could do a
segmentation and then do some more
sophisticated computer vision techniques
to actually find the regions and there's
a lot of work done of course but in our
case we just do that but of course
you can do something more sophisticated
than that yeah I would imagine it's so
the distribution is so varied that it's
not kind of cleared me if the thing is
if the douche mission very very few
significant values will be very low so
you could just do a trash hold and say
if I don't find a very dominant concept
I won't touch the image for God you
definitely have a high significance blob
in the yellow region that is very
dominant and if you detect that you can
say okay like in this case I really know
what to do and then you go for it okay
so now we have our an image in asthmatic
component and we going to fuse them in a
semantic processing step and what we're
doing is so we globally process the
image with the store mapping of a
function and then we do re waiting with
this rate map here so we take the input
image where the rate is zero and we take
this intermediate image where the weight
is one and in between we just interpret
linearly and that means that we just
enhance the characteristic in the
relevant regions so in this case we just
touch either bottom part and increase
the gold is here but we won't touch this
guy now i'm going to show you like a
couple of example images that we
processed here for instance here this is
the i will just dim the lights here that
you can see this a bit better so here's
a force and here this is a force know
here's an input image for dark and now
this is the output image yeah this is
silhouette they're sunset there's grass
autumn there's a strawberry so you can
see you can do any kind of keywords
you're not limited to restricted
vocabulary here it is an enhancement for
sky there's a banana makes the banana
more yellow and so this now is for depth
of field so the one before the colors so
now you see the keyboard macro indicates
that the the artists intent was to have
a depth of field effect and if you plug
the macro keyword to the our framework
you will get this as an output where you
blow out the background but the
foreground object remains in focus and
you have here the four flower also a
special frequency processing or here is
this little boy for the key word macro
also Brewers out the background so
intuitively we found this looks quite
good but of course he wanted some wanted
to measure the performance of the system
so we did some like if his experiments
and the experiment was so we showed two
images to an observer the original and
our proposed image and we also show them
the keyboard and we just ask them which
image do you prefer for this context in
this case contact is sand of course he
don't he don't even really that there's
a beach so in this case people would
probably like brother click on that
image so
and so we did 84 keyword 30 images each
different parameters for the scale
variable through the observers on total
they were almost 30 in 30,000 image
comparisons so we did an Amazon
Mechanical Turk and the result you see
here is so at the bottom you see the
scale parameter and here is the approval
rate so anything above fifty percent
means that they approved our image so
you see that there the approval rate is
about fifty percent for almost all the
keywords except light and I'm gonna come
back to that later why that is and so we
did a second experiment where we checked
for which is called them reciprocal
keywords so this is an image that you
can hand hands for two different
concepts so for instance here this image
you can enhance it for snow one for dark
and our proposed images are this one for
snow in this one for dark and the tested
other methods like histogram
equalization and photoshop Auto contrast
and of course the thing is that none of
these method actually is able to take a
semantic concept as an input so that's
why the photoshop and the histogram
equalization like you have the same
image here and in the bottom so what we
did is we we took 42 observers and we
shot them 29 of these image key word
pairs and we showed them all four images
and roll together if the key word and we
asked them which one we prefer and they
had to pick one out of the four and the
result is here the following so you see
here they see 25% line because if you
have one out of four images if you had
pure random that would be twenty-five
percent and you see here that our method
does indeed outperform the others and
the main reason is that these are
keywords reciprocal keywords and we show
them images that you can enhance for the
one or the other and we didn't find any
method that it was actually able to
enhance an image for sand or anything
else and that's why
the state has set we significantly
outperform the others because there's no
method than actually can enhance an
image for an arbitrary semantic content
so I gonna conclude this section with
limitations and future work so there are
a couple of key words that don't we have
a significant restricts and these are
mainly abstract keywords like friendship
of boredom and so at the moment we don't
really know what you would do to an
image to increase its friendship pneus
it's very difficult these are very
high-level concepts and so we don't
really know what to do with those and
the thing is also the significance
values for these keywords are rather low
so that just means we don't have any
significant characteristics for these
keywords then there is some keywords of
conflicting meanings and this is not the
reason why we underperformed for the
keyword light the thing is what our
algorithm does so if you have this an
input image and apply the keyword lie to
it you will get this an output so what
our I've learned is that light images
actually are rather dark and the reason
is that in order to have a light source
an image that makes only sense if you
have a dark surround and but this
consequently our algorithm just darkens
all the darker region of the image and
actually the light source here is more
salient it pops out a bit more of the
image however the observers on Amazon
Mechanical Turk they thought about light
I just want have a bright image so they
went for the one the left so in our
current implementation we are not able
to tell whether the keyboard light means
this more artistic interpretation of
light or just something bright and then
another problem that we haven't tackled
yet sand this is exactly your question
is multiple keywords or also
machine-generated keywords so it would
be good to have something that if you
have multiple keywords that you can
weigh them how important they are for
the image or they could also be
machine-generated keywords or either low
key words are just wrong so it would be
good too
some mechanism to detect these ones and
discard them or just remove the
influence on the processing so there was
actually a summer okie from the group
that was presented last week in ACM
where their deal with this problem of
imperfect tagging and so that could be
relevant to solve this problem and the
last thing is we talked about it before
is there's this publication from singin
here that does also a context-based
automatic image enhancement and the
context base in this case is images that
have a similar context in terms of image
features so and it would would be
interesting to see what this framework
does actually if you query for images
with similar keywords maybe you can have
the same output images or similar
effects so to summarize i present a Suzy
framework that links images
characteristic and semantics I presented
to image enhancement methods and you can
download more images and also the code
here from the website do you have more
questions for that or than otherwise I
will move on to the color naming okay
okay so this is going to be a bit
shorter because it uses the same
statistical framework at the bottom so
color naming is actually the tasks it is
a bit tedious so the before or like the
traditional way to do it is you you show
a color name to an observer and you ask
him to adjust like some color sliders to
find the right color patch or you go the
other way around you just show and a
random color page and ask what is this
color and then the user has to type in
the name so and of course this is a task
is very time-consuming and so our
approach is not just use these specific
framework so if you have for an input
keyboard green we can directly compute
from a large database can compute a
significant distribution in color space
and you will find the maximum here in
the green region of color space so
and because we can go a large scale with
this framework we actually wanted to do
it so we took a database of 950 English
color names + color values it sir the
so-called XKCD color survey and this
survey was done we've actually plug your
physical experiments people are asked to
type in the names for different color
patches and to go in more large-scale we
the ass native speakers to translate
this list of line of 50 color names 29
other languages so we did Chinese French
remand yes some European and Asian
languages and then we did their septic
analysis so we started event on google
image search and we downloaded 100
images / Connor name and then you
convert it all to them to see lab color
space assuming that they're an srgb and
then we ran the statistical test for all
color names and then we just pick the
color bin where we have the maximum
significance values and to account for
quantization arrows from the histogram
we just ended a billionaire
interpolation in a local neighborhood
around this maximum bin and I just going
to move on to results now so you see
here for the ten different languages
that we covered and 50 example color
names you see here color estimation so
you for instance e pale pink in English
was estimated to be this color different
the Chinese sensation of pale pink then
was estimated to be this color they were
English word or the Chinese equivalent
um when they were with the Chinese
before asked to take a pale pink the
disk CD did ever they were not asked to
pick up a plane he just awesome
translate these 900 color names to
Chinese so he asked one native chinese
speaker to translate all the color names
to his native language so that's just
one person one person did the
translation of the color names okay and
then we did an automatic approach to
estimate all these color names all these
color values yeah okay but the
translation there could be some bias in
the
to take your word a person since it was
just one person translating that's the
future there is some translation
problems I gonna go to that next day so
this is the accuracy so this looks quite
good but it is actually accurate and it
turns out language is one problem that
you you're dealing with so we look at
the egg as an example at the column arun
so these are our estimations from arun
you see here the bars and maybe dim line
so that you can actually see that the
colors for the different languages and
here this is the Delta Y distance to the
ground truth from the xkcd data set and
the thing is this XKCD set gives you
only the english names and the
estimation for the english color name so
that's why i like in english like the
delta e distances 10 this is reasonably
good but like it turns out that there's
like a few languages where the arrow is
quite low and then there's another group
of languages where the arrow is quite
high and the reason is that the
translator for instance for chinese he
could not find a direct relation for
Maroon so he picked something at his
clothes also for portuguese for instance
the translation is Castagna which just
means chestnut and that's why like this
is more brown and consequently there
arose a bit higher and it turns out that
all the translators of these languages
they have the color name is something
with chestnut in there so in german is
chestnut brown and this is just chestnut
so these are all colors that are
chestnut eat and that's why they have it
brownish and consequently the arrows a
bit higher but it's not really an error
because it's just it's a different word
and each key can just compare to the
english value that's why it seems high
but actually the bronze color he is
actually correct and you also put this
in perspective so what we did is event
on the internet we found different
databases that proposed themselves
estimations from a rule so / brownies
are an online color database w3c they
have a definition for HTML web pages
there's x11 from unix and maroney so the
diff
databases and we compared their values
to the xkcd value for my rune and you
see actually they are the same scale as
ours XKCD is has to be 0 because it
measures to itself so and what you see
here now is the Delta II distances not
only for Maroon but for all the color
values we have and and you see like the
like 30 seems to be like a reasonable
error this is also what different
databases can't agree on and what you
see here is all these Delta e distances
for our color names and you see it the
median here is is roughly in the region
where 30 is so we could say that half of
our color value estimations are actually
in the range of human disagreement and
the other thing is actually these
language translations are also
challenging and they add error to the
estimation because we don't have ground
through for all these languages so if
you had drawn for all these languages
actually these errors would all be a bit
lower okay so i think i can show you a
quick demo of the holidays hours so we
put this online so you see here the the
color name then srgb values heal our
values and what you can do now is you
can browse through color space so you
can say uh I like this color but just
want something that is darker so you can
just click here and you go to a darker
color or you can say I are this cook but
I would like you angle to be a bit
different and then you can click through
here and it will like walk you through
color space to go to different colors so
it's the upper part changing as you do
this or that's just for paper
this is a color picker okay I can tell
you this now so what you can do now is
you can also pick a color here from the
color wheel and then it will just give
you the closest color it can find Oh
cliff down on the bottom yeah this
doesn't update it does not I have to
implement that yes well it's the color
that the algorithm predicted and then
particularly here this is the so this is
an input color name okay light neon
green it will give you this as an output
srgb values I city lyrics from the name
yes you you give it a name and it will
automatically give you the color values
and what you can also do now is you can
translate it but not in terms of
language but in terms of color so what
you can say I a it's a nice color in
English but what actually is the closest
color that a German person would use and
then you Wilson RK this color is close
to the German color experiment which is
a quina means in german or you can also
go to chinese and then say ok this a
Chinese person would call this color
spring green and you can of course also
query so you can say you have been
different types of burgundy and so if if
you want you can we can make a
challenger name me a color name and I
will try to look it up in my database ok
ochre
there you go let's try for Rhydian this
is me there Rhydian te RI dia n RI ok
suck on it don't think I cannot occur
this collar so you won the challenge
yeah so we have jacket well what kind of
Clint is very didn't they can sit and of
green I kind of green yeah ok but it's
in it if you're an artist then certain
paints are named by the pigment just
like ochre yeah there are certain like
vermilion oh you have per million so
that's a red yeah that i have here
that's right yeah
okay so sue so the the survey was not an
artist these are just random people on
the internet and they might not know all
these special terms but I'm sure like
you neatly could edit so I will what was
it viridian everything I try to find if
it's actually on on the web so I can i
find it sunbathe just hallucinate that
well theres a river the radiant color
need be I r.i.p I am bi bi RI there it
is just misspelling ah it is really hot
bit Rhydian okay i just did so like
Giovanelli ok all right ok so you can
play with this it's it's online and if
that I come to my final conclusions so I
showed you a sissy the framework that is
easily scalable because it's there's a
very efficient way to implement this
test i showed you two applications
they're somatic image enhancement and
automatic are naming and i hope i could
convince you a bit that semantic context
actually is helpful for image processing
in general so I thank you and if you
have question answers I've taken
I should have one I was I'd only kind of
glossed over the case where you if
you're doing the automatic depth of
field so the training is not the
translation of how many more rises is is
the amount of blood exactly yeah so how
do you decide what is foreground yes
good question so the thing is let me
just go
because that seems to be a pretty strong
it is so for the color case we had a
semantic part that is just a tone
mapping and an image part that tells you
where to apply it okay so for the
scepter field you have the same you have
like a filter and the frequency domain
in the Freud domain that you estimate
also from your significant values and
for macro it just tells you you have to
reduce your high frequency content so so
it has to be yes you have the original
love it start with that that's why you
figure out that that's actually check
later and the map here the baked map is
it sir it's a so-called defocus
estimation that tells you which each
which region the image are already a bit
out of blur and then you just add it
there okay just enhancing the marlowe
exactly and so this is this is not our
own algorithm there but what I said at
the beginning we have from zoo and
simply use their depth maps to do this
task there's different methods out there
to do d focus map estimation and we just
used one of them
right so if you want to shop and
everything and still do it do the
opposite base to figure out well I guess
you haven't tried that but I guess you
can also do that yeah sharp is hard the
guy does you know the deployed content
isn't there and you get ringing experts
lucky shopper slightly sharp yes what is
here what you could do if you have
keywords that indicate structure that
you know Constance keyword fans or
architecture or grid or something you
could try to learn that finds an
architecture you have a lot of right
angles if you have a if you have a
descriptor for that you can learn that
there's a significant amount of right
angles in the image and then you could
try to do some enhancement based on that
that you say okay this image is additive
is architecture so I try to find
something like right angles and then
increase your sharpness there or
something so I guess it's possible we
didn't do it another very common you
know images like dr. Moise text so you
know anything they know that they might
be part of the text we sharpen it yes
so you could what you could try to do is
if funds if you have a scanner and you
have mixed mixed regions so you have
some region with texts and some of
images you could if you know thats a
mixed document so you what you could do
is you could try to detect is there any
text in the image and then you could
actually do two things so if you know
that there's text in the image you
sharpen these regions but you could also
try to use an OCR character recognition
to actually read the text and because it
might tell you something to the image to
this next to the text and then you can
and if it talks about a sunset or
something then you can not only shop the
text but you can also make the sunset
look good so the processing from left is
just in fashionable second you get
senior it's like all you're doing is so
you have to look up significance values
for the keyboard it's just a look up and
well then you just have a tone mapping
it's it's very very fast so this is I
think it's suitable also for embedded
system because all you need is a
database with significant values and
like maybe a for a few thousand keywords
and that's it and you just look them up
and apply to the image
you could yeah you could and then you
could say I had a nice image and I want
to enhance the beach pneus of my image
or something and then you could have a
slider that increases speech in your
image you could have that yeah sure
nervous Thank You speaker once more</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>