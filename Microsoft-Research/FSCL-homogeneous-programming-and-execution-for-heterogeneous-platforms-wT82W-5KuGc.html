<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>FSCL: homogeneous programming and execution for heterogeneous platforms | Coder Coacher - Coaching Coders</title><meta content="FSCL: homogeneous programming and execution for heterogeneous platforms - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>FSCL: homogeneous programming and execution for heterogeneous platforms</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/wT82W-5KuGc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay everyone I'd like to welcome
Gabriela coho here from the university
of pizza Gabrielle is a student of
Antonio sister Nina who is a been
long-term collaborator from this lab
with his people in this lab back I think
15 years or more since he was in intern
here and now he's sending his his his
students along as well Antonio is
interested in the sort of execution
environments for heterogeneous platforms
but he's actually interested in it
because he is working on sort of machine
learning up sort of up approaches to
machine learning which can take
advantage of this executing on this kind
of platform it is also interested in it
because he works for a start-up as well
called bio beats in California he's
working on adaptive media from on mobile
applications for the Apple and Android
platforms their image and audio
intensive applications where he he has
to sort of hand optimize them currently
for taking advantage of the GPUs on the
on the on the client side on the device
side of for those applications now he
doesn't get to use this framework for
those applications but in a sense he's
building the framework you know for him
for himself in a sense in wealth to it
to enable better development in those
kind of scenarios and also in cloud
programming scenarios so I'm a big
believer in people building tools for
people like them you know if building
tools which will make their own work
easier because there's no one who knows
that work better you know what makes
that work hard better than someone who
actually does a day to day so welcome
then we look forward to talk thanks
skipper thank you and so I'm really glad
today to to be here to talk about
homogeneous pronger mean an execution
for a true genius platforms yeah I am on
the eye-catching clothes
yeah and okay i will talk up in
particular about the FSA which is a
framework which is part of my PhD
research and Pisa stands for F sharp
open CR but I'd like to begin with a
with a problem the the or better the
situation today is that we are we have a
lot of different company resources
available on the market CPUs and GPUs
and coprocessors air like sci-fi and
almost every CPU that you buy today when
a laptop or a desktop system has two
different resources on that like the CPU
nanine integrated GPU also mobile phones
have a true genius company resources see
and an Android is 0 is a supporting
opencl today Apple did you know opencl
was originally from Apple Bay it is
supporting opencl on mobile through a
private api by now and we have a we have
also access to cloud computing and that
is becoming a true genius in itself for
example think about the amazing the
amazon services and users and
programmers with I mean software user
and programmers are generally unaware of
these heterogeneous nests inside the
platform they use and this means that
the reason over spending in the cloud
computing and inefficient competent
because we are not that people users are
not using the best device they could use
and so they have a poor performances and
if we think about mobile developed the
execution of mobile been poor in in for
example now from the energy point of
view means means a higher battery
consumption so the what is what's needed
is a simple high level homogeneous way
to code across these different devices
and and
way smart way smart runtime support to
get the most out of the of each device
that we add from time to time every time
we want to execute a particular
computation say I think that this is the
key to exploit a to eat Regina SNAs and
in particular the emoji news programming
layer means being able to port code and
possibly performances which is much more
difficult across all the devices that
that we have and from and behind the
curtain on the underlying layer there
should be a support that is not treating
each device that this is not treating
the set of device as a no mo genius at a
note but instead the support knows the
difference is between the devices to be
able to exploit each of them in a
particular case in the various
particular cases say if we talk about it
Regina's programming we have to talk
about open sia which is a specification
for it to genius parallel programming it
is widely supported by in telling the
nvidia android also it runs on oxy and
Phi it runs in mobile on mobiles and and
I think that Oakland see Alison on track
to become the dominant solution for a
for its reginus parallel programming
neverthless opencl has some drawbacks in
particular it is quite hard to eat
program with opencl it's low level it's
error-prone general in opencl you have
two things to code the colonel side
which defines what the computation has
to do on the particular device and the
austere side which is orchestrating or
coordinating the execution and this side
part is generally a boilerplate it's
really the same from program to program
but programmers have to define it you to
execute a computation actually so that
should be a way to automatize this
disco'd and also kernels have to be even
though the code is ported the Colonel's
have to be reconciled every time you
from the device to another device say
it's quite difficult to program on an
open CL and from the other point of view
from the exploitation of different
devices in a platform opencl doesn't
provide any help at all which means it
doesn't provide a way to optimize for a
particular device say if on my platform
I have one only GPU i would like to
produce a particular code which is
optimized for GPUs if i have a cpu only
i would like to optimize for cpu ended
up with two different quite different
code Colonel codes so programmers have
to expect you to write two versions of
the Colonel's depending on wire on where
they are executing the computation if
the programmer does not specify anything
and inside our platform we have multiple
devices it would be really really great
to have a support that helps the
programmer to decide where to execute
because it has been demonstrated that
some algorithms generally those
algorithms with a low complexity but
that require a huge data transfer I must
are more efficient on the CPU so it did
CPU execution is as a lower execution
completion time and some other
algorithms for example are more suitable
to execute on a GPU so how to decide
these to decide these and the programmer
have asked you to know that various
devices populating the platform and the
difference among them among each other
so this is these are the two problems
that we are trying to face with these
framework with aphasia which is on two
percent open source is on Gita and this
framework tries to address from the top
level point of view the the need of a
simple abstract homogeneous way to
program across app across different
devices and from the the shadowing point
of view from the exploitation of
different devices away an automatic way
to to understand where to execute
each computation from time to time and
optimized code for that particular
device from the programming point of
view we help programming for a kernel so
if you see our Colonels by moving from
opencl to inside a sharp allowing enable
enabling the use of higher the
high-level construct and employing a
compiler to translate a sharp to you
opencl from the other point of view
which means exploiting different devices
we still employ the features of the VTOL
execution environment like reflection
introspection and quotations and
analysis in thanks to F sharp and a
Martian learning approach to optimize
code and machine learning approach in
particular to to understand on which the
ice have to share on a particular
application wyatt is a sharp i choose a
sharp for various reasons i think the
first is the the the strongly typed
quotation metaprogramming support that
allows to you to get the asd out of
arbitrary pieces of codes so you there
is a very easy way to understand
transform analyze trailers HTS which is
the ninety percent of the framework it
provides efficient CPU execution it is
excelent for domain-specific languages
the functional programming is really
natural enough sharper but at the same
time it is possible to program an
imperative or objective oriented way and
also it is open source and class
cross-platform it runs on OS x android
linux and three said before moving to
the details about the framework i would
like to to just give a brief overview on
destructor say the structure of the
framework is mainly based on two blocks
the the compiler and the runtime the
compiler is completely independent from
the opencl client driver which is that
program that
compiler that translates opencl source
code to to the device target for example
device target of an NVIDIA GPU or a fan
intel cpu and this just because the
compiler is producing open source code
and then we rely dynamically on the on
the potentially available opencl client
driver to do the final step of the
compilation so from opencl source code
to device target code and this is made
possible by the fact that the runtime is
dynamically trying to find an open CL
client driver on the particular platform
where we run open yet so let's see what
we can do in a CL from the colonel side
programming point of view i would say
that the the compiler at at a glance is
translating the f-sharp into opencl this
means for example that a kernel which
looks like c function in open CL can be
can be written in using an f-sharp
function or instance method of that
needle or a lambda and this is the basic
feature of the earth of these of the
object model exposed by the variety
compiler block and but the compiler and
the object model and dapi to program a
fcl kernels is much more than these in
particular it i structured the framework
to provide multiple layers of
abstractions and in every time you move
from a layer 2 to the upper layer you
increase your abstraction you lose elite
a bit of flexibility so if we start from
the ground layer which is what i call
def str colonel coding or f sharper pcl
current encoding the functionality so
the way you program parallel computation
is just as like as an open CL but
instead right and see you right off
sharp so you have the the lowest
abstraction body highest flexibility
because every computation that i can
code in open sialic encode it in f-sharp
as an f sub function
but even though the abstraction is low I
still have some benefits and working on
a virtual execution environment I have a
tire type safety I have some
higher-level constructs that i can use
without constraining the expressiveness
for example two-dimensional
three-dimensional raised i can use them
while in a pissy al you can use only
single dimensional arrays I can use
record reference cell struts even though
also an open CL you can use structs and
soon you will be able to use objects but
I think that the most important thing
features here here is return types
opencl kernels must return void which
means they do not return any values and
this makes impossible to compose
different kernels using the most natural
way to compose things which means which
is functional composition but enabling
return type made makes it possible to do
this disc right it's really easy and
intuitive composition of kernels on top
of this layer which isn't as I said the
master the most flexible and expressive
we build another layer which completely
hides the opencl programming and instead
of writing curr opin co-occurrences
official functions you use collection
functions which means array map map to
reduce some also least the equivalent of
leases are supported so a pencil eyes
open CL is no longer viable risible you
do not see anything about open file you
just use these collection functions
since you're putting yourself this
abstraction level and you don't see any
opencl code the compiler is able to
generate the best code out of a remap or
array reviews so we generate two
different opencl source codes depending
on what you want to run where you want
to run your map for example CPU as a
particular open source code generated
out of the compiler and the GPU as
another one well so we have to
different levels of abstraction or use a
function collection function or you
define your custom kernels but the
interesting thing is the third step
which means composing the to merge in
the tree so that is in you can execute
an expression that I call Colonel
expression where I have a functional
composition of many things and some of
these things are custom kernels some
some others are collection functions so
the idea the approach is that as long as
you can use collection function and do
not program opencl you can do it as soon
as a step of your entire computation
power computation cannot be described
using a collection function you just
have to code to change that particular
step and switch to custom kernel coding
without having to switch the entire
computation to the lower abstraction
layer and something that is really
experimental I did it to you two days
ago is being able to put inside the
functional composition not only
collection functions and custom kernels
but also i would say regular functions
or a sequential function so functions
that there are invoked through
reflection and this enable what i call
the sub execution which means i don't
know if you if you have ever programming
khuda aur opencl but generally to solve
a problem like for example reduction on
an array or traffic some one kernel is
not enough you have to or or better one
kernel single execution of kernel is not
enough in reduction you generally do
eater ative colonel execution until you
reach the end of the problem you get the
end of the computation in perfect son
you optimize perfect some you have two
different kernels to run iteratively to
solve the problem of perfect some and
say since some kernels have to be
executed in in an iterative way you
generally are unable to compose them
with other in
in in the functional way a functional
composition way save the idea is that
you can define your function that takes
the barden of executing iteratively a
particular kernel or multiple kernels so
that function is the soldier of the
problem through the use of teeth of
multiple kernels and then you can put
your solver that now is a regular
function inside a kernel expression to
compose it with other with other
computation and this makes possible to
compose everything at the end ok so just
say yeah the naive question but could
you give us an example of what a kernel
expression is what a kernel expression
is is that is every possible combination
of a function calls say it has not to be
something so straightforward but you may
you may for example take a kernel where
the two inputs two arrays are produced
by the executioner of two other kernels
for example there was an example the
colonel yeah the colonel maybe for
example vector addition or maybe
something more complex like I would say
traffic signers bit more complex and in
an open sea I'll even those semantically
is pretty easy to understand it may be a
baby finite element for example the
solution of finite finite element
problem or a fluid dynamics can be any
problem can be a matrix multiplication
or the composition the slide before the
slide with two large rectory red
rectangles and two small ones there you
see this yeah this is vector addition
simple vector addition very it it's like
a function but is it is executed on on a
GPU or cpu depending on the particular
execution order of the other of the of
the device three multiple threads say
patent parallel
it's okay okay thank you so the the way
you you compile this if you want to
compile these and we will see in a
minute that generally do you do not
compile directly at all it's really easy
you instantiate a new compiler can pass
some options like for any compiler such
as bars only or enable some optimization
and you pass the quoted expression the
colonel expression that may continue one
single kernel call or a composition of
Colonel codes the result of this
compilation is pretty complex it doesn't
have contains only the open source code
many information that are generated such
as permitted per meter access analysis
or information on the refuse data types
to use data types inside the the
Colonel's this is all returned when you
compile it and the runtime user leverage
and disinformation 240 optimized
execution of kernels say just a brief
family on how a program is should
generally approach to see our
programming as I said generally you use
collection function they are really
straightforward in today tell you that
really easy to use you can compose them
as as as far as this is expressive
enough to realize your computation and
once we're a step of this composition
cannot be described using collection
function you have only to change it so
to define your custom kernel people
reflect the definition attribute to you
to allow the the compiler and the
runtime to Spector code the ast and then
replace your collection function with a
call to D to do your custom kernel
sometime you have some iterative kernels
to execute like in reduction or scan and
in this case what you do since you can't
compose that current this way because it
requires to be too
executives multiple times you you put it
inside a sequential function a classic
function that takes about an executor
Nate I returning the result of the
problem and so then you can you can
compose the solution of the problem with
other computations sometimes it also
happens that the sequential composition
is not expressive to compose the current
because for example one of the colonel
is taking once today no needs to take
the result from the preceding colonel
and from the first Colonel of your
computation so it's quite difficult to
pass data in this way or to model data
passing in this way so what you can do
is to split the expression into sub
expressions and run them each of them
sequentially and the semantics of the
execution is the same but the in this
case you have a pointer here in this
case my kernel and now have access of
everything that is written and that is
viable outside to all the side effects
generated by these not only here that it
had axis only or tudod to the result of
the pre of the the preceding colonel so
if function composition is not enough
you can split it so this is the way you
can program kernels but obviously you
have also to define so you should define
what a computation has to do but
obviously you should also define how
computation has to execute and this is
the runtime the FOC I runtime which
takes the burden of executive thing the
the code that is generating that is
generated from the compiler generally as
I already mentioned programmer are not
interacting with the compiler at all
they are interacting with the runtime so
they do not ask to compile a quoted
expression they asked to run a quoted
expression and
underhood the the runtime is interacting
with a compiler to get the open source
code out of the f sharp one and then I
to execute temptation a returning
returning eventually the value to the
program which means that from the
program a point of view executing
something in parallel using the f ser is
just as like as evaluating the
expression itself is really the syntax
really similar it's the same as except
for instead of a valve we have run and
also the semantics is the same the sense
that when we evaluating an expression
which mean means a executed the
expression what is inside in the quote
the quotation returned the result and
also in F SEL but the runtime is support
is different so we are running in open
sea and instead of sequentially on the
CPU and the structure of the runtime is
is layered this way stack as for the
compiler and and also in this case we
have less or more abstraction depending
on the case where you are putting
yourself while programming and in
particular the the topmost is the most
interesting one which allows to run a
computation the parallel computation
with one only meet at co which is run
and so you do not have to do all the
things that you have to do in the heart
in what i mentioned early was the
off-site coding so coordinating the
competition creating before allocating
buffers allocating initializing data
transfer synchronization with the device
all this Borden is as transparent now to
the programmer and is handled
automatically by the FCA runtime which
means that I can move from open cielos I
coding this is for a vector addition
which by the way it is one line of
kernel code is really one line and
requires 80 lines of code to set up the
computation to set up arguments a lot
Thanks while in socl you have only to
write one line of code sorry a musician
is that just yeah this is not part of
this code anyway say the what is called
computation I'm there is a generic
koerner and that is not part of this
code at all so 80 lines is T is
excluding the definition of computation
so it's fair comparison I thing right
say yeah you yeah this is not a colonel
this is only to execute a corner so the
colonel is inside another another file
95 characters ya may that's anonymous
function which takes a cetera sorry you
can make an anonymous function inside
the the protection yeah for sure yeah
it's a bit it's a bit slower from the
performance point of view because I have
to create a new dll a new module on the
fly because I have to give a name to
that thing before executing an open CR
to have to generate a name for that and
to expose it to the rest of the compiler
steps in a in a way that is that really
looks like the dynamic method or method
itself say so i have two uniform the way
the rest of the compiler step see an
anonymous me to the aura static instance
method so it's a bit it's a bit slower
but you can definitely use it okay so
even though it is so abstract this these
so high level this type of execution we
are trying to make it to keep high
performances this is particularly done
through two different strategies the
first is called buffer storage or buffer
pool and say every time I find a kernel
that is that is using a particular array
so the parameters use is a particular
the actual argument for a permit
for a parameter is particularly i create
a buffer for it but I I store it locally
say that every time I find the same
array I used by another Colonel I reuse
the same buffer without reallocating and
reallocate on it and also behind the
curtains in the runtime we are trying to
emulate all the good choices of the CUDA
or opencl programmer which means for
example that if a buffer is read-only I
do not transfer it read-only which means
it is just only read by that inside the
colonel body I do not transfer its
content back to the device to today also
and the competition ends because it has
not been modified and yeah the than for
example if the target device is a cpu i
use particular memory flags for our
locations from particular options for to
allocate buffers which are more suitable
for for a cpu device i try to implement
some strategies but sometimes we want to
break this layer of abstraction and try
to have more control on the execution of
particular computation and you can do
this using custom attributes chasm
attributes can be associated to
parameters or two entire kernels
depending on what you want what is the
metadata that you want to associate to
for example you can associate you can
use custom attributes to control the way
data are transferred forth and back the
device or to say something like use this
particular opencl memory flags this
particular opencl options for the
allocation of the buffer matching a
particular parameter or you can use for
example custom attributes marking the
entire corner to force the device used
to execute a particular device and show
this here and also you can use instead
of custom attributes you can use
functions or mon amis functions that
wraps actual arguments or or the colonel
quality to change these options
dynamically
very end if you want to have the full
control of the computation it can break
this other layer of abstraction and go
to the ground and and and use this kind
of approach where I explicitly define
what happens before exactly taking a
computation and after the execution of
the computation in terms of how to
allocate buffers and how to transfer
data and now I have the food control of
of the the colonel set up so there are
three levels of abstractions I can as
long as it's fine for me I can simply
say run but if I have if I want to have
full control I can break down to this
layer of abstraction where I have
complete control without escape in the
framework itself we are still in open in
in FSA but one big part which is the
part where on which I'm working right
now is a we said a lot of time
previously if the device type is a cpu I
use particular memory flags I said if
the device type depending on the device
type the particular collection function
can generate the compiler can generate
out of it different source code but who
defines which device should be used so
this is the part where I'm working and
in particular the runtime is integrating
scheduling and gene which is which see
whose goal is to the TAC which is the
best device where to execute each
computation that I want to run so at
very beginning I thought you to use a
different popular and quite reliable
performance models to understand which
is to predict the completion time of the
various devices i have and use this
strategy to select the device but the
problem is that we have to cope with
very a huge plenty of different models
for GPUs for APU so we have a cpu but
also an integrated GPU with
a different communication across domains
it's called communication model so
between D that the two device into the
same chipset and we should model eight
am-5 there are no there is not a plenty
of different performance model for jion
find out yet say its difficulty to find
the best the execution performance model
for each device and and on top of all
merging these different different models
some orders aim to be really really
precise so they try to the insert inside
a formula that gets you the performance
some parameters that are really
difficult to extract use micro
benchmarks such as I don't know cash
latency and if those parameters have to
be set using but by the programmer or by
the user to understand the the
performance of particular device say the
abstraction is broken because now the
user the programmer has vies ability of
what's behind so we decided to go a tree
for a different approach machine
learning-based and in particular what we
do is we use a a training set of
customizable training set of different
algorithms matrix addition matrix
multiplication virus various type of
convolutions and prefix on and manda
brought trying to be heterogeneous which
means different computations with
different things going on in their
bodies and we extract from each of them
a set of features where a feature may be
the number of memory accesses or also
the shape of memory accesses so if they
are contiguous or interleaved of which
opposite we use we try to understand how
branches are nested and how we use for
example barrier to synchronize the
threads and all these features are
extracted from the train from the
classifier to train it and then we
obviously we get also the completion
time for all the devices dynamically
available on the system so to detect for
each train set how the features relates
to a particular convenience from on one
of the devices and then for each
computation that i watch want to execute
successfully what i do i extract the
same features from the computation and I
ask the classify where I should run
eight which means one of the end classes
I have to choose fall for the the
particular computation belongs to and
this is I'm still working on it I have
some numbers if you want on the on what
and in particular doing right now but
anyway and from the language part of the
you friend from the i think the the most
interesting part here maybe is the fact
that we are we are able to do this at
runtime really efficiently which is a
problem that I that I I really felt a
lot this problem because suppose that
you have to run something really simple
and the completion time is really low
and so for example we'll put it one
millisecond if you add 10 or 20
milliseconds just to decide which device
to use you're wasting your time so the
idea is to go for a two-step or a pre
evaluation which means that I try to I
can't extract the value of a feature
until I know the the actual arguments
because some of the features I'd like
the memory accesses may depend on the
particular actual argument so but what I
can do is to simplify the abstract
syntax tree so that i get the mini moon
or the most efficient version of the
abstract syntax tree to enter and to end
up the completion of the evaluation of
this only when I i get a particular set
of our actual arguments for example this
is really straight example let's
consider that
is your kernel and you want to compete
the feature is number of memory accesses
obviously we have one memory access at
the end and then we have one memory
access inside the loop and the trip
count of this loop is imputed length
plus one so what I can do without no
indeed the value the value of the actual
argument is to lift all the things that
are not relating that are not
contributing to the feature value and
try to to understand what is going on
inside the loops and so I end up with I
transform these into an arithmetic
expression that computes the number of
memory accesses as soon as i get the
value of the actual argument and it is
really really faster to do it every time
I run a particular computation than
evaluating from scratch the entire
absurd syntax tree and so this makes
possible to adopt this heterogeneous
shadowing strategy also for very high
wave from things that are quite simple
like matrix addition like so both
filtering that I have a really low
completion time but since we are not
putting any additional over L it is fine
to try to automatically detect which is
the best device some early results
really really early yes you're something
of their code does it work if you're
using the collection functions as well
like map and so on yeah that's straight
because obviously the compiler knows
everything about the code in that in the
specific case obviously we have some
restriction we are try to to work on
them for example the branch prediction
is really used would be really useful
and efficient branch prediction to
understand when or which treads enters a
loop by now we are really doing I have
to be honest we are doing half
probability the branch of probability
else but sorry most of the time it's not
like that so we have to understand how
to apply some branch prediction and
these are some early results a problem
that I faced at the beginning and yeah I
thought about it a lot during these this
time is that it's really great program
at a high level it's really easier it's
more productive but if you go to some
specific high performance in contexts
like three dynamic or something like
that generally what is really important
is performance and it doesn't matter if
the learning curve of the language you
have to learn to be to be performance is
is high say it takes quite a lot of time
because the important thing is
performances but fortunately thanks T
the mostly to to the way buffers are
reused and data tried at ax transfer
meaningless data transfer is avoided
coding a smart coding of of a see opencl
computation has quite the same
performance of coding using FS yet just
because the same smart strategies that
is that you apply and see writing
directly in OpenGL are mostly the same
strategies that FS here that the FSIA
runtime is usually right under the hood
and this is for matrix multiplication I
have also some other results for a
vector addition and reduction in
probably perfect son and say for very
very small data at least like 64 x 64
matrixes obviously there is a set-up
time say the open D the sea version is
fast but when we do raise the data size
the two versions the completion time is
actually the same and these and the
speed the convergence speed is here is
really high because the matrix
multiplication is quite complex in terms
of complexity
and so the completion time of the Kern
is really high say the N and the
completion time is the same in the two
versions because it's a price to pay
because at the very end oficial
generates target code for matrix
multiplication so it generates opencl
source code and then target code in the
other way and the other solution you
directly translate opencl into target
code but at the end the two target codes
are the same and these on the device is
really pricing it's really have a really
great big completion time and so it's
really fast for the two solution to
converge to each other in some other
cases for example vector addition the
completion time of the Colonel's really
low so you can see easier the the gap
for for a small data but eventually you
know in both the cases there is some
point for example in vector addition I
think it's around 500 k bytes you have
you have the curry solution converge so
we have we are mostly mostly of the time
we are we have the same performance of C
solution but we're really the depakote
in time is really simpler either the
coding approach so I would like to end
up with some notes and future plan
citing future possibilities once I note
that for me it's really really important
in particular for contribution is that
both the compiler and the runtime our
thought from the beginning to be
completely extensible which means that
at runtime for example every time you
run a particular computation you can
tell the run time to compile using a
different compiler and when you
instantiate a compiler there are various
type of configurations but for example
you can use you can define your own
steps and the order of the steps say
from this point of view the official
compiler is more than enough sharp to
open C compiler is a dynamic cabal
infrastructure so you can dynamically
define your compiler from time to time
and you may use it to translate F sharp
into something completely different from
open sia also the runtime is now
composed by is now the pipeline like a
compiler of only two steps the first is
the flow graph builder in the second is
the executor of the computation and you
can replace the step were add your own
steps at the runtime but maybe the most
and this makes possible to for you to
test it with additional steps if you
want to add some other features for
example support for objects in FC a--
but the most interesting thing is that
as the fact that every time I run in
particular computation I can use a
different scheduling engine or a
different Sheldon strategy which means
that I'm a this makes it possible to use
to to change your the way you prefer
device over the others from time to time
say every time you execute a particular
computation this is part of the future
possibilities in particular I see many
paths you'll be known from from fcl one
is porting integration with rustling to
the which is the kinetic entire
infrastructure that gives that enabled
to do all these in c-sharp see in other
languages yeah I I would really like
also to take into account to the
integration with dandelion which is work
from Microsoft Silicon Valley of one
year and a half ago or two years ago so
the same time I started thinking about
FSE our shrine is really spiring it was
really inspiring anyway they tried to do
the same thing but using link so link
operators instead of coding instead of
the object model that I use so there are
some limitation but at the same point
that they can do the same thing sinan s
in c-sharp out of the box and say it
would be quite cool to to think about
integrating for example the support to
open CL they did
use CUDA to your personality to dental
iron ore for example to enable dandelion
tea to do device aware shadow link
approach to adopt the device aware
shadow lanka brush while they try to
they use device unaware which means i
said i three told the devices as as they
were the same device and i use the first
available device in a farm like way so
another interesting point of development
may be right in the back end of the
compiler and a support for the run time
to run on cloud cloud / to support cloud
platforms obviously as i said we now are
adopting in machine learning approach
where the the interesting thing what I'm
interesting to monitor and end the way i
classify different competition is based
on completion time so i want to choose
the fastest device but i may want to use
the device dust as lowest energy
consumption for example some contest it
is really important or i may use both
depending on on where the the function
the the Kerner that i want to execute is
declare for example i may say that if
the we may exposed to different API
where the where the functions that are
declared in first api or under certain
namespace are shadowed on on a
nitrogenous platform on an energy-based
with an energy based approach which
means these functions and are not really
so important that i want the lowest
completion time what i want is the
lowest energy consumption for this task
and if i stand use the functions that
are declared inside another namespace
and sign-in hire a p.i those function
may be critical say i want the lowest
completion time and so user put another
shadowing strategy and these completely
transparent to the programmer based on
the fact that i may say in the runtime
if the
if the source of a particular curve
Clara is declared inside a particular
name space or module just apply this
shadowy strategy otherwise apply another
one I would really really like to
receive feedback from you now and no
longer term so if you would like to
contribute to the development of this
project I'm it's on my shoulder right
now I'm the only developer but I would
really like to eat to contribute there
are two main get up rappers one of the
compiler so if you want to work only on
the compiler you may want to to to work
on the run time the compiler is a
submodule of the runtime say if you want
to use it just clone the runtime and and
then in eaten up dates a sub-module
there is a blog that i have to paste
actually but it's on and on FSC l and
there is also all the news about oficial
opossum and twitter and if you want to
contact me directly there is my mail and
also skype account I'm think I'm up 24
hours day because I have no time to
sleep lately and thank you very very
much yeah I would be glad to you to
receive some if you have any question I
would be really really glad to a
transfer may have one you discussed a
little bit about conservation but so one
thing I missed maybe is that if you have
these complex constructs like
composition and then maybe a complete
completion where you have to reuse the
result of something else obviously there
is this cost per you refer to of being
read only or not and then if it's not
read only yeah yeah yeah yeah you're I
can I've heard that and so this one
particular performance example
really seems to me I mean that's why
when I wanna clarify seems like a very
single kernel yeah just watching him
again so there is no potential reckon
but it is not a complex problem do you
have you measured it and tomorrow we are
doing it for for a composition obviously
there are two things to to understand
say that the first is the fact that in
Colonel composition I understand both
from the compiler and from the runtimes
perspective that this is a composition
so for example one thing that I do is
that if a runtime is a sorry koerner is
returning a buffer to be used by the the
successive Kerner generally what the the
approach is that the first Colonel
writes a buffer in the second Colonel
reads the buffer but if I looked only
under specific on once at a time I would
allocate it the first time I would
allocate the buffer right only and that
I wouldn't be able to use it in the
second Colonel but I detect these
dependency and so I allocate eating
read/write say they both they both can
use it without data copy I avoid data
transfer in this particular situations
what is you are right that there may be
a change in particular because obviously
now I have in this situation I have
multiple kernels and Africa and
introduces and over add to be scheduled
to you to say there there will be a
bigger / add probably if i if i use a
kernel expression i I'm still monitoring
how much this over ideas related so you
sure that you could have this very
simple linear conversation and then
sometimes you will need to break it if
someone would be to go to multiple
things so the standard and device in
program languages is to have variables
referring to the intermediate results
and then going to those variables will
look like you did not have the bear
but you said that just that everything
in the pipeline available g yeah you
mean that the results of execution of
pieces are assigned two variables that
are they're using following pieces or
same days so that's why yeah yeah yeah
that's right say I let's think about
this a minute I'm a show you it to you
as an example if I can there we go so uh
so the idea is that for example let's
consider my early vector addition seven
this is a bit okay say for example here
I in the example one I don't know if you
can see it but the vector dish I create
the three arrays a B and C C is written
the other two are rad say when I execute
the computation here here I see the here
sees change so i may hear do another
computation that is using C so that's
the way they the date of is flow from a
competition at to another obviously
since in in this other example instead
which is not coded for but return type I
use a leveraged owner on an introduction
or on an additional feature of fcl that
opencl doesn't have which is able to
allocate something just then lifted out
by the compiler anyway because we can't
allocate on the GPU but I can allocate
the return array here and then I write
it and I return it in this case you are
I it this should be assigned to a
variable which is the result time and
here i can use see in another particular
expression okay
if you the sun international variable in
order to change to come on expressions
on the device does it have to flow back
to the host before going back down
device both yeah that's a good question
the approach now is that if I have me
take one at least yeah if I have a
composition of different kernels there
is one which is the root the root is
obviously now is array some maybe a
custom Kern the root if the root returns
a buffer it is transferred back to the
device into an array because obviously
they ultimately access it lately before
another another colonel is executor but
that's why we use custom attributes
because I may say do not transfer back
the result of these of these of this
kernel because we know there is only
there is also the most intuitive custom
attributes which is called not used
since it is not used I do not transfer
back and say I can if I if I say in a
transfer back here here will not be
transferred back and I can therefore use
the result for example here and without
the need to transfer back to do stand
down to the device again sorry now in
that in then in the particular case of a
returning type you can do this but you
have to use a parameter which is right
only because obviously in opens here in
stl we have concept of two different
type of buffers which are called tracker
than intricate and track it are those
exactly those kind of buffer that I
can't repair in the off side obviously
the return buffer isn't racket because
it hasn't doesn't have a name but it's a
it's really particular because it is
returned to the earth in the particular
case
and so if I want to do something like
these so you right into a parameter do
not transfer the content back to the
aust and then execute another particular
computation because I split the original
expression I have to use a named
pyrometer so I can do for example in the
original these that is not returning any
value here i may say transfer mode what
cool so we may use no transfer for
example and should be like we have to
define what to do when the data is
transferred has to be transferred to the
from the austere device and we may say
okay this transfer if needed it won't be
transferred because it's three now it's
right only so it won't be transferred
anyway but I miss ace these and and
saying that when the computation ends i
dunno transfer see and so in this case
the the pet did the value is not
transferred back to the RAC and so here
i can and i can do something else
without without having this see updated
but this means that i can do some other
pieces of the of the the colonel
expression without this this over end of
transferring data yeah yeah okay yeah
yeah sure ah so it's not to mention one
thing which you actually can see here
first of all it's running on the back
which is great see if shopping news or
not only it has been used on the mac but
I should be used to do research on other
platforms secondly you can see one thing
that we didn't see cuz we didn't see
much live coding is it
do the edge Virginia's programming
enough sharp interactive either of those
or on the map which for me I think it's
the immediacy of be able to do GPU or
heterogeneous programming directly in
the rebel is a great thing and with if
you watch him all live coding with the
framework gabrielle to give a talk add
my skills matter last Thursday which was
mostly live coding yeah and that's all
recorded and Anya on there yep on rather
screencast it's available on the skills
matter site the invites that I spam
around for Thursday evenings out Laden
if you're on that list and with that
will thank everyone once more and
greatly thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>