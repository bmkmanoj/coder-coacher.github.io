<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fairness in Theorem Proving | Coder Coacher - Coaching Coders</title><meta content="Fairness in Theorem Proving - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fairness in Theorem Proving</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vMO8ZxaE6pc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
is my pleasure to introduce Maria Paula
bonacini she's a professor in the
University of Verona she's no stranger
to us she was a visiting his social here
in 2008 and this year she was a problem
Sheriff Cade and she took in New York
and she took the potential visitors this
week here thank you thank you very much
ok so thank you very much for coming the
talk is entitled unfairness since you're
improving I'm going to go through some
motivation and then distinguish two
concepts uniform fakeness for
circulation and vagueness Fox you're
improving now what is the main message
of this talk the main message of this
talk is that theorem provini search not
saturation the relevant property to
distinguish between search and
saturation is famous which has to do
with the control of the influences
rather than with the influences
themselves and the message is that
fairness really should earn us less than
saturation because saturation in a sense
is too much for improving in fairness
should consider not only expansion
inference rules namely the inference
rules which deduce consequences from
given premises but also contraction
inference rules namely those inference
rules which remove redundant formula
they shield I mean provide
now fairness is a common concept in
computing for instance when it comes to
scheduling we are used to the idea that
a schedule should be faith in the sense
of preventing any process from starving
it is also common concept in all
applications which require a search
because in a search we don't want to
neglect moves that are useful now that
the crucial point we define in fairness
is exactly to determine what is useful
now eggless comes up a lot in automated
reasoning where we have essentially two
ingredients we have a system of rules
which we usually call inference system
but we can also call more generally
transition system and we have a search
plan which is responsible for guiding
the served from deciding which
inferences to apply to which premises
and therefore guiding the search for
proof or a model now the inference rules
themselves are non deterministic they
define the search space we have the
search plan is the ingredient which adds
determinism in the sense that once we
have an input and the inference rules we
have many possible derivation once we
also add a search planet the derivation
from a given input becomes unique and
and therefore we do have a procedure at
that point so simply given a set of
inference rules doesn't really give us a
procedure we need to add the search plan
to have a deterministic procedure so a
procedural strategy is always given by
coupling these two ingredients a system
of rules and a search plan now the
typical requirements on these two is
that for the system of rules we wanted
to be complete in the sense that there
exists successful delivers what does it
mean to be successful well if we are
looking for the refutation a successful
deliberation will be one which
terminates with the empty clothes with
the contradiction if we are looking for
a model a successful deliberation will
be one to ten minutes with a model now
because the influence system determines
many possible derivation yes question
about your customer
so why understanding of inference system
work that you are inferring facts yes in
theory but you somehow overloaded the
derivation to include both facts and
models well because I mean in this talk
I'm going to talk about mostly theorem
proving in the sense of semi decision
procedures for validity for an
satisfiability and then for rotational I
validity but the concept of having a
system of rules and the search plan and
therefore the issue of making sure that
the search plan is fair applies to any
inference of transition system it
applies also to a decision procedure
which will give you a proof if the input
is unsatisfiable and the model if the
input is satisfiable so this general
concept don't need to be given only for
a refutation and theorem proving they
could be given also for model building
now the system of rules needs to make
sure that the guard elevations that are
successful the search plan needs to be
say which means it needs to ensure that
the unique derivation that the procedure
will generate from the given input will
be a successful one whenever successful
deletions exist we find one now let's
see for instance in theorem proving in
still improving the completeness
property we want to have is refutation
and complete completeness if the input
set is unsatisfiable we want the
inference system to generate the
relations that arrive at an ed empty
clothes and therefore ready proof the
search plan needs to be Fae means that
the generated elevation should reach the
contradiction and the fully complete
theorem proving strategy will be given
by arif additionally complete inference
system and this end the fake search plan
in this sense now the the topic of this
talk is fairness clearly there is a good
folks way to be Fae and it is to be
exhaustive turns there eventually all
applicable steps but that's not it
racially especially promising and in
fact the less it is amazing how much
would force exhaustive search to steal
reasoning mechanisms and and n theorem
proved x2 the question is how to be
saying without being exhaustive so we
want non-trivial definitions of fairness
non-trivial search plans the idea is
that non-trivial fairness will reduce
the gap between completeness and
efficiency because we want some how to
reconcile these two to preserve
completeness and and still have
efficiency now one could counter to
these that in practice we often work
with systems that are not complete so we
give up on completeness we have
completeness in the papers in the
theorems the meta serum about the
systems but then when we run the
experiments with the theorem provers we
often give up on completeness for
reasons of efficiency that's true but
it's true that we give up on
completeness in a controlled way by
weakening a system of which we know
upfront DVDs complete so we are still
interested in fairness to know where to
start from in order to weaken the system
and also we are interested in fairness
because we would like to have
non-trivial fake search plants that
somehow reconcile completeness and
efficiency and this is the very
long-term goal and probably we are still
far from that but I think it is
important to state it at least as a
challenge now remember what I said about
search we don't want to neglect anything
that is useful so the point is what is
useful what is needed in order to make
sure that our derivation will be
successful if in the heel of all
possible deliberations they exist
successful ones so what is useful or
even more stringent what is needed now
dually we can ask what is not needed
that is what is redundant what should
not be done because one way of doing a
focusing on what is needed is avoiding
doing what is not needed so these two
state that two concepts that are very
important in theorem proving namely
famous and
london see are related so let's take
first let's see a first notion of
redundancy assume to have a solution is
absorption to win own inference goals
then we can say that the clothes is
redundant if it can be subsumed I assume
we all know what is absorption a close
c-can subsume a close d if d is less
general which formerly means that the
existing substitution Sigma such that C
Sigma is a subset of d where the clauses
are seen as multi citizen and then we
can say that is the subsumed clause is
redundant let's see oh yes that is a C
Sigma upset of D then which one is
redundant DRC d is it because he implies
B now whenever see is true all existence
is a true therefore sisig maestro and
because this Sigma is a subset in these
disjunctions whenever sisig maestro d is
also true so it can go away the bigger
clothes another example the notion of
redundancy that we usually have when we
carpool superposition and simplification
simplification is resolution with the
quality built in we assume to have a
well-founded order in on terms and later
girls and we superpose maximal sides of
equations into maximal sides of other
equations or maximal non equation and
leader also simplification uses this
ordering to do will founded by guiding
and then one definition of redundancy is
the following that works well with these
influence rules we say that a ground
close d is redundant in a set s if the
exists ground instances c 1 CN of
closest in s such that C 1 CN and
smaller than D we can see C 1 C n is the
multiset ND is a multiset made of only
one element and use the multiset
extension of the ordering anything once
the N in
al D and then we can say that now this
is for a ground close and then we can
say that the clothes d is redundant if
all if your own distances are so this
notion of redundancy is not related to a
single inference cooler like subsumption
in the previous slide but it says that
the closest redundant if there are
smaller closest that implied essentially
yes should be that funny has them as the
conjunction of c1 2 cm implies D is that
how I should mean it yes c 1 c n let's
see c 1 CN here is a set or a
conjunction and this one is logical
entailment so d is a logical consequence
of c 1 c n so what is the difference
between today there's that there's a
word and there there's something to the
left of end there's something with the
right amount what is the difference
between some this and this oh okay sorry
this one says that she wants the end are
smaller in the ordering so these all be
digging on terms and leaders can be
extended to closes by using a multiset
extension and then it can be extended to
multi set of clauses by doing another
multi set extension so we can compare
the multi set of clauses containing C 1
CN with the multi set of clauses
containing only d and this guy
essentially will be bigger than each of
these okay so we have smaller closest
that entail
now once we have a notion of redundancy
for clauses we can have also notion of
redundancy for influences because we can
say that an inference is redundant if it
uses up generates again evident on
clothes now famous is a global property
though it does not apply to a single
inference it applies to the whole
derivation so we need to say when a
derivation is read and is saying sorry
and then a search plan is fake if all of
the derivation seeds if Jenna gates are
so we need to define the limit of a
derivation because the derivation is
you're improving is not necessarily
finite and the limit is defined as the
set of big system clauses that these
clauses that appear at some point and
never go away that means to be
persistent why do we need to define it
in this way because we have both
inference rule that generate closes and
inference rules that eliminate closes
and so as infinity the limit will be
defining the Union forge a category
called n0 of the intersection for our
ethical equal than J of the sets s I in
the derivation that means when j is
equal to 0 we take the closest that are
in the input that persists always for j
equal 1 we take those that appear at
stage one and then always remain and so
on so that is the set of clauses that
appear infinitely often no no then they
only appear once but then always remain
so always remain they never go away okay
now these are the definitions that are
most common in the literature so to say
and this is what is usually called
fairness but instead i call it uniform
fairness because the message of this
talk is that this property of fairness
is a good for saturation but it is
somewhat too strong for feel improving
so let's say that I sub e represent the
closest that can be generated from s by
expansion
and then we have a delegation so we say
that the division is Faye if for all
closest see that can be generated by
expansion from persistent closest they
exist the stage J wear the clothes are
you that appear Susie all is redundant
expansion is something like a solution
superposition all the different rule
which add something so they expand the
set of existing closest alternatively
are in the literature it has also been
formulated in a slightly different way
which is equivalent people say for all
closest see that can be generated by
expansion foramina redundant persistent
premises there existed a such that CRP
exit stage J or equivalently one can say
only on loan at Downton expansion in
frequencies have done eventually in this
is how famous is usually defined now the
question is can we have a weaker notion
of fairness and claim that uniform
thickness is for saturation in fairness
these folks you're improving now this
can be done by working with the proof
ordering rather than only with the
formula ordering the notion of proof
obliging is also a classical notion in
the field improving literature ended
appeared said the galaxy go there are
papers by Leo Backman now extra bits and
Leo back magna hoon Davidson jianzhong
gone through four innings now one could
say well what's the difference indeed we
the proof of learning can agree with the
formula organelle you know he'd used
with formula ordering if we compare
proved by the premises which is what
certainly can be done but the notion of
proof obliging is more flexible because
we can actually have small proves we can
define put forward earnings in such a
way the small proofs of larger premises
and vice versa so it doesn't have to go
exactly with the formula now if we have
a well-founded proof ordering we can
talk about proof reduction so we call
justification a set of proofs and we can
compare justifications and we can say
that a set of proofs q is better than a
set of proofs p
if for all proofs in P that existed
proof in queue which is a small legal
equal now the organ is defined in such a
way that we compare proof of the same
theorem of course that it makes sense to
compare them if we have a bit of
ordering not only we can compare sets of
proofs but we can also compare
presentations by the proof they make
available so say that s is a
presentation of the theory THS so THS is
the set of all the theorems that follow
from s then we denote by p f of s the
set of proofs with premises in s and
then we can say that given two
presentations s Prime and s that are
equivalent so they are the presentation
of the same theory of the same
deductively closed set of the urohms s s
Prime sorry s prime is simpler than as
the flow it's preferable intuitively if
the set of proofs of s prime is there
rather than the set of proofs of F of s
so this was a presentation is a set of
formulae or clauses which presents feel
it's like an axiomatization presentation
is a synonym for axiomatization you have
a bunch of formula which presented
theory in the sense that they define the
set of all the theorem is the biological
consequences of those formula but for a
given set of helium if you have more
than one presentation of course so the
presentations are equivalent and one is
better than is simpler than the other if
it has better proofs according to the
proof ordering so small and proofs
according to the proof ordering now
again we will founded proof ordering we
can look at the minimal proofs in a
given set of proofs p so let's calm U of
P the minimal proof in the set of proofs
p and then we can define the normal form
proofs and f of s how we take s we take
its deductive closure so the h of s
is the deductively closed presentation
then we take all the proofs in here and
then we take the minimal ones in these
are the normal form proofs so for any
set of proofs because the oberlin is
well founded we can look at the minimal
ones normal for proofs ones are the
minimal ones if ideally I could consider
as presentation the whole set of
theorems now of course it is not
something that I have in practice
because in practice I have a concrete
presentation I don't have and I don't
even want to have the whole set of
theorems but I I'm going to be
interested in a normal form a proof of
the onesie or am I want to prove so the
normal form proof is the minimal proof
with respect to all possible formulated
follow from the presentation and that
they could use to build that best proof
now here comes an important distinction
I said we would like to have a notion of
fairness which gives us something we
clean saturation so we need a weaker
notion than saturated set now we call
segregated a set a presentation which
provides all the normal form approves we
call complete a presentation which
provides at least one normal for proof
for every theorem now the two will
coincide if minimal proofs are unique
for instance if the proof over the
Guinness total however this is not
necessarily the case because in general
already an example of ordnance a partial
so in many cases we can distinguish
these two in the complete we only have
one normal for me proof in the saturated
we have them all let's see an example
assume we have only three equations i
equal be B equals C and a equals C and
assume that we can see there is minimal
proves the valley proofs which means the
proof that proved that s is equal to t
by a guiding s and T to a common form so
what we have here this is regarding
zeagle more steps because there is a
star over the hour before the guy
this is just composition so this means
there exists some term u such that as
regards to you and t right to you and
this happens and this happens by pure
hiding so basically we have something
like this which God goes down to a
command for now if a is greater than B &amp;amp;
B is greater than C is the origin B
equals C and a equals C is a complete
presentation for the above said we do
not need the equation a equals B because
the valley proof a goes to see by a
equals C since a is greater than C and B
goes to C by B equals C since b is
greater than C gives me a minimal proof
of a equal P on the other hand is
saturated set has them all because it
wants to have all the normal form proofs
and therefore it wants to have it keeps
also the equation a equal B because it
wants to have also the proof is a goes
to B in addition to this proof this is
also a valid proof of course because
this can be 0 ok of course the notion
depends on the ordering because if we
take the same set a equal B be equal CA
equal C in the same notion of valley
process minimal proofs but a and B are
not comparable so this symbol means that
neither a is greater than B Nobi is
greater than a nerve a and B are equal
so they are not comparable now so
together then complete coincide why
because also the saturated presentation
does not want to have a equal be any
more since with a and B and comparable
the proof of a equal B by the equation a
equals this with no orientation between
A&amp;amp;B since they are incomparable and this
proof is not minimal with respect to
that definition ok now and based on the
notion of proof orderings a notion of
condone a CD was
developed we say that a presentation is
contracted if it contains all and only
the premises of its minimal proofs and
then we say that the presentation is
canonical and we represented with these
shafts symbol up here so as sharp is the
canonical presentation for the theory of
s if it contains only only the premises
of normal former proofs now conducted
means contain only only the premises of
minimal proofs canonical says normal
phone proofs so if you think about it it
means that canonical is saturated and
contracted because the separated is the
one where because they are all the
minimal all the normal form improves
minimal and normal form essentially
coincide and then one can prove that one
can also collect the guy is a canonical
presentation is this more or less
saturated presentation with respect to
subset ordering and the simplest
presentation with respect to the augury
no presentations based on the old arena
profs now all these applies to
equational theories in the standard way
that people may know from timely guiding
a normal for the proof of the theorem
for all X 4 X's s equal T is a valid
proof connecting this colonized forms of
s and T as with heart is as with all the
valuables replaced by column constants
and the same 40 now in this context
saturated means convergent the
traditional notion of rewriting systems
that are confluent in terminating
confluent means uniqueness of the normal
form contracted we mean Intel reduced so
all the equations in the presentation of
it used by guiding one we'd expect to
the other in canonical will be
convergent and introduce them and
economical system if it is finite give
the decision procedure but we all know
that this is a rare event because
unfortunately finite canonical
presentations are difficult to find
however they do exist for some fortunate
theory but now let's go to see how
redundancy changes if we use proof or
beings rather than four
overhang as before we can say that the
clause is redundant if I did it to the
set does not improve any of the minimal
proofs so the set of minimal proofs that
we have in the presentation s is the
same as the set of minimal proof that we
have in the presentation we see ad that
you can see is going down that it
doesn't help or dually we can say C is
redundant if removing it does not work
some proofs so if we have s and we
remove see from s s minus C is simpler
is more likely simply because it's
proofs are better because C is
irrelevant for the minimal proofs for
the good proofs now working with proof
all the links we are going to view
inferences as a sort of proof reduction
both expansion inferences and
contraction inferences with the same as
proof reduction and we say that it that
influence is good if it even invest SI
plus 1 from SI and si plus 1 is simpler
than SI so it has better proof better or
equal proves nothing gets worse and the
whole division is good if this happens
at all the steps for all eyes now an
important property of good relations
since you're improving can be
synthesized by saying once redundant
always redundant so if something becomes
redundant at some point it will be
redundant forget work so we connect we
can commit to throw it away and it will
never need to come back this is why
these different systems although the
ventric to remove data they can do so
permanently in the four they are also
what is called proof confluent in the
sense that they don't need backtracking
to go back and possibly put back
something that was removed before so
formally if we take the intersection of
what is in si plus 1 in israel and was
redundant in si it is also redundant in
SI plus 1 so if it was redundant before
and it is still there it is redundant
also in SI plus 1 so essentially once we
throw it away this forever now
I already talked about expansion and
contraction and here comes the formal
definition expansion means adding
logical consequences so from a we infer
a union B will be our logical
consequences of a contraction means that
we remove something we remove be from a
union B we go to having only a but we
want a to be simpler than a union B
which means they are equivalent so
whatever they throw away is still a
logical consequence of what I keep in
other than what I keep has better or
equal proofs and both expansions and
contractions are good I mean expansion
is still really good because if I only
add things the proofs are not going to
worsen because I have everything I had
before the crucial thing is that
contraction doesn't make the proofs Wars
now I can then qualify the revisions
based on the properties they are sure to
the limit so I'm going to say that the
derivation is saturating if you generate
situated limit I'm going to say it is
complete if it generates a complete
limit contracting if it generates a
contracted limit and finally canonical
if it generates if it is saturating and
contracting so it generates a saturated
and contracted limit which is a
canonical now-famous at last how can we
define fairness based on before things
so what is the intuition we would like
that when a stage of a derivation say as
I they exist a minimal proof of the
target theorem that is the specific
theorem we would like to prove not all
the possible theorems in the theory
which is reducible by our influences by
and by an expansion or contraction
incidence then it will be get used
eventually now how can we guide this
formally we can say for all are you
going to go equal to zero in for all
proofs p of the target theorem that we
are interested in that is minimal at
stages I if there exist inferences from
SI take infinitely many steps to some s
Prime and the
the proof queue in the minimal proofs in
s prime which is a student visa more
than P then we would like to have a
stage J in our derivation greater than I
such that they exist the proof are in
the set of minimal proofs provided by
these SJ such that our is Molly girl
equaling the proof ordering them this Q
that was battling p this notion applies
to both expansion and contraction so
while the standard the classical notion
of fame is focus only on expansion this
one focuses treat expansion and
contraction both as host class citizens
because it emphasizes that contraction
is not only deletion but it is also
generation of reduced forms of clothes
is like in simplification now when it
comes now how can we instantiate this
intuition because I mean this one is
nice but basically it says it is in the
guardroom more than how to achieve it we
would like that when a reduction is
possible somehow we will get it so how
can we point out exactly how to achieve
this we need a notion of critical proof
because in the realm of all the possible
put so if we don't want to do saturation
which means reducing everything
essentially we need to focus on
something so we need a notion of
critical proofs so eClinical proof is a
minimal proof which is not in normal
forme but such that all its perfect sub
proofs are in normal form and the
classical example in equational theories
is a peek that is a proof down like that
when we have something and the
possibility of going down from u2 s or
2t this is what typically suit
opposition step does a superposition
step will detect these and will generate
these and then possibly oh he ain't it
into no into
sorry one way or the other so um then we
call CS the self critical proofs of s
and as usual bein going to look at the
critical proofs with the persistent
premises so we look at those and we say
that the derivation is say if eventually
it manages to reduce all the critical
proofs so for all the critical proofs
from persistent premises they exist some
proof in the sets that I'm going
generating in the derivation that is
strictly smaller now how about uniform
thickness how we recorded it in this
blue folder in based framework now
uniform thickness will be a struggle
property so we call trivial proof a
proof made of the serum itself so we put
a hat on s to denote the set of all the
trivial proofs of formula in s then the
trivial proofs with persistent premises
are is s infinity with the heart and
then uniform failures will say that all
these trivial proofs that are in the
existing set except those that needs to
be in the canonical system will be get
used eventually so it's a stronger
property then fairness and we show it
with these theorems so we show that if
the derivation is faye then it is
completing which means the limit is
complete the derivation is uniformly
fake if and only if it is salivating so
it did give saturation and if there is
saturation in the end it must be
uniformly fake that's an if and only if
and famous is sufficient folk theorem
proving for proof search rather than
saturation essentially it is not
necessary to add all consequences of
critical proofs but just enough to
provide a smaller crew for for each
critical proof now this is a nicely
theory but in practice it is a still a
challenge because we
only have a set of clauses in practice
the proofs are not there the proof only
exists implicitly it's not that we have
explicitly a proof and we know how to
reduce so the question is which
properties the search plan should have
to try to approximate in in practice
this desirable sameness property
although I claim that already having at
least in theory a notion of faintness
that earth nas's something wicked and
segregation is a good thing because at
least it makes it possible to work to
work the same search plan that are not
uniform if a anyway we want to search
plan to schedule enough expansion and
contraction to be saying and therefore
completing it should schedule enough
contraction to be contracting and we
also wanted in practice to schedule
contraption before expansion so we talk
about eager contraction how does this
happen in in existing theorem provers we
need to distinguish between two forms of
contraction for work interaction and
backward contraction focal contraction
is when we have generated by expansion
for instance by resolution of
superposition in new clothes see and we
contracted with respect to previously
existing already existing closest and we
get a reduced form C Prime back wall
contraction in is when we use C prime to
contract to possibly reduce simplify
subsume the already existing closes now
a first basic idea in the implementation
of theorem proved x is to implement
backward contraction by football
contraction usually one uses an indexing
scheme to detect that the clause is
reducible and then once that is being
detected the reducible clause is just
deleted like if it were a new clothes is
subject to forward contractions so we
really need only one procedure for the
two and then what does it mean to have a
gift contraction ideally we would like
that at every stage I there is no
redundancy in si but that's not possible
if every step represents a single
inference so we can get in practice is
that periodically the set of redundant
closes at stage I is aimed
it for instance the existing theorem
provers implement the given closed loop
which means the executed cycle will buy
a little iteration they select the
clothes and they pick for all possible
steps between that clause and the clause
is that we're already selected before
now these two sets are usually called
active and passive active are the
closest the web already selected is
given closest and then therefore have
used as premises for expansion
inferences that's why they're called
active vs passive contains all the other
clauses that has been already generated
but if not being selected yet for
inferences now a given closed loop may
do enough contraction to keep the union
of active and passive introduced so
that's the most aggressive contracting
strategy a more a more prudent one one
that decides to invest less in
contraction is to keep only the active
set into reduced because this we don't
want to use redundant clauses for
expansion inferences it is enough to
keep into hideous the set out of which
we are going to get the premises for
expansion in most theorem provers
implement both these two search plans
and sometimes one is better and
sometimes the other is is better now
let's see an example of the kind of
aggressive contraction that we can get
with this kind of framework for instance
working with conditional equations now
to go beyond the equation and stay in
state the equation and theories and see
also conditional equation and theories
of how an equation or theories assume
that we have a equal B implies F a
equals C and a equal B implies F B
equals C now assume that we have been
ordering where as the original symbols
is f is greater than a is greater than B
b is greater than C then I equal B
implies if a equals C reduces to a equal
B implies C equals C and because C
equals C is trivial this can be deleted
so how does this reduction
happen because I equal B is the same
condition so we can the idea is that we
use the condition itself to do the
guiding so a this a good uses to be and
then because FB is equal to c FB uses to
see so the entire first Clause can go
away so the idea is that we can use also
the conditions as the right goals for
another example saying we have a greater
than be a greater than C this set a
equal B implies B equals C AE equal B
implies a equal C is saturated the set
with only a equal B implies B equals C
is equivalent it is the same theory it
is complete but not saturated you see
that it is a smaller it doesn't have a
equal B implies a equal C and it is
reduced why why can't we get rid of a
equal B implies equal equal C because
this conditional equation can save
videos can reduce itself to a equal B
implies B equals C which is subsumed by
the other copy of a equal B implies B
equals C which we already have because
again we allow us to the condition to be
used for guidance so that a this a goes
to B alternatively we can reduce we can
reduce a equal B implies a equal C to a
equal C implies a equal see why because
under a equal B we can what is it we can
reduce the the B to the see this time
and so so what is it
no oh yes we are reducing the be in the
condition to see by using this one I
equal B implies B equals C so this B
goes to C and this is also to be deleted
because it's Trivium anyway to summarize
the idea is that faecalis should yield
something weaker than saturation this
can be achieved in the theoretical
framework of definitions by adopting
proof over leggings as opposed to
formula or leggings and this opens the
possibility to study not really saying
any good conduct research plans and to
prove them fail and this is in my
opinion valuable to reduce the gap
between the theory about theorem proving
which always ask us to get saturation
and even theorem proving is presented as
the saturation based strategies and the
search plan Center the techniques that
we have in productive in practice which
usually allow more contraction and the
claim is that that can be done not at
the expense of completeness by
preserving this weekend notion of
fairness and this is all I have thank
you very much these are some papers with
this kind of work was developed so if
there are any questions I'd be happy to
take them
well I mean I'm I realized that it is
somewhat abstract and theoretical topics
it's been it's been with me a long time
actually because you see 2013 2007 1995
this actually was a tape and publishing
results in my PhD thesis because I I
have this notion that we should have we
should do less than saturation in fact
since then and it is something that I
worked on over time more than once
because they really think that the very
same notion of saturation basis you're
improving the name itself is somewhat
misleading I mean why should we want to
do saturation when in reality we are
searching for one specific product for
at least one proof of one specific
theorem so saturation is too much and we
should focus more on on weaker
requirements that may also bring the
theory in the in the practice of the
improving clothes and by giving more
importance to contraction and them and
in fay but not uniform if a search plans
please I was wondering if we
I was wondering how the different can
you compare the type of furnace that you
discuss here with the some of those
standard definition of fairness for
skipping need a conquering comprehend
music video setting and so there's think
like we frame this from white fairness
etc if you see for instance if every
family of techniques like it's a
expansion versus attraction for instance
and you see a different like coconut
process of each monster open I don't
know if this is really I mean if there's
a way to have a map thing or it just
completely different basically ah and
the guarantees that you get I guess also
are kind of different here but I'm not
sure
now that that's a very good point thank
you i don't think that this year
improving community to kiss pilation
from the notions of fairness in other
fields of computing and that's probably
something that we should look into but
because the fakeness was enshrined
pretty early as something that should
produce the saturation effect somehow
that was sealed that way and the very
same theorem proving methods based on
superposition and resolution was called
saturation base and they was it and so
famous was no longer the problem and
somehow has been contained in for all
these years that fakeness is something
to be discussed instead that we should
weaken that notion and maybe precisely
looking at how fakeness is defined in
other fields we might get inspiration
for non-trivial practical face search
plans so that that's thank you that's a
nice lead oh maybe also i mean it's
upset i mean i do not have expert
fairness either for conferences the
better they are only mean there may be a
few stand notion of fairness i don't
know how many they are but then there is
a more general kind of notion of
fairness for algorithmic I mean progress
okay and so and that's the concrete
setting doesn't perhaps address this it
perhaps closer to that a more general in
a way i but i don't know if this
actually this kind of meta theory of
fairness before
but what if I may I mean considered also
how we don't really discuss much famous
for decision procedures I mean this
whole theoretical framework about
saturation was done for first dogleg
field improving we're all we have is a
semi decision procedure and things you
know in general its infinite so we have
these search process and we view it as a
saturation is accumulation process but
also in decision procedures there is a
search because even if the search space
is finite we still don't want to be
exhaustive because it's huge so the
question of starting more fairness also
for decision procedures the order with
which we do things in my opinion should
be relevant and somehow this is a topic
that is not gotten much attention
because much of the effort went into
designing the grief additionally
complete inference system and then the
decision procedures that are completing
the sense of being able to generate the
proof on the one hand or the model of
the other but I think that also there we
could discuss what does it mean to be
failing practice and how in how we can
be best to it that he is considered
propositional logic hmm and the
satisfiability question or it
so what I don't understand is that how
can fairness are paying attention to
fairness it cannot help us in the worst
case right in the worst case you have to
be a glass to act so what is what is
that what is the point of it is it that
it gives you some heuristics that tipped
up if the thing is that in some cases
maybe hopefully related to practical
problems that arise you terminate faster
I would say two things on the one hand
it can inspire you he sticks on the
other hand we cannot improve okay for a
decision procedure the search space is
finite we have an algorithm and thick
fog we also have the classical notion of
complexity we know that is exponential
so we cannot do better than that in the
worst case however we can investigate
other notion of complexity because one
can investigate notions of proof
complexity one can investigate notion of
search complexity so although the search
thing remains explanation I can still
compared to procedures by which part of
the subspace they visit for instance so
we could develop measures say all the
Rings the capture how large is the
search space visited by one and how
large is the surface visited by the
other and establish a comparative result
which doesn't change the fact that in
the worst case it's exponential but it
can say that for instance for a certain
class of inputs the refugee a generates
a small excerpt taste and strategy B so
this is also something that might be
useful to investigate if we see fairness
as something to be discussed also for
her decision procedures
any more questions
thank you very much for coming for
listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>