<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Deep Learning - Sergey Ioffe | Coder Coacher - Coaching Coders</title><meta content="Symposium: Deep Learning - Sergey Ioffe - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Deep Learning - Sergey Ioffe</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/z5fxGani_wA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
um so quickly introduced Sergei I'm
actually curious how many of you in this
room are already using batch
normalization in your work raise your
hands all right cool
so those of you that aren't yet you know
this is one of those algorithms that was
published earlier this year
feels like has really taken the deep
learning community by storm and is
helping so many of us speed up our deep
learning algorithms so if you haven't
yet seen this work listen to Sergei is
talking and I hope you go home and try
out these ideas you ready so again thank
you all right so this is gonna be tricky
because I don't actually see the
presentation on my screen so let's see
okay yeah so my name is Sergei and this
has joined to work done with Christian
Sogeti at Google research and we're
going to show how to train deep networks
faster and better so just for the set up
we are addressing the problem of loss
minimization over the training data and
we are looking at the stochastic
stochastic gradient optimization so what
we do is at each at each iteration we
take a random mini batch of M examples
we compute the gradient of the loss
computed over those examples and take
the step in the opposite direction so
that's the standard SGD but the same
thing applies to all the other
stochastic gradient optimization methods
right and so in in this talk I will
start out by outlining the phenomenon
that we refer to as internal covariate
shift which basically refers to the fact
that as the model trains the
distributions of the activations at
various layers of the model changes well
and that makes the training slower
I will then describe a very simple
method which we'll call vegetable
ization which tries to counteract this
by normal I think that activations using
the mini batch statistics and then I'll
show some results in particular show
that we can train the models state of
that models for image recognition more
than ten times faster
all right and so just as a motivation
let's consider a sigmoid unit and the
network and you know usually when you
have deep networks sigmoid or units are
not used because they're very hard to
train and so in this case the sigmoid
unit is fed the input which is an affine
transformation of some input X and the
problem happens when when the input to
the sigmoid falls into one of the
situated regions which is basically most
of their input range for the for the
segment and so when one is in a
saturated region
the gradient optimization becomes very
slow because the gradients are very
small and so what we really want is we
want the sigmoid to be hits right around
in the middle in the in the more
unsaturated region so so we can take
more meaningful steps but what this
means is that the output of the previous
layer which in this case is the X plus B
has to has to has to has to be kept in a
fairly narrow range and so this becomes
especially difficult when the when the X
itself is obtained but as an output of
of some sub network with potentially
quite a few parameters each of which can
can make the sigmoid input fall out of
the unsaturated regime and and so this
is this is something that people people
do address so one one way to address
this is by very careful initialization
another important aspect of that is
keeping learning it's small which
basically prevents large jumps
throughout the training and then finally
what people really do is they just don't
use sigmoids at all for deep networks
and instead they use non saturating or
so less saturating nonlinearities such
as values so let me just digress a
little bit to the concept of covariant
shift which is basically a phenomenon
where the training and test
distributions are different and this is
a very well-known phenomenon and this is
something that reduces the
generalization performance of the model
so training on on one distribution of
data
testing it on another is Pauline I'm
going to be doing quite as well and
commonly what is done is the main
adaptation which is basically
fine-tuning of the model on the on the
data that comes from the correct
distribution and so it turns out that
the same phenomenon actually applies to
sub models also so in particular let's
consider a composition of two
transformations the inner one of one and
the outer one F two each of them has its
own set of parameters and the output of
the inner transformation serves as the
input to the outer transformation and so
what this means is that as the motor
trains and the parameters of the inner
Transformations change the distribution
of the inputs to the outer
transformation will also be changing and
basically what this means is that the
other transformation will continuously
have to adjust for this change basically
it will it will have to perform
continuous domain adaptation and this is
something that that we argue will slow
down the training and what we would like
is for various activations in the
network to have probabilities that
remain fixed throughout the training but
how we're going to do this and you know
it's obviously hard to accomplish so
we're going to take a very simple
approximation which is that each each
scalar activation so in this case we
denote it by X will be normalized and
the normalization is simply subtracting
the expectation and dividing by the
standard deviation were both the
expectation and the standard deviation
are computed over the training dataset
so this basically ensures that the
distribution it retains the fixed mean
of zero and the fixed variance of of one
right and and so one natural way of
doing that would be to to essentially
compute the running average of of the
examples that that would give us the
expectation and do something similar for
the variance so so we're trained
consider mini batch after mini batch
maintain this running average and then
use that for normalization well it turns
out this doesn't deliver
and the way it manifests itself is that
it blows up the model which basically
means that a lot of weights go to
infinity without the loss doing anything
useful and one reason for this is that
the normalization
the normalization parameters are
dependent on various parameters in the
model and in this approach the this is
not something that has accounted for and
will perform the gradient optimization
and so what we really need to do is make
sure that we involve the gradients of
the of the mean and the variance with
respect to the parameters in this
optimization and so the approach that we
take here is not really think of it as a
way to do optimization but rather think
of it as a way to modify the model so in
other words in other words we modify the
model in such a way that the activations
keep their zero mean and unit variance
in which case we can just use any any
optimization methods with this Augmented
model and this augmentation procedure is
false so here we use the fact that that
the examples come in a mini batch so we
consider a particular activation we'll
look at at its values in all the
examples in the mini batch so there are
M examples in the mini batch therefore M
values of this activation we compute the
sample mean and similarly we'll compute
the sample variance we normalize the
input by subtracting the sample mean I
am dividing by the square root of the
sample variance with a little epsilon
edit for numerical stability and then
finally we scale the result by gamma and
shifted by beta where gamma and beta are
not parameters and the reason gamma and
beta I needed is to make sure that this
augmented network does not lose any of
its representational ability so anything
that the original network could
represent this new network can represent
too and in particular what we could do
if it turns out to be optimal is to set
gamma to be the standard deviation and
beta to be the mean in which case the
scaling and shifting essentially covers
the input so so this segmentation
introduces normalized values
in this case we denote them by X hat
into the model while still making sure
that the model can represent whatever
the original model could so this
transformation is very nicely
differentiable so it can participate in
back propagation so given the gradients
of the loss with respect to the scaled
and shifted normalized values we can
back propagated through everything in
this formulation including the mean and
the variance and recover both the
gradient of the loss with respect to the
inputs as well as the gradient of the
loss with respect to the learned scale
gamma and an offset beta right so during
inference obviously we don't really want
to do what we did here because during
inference we might just have a single
example that we'll want to to make
predictions for and we can not only
compute the means and the variances over
the mini-batch in this case but what
we'll do is we just realize that the
mini-batch statistics that we used here
were used necessarily because of the
mini-batch nature of this process but
really they were just used as a proxy
for the population level expectation and
in the variance and so during training
we use many by statistics during
inference we just use the population
level statistics and in practice we
usually compute them by maintaining the
running running averages of the
mini-batch statistics and those are only
used during the inference right so
better normalization is something that
can be very nicely used with convolution
aware is coming in in image networks and
one modification we'll make here is do
the statistics computation not only over
the examples in the mini-batch but also
over all the all the nodes so basically
what this does is it ensures that that
the best normalization is is also
convolutional so convolutional layers
remain convolutional
in our in our implementation we do the
best normalization after the linear
transform and before the non-linearity
which among other things makes make the
scale of the weight normalized out which
means that we don't really have to worry
so much about what the initial standard
deviation is so in our bottles we
I believe we just said the standard
deviation of all the weights initially
to zero to 0.1 and that just works the
girliest of the size of the layer I know
that some groups have experimented
successfully with using batch norm after
the non-linearity so there are various
options here all right so now let me
just show a few experiments and I'm
gonna show that batch normalization
doesn't 5 to reduce internal coverage
shift and I'll show that we are able to
train networks much faster and I also
show that doing so allows us to to set
the state-of-the-art in large scale
image recognition so for the first
experiment we took a very simple Network
three fully connected layers on em nest
and we used a logistic non-linearity
logistic because those are those are the
ones that are generally harder to Train
and we pick the random and it's a
typical sigmoid in this model both
without better musician and with nominal
normalization and we have observed in
fact that without best normalization the
distribution of the inputs to this
sigmoid which is polit on the left
it changes rather rather quickly
especially in the first stages of the
training but it continues changing even
after so the x-axis here is the number
of training steps and the on the y-axis
we have the median as well as the 25th
and 75th percentile of the input it
would values to the sigmoid on the right
we have the same picture but when bash
normalization has been introduced and we
can see that in fact the distribution of
the inputs to the sigmoid remains much
more much more stable over the course of
the training and we do see that this
doesn't fact lead to the best normalize
model training faster and achieving the
higher accuracy so for a large-scale
experiment that was just thank
yes so for the for the for the largest
scale experiment we considered the
problem of image enough classification
and we started with the state-of-the-art
model which is which is called it's
called inception which is a deep deep
convolutional model with values and and
we applied veteran organization at every
at every layer so after after every
convolution and before the non-linearity
we apply by normalization
this doesn't add some cost thank you to
the to the training time per step
although the amount of that cost is
implementation dependent and in our case
we used s DD with momentum i have to
notice by the way that julien inference
based organization does not have any
effect at all because it can just be
folded into the linear parameters and
you know just by adding bash
normalization we can achieve the same
accuracy as our state-of-the-art model
except with half the number of steps but
when we have better normalization we can
do we can do we can do a number of other
things for example we can increase the
learning rate by a very large factor we
can also either remove or significantly
reduce the dropout and when we do that
we we end up with a model that achieves
the same accuracy as our baseline eleven
times faster in terms of the number of
training steps but furthermore it goes
on to achieve a much higher accuracy by
I think two percent absolute again only
in a fraction of the number of steps
required by the baseline to to get the
lower accuracy furthermore we can we can
do this with a network that uses
logistic nonlinearities which is
something that doesn't work quite as
well as there are your models so we get
seventy percent accuracy but in contrast
the baseline achieve the accuracy of 0.1
percent which is basically chance so so
we're able to train the kind of networks
that previously we could not and these
are just some numbers that so this is
from the paper that were published in
February and we beat the
state-of-the-art at that point we
achieved the error rate of about 4.8%
this is top five classification on an
ensemble of such models I think we use
six with multiple crops per model and
then of course since then we've made
some who made some improvements and so
the current best model achieves the top
five error of three-point 58% this is
according to the image net test results
that were out today and we are in the
second place and we're 0.01% behind the
winner so I stated and so to summarize
we will describe the notion of internal
coverage shift and hypothesize that
reducing it allows us to speed up the
model we introduced a method called best
normalization for doing that this method
allows us to use higher learning rates
use less drop out and make the initial
initialization much easier it also
helped us set the state of the art in
image recognition and this is something
that all of you can use best
normalization is very easy to implement
and it has been implemented by other
people in a variety of open source
learning frameworks whatever optimizer
you like you can use it because again
this is just a modification to the model
not an optimization method it has been
successfully used by many labs both at
Google and outside and it has been
successfully used for analyzing images
finalizing sounds it has been used with
recurrent networks it has been used with
natural gradients at the stereo training
and the number of other applications so
please use it
thank you
well questions yeah hello
so currently you are using the batch
normalization in every convolution layer
have you tried using it every other
layer or every three layers or something
yeah that's a good that's a good
question
we have not tried that but yeah so I
guess I guess the idea is that among
other things if you if you don't use it
in every layer then we could potentially
have less uncertainty in the model so
that could be something to try think
haven't
take another hi
so I was wondering if you have any
experience with using bass realization
in a generative modeling setting the
reason why I asked this question is
because I've noticed that adding bass
normalization in a variational auto
encoder tends to degrade the quality of
samples and I was wondering if you had
experienced something similar yeah so we
have not used best normalization as far
as I know for for generative models I
think some of the other teams might have
done that but we haven't
yes perhaps you could comment on why you
think that with vegetable ization a
lower amount of dropouts is better okay
yeah so why why do we need last report
with best normalization so the
hypothesis that I have is that with bash
normalization with Dusen we normalize
these in the mini batch statistics and
those depend on the examples on the
other examples that a particular example
is being seen with and that in some
sense introduces noise into the model so
that it may we think it serves as a
regular eyes web sort sounds very
plausible thank you have you tried using
a similar technique to standardize like
any of the other moments to make the or
some other method of making the
distribution match more consistently
over time that's a very good question
we have not I hope somebody tries that
it would be very interesting to see I
should bring me well while we wrap up
there were many questions maybe Harvey
could set up you can keep answering
questions all yeah so making the batch
size smaller reduces the statistical
significance significant C of the
statistic batch statistics if on the
other hand were made about the batch
larger than the stochastic gradient
descent starts to suffering so what is
the right way to actually choose the
batch the batch size and also shoot the
batch size be adaptive
I say yeah so you're right
and in fact we can we can have quite a
bit of noise for small for small mini
batch sizes so you know you know in our
experiments we use the mini batch size
of 32 and that works very well
adjusting and reducing that for for
example for fully connected wires would
would be detrimental but for
convolutional layers it's actually okay
because we're also averaging over the
nodes so so fokin volution elias you
could get away with smaller batch sizes
and we have not done the adaptive ones
thank you just stick to one more
question real quick while harvey says up
and they were would you wrap up
just take one more question yeah I said
the question clarifying the covariance
shift reduction interpretation so you
have this scaling and shifting
transformation that you do after The
Bachelor ization to sort of allow it to
model anything without affecting the
original expressive power of the model
so giving you intuition for why you get
this reduction when in principle the
model could sort of learn to undo these
operations and still shift around and
and so so so the question is why why do
we need this scale it's getting and
shifting why why why does that not undo
the covariance shift that you're trying
to combat like you said you're trying to
keep it in the region where the oh the
unit is well right so so at that point
we only have a pair of parameters right
so if you think about the single
non-linearity we only have two
parameters that control where is going
to go in that in the in the domain of
that non-linearity right but but if you
don't have the best norm then you have
all the parameters of the model that
lead up to that so basically we have we
have we have our field parameters that
control that so there's not as much
amping okay
all right we should we should break so
thanks a lot Sookie
you
each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>