<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>BDA - Incremental, Iterative, and Interactive Computation using Differential Dataflow | Coder Coacher - Coaching Coders</title><meta content="BDA - Incremental, Iterative, and Interactive Computation using Differential Dataflow - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>BDA - Incremental, Iterative, and Interactive Computation using Differential Dataflow</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zsL2YxMXaKw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
I'm gonna tell you a bit about
differential data flow this is part of a
system we've been building an MSR in
Silicon Valley with some collaborators
Derek's up in the audience there Rebecca
and Michael as well we have a poster in
the poster session that's coming up what
I'm going to tell you about now is sort
of the data and computational model part
of this which is cool and fun and
interesting the underlying system is
also really neat and slightly different
that I'm not going to give you a
software stack here and tell you about
different pieces too much I'm gonna try
to tell you about that the thing we
think is new and awesome and sort of
fascinating feel free to interrupt if
you have questions I will probably screw
up some explanation but everything
should be explainable to just about
anyone in fact the first slide is going
to be embarrassingly simple and you'll
probably be slightly put off but all
right so here's data perello data flow
in case anyone wasn't familiar with this
this is not necessarily because I don't
expect you guys to know what's going on
but just because I want to get on the
same page and get some of our I can
auger feed out and get you understand
sup so the the general pattern in data
parallel data flow as I'm going to think
of it is you have big collection on the
one hand the collection is then
processed by partitioning up the records
according to keys in each group
associated with the kia's process
independently produce some output
results and they're essentially collapse
together into one big one big output set
and this is exciting you know this is
basically a primitive MapReduce style
step here people have been using this in
Big Data land for a while of course
because the independence that's
described here lets you partition the
computation across lots of machines
we're going to see other reasons that
this independence is important
particular with respect to incremental
and iterative processing but basically
one of these two parallel data flow
things is as a certain type of
computation that I'm going to describe
without with an edge there a little
arrow from one collection to another and
there's some logic associated with the
edge that I'm not going to not go talk
any more about at the moment all right
so so we had MapReduce that said here's
how you do one computation from a set to
another set people moved along from
there and came up with more general
directed acyclic graph frameworks where
you might have
well collections you describe the entire
computation has a big graph this is
useful for the execution environment
because it knows what you're going to do
with the outputs of each of the
computations it can do some optimization
between the different arrows they have
the data site differently fuse things
together if it needs to sort of stuff
several people then observe that it
wasn't strictly speaking necessary to
actually have to be asic like you could
certainly put cycles in there if you
wanted nothing's preventing you from
running some of these edges multiple
times producing new input rerunning some
of the output edges you need a little
bit of help potentially to understand
when you should stop right so if you
have a cycle on your graph should you
run it 10 times should you run it until
it stops changing which could be
millions of times or never so there's a
little bit of different approaches
people have taken there I'm not gonna
tell you too much about the related work
that release that's not my goal in this
talk so all of these these systems that
basically do this batch data processing
pick up collections of data process it
put down a new collection of data are
good they're good at doing what they do
which is a lot of computation they're
not especially good at certain
computational patterns that we would
actually like to see or several people
would like to see in these competitions
so for example incremental recomputation
or re-evaluation so if i have my
terabyte of data I've gone and processed
it and I took half an hour or so and
someone comes along and says here's
another megabyte in that half an hour I
crawled a little bit more data another
megabyte how does the answer change
these systems don't have a story really
other than well hold on let me let me
take another half an hour and I'll go
and I'll reevaluate everything for you
which it's too bad we might like to do
something incremental there are systems
that have started to do incremental
computation so this is something we
should be looking for similarly fixed
point iteration or iterative
computations where you're expecting the
literate the collections within the
iteration to stabilize I have the same
same sort of property that each time you
go around this loop you feel a little
silly redoing all of the work from
scratch if only five percent of the data
have change do you think you might like
to push those five percent of changed
records through the computation only do
the work related to the reevaluation
they really need to do and leave
everything else stable and finally
there's this thing called prioritized
computation which not a lot of other
systems do
critter being a notable notable paper
this is a little tricky to explain and
I'm not trying to do a great job but the
idea is you have these iterative
computations where you might like some
of the records actually move through the
loop a little faster than others if they
can make progress best notice they can
preclude some computation that you might
otherwise end up doing it I'll try to
give an example and the worked example
that we have going throughout the paper
but I might forget and if so just harass
me about it afterwards either at the
poster or in the question section um all
right so people have built these sorts
of systems before 11 point maybe you
know the ideas here or not not
fundamentally new the incremental
updates fixed point stuff is bottom-up
78 data log evaluations done this for 30
something years I don't know the tricky
thing and the thing that's we're going
to deal with here is that these are
fundamentally really hard to compose for
relatively non trivial regions so if you
look at the literature on incremental
view maintenance for recursive queries
is just an explosion of messy stuff all
over the place there the problem being
that these collections can change if you
want to do both incremental updates and
fixed points they can change for
different reasons and they're not you're
supposed to do different things in
response to the changes for different
reasons and keeping this all seen in
your head is just sort of a mess so one
example that I'm gonna throw up and
we're going to work with throughout this
is our motivating example many many
years ago and it also serves as a great
tech number because it teases out each
of these features and we're going to end
up being able to do a relatively good
job on it think about something like it
so take on the one hand all of the big
data computations you're familiar with
like grouping things together and I'm
counting them that's that's a
computation think instead about
something like maintaining strongly
connected components structure of a
graph like a social graph Twitter
mention graph something like that as
edges are continually arriving and
departing we're going to do an example
we maintain a 24-hour sliding window
over the Twitter graph second by second
how long do you think that should take
in terms of the amount of time to
actually compute the correct answer or
not not a you know slightly slightly
correct answer some of the actual actual
correct answer you know with existing
systems I assert this as in the hours
time scales from connecting points uses
w nested iterative computation
and if you need to start this over from
scratch every time as a disaster by the
end of the talk we're going to be doing
this in sort of sub second time scales
so this is if you want to fall asleep
now great i'll wake you up at the end
where we show the the results you're
welcome to do if you want but but the
animations get better so maybe maybe you
should stick around so i'm going to tell
you a little bit about Nyad the skull
tonight the system and then after
telling a little bit about night and how
it works you try to describe the core
technical contribution inside it this is
the differential data flow computational
model which those things slightly
differently than a lot of other big data
systems not wildly differently but
differently enough that it's fun to
explain just to tell you what about Nyad
first this is a date apparel compute
engine it does relatively standard
things that you think of with the data
parallel computing data are partitioned
up by Keys across multiple different
machines we strongly leverage the
independence and the sub computations
once we partition the data up the main
thing that could be doing differently
compared with a lot of the existing
systems as it's gonna be using an
incremental form of data flow so rather
than pushing records around along edges
will be pushing dips around we're going
to imagine that both endpoints of an
arrow are familiar with what the
collections were in you know just just a
moment ago and they're only going to
communicate what's changed so this is
going to allow us to send a lot less
data around the differential part of the
data flow this is sort of a incremental
data flow plus plus so we're going to do
something slightly different than what
incremental traditional incremental
approaches to data flow do that it's
going to allow us to generalize to as
weird arbitrate combinations of loops
and period of updates I'll show you some
examples but there are a few sort of
nice properties that we think they're
nice say how night it has it integrates
really nicely with C sharp link you
could use your favorite other language
if you want so we're not trying to make
value judgments there but you don't have
to have some new language that people it
blows people's mind or anything like
that has all these nice features and
importantly is fully composable so you
don't have to go and pick a different
system if you want to do loops or if you
want to do streaming low latency
computations all of them fit together
also relatively excitingly all of the
performance optimizations compose pretty
well so go into an incremental update to
an iterative computation it's not
you can pretty easily imagine that you
could get the benefits of either
incremental updates or a veteran
computation you end up getting the
benefits of both and things go really
really really fast the main trade off so
that people aren't confused and where
that I'm just trying to sell you a lot
of a lot of Hydra we're trading a lot of
memory for performance the idea is that
when we do these incremental data flow
computations more so even with
differential data flow we're keeping a
lot of information about previous
computation stuff that we've already
done in the past we're going to keep
this resident in memory so we can
quickly get to it and pull it out as
required and as we go through our
experiments I don't have any plots for
these things but memory is going to be
the restricting resource I think it's
correct with industrial trends at the
moment that memory prices are going in
absolutely delightful direction and you
know our machines which currently have
16 gigs because they were built in a
different millennium you know can help
easily be replaced with things that are
you know have hundreds of gigs on them
there's just a few thousand dollars so
we're sort of comfortable with this
maybe it's the wrong decision but the
decision we made when we went with the
system all right I'm going to show you a
few pictures about using nayad and what
a computation ends up looking like to
motivate the differential dataflow
computational model this is largely
pictures so all right so we're going to
start out a programmer writes some
declarative program in the language of
your choice I've just drawn a picture up
here with a few things like relational
join and union in a min operator to
suggest that you don't really have to
commit to a particular language I'll
show you the language you use the next
slide but this is a connected components
computation directed reach ability
computation let me just explain it
because it's simple and we're going to
see it a lot throughout the throughout
the talk so you start out with two
collections a collections of directed
edges so think of these as pairs of
integers or something like that so we
have a comma B is in the set if there's
an edge from A to B and we start out
with an initial set of labels one for
each vertex that are initially going to
be the name of the vertex itself so for
every vertex a will have a comma a in
our set of labels and the idea of the
algorithm which will make sense that the
picture will make sense once I explain
the algorithm is is it iteratively going
to try to improve the set of labels by
repeatedly having
vertex announce to all of its neighbors
its own label and each vertex will then
collect all the incoming messages and
say let me change my label to be the
smallest value that I've seen so far and
if you feed in a symmetric graph here
what will happen is that this will
iterate iterate iterate until eventually
every vertex is labeled with the name of
the smallest the smallest name within
its connected component and people in
different components are different names
because they're all assumed to be
distinct in the first place and and
right so this is something that will
compute connected components for us and
you can certainly execute this in
traditional MapReduce style setting but
it would be relatively relatively
expensive all right the program's deny
it look a bit like like this there's
c-sharp type things there there's this
new link language if you're not familiar
with I recommend checking out it's a lot
like a declarative language like C equal
except as a few little imperative bits
and a lot of nice c sharp things like
scoping and you know things like
subroutines that allow you to get a lot
more modularity then I think it's equal
easily supports this is the directed
reachability computation that I
described we start with some edges we
run this to get our initial set of
labels which I guess I've called nodes
that's that's a mistake we take our set
of edges and we we pull out all of the
nodes that we see in the in our edge set
we do a distinct and then we do this
iterative computation by invoking a fix
appointment that this is the new and
exciting thing in sort of the Naiad
version of of link the fixed point
computation which will take the thing
you're seeing over on the right hand
side is an anonymous function and
c-sharp so it's a method from X to
something and c-sharp figures on what
types for you which is nice but
essentially saying each iteration take
the collection that you've got so far
and do the thing that we described on
the previous slide join it with the
edges concatenate in the the current set
of labels you want the option to keep
your own old label if that's actually
the best one and then do a nice data
parallel min essentially group by
aggregate to find the smallest small
table so that's that's the program you
would write it's not super complicated
there are a few lies in that the actual
program has a few more little
annotations that I've left out but it's
the same number of lines the lines are
just a little little wider and fit on
the slide otherwise
alright so Nyad to existing compiles it
down to a nice cyclic data flow graph
it's not exactly this graph but it
figures that everything using so putting
the loops in where they're these fixed
point operators and otherwise using the
references that are actually made in the
program to build edges between between
vertices the the graph gets distributed
out across a lot of independent workers
so every worker has its own view of the
whole graph so everyone every worker is
going to independently work on a little
fraction of each of the vertices in the
computation with a little different from
other big data settings is that this
competition actually going to stay
resident so it's going to start running
out on the your compute Buster initially
it's not going to be data so you just
started it up and you have a handle to
both its input in its output and when
you start it up you register some
interest in its output so you do this up
here so read the the line sir we've
invoked the director of each ability
code gotten a graph back the result of
which is something that we can subscribe
to and the SUBSCRIBE method this is a
observer pattern so the subscribed
method says every time you you produce a
new output tell my my handy little
function process labels about it will do
something exciting maybe process print
something to the screen perhaps or right
at the desk or or who knows why and what
we do at this point once we've set up a
computation and the response we want to
take to it we run through some input
stream who know this is you know you
reading stuff off of your local your
laptop's discourse or who knows what and
as often as you'd like you you insert
new sets of edges into the input so you
basically have the ability to just feed
more and more data at your your rate you
get to choose how fast you do this I
into the input of the computation and
each of the epochs of data that you push
in so each time you call on next a
subscribe will eventually fire with the
exact changes to the collection of
labels that have resulted from the
corresponding congestion of input now
know yet is clever and complicated will
execute a lot of these things can
currently it can have multiple epochs
outstanding in flight and whatnot but it
gives you the appearance of running
these things essentially sequentially
you push in some data it pushes it
through the entire computation gives you
the output and that's what you will you
see so the programming model is
hopefully although it's distributed in
asynchronous and weird should appear to
be
as if it were relatively synchronous
problem all right so I'm gonna tell you
a bit about differential dataflow now if
there were questions about the the way
the system works probably a great idea
to ask us at the poster will have a demo
there and stuff like that I'm said
enough for anyone to reproduce it which
I understand but just to give you a
picture of what the the needs are going
to be for the differential data flow
problem so I'm gonna start by describing
incremental data flow the title of the
slide will change at about the same time
that the new cool thing that happens in
differential data flow comes into play
if I forget to jump up and down and yell
at you about that you'll notice and
hopefully people will look up and twitch
their heads and stuff but so incremental
data flow which is which is old it's
been around for a while it takes the
approach that data pro operators which
for a lot of systems like let's say
Hadoop MapReduce operate on collections
so big multi sets of records and they
can actually be designed to operate on
differences between collections not not
just collections themselves so when we
have a we think of a collection as a
multiset it's a big set of records and
counts so one count for each record you
know we could just make that count be a
delta instead it doesn't have to be a
positive or non-negative integer it can
be positive or negative integer with the
interpretation that as we get more and
more data we accumulate it up and when
we produce new outputs we should prove
the differences that appropriately tweak
the the output to look like the right
alright pedagogically I'm going to start
putting green regions around the diffs
their green region to get correspond to
what are the discs that i'm currently
accumulating at the moment in the slide
while I'm trying to explain things to
you and and the thing we can have to
deal with basically is what happens to
this operator as new dips arrived so
dips are going to go stream in we sort
of in the incremental dataflow setting
do the obvious thing we produce some
output tips so that the accumulated
input is now equal to the accumulated
outputs we have total flexibility to
pick dy in response to the change DX
coming in it's not too not too tricky
all right
this goes on indefinitely basically we
keep receiving new inputs produce new
outputs using the appropriate
appropriate sorts of rules and we
essentially build up a system that can
react in indefinitely to these inputs
producing output indefinitely as I've
said before this isn't particularly new
people have done this in streaming
systems before people have also done
this if you think of going off the
right-hand side and then through some
other data flow edges coming back and
riping on the left hand side this is
essentially how people do bottom up semi
native data log evaluation so you
produce some some tips they correspond
to a new output collection which changes
your input collection changes your
output collection the system keeps
iterating doing work only proportional
to the disks until they diff stop
appearing for some reason and that
reason could only be that you found a
fixed point essentially in your
computation the output iteration by
iteration has stopped changing stays the
same and of course it will say the same
indefinitely so both streaming or
incremental computation and nice
iterative computation doing them both at
the same time there was a little little
complicated if you if you think about
what happens if we took for example or
connected components computation and
asked oh oh let's just let's let's
remove some edges well you know we want
to change your input slightly so we take
some of the edges out what should the
consequences of removing some of the
edges from our computation be and it's
not too tricky to see that it's really
hard to add a diff to the set of labels
that causes us to you had just repair
the absence of those edges so you might
have an edge between two nice big
connected parts of the graph that was
really crucial in propagating labels and
from one component to another and if you
remove the edge you got to go back and
track down everything that went across
that that that edge in the past and so
it's the approaches historically have
been let's just start over so just blow
away all the history we'll just we'll
start over tough to slightly
sophisticated ways of tracking
dependence basically keeping track for
every record of its lineage where did it
come from why why is it a record in our
system now we're going to something sort
of in between that's relatively simple
in fact and ends up working quite well
which is the following so we're going to
implicitly sorry implicitly we
previously had these versions associated
with our with our records I put
on the screen as you know the first
difference the second third fourth so on
going on to the side let's just make it
explicit now so these could be ince in
this particular case positive or natural
numbers but they don't have to be so
they don't have to be just integers in
this particular setup for example they
could be they could be a little more
complicated they could be two integers
so a difficult come along and instead of
just appending itself to the end of our
our sequence of deficit could say I've
have a new second coordinate that's this
distinguished it's different from the
one coordinate we've been progressing
with I correspond to the second epoch of
input data first iteration now in this
particular case so this would correspond
to on the connected components example
where we've figured out a nice fixed
point for all these labels tweaking one
of the edges and saying god please
please try to restart the computation
essentially without this edge see what
happens and to do that we've basically
run back to the first iteration so you
know okay we need to modify this
appropriate but we can do the same thing
with our with our output since we now
have this additional coordinate we can
just say all will produce a dip that
corresponds to this first iteration
second epoch of data and we do it the
normal way which is to draw a different
green rectangle one that doesn't include
dips that we do have access to these
gifts are really useful they're going to
be valuable in the future but they
actually just get in the way if we're
trying to figure out what is the diff we
should produce in the output we have the
diff them all back out and put them all
back in against it's really annoying
above this this process can be can we
generalize it just continues basically
these these records that we just produce
to go back around the loop again and
return to our inputs and we produce more
outputs always growing the rectangles in
a particular way that that ends up
looking a lot like integration or
discrete integration if you guys are
familiar with Mobius inversion is the
key word that you should be thinking
about I didn't know about it until this
work but so this this continues we
develop a new basically a reacts acuson
of this iterative computation as diff
top of the previous trace essentially so
it's a nice data perla way of doing dips
that dip off of traces we don't have to
do complicated trace analysis like am i
doing in general purpose incremental
computation because we know that
everything's collection oriented but
anyhow
it continues as before we now have
something that extends arbitrarily in
either direction so we can incremental e
update our inputs as much as we'd like
filling out the incremental e
reevaluated iterative computation quite
efficiently the general property we need
here is a lattice or a well-founded join
semi lattice i think is sufficient if
people get excited by these distinctions
but i don't think you do i'm just going
to throw up a let's see it works great
so I'll do this slide and then I'll wind
down I think time was pretty much kicked
this is initially going to be a
connected components computation on not
very big you know 100,000 nodes edges
are random if you wanted to use a
baseline style system each iteration we
would need to play around with
approximately 100,000 labels it takes 29
iterations it turns out in this case so
baseline system would just be a straight
line if we look at the inner iterations
going along the x-axis if you take
something like an incremental system
which says I'll just do as much work as
I has changed from the previous
iteration you see something that looks a
bit like this some of the labels start
to start to stabilize the y-axis by the
way log scale so even though they're not
very far apart that's a factor of two or
so you know there's still a lot of term
though at the beginning there are lots
of lots of people who can't figure out
whether they want label 5,000 something
or 3,000 and something even though
they're all get labels zero at the end
of the day so they're turning and
turning turning and after a little bit
of the label that takes over that large
connecting important dominates and just
everyone quiets down so that's that's
nice it's not particularly newest I've
as I've said so if you come back and
change the input graph for some reason
and there's a very good reason in fact
which is that if we want do the strongly
connected components computation
directed reach ability is a sub-sub
computation in that basically if you do
directed reachability you find an edge
where the labels on the endpoints of
that edge are different that edge is not
in a strongly connected component it's
not too hard to convince yourself of
that with just a little bit of
head-scratching you can we can do the
head-scratching after the after the
talking if you run this in the SEC style
exam
you come back and you change some of the
edges you rip a bunch out of the graph
but you basically need to rerun the
computation now the director beach
ability again and it takes a little less
time not not so much less at this point
that we would be delighted there's no
pee a lot of overhead sort of bad news
but uh yeah okay so we do it right to
reach but again we've got a few more
edges now and this is where it gets a
bit more exciting so the third time
around that we've ripped out some edges
this is the amount of work that we need
to end up doing and this is this is
nothing essentially compared to all the
work we've done previously where as a
baseline system courses we need to go
and redo all all 100,000 operations
moreover the section ends up taking nine
outer iterations to do sec properly and
as you can see once we're done with the
second one essentially we're more or
less done with the entire computation
everything else just happens more or
less instantaneously and this is this is
really nice basically the the few edges
that we're cleaning up at the end that
aren't technically within strongly
connected components but do influence
the computation we were able to track
what are their implications fix all that
stuff up really quickly and not worry
about any of the issues there ah let me
so I'm is pretty much pretty much yeah
in Grand cess I'm done um great sec is
wonderful I'll tell you about it more in
the poster session if you're interested
or Derek and tell you bout it that's the
code it runs really fast this is a
actual sub-second Layton sees
maintaining the SEC structure of the
Twitter graph to Twitter mention graph
sorry I should say people talking at one
another and let me just wrap up then so
differential data flow is this neat I
think neat sorry you guys decide for
yourself but need a computational model
that allows systems like now I had to go
and do essentially these incremental
iterative computations but more
naturally where I think of naturally as
only doing the work you really need to
do so it figures out and does a better
job of managing the previous work that
it's done by allowing you a bit more
flexibility in terms of defining
previous so instead of just the
collection mere moments ago you have
this logical notion of previous that's
general enough to so where it's our
multi-dimensional multi of multiple
reasons why collection might change
empirically we get a bunch of good
things though I haven't actually showing
these the Milice kill millisecond scale
updates for these complex computation
stuff like sec CC which if you want to
rerun from scratch would be disaster
actually can get updated really quickly
something that I think is really cool
these last two sort of points here with
SEC and some things like it graph
coloring a few other exciting things
there these program patterns you
wouldn't necessarily expected we
actually fit into Big Data computation
if you're thinking about MapReduce and
just running a bunch of MapReduce
operations atomically over and over
again you would you'd run away from a
lot of these programming patterns nested
iterative computations just as being
disastrously expensive and the the
efficiency of these program patterns in
differential dataflow gives us the
opportunity to a bunch of things that
previously we've been really expensive
we just wouldn't have even approached a
bunch of graph partitioning graph
coloring stuff pretty much everything
that you can do i think in graphs lab
pretty much you can do in in Nyad as a
general form of big data computation
rather than worrying too much about the
specialized systems aspects of it and
we're looking at broadening that too so
if you have problems you think fit into
a nice incremental iterative pattern
bring them to us and we'd be happy to
help you figure out yes this works great
or no it doesn't work and we need to fix
something in the in the system but I'll
wind down there and if there are
questions that don't fit into the
question section again we'll just be
right outside with everyone else in the
demo session thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>