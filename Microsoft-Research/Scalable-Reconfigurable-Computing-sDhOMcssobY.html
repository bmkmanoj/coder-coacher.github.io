<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scalable Reconfigurable Computing | Coder Coacher - Coaching Coders</title><meta content="Scalable Reconfigurable Computing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scalable Reconfigurable Computing</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sDhOMcssobY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
good morning it's my delight today to
introduce kerman a Fleming who goes by
Elliot the e is for Elliot know and he's
visiting us from MIT as a FTE hiring
candidate who is I think very
intellectually aligned with a lot of the
work going on here at Microsoft and he's
done some really tremendous work as part
of his dissertation both on systems
FPGAs and compilation to fpgas so really
excited to hear your talk and thank you
for visiting us pleasure to be here so
my name is Elliot and today I'm going to
talk about how we can scale programs to
multiple FPGAs so before before we get
started I'd like to thank everybody that
I've worked with so basically my
advisors were Arvind Angela Murr and all
of these folks here were involved in the
papers covered in this talk and here's a
bibliography so the compiler work is
categorized under leap air blue is a
wireless transceiver project and there
are a few other designs presented so
let's get started so basically FPGAs
have traditionally been used as a sick
prototyping tools there's drop-in
replacements for ASIC however recently
FPGAs have gotten quite large and also
much easier to integrate into systems
with pci-e ethernet various iOS and so
now we can talk about designing big
systems with fpgas in them is a first
order compute with the goal to
accelerate some algorithm so we have
some algorithm that we were running in
software the software is is not fast
enough for maybe burns too much power
and so we want to run on the FPGA okay
the goal here is time to answer so time
to answer has two components so one is
of course accelerating the algorithms
that runs faster but the other is also
to reduce the amount of time that it
takes an engineer to build an
implementation okay the second goal is
functional correctness so here unlike in
traditional FPGA flows where we cared
really about preserving the behavior of
the ASIC that we're going to produce and
we have to make sure works right
otherwise we will aced a lot of money on
a mask set here we only care about
functional correctness that is that
whatever answer we wanted to compute was
computed correctly and of course as fast
as possible okay so here are a couple of
examples of this kind of program so one
is hasten which is an emulator for
processors the other is air blue which
is a framework for building Wireless
transceivers which are actually
compatible with commodity hardware so
you can talk to you know yer 802 11 base
station okay and again the goal here is
functional correctness for both of these
codes as long as we produce the correct
answer within some high order time-bound
we're good okay so now that we're
writing programs onto FPGAs you can ask
the question what happens if our program
is too large right remember that fpga is
of course or structural things so we can
unlike in a general purpose processor
express a program that is too big to fit
onto the substrate so here we're laying
down CPUs and eventually we have too
many to fit on the single FPGA so what
are we going to do right so one thing we
can do is optimize we can try to make
our design smaller that works to first
order or we could go out and buy the
biggest FPGA we can again you know these
these are patches but at some point we
have to use multiple fpgas and so what
does that entail one we're gonna have to
partition our design so here you know
there's quite an obvious partition right
we just put you know the CPU on the
other FPGA to we're going to need to map
our design this partition design down
onto multiple F pj's and then finally
we're going to have to synthesize some
network between them and of course we
can always do smelly so we can take our
engineers and have them implement this
whole thing and we'll run it and I'll
probably run quite fast however this can
be tedious and error-prone particularly
if one is exploring a design space need
to change the implementation and the
question is can we do this automatically
ok so the remainder of this talk is
going to discuss how we can we can
achieve this this goal automatically
he's an error code yes non tedious
narrator
Yeah right probably error-prone is the
most important there so before we get
started on how the compiler actually
works let's talk about what we should
expect we map the design to multiple
fpgas so more fpgas mean more resources
and just like in a software program when
we throw another core a better cache
hierarchy to problem we should expect
more performance okay so one thing in
fpgas want one metric of performance of
course is the problem size that we can
implement and so what I'm going to show
is that one of the examples actually can
be ten times larger when it fits on
multiple FPGA so on a single FPGA we can
fit a 16-core model and on multiple
FPGAs to to be precise we can fit a
model that can model up to 120 one
course so that's the 10x problem scaling
for this particular problem also when we
give more resources to a problem just
like in software we should expect it to
run faster this can happen for a number
of reasons for example you get more drm
banks on multiple FPGAs but also since
you're asking the tools to solve a
simpler problem when you partition of
design sometimes they can come up with
frequency scaling as well and so what
I'll show you is that one of our
examples can actually achieve a super
linear speed-up when map to multiple
fpgas this is performance normalized a
single FPGA and up is good okay so in
summary what can we expect design
scaling so we can get bigger designs
when we have more resources more fpgas
we get faster run times and then finally
although i'm not going to discuss this
we can also get reduced compile times
because again we're asking the highly
nonlinear tools to solve simpler
problems okay so the good news is so
again our goal is to sort of produce
these implementations automatically and
the good news is that multi FPGA
compilers exist commercially so you know
if they were good we could stop right
and they operate on arbitrary RTL which
is also good okay the problem though is
that they have to preserve cycle
accuracy so what is cycle accuracy right
cycle accuracy is a bijective mapping
between the original RTL description
which was clocked right and whatever
implementation we put on the FPGA okay
so
in the fpga implementation there's a a
precise way to resolve the original
behavior of the RTL and so kind of what
you can see here is that the the model
clock which represents the original
behavior of the RTL is ticked
infrequently the fpga clock of course is
running very fast and then between model
clocks we're doing some kind of
communication between the chips in order
to preserve this idea of cycle accuracy
right oh and feel free to stop me at any
time if you have questions ok so and of
course you can see how this would be
useful in a sick verification because we
want to preserve the behavior of the RTL
because if we make any mistake in that
translation or for our tails in any way
buggy we could break our chip ok the
problem though of course is this like
lie accuracy gives us low performance so
what you can see or is that the fpga
wants to be fast it wants to run fast
but because we're having to preserve the
cycle accuracy we're actually going at a
very low speed relative to what we could
get out of the fpga ok and again this
comes from the need for distributed
coordination it also comes from the fact
that there are very poor semantics here
right here in maintaining cycle accuracy
we have to transport every bit just in
case some logic might behave funny in
the case that that even if a bit is
invalid it was transported right so you
can imagine here that if this data is
invalid right so the control here is
invalid right but we still have to
transport all the data in case some
point in our circuit might misbehave
right so we have some rent we could have
some random data vector here right but
that might cause a bug in our design we
don't know and if our objective is
verification of course we need to
preserve that behavior so we can fix it
yeah
I'm sorry if this sounds like a
moderately hostile question but it seems
to me like you're setting up a little
bit of this drama in here sure you know
you're saying law I want to set aside
rtl to a large logical at PGA but then
i'm going to partition it to multiple
ones that I've got you know
comparatively enormously slow
communication and low bandwidth between
them and so that won't work well unless
I partition the design well semantically
says extent seems like it's I mean it
really does seem like a straw man
because there's no hope of getting to
that that magical point where you can
say this partition any design and have
it run that your deviated very precisely
and that's why we're not going to
partition a design we're in fact going
to restrict designs in a way that leads
to good partitioning so so basically
we're going to give programmers in your
primitive which we'll talk about next
couple slides that will enable them to
describe designs in a way that we can
easily map okay all right so we'll see
how that works ok so again here we're
preserving cycle accuracy but remember
what I said the goal of this new use
case for the FPGA is functional
correctness as long as we get the right
answer we're happy okay so the question
is do we actually need to preserve all
of this this cycle accuracy answer of
course is no so what I'm going to
advocate is is this new style design
called latency and since it is on the
basic idea here is that inter module
communication occurs only over latency
and sensitive channels the idea is is to
decouple the behavior of different
pieces of the design from one another so
that we can change their implementation
okay changing the timing behavior of a
module then does not affect the
functional correctness of the design
right as long as the data flows between
the modules and then that data flow is
preserved then the behavior of the
design will be the same the functional
behavior right of course the timing
behavior will definitely change okay
many hardware designs already use this
methodology right so most hardware
designs are described in terms of these
FIFO explicitly for the obvious reason
that there are many unpredictable
latencies and hardware designs and of
course you know Hardware designers also
want to do design space exploration okay
why again improve modularity
improved science-based exploration and
today what we do is we simply insert phi
foes guarded fif--is between the modules
in the design right and of course we
don't in queue data into the fifo unless
there's room to in queue data we don't
DQ unless there's actually data in the
FIFO and we express our design in those
terms this is a very simple model okay
so let's think about that a little bit
so what I said was that we could change
the behavior inside of any module in any
way we wanted while preserving the
functional correctness but what this
implies is that we can also change the
behavior of the channels themselves so
if I can change the behavior of the
channel and I have a design describe
this way on an FPGA Matthew to FPGAs is
straightforward I simply stretch the
channels between the boundaries and of
course logically these are still 5 o's
okay but there's a problem I can have
lots of fibers in the design and not all
of them may have this property right
because remember that a compiler an rtl
compiler sees only wires and registers
they can't even tell probably that
there's a FIFO here okay right so so
semantically it may see some wires and
registers with some logic but it's very
difficult to even determine that there's
a FIFO ok and additionally of course
reasoning about cycle accuracy is is
difficult ok so it's very hard in these
things even to decide whether or not
it's safe to add an extra pipeline stage
in a FIFO but the programmer knows about
this property this latency insensitive
property he expressed its design this
way to sort of get the the benefit of
modularity right so what are we going to
do we'll just give the programmer syntax
to describe these kind of latency and
sensitive Pfeiffer's yeah can you give
me more precise description or semantics
of what you mean by latency
okay so what I mean is is so first let
me let me clarify that latency and
sensitive does not mean that we don't
care about latency right so this is this
is a common a common problem with with
when we use the term latency and
sensitive right what it means simply is
that we're free to change the behavior
of the fifo this will come up all right
so what we'll get to it in a couple
clicks but basically we're free to
change the behavior of the FIFO and the
programmer is asserting that they've
described the rest of their design in a
way that permits us to make this change
okay so for example they won't try to
include data into the fifo if the fifo
is full so kind of they're leveraging
the back pressure on both ends of the
fifo okay again this is not the only way
to write a FIFO near design you're
always free to use the register and wire
fifo so is latency insensitive been
defined in terms of this particular
implementation technology of bicmos is
that the only way to characterize
I I don't think so but it's hard for me
to imagine the other way of
characterizing it I see those magic
what's that yeah you could think of it
that way perhaps so that's fair you can
think of it as a synchronous logic in
that whole field that's sort of what
we're doing here again computers is
happening on data flow tokens and we're
decoupling the notion of clock for and
compute I mean that's the fundamental
difficulty right is in Psych like your
see you know clock is a first-order
thing here we're trying to remove the
notion of clock so that we can perturb
the design in some ways that are
beneficial to the programmer yep so
indeed is new how it is benq and on one
end and the commands i'm going to them
and the data valid pop something through
to the other side how do you have a
latency insensitive channel that you
can't they can't stretch come
so source image so the question is why
aren't all FIFO latency sensitive right
the simple reason in this is because you
may make assumptions about the
particular implementation of a FIFO for
example you may make the assumption that
this FIFO has a depth of one that is
when I in queue something into it it
will be full and that will maybe you'll
use that control logic to determine some
other things in your pipeline so for
example you may say if this thing is
full I I will issue some other requests
though the fifo may have proper flow
control but you may make some assumption
about the buffering for example you
could also make an assumption about the
latency but I think the more common case
at least in the designs that I've worked
on is you make some assumption about the
depth of the fifo for example that it
has one or two buffer slots and you
write other logic to expect that and so
actually adding more buffer slots than
perhaps one or two would break your
design because that assumption that you
baked into the logic is no longer true
right I mean you could imagine having a
single entry FIFO with flow control you
know not full not empty right and then
using that assumption that it's got a
single buffer slot in it to actually
implement some other logic I mean people
do that the basic issue is is if if you
allow people to to leverage that
assumption then it makes it very
difficult to make the kind of changes
that I'm going to propose the next set
of slides me the thing that you want is
to find large regions of code that have
no recurrence
larger versions of code that have no
recurrent salts in other words if you
know think of it like a pipeline you
know your drink partitioning and the
light the degree of latency and
sensitivity is the amount of compute you
can do that's decoupled through a FIFO
before you have to go back and close
balloon that's right so feedback right
that's right and and you know this
pipeline for decades mm-hmm and so if
you if your tools can you know analyze
your design and find the partitions that
they don't talk back to other regions
you do some sort of graph partitioning
right but that's that's how you can
actually map this is that are you taking
approach like that so we haven't studied
mapping right so mapping has not become
a problem for us yet I'll talk a little
bit about how we do mapping but it's
quite naive but you could imagine some
approach like that being necessary as a
refinement to this however I'll also
point out that generally speaking even
if there is feedback in a pipeline very
often hardware pipelines will have a
pipeline depth that is sufficient to
cover the latency of inner FPGA
communication this is certainly true in
DSP algorithms I think it'll be true and
others so for example hey sim because of
the way it's implemented is kind of a
time multiplex pipeline will actually
have enormous potential to hide the
latency with useful work billion sea of
communications thinking about our
arbitrary applications sure general
underlying approach yeah so so so so
generally speaking yes you would want to
try not to partition across feedback
paths too often I think but we don't
have any way of doing that automatically
not ok so anyway here's the syntax
basically one frames one's design in
terms of sins and receives and a
compile-time the compiler will choose an
implementation one example of an
implementation is just the vanilla FIFO
that you could have written anyway okay
on the other hand you may choose to
synthesize a complicated network again
depending on placement and other design
goals all right and again here we have
an explicit programmer contract when the
programmer writes down the sindh
received channel right he's willing to
accept unspecified buffering and
unspecified latency right and he
guaranteeing that he's written is
designed in a way that will admit of
that choice by the compiler so of course
you know the programmer can write a
buggy design and the compiler will
happily generate a buggy implementation
it's a more of a programming tool in
that sense okay however generally
speaking we found that this primitive is
pretty easy to use often you can take
five phones in your design and just
substitute them out so it can be a
simple substitution and this is this has
been our experience for most life owes
except the ones which I was describing
to you in the back wherein you're using
them for control the fifo the depth of
the five over control okay so now we
kind of talked about a syntax for
describing latency insensitive designs
and we've seen that latency and since
designs can at least in theory be mapped
to multiple F pjs so let's talk about a
compilation flow so so will now discuss
a compilation flow to do that and then
afterwards we'll talk about how we
synthesize networks between FPGAs okay
so what we're going to do is we're going
to start out with an arbitrary RTL
augmented with the latency insensitive
channels so what we've got here our
little state machines they could be any
RTL or they could be software for that
matter does it necessarily even have to
be an rtl connected by latency
insensitive channel shown by the dotted
line so we just have some graph okay and
again I mentioned that to produce a
multiple FPGA implementation we have
three phases so the first we have to
build some graphical representation of
this which you can partition okay then
we're going to have to take that
partitioning and map it down onto some
network of fpgas okay and then finally
we're going to have to synthesize a
communications networking carrying the
community yup sorry
it was responsible for establishing that
the archaea behavior doesn't change with
the latency of the channels changes the
RTL behavior may very well change
absolutely the point is that you're
asserting that those behavioral changes
are not going to impact functional
correctness okay again the RTL behavior
will absolutely change in much the same
way that your RTL behavior would change
if you interposed a level of cache
hierarchy right it will change but it's
the programmers job to ensure that these
these changes don't perturb the
functional behavior of their design in
practice this is not a very difficult
thing to do RTL behavior you talk about
timing you're leaving a planning event
to Tanya timing exit I've seen
Corrections I mean oh yeah correct is
gonna be the same bits emailing delay
gives us channel starting yeah no
absolutely you know if you wrote it if
you wrote a bad design or at least a a
bad design in terms of you know this
property right you could very well get
an incorrect implementation although of
course I'll ask the question if our
design was too big to fit to begin with
how would we have implemented a
transform to preserve that correctness
well we would have just we can do it
right there are tools that do it but you
pay all your performance right so you
lose order magnitude performance to
preserve the property right so this is
this is the trade-off that we're making
here is is instead of preserving that
exact timing correctness of the original
RTL we're giving freedom to the designer
to express points at which that behavior
may safely be changed and we're going to
leverage that right yep yeah it also
matters this is a Verilog so well so too
for full disclosure we actually
implemented respect for a number of
design choices mainly because blue speck
is easier to augment with compiler like
features just like Haskell its
predecessor but you could imagine these
are tl's being Verilog also so you could
think of this as just putting these send
and receive endpoints into two Verilog
that's certainly admissible okay and
then finally what we're going to do is
given this sort of implementation will
produce an rtl for each FPGA and you can
run it through the backend tools to
produce some
tation okay alright so the first thing
we do is we're going to construct a
graphical representation that we can
partition of course remember that the
only thing we know how to modify or
these latency and sensitive channels and
that's going to kind of give us this
graph structure over here where we have
blobs of RTL connected by latency and
sensitive channels we call these latency
and sensitive modules although I've
shown rtl here again it is possible to
put whatever kind of computation you
like in there including software right
as long as it obscured to the the
latency and sensitive channel
communication model okay and then what
we're going to do is chop up the design
in this way and map it down onto a set
of fpgas okay and again the vertices of
latency into the modules and the edges
are latency insensitive channels and
here's an example of the syntax here so
we have some channel a and it induces
the edge here okay all right so now that
we've got that representation our next
objective is to place it down onto a
network of fpgas okay so the first thing
we need to know is actually what the
topology yep so if you're actually
previous slide on her
the this the border is between these
devices sodium's that I'll have one
cycle data out to data again no start it
all out of that every cycle I could
produce data and such that the bandwidth
between each new devices is full ban no
novice Farrell okay okay these are these
are just Q's right so if you don't if
you don't push anything into the queue
there's no communication at all fine if
i push something into the queue every
cycle sharp and the interface between
say the block a the Block B is
insufficient to run at that speed that's
so for example the the time mo
multiplexing cheating that you described
very early and when you're very early
slice is often done because the
bandwidth externally is so much smaller
than the band with it yeah that's shirt
and we'll do time multiplexing this
approach to but will also have back
pressure okay so if if the band wit oops
I I hit the wrong button if the
bandwidth on C is insufficient to carry
all the traffic between a and B then a
will stall they will I will get back
pressure in a whilst all the hope is of
course that is as things scale up you'll
get more and more bandwidth but it is a
problem right if if C is enormous if C
is is you know ten thousand bits then
yeah yeah
ah so we'll talk a little bit about
we'll talk a little bit about that when
we talk about compiler optimization but
generally speaking first of all the
conception that these are pens carrying
traffic between F pjs a little bit
mistaken so actually they turn into
these high-speed transceivers right so
so actually there aren't pins at all in
some sense but we will talk about how
exactly we allocate the bandwidth of the
transceiver you know in a sort of
intelligent manner perhaps in 10 slides
will maybe less than that yep okay did
that answer your question we'll talk
about bandwidth allocation at some point
in the future all right so anyway we
need to know what the physical topology
of F tjs is this is a little syntax for
describing that so basically we have two
of pjs at pga 0 FPGA one and they're
connected by bi-directional channels and
of course you could scale this up to to
have whatever system topology you would
like even though I'm only showing a
short example here okay and so this is
the physical system where we have two
fpgas and they're connected by some
high-speed transceivers okay and then
what we'll do next is will map the
modules based on area so of course you
have to have a feasible implementation
but also you want to minimize
communication between the fq Jays at
least ideally we would have some
algorithm that did this automatically
and so you get some mapping like this
where a and B are on one FPGA and seem
to your on another but currently require
user input so the user is going to have
to tell us which module goes where okay
now of course an important future work
is doing that automatically okay I'd
like to point out at this time that this
configuration file is the only thing in
the in the design that differentiates a
single FPGA from multiple FPGA
implementation or even a three or four
whatever FPGA implementation this is the
only part of the input to the compiler
which is changing okay the program
itself is fixed which is of course an
important property all right so now that
we've done this mapping we have to
synthesize a network so basically this
entails choosing an implementation
for each of the channels local
communications of course just turn back
into our vanilla fios so you know
they're just this sort of ideal high
bandwidth interconnect right however
remote communications will actually go
through some kind of network hierarchy
okay at which we will synthesize based
on the program I'll talk about that in
the next set of slides okay so all of
those channels will be tied down to some
Rooter which will which will manage the
inner FPGA interconnect okay and of
course this link will appear as a FIFO
but the rooters themselves would be
quite complicated okay so now we've seen
basically a flow of how we can get from
our TL to a multiple FPGA implementation
and now we'll talk about specifically
how we build the rooters okay so here's
a kind of a cartoon of the network
architecture right so basically the user
program is seeing 5 o's with back
pressure okay and of course these five
phones are going to be multiplexed onto
the Rooter infrastructure okay so all of
this programming model is quite simple
the hardware to support it is actually
quite sophisticated so basically we have
this this automatically synthesized
layer of network hierarchy so the first
layer is is marshaling so we have to be
able to handle some wide links and
convert them into a fixed packet size
okay then we have to have some virtual
channel buffering to ensure that
channels don't block each other okay and
to ensure deadlock freedom and then
finally to improve the parallelism will
actually run multiple lanes across the
link in order to try to soak up as much
in a refugee I'd bandwidth as we can
okay is clear alright so the first thing
we do is is channel marshaling so in the
original user program of course these
are can describe whatever data types
they'd like to be carried between our
pjs but the network with is fixed so we
have to introduce some layer to packet
eyes the data types so for very wide
data types of course you just do the
shift register but for narrow data types
will actually just pack everything into
a single Network word and we will do
this based on the links right so this
will be automatically chosen by the
compiler and this is actually important
because remember we're working on
hardware designs and of course Hardware
designers are always trying to economize
bits and it turns out that in many
hardware designs the width of the
channels is actually quite narrow this
is an example from hey sim and what we
see here is basically that the
overwhelming number of channels or
narrow okay so next layer is channel
multiplexing right so the good news is
that most channels actually don't have a
lot of activity and remember we're only
carrying data between FPGAs when data is
explicitly and cued so if there's no
activity then there's no bandwidth
consumed okay the bad news though is we
don't control message creation
consumption and this can lead to dead
locks because we have a shared network
infrastructure right to see how that can
happen look at an example so now we need
both a and B to do the star operator a
sends a value and of course a is going
to send again right you know how this
works right so now B is going to send
something which we actually need to
proceed and of course it's got head of
line blocking so we're deadlocked right
how do we solve head of line blocking
well so one option is we could try to
compute the dependencies and do
something intelligent with virtual
channels but in reality will just give
every channel its own virtual control ok
how do virtual channels work oh so this
is going to be deadlocked free via the
Dalai sites theorem because of course no
we've broken all the channel
dependencies right since these channel
has its own virtual circuit then we
can't have a deadlock ok so how does
this work well now a sins but it doesn't
have any more flow control credits so it
can't send again oh sorry stop so now be
will send and of course it's not out of
flow control now the operation star can
proceed and will send flow control
credit back and it can proceed again
this is very simple ok it's kind of our
flow control works in general now of
course we have some options in
implementing this so one option of
course is to have very small buffers
small buffers are inexpensive of course
there's a problem because in our fpga
Layton sees can be quite long and so if
we have small buffers then if there's a
hot path between the FPGAs then we can
stall ok on the other hand large buffers
are expensive so if we just give a large
buffer so say we give eight registers
per channel do we end up using most of
the area
pga and of course this is problematic
because what we want is the user program
to have mostly area of the fpga for its
own implementation okay so what are we
going to do well so observe that the
channel connecting the FPGA is actually
cereal so what that means is that we're
basically getting one data word per
cycle okay what that implies is that the
store for all of our virtual channels
can also produce data at one word per
cycle and will still satisfy the full
throughput via littles law okay what
this means in practice is that we can
use a serial structure specifically be
Ram to store all of these virtual
channel buffers and what that means is
because you know be Rams are quite dense
we can actually have enormous channel
buffer / virtual channel and will still
be deadlock free because the virtual
channels don't block each other in the
shared structure okay oh yes maybe so
basically right what we'll do is because
this is cereal and the SRAM is 0 we
won't lose any throughput but we can
have a very deep buffer per channel
which allows us cover the latency of an
RFP trade links yes single right for
every single right forward single report
so we get full for fun okay a little bit
of latency maybe but we get full
throughput all right so this is what the
the multiplexer microarchitecture looks
like so basically again as I said we
have all the virtual channels map down
on to the SRAM ok then we have some
bookkeeping bits out to the side also
mapped into what Ram what's that oh ok
so data comes in right stored in the brm
then we have some arbiter that selects
which virtual channel we're reading out
of based on the bookkeeping bits ok and
the great news here is that even if we
give enormously deep buffers more than
100 buffers per channel we use only a
small percentage of the fpga for typical
designs and this allows us to scale the
size of our implementations so we can
actually you know instead of you we can
have connections between several
different FPGA devices without
overwhelming or error a usage ok
whether you should be using this this
architectural previous architect is done
how does Basin how you do the alligator
what do you mean about is this is this
hasn't this says of course the issue of
you know you are doing time'll deposit
that's right by its very nature along so
how do you know what you can afford an
on time limit vois architectural versa
seen we'll get to that in a couple
slides I think okay so the last level so
at this point in time actually we get to
it in this slide so at this point in
time we have a fully functional Rooter
so we could just lay this thing down and
we'd have a working multiple at PGA
implementation the question is can we do
better and the answer as you allude to
is yes we can do better okay so let's it
in order to better let's kind of look at
the properties of user designs so
specifically what the the width of the
channels look like and what their
traffic looks like and what we see here
is that of course as I already mentioned
channels are narrow and also that these
narrow channels can have some some high
occupancy right so ideally what we want
is to sort of service these channels as
best we can okay so user designs have
pretty low clock frequency and narrow
channels whereas the inner fpga physical
layer is very fast and as hundreds of
bits wide as a result so of course you
have to do this this clock frequency
sort of gear boxing right so if the user
is on is running at 50 megahertz and the
inner fpga Phi is running it you know
hundreds of megahertz then we have to
sort of multiply up its width and you
end up with with a few hundred bits per
cycle of data that you need to stuff
into the Phi in order to get full
bandwidth okay and what this is telling
us is basically that in the presence of
all these narrow channels single channel
at a time is very wasteful so if we just
do a naive time multiplexed approach
we're going to waste a lot of bandwidth
okay so how do we do better well we'll
have multiple lanes okay and then we'll
share the bandwidth okay so how does
this work so basically what we'll do is
will instantiate several multiplexers on
top of the wide Phi okay forming lines
so here we have one multiplexer to
multiplexers three multiplexers all on
top of the same wide physical layer okay
and these can all go in parallel
okay so we can recover some of the
parallelism of the system okay right so
we have some time multiplexing of course
these remain time multiplex but they can
all transmit data in parallel okay so
now that we we have the capability of
adding these lanes we have to ask the
question how many lines should we have
and how do we allocate channels two
lines right so these are free parameters
in our route or architecture okay so we
could look at the dynamic behavior of
course ideally what you would not do is
you would not allocate two channels
which are constantly being in queued at
the same time to the same Lane right
because then of course they're fighting
each other for bandwidth okay we can't
really reason about that behavior at
this point in time although maybe with
some better program analysis techniques
we could but what we can do is observe
advocate channel load so we can do is
instrument the design and look at the
traffic across each of the channels in
the design and try to do something with
that okay the idea being that what we'll
do is we'll minimize the maximum load on
a given line okay so we take that
maximum load is kind of a measure of how
fast our program is running assuming
that it's communication bound and we'll
try to make that as small as possible
okay unfortunately this is a processor
scheduling problem or at least it turns
into a processor scheduling problems NP
complete but there is a good heuristic
longest job first it's not as long as
job first work okay so what we have here
is a set of channels in a program that
we're going to route between two fpgas
the height of the bars represents the
loading that is the absolute amount of
traffic across the channel and the width
represents the physical width of the
channel so you may have some channels
which are wide and some channels which
are narrow okay they produce more or
less amounts of traffic okay so the
first thing we'll do is sort according
to load and we're going to try to make
the situation the best for these heavily
loaded links okay because of court
what's that with the rubbers
the packet size and the high represented
rate the height represents total traffic
right which could represent right
although you know already you've already
taken on a pack aside that is the total
traffic by exercise the same is rate uh
yes across the run of the program right
but you may have some I guess what I'm
trying to say is is there needs to be a
distinction drawn between the aggregate
behavior across an entire run and
dynamic behavior right and so we're not
yes vs. that's right so how are you
capturing person we're not that Leslie
where I was going right yeah we're not
enough reimbursements right that's right
so you know obviously you know if the
total program run time is something up
here then the right may be low but you
may have burst ness and and that might
perturb your reader architecture but I'm
not trying to capture that at this time
ok so anyway basically what we'll do is
we'll take our heaviest loaded lanes and
obviously loaded channels and
synthesized lanes for that right so 123
for the three heavy sleet look bloated
channels okay and then will allocate
those heavily loaded channels to the
lanes okay now with the remaining
channels will try to load balance
allocating the channel to the least
loaded lane okay so now we put this one
here and we'll put this one here and so
on and what we've got is basically load
balancing ok so in on average the the
total amount of traffic across each line
is more or less equal ok and it yep take
it into a comfortable with as well on
this Allah case that's right yes yes yes
because if we put a sort of although
we're not doing it here if we put this
fat channel on a narrow lane than its
traffic will change so yes we actually
do account for that so we make the
choice we we change so you can of course
because you've you've kind of statically
allocated the widths you can see I'm a
traffic will be across each lane yep
simulation
or a run yes yes it's feedback during
compilation that's right that depends on
how you said it oh that's right so we're
closed its workload dependent axles I
don't wanna PGA this simulate a wannabe
did oh no you can simulator on TF you
guys will instrument all the channels
for you and find the looks so you make
it would be mapping right that's right
so basically so basically you can send
your always free to synthesize a crappy
network and and ideally the loads will
not change very much or you could do it
in simulation of course that's that's a
infinite capacity of pj although
generally speaking for for most of these
designs there have a sufficient size the
simulation is not your most attractive
option because you can't really run a
large enough workload are you going to
talk about topological mapping of the
FPGA network
what do you mean by that so so this idea
perhaps adding route throughs to handle
strange strange physical topology
relative to technological topologies
strange is perhaps pejorative you're
assuming that your producers and your
consumers are on adjacent MPG you know
not at all okay and several producers
and consumers are not on fpga well that
route through you have read through
that's right and then now you're talking
about a router network that's right and
now your virtual channel approach is a
little bit trickier no not particularly
because again there's the question if
you have up with the observation kind of
a shared link in with a direct
communication and then are out through
so I you've got to worry about both sets
of virtual channels if you are trying to
do something clever i could imagine that
being the case however what we do is
that each and are fpga crossing we will
give a new virtual channel okay so
basically the virtual channels are only
handling deadlock free on that single
NRF pj link so basically what would
happen is you bounce what's that see you
at all right so you even some sense
turned it into a static weave out of
network coming better hope the thorough
know dynamically routable pass you could
have a better implementation perhaps if
you had some some capability to dynamic
load balancing but you know well no no
such as what we talked about this we
talked about this this morning if i have
a failure right and i want to remap you
know a fpgas role to another fpga sharp
now all my all my routes through your
network change in the virtual chawla
sharp allocation changes that's right
and it doesn't sound to me like you've
provisioned
no no no right but it's again the
virtual channels are so cheap that it
wouldn't be beyond the realm of
possibility to have spares I mean again
you know these things are very
inexpensive I mean really the cost of a
new virtual channel is just the cost of
adding extra space in an SRAM and the
SRAM has you know 64 kilobytes of space
I mean once you have one of them right
you actually have a lot of space maybe
for the problems you you've allocated
the packet sizes are going to be small
but if you start having large package
rather than virtual channels and the
email vestrum you talk about you get
really high men make the packet is four
kilobytes for again yeah of course you
could break that pack it up into chunks
and just do chale occasional chunks
could certainly do that so yeah in fact
that's what we would do we have
marshaled it can just flow control the
marshals Marshall so as you can constant
number of official channels do you see
the logic you have rescheduling official
channels and deciding which one to send
out you see the overlay of that yes so
it does grass tickly that's right it
does go up and in fact depending on the
number of virtual channels you can
choose different orbit or architectures
so yeah we have several layers of
pipelining so of course if you have a
handful of virtual channels that you get
single cycle scheduling otherwise you
have to do one or two cycle scheduling
but to cycle scheduling goes up to
several hundred lines so it's it's
scalable I mean you could even if you
want to add a third level of hierarchy
there but again it's just dropping it in
right and the compiler can choose based
on the number of virtual channels yep
okay so what happens when we do this
optimization to a real program that is
hasten so here we have the naive
implementation of hasten right and again
up is good so up his aggregate nips for
the simulation and so when we do this
this longest job first algorithm we do
get some some ten percent performance
gain again here what we've done is we've
eliminated sort of collisions between
packets right so in hasten there were
perhaps tokens being generated
simultaneously and having more lanes
remove some of that effect I'll point
out that hasten is actually not
communications bound so it uses only
about a third of the bandwidth between
the FPGAs so that's why we don't get
some higher throughput because Haysom
actually isn't stressing the network if
of course you use some kernel some some
kernel which is in fact producing a
large amount of traffic then you will
get linear speed-up as you scale the
number of lines as you might expect okay
so now we've talked about sort of how we
synthesize the inner FPGA network and
how we actually describe and implement
designs that can be partitioned across
FPGAs now let's talk about a couple
examples okay so we'll get two case
studies airblue the wireless transceiver
and hasten a simulation framework for
modeling multi-course okay so the basic
idea of air blue is we want to implement
Wireless transceivers such that we can
operate on the air with commodity
equipment to test out new protocol ideas
so this works well if the protocol is
simple like 80 to 11g but newer
protocols particularly those of I'm oh
of course require much more area and so
they don't fit into a single FPGA and
that also includes the need for multiple
antennas okay so what do we do well we
just throw another fpga at the problem
so we go from one fpga and are a
front-end 22 f pjs in our from it it's
that simple ok so the baseline 802 11 g
implementation looks like this so you've
got a TX pipeline an RX pipeline and
what we want to do here is implement
some new algorithms final codes because
final ko's is some new error correction
algorithm okay the problem is it's much
larger than the existing it's new so it
was a sitcom in August
it's actually quite good so it's
actually better than turbo in most
respects scared oh yeah I you know maybe
talk offline about how the name came
about it wasn't my choice I always think
of spinal tap ok so anyway basically the
problem with this code though as good as
it might be is that it's much larger
than Viterbi and so we exhaust the area
of the single FPGA yeah it is asking for
ignorance is waistband mean anything
until you stop playing with
radiofrequency that's right so basically
it's it's the part of the the wireless
transceiver between the RF and the Mac
working on on packets right so it's the
thing is taking that baseband signal and
turning it into packets right with error
correction and various other algorithms
running ok so anyway of course as you
might expect we just simply partition
across two fpgas right again these these
little FIFO zero latency insensitive
channels and no source code modification
is required right so that same design
that you would map in simulation you can
map across to FPGAs and meet the high
level protocol timings again high level
protocol timings being at the scale of
tens of microseconds so the latency of
the inner FPGA interconnect is not a
problem and of course also because this
is a largely flow through pipeline with
a tiny amount of feedback here you would
expect that we would have no problem
with feedback latency ok so the second
thing we're going to do with air blue is
actually simulation so often when we're
evaluating protocols we care about
operating points at bit error rates of
one in a billion and of course you know
if you want to test that operating point
you need to generate billions and
billions and billions of bits which of
course is a problem in software because
the software simulator is running at
kilobits now of course the FPGA is
running at megabits so by choosing the
FPGA we run a thousand times faster now
of course we can implement this on one
FPGA so we can simulate on one FPGA and
the question is why would we want to
well the reason you want to is because
the the tools can actually find better
implementations so what we can do when
we take a simulator and partition
to fpgas even worked on one fpga is get
speed up so here what we show is speed
up relative to a single FPGA
implementation most of the speed
outcomes from clock frequency
improvements so we just take the part
under test and we amp up its quantum
frequency as high as possible and this
gives us a faster simulator okay so in
summary basically air blue and n
wireless pipelines in general are these
deep pipelines with infrequent feedback
and at the protocol level we only care
about 10 micro second timings so this is
an ideal solution to sort of take a
prototype wireless transceiver and
actually get it to work on the air okay
so now let's talk about something with a
little bit more complicated
communications graph that is the
processor simulator hasten so what is
hey Sam hey sim allows you to basically
simulate complex multicores so full
cache hierarchy out of order and cycle
accurate so one key point about hey sim
is that it's time multiplexed which
means that we don't say we're simulating
64 processor we don't lay out 64 course
we lay out a single compute pipeline and
multiplex it among all the course just
like smt okay and of course with that
approach it's very easy to parameterize
the design for scalability so of course
hey sim can go anywhere from one core to
10,000 course the question is whether or
not you can actually implement it on the
FPGA yep
I'm
here your time slicing architectural
state on the same underlying logic
substrate that's right you're not
dynamically provisioning micro
architectural resources those tight
spaces that's right it's much more like
conventional Tara escamole ready if
you're gonna take a little technology
so anyway it is multi-threaded yes
and of course it has a complex
communications graphic lots of feedback
right so it's different than the than
the wireless pipeline in the sense that
all of these parts are communicating and
moreover they're communicating almost
constantly although the time
multiplexing is going to help us cover
some low agencies okay so what happens
we map hasten to multiple fpgas well the
first thing to notice is that on one
FPGA we can map 16 cores and then on to
FPGAs we can map you know more than 100
right again this is because in hasten
this time multiplexing means that we're
not replicating the entire structure of
the processor to add another core we're
only adding some state okay so there's
there's a big constant cost to building
a core model but the cost for adding a
new core is not so high and that's why
we we get this this highly nonlinear
scaling just interested if you look at
modern out at what a microprocessor yep
most of the most of the area is devoted
to the micro architectural state whether
it's branch predictor
tables reorder buffers caches okay and
very little of it relatively is control
state okay so you have to multiplex all
of that and it seems like you know as
you add threads or logical cores you're
going to see a linear increase and all
that state with a job offer that's right
and we do so so part of part of i think
the savings here is that most of these
things are mapping down on too dense
structures right so it's not quite one
year there is some room to scale so so I
mean you know going from 16 cores to to
you know a hundred for some of the
structures means just stuffing more data
into a beer am right and for many
structures that means actually they
don't increase in size only some of the
structures are increasing yes you know
the beers to buffer all of the state and
the scale up to large numbers of course
in that works fun I'm just surprised
that's in fact in cache sizes so in
hasten there's an ancillary hasten as a
model not a an actual implementation so
much of the cache state is stored in an
interesting way so we actually
synthesize a cache hierarchy and so this
this hierarchy actually goes out to host
virtual memory okay so if in some sense
that that cache hierarchy is fixed for
any choice of course right right and the
pressure on it of course changes and its
performance will change the more course
you have the more misses you will take
but the size in terms of fpga area is
not changing okay yep visual meaning you
are using ddr3
he so so I'll actually talk about how
the memory hierarchy works in detail in
a few slides I think it's actually very
interesting but we'll get there in a
couple slides looks like we've got
plenty of time to do so it's actually
the first talk that I've made it this
far in this amount of time before you
move on yeah are the different dual vga
ah so again we mentioned that as we add
course we increase the amount of
implementation area but that has impact
on clock frequency so the more things
you try to stuff on the FPGA typically
the worse that the tools do again we're
not we're not we're just naive users of
the tools we're not trying to float on
everything right so we just take
whatever frequency is given to us by the
tools and so basically what happens is
if let's let's take this bar for example
right so this is say 36 course right so
either a 64 a maximum 64 implementation
or a maximum hundred twenty-eight
implementation can handle this model
it's just that because the maximum 64
implementation is smaller you get a
higher clock frequency and so you get
some performance benefit as a result
okay so one last thing to note is how
much performance you lose going from one
FPGA 22 fpgas so basically it's these
two bars here writes the gray bar is a
single FPGA implementation when we go to
two fpgas we lose it most maybe half of
our performance okay this is already
much much better than the traditional
tools which would lose maybe an order of
magnitude or more in terms of
performance okay and of course as we
scale the number of cores we can cover
more latency and so our performance
comes back up right so in summary single
FPGA gets filled at 16 course but with
multiple fpgas we can go to 128 and
actually now we're trying to build a
thousand core processor on some Richard
yes actually
beginning to do a commercial tool to try
this awesome we never attempted to run
the commercial tools in part because we
think that that's going to require major
surgery so the commercial tools are not
quite so easy to use they usually
require that you do some modification to
your RT else anyway so and also of
course you have to buy a box that costs
a lot of money damn you later boxes are
not cheap uh yeah I mean we talked about
it and we decided you know it wasn't a
productive exercise yeah yes so it's
sometimes your previous graph here is is
um it's not necessarily with people make
certain approaches system it certainly
that is that first they have a
throughput requirement and then they
build hardware sure this is sort of
cloudy I making that making that sort of
design today's a little bit because of
the back patio multiple acts how how
would you see somebody so I'm a big
believer in getting a system to work and
then understanding its bottlenecks
before trying to optimize so I mean yeah
you have a throughput target but it's
very hard to know where bottle exodus
system are particularly a new system
without actually having something
working I view this tool as first and
foremost enabling implementation so it's
entirely possible that we'll get
something to work here will discover
that there's a bottleneck right where
the bottleneck might be could be I mean
my feeling is probably you know the
compiler is not going to produce the
bottle like that there'll be some
intrinsic bottleneck either in the NR
fpga throughput or maybe in memory or
something like that and then we go solve
that right that's just my approach to
the problems in general is get something
to work first and then debug later so I
also ask the question if we have this
requirement of running all these
channels between fpgas would the
architecture that you hand code be
substantially different from the one
with the compilers producing
automatically for you so I think that's
another way to look at the problem right
and and I think if you if you consider
it that way the answer is probably not
that you know at the end of the day
you're going to be building this route
our infrastructure anyway it's just
you're going to have to go through the
pain of debugging it by hand and and
moreover if you make any slight
perturbation to it you'll have to rework
the whole system right so it may be the
case that you can do the kind of longest
job first optimization I'm advocating
but I'd hate to have to write that code
myself something like that yeah but how
much soon is redoing the driver
all the driver beg use that will be
easily another machine how much work
into the bow so it's actually very
simple right so we abstract that layer
it's just being a FIFO so if you look at
for example the transit so so we use
these high-speed interrupt PJ
transceivers right so basically all we
have to do is get the core code test it
out make sure it runs and then abstract
it is a FIFO and feed it into the
compiler so actually it's quite
straightforward if you look at something
like pcie right going between host and
end and fpga that's a little more
complicated but the end of the day it's
still a FIFO as well right and it's just
multiplexing on top of that fifo i mean
at the end of the day if you look at the
drivers that you're writing this is what
they look like and it's not clear to me
that you're going to do better than than
this although of course if you are doing
better there's probably a way to
generalize what you're doing and feed it
in here right I mean Rueter is just
generated Reuters just a phase of the
compilot right so you could easily come
up with a new route or architecture and
test it out right again that's the
advantage of the compilers that makes
things like that easy or anyway okay so
now let's talk about resources in
multiple fpgas so i'm going to talk a
little bit so so again right i mentioned
this is the very beginning if you
remember all the way back what we get
when we get more than one fpga is access
to more both the most obviously more
slices but we also get more access to
memory okay and there's this analogy to
multiprocessing here right where if we
have two cores and two threads right
both threads get a full cache hierarchy
of their own or at least parts of the
cache hierarchy and so they run faster
so what we need in the fpga is an
abstraction to sort of allow our FPGA
programs our HDL is to exploit these
resources so we need an abstraction
layer between us and the physical
devices so what I've shown you to this
point is an abstraction for
communications right these channels
abstract the communication between FPGAs
now I'm going to talk about extracting
memory in fpgas so basically what we've
got is this very simple interface and
this is how we'll do memory in our
designs right so just like a beer am
you've got read request read response
and right so a very simple interface
right is that clear to everybody so a
few points about it one we have an
unlimited address space right so this
will permit us to specify any size we
would like even if that size doesn't fit
on an FPGA so if you specify 32 gig of
Ace you probably don't have 32 gig of
DRAM you still write that down and will
provide a virtualization infrastructure
to back that storage space run as slow
as you want sure okay you also have
arbitrary data size of course right so
again it's a parametric interface so if
you want 64-bit words or 36 bit words
will generate the marshalling logic for
it okay and then finally again as I've
said before it's latency and sensitive
right you aren't writing a program
assuming that there's going to be some
latency that you get between the the
read and the some some some fixed
latency right so you actually write your
program in a way that that can basically
decouple the read request and response
okay so how does this look on a single
FPGA so what we'll do is each one of
these are a memory client will aggregate
them all together on a ring and feed
them into the onboard memory okay so the
first thing you'll do is have an l1
cache here okay and if you miss out of
the l1 cache and you'll go to the
onboard memory which will be in dram or
SRAM depending on your board and finally
if you miss out of that cash should go
to host memory okay so host memory is
what will take care of this this
arbitrary a large address space and in
the case that you need it huh memory is
the
pc will assume it will assume that
there's a server attached or what so
it's not local derail attached to the so
we'll use the local drm as an l2 cache
right so basically the flow will be
something like this you'll make a
request to your local beer I'm cash you
may miss okay if you do missed and
you'll scurry off to the board level
resource which will be shared among all
the boards okay if you miss there then
you'll go back to his virtual memory
okay and again it depends on your
address space right and how big it is
and how much data you're accessing but
again the point here is if you need the
larger dress space we give you the
ability to describe that if you don't
need it say you know you say I need an
aggregate a gig of memory you will never
miss in this cash for example okay what
is the use case the motivating you space
versus
that would be going down well I think
the most the most obvious use faces
portability right so that's the first
thing is I've given you an abstract
interface and I can build you a memory
hierarchy on any board you want to
implement on including a board that
doesn't even have memory right the
problem comes in hardware designs
frequently when you bake in assumptions
about the underlying infrastructure to
which you're mapping and then suddenly
that infrastructure gets pulled out from
under you either because you build the
next generation or maybe because you
have to do something like move between
boards and at that point if you baked in
some timing assumption that isn't true
anymore you've got to rework all your
code and that's a big problem now again
you're trading something for the
abstraction perhaps right we're
introducing all of these layers so maybe
add a little bit of latency that's
certainly true but the latency and the
various performance loss comes at what
I've use is a very important price you
know the price of abstraction right
imported bility so I can frame it is on
in terms of these caches and as long as
the platform whatever FPGA it is it
doesn't matter if it's styling Cyril
Tara or what generation it is I can run
that design on any board and that's
pretty powerful right we're going
through the effort baking the system
hardware
sometimes meant assumptions
resources available to get good
performance on
is out of hope a first time the first
Maya so again yes again right
performance is critical here right and
and I don't know that we're trading a
lot in terms of performance of course I
haven't ever done the study but again I
can tell you what would it look like if
you were doing this yourself in hardware
right would you have an l1 cache here
maybe we also have a way of eliminating
the l1 cache so if you want to go
directly to the the drm you can
certainly do that we give that as an
option Erastus to Andrew others thesis
work is this code on paper no not having
heard those things I can't okay you were
synthesizing captions out of awkward
ears right industry being around around
around the ship I don't know feat I
don't remember he was you know too I I
don't know yes we're looking right so
anyway I mean you know whatever
technology you have to generate L ones
is certainly useful here I'll say that
much but again the idea is that that
we're providing an abstraction layer and
that's going to be important again right
unlimited address space right fast local
caches and what happens when we map it
is on across multiple F pjs right so
here are two things are happening right
again when we have multiple F pjs the
boards may be homogeneous and then again
maybe they're not so we want some
portability of design right I mean
intrinsically we've already said we have
multiple fpga so we expect asymmetry
right and in fact we may not even know
what pieces were mapping to what boards
so it doesn't make a lot of sense you
know the more you fix a piece of a
design to aboard the less of this this
automation that can actually happen so
what are some cases that can happen here
so one we automatically route clients to
the nearest cash even if it's on a
different board right so here we have
clients that are sitting on a board that
doesn't have an l2 and they will simply
route to the local l2 the closest one
even it's on another board okay
yeah so inherently so you have all these
theorems on the FPGA yup each of them
have one or two fours why do you want to
build a central cash as opposed to a
distributed cache each of rights so
remember that each of these clients has
its own brm l1 cache and those things
can soak up all the resources on the
board in fact we're working on an
algorithm now when we do area estimates
for placement so we place design on
board we look and see how much beer am
is available left over after the user
design and we just scale up all the
caches to soak it up each client have
you saving when you say each client has
a cache is that the single cash yes a
private l1 cache each one but still I'm
trying to understand why don't you want
to have so in a processor there's a
single cash because you want to have
limited number of course but if on an
FPGA there's already there are these be
Rams based in with it all these pores
hmm don't you think that having a single
cash as opposed to multiple multiple
ones that you can access your paddle is
that might be a final signature so if I
understand you correctly you're asking
why is it that I don't just give each
one of these guys a place in a shared
chip level beer I'm cash fact that's one
option right we do have a beer m central
cash that you can use of course the
scalability and clock frequency issues
there are pretty obvious right that is
once you have a resource that's being
used by a bunch of guys the multiplexing
logic can be problematic but that's a
perfectly valid implementation will lay
out a half mega ohm egg of beer on cash
and you can use that as your shared l2
if you'd like right but again you'll
never get the clock frequency to that
that you can get to the local couches
right i mean if you have to run wires
all over the chip you got to run wires
all over the chip and it's going to be
slow so is that that trade-off to be
considered too and as I mentioned we do
give an option where and you can disable
the l1 caches if for some reason you
don't want to pay the latency for
example if
a streaming workload or you know that
you're never going to hit no one for
some other reason you can just eliminate
that cache of the done coherence at this
point we have these are independent
address spaces although what we're
working on now is so ideally let me tell
you how coherence works in my mind if I
can just get the junior grad student to
work on it right so basically what you
would do is when you say scratch pad you
would also specify coherence domain so
you would say i want this set of scratch
pads to be coherent and you would
synthesize some directory-based protocol
on top of those specific scratch pads
again the approaches automatic synthesis
right but it would most definitely be
some kind of directory-based protocol
sharing the space again in the
applications that we're considering
primarily being these DSP algorithms
although hey sim is starting to run into
this coherence problem now which is why
we're specking it out because what we
want to do is basically slice that thing
across 16 fpgas and of course then
suddenly the functional memory has the
coherence problem right and so we will
be synthesizing coherence algorithms
soon I expect
so the design space exploration certain
dimension before I mean that's going to
just highly manual process
could those could see is there this is
there some sort of plan that you can see
moving forward what is that something
that automate drive space exploration so
in some sense what I just described to
you with inflating the cache sizes is in
some sense that that design space
exploration constraint to the memory
subsystem so I think if you could
express parameterizations and maybe
their relationship you could look to
maybe have some machine assistance so if
the compiler throws down an algorithm
and that algorithm has some parameter by
which it could be scaled and it turns
out that there's area on the FPGA we
could easily scale it up so generally
speaking I think modeling this is some
kind of linear system or I mean I I
don't know if it work as a linear system
but that's how I would approximate it
something like pecora this thing out of
your colleagues down south i think is an
interesting approach to this this idea
of allocating area two different pieces
but yeah I mean if the compiler knows
how to scale then certainly it could
maybe have some assistance than that all
right so anyway what does scratch pads
do so basically like a processor
architecture I'm sure we've all seen
this diagram before in in general
purpose processors we have a bunch of
pluto's so we have some Pluto Adele one
where we get lots of hits oh oh so this
is a stride versus working set size
right and up is is bandwidth so up is
good so these these values are hitting
in the l1 cache and then we kind of
start we have some region where we're
hitting in the central cash and then
some region where we miss out to have
some memory and our performance sucks
but it still works right so just like in
a processor if you have to go page then
your performance will be horrific and
that's just the way it is so write
programs as good locality I guess oh
alright so what else can we do with
memory sorry did somebody else have a
question except you decided what WI's
the scribe lines there that they're low
points of my point is you don't read
divots that's right on that yeah on that
very last write this David here yeah so
they'll all of those did it ok so this
divot comes from the way which we do our
l1 caches so we actually do sku catches
and unfortunately with sku caches
sometimes you get collisions and some of
the stride patterns produce pathological
collisions in this queue dissociative oh
no these are drunk mapped it so sku
associate drive my couches sounds like a
nasty boy so what is it askew we hash
they dresses into the couch we just do a
straight touch well I ain't cash is
hashed uh yeah yeah yeah but it's it's
not a it's not a linear hash right we do
some x-force right so there's some hash
function perturbing the index of the
cash yeah so it's a skew cash right
you
you know the the the literature the
benefit doing skewed associated
associated caches was reducing the
probability of pop sets and why would
you make a skew function rather than I
standard you know just grab the bins
hash function um well for a simple
reason actually so if you do that in
hasten we're in you're doing indexing
based frequently on some processor
enumeration it can also can be the case
that you actually do get a hot set on a
single block in the cache right so you
still have a hot set in these things
right it's just one entry right so if
you have many addresses hitting the one
entry right so we just want to provide
some randomization there that's all
although we are now working on doing a
set associative l ones but will still be
skewing there too ok so anyway that's
how this works so now you know we
mentioned that we get more memories so
we can actually increase the size of the
cache where we can also introduce new
algorithms so one of the things that we
can do in terms of introducing new
algorithms to memory Harkey is adding
prefetching right again we're optimizing
under the memory abstraction so the user
says I have this read request read
response.write interface and the job of
the compiler is to soak up fpga
resources to make that thing as fast as
possible ok so we're going to do here is
is add prefetching so how does that work
so if we remember back to architecture
class right you have some table right
which you index into based on PC and
then if you have sort of a stride rite
so stripe pattern on a particular pc
then you'll try to prefetch out in front
of that stride to get a new value to
cover some of the memory latency ok so
we can do the same thing in FPGA
although there's one key difference and
what key difference is that anybody
one of these fields is not right
there's no PC alright so there's no pc
in the fpga so we can't use that as a
hint okay so it turns out that hardware
programs don't have a pc which might
make prefetching a little bit more
difficult however balancing this
difficulty is the fact that hardware
programs have a much cleaner access
pattern right so if you think about how
a software program works right you're
passing data through the stack right
you're constantly inserting new accesses
to memory which really have nothing to
do with the data flow of the program and
everything to do is sort of like stack
frames and function calls right and so
that could screw up the prefetcher but
in Hardware of course we don't have that
problem right it's just a clean access
stream so even without pc we can
actually do a pretty good job of
prefetching ok and again the idea here
is if we have extra resources on the
FPGA we can add more complexity at the
compiler under the abstraction and
hopefully get more performance so how
does that work so basically here we have
an implementation of matrix
multiplication and this is runtime
normalized to a single I'm sorry an
implementation without prefetching right
and so of course here we do
matrix-matrix multiplication which has a
very predictable access pattern of the
kind of destroyed professor should do a
very good job with and we find that
depending on the size of the matrix
multiplication size going this way we
actually get a lot of performance
benefit with prefetching and again this
comes because the original hardware
wasn't doing a very good job of handling
the edge cases in the matrix
multiplication algorithm right of course
as the matrix gets larger the edge
conditions are less important right we
spend more time running down the rows
and so the benefit of prefetching is
much less although it's still measurable
ok but for small matrices 64 x 64 the
performance gain is enormous again
because we hide the latency ok so it's
great so matrix multiplication we shall
to be able to prefetch so what happens
with h.264 so h.264 has a data dependent
access pattern still predictable and we
do pretty well here again we get a
maximum of about twenty percent
performance game with prefetching and
again these are codes that have existed
for many years who wrote papers about
them and we're just using them as
workloads again because we frame them in
terms of them
abstraction we can actually extract
performance as we improve the algorithms
in the cache hierarchy okay
so in conclusion latency and sensitive
channels enable automatic multiple FPGA
implementation with minimal user
intervention and when we do such an
implementation we can get higher
performance better algorithm scaling and
although I didn't talk about it we get
faster compilation okay and the
conclusion here the high-level takeaway
is that high level primitives can offer
powerful automatic tools right and as we
move to more complex systems these kind
of tools will be necessary right they
can no longer be in the purview of the
hardware designer to produce these
implementations future work so place in
route so as we know those of us working
with FPGAs place and route is taking a
long time and it continues to scale up
as the FPGAs get larger however the
latency insensitive modules provide a
way of dealing with this and that you
can place it at the latency insensitive
modules independently and then
synthesize a network between them the
general idea is that I long wires with
with long latency which you would get if
you if you do this sort of distribute
approach can be broken with register
stages much in the same way that we use
buffer boxes and asic process so
additionally right if we only have to
recompile a part of the design then
network resynthesis should be cheaper
than full chip resynthesis okay and then
finally hardware software communication
we already do this a little bit but I'd
like to formalize it a bit more so again
the problem with hardware and software
communicating is that hardware or sorry
software is it's not a terminus take
thing in terms of timing but generally
speaking the latency of channels allow
us to capture this so basically you
could have latency insensitive channels
and software latency insensitive
channels and hardware some ensemble
program composed of pieces of software
pieces of hardware and we would
synthesize all of the communication
between them so that's how that would
work and with that I'll take questions
since we're at time you may have time
for one question one compressions and
now I'm dissing water comment you
started off with your isn't saying that
vgl use republication and yet you spend
at United cuz I know your time talking
about processes crosses our simulations
and application for fpgas no because it
doesn't talk to pc so the way yo you
left no it's absolutely does how do
something no no no no no no no so let me
explain to you more of how hasten works
right so I'm just giving you an
impression from what we do and what
you've been talking about this tonight
and you're going to architecture and
yeah I don't care what catches that
does nothing for me well maybe they do
and maybe they don't I mean it just
depends on what your access pattern is
right like you is to see for example or
something or visual or something that
has high data rates going through and
see and then you get into diagnose rates
mobloc amazed well all of these things
have multiple continents right I mean
all of these implementations are
multiple talk to me in fact what is nice
about this approach is actually once you
have a latency insensitive module you
can pick a clock for each module right
if it's beneficial right again you know
synchronizers change the timing behavior
so actually this this model kind of
supports intrinsically multiple
activities to do
and as fast as possible magician sure
well I think I think I think air blue is
actually timing sensitive at the high
level right because you have to produce
a result in 10 or 20 micro seconds and
it's a question of whether or not the
latency between the FPGA is tolerable
same is true in h.264 decoder I didn't
talk about h.264 mainly because I think
the results are not terribly
enlightening they're no different than
the wireless transceiver I mean the
basic ideas yeah we can partition it is
on yeah the bandwidth between the chips
is sufficient and yeah we meet the
millisecond or whatever timing even with
introducing this new latency component
but I you know I think I think what is
is is is different here let me address
address the process of simulation
problem so understand that the processor
simulator is actually a hybrid design
because of course we're not modeling
things like disk we're not modeling
things like most of the operating system
on the FPGA actually that stuff is
running in a software simulator on top
right I understand you can do that
the reflection is more like all of that
work put your mind into a certain set of
problems and that we like to have an
application with specific time
requirement letters etc you end up
fighting and all set different set of
problems with the tools you get much
closer to the tools you won't forget to
understand ucf constraints and that you
mean timing is simply by the hallways
and all the time and then the difference
between synthesis simulation and actual
children becomes a problem usable for
myself so that does more more money
fraction you phrase it as this is good
for application but if I can talk about
basic synthesis well I understand well
any night i only talk about a six
sentences in the sense that this is what
if multiple images were used for in the
past right more the more the point is is
that a decent represents a single day of
a very specific application for sure and
many of their applications and with a
certain set of constraints and throat
incentive sort of characteristics and
the right many other
applications have different white widely
different sure absolutely absolutely
system level behavior did the bandwidth
level behaved Atlantic sure I I don't
doubt it I I would think that this would
accommodate of those designs though I me
that because as we move forward in a
more automation there I think it's
really important to understand these
different spaces and think carefully
about I mean that's a that's an easy
statement to knock off but with a lot of
fpga intuition of the room that's not
lining up with with the approach that
you're taking for many problems so I
think we should sure it'll be important
to understand those spaces corkboard i
may or may not i mean if you have to do
things by hand you have to do things by
hand I mean you know sometimes we have
to write assembly to write and there's
nothing in this that prevents that it's
just that if you can get away with not
doing that then it's probably best that
we stay at the high level
but of course you know can always live
in that world crashed off practice when
you got 10 hours of turnaround thank
proposing that you make a run in over 11
another run is fairly time consuming so
you would spend more time in two areas
measure for example as opposed to say go
ahead as in redundancy what happens for
reusing as much as possible to you're
saying it's different kind of things
you're getting your way then something
else sure I mean I completely believe it
anytime you use the compiler the
compiler may always be bad but i think
the license of history is that compilers
inevitably get very good but i'm saying
island i would like to see more of
umpires before d what do that for III
mean I I completely agree that there is
more work needed here particularly on
the quality of service front but I think
it is possible to model some of those
things in the compiler and get good
answers I think I mean at least I think
so but we'd have to look at particular
applications before we could before we
could get some conclusion all right
thank you very much nope</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>