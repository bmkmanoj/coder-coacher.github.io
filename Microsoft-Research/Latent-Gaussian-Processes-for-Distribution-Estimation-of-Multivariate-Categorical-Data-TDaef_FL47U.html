<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data. | Coder Coacher - Coaching Coders</title><meta content="Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data.</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TDaef_FL47U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
this has been joint work with you can
change is going to give another talk
later today and zubin I'm going so first
of all distribution estimation in
distribution estimation we try to
estimate the distribution that generated
some data set in the simple example of
categorical data where the data set is
just takes values from finite search for
example want okay this is quite easy you
can just look at the relative
frequencies of the different values for
example just use the multinomial modern
what do we do if we have vectors of
these categorical values this turns out
to be a bit more difficult so first of
all where do we come across these
vectors of categorical bodies we have
these in data analysis for example a
person answering series of questions in
the survey or medical diagnosis these
often generate sequences of categorical
values as well with the distribution at
hand for medical analysis for example
for breast cancer diagnosis we can save
a patient from going through unnecessary
examinations by looking at which
examinations which tests are needed and
which tests can be deduced from others
so looking at best concerts an example
why is that a difficult task well do we
are doing have a point over here but no
okay nevermind what is it a difficult
task well first of all and the number of
possible the number of possible vectors
that we have goes exponentially with a
number of values we number of variables
that a vector can take for example in
the best cancer data set we have a 10
plus you have nine categorical value of
variables taking 10 possible values that
gives us 10 to the 9 possible possible
configurations but you only have 600
patients and in these data sets in
survey analysis in best cancer diagnosis
we often have small data which makes it
very difficult even even more
the diversity of the data that we have
is often poor compared to the
exponentially many vectors that we have
in the data set existing models using
discrete representations you look at
again frequency is exactly like what you
had in the mood in the single
categorical case which doesn't work
quite well because of these sparsity
issues even looking at veigar models
taking pairs of variables even that
feels very very fast we took a different
approach we are using later recent
developments in sampling based virtual
infants to develop a continuous
continuous space model we defined our
model as follows intuitively we take so
following the our breast cancer example
from before we take each patient and
embed the patient in a continuous space
so that basically means that we have
some distribution over the embeddings
for the for the patients we also have a
distribution with functions that map
from these patients to the to the
categorical tested so that we can have
for each one so we have the functions
for the categorical test reasons each
one giving us a sequence of K weights
now we use softbank's to take for each
to take these sequences of weights and
discretize them into medical assessments
as we get at the end so again we have
patients distribute according to normal
0 1 we have some distribution of the
functions let map these latent patients
embeddings into sequences of wage which
are discretized to get a medical
assessment we use a sparse gaussian
process like what might explain i'm
going to use sort of normal gaussian
process i'm going to use an
approximation to the model that
basically works as follows the speckers
sparse casting process is a distribution
of a functions where instead of we use a
small number of points inducing points
over here these are in red to support
the distribution what do we have over
here we have in dark blue we have the
data set for example the x-axis
would be the patient's the y-axis would
be a weight the dark blue points would
be the X weight pails and the red points
would be the inducing points of views to
define our function we take these with
we use Z denote the X values of the red
points and new to denote the Y values of
these at points of these inducing points
we define kmn to be the killer evaluated
at Point said which basically captures
how similar the points that are to each
other and defined kmn to capture other
canal between patient and each one of
these supporting points basically how
similar is that patient to each one of
the supporting points that we have
sparse carson process it's just defined
for the conditional gaussian
distribution by waiting the use that
values that these weights take that this
inducing points take the y-axis
according to how similar patient is
which one of the inducing to do this
adds to the locations of the inducing
points and the violence where that is
just how far away it is from each one of
these points this gives us the following
formal bottom X distributor co2 normal 0
1 patient we have a distribution GP pile
over the inducing points conditional
distribution to generate the weights for
our soft ranks we collect these weights
to get a single distribution a it will
go like this page to get full the
softmax a single categorical value for
our examination and for that we get a
medical assessment we want to perform a
we want to find the posterior over X and
you for this model well this uh this
turns out to be quite difficult because
the softmax likelihood is not conjugated
to our GP pyar so instead of so what we
end up doing what you ended up doing was
using we used verizon variational
infants we approximate the approach we
approximate the posterior p of f
p of x fa new given Y using q here is
defined as just put a small Gaussian
bump over each one of our patients our
patient embeddings and we model you as a
joint Gaussian distribution we can only
permit I as a model to use standard
normal distributions this is basically
equivalent to x equals the sum of mean
plus standard deviation x noise same for
you in the same for F but doing this
allows us to add the expectation in our
low bond from the version in France but
we are using with respect to a
parameterless distributions just epsilon
distribution code to normal 0 1 for all
of this we then use Monte Carlo a
anti-gay integration to approximate the
expectation that we had over here which
is intractable we have an expectation of
flocks of pants for the Gaussian process
we approximate that using multicolored
integration where the noise is just a
noise is independent of the parameters
which means that we can take derivatives
very easily and we get very very small
variance for this for this estimate we
take we use adaptive learning adaptive
learning rates acoustic optimization to
optimize the noisy Guardians that we
have over here to find the part optimal
parameters for our vinyl distribution n
s mu and error we then used symbolic
differentiation to give a simple modular
code this piece of code lesson 20 lines
is enough to end the entire model and
infants and a for single for single
categorical value and is extremely easy
to adapt and change we assess our models
and so our model relates to some
existing several existing models in the
field quite nicely actually you can look
at linear regression starting from
linear regression we can look at the
nonlinear equivalent of that we get
Gaussian process location oh we can look
at the discrete equiv not to Flynn
delegation we go
logistic regression a nonlinear and
discrete equivalents of any allegation
give us Gaussian process classification
at the back moving to the fold of a cube
we get latent input models linear
regression becomes factor analysis
gaussian process legacy imposes
regression becomes the gaussian process
latent variable model logistic
regression becomes a model called latent
gaussian model and gaussian process
classification becomes the model that
we're proposing which we named the
categorical latent gaussian process and
the latin girls model is actually quite
interesting that's a linear model that
it basically it works as follows you put
a standard normal distribution of
related space you transform that
linearly and then you discretize the
outputs but sadly that means that you
can't capture multimodal distributions
for example and also then the mode is a
bit cumbersome because it uses a
piecewise approximation to a likelihood
which approximates a softbox likelihood
in the in the model itself we assess our
model comparing it to a comparing it to
the others on the sequence of a
unsupervised tasks involving multivac
attic Oracle data so first of all
comparing the nonlinear model to the
linear model over here we just have a
simple example using the XO relation we
just do our ocean embedding we are given
sequences from the relation like 0 0 0 0
1 1 1 1 0 and the model is to embed that
relation in in our latent space over
here the two-dimensional latent space
the two-dimensional attic space over
here we see the semel-aten space again
and again again and again and again and
again well each pattern left to right
shows the probability for each point to
discretize to a different digit in our
relation the left panel is for the next
filter checked second and third agent so
the bottom model you can see
the top left corner he captures the
digits 110 and the bottom white captures
000 for example well the linear model
can't be captured at multimodal
distribution we next looked for data
visualization we took the binarized
alpha digit data set and we again use
that to delay turn space to try to
visualize the data the alpha digit data
set is composed of very small number of
pictures we have 30 pictures for each
digit each digit is a so we have digits
and numbers 36 in total and each one has
only 30 examples we have these are
composed of binary values we have 80
binary values that we try to embed in
our 2d latent space we have the linear
model again to the left and the
nonlinear model to the light you can see
that we get very fairly nice separation
even though we have 36 classes and we
were not given the labels for these a
for these classes and we still get very
nice separation for the different
classes next we looked at data
imputation going back to our earlier
example looking at whether we can look
at basically looking at whether we can
tell missing values from a partially
observed vector for breast cancer
dataset that would be used for example
to tell where the examination is needed
or whether we can say quite quite
certainly that oh we already know what
that answer is going to be we scraped
the start terrorism data archive for the
tail warning effects on political
attitudes data set that's basically
conceived that data set consists of 17
categorical variables with five to six
values which one and we use the
Wisconsin breast cancer data set from
before nine categorical variables with
10 values each one and we compared
several models just you knew from
baseline distribution a predicting just
baseline mother cook
predicting uniform value sort of all
missing values a multinomial model
looking at the frequencies again the
linear model and the nonlinear model and
as you can see we can as you can see we
actually get some pretty good results
for using the linear model compared to
the dinner and the discrete model again
the discrete model that would not we
have some users in the paper where we
use a biker model over here we just use
frequencies of single variables even
using by graham models without smoothing
you get you get a very bad results
because some configurations some
configurations just don't appear in the
data set and using smoothing you get
still fairly battle zones by the way so
I forgot to mention over he'll be
looking at tests perplexity that's
basically a measure of how much the
model is confused about the prediction
that it has to make so for example if
you have ten possible values and if you
have ten possible values and you predict
only one of them at random then you
would have test perplexity of ten
basically saying that oh and it's I
don't know what I'm supposed to do it's
going to be any one of these ten bodies
lastly we looked at influence or
business we looked at the lower bound as
we optimized by hours up as we optimize
your model and at the standard deviation
that we get from the Monte Carlo
estimate that we used to estimate the
lower bound and as you can see the lower
bound the standard deviation decreases
as we as we as we optimize model which
is quite interesting actually suggesting
that the motor is quite robust when you
some when you sample from the model you
from when you sample from the generative
model you get fairly small noise for the
estimate so future directions why not
scale up the model there's no real
reason why this model should it should
be a ticket one small data we recently
showed a joint
me mark Pavelich and callousness on that
we can scale its past cousin processes
other easily two data sets consisting of
millions of data points and they work
quite well they work they do work better
than the most neighbor poachers given
land of forests certain stuff like that
and there's a paper from Sheffield as
well showing that you can use mini batch
optimization for gaussian processes
these models do scale well we do have an
issue with a number of with the Layton's
that we need to optimize because if we
do use large amounts of data that means
that we have lots and lots and lots of
latent points to optimize over for that
we can use the lake ignition model this
has been used a lot in the deep learning
community usually you would use a neural
net as I echinacea model but why not use
a Gaussian process like as iconic model
as well as alternative model this model
has a using Gaussian process you have
the advantage that you can margin eyes
over some of your inputs you can have
possibly observed data for example you
can have only 20 of your twenty percent
of your data observed and you can still
find a purpose you can still find the
posterior for these data points and
lastly we can very easily use mixed data
by just using your placing the link
function which currently is a softmax by
any for example any other one to get
positive or dinners continuous variables
and all these using exactly the same
influence that we have that we have now
changing one line of code so we caught
the running experiments follow these
basically comprising all of this for
mixed data recognition model at scale
and we hope to where this leads on we
have the stuff that I showed you earlier
we have a poster and github that I'm
going to upload code to that on to the
next couple of days and you're welcome
to use that and play with the code I
mean it's very very simple piece of code
just 20 lines for the basic part for the
basic model efficient implementation
doing caching for the mattresses that's
a bit longer but it still is extremely
simple model to use which is very
powerful thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>