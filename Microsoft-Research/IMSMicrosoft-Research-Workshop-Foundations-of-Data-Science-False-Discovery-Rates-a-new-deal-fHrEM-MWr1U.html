<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>IMS-Microsoft Research Workshop: Foundations of Data Science - False Discovery Rates - a new deal | Coder Coacher - Coaching Coders</title><meta content="IMS-Microsoft Research Workshop: Foundations of Data Science - False Discovery Rates - a new deal - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>IMS-Microsoft Research Workshop: Foundations of Data Science - False Discovery Rates - a new deal</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fHrEM-MWr1U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good afternoon everybody my name is
Ralph Irizarry I am the chair of the
section on biology we're gonna focus in
on genomics and electronical medical
records our first speaker is Matthew
Stephens from the University of Chicago
who has done a lot of work in genomics
and also now more recently in functional
genomics where I think this is more
related to where you have often many
many tests hypotheses tested so false
discovery rate SAR a important very
important part of that and it's also
something that we think extrapolate to
many other big data applications I'll
let Stephen Matthew Stephens explain
this this is gonna work like that great
so before I start on false discovery
rate so I just wanted to kind of you
know one of the things I I like to think
about is how by working together or at
least putting our heads together we can
kind of make more progress and it it's
kind of obvious in some sense that we
ought to be able to make more progress
but it turns out like working with other
people to make progress actually turns
out to be harder than you might think
and my main experience is actually
working with graduate students let's say
and it always turns out that we don't
make as much progress as I expect but I
think some of the tricks that I use with
my graduate students to help them make
progress may be helpful more generally
so when I first sit down with a graduate
student I don't give them the real
problem that I want to solve because
it's too complicated there's too much
going on so you have to like pick out a
little bit of the problem that you think
they might be up to kind of thinking
about and making progress on abstract
the problem - usually when I'm working
with statistics graduate students
abstract them it - a problem that
they're going to be somewhat familiar
with and then when they've started
making
progress on that then we can layer in
the complication and another thing I
like to do is to try and early on
establish some kind of benchmark that
says you know if we if we could do well
on this say simple simulation that would
be an accomplishment we know we were
going in the right direction I actually
think that's kind of another good way
that you can measure Pro measure
progress and you're probably familiar
but you know communities have set up
community benchmarks like the protein
folding community have set up benchmarks
the image analysis communities have set
it up benchmarks benchmarks where people
come together and work on the same data
set and try different methods and
compare them and I think that's been one
of the most effective prototypes for
people actually making progress together
so I think those are two things you
should think about when you're trying to
talk to other people and get feedback on
on your problem and the last one I want
to talk about is we recently I've become
increasingly aware of the benefits of
building what I'm going to call generic
tools so what do I mean by generic tools
well here are some examples you've
probably heard of linear regression
k-means clustering principal components
analysis these these are probably are
not the optimal tool for any given
situation and yet they're really widely
used because they're so generic and the
problem I'm going to talk about is maybe
not quite as generic as those but still
become pretty generic is the problem of
estimating false discovery rates and
more generally in this in this talk I
want to outline ultimately going to
outline plans to develop generic tools
for what I'm going to call shrinkage
estimation but I mean a motivate them
through both discovery rates so so
here's my abstraction of the generic
problem so in a genomics we often might
measure lots and lots of things and we
measure them with error so this results
in estimates of lots of effects so just
to give you a simple example you might
be interested
the difference in average activity of a
gene between males and females or
between humans and chimps or between two
two groups so b2j here the effect that
you're interested in is the difference
in the activity of gene of gene J but
you'd and yet so you're interested in B
2 J but you have a noisy measurement of
Peter J which I'm going to call B 2 J
hat and you've got some kind of measure
of its precision standard error and the
typical analysis turns these things into
z scores or T scores by dividing the
estimate by standard error turns those
into p-values and then applies this
magic software tool Q value that will
give you an estimate of the false
discovery rate any given threshold gamma
so the false discovery rate for my
purposes I mean asking me to give you
this Bayesian definition of the false
discovery rate which is the probability
that B dejay is null given that it's
p-value is less than the threshold so
the entry idea is that if you if you say
everything is more significant and sorry
everything that exceeds a threshold is
significant and you call it a discovery
the question is what proportion of those
things that a significant will be
actually false discoveries so this is an
estimate of that quantity so Q value is
a kind of a generic tool you just feed
it a bunch of p-values and it spits out
false discovery rate estimates that's so
how does it do that well here's a simple
example so here's a distribution of
p-values and the as you we're trying to
estimate sorry any given threshold let's
say 0.1 what's the proportion of things
that exceed that threshold they're
actually false discoveries well we don't
know which of these p-values are false
discoveries and which are true but we
see a nice enrichment of p-values near
zero compared with the expectation under
the null which is of course uniform so
if Q value uses this idea that the
p-values will be uniform under the null
so kind of draw this horizontal line and
say well that's what we expect the
people
to look like under the null so if I draw
this vertical line at point one then I
can estimate among the findings that
exceed this threshold the area B tells
me roughly how many are null and the
area a tells me roughly how many are
alternative and although I don't know
which a which I can estimate the ratio
of those areas to get an estimate the
false discovery rate so you could ask
well how does it know how high to draw
this line and the answer is well it just
draws it as high as it can by looking at
what's going on over here and just
assuming that everything near
essentially that everything near one is
is null right and that's basically as
high as it can go if it can't draw the
line here because then we would have
seen more people who's near one so that
that's it that they aim there is to be
quotes conservative so I just want to
point out that I generic procedures have
really an outsize impact so this still
astonishes me the the original paper
introducing the FDR has been cited over
27,000 times that's more than three
times a day for the last twenty years
and in fact I I went to update this
yesterday as I was preparing the talk
that I last looked a year ago as it
happened and there were well there were
I think I I think we were over to 6,000
additional citations in the last year so
it's nearly 20 times a day for the last
year I mean it's out of control
okay so yeah yes so so generic
approaches can have value so let's get
in the game and try and develop some
generic approaches so when I look at
this approach I just want to I guess I'm
going to mention two issues that I'm
gonna I'm gonna actually change in order
to at least potentially in some
situations improve the approach so the
first problem I wanted I'm going to call
it a problem is that this sorry this
previous approach assumed that all the
p-value
near one a null as I said that's
actually I that that never seemed like a
unnatural assumption to me in fact the
way I drew the picture before it always
seemed a very natural assumption to me
so it never bothered me until you until
I started looking at and I drew this
same picture in the z-score space so you
can just draw the same picture in the
z-score space and you get this picture
so now I've put the alternative ones on
the bottom just so you can see the shape
so the blue ones here are the null
z-scores so they're normally distributed
and these are the alternative z-scores
and what you can see is that well
there's this big hole in the pattern of
the z-score distribution there's zero
and and it doesn't really make sense
even if there was an effect you would
sometimes see a z-score near zero just
by chance and in fact what I want to
claim is that this whole big hole here
is really quite unnatural and not what
you expect to see in practice at least
under some assumptions the the other
thing I'm going to say is that by by
basically taking this beta hat and a
standard error and turning it into a
single number the z-score or the p-value
you're kind of throwing away some
information and particularly you're
throwing away that the idea that some
measurements were more precise than
others so in in genetics for example you
often have some genes that are low
expressed and they're harder to measure
and essentially they they have a lower
signal-to-noise ratio in a higher
standard error than the high express
genes and this can cause a dilution of
signal so just to illustrate that I just
did a little simulation where we I
simulated the effects from a normal
distribution and I had just assumed
there were 500 good observations which
have a small standard error and 500
really lousy observations which have a
really high standard error and then if
you look at the distribution of p-values
of the good observations you see a nice
distribution and you know you can see an
inflation there 0 indicating some signal
but if you look at the distribution of
the poor people use
you get essentially a uniform
distribution because even though these
are simulated under alternative
hypothesis it's a hypothesis where
you've made really noisy measurements so
you have no power and so the p-value is
essentially a still uniform and when you
pull these two groups together you get a
dissension the bad measurements are
diluting the good measurements so if you
ignore that during the analysis you'll
get a different estimate of the false
discovery rate and particularly when you
do it the analysis of everything you get
a much higher estimate of the false
discovery rate 24% then if you only look
to the good people use alone that is if
you said look some of these measurements
are lousy I'm just going to ignore them
which probably seems like the right
thing to do if some of your measurements
are really lousy so the idea is that
translating your tube numbers into one
number loses information that some
things are better measured than others
and that can kind of dilute your signal
okay so okay so what are the ideas one
idea is to instead of using one number
use two numbers for our generic
procedure so instead of having a
procedure that's going to take P values
we're gonna take two numbers for each
measurement the beta hat in the standard
error and the other thing I'm going to
do is introduce what I'm going to call a
modeling assumption which is essentially
that the true distribution of the
effects the beador's is uni-modal about
zero so this is this is the this is not
always going to be true but I'm going to
argue that it's often plausible in
settings where beta equals zero is
plausible
then Betar very close to zero should
also be plausible and in fact it's often
the case I think that the larger the
larger the effect they're less plausible
it it becomes another there are a couple
of other ways you might motivate this
assumption another way is that this
assumption results in shrinkage towards
zero so any setting where you think it
might be appropriate to fit shrink
things towards there this assumption
that the distribution is unimodal about
zero really kind
fits with that assumption and another
way to motivate it is that it's an
assumption that's made almost always in
large-scale regression analyses where
you're trying to choose which variables
are relevant in a large-scale regression
almost every method out there assumes
some kind of unimodal distribution about
zero so a double exponential or a spike
and slab or you know so it's a modeling
assumption that's actually used in
similar contexts but ironically enough
not not so much that I've seen in this
context okay so just to briefly you know
the the idea but we're going to do we're
going to approach the prom very
empirical Bayes which is emphatically
not a new idea
so Michael Newton Brad Efren and his
student Omar moral ed Herron have all
among others suggested this kind of
approach so for example Ephron's
approach is to take the z-scores and to
model them as a mixture of a component
that's due to the null that's the
normals there are one component and a
component that's due to the alternative
which he estimates and essentially what
he does is estimate he estimates F
nonparametric ly and then he subtracts
this part out in order to leave this
part so he doesn't explicitly model f1
he does a non parametric estimation of
that and then he subtracts this out to
get f1 implicitly mirela Darren actually
explicitly models f1 which is much more
similar to what we're going to do so
essentially we're doing the same thing
except importantly instead of modeling
the z-scores we're modeling the beador's
so the model is that the beaters come
from a mixture of some which are 0 and
some which are nonzero but come from a
unimodal distribution centered on zero
and we're going to estimate this
distribution from the data and the
second component is that you know these
are unobserved things we're interested
in our observations are the beta hats
and we're going to connect them to our
unobserved quantities we're interested
in through this likelihood so
we're going to treat the beader hats as
as our data and we're going to assume
that they have a normal distribution
centered on the true value of feeder and
with this standard error and we can
relax this slightly we can say change to
a t distribution which can be helpful if
the standard errors are estimated with a
small sample sizes and this is connected
to ideas from valent Johnson and John
Wakefield the idea of basically doing
Bayesian inference using just by
treating this summary statistic as
having this distribution so I've said
we're gonna estimate G and we're going
to assume it's unimodal so a really
convenient way of doing that is to
assume that it's a mixture of normal
distributions all centered at 0 but with
different variances so the way we do
this is to just assume that there's a
very large number of normals with
variances that go from very very small
to very very big we just put a large
number down a grid of variances going
from very small to very big and then
that only leaves us to estimate the
mixture proportions and the mixture
proportions are not actually
identifiable because these grid this
grid is includes components that are
kind of heavily overlapping with one
another but the actual estimate of G you
get out doesn't depend much on that and
so G is kind of identifiable even though
the PI's are not and it's essentially a
trivial e/m algorithm to maximize the
PI's in this setting so this this by
making the grid finer and finer you can
approximate arbitrary closely any scale
mixture of normals which includes pretty
much everything anyone's ever used in
the in the regression context but if you
want you can be more flexible by using
mixtures of uniforms so uniforms that
start at zero or a centered at zero can
be more flexible and particularly can
capture skewed distributions non
symmetric distributions which could be
important some kind say just to
illustrate that here's a mixture of
three so what we do right you could do
that
we have implemented a variational Bayes
approach that uses a deletion and the
weights but what we do by default if you
like is to put a small penalty or a
small penalty that's a bit too reach
they like on PI zero it's not it's the
opposite of a penalty to encourage the
weight on to PI zero if and then uniform
on the rest
so we there are some details you with
which I'm skipping over yeah so but you
don't need to is the point you can get
you only the only time it matters is
when you start to Adams ask the question
is it exactly zero or is it very very
small and so that's what that penalty
term is doing is just encouraging you to
answer that question it's zero unless
you have strong reason otherwise okay so
here's a mixture of three well here's
three normals and when you mix them
together this is a unimodal distribution
you get out of it here's some uniforms
all these uniforms either start or end
at zero so when you mix them together
you get a distribution that it's
unimodal about zero but it can be
isometric now okay
so although I've kind of motivated this
by false discovery rates this whole
approach actually provides a posterior
distribution for each beated so they I
guess to be explicit what we do is we
first estimate G this this mixture by
maximum likelihood combining information
across all the observations and then we
given G we has to make the posterior
distribution for each Peter J given
Peter J hat and its standard error so
because G is unimodal the point
estimates and the confidence intervals
all get shrunk towards zero and because
G is estimated from the data the amount
of shrinkage that you end up doing
depends on how much signal there is in
the data so if the data say oh there are
lots of really big effects and very few
zero effects G is kind of flattish and
you don't get very strong shrinkage if
on the other hand you give it data
that's very null that looks like noise
then
she gets estimated to be very spiky
about zero and you get very very strong
shrinkage and similarly the data points
that have a larger standard error get
shrunk more than the data points that
have a smaller standard error so for
these reasons we call they call out our
package adaptive shrinkage so yeah I do
is it's adapting both to the overall
signal in the data and to the individual
specific measurement errors so just to
show you the results from this approach
this was the results of the q-value
approach effectively and when we do our
approach you get you know a very
different distribution of z-scores
because of our union modal assumption
and when we analyze these rather you
know naively simulated data with good
measurements and bad measurements when
we measure all B when we analyze all the
data together we get essentially the
same results as if we only analyzed the
good ones and that's because of this
likelihood component with the very flat
that the observations with big standard
error have very flat likelihood and
essentially don't change the estimate of
G so they don't change the inference for
the good measurement that's the idea we
actually if you're sharp eyed where you
get few fewer findings in this
particular simulation than Q Ballard
that's the exception rather than the
rule actually oh okay my figure this
appeared
hmm okay
so I can't show you the let me just
mention so okay so that this figure
shows that when you simulate data under
various scenarios we still are
conservative in our estimation of the
proportion of nulls so people in
genomics are especially worried about
being anti conservative and declaring
things to be you know to underestimate
the false discovery rate so that doesn't
happen
except you if you simulate a bimodal
situation which of course breaks our
union modal circumstance then you can
force it to be a bit anti conservative
but it's not too bad
sorry the the figure didn't come out
yeah I made it on a Mac so you can also
ask well how well do we asked to make
the underlying G and one answer is well
we do pretty well in the middle of
course the tails of Geo a lot harder to
estimate so a bit more we're not quite
so reliable in the tails but a general
point is that estimating G is what's
known as a deconvolution problem and
Brad Efrain talked about trying to
attack the problem this way by it pretty
pretty much the way I've described it
but without the Uni modal assumption and
what he reported that essentially he
found it very difficult to deconvolve an
estimate G was very unstable and one of
my points is if you're willing to make
the assumption that G's uni-modal it
basically solves this problem of being
incredibly unstable and becomes
incredibly stable and that's what this
figure shows ok last two minutes I'll
just mention one problem that arises
when applying these methods in practice
is that sometimes you start to estimate
that there's a lot more signal in the
data this uni modal assumption which
allows for there to be a lot of very
close to zero but nonzero effects has
Leah has the
potential to tell you sometimes you fit
it to data and it fits of AG that has
very little point mass at zero but lots
of mass near zero and then when you ask
the question alright am i finding zero
the answer you get out from your
bayesian calculation is no because the
prior is very weak on zero itself it's
got lots of mass near zero but not at
zero and you start to report that
everything's likely to be nonzero even
the ones with P bellies near one so this
can be disturbing to a geneticist who's
come and yes exactly
so yes instead of asking the question is
it zero we can ask for example is bida
positive can we be sure which ones are
positive and which ones are negative and
in fact I mean I'm saying Gelman here
who's the most recent person I've seen
pushing this idea but to key in his 1961
paper actually says we should be
focusing on this question and he the way
he says it suggests that there were lots
of people at the time would have agreed
with that it wasn't like it was he
didn't seem to be pushing that as a new
a non-local prior which means
so the non-local prior doesn't allow it
smoothly decreases the density zero at
zero and so so that's actually going
back to the assumption that there aren't
any effects near zero yeah I don't like
that assumption cuz I don't think it I
think there are lots of effects near
zero so I prefer to change the question
but yeah
okay so I'm not gonna go up I'm not
gonna go to in some way I'm gonna just
mention because it's a generic procedure
we can apply it to other settings so one
setting we've been playing with is
wavelets where you do a wavelet
decomposition of a signal you get an
the estimate of each you can think of
each wavelet coefficient as a noisy
measurement of the quotes true wavelet
coefficient and you can estimate a
standard error and then you can
basically apply this kind of approach to
shrink your wavelet coefficients you can
also shrink the standard errors I kind
of fixed all the standard errors and
treated them like they were god-given in
this but actually if they're estimated
using only a small number of
observations we believe it's actually
important to shrink the standard errors
as well and okay I'm out of time that
we're working on multi-barrier versions
and also everything I said assumed
everything was independent but we'd like
to be able to allow for correlations
before and I'll leave my slide up there
with acknowledgments and take any
questions thanks very much it's it's
basically normal like conquer and mosura
have a really really fast algorithm for
doing approximate nonparametric
likelihood estimation of G yeah and that
stuff seems to work pretty well think of
if you're gonna do an interval instead
of exciting on point mass be good
compare yeah so what the stuff I'm aware
of assumes all the variances are equal
for that they are working on non equal
variances I'm not sure it is but you can
point me to the most reason aware that
there that so there they're doing fully
nonparametric stuff rather than without
the Union modal assumption and what I've
from when I've tried that kind of
approach not specifically there's it's
very delicate which is basically what
Efron was reporting you end up getting
very spiky nonparametric distributions
for G with point masses on things away
from zero that you probably don't really
believe but are the nonparametric
maximum likelihood
some particular mixture of gaussians but
isn't per metric the original so you're
talking about the likelihood right the
normal likelihood yeah so the idea is
that we're making a normal assumption
for the beta hat rather than for the
data so if you can fit a generalized
linear model if you've got enough data
points you might believe that the beta
hat is approximately normal but we're
currently working on you know testing
how well it actually works on RNA seek
data particularly when you've got
smaller sample sizes it's a bit more
delicate so yeah it's a great question
we're working on it each year Microsoft
Research hosts hundreds of influential
speakers from around the world including
leading scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>