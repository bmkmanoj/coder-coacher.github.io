<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session: Solving Random Quadratic Systems of Equations-Nearly as Easy as Solving Linear Systems | Coder Coacher - Coaching Coders</title><meta content="Oral Session: Solving Random Quadratic Systems of Equations-Nearly as Easy as Solving Linear Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session: Solving Random Quadratic Systems of Equations-Nearly as Easy as Solving Linear Systems</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/M6l0IQd1rok" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
all right next up we have a talk on
solving random quadratic systems of
equations is nearly as easy as solving
linear systems by you shin-chan and
Emmanuel kanda's and you Sharon will be
giving the talk thank you thank you very
much and it's my great pleasure to be
speaking here so today the talk is about
some good news about solving a quadratic
system of equations and this is showing
work with my advisor Emanuel candies
okay so what's the problem so imagine
that I have for an unknown vector X
which contains and variables and what we
are given it's a set of quadratic
equations or quadratic measurements
about this unknown object so more
precisely suppose that we obtain M
quadratic equations or quadratic
measurements about this object and in
each equation AK is there a sampling
vector that we are going to employ and
YK is the resulting data that we collect
so the total sample size is M now the
question is can we hope to recover X
from this nonlinear system of equations
so this problem has found a lot of
applications across different domains so
let me give you two examples so the
first examples is about clustering so
consider a collection of nodes and we
are trying to partition them into two
clusters based on some similarity
information between some pairs of nodes
so if we use on a variable I the 1-1 to
encode the cluster membership
information in this problem many times
can actually be reduced to finding a
solution of the following quadratic
systems in fact in many combinatorial
problems we can always use some sort of
quadratic constraints to encode some
latent discrete variables and moving
from combinatorial optimization to
physical sciences and this problem has
found a lot of important application in
particular in imaging applications in
many sim scenarios the optic
detectors or sensors are only able to
record the intensities of a light with
all the phase information lost because
of the super high frequency of the light
so this problem which is also called
phase retrieval it's about how to
recover the phase of the measurements or
equivalently how to recover the signal
itself if we are only given intensity
only measurements which are naturally
just a set of quadratic measurements
okay on the other hand the importance of
this problem might be even more apparent
if we look hey from a different angle so
suppose that we introduce a new variable
matrix variable capital X use it to
represent a little x x little X
transpose the set of quadratic
constraints we have will naturally be
converted to a set of linear constraints
about this new matrix variable so
essentially solving quadratic systems is
about finding a rank one solution based
on a set of linear constraints as a
result our efforts in solving this
quadratic systems will have a lot to say
about more general low rank matrix
completion problem okay unfortunately
the solving quadratic system is in
general a very hard problem because many
combinatorial how problems can all be
cast in this way but this talk is about
some good news so if the system of
equation we have has some random
features they actually these hope that
we can solve it and actually we can
solve it in linear time okay so how are
we going to solve it a first impulse
that came into our mind is that maybe
let's try some maximum likelihood
estimation assuming that we have some
statistical model about the noise okay
for example if we pretend that our data
is gaussian data and then we have some
objective function that looks like a
mean square error if we have persone
data we also have some very simple
formulation for the objective function
unfortunately however simple these
formulations might look like they are
highly non convex surfaces so this means
that if an algorithm it's not designed
carefully it might well converge to some
local stationary points which might
actually be far away from the desired
solution in order to address this issue
a lot of prior work have been proposed
in particular for in presence of random
quadratic systems so when I say random
quadratic systems I mean the sampling
matrix capital a it's random okay so let
me summarize the prior art for you and
to start with so let me start with the
example complicity so first of all
things we are going to recover n
different unknown variables so the
sample compressed is naturally lower
bounded by n okay on the other hand
since even reading all the constraints
basically reading the matrix capital a
already take time n times n so this is a
lower bound for computational complexity
and now we can see where those prior
algorithms so are lying in in on the
under on this diagram so first of all
let's look at converse realization
commerce realization is extremely
powerful in terms of this statistical
accuracy and he achieves the minimal
possible sample complicity but increase
very large computational expense and in
order to improve these computation there
are several non convex procedure that
have been proposed including most remote
remarkably alternating minimization and
wetting of flow but both of them are
still suboptimal in some of the aspects
so in today's talk I'm trying to propose
an algorithm that can hit the lower
limit for both sample complexity as well
as computational cost and this is more
precisely we propose an algorithm which
I will describe a bit later such that
probably this can success with in linear
time
and this can happen with minimal sample
complicity and this is good in theory
and this is also some procedure that is
appealing in practice and in order to
show this let me show you some numerical
plots to make some comparison let me
compare the performance the proposed
algorithm which I will describe later to
some baseline and this baseline will
choose it to be a strictly easier best
baseline which is about solving least
square problem of the same size and
using one of the most popular algorithm
which is conjugate gradient and then we
are trying to plot a relative error as a
function of an iteration count and in
order to facilitate comparison I rescale
the iteration count axis so that each
iteration for our algorithm is about
four times that of the iterations for a
least square problem the good thing
about this risk ailing is the following
after is scaling you can see that the
error curve is almost relying on top of
each other and this gives us the message
that for whatever accuracy that we want
the computational cost of our algorithm
it's just for solving quadratic system
of equations it's just about four times
that of solving ali squared problem of
the same size and this just divides our
title which is solving quadratics random
quadratic systems is not much harder
than solving a lease cuerpo ok so this
these are the main performance that I'd
like to show you and what is missing is
what really this algorithm is and which
I will describe to you and the rest of
my talk ok so this procedure it's a non
convex procedure and typically a
standard non convex paradigm usually
consists of two and stages so we start
from somewhere which is not too far away
from the ground truth view some smart
initialization and this is because
usually around the ground to solution
there is a region which people co basin
of Attraction such that within this
region the function we are going to
optimize is still
converse but it has a unique stationary
point which is the ground truth so if we
are able to start how algorithm from
within the basin of attraction and then
if the following iterations are
carefully enough so that we don't jump
out of this valley and then he freaking
verges in my will converge to the ground
troops and our algorithm will also
follow this standard paradigm so first
of all for initialization one of the
very popular way to do initialization is
called spectral method and which has
been using both automating minimization
and what in your flow so basically it's
a belt so we start the algorithm using
the principle component of this sample
matrix and since this is like a pca type
of scheme and we know that if it works
it basically means that this y matrix
where at n is like almost maximum energy
along the direction of the ground shoes
so this is the ideal case but
unfortunately this is not really the
case unless we have a lot of data so in
practice okay if we don't have that many
data but this is what we are going to
get so there are quite several
directions it's not hard to find them
such that they are far away from the
ground shoes but they why has higher
energy along these directions and this
is not a good news for us we care this
will if we compute a principal component
we is more likely for us to get
something closer to these outlier
directions and the reason that this is
not good it's because when we are doing
PCA of Y and Y is actually a sample
average of a lot of heavy tail samples
so necessary they are some of the
examples they have too high of a
leverage on the principal component so
in order to adjust this issue we propose
you to undo this leverage by throwing
away some large data and so we'd run a
PCA for this
truncate the average of the sample
matrix and this gives us some
theoretical advantage also more
importantly this actually has a lot of
practical advantage so let me give you
some numerical example to suppose that
our ground choose X is this Stanford
coil image which contains a lot of
pixels and if we just do spectral
initialization without any trimming
procedure and this is the initial
initial guess that we are going to get
which we can hardly see any information
from it but if we first do some trimming
which we trim a few data before we
perform PCA this is the image that we
get which you can already see a lot of
information about this difficult okay so
this is about initialization in our next
stage it's about this iterative update
and there are a lot of ways to do this
stage and one of the remarkable way is
the so-called wording of flow basically
we choose the search direction as the
wording of gradient of the objective
function so what Ingo gradient you can
just think of it as some sort of
ordinary gradient in the real Eddie's in
the real case but there is some issue
about it so let me try to plot all the
gradient component with respect to each
data to you so this is the typical
distribution of all their grading
components and z is my current iterate
and axes the ground truth and you can
see that there are many of the gradient
components which are more or less
pointing towards the right direction
which I plot improve but there are a few
of them which are plotting read that
they are actually is sexy fleur-de-lis
large and they are pointing towards some
arbitrary direction which we don't like
and this is not a good thing because it
introduces quite a lot of variability on
the descent direction so it descent
Direction is not really well controlled
and to address this issue so why don't
we just discard these high leverage
samples before we form this descent
direction and the way we detect these
outlier is
so okay let's look at each of the
gradient component if some of them is
excessively large compared to some
typical size then we just remove it and
we make sure that we don't remove too
many and this DP spoopy because we are
discarding some data so necessarily we
are somehow introduces a little bit bias
but more importantly with significance
you reduce the variance and this will
give us the benefit in fact if we do
this trimming procedure because the
grading component is not well control
and in order to avoid overshooting and
we need to choose the sample size to be
very small in order to avoid
overshooting but after do this trimming
we get much more stable descent
direction so we can move much faster and
much more aggressively than the previous
case and this is the reason that we get
to grate computational benefits okay so
to quickly summarize our algorithm it
consists of two stages spectral
initialization following some wording a
gradient type of update rules in each
iteration each step we use a more
adaptive rules by discarding a small set
of high leverage data in order to
stabilize everything and this gives us
an enhanced performance of for solving
quadratic systems and more precisely we
can rigorously prove that if the system
has random features the algorithm will
start with in the right basin of
attraction and after that it converges
geometrically fast to the ground true
solution and all of this happens with
minimal sample complicity ok so this is
showing the growth resulting theory and
this is also good in practice so let's
come back to our image example the
record that after the proposed
initialization we get something that
looks like this and after we run the
proposed iteration for 50 times and then
we get an image that is almost noiseless
and finally just a quick comment about
some
technical difficulties so knows how we
are doing some iterative procedures so
each ate all of the iterates are highly
correlated to each other so this results
in some technical difficulty and in
order to adjust that many of the prior
approaches require that we employ some
fresh samples new samples at each
iteration in order to decouple the
dependency but the approach that we use
here do not require that we can actually
handle the type of iterations where all
of iteration reuse all the samples all
the time and in many situation and this
is this might be the diversion that we
actually want to run in practice so to
conclude we have proposed an algorithm
to achieve the best sample complicity
and best computational cost
simultaneously and the key message is
that we would like to discuss high
leverage data in order to achieve a
better perform more stable performance
and they are quite a few things that I
have no time to talk about but I will
stop here thank you very much
so thank you Jim before we take some
questions can I ask the spotlight
speakers to come up to the front and
prepare for your talks and do we have
any questions
thank you for the talk so my question is
about the future matrix I saw you have
an assumption that the features need to
be independent in this case like it is
essential like we all the methods do
work when my future are correlated with
some kind of a correlation matrix like
so you say the matrix a right yeah okay
so what we are proving here is that when
a has idea random sub Gaussian features
we can do that but this is not very
crucial for the algorithm to work at
least for example many of the cases we
just need these majors to be has some
sort of incoherence so we don't actually
I mean this can not be something like an
identity matrix something either which
is highly structured but if this is sort
of like an structure and has a lot of
incoherence probability than issue work
so for example some sort of light
restrict the isometry properties on some
sort of letter but this can well beyond
this ID gossamer another question
so your contribution combines a
initialization step and the truncated
gradient step which can you sort of
quality of Lee's slip these into which
is the how much of the fact both the two
steps have on the overall performance so
if we do this truncations that lets say
for initialization this the
computational cost is almost the same
that you will affect your sample
completely by a lot and for second stage
if you do this regularization things it
will affect both computational
computational cost as well as the sample
complicity in particular for
computational cost there will be a gap
of largest order n if you don't do any
trimming do you have any further
questions in that case let's thank the
speaker again and move on spotlights
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>