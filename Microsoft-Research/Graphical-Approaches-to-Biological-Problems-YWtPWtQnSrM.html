<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Graphical Approaches to Biological Problems | Coder Coacher - Coaching Coders</title><meta content="Graphical Approaches to Biological Problems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Graphical Approaches to Biological Problems</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YWtPWtQnSrM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay it's a pleasure to have onus for
this earnest of course does a lot of
great work in computational biology and
you know he's practically one of us it's
really great thank you very much so I
recognize something but not everyone in
the audience and I recognize it's a
heterogeneous crowd so if things are not
clear you might as well interrupt me
rather than sitting through the whole
talk and nodding off the way I might do
if I was in the audience instead to now
stop me quickly so what what I hope to
do today is show you how the kinds of
algorithmic work that is being done here
at Microsoft and other places around the
world of course can have a big impact in
biological problems and they have
applicability to a lot of in biological
problems but I'm particularly interested
in the biomedical aspect and in how this
coupled with other new technologies
could really impact human health and
give you a sense of ways in which
technology came back human health I
think it's a little historical
perspective helps this is a plot of
mortality rates the number of people who
died per hundred thousand the population
over the course of the 20th century and
there a couple of trends that are really
very striking here the most striking one
is the least relevant is spike in 1918
which is some of you may know the
Spanish flu but what's really dramatic
is this a biphasic decline first it's a
very sheep steep decline in death rates
in the first half of the 20th century
and then a slower rate of decline in the
second half and this was driven by
technological innovation not always the
most sexy technological innovation so if
you separate this into infectious
disease deaths in red and non-infectious
in blue the infectious disease deaths of
what are really accounting for the
decline and it starts with sanitary
engineering and food science research
that really made it possible to live in
cities and that dramatically caused the
drop in infectious disease deaths and
then about mid century or what most
people think of the pharmaceutical
industry and its development of things
like penicillin that really continued
the decline so at the beginning of the
20th century about half of all the
deaths in the united states were due
infectious disease
and at the end of the 20th century
regardless of the fact we had the AIDS
epidemic which is the slight little blip
here there's really almost no one who's
dying of infectious disease is a
fraction of the total so technology can
have a huge impact on human health now
we've done really well as a society in
conquering infectious disease in the
developed world we've not so done so
well in dealing with non infectious
disease causes the rates of death are
pretty much the same over the course of
the entire 20th century despite the fact
that billions of dollars being poured
into the pharmaceutical history and so
the question is whether there's a new
approach to biology different what was
going on from the over the course of the
20th century a 21st century version of
biology that can get us there and this
is where yes I haven't so this is these
are not my data this is from the JAMA
Journal and I don't actually know what
the plots look like for the developing
countries I don't think there was such a
steep decline in infectious disease
deaths I think it's been much more
stable unfortunately yeah aids yeah yeah
in the United States right if it was
Africa would be very very different yeah
well I can't go to zero right can't go
to zero but you might expect people
would both live longer and so it's you
could add you know this is deaths per
hundred thousand the population so that
number can drop it's not going to go to
zero but it could draw yeah okay so so
for a lot of the 20th century a biology
has been driven by somewhat anecdotal
approaches to biology where someone
stumbles across certain molecule
discovers that's important and then
tries to understand its larger
significance and that's driven a lot of
pharmaceutical development as well but
what we're on the verge of now is a very
different approach which uses a lot of
different high-throughput measurements
to systematically look at different
kinds of molecules inside the cell and
then computational models help us
understand what's going on in the cell
in a way that could predict where
we should intervene and there are a lot
of these measurement technologies the
oldest of the more technologies that
measure RNAs in the cell there are new
technologies measure proteins and the
way the proteins interact with each
other and the way they interact with the
genome and they're called ohmic data for
short because of genomics
transcriptomics proteomics all these
different ohmic technologies just a
shorthand for measuring as much as you
can about what's going on the cells so
that in principle you could measure tens
of thousands of data points about each
cell in a diseased state and a healthy
state and then try to reconstruct
develop clear strategies for how to
intervene so in order to make this
understandable i'm going to do a little
digression sort of everything you need
to know about molecular biology in about
two minutes i don't know if you're
familiar with this for Dummies series
but i like to think of it as books for
smart people with very little time so
i'm going to give you the molecular
biology for dummies course here when we
think about biology we can think about
in many different scales but
fundamentally a lot of biology deals
with how cells and organisms respond to
external stimuli so you could think of
for example of toxic compounds or
mutations or infectious agents and those
obviously can have very macroscopic
effects on biology so animals can new
cells can move both animals and cells
can die or grow but we're very
interested in the molecular changes that
underlie that so the kinds of molecular
changes we're interested in every gene
in the genome doesn't get turned into
RNA and then ultimately into protein
through the process of transcription
translation what happens to those
proteins they get modified did they get
moved around in the cell do they get
broken down and one of the kinds of
modifications that I'll talk about later
so introduces hear about that happens to
proteins is something called
phosphorylation and it's a simple
chemical reaction that changes an oxygen
and hydrogen to this thing that has a
phosphate and other oxygens this is
called a phosphate group there are
proteins that carry out this chemical
reaction and they have unfortunately
names that are not intuitive so the
protein that adds the phosphate group is
called a kinase
and the one that removes it is called a
phosphatase this chemical transformation
is particularly important because a lot
of proteins when they get undergo this
change will go from a state where
they're either active to inactive or
inactive to active or they can be
designated for you moved around to
different parts of the cell based on
this phosphorylation reaction so
understanding oh which proteins are
getting phosphorylated and what happens
then when they get phosphorylated will
play a big role in what happens later
here's a cartoon of some of the things
that can happen so if this is the
barrier between the outside of the cell
and the inside of the cell and various
signals are detected from the outside of
the cell and then that information gets
transmitted into the cell so they'll be
proteins that span the membrane on the
surface of the cell they detect these
molecules and then one of the things
that they can do is get phosphorylated
that reaction that I told you about in
the previous slide and that can cause
two proteins interact that would not
have interacted previously without the
phosphorylation and it goes to change
reactions so that one of these proteins
that gets phosphorylated then can then
cause other proteins gets phosphorylated
and chain reaction that then ultimately
leads to something macroscopic that you
can observe so this is a kind of pathway
that occurs when information goes on the
outside of the cell to the inside of the
cell and those signals then often go
through the cytoplasm into the nucleus
of the cell where they influence how
genes get transcribed so this will be
the symbol I'll use for a gene and under
some conditions the gene gets turned
into RNA that ultimately gets made into
protein under other conditions it's not
what determines whether a gene is active
over here or inactive over here is
whether these regulatory proteins find
the right gene and the genomes sit there
on the DNA and send the signal to
transcribe them and so single coaster
outside sell it goes through the kinds
of phosphorylation changes I showed you
on the previous slide against those
eventually gets a nucleus where they
cause these proteins to change their
binding or change their activity and
then you get jeans transcribed so that's
the core signaling process and here's an
example one of these processes looks
like so you can see this looks pretty
complicated right there are a lot of
signals are coming
from outside of the cell these have to
do with insulin which changes after you
eat a meal they go through a bunch of
different proteins that ultimately go to
these complicates processes that are so
complicated they're not even represented
here they're just shown by words so the
cells take up more glucose or they make
more proteins or and so on now the
problem is that while this looks
complicated is actually a gross
simplification of what's going on in the
cell and in fact a lot of what goes on
and so we don't know about yet and so
what we'd like to do is stop reasoning
by drawing pretty pictures and start
reasoning on the computer and that's
where the algorithmic approach has
really come in can we take a much more
systematic approach find all the genes
and proteins involved in these
complicated processes and let the
computers tell us what's important
rather than saying oh you know gsk-3
looks really interesting i'm going to
spend a career proving that gsk-3 is
important for insulin resistance or
cancer whatever it is so that's the
motivation we're going to measure as
much as we can about what goes on in the
cell and computationally figure out
what's important rather than trying to
do it all in our heads so that's the
goal and what I want to do in the rest
of this talk is explained some of the
challenges that we face when we try to
take that comprehensive approach and
really unbiased by our preconceived
notions and then show you two things we
need to be able to do this to really
integrate everything one is focused on
that process of transcriptional
regulation which genes get turned on and
off and then assembling larger Network
pictures of the whole signaling process
okay so let's begin with the challenges
so the first challenge that we face when
we try to take a very systematic
approach to these biological problems as
we find that a lot of the responses are
not at all with people expected so
people have been studying some of these
signaling pathways for decades and
they've built up those nice pictures
that I showed you before but when you
actually do a specific experiment to
test what the response is in the cell
when you perturb those pathways you
typically find that only about ten to
fifteen percent of the response is in
the expected pathway and a lot of the
response falls in sets of proteins and
genes that are not annotated it's being
involved in any pathway people just
don't know what they do they may know
some specific aspects so they don't know
the larger context of what they do so
this presents both challenges and
opportunities the challenge
is that if you get data from some very
systematic measurement and you plug it
in to some program that's as what do
people already know about it you're
tending to tend to get the wrong answer
right that the programs will tell you oh
well this was a perturbation to one of
these pathways and the factors of
perturbation to this pathway that
constitutes ten percent of the signal um
the opportunity though is that we
obviously are as a field been taking too
narrow a view of what's going on if we
only call ten percent or fifteen percent
of the response the this pathway and
call everything else something else
right and there's a lot of this we don't
know anything about so it could be that
if we had a better understanding of what
the 85 to 90 percent of the response is
we could design better therapies we'd
understand the more large-scale context
of what's going on so there's this first
challenge seems like mostly opportunity
not too bad second one seems a little
bit more daunting and this is what we
find when we try more than one
experimental technique to analyze the
same problem so here's just cartoon
showing some cell that's been exposed to
a perturbation from which we've measured
different things we've measured which
genes are changing an expression which
proteins are changing in their mounts
and which genes when you knock them out
in the cell make an inactive have an
effect on the response to this say
mutation or environmental change and the
Venn diagram of the hits from each of
these assays the things that are
significant I'll look something like
what you see on the right here so
there's there's rarely any overlap
between different data sets that are
measuring ostensibly the same thing in
fact we found that it's usually less
than you'd expect by chance so let's say
you're laughing you should be concerned
right so so so why is it that you get
less overlap and you expect by chance
now one possibility the pessimistic view
is that these assays are just terrible
all right and they're just all noise but
we know that that's not the case because
each of these assays independently is
highly reproducible so if I do this
essay more than once I'll get a Venn
diagram that has eighty ninety percent
overlap so why is it the one I do this a
sign compared to this one I get so
little overlap so that was a question
that two postdocs who in my lab
now have their own labs in their home
countries tried to solve SD a girl
ottoman and la Riva and they were
looking specifically at two kinds of
experiments one is this gene expression
what happened what genes get increased
in expression or turned down when you
expose Excel to some perturbation and
the other one if you knock out the gene
make it inactive in the cell what
happens does it have a phenotype does
affect how the cell grows so what they
did was they looked at lots and lots of
data where they had both kinds of
experiments the gene expression and the
genetic data and just like on the
previous slide there was almost no
overlap between the different data sets
when they were conducted in exactly the
same setting but what they found was
there was a simple explanation which was
each of these kinds of experiments was
biased towards a different type of
molecule so in this case the genes a
change in expression tend to be what
they call effector molecules they're
carrying out some function whereas the
ones that were detecting the genetic
screen when you knock them out those
were master regulators and I'll explain
what I mean with a specific example so
if you look at cells that are exposed to
dna-damaging agent and you look at which
genes the cell turns on when it's
exposed to dna-damaging agent versus
which genes when you knock them out
affect the ability of itself to live in
the presence of a DNA damage age so we
get to two very dif what do you if you
knock out certain genes like the ones
that detect DNA damage then the cells
die when exposed to dna-damaging agent
um so these detectors of DNA damage they
actually run across the DNA they say up
there's been DNA damage here and they
start a response pathway these show up
in the genetic data but they don't
actually show up as differentially
expressed they don't get turned on by
the cell now that actually makes sense
right you think about this analogous to
say a smoke detector you keep the smoke
detector on in the house all the time it
doesn't go on after the fire occurs it's
on before during and after the fire so
the DNA damage detector has to be on all
the time it's not going to show up as
changing an expression in response to
DNA damage by contrast things that
affect the response things that make the
cell stop going so it can repair the DNA
damage or the enzymes that actually do
the repairs those get turned on
specifically when you need them
they're not on all the time the same way
that say this these sprinklers are off
now but we're there to be a fire
hopefully they would turn on now why
don't these effector molecules show up
as genetic it's all the same reason if I
knock out one sprinkler in the room will
probably be okay if there's a fire
because there's redundancy so there's a
lot of redundancy these affect your
pathways so when we have discrepancy
between two different kinds of data it's
not always because there's just noise
and the prosody sometimes they're asking
very different questions about what goes
on if you understand the question then
you're going to understand what the
answer is right thinking of them as the
same and now these kinds of experiment
it was measuring a one this is a
measuring a tube exactly so the
different kinds of experiments detect
different things and there are some
molecules that are on the same pathway
that aren't detecting either experiment
that people have done other kinds of
experiments that detect and so this gave
us the idea that um maybe a way to
reconstruct these complete pathways
would be looking for connections between
the things we get from one kind of
experiment and the things we get from
the other kind of experiment and a long
way then we pick up some of these other
things so in a cartoon sense what we're
going to do is we're gonna have some
cartoon of the cell here and maybe this
is the outside of the cell and some
detector molecule and this is the
nucleus and I've painted things with
different colors based on the different
assays that pick them up and nothing
gets more than one color because general
we rarely find anything detected by more
than one assay but there could be some
pathways maybe no one's called 02
pathway before and those we'd expect to
have some molecules detected by one
technique and some molecules detected by
another so we'd like to develop
algorithms allow us to reconstruct these
previously unknown pathways and then
along the way we'll get information
about proteins that weren't detected in
any of the assays process with different
measuring techniques but not
simultaneously you can't do it
simultaneously that doesn't really
so that should show the overlap
represents pathways in which both
versions of these things play a role is
that poor point if you say no there's a
blue and yellow thing on the pathway
then that will sort of show up in the
overlap oh so they overlap is the same
would the same molecule be detected by
more than one a saying that turns out to
be very rare but we think that most
pathways will have contributions from
all the different kinds of molecules so
what do these data look like if you
leave it back you know various
experiments and assays and then these
pathways like biophysical models or
something like this all right and you
just have like these tables of disjoint
datasets or something like I'm trying to
figure out that yeah so the kinds of
data this might become a little bit
clearer as I go through but the kinds of
data you'll get it would be for every
say gene in the genome or every protein
that's in the proteome you'll get some
number that represents the say the
amount of change between two conditions
and then you can assign a statistical
significance to that and say here are a
set of out of the 20,000 genes in the
human genome these are the ones that
seem to be statistically significantly
changed between condition a condition be
and so that might be for this and then
you get a similar list from watch
proteins seem to be changing at the
protein level which genes when you knock
the mouth seem to be changing the
viability of the organism so you'd have
long lists
put together an address to your
sequences well so the pathway
reconstruction is really what we want to
accomplish so there are some pathways in
the databases those are like those
pictures I showed you the outset those
are very incomplete individual cystica
interactions then you have these
databases of chains that's right and
then there's another kind of of
information that I'll bring in a little
bit that represents the links between
proteins that may not have been
annotated as pathway yet but do you
represent physical links among these
proteins are between proteins and genes
yeah other questions these are good
questions okay so we want to be able to
find the connections among all these
different types of data the risk is
though that that we get these giant
networks that really are uninformative
we lovingly in the lab call them hair
balls right and a lot of these
algorithms you take a small data set of
things that came us this looks
significant from these assays maybe
there are 20 genes and 30 proteins and
then you end up with a network that has
three hundred or a thousand or ten
thousand genes and it's very hard to
figure out what you would do with that
that would actually lead your deeper
biological insight so we need Hughes
some algorithmic cleverness here so
approach then is going to be to try to
find these networks but try to find
compact ones and that's where a lot of
the the work goes in the other thing
that's important to know about the
approach has to do with with how we
treat certain kinds of data so some of
you may be familiar with the idea that
biology has what they call the central
dogma I would say you should be afraid
of any field where they claim they have
dogmas and they're proud of it right but
the central dogma of biology is that DNA
gets converted to RNA which gets
converted to protein and it turns out
it's much much easier to measure changes
in RNA levels than is to measure changes
in protein levels or various
experimental reasons so for a long time
that people have been assuming that
changes in RNA levels were decent
proxies for changes in protein levels so
I measure something systematically about
all the rnase in a Cell and I say okay
well that must be true for the
corresponding proteins
and therefore I can do something about
consequences for the cell recently
people that actually able to
experimentally test whether that's true
and it turns out that it's a really
really bad approximation so on the
x-axis are RNA levels and on the y-axis
is a corresponding protein level for
each of those jeans and you can see that
yeah sure there's some crude correlation
but there's a huge range of variation
three orders of magnitude for Bernie you
know all the rnase of the same level I
mean if you is a variance issue here too
nobody's so from different experimenters
instead of the same condition repeated
same condition thank you yeah okay so
it's not an issue just a noise it's an
issue that there are regulatory
processes so once you've got the RNA
doesn't automatically get turned into a
protein and not every RNA gets can learn
turned into protein at the same rate and
once it does get turned into protein
some of those proteins get broken down
quickly now there's last long time and
that's what causes the 34 order of
magnitude variation so for the longest
time people have been saying okay RNAs
could turn into proteins to my RNA data
which is quite abundant will serve as a
proxy for whatever the consequences are
the increase in the proteins and that
turns out to be really bad idea so then
we're faced with this problem there's a
lot of information about RNA levels what
are we going to do with that right we're
just going to throw it out so yeah this
is that each of this point sorry for my
different okay each data point is
different protein right yeah so what do
you do with it well it's terrible proxy
for this but what actually is a fairly
direct readout is of this process that
turns the jeans on so if I have more of
a transcript it means something turned
that gene on right if the lights go on
something was touching the light switch
so what was touching the light switch it
was these proteins that bound to the
genome and changed either their binding
or their activity and so we're going to
try to use those RNA levels as evidence
of this a proximal upstream process
rather than these downstream things
they're badly correlated and so what
I'll talk to you about today is how to
actually do that to take a gene
expression data which will be
represented down here by these boxes and
use its map back to the DNA binding
proteins that sat on the genome it's a
turn on this gene or turn this
and then once we've been able to do that
then we'll look for connections to other
kinds of data so will then have data
from other kinds of experiments about
proteins and we'll look for the links up
to that so we'll take this bottom-up
approach gene expression to the things
that turn down the gene expression a DNA
binding proteins the things that turned
on the DNA binding proteins all the way
up so in order to do that we need to get
good at two things one is seeing out
this transcriptional regulation question
which proteins are binding to which
genes and when and after we've done that
we can then try to reassemble this
network model and so what I'll show you
is just a little bit of how we do each
of these steps okay so there's one way
you can measure protein-dna interactions
was just basically cheating you can
actually do an experiment one protein at
a time figure word binds in the genome
and that is called genome-wide chromatin
immunoprecipitation it's kind of a long
name it's been done successfully we did
it with a guy named brooke young the
Whitehead Institute for almost every
protein that binds the genome in yeast
it turns out that it's a lot lot easier
to do this in a single cell organisms
like East and is in humans and in mice
is virtually impossible do this on a
global scale so even this giant
multi-institution effort to map out all
the things there's only covered about
ten percent of all the transcription
factors these DNA binding proteins and
only under very specific conditions
probably not under the conditions that
any scientist looking at a particular
disease would be interested in so people
have tried various computational
approaches to fill in the gap and one
idea is if there's some change between
an active and inactive state of the
protein well maybe it's well it must be
because there was some change in the
regulators the light switch Turner
turning on or off and one thing that
could change the the regulators to be if
they themselves are changing their
amount of transcription right so if the
gene that encodes this dash protein is
off then the dash protein is made it's
not sitting in the genome and then its
targets will be off so people have
looked in the gene expression data
itself or the evidence of the regulators
and tried to build a self-consistent
model of what's going on and there's a
lot of literature on this these
representations you'll see usually the
red boxes represent a gene on the rose
and condition
on the columns and if it's red it means
that the gene is increasing expression
in that condition and if it's green it's
decreasing and so people look for the
self-consistent models where there's
some protein and it's all under these
conditions and that and that's why these
are on and so on and so forth activators
depressors and there's a huge literature
on this but all of these are based on
the assumption that the transcription
factor the regulator will itself be
regulated by the same mechanism so that
if I've more of the activator it's
because I've got more of the RNA for the
activator we've already seen that that's
not necessarily the case right because
there are all these other mechanisms
right there's very poor correlation
between RNA and protein levels so it's
more realistic to assume nothing about
whether the regulator itself is
regulated level of transcription and so
working with an emo who many of you know
Tony has actually done some nice work
using layton to graphical models to
actually try to discover some of the
regulators that are not themselves
present in the transcriptional data and
you can ask Tony for the details of this
but the the basic idea is the following
we're going to have a graphical model
where each node starts off as being a
gene and the edges represent the
correlation a minus log of the
correlation between the expression and
some large compendium of data and so on
Emma developed these kinds approaches
are working with a lot of other kinds of
not gene expression data here's an
example of hers looking at co-occurrence
of words in text and so you might
imagine a lot of correlation in the
occurrence of these words and text which
could be better explained if you
introduce some hidden nodes in the data
right so all of these teams on the right
hand side are British football club
soccer clubs all the ones on left hand
side or American baseball clubs those
terms baseball and soccer might never
occur in the data set but if you put
them in you'd have a better
understanding of what the relationship
is among all of these nodes and so we
want to do something similar with jeans
where these stars would be the
regulators which might themselves never
appear
is changing in the data set but would
explain why that the black things the
genes are changing hey so uh her method
in an instant she starts off with with a
minimum maximum spanning tree where the
nodes again are the genes the edges are
weighted by the correlation minus log of
the correlation and then does a local
search to try to figure out what the
relationship is among the nodes in a
local neighborhood and whether it makes
sense to add these hidden nodes that
would represent parents of these of
these child nodes and you can use
bayesian information criteria to decide
whether you've added too many of these
latent variables or not so in her kinds
of problems here's an image a problem
that she had where they were detecting
objects images and so each node here
represents an object detected an image
and if you zoom in you can see they've
added a hidden note here that explains
why you've got a cold currents of desk
and book and screen and clock and
monitor and presumably this is an office
setting whereas they'll be other
settings where you have sofa and
armchair and television that might be
hidden node might represent a living
room setting and so what what they did
working with Tony was to try to apply
this now to gene expression data so
you've got some vast compendium of gene
expression changes in yeast again each
row here represents one of the genes and
yeast in each column represents some
condition the East were exposed to
different types of environmental
perturbations so they look for the most
variable genes across the data set used
exactly the same approach to build the
light and tree tree itself little hard
to understand what it means and so some
of the the hard work here was how to
interpret this and figure out whether
the early nodes are actually telling you
something biologically meaningful and so
the approach that they took was to look
at the neighborhood around each late
node look at the genes that are in there
and see whether that grouping of genes
together correspondent to genes that we
know to be targeted by the same
transcriptional regulator from the
experiments that we had done a long time
ago to actually physically determine
that and the results are pretty good so
if you compare the results that the
they get with the latent true graphical
model to the approaches people got and
previously lip by looking just for
self-consistency in the data you look at
a bunch of stress conditions which have
been swell studied and the regulator the
late nodes that that the late entry
graphical model comes up with correspond
to transcriptional regulators that we
know from other data are actually
regulators of stress response
correlation structure of the RNA
activity that's right and then the light
nodes we believe should represent
transcription factors we ask do they map
do they line up with known transcription
factors and so they do when they map up
with known transcription factors that
actually our regulators of the stress
response whereas this other kind of
approach which looks for
self-consistency it found some genes
that seem to be consistent oh maybe this
is regulated it turns on these genes
that's why it's red when they're red but
the identities of these things do not
map at all to what's the known biology
of stress response do they overlap a lot
or is it actually reason to believe this
is tree like for the things we know no
it's probably not tree like yeah so
that's an approximation that we're
making good oh good save one thing is
like because it doesn't seem like you
that computationally constrained here
roni work maybe stream model is like if
you just try other ways there's a whole
bunch of kind of consistent ways to
build these trees at the models right
and the drawing like you know tinker so
like how stable because you're getting
kind of good things here but if you run
it will you get other stuff that looks
interesting because it's not quite a
tree yeah so how stable solution maybe
I'll turn this one to Tony you ever
talked with the extremes and not
stability in narrow regions
what the red edges between these two
lines mean I mean what are they oh yeah
it's a bit but between these nodes these
actually represent known physical or
other functional links among the
proteins so the reason that we get a
highly a lot of these hid nodes were
highly connected in or eyelet we're
close to each other in the latent
restructure and that makes sense because
the proteins themselves are known to
interact with each other so so one level
up is just the transcription factors but
other transcription factors did it
affect other transcription factor is it
or are yes are you actually discovering
things sort of beyond level so we get
rid of the tree structure we actually do
the analysis so we look for things in
the neighborhood of the latent nodes to
try to interpret need all the way down
to those yeah see so you don't you
ignore any kind of hierarchy that's
right and I'm not sure that there is too
much biological meaning to the hierarchy
you get out of the latent tree for
precisely the reason you talk about
because it's not likely to be a tree
structure extracts them nice
correlations exactly
visit toy sample the previous one that
you described rustling 5190 lift was
being matching with a current think how
many these red nose were there that so
this is not a for example this is a
piece of of the that bigger one yeah how
how big was the overall instance yeah we
start off with I think this 500 genes a
5000 g no we start off with a thousand
genes we threw out 5,000 so we saw of a
thousand genes across 500 conditions
thousand genes across 500 musician so
the matrix is a thousand by 51 so what
are the conditions mean in the tree
that's being you know what that's like
that's where the correlations come from
right ok that's a sample size sample
sorry a different Croatian
alright well we can discuss that more
later maybe this kind of approach though
as takes 500 samples takes lard you know
you need a relatively large matrix to be
able to for biology it's a very big one
like you know significant issues are
important yeah that's right and now how
much it's nothing that's right yeah
exactly so if you were doing this on on
text mining 500 would be tiny right and
so one of the big issues we need to do
with this a lot of these statistical
methods are not going to work under the
kinds of settings but what we are trying
to answer biological questions so if
you're dealing with a disease model
you're not going to have 500 different
variations or a thousand or ten thousand
you may have two or three and so these
approaches just don't work in those
settings so we needed a different
approach for that I'm going to just very
briefly try to explain what that is it's
called using epigenomic information and
they do is that this is some cartoon of
the region preceding a gene where these
regulatory proteins are bound we don't
know what they are but there are certain
things that change when these regulatory
proteins bind they're not changes
directly to the DNA but they're changes
to things that are wrapped around the
DNA which is why they're called
epigenome around the genome and there
are individual experience so you know
some of you may have heard some these
terms maybe not histone modifications
DNA methylation important thing is that
there are very efficient experiments
that can measure these epigenetic
modifications which are indicated here
in the cartoon by these arrows in a
single experiment across the whole
genome without having to know what the
identity was of the proteins and so if
we do this and we measure systematically
where all these epigenetic modifications
are then we can look specifically in
those little pieces of the genome the
genome human genome is about three
billion with the be base is long but
there might be a very very small
fraction of it where we find these
epigenetic modifications and so we can
then go and look at the sequence of
those regions and try to figure out if
those sequences give us an indication of
what the proteins were that are
indicated here by the dashed line and so
we have various different approaches a
lot on their base them some sort of
correlation approach where you might
expect stronger copies of matches some
sequence pattern to be occurring in
places where there's a bigger biological
readout I won't go through the details
of that and we can go through and
actually do experiments so we make some
prediction that this you know yellow
protein is bound there we can actually
do an experiment we try to physically
detect whether that y'all protein is
bound to those regions using massively
parallel sequencing approaches and we
can get pretty good results so grads in
my lab Christine recently had two papers
where he showed in a neurodegenerative
model Huntington's disease that he could
use expression data and these epigenomic
features to identify what the regulators
were and this does not require hundreds
of samples it can be done in this case
with two samples I mean a little more
than 24 biological replicates but we're
telling that order of magnitude okay all
right so so we've shown shown you a
couple different ways to try to figure
out what these regulatory proteins are
that are binding the genome now we have
all the information we need to try to
put together the signaling all together
so again we're trying to reconstruct
this pathway some external signal starts
of that whole phosphorylation cascade
that you learned about in the guide
instant guide to molecular biology that
sends some signal to the nucleus these
proteins changing their binding activity
that makes RNA changes right and so Carl
Hong who was a graduate soon my lab in
time is now a postdoc of the sole
recently published a paper with us where
she showed how to combine all sorts of
different kinds of data to understand a
brain tumor called Leo blastoma this is
the one that killed senator Ted Kennedy
for example and it's usually detected a
very late stage there aren't really very
good therapies for its most patients
don't survive more than about a year
after detection so that hope is again
and we're not there yet the hope is that
these kinds of very systematic
measurements would lead us to come up
with new therapeutic strategies that you
might never come up with through the
traditional biology approaches so Carol
focus in a particular mutation that
occurs along the patients who have the
worst prognosis in this disease and it's
mutation in a protein that spans the
external internal boundary of the cell
the cell surface membrane and it changes
signaling through the cell and actually
changes one of the the activity one of
these kinase is the proteins that add
these phosphate groups and so she used
data that one of our colleagues had
collected on which proteins are changing
this phosphorylation
and she had cells that express the
mutant form of this molecule or an
alternative form that isn't the kinase I
because it's been changed so it can't
function that way so she had data on
which proteins change in that
phosphorylation signal and she also data
on these epigenetic information about
where proteins are likely to be bound
and she did a similar kind of regression
approach where I described previously to
try to infer which the regulators were
and so the goal was to combine all the
data make this network model now the one
thing I haven't explained is how we're
going to connect the nodes in this
network going to be proteins and genes
where the edge is going to come from and
here we use databases of physical
interactions that have been detected by
lots and lots of different measurement
techniques some were very focused on
particular proteins jeans and some of
them were very unbiased by any
preconceived notion about what was
interesting and just systematically
measure do I detect protein a physically
interacting with protein B so we have
tens of thousands of these edges and
this giant thing that sometimes people
call the interactome because Y all just
love adding the suffix ohm to anything
they can think of so we have the
interactome okay so we've got this
interactome and we've got the
experimental hits these were the lists
of proteins have changed in
phosphorylation the list of genes that
change in expression the after genomic
data they are pairwise predominantly
that's right you can reconstruct
complexes there are other kinds of
experiments that do detect complexes so
there are ways of doing it but primarily
it's it's in the databases pairwise
interactions and that's how we use it so
what happens if you ask whether the
experimental hits can be connected
together by the detected protein-protein
interactions in the interact oh so good
news and bad news the good news is you
can connect things the bad news is that
it looks pretty much like what you
started with right so you get one of
these giant hair balls at the end and
the reason for that is that the data
consists of a lot of false positives and
false negatives you've got false
positives and these data you've also got
false positives and the interactions and
false negatives so if you try to force
everything to be connected you get a lot
of meaningless information
so the approach that we took was first
to try to take a probabilistic view of
old these interactions assign a
probability to each edge based on the
likelihood that it's real and then try
to find a algorithmic approach of a
sense of the fact that our hints from
the experiments the proteome the
proteomic information the gene
expression data has a lot of false
positives as well so this issue of how
to assign probabilities of interaction a
lot of people have looked at this
they're many different approaches some
of them are Bayesian which are kind of
nice they say how often do does each
experimental technique get the right
answer on some gold standard or two
positive intra- and then you can
calculate okay now i have an experiment
that detected interaction i sorry if
interaction was attacked by particular
experiments what's the probability
that's true there are other techniques
that are a little bit more ad hoc but
there are a lot of scoring schemes that
have the general flavor that can give us
an approximate probability of
interaction for each edge based on what
the underlying data are okay so let's
this is for example now where we've got
some set of gene expression changes as
the squares these triangles or the
transcription factors the DNA binding
proteins that we inferred from all that
epigenomic data alright that we believe
are turning on these jeans and then
maybe we have the changes in
phosphorylation that are the yellow
circles and this is some piece of that
giant hairball and we'd like to have an
algorithm that could help us find the
things that are highlighted with the
purple line it's a piece of that
hairball that's most likely to be
functionally linked to each other by
these physical interactions in order to
do that sometimes we have to throw out
certain data points say well I can't
trust the connections between this a
measured phosphorylation change in
anything else because these edges maybe
are too unreliable and that when I'm
trying to figure out what include this
okay it's some long chain of
interactions but maybe these are all
really high confidence and I should
include them or maybe they're low
confidence I shouldn't so we want an
algorithm that sensitive that approach
and what we ended up doing was using
this prize collecting Steiner tree
algorithm and the core idea is this
we're going to give every node a prize
it was detected as changing in the gene
expression or the proteomics there's no
prize associated to genes that were not
detected is changing
and we'll give them prizes to the
transcription factors as well and we'll
assign costs to each edge based on the
these probabilities of whether they're
likely to be true or not so I'll have a
high cost if it's a very unpro
improbable edge not likely be true in a
low cost if it's likely to be true and
the algorithm is trying to decide every
time includes a node in the final
network does the prize that it gets to
keep by including this node justify all
the costs for the edges right so if it's
a big prize and low cost then definitely
gets included and if not then there has
to be a trade-off and so what you end up
optimizing is you minimize number of
prizes that are not included in the
final answer and the cost of the edges
that are included in a tree you
necessarily then with this approach will
end up with a tree now what we actually
end up doing with it afterwards we relax
this tree requirement by taking a lot of
sub optimal solutions we add back edges
so we're this algorithm gives us a tree
but we don't think the biology is a tree
but what then does is you go for having
one of these hairball diagrams so this
is what you really get when you take the
data from the glioblastoma example and
just look at the nearest neighbors and
you get thousands of hits and instead
this is the network that we get with the
prize collecting Steiner tree where
everything that's outlined in red had a
prize and the nodes that don't have any
right around them or what a cold Steiner
nodes there at it just because the
algorithm says they're needed to connect
up the things that do have the prizes
you look like you have a question the
standard is me we are at what were the
boss big bosses these are our biological
interpretation of what we found the
triangle those are the gene expression
these are the proteins that we think
explain the gene expression and then the
circles are the diner nodes sit what are
they questioned the proteins that we
didn't detect anything about but they
are needed to physically linked the ones
we did detect to each other you know
with so you don't know what this
proteins are there they're just in some
cases we we know from the literature
what they are in other cases we don't
know but we know that they have physical
links yeah so exactly yeah all right so
um so when you get a network like this
they're all sorts of statistical tests
that we can do to try to figure out if
it makes any sense based on prior
knowledge that's good and did some
interesting things that I actually
didn't expect that would work out some
of which we were forced to do by
reviewers but really you know if you
just recapturing the prior data that
that's not the goal of Allah sorry
wasn't want to try and fund something
new so we asked some questions about
some of these Steiner nodes for example
I was very suspicious of this Steiner
node because it's a protein that's been
started a lot has a lot of interactions
and it could be there just because the
hub and the network and so if you've got
home network and you need to connect the
things with prizes to each other you're
going to naturally have a certain
preference to picking up hubs so we did
an experiment where we physically
measured where this protein what this
protein did so it doesn't bind directly
the genes but it binds to proteins bind
to the genes we detected where it was
found in the genome and then we asked
whether the genes which is bound are
also places where that epigenomic signal
was occurring all right we had measured
these things independently and it turns
out that this protein which was not
directly detecting any of the
experiments but the algorithms told us
was relevant was highly statistically
correlated with this signal that was
actually in the samples that we were
looking at so that was good but we
really want to do that we'll see whether
we could actually come up with potential
drug candidates for this approach
remember this is data from a tumor cell
so we ranked every protein in the
interactome by whether it was in our
core solution or very close to it highly
connected to it or whether was very
distantly related so we have a ranked
list of every protein in the interactome
and we choose a chose a few of the high
rank things that were really tightly
part of our network on the things that
very low ranked seem very distant and
then we used chemicals that block the
activity of those proteins and we asked
whether they block the growth of these
tumor cells and for things that were
very distantly related to to our network
we didn't do very well for things that
work or in our network we did very well
a lot of these hypotheses held up
some of which I didn't think we're going
to work in a million years so this is a
brain tumor one of the things that was
really highly ranked was the estrogen
receptor which would have no apparent
relationship to brain tumors but when
Carol did the experiment she found the
tamoxifen which is a compound that is
used to prevent breast cancer actually
block the growth of these tumor cells
very effectively so again this is all in
vitro we're not curing cancer yet we're
carrying cancer cells and a plastic dish
it's important to make that distinction
but it does appear that we can take a
lot of on data that's unbiased by
preconceived notions build a model and
use that model to God experiments that
are going to be much more accurate ways
of finding these novel targets than just
you know chance or our preconceived
notions having the black body put into
the white bar is bad news and it's a
black bar suppress a lot that's good
news right so so that's mostly correct
so we have two kinds of tumor cells here
one that have the specific mutation and
others that are tumor cells but don't
have that specific mutation so if you
knock down both the black and the white
bars that's good but it's even better if
you knock down the black more than the
white because then it's very specific to
the to the this particular mutation we
even then two three four five yeah so
that's obviously very very strong
blocking of the growth of these yeah
it's so is there a prediction it is a
model just saying this is an interesting
thing to look for or is there actually a
prediction of which one's gonna be
higher yeah so that's a really great
question so these current models say
this is an interesting protein to study
doesn't tell us whether we should
activate it you're inhibited or
something else one of the things that
we're working on now is actually trying
to convert these kinds of models into
other models that are much more makes
much more quantitative predictions I
have a slide on this later but the
basically it is a lot of those kinds of
models can't work on a genome proteome
wide scale you need to have it boil down
first to some couple dozen proteins to
look at and we think this is the way to
boil it down to a couple dozen proteins
and then
into these other algorithms that are a
little bit better developed for making
quantitative predictions he's with is
known and to see if thinks I mean how do
you know that somebody of these yes so
that's that's what the stuff I I skipped
over we compared what we found to a lot
of known date about glioblastoma and so
we looked at the ranking induced by our
genes compared to a list of genes of
people that previously said were
relevant to glioblastoma there's glue
blossom is not a completely unknown
disease people have sense of what's
important what isn't and so we
recapitulate that it is if you know that
there's not a major remedies another
thing that you could have tried do some
other gene that you're trying to attack
further down but you know you look at
the top 27 and you found some very
significant factors oh so do the whole
experimental system where we really know
how to cure the disease no I mean it it
sounds like a great idea but in terms of
the the human and and and and real cost
of doing these it's it's not really
practical but but yeah that would be
interesting I guess I guess I only have
a few minutes so I won't try to go
through all of this I'll skip over some
we've been able to apply different kinds
of data and come up with different ways
of ranking things in these networks to
decide what to target one thing that
Tony's been working on that I think is
really exciting is what to do when
you've got lots of information about
different patients who you think have a
common disease but the actual data for
those patients looks very different one
from the other and I'll skip some of the
motivation just this is I think a very
informative slide so there had been a
lot of excitement about trying to
sequence lots and lots of tumors in the
hope that if you look at a large enough
population of tumors you'd see certain
mutations occur over and over again and
those must be the mutations that are
really driving the tumors those other
mutations just occurring along the way
so a huge amount of money gets poured
into sequencing tumor
but what people found really
disappointed them because they found
their only handful of mutations that
occurred frequently so this is a whole
bunch of genes and the list could go on
and on and on and then the frequency
which they current tumors and the ones
that occur frequently ones people
already knew about and then most the
other genes occurred very very mutations
good very very rarely and so there
wasn't really a lot of immediate
biological insight that came from these
but each patient tells a story here
right each patient had some had some
mutational process that ultimately led
to the disease since the way we've been
thinking about this is you've got some
network and some peace and then it work
gets if some piece of that network gets
perturbed then the cell turns into a
tumor cell and so there's some
probability that any protein in that
network will show up in a final patient
but there's no guarantee that the same
one would right so patient will be a
combination mutations that occur with
some probability over some network and
then a bunch of other mutations that
occur by chance in that work network or
in other networks and so the final
network for all the patients might look
very different but they might have a
pieces that are common and so we take
the same prize collecting Stein approach
that we had before where before we were
saying does adding this node makes sense
in terms of the edge costs and the
information that this is changing in
that sample and now we can say well what
do we know about this from the other
samples from the other patients so if I
know the ones in green were mutating
some other patient then I would have
more confidence in adding this nodes in
my network even if the edges are not as
reliable whereas if there were no other
mutations and I might have less
confidence and adding that piece and
Tony's been able to show and I'll have
to skip all the details well just go
through this bit on the on the equation
so we had this equation that showed you
before where we optimize the network to
minimize the cost of the edges that
included and the number of prizes that
we leave behind this is for a single for
a single tree or forest and now what
Tony's done and said okay now I'm going
to do my optimization across a whole
bunch of different patients this
represents this optimization function
and then I have another term that
couples
nodes that are included across all the
patients that tries to bias me to have
those green nodes that were shown before
and so we have a whole bunch of patients
we map the data onto the networks we
learned the individual trees or forests
and we see okay certain nodes how
frequently does each know occur across
all these and then we can bias network
to network for each individual in a soft
way to prefer pieces the network that
have shown some evidence in other
patients but not require it and I'll
skip all the details is so a few things
on cancer patients so this is from a
giant sequencing effort of breast cancer
in yellow are the mutations in an
individual patient and in green are ones
that occur in some other patient you can
see how dense this is in green so even
though the individual patient might be
very sparse in this network mutations
are seen in this now a piece of the
network lots of other people you can see
for example that this kind of approach
can pick up mutations that we know from
other data to be to be relevant but
would not have been picked up if we
analyze the patients individually
without this coupling escaped so just to
sum up we think that this kind of
problem where we integrate many many
different kinds of data by looking for
relationships among them the physical
links among the data that are generated
by different techniques will be more
powerful than simply trying to analyze
in terms of known pathways and that we
can actually find novel stuff new
approaches to diseases by this kind of
integrated approach that we could not
have done previously and so we're
working with a whole bunch of problems
in our lab and with collaborators from
everything from pancreatic cancer to
plant metabolomics see if we can push
forward to really develop new approaches
to important biological problems I tried
to mention all the people who were
directly involved within the talk
obviously a lot of this benefited from
great interactions here and also with
ricardo sakina at the public technical
and turin and we have a lot of great
collaborators at MIT in the area who are
suppliers of great biological insight as
well so I'll stop there I think I've run
over thank you
this article estimates on scientists new
model fiscal conformations and they were
talking about how they used to build
physical models and then they had famous
stop sort of and then that now they do
it computationally the article was sort
of stopped talking first about a
relationship to the crystals of a
scientist form and it felt like there
was sort of a loss there like in
understanding no but then also a sort of
pinnacle pedagogical loss and they found
themselves really struggling to explain
crystal formations just students without
making that intermediary step of
building a model and I was struck by
that even in your talk where it was like
we sort of have these moments but you
would show more better like expressing
them public wait what does this mean and
what does this mean and like how do you
read these simple as I guess I was just
wondering if you felt there there were
any similar losses in this transition
from cartoons to computational models
that's a really good question I don't
know if you were picked up by the mic so
I think it's the recording so I'll just
say that the question had to do with
whether there's a loss in understanding
when you go from the cartoons as i call
them to these computational models that
may depend a lot on how you were trained
so i got my PhD in a biology department
but i used to hate signaling because i
could not make heads or tails out of it
it just seemed to me you know too
complicated just this morass and now i'm
doing work on signaling but it's
precisely because i think these
computational models make sense of the
data in a way that previously the
cartoons didn't people will really who
loved the cartoons probably missed them
i personally won't miss them at all some
level though you do want to be able to
understand what the models are
predicting right and maybe the the
analogy should be too
other kinds of engineering where when
people are designing airplanes they had
to go away from building balsa wood
models to you know computational models
I'm not an aerospace engineer but I
imagine you still need some training in
the balsa wood models to really
understand what's going on right so how
do you train people in this field is
something that's important it could be
though that the modeling takes now more
important role so you could have a
computational model where you say well
what happens if I perturb this and if
the models really are predictive then
maybe you can develop better intuitions
than you could in the old-fashioned way
so but it is something we struggle with
how to best convey look what does it
mean to understand what's going on right
is it under staying enough that you have
a program that gives you the right
answer or should you have an intuition
of what's going on yeah some of these
combination was there really bad right
the end and somehow because you see this
equity when they play on this date and
tree is right you never get deeps kind
of really deep trees what you end up
doing is trying to make these internal
nodes where all the observables hang
like right off of it so which is sort of
the point you're kind of ignoring any
deep structure the transcriptional stuff
and so I don't know about the other one
but I suspect you wouldn't buy anything
like to deepen you know in these trees
they don't kind of lots of observe stuff
hanging off and I think would find it
actually I would disagree without so I
know in the Steiner instead nodes
because the links that we're getting our
physical links the structure actually is
more informative so you do get signaling
you remember those cartoons had you know
this chain of events so the those chains
of events are actually physical links
that are there so we don't rely on
statistics to reconstruct those we
actually use accident observe staff is
hanging off them like all the way
through it right then they can be
internal as well so it doesn't just have
they don't have to be leaf nodes they
can be internal the observe stuff can be
internal nodes observe staff I mean
stuff that you can measure is kind of
hanging something you measured it can
also be in the middle of it right
remember remember the
on with the red boxes the wasn't wasn't
all leaves her notion of middle like so
we've got rid of I mean so this is not a
tree obviously right yeah that I'm
saying and that you fix this because you
explicitly don't make it a tree right
hey like somehow it's more like you want
some reason away of capturing local
interactions that aren't mush and and
this seems to come up all the time and
using tree boys like a computational I
hear a state that's right priya is a
purely computational heroes lacrosse and
instead of running into problems with
that like all the time yes and so just
intuition for how because look for the
latent tree stuff like you know working
on you know the one with an email like
one way to these datasets is you try to
build a tree another one is you try to
you know do something much more shallow
that's overlapping because of this
interactions there and showing what
intuition you have to sort of fix these
models in a more reasonable right so we
throw away the tree structure in fact
the way I think this is it gives me a
bag of nodes right and then
understanding the relationship among
that bag of nose is a separate
biological problem and in fact we've
been exploring trying to do that to try
to reconstruct what the right edges are
using other kinds of data so if I start
with this bag of nodes then maybe I can
reconstruct what the right set of edges
is if I had data say on the time
dynamics of these nodes so that's
something that Tony's been working on
with collaborators as well time dynamics
is important component the other kinds
of data as well that we might be able to
then roll in so I see this as a way of
finding out where to focus our energies
and then choosing the right experiments
to reconstruct it so the sine dynamics
you could then do very focused
perturbations of a whole bunch of these
nodes once you have a down to some
manageable set maybe you could
reconstruct the logical network just
sort of uh I'll give you my view of how
all these modeling approaches put
together
okay so I think about these modeling
approaches is whether they're good for
systems where I've known components or
unknown components and whether they work
on physical relationships or statistical
relationships so if I've got some giant
data set includes all the proteins and
genes and maybe I'll do regression or
clustering and I get some statistical
relationship there are other approaches
the people in biology like a lot of all
not biology but biological engineering
like lot like differential equation
based approaches or logic models where
if you've got a very small system you
can make very quantitative predictions
about what's going on so we've been
doing lives over here it works on
physical relationships at the genome and
proteome wide scale and what we're
hoping is we can take all this kind of
ohmic data that will lead us to which
pieces to work on then we could feed it
in to reconstruct some of these
differential equation based models or
logic models so on so yeah the these
models don't really explain they don't
make quantitative predictions in a way
that we're going to need to actually be
able to solve serious biological
problems but they do help us figure out
where to focus</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>