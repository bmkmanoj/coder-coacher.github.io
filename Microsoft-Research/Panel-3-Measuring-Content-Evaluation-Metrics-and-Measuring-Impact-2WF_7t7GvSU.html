<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Panel 3: Measuring Content: Evaluation, Metrics and Measuring Impact | Coder Coacher - Coaching Coders</title><meta content="Panel 3: Measuring Content: Evaluation, Metrics and Measuring Impact - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Panel 3: Measuring Content: Evaluation, Metrics and Measuring Impact</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2WF_7t7GvSU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I'm great boarded from assessor and I
know many of you but not all of you were
kind of a repository that hangs out in
social sciences and humanities and
strongly support open access that's all
I'll say press this run thank you to
Microsoft digital science and all of you
for being here Amy asked me to put
together a panel on metrics and I said
yeah maybe not really sure I want to do
that and I said can I do it a little bit
differently and she kind of played along
kind of playing the straight man and I
said I don't want to do one where we
talk about the difference between a LMS
and altmetrics and gaming and all that
kind of stuff that bores us to tears
especially right after lunch so she said
my gosh the title of this workshop is
shake it up so do whatever the hell you
want so we're going to try to shake it
up you know Margie said that there's a
lot of different ways to think about
metrics and think about the way that
they're used and think about the way
that they actually enter it into
conversations to maybe be a little bit
helpful instead of dogmatic or the Grail
that a lot of people start talking about
them and what we're going to try to do
right now over the next session is give
three speakers three very knowledgeable
people from different perspectives a
chance to share how they see metrics
working in the world today so Mike
Taylor from elsevier will start off with
WTF and kind of challenge the way we set
and think about metrics and ashley will
come in from the funder perspective and
talk about you know what is the impact
of the money and give us some real world
examples and really kind of the stories
that play out for the funders and these
nuanced stories and ways in ways that
metric can help us see something that
may be traditional numbers don't and
andrea is going to bring it all together
at the end with some real-world case
studies about how different people not
just 10 year committees but different
people in the real world use
tricks to make some decisions and what
we're learning from now I will first off
say that we're as Mark said that he has
the easiest job by basically just coming
and showing up and letting somebody else
present for fig share I have the second
easiest job which is to stand up here
for a minute and actually have really
smart people tell you things that may
question the way that you think about
metrics so the goal for our session is
to leave you with more questions than
answers but we will have some time for
Q&amp;amp;A at the end so maybe we will get them
answered but without any further ado
like I could afternoon my name is mike
taylor i live in a world which is
constantly changing up until two weeks
ago I worked for elsevier labs and then
I moved to the new and gigantic elsevier
metrics team it's so big that we could
all sit at a table there there are three
of us and we're charged with steering
the metrics and how else with computer
metrics what those metrics mean and
highly displayed this is going to be our
task for the next few years and as of
this morning I'm delighted to announce
that I started doing a PhD in the
subject as well this is just kind of
rounding out my life really really busy
KY already have three careers so i've
been working in metrics for three years
and i've been building up cup of really
enormous data sets most of which comes
from altmetric calm and one of the
things I really want to observe about
this is because I am altmetrics number
one fan and but any mistakes that are
appear on this any conclusions that you
don't like Greg asked me to say things
people are going to disagree with so I'm
going to say that but I'm only human
when it comes to these things now as I
said I work for elsevier metrics and the
tasks that we're working with is to
challenge the way that we communicate
metrics the three of us Carly Halevy
Mike Taylor and myself and uh sorry Mike
Taylor Lee callowhill EV Lisa collagen
myself we've all got experience of
working in collaboration I work with the
OP metrics people I've been hanging out
at conferences and writing papers for
the last three years lisa has been
working on a project called snowball
metrics which is an open academic lead
scholarly
metrics for comparing institutional
behavior so we have a real commitment to
open metrics methods it's really
important for us that we do this because
we can't go back into a place where
there are black boxes that pull out
numbers that general Rick jenya generate
our views and interpretations of what's
happening in the skull d well that can't
be challenged and can't be understood
can't be manipulated and reproduced by
other people so our commitment to open
methodologies is a really really
important thing we believe in agnostic
data we even have a little manifesto
Lisa I wrote this pretty good document
the hef key statement on metrics which
is about 90 pages long there are more
elements to the manifesto than just the
ones that I've got up here but
nevertheless this is the kind of thing
we're talking about don't expect us to
be producing numbers that plane to
answer all those things because we live
in a world where we're changing to an
open environment where people are
talking about science and research in
ways that they have never had
conversations before and the resultant
of this is that there are different
people using different kinds of language
there are people asking different kinds
of questions that take us away from the
narrow very narrow worlds that academia
has had for the last 40 years and we
have all known we've all witnessed what
has happened when you try and take one
number and expand it out of all shape
and use it for reasons and we're talking
about the wonderful journal impact
factor something that we can still be
used for a variety of things but we've
got all kinds of opinions about it
generally speaking use for its narrowest
sense it probably does the job one of
the great things about 40 years of
research is that we can clarify those
caveats we can say what we mean when we
talk about it but what we can't do is
then going to say well this means that
an institution or researchers work is
valuable or valueless it's being
distorted out of all kinds of use so our
commitment is to move away from having
one black box metrics to having a series
of open metrics I would rather have our
say for example three metrics on a
subject that go to eighty percent of
answering the question that are clear
and validated and crispy
understood by everybody who needs to use
them I'd rather have those three and say
to you the people who consume our
metrics and use these things that this
is how you may use them it's up to you
to come to the conclusions about it I'm
not going to tell you whether what the
answer is to your question but I'm going
to give you the tools and the analysis
and the data and the validation to make
sense of the numbers so we have this
little background where the impact
factor another citation based metrics
are being used to answer questions that
they were never designed to answer and
all of a sudden a few years ago
literally the best thing in the world
happens altmetrics rolls into town and
it is literally the most exciting thing
that I have ever read about in scholarly
communication it promises so many
answers for us it gives us a completely
new perspective and insight into the way
that scholarly communication is being
committed to the world it get answers
all sorts of incredibly interesting
questions and I have spoken at length on
the art items in the manifesto with
apologies to anybody who is involved in
writing it I have nothing to do with it
I'm merely read it I consumed it I
translated I agree with most of the
points in it I don't agree with
everything but I agree with most of the
point in it these are a wonderful set of
ambitions to have when we are thinking
about metrics what can we do with the
new data that's coming into vault
metrics but I mean I need to pause
because I'm going to be really mean
about art metrics and you really have to
remember that I am its number one fan we
have all these observations all these
hopes that are embedded in the manifesto
and yet when it comes down to looking at
the data from the last three or four
years and interpreting in an
understanding it there is a really big
gap between what we had hoped to see and
what we are actually seeing these are a
few of the conclusions that I have
reached from my very large data sets we
hoped that Ultimatrix would give a
broader picture of the universe at the
moment it doesn't we hope that it would
discover hidden of publications and at
the moment it doesn't in fact if you
compare the performance of citation
based and metrics against none site none
formal traditionally
alternative metrics we find that it's
pretty narrow it's focused on some very
narrow subjects and it doesn't give us
the breadth and the width and the
richness that we're hopeful but this is
a question of at the moment this is the
last three years so back in 1976 my
teachers had some pretty bitter things
okay my first report in high school was
pretty bad reading it's still pretty bad
reading and but it's pretty much the
same for altmetrics it's got a lot of
promise it's not smart enough it doesn't
answer the right questions these are
things that we need to work on to expand
to ah really gets me see some traction
going on in our alternative metrics we
take it really seriously in the elsevier
but at the moment it's it's a lot of
promise and here is if you like the
problem that I have I'm going to go back
to that slide from oh here's the problem
that I have because on the one hand we
have citation based metrics and we have
40 years of research and citation based
metrics we really understand what's
going on we understand disciplinary
differences we can go and buy books on
the subject we can be papers there are
people doing phd's in it there are
departments university departments
working in bibliometrics and we don't
have the same thing going on in our
metrics we have a few PhDs we have it a
load of papers you know we're really at
the beginning of the understanding and
people being really mean to citation
based metrics are really really horrible
and it's because they've been distorted
out of shape but at the same time we're
getting away with awful things when it
comes to alternative metrics because we
just don't have the data points we don't
have the background we don't have the
research we don't have the books we
don't have the papers we don't know the
department to justify some of the things
that are being said about altmetrics at
the current day so this is the thing
that I have recently learned in the
course of one of my other careers my
theatrical career and is that we know
that there is difference between
measuring a thing and the thing itself
this is something that Heisenberg on the
left and bore in the middle and
discovered in the 1920s and anybody who
is an astrophysicist OMG
this can meet me later and beat me up
for Miss reading the script um but what
they said was that there is this gap you
need you need to understand that when
you measure the part the spin of an
electron that what your me tick what
you're measuring isn't the actual spin
of the actual electron it's a it's a
proxy for the real thing what we need to
do when you say these things the
responsibility for the researcher is to
make those caveats to speak to their
results but to point out where the
weaknesses are measurement is always a
proxy for the real thing and this is one
of the criticisms that we read of the
impact factor that it is a proxy for
impact it's a proxy for quality people
say these things about citation based
metrics and they rarely say them about
alternative metrics knowing that
something is a property doesn't
understand ermine the the essentials of
scientific discourse but but it places
the onus on the researcher to make the
conclusion to to say what it is they're
talking about to argue their position to
validate what they're saying about their
metrics so as people who are involved in
metric feel we collected the data
responsibly we make sure that there
aren't people who are lying about data
or fabricating data we do really good
statistics with it we can make the
conclusions we say the numbers but when
we say those numbers it is up to us as
good scientists as good communicators to
say these are the caveats these are the
weaknesses this is the error of proper
building and these are things which are
not being said about alternative metrics
at the moment and they are being said
about citation based metrics so when we
talk about bibliometric impact
previously it's only been the people who
are involved in bibliometrics that talk
about these things right so so if a
bibliometric on says I are the impact
factor they're talking when expert to
another they using language which is
qualified by the disappearance in the
context of a particular kind of
discourse and as a consequence they
don't have to have the same kind of
explanatory discourse that we do when we
start talking about some of the issues
that arise as an alternative metrics and
a classic one for me is the case of
social impact which is something that
is used a lot right so we can talk about
all these citations and we can talk
about impact factor it's got enough
capital I and a capital F in this guy
definition equation and it's got
references but we can't at the moment
use the same conversations to talk about
alternative metrics because we have to
explain everything and in any case the
data really isn't big enough for example
um in the year up to June the first
there were two and a half million tweets
that refer to two papers which is a very
very big number um that's what just over
one tweet paper right to point to yeah
there's 2.2 million papers published a
year so that's just over one tweet /
paper and turns out that almost ninety
nine percent of ninety percent of papers
don't get any kind of Twitter mentions
so two and a half million tweets then go
into a very small number of things and
you you end up with a mean of about fire
and a half tweets per paper that
actually gets cited but there's this
really huge gulf we don't see that big
gulf and when we come to talking about
citations we don't see ninety percent of
papers not being you know consumed or
read or cited it's it's the differences
here are really important when we start
thinking about making conclusions about
what data means so what can we say about
two and a half million tweets well we
can certainly say that they're really
focused I'd wrote a paper with andrew
plume also of elsevier last year we were
looking at the kinds of paper that get
the really big pieces of impact we were
expecting I'll be honest we did the
research expecting to find that there
were really trivial papers there were
papers about sex drugs and rock and roll
that we're getting all this week's turns
out its climate science and vaccines but
you know okay nevermind good paper find
something else out right but we talked
about people talk about there are papers
that talk about tweets Twitter being in
some way analogous to social impact and
yet we're only seeing any indications of
this form of social impact in about
twelve percent of papers and even when
we do that it's a pretty meaningless
level of pretty meaningless meaningless
level of activity in my opinion at least
for most papers yes we look at
top end of those papers we're seeing
great rich data at the most most
publications don't get anything and
those that do get virtually nothing so
it's kind of really to talk
about anything meaningful coming out of
this other than knowing the topics that
make people tweet and as soon as we step
away from our tightly little world of
altmetrics or bibliometrics and we start
talking about social impact of real
people they don't have the same
definitions though biblia veterans do
there isn't quite narrow there isn't a
generally accepted definition of what
social impact is certainly if you if
you're a banker or if you work in our
not-for-profit then there are
definitions that people use on social
impact people try to do this I mean
there's there's a whole level of our
publications that are being made to try
and explain to define social impact is
in very narrow areas but there's nothing
at all in terms of in terms of
altmetrics and very little in terms of
bibliometrics so there's a real gap here
because people want that answer they're
asking is what is the social impact of
my research they're asking us all sorts
of questions and so take take for
example how we might use different
models to think about in this case the
value of extending a life I made most of
these numbers up but I would defend them
over a pint and so this is a metric that
I created to thinking about why where
you spend the money to invest in
extending somebody's life by ten years
and though there's the same numbers but
taking a different kind of perspective
and I thought this was quite an
interesting thing to do because and
we've got two different models both of
which might be defined as being social
impact but actually giving completely
different answers using the same data
and I think this is the same kind of
thing we're driving at that when people
talk about social impact on what
scholarly what scholarly communication
is after we don't have a consistent
clear view of what it is to do that and
there are a number of problems getting
in the way of us being able to answer
these um answer these things he said
skipping through his notes really
quickly um another example would be on
internationalization so if we think
about our an international a metric to
say whether someone how some to produce
a number on how internationally focused
researcher is we need lots of different
data we can ask many many different
questions we could say well is this
person the question might be how good is
this person at getting money from
international funding agencies the other
question might be does this person spend
all their time flowing two hot spots in
the winter like Cambridge what kind of
person is this what's the question you
want to know when you're thinking about
what internationalisation means do we
want to know whether they're being read
in countries other than one the writing
and do we want to know whether they're
engaging with researchers in the
developing world these aren't questions
that are embedded in the data we need to
take those questions and do a gap
between how we are going to go about
answering what does a hypothetical
answer look like what else do we need
other than data to answer these
questions where can get the data from
how good is that data and actually
that's the how good is the data how
complete is the data is a really
important issue for us because um you
know we have this world where in our
citation data we have 40 years of
research so we have an idea of how
complete something is but when we
started building when the community
started building on metric tools what
they weren't thinking themselves well
what they do I need to answer this
question rather they were saying who has
an API and how can we query it and if
you have an open API there was free and
the license terms were unconditional and
you could answer there you could answer
queries based on the day away then you
will be included in that big box and
numbers that we get so Mendel a was in
and Zotero who I didn't have an API at
the time where it's just too hard to
work was out and other papers like other
software solutions like papers don't
even met Kat at all because they don't
have an API that's open so we never
actually really know how complete how
open the data is so if we try and make
it qualification we're saying yes but I
don't know what the answer is you know
we say this is a highly impactful paper
on mendeley and we don't know how it
performs and other things and we know
that there is probably a disability just
difference between mendeley in Zotero
but we don't know what that difference
is yet so it's very very hard for us to
come to any kind of conclusion to to
qualify the data that we
have to answer the questions that being
asked and this is why I want to
personally press a big stop button and
think to ourselves about the problems
that lie ahead of us altmetrics
alternative metrics is such an exciting
place at the moment you know we have new
platforms coming on stream we have more
people talking about things we have data
coming up we have young people doing
PhDs we have old people big doing phd's
it is a really exciting place to be
involved in but the problem is right now
is that somebody might take us seriously
and and this this would be a really big
problem if somebody woke up tomorrow
morning and said right I am only going
to fund papers to get 5 tweets or only
going to fund research for people who
can get 5 tweets I mean it's like
crazies but this isn't like citation we
can look back at 40 years with of
history and say ah we know something
about this discipline this is something
which changes every year there aren't
the benchmarks against which we can make
comparisons of performance we have to
create these benchmarks we also have to
expand the amount of data we have a
great deal of work to be done we have to
advocate for people to open up their
data we have to ask places that don't
have AP is to publish auditable data
about usages so we can make comparisons
between different applications so we can
draw up a full view of what the
behaviors going on and we have another
problem in terms of working from our top
down and saying well what questions do
you that does the community want because
it's not up to me as an individual or me
as elsevier or any other individual
person or other corporate entity to say
these are the questions that we think
that you want to answer use our number
to answer this question rather it is the
question of what what academics have
questions Dean's have questions funding
agencies have questions and these
questions need to be defined and worked
on understood the data needs to be
gathered we need to advocate and argue
for the data to be open up there are so
many questions to answer i am going to
shut up and about a minute but there are
so many questions or one of the
questions and that I find so interesting
is when you look at an analysis of data
we can see that our issues in math right
they make the headlines everywhere in
the social media yet they don't appear
on
v or the news or the radio at all I
don't understand this there is this
massive gaping hole about math in the
mass media that's really hard to say
after lunch why is aquaculture so
popular in northwestern USA I can't
imagine it but these are questions that
can be asked but at the moment we don't
have the tools to answer we don't even
have the language or ask button so this
is why I want to press the stop button
and ensure that the least I have a
conversation that I know what I'm
talking about for the next 15 years
because frankly that's what my career is
going to be doing thank you
thank you um I just realized while
sitting there what my purpose here today
was and it's to be a buffer or
potentially an interlude between two
altmetric presentations yes and to
provide a little bit about the funder
perspective I'm Ashley with uber
research we work with funders globally
very small funders that fund less than a
million dollars a year in very focused
rare diseases all the way up to the
largest usual names NSF NIH and we work
with them during the review process to
find reviewers and make other decisions
and then also from a portfolio
perspective to help answer questions
like what should we fund so it's our
work that we do with funders that is
that is represented here and it is by no
means any answers it's it's more of a
problem statement and a a call for help
generally speaking when it comes to two
metrics so what I would like to share is
a little bit about the questions that
are being asked today by funders when it
comes to metrics and impact and then
share with you a little bit of the
perspective of the gap to use the mind
the gap that we've seen that exists in
that need so a few a few quotes that
that we heard recently but their
representative from what we've heard
from many funders which is for the type
of impact that they want to measure
publications for them don't cut it
however if you connect it with what we
just heard that's really the only data
that exists which is mostly true so how
do they deal how do you deal with this
this gap and also a statement of one of
the realities which is not only is data
needed and tools needed in common
languages about what is impacting all
that but also just represent here a
shift in culture and awareness that of
the need and to get people thinking
beyond that the usual
the obvious and the tradition and what
may be slightly ambitious when again you
think about the type of data available
not only do they want to look beyond the
impact within research and how you can
measure that with outputs but also want
to dip into health impact and social
impacts and economic impacts so not only
our funders like everyone else short on
data when it comes to research impact
but then want to bring in all these
other areas and start answering
questions it's huge but that's also
where many funders are going and I use
the word health here and this was a
quote from a health research Thunder but
you could put in from any funder in the
type of research that they're coming so
an example of the type of questions that
are being asked by funders that are
unanswered again the problem statement
this is one example of a research to
impact framework and what's interesting
is there are many funders out there who
don't even have a framework like this
you'll see at the very largest funders
but in the small to medium sized funders
they don't have the time to even come up
with something like this what is what
this can help you see is the range of
questions and the range of metrics that
are needed that a funder needs they need
data about patient input but with
stakeholder engagement there's more and
more patient or viewers but more data
about what's going on then in the green
there they're making decision about what
the fund which leads potentially to new
knowledge which is informing decisions
which hopefully leads to some
improvements in care reduce recidivism
all these things all of this area is
what they're looking at to measure their
impact every step of the way and some of
the questions that are within there and
this is probably for of thousands where
they're looking for for metrics for
indicators for answers for data what's
interesting is I thought I'd put it in
the
order of ambition where the easiest ones
easiest one came first and follow
through but which thing is most fun on
lassalle funders that we've ever spoken
to and worked with they can't answer the
question how many researchers are in
their in their state or in their country
or in their domain and have a reliable
number that they can use is the
denominator for certain metrics many are
focused on building capacity and
expanding field so they're asking
questions about who they funded and then
these more abstract questions are also
being asked which metrics and data to
inform those metrics and the tools in
the common language don't exist and in
that framework you saw huge ambition in
the type of questions they want to
answer and so many of them are still
stuck answering a basic question like
this which if you looked at that
framework kind of have a red box there
it's kind of small that's where many of
them are spending their time and don't
have a complete answer and that is just
one little piece of the impact question
when it comes to a funding perspective
so you talk about a group of
organizations that have huge need and
very little insight and well little
support when it comes to two metrics and
indicators another way to look at it
this is a list of actual indicators that
inform that that framework and that red
box there are the publication based
metrics and so if publications are the
only data set that is generally
available which is absolutely true then
how are they going to answer the rest of
the questions that they're trying to
answer and with so we talked about a
little bit of those challenges to
connect it to the infrastructure
question you might say that that bridge
is the publication data and the rest of
it is completely wide-open some other
challenges some very realistic time
lines of what was your impact
they're asked well what was your impact
last year how if you can't ask if a
researcher has multi-year project you
can't answer them ask that question
every year how could you ask the funder
of those researchers but they are being
asked what was your impact in the last
12 months quite unrealistic and if you
look at this is a rough approximation
from hey I have an idea for a project
submitted to a an application to
eventually writing paper any citations
if you look at it from the perspective
of if you're looking at national level
funders of the political
administration's the political
administration's are wanting results
within a very short window of time which
is you wouldn't even be able to measure
the citations that came from the
projects that were funded at the start
of that so again very challenging
environment another challenge is also
just how to practically do this and so
sort of as a an awareness campaign there
a school has started to help with the
challenge of research impact assessments
so for any of you when it comes to
metrics and impact in that general topic
it will be in its third year next year
was held in Banff in the Canadian
Rockies wonderful location kind of like
what we have here with beautiful views
and they're also hosting it next year in
Doha Qatar which would also be a nice
place with my fuse but that gets to the
part of the challenge which is not only
is their difficulties in the data to
answer these metrics for all the
questions they answered but there's not
a common language about what the metrics
should be let alone the tools let alone
the capacity within the funding
organization to start measuring your
impact we we've worked with
organizations that are multi-billion
dollar funders and they have two people
who part of their job is to measure the
impact of the research funding and you
can imagine how difficult it would be
with all those challenges at clump so
again more of the the problem statement
of how what funders are facing when it
comes to measuring their their impact
much too much so my name is Andrea Haluk
and I am the co-founder and president of
plum analytics so what I'm going to talk
about are some real stories of how
people are using metrics to talk about
research impact so as Mike started off
talking about this you know
traditionally how is impact talked about
it's all about the citation count and so
if you're Thomson Reuters you have a set
of publications that they said this is
what we're going to measure with web of
science and then there's metrics a
citation based metrics that build off of
that data set if you're with elsevier in
scopus again they decided this is the
universe of what we're going to measure
that we deem appropriate to be inside
scopus and then there's additional
metrics that build on that and you're
getting just what was in that original
set from elsevier is deciding what what
is worthy to be measured Google Scholar
entered the scene and they have their
you know Google metrics and their
citation metrics and now they develop
metrics as well so when you take all of
those citation based metrics together so
this is a Venn diagram that looks at a
particular book and looking at the three
different providers of citation metrics
and saying you would need all three of
them in order to tell a complete picture
of just citation impact right so the you
know to get those 153 unique citations
you need all three tools to tell you
that because they're all sampling from
different places but it's a bigger
problem than that to tell this
comprehensive story of impact so if you
look at citation counts that was for the
longest time all we could measure that's
visible light right we could measure
visible light we can count it we can
talk about it but there's more that you
can measure here right the whole
electromagnetic spectrum as other things
that when you had basic tools like print
journals you can measure certain things
but the world is changing and now
now I'm sure people have seen diagrams
like this before you know what's
happening online in 60 seconds or this
is data from 2013 and this is talking
about how there's 72 hours of YouTube
video created every minute right the
content is exploding the amount of data
that you can capture is exploding and of
course this isn't just consumer based
behavior this also happens around
scholarly output so here are some
sources of data exhaust of what
researchers produce or people
interacting with research produce so
it's more than just a journal article so
you put your data set you've shared a
data set you can tell how many times
that data sets been downloaded you have
a video that you've produced it talks
about your new research approach or
educating patients or educating parents
of patients you can get views you can
see how much that's been interacted with
and this is just a sampling and we
harvest from all of these places and
anytime we build this slide it's
immediately out of date and we have to
go search new logos and you know add
more to it so this is an expanding
expanding field and it gets us to the
definition of something called
altmetrics so Mike talked about
altmetrics of tweets and many people
that is synonymous so I they think of
altmetrics there's a company in the
space a digital science company called
altmetric com which also you know their
definition of what altmetrics is is is
one what we talked about with plum
analytics is really looking at all as in
complementing the traditional metrics
all that way and we honestly unless I'm
asked to speak about altmetrics we don't
even really use the term because we
really consider it all metrics being
trying to have this comprehensive view
that full spectrum of metrics is what we
endeavor to capture and we do that so
that we can answer the questions and
tell the stories behind research impact
and most notably that hard question of
what's happened in the past year you
know though what have you done for me
lately
so citation counts take anywhere from
three to five years to get to the point
of where someone has done their research
published the paper someone else's
reddit done their research their paper
went through peer review and now it's
gotten your first citation count that
whole cycle takes years to get to the
other side of it so what's happening in
the meantime what can we use to tell the
story in a shorter point of view and the
way that we've looked at this and we
started gathering all that data exhaust
was to gather them and into five
categories first is usage so the first
and foremost I wrote a paper did anyone
download it I created a video did anyone
watch it this is if you ask researchers
what's the primary way you want to tell
if your work had any impact they'll tell
you this did anyone even look at it so
we're constantly trying to add usage not
just for journal articles but find other
kinds of proxies for usage like books
how many libraries hold a book is a good
proxy for book or monograph usage
captures our when someone is bookmarked
something favorited it put it as a
mental a reader or somehow said I want
to come back to this and the studies
that are being done are showing that
captures are a good leading indicator of
what will be cited later and that
intuitively makes sense because if you
are using one of those platforms that
we're capturing metrics from and your
researcher and your bookmarking
something as you go back to build your
reference list where are you going to go
back to well wherever you captured it at
wherever you saved it so the data does
trend nicely with future citation
mentions are where the the stories
themselves are hidden this is where
someone's written a Wikipedia page that
link to that book or to that article
where they commented on it written a
review about it written a blog post
about it these are all minable in an
automated way to give the researcher who
produced that research a way to quickly
and easily find what people are saying
about it who are the key influencers
that it's interacting with and really is
start to learn
what's happened with their research
social media love to hate social media
in many places right because people
think that does this correlate with
anything else what we found is that
social media is a great measure of how
well promoted something is so I've
written an article I tweet about it my
followers retweet it often a millisecond
later without even having read it it's a
really great way especially for early
career scientists and researchers to be
able to tell are they doing a good job
of promoting their work it gives you
that lens into it and then of course
citations they don't go away their part
of this comprehensive picture and
especially as you look back five ten
years they are the best measure of is
was this quality research is this a
seminal article compared to other things
i mentioned before about going beyond
just a journal article and this is the
sort of things that you can measure
these are all types of research output
being able to gather metrics about all
of these different things is another way
to tell that complete story or more
complete story and how that all kind of
boils down is into these building blocks
of article level metrics so for each and
every article video data set conference
presentation each one of these gathering
metrics around all five categories and
all you know the published version of an
article the open access version of an
article the you know the version in an
institutional repository gathering all
the metrics and being able to see what
you know being able to tell that
complete story and being able to
navigate through to see where those
numbers came from so making it very
transparent so in general before I jump
into the into the case studies I promise
there's three main steps that we go
through the first is to categorize so
you categorize the different types of
metrics right you categorized by the
type of output it is but you also
categorize by what you hope to later get
data about so being able to break it
down if you're an institution if you're
you know university you might care to
authorized things by department by
researcher by lab if you're a publisher
it makes sense to categorize buy books
you're a funder you might want to
categorize by the the grand c fund in
the different areas in which your
funding them this if you put that human
intelligence in then when the platform
the technology goes and measures all
these data points you can then analyze
out at the other side so it's putting
that structure in that human
intelligence that lets you get get
insight at the end of the process so the
first case study is University of
Pittsburgh what they do is a couple of
different things so this is an example
of a dashboard so you can see so I
haven't oh I do that's always exciting
when your presenter and you don't know
what kind of kind of fun thing you're
holding in your hands so they have
Reaper researcher profile to give a
dashboard researcher by researcher what
impact have they had they've mapped in
their digital collections so this isn't
necessarily work produced by their
researchers but the different subject
based collections that they hold being
able to look at metrics by subject and
across the whole the whole collection
their library publishes 26 journals so
we have article level metrics and
journal level metrics about the journals
and then of course on by Department
being able to get views by Department
and across an entire institution so the
first insight here is when we look at
this graph so this is looking at
articles published year by year by year
and saying what can we see usage isn't
on this chart but we do have social
media which is blue mentions yellow
captures and citations so if you look at
the red bars on this chart and I guess I
should point out this this line across
the top is the total number number of
artifacts in a given year so that's
pretty constant year over year and you
can see the the citations for the last
two years and 2013 and 2014 the red bar
is barely visible I showed this slide at
a talk last week the week before and
someone raised their hand ago you better
tell pit they have a problem their works
not be
cited in the past two years like no no
that's not it at all what it is is the
citation counts lag they're just not
there yet but the other categories of
metrics the other things you can see can
help fill in that data to tell you
what's happening more recently so these
metrics provide a feedback loop like the
the Fitbit you wear on your arm or the
speed limit sign that adjusts and tells
you your speed on the as you're going
down the highway a little bit too fast
they're giving you data that you can
react to and much more real-time and
understand what's happening so here's an
example of an article in pitts
institutional repository so the article
level metrics for this include their
download counts here you know 75 times
it's been downloaded and this bottom
graph is what you would typically see in
a you know in an institutional
repository of what download metrics are
there but you can see that you can fill
in across the other categories of
metrics the other types of usage that's
happening all different ways to tell the
story about what's happening you click
on the the tweets about this particular
article what you can then see are the
conversations that are happening in this
case you know a potential funder is
tweeting about their work you can see
you know different parts of the world
interacting with it researchers
connecting the dots and this kind of
data when embedded in the in the IR is
much more of a carrot rather than a
stick this is you deposit here you get
value back out as a researcher and it
helps make that you know that feedback
loop and go full circle one of the other
things that you can see or I talked
about captures those bookmarks being a
good leading indicator so looking kind
of with a more publisher centric hat on
you can start to look at the different
journals head to head and say you know
look over a particular time period and
say how much are they capture and how
much is this leading indicators show one
journal versus another you can look at
you know social media / journal the same
kind of thin kind of things and drill
into the data from there the net
case today I want to talk about is
Autism Speaks and see if this auto goes
or not this is um typing autism into
google scholar like doing a quick Google
Scholar search what's being pointed out
here are the the publication dates 2001
2000 2002 if you do any general search
like this on google scholar you're going
to be surprised if you look at the
publication date there's in general
nothing newer than a decade ago that
show up on your top results in Google
Scholar and the reason for that is if
you look at the decided by counts that
are excited by 552 things of course if
your relevancy ranking is based on
highly cited things you're going to
favor old things so if you are trying to
get the work you're doing exposed
there's a gap here so Autism Speaks used
plum analytics in order to tell their
community of what they fund you know the
world largest funder of autism research
and they wanted a showplace of saying
you know they're out there raising money
twenty-five dollars and fifty dollars at
a time at you know it walks and things
like that they want to tell their donors
what they're doing they want to be able
to have their researchers see what other
researchers in the field are doing so
here this is one of their one of their
grantees and you look at the publication
year here and all five categories of
metrics or something to tell there's
data that they can go in and explore and
dig into they fund different proposal
areas and when I first showed this graph
to to my colleagues at my partners at
Autism Speaks one of the interesting
things was they were very excited by
this right it's five bars showing their
different proposal areas biology
treatment and prevention screening risk
factors and dissemination and they
immediately said oh my god this is
amazing it's like how you know
like I see five bars I'm glad you like
our new reporting but what do you see
here and what they said was we always
thought this but we never could prove it
that this one we fund treatment and
prevention the gray bars is how many the
the ratio of the amount of social media
activity around treatment and prevention
is very high and if you compare that
with captures treatment and prevention
barely happening and for this data set
not a single citation and any treatment
and prevention article so they're saying
we know this has impact for our
community and now we have metrics that
that back up some of the funding
decisions were making similarly beyond
just the numbers being able to discover
those conversations that are happening
what are they saying and where are they
saying it is something else they find
important and to make this all public
that this site will be public and open
to the entire autism community and it's
something they're very excited and proud
to showcase what they're doing and talk
about ROI in a brand-new way right they
they've been talking about ROI of what
they fund the same way for the past 20
years they finally now have new data and
something you to tell and not just
Autism Speaks as funders in general last
case study really quickly is ohsu there
a medical institution and and one of the
things that they're doing is working
their library is working jointly with
the office of research to say we want to
compare like with like we want to
compare what we're doing in different
disciplines and there was no easy way to
do that so they took mesh keywords and
that ontology and created a custom
ontology based off of that that has
everything at the same level of
granularity went through and tagged
their researchers output which they then
can feed into a metric system like plum
X and be able to look in this case at
metabolic bone diseases compare all of
their researchers output to that and be
able to find well what are the what are
the publication outputs that have the
most impact and our own researchers
output around this very specific field
so it's taking again that intelligence
up front of how to struck
for the data of what kinds of questions
you want to ask being able to then have
the things be all the metrics and data
points be automatically generated so
that then you can you can analyze it out
the other end other things that they're
doing are working with the expanded nih
biosketch to be able to include these
alternative measures of impact as a part
of their researchers proposals and
working collaboratively to do that as
well as really focusing on lay and
clinical outcomes and things again that
go beyond just traditional citation
counts to help tell that story and with
that I will I guess we're in panel mode
right okay so we've got a few minutes
left for questions we kind of took you
around the world there with how screwed
up everything is and the questions that
aren't answered to some actual case
studies that kind of show the ways in
which people are using the limited data
in creative ways to at least make some
comparisons and learn some things so I
will open it up for questions what so
that the early session everyone can I
had to answer the question about
platform lock-in and you know what are
you doing with this so Mike Taylor
opened this session with a discussion of
the importance of openness and
transparency for four metrics and I
guess the question be4 for both of you
is how are you there i guess you know
leveraging the power of openness to
promote you know community engagement
and trust in the data that you provide
sure i think the biggest thing is to
make it auditable right there's no
secret hidden formulas there's nothing
that we're not exposing the data and if
when possible giving a human readable
explanation to click through and drill
down so making it transparent as
possible is I think the best way to to
address them
so willing is asking me a question I
didn't mention that we provide data and
have data I think that's what you're
asking about we collect data globally
about what funders are funding at a
project level that's the data that we
have and one way that we are making it
open and transparent and helping as an
example one of the many things we do the
data that we collect we feed it back
into orchid so if as a researcher you go
to orchid to sign up for an orchid ID
you can automatically pull in all of
your funding from our database so that
you don't have to mainly key in all your
grants which then gives everyone who has
access to all the orchid public data
files all of that records where it's a
person link to their funding as well as
the publication's their employment work
history as an example great um Andrew
you're just mentioned something very
interesting in your presentation of
questions for you but if anybody else is
an answer to that would be great you
said that the interest level that people
express by clicking and saving things is
a precursor potentially a preview of
citations and curious whether that's
testable and true in other words I want
to believe that that's true I'm not sure
I do believe it that's true because I
think that sometimes people are
interested in things in ways where it
informs them but they're not necessarily
going to cite it later and so I think
there's value in full but I'm curious if
it's really Carly yeah I mean there's
been some good studies done on mendeley
data right because men delays made their
data open and available and research
studies that are showing a strong
correlation with mendeley readers and
citations so it's it's new right it's
early so we don't have you know we have
ten years of data accumulating from all
these different sources not 40 years of
data but it's just really people are
doing that research to bake it in and
some of it you can you can start to see
that right you look at these studies you
see the strong correlation and as we
instrument more and more proxies of how
people are using this
hopefully the data trends continue to
continues to trend that way
so might you say we understand citation
based metrics who is we because given
that administrations and people making
very important people decisions about
people's lies don't seem to understand
citation metrics I'm quite concerned
that even when we do understand if we
understand the alternatives to these
that we're going to face the same
problem you say that there are a bunch
of questions that we need to ask and
from my mind what I've seen is that the
fundamental one is going to be how can i
make promotion tenure grant decisions
without having to read or understand the
work and with and in a way that enables
me to avoid getting sued how you know
this seems to be the elephant in the
room which is that is that this stuff is
just being misused period yeah when I
say the metric citation based metrics
are understood perhaps it'd be more
accurately to say that the material
exists for people to understand them
should they choose to read it there are
some really good papers available freely
available that any person can read that
surmise is 40 or 50 years worth of data
that talks about motivations and
discipline differences and so forth it
is possible to understand these things
being an idiot with numbers is not
restrained to people who have to make
decisions about these things the
question about making tenure is one of
the things I mean you probably heard me
say this before Jeff that i live in
horror of somebody making funding
decisions on the basis of some of this
data like I say my big worry is what
happens if somebody takes it seriously
at the moment it's not in a position to
take seriously there was an example used
by someone from I think that from kudos
about a usage data and in this
particular case she was using as a great
example of how you can get readership
through having a social communication
policy as a research if you use the
example of a team of researchers who are
basically constructed a marketing team
around the
what they were doing with the ambition
of making their paper the number one
down bread downloaded read article on
pause for a given time period and they
had a whole strategy and a set of
tactics and they they managed to do it
now there's nothing wrong with the paper
there's nothing bad about the paper the
paper is probably a good when I wouldn't
understand even the abstract frankly but
nevertheless there are such distortions
available as Andrea said taken cold just
looking at these numbers without the
context about the ability to look at
trends year-on-year trends disappearing
differences trends these are just
numbers perhaps of how well these things
are performed what is interesting is
when we look at things like mentally
mendeley counts are a private activity
right so I'm using mentally i upload a
document to it I don't do it necessarily
the intention of telling the world that
I have just added a link to it so you're
measuring a kind of private behavior
whereas for many of these types you're
looking perhaps at the products of what
happens when you invest a press release
behind it what happens if you write a
blog about it that can be understood by
journalists what happens if you have a
large network of friends and we don't
have the data to be able to draw any
kind of conclusions other than site the
the numbers themselves as somewhat of a
counterpoint to that though when there's
people can game all sorts of metrics
right and and saying hey we're going to
put a campaign to get the to be the most
downloaded paper on a certain platform
is one case and what are they going to
use that you know that researcher what
are they going to put their name on
behind that data to say yes this is
something I did and I'm proud of it I'm
going to put it on my tenure review
packet they'll get they'll get found out
I'm an example that I'd like to share is
an early career researcher who was
trying to put together her tenure and
promotion package and she had written a
paper it was in the humanities and she'd
written a paper that kind of took down
the standing wisdom at the time and went
out there and said yeah this is wrong
and she wrote her paper and time was
going on and it was not really highly
cited and she's getting nervous right
like this I
I've invested all of what i want to do
on this and i'm coming up for tenure and
it's not there it was right at that
point that she looked she was given
access to her plum x profile to look at
her papers impact and that happened to
include usage data in this case that she
had not gained she had not didn't even
know that it was available to her and
she was able to show that this paper in
a very niche field even though it wasn't
highly cited had been downloaded over a
thousand times from their institutional
repository and she was ecstatic right
because it gave her new data that said
look it's starting to get cited and it's
a way to show the impact I'm having in a
non-traditional way and you can be sure
she included that in the you know
defense of the work that she's doing so
you have to there's different there's
different ways to look at it and
different times when these metrics make
sense to use and other times when they
don't any other questions
I think this puts us two minutes over
thank you again for the panelists and
thanks for you each year microsoft
research helps hundreds of influential
speakers from around the world including
leading scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>