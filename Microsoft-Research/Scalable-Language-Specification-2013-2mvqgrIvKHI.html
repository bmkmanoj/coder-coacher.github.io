<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scalable Language Specification 2013 | Coder Coacher - Coaching Coders</title><meta content="Scalable Language Specification 2013 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Scalable Language Specification 2013</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2mvqgrIvKHI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
uh thanks to her and Peter the
invitation to be here I thought I would
talk about scalable language
specification words scalable has been
tossed around several times in the last
few days so I thought I would start by
just picking apart some different
notions of scalable that we might
separately have in our minds because
there are lots of some sometimes might
have been completely clear so one is
whether we're talking about big
languages or little languages several
people have been talking about quite big
languages and slightly to my surprise
very few people have been talking about
the things that semantics people usually
spend their time working with which are
usually the use of more modest-sized but
possibly very subtle calculate and
sometimes these semantically designed
research languages so there's an example
that some colleagues and I produced a
few years ago with you know sort of an
80-page definition full of rules and
syntax and operational semantics and
type systems and that kind of thing so
we might be aiming to address any of
those or all of those another thing that
there is a lot is the subtlety of the
things you might easily have large but
very boring languages a very different
thing is the requirements that the
language spec puts on the meta language
that you're using which I'm going to
talk about quite a bit a important but
disgusting thing is the amount of
contingent goop goop gloopy slimy oily
stuff good work coupe another
interesting thing is the rate of change
of the language that one thinks one is
trying to help the design analysis
itself which can be very high some of
these and sometimes very very low
of those something that some people here
are very concerned with but I am mostly
not is the scaling of tool performance
with program size because mostly today
at least I'm concerned with just
defining the rectory languages in some
sensible rigorous fashion if you can't
even do that then the time to start
thinking about tool performance has not
yet started okay so so that's like
preamble so this is going to be a
slightly strangely structure talk
because it's not going to be me selling
you a thing and then a big punch line at
the end telling you that telling you
that you should buy it what it's going
to be is me taking some snippets from
five different research project kind of
things and trying to tease out some
punch lines from each of those you may
find some common themes appearing in
those punch lines may not so the first
snippet scalable language specification
a large scale language specification
well that's a lot like large scale
specification right we might look at
some other large-scale specification
activities and look at what we had to do
for those specifications and see if
there's anything in common so let me do
that this kind of activity builds me a
large-scale specification you could do
that at design time or you can do that
for some existing artifact so let me
talk here both of those are good and
valuable let me talk here about doing
that for some existing artifact so some
brave colleagues and I spent quite a
while on and off specifying the TCP
network protocol and the socket API for
that protocol so in linguistic terms
this is very very simple there's an a
programmers level interface which just
has you a few calls to bind and
lect and read and write and that kind of
thing and a network interface on which
IP packets get dropped on the wire and
pulled off the wire there's no inductive
syntactic structure here whatsoever so
in that sense this is not a language
problem but it is a specification
problem the behavior of these thing is
is extraordinarily complicated and
unsurprisingly terribly badly specified
even though this is an interface which
most of the software and machines on the
planet have to deal with so what can we
do to build a specification for that so
what we did was something that we called
experimental semantics which was not so
common back then still not so common
despite the efforts of our fine
colleagues in the back left corner so
what we did was to start off reversing
reverse engineering a spec from the
textbooks the code the industrial
specifications and thats back at the end
of the day turned out to be fairly big i
made about 25,000 lines expressed in a
hole for about two-thirds of that was
comments there will not not to 23
comments and then so you sit there I
imagine you fat there for two years just
writing into an editor buffer these
25,000 lines then of course you should
not believe that and therefore the
obvious thing to do is to experimentally
test it so we generated a big collection
of implementation traces from actual
implementations of bsd and linux and
windows and whatnot and check that the
specs admitted those traces and
otherwise well maybe that's a bug in the
code maybe it's a bug in the spec and I
way we have to fix something and go back
to one go to being the operative word
here so echoing things work that
three-run was
testing against semantics it's great
it's the most low-cost straightforward
activity that you could possibly do at
least assuming that you're in this sort
of world in the first place and as a way
of engaging with systems people it's
great because they can understand that
and especially the thing that we were
doing here was testing this
correspondence between implementation
and model step by step along the traits
so we were very intentionally looking at
the details of the state at each point
in so far as we could match them up
that's a very discriminating way of
testing what's going on and we found
some bugs in the implementation that you
could ask me about offline but you would
never find by normal testing they would
just take absurd quantities of testing
so so that's building a semantics
somebody said maybe several of you have
said a specification should be
executable that's a good thing and there
was you know a lot of nodding going on
as if we all agreed about that so what
pens really so let's look for a moment
at the expressiveness of the language
that we need to write this spec and
about what that means for execute
ability so it has to be a very loose
specification and it has to be a loose
specification because there's a lot of
variation between the implementations
you know different algorithms for
reassembling packets and picking
sequence numbers and things and within
one implementation there's a lot of
runtime non-determinism coming from
timing or particular sequence numbers or
operating system scheduling all that
kind of stuff so we have to be writing a
very loose spec and it's going to be a
relational thing on a functional thing
end of the day we also have to have
a fairly complex structured state with
finite maps and lists and records and
all that good kind of stuff and in order
to match the implementation maybe at the
end of the day we couldn't get rid of
all of the imperative structure there's
some large chunks of C code that
effectively define the behavior of a TCP
endpoint and we didn't have to you know
model every stake change but at the end
of the day we could only chop those up
into chunks of imperative stuff so we
use some relational monarch structure so
this is saying you need a rather
expressive specification language and
indeed we used a pretty expressive
specification language is the higher or
logic of hole for we didn't need
terribly fancy types and so that was
enough but we're in higher old logic are
things decidable know let's think again
about execute ability what do we
actually need well what might we want
given a spec given any kind of a spec we
might want to do lots of very different
thing so one thing is just run the spec
you know like an implementation as an
emulator just resolving that non
determinism any old how that's useful
another run the spec as a Dean monic
emulator using some heuristics to pick
interesting hulls yet another to use
suspect to calculate the set of all the
behaviors that the speckle owls for some
scenario and sometimes that's very
useful because that's what you want if
you want to be able to compare that
against some set of practical
observations and last you may want it to
act as a test Oracle if you've got some
particular observation you may want to
decide whether the spec allow that or
not so in a deterministic world these
clams but here and they're all
completely different and in fact the
last one is what we needed to do we had
some experimental e observes trace
picked out off the wire of some test
Network and a definition of the spec of
some transition relation and an initial
state and we had to be side where there
exists that path and you can't do that
in general but in hole for excitable but
for the particular spec we could build a
whole big pile of tooling that could do
that in practice okay another thing that
you need it's a big spec this goes
without saying too many people but not
all of them you have to be able to do
production quality typesetting and
comments and documents structuring and
all of that kind of stuff so we spent
quite a while building ad hoc tooling to
do that kind of thing so if I show you
this is deliberately slightly fuzzy this
is a little bit of that's that spec this
is one of those one of the rules it's
quite a little bit this is a rule
dealing with a packet coming in like a
it's a passive open receives a syn
packet send us in that packet if that
means anything to you there's a state
which takes a transition to another
state and here is the side condition on
that rule you can see it's on the side
mostly and you there's a lot of little
comments in here that's an careful
structuring there's use here of record
record update patterns too so you can
frame out all the stuff that you're not
talking about echoing something Gregor
was saying beginning yeah there's a lot
of attention paid to presentation ok
that let me now shift to my next little
snippet one of the things you want from
a scalable language specification is to
be able to do some trustworthy meta
theory if you have a type language you
might want to prove some type soundness
results or in something
is you might definitely not want to
prove some types town this results so
what can you do if you want to do that
well you got two choices basically you
can do pencil and paper proofs or late
fruits or you can do mechanized tricks
and pencil and paper proof that easy but
volts and the mechanized proofs are
correct but painful to anyone who
believes that they can do correct pencil
and paper proofs or non-painful
mechanized proofs then you're a better
man or woman than me we've certainly not
been able to do either of those so a
group of us back in 2005 observed a
situation and said hmm wouldn't it be
nice to have a little challenge problem
so we could figure out how and whether
people can do mechanized proofs in an in
the best possible style and document
that style and we took several lessons
from that but one lesson is that any
language specification worth its salt
has to be able to support mechanized
proof of meta theory if there is any
method area that you care about and that
doing that for real needs you to be able
to produce idiomatic or at least pretty
idiomatic definitions in a good proof
assistant now it's all good for
assistance may not matter terribly much
which one but for each particular one it
does matter what style of definition you
produce you know with the right sort of
inductive structure and without too much
gratuitous coding and so on otherwise
that level of pain just becomes utterly
unbearable okay so that's a useful thing
to know so then let's go back to that
those different kinds of semantics that
we might be aiming for
to get the start so what kind of cool
support for the practicing working
Samantha cyst would actually help them
get on with their job so back to that
state of the art this is an extract from
the source of one of our fine so fine
papers written carefully by some very
careful it's you students and postdocs
in the best tool available at the time
you do get that presentation it's good
you can just about do it for small
calculus but it does get error-prone
even for that I know because of the
arrows that we made but then you just
get this this overhead of that markup I
mean really it's just disgusting and the
lack of any kind of checking of any by
even basic sanity properties is just
ridiculous not having any support for
checking conformance between your spec
and your implementation is ridiculous
and informal proof hopeless option two
is directly use one of the provers for
example one of those four so you get
some of that automated checking but a
lot of other downside so you more or
less you don't really get production
quality presentation from any of them in
my opinion all of them have some
facilities some more fancy than others
none that were sufficient for our
purposes you have quite a bit of
syntactic noise you do have some
non-trivial encoding you have to do but
then these are maybe the most serious
thing is you have quite steep learning
curves and long learning curves and you
have a community partition right there
are I know there's Andy Pollack and
three other people in the world who can
really use more than one prove ER in
affluent way so if you were to spend
your time saying yeah I shall make a
semantics for some important API like
why not tcp
and then introduced a whole for spec and
then you put it that out to the
community and who can use it the people
who are already whole for experts nobody
else so this is a big problem if part of
your business is making specifications
usable insightful useful specifications
of the infrastructure that we depend on
salute absolutely okay so so what can we
do so oh yeah sorry another piece of
state-of-the-art state-of-the-art 2005
language is with a sort of vaguely
mathematically rigorous mainstream
languages that I was vaguely familiar
with with a sort of rigorous definition
lit sorry sort of rigorous normative
definition lit and that was full of
mistakes too language is defined by a
few words and maybe an implementation
flip okay so so what could we actually
do about that so here is a very
straightforward kind of thing that
Francesco Zappa nardelli and I embarked
on a night of scott owens and some other
people joined us but was to build a very
ad hoc tool to help us do the kind of
semantics that we normally did so what
it does you can write a definition of a
language syntax and semantics in what we
thought was a nice way and then it
compiles that two definitions in latex
and and hole and as well and
they're also LEM and a bit of
boilerplate in programming language and
it built a some kind of a part and
pretty printer for symbolic and concrete
of that language so you could have
examples in your papers that had
actually been passed okay I'd like to
draw your attention to how radical that
currently simple notion is and there was
also so we were doing a lot of thinking
at the time how you specified binding in
language definitions which turns out to
be much more such a problem than it
ought to be so hot included in fact the
genesis of it was a language for
specifying binding structures so what
does it look like so i'm going to show
you the smallest possible example first
I think you probably know what that is
so what do you what'd you say if you
want to define that he not if you want
to write a bubble paper in which you say
I've just had this great idea of the
untyped call by value lambda calculus I
want to write it down as directly as
possible you write something like that
so this is an odd source file it's
actually literally an ox horse I'll
accept that really you'd have to put
these on different lines value in term
and so forth on that final you have
those annotations on the right term
valuable lambda app you get a silver
staff paying attention and we will add
those in just a couple of seconds okay
so leaving aside those words and the
font of that identifier and the
typesetting of that symbol you write
that so the the blue stuff here is all
as it were keywords and the black stuff
here is all user notation and user
identifier so you see it doesn't know
very much you written down here you've
introduced
class of meta variables you've written
the song context-free grammar using some
tokens or other but you didn't have to
say what you've written the grammar of
values and you've written a few
inductive rules yeah that's already
enough to for the system to part it
untie jacket and give you some basic
basic consistency checking as a
consequence which is fabulously fabulous
for useful very simple fabulously useful
and that grammar notice it's defined
thought of the concrete syntax of terms
although we haven't actually told it
enough of how to do that you know enough
for it to know how to do that in a
serious way we have been totally impress
identities or what have you it's also
defining the really what it's defining
is the grammar of abstract syntax that
we're going to use in a semantic rules
it's also defining some meta syntax that
we're using in a few places it's
actually quite a bit going on there
already if you just look at that rule
there's some symbolic meta variables and
a few non terminals built in some quite
structured way and that naming
convention which is standard practice in
informal maths but not easily captured
in our tools is one of the things that's
letting us find errors and disambiguate
things early there's some sub grammar
relationships which are declared and
checked with reuse but call by value
lambda calculus some user-defined syntax
with judgment forms and formulae and all
that kind of stuff and that's being
passed with some kind of an extended GL
our party so semantic roles are lovely
because they're very small and not very
ambiguous so it's just fine to use more
or less arbitrary context-free grammars
for this kind of thing
okay so if you want to add in that extra
little bits of extra bit of eye candy
then you might say please type set these
things like that inside a math get for
example and you have to be able to
support that kind of thing if you want
people to be able to write their
production quality papers with bits of
semantics in with carefully tuned and
designed notation and then you might add
a bunch of comments and then you really
would get exactly that thing as i showed
earlier what happened oh that's also
enough to let you write and some ascii
syntax embedded in a latex file and have
it caused and typeset properly if you
want to actually make it mean something
which i would everybody here would then
you need to say what binding what in
this case that's quite simple that
doesn't exercise much of the language
and you need to say just tiny bit of
extra data to map that into a theorem
prover definition and the tiny bit of
extra data you need to say is a please
or generate me a substitution function
for single substitution of one T for 1x
and call it t subs and then you need to
say please at this random user syntax
that I've invented I'd like that to be d
sure good into a use of that
substitution function and this random
user syntax which I've invented please d
sugar that into just a tea itself and
then I'm done and push the button and it
tells me some error messages and I fix
those repeat like that a couple of times
and each time I say thank you opt for
finding line stakes in my nurse and then
it spits out definitions in your
favorite favorite theorem prover
whichever one it happens to be which are
reasonably idiomatic unreadable and all
that
so just reflect on the design of this
thing for a second so there's not much
built in there's no there's not much
assumption on the form of the semantics
building to this thing the tool lets you
define some pretty much arbitrary
context-free syntax and some pretty much
arbitrary inductive relations and then
it's letting you like those ways of use
special casing the later output you can
override its defaults you can add
clauses saying please map this type of
this production into that in this
particular theorem prover which you
sometimes need to be able to do and then
because we're really trying to support
our normal working practices there's a
whole rag bag of miscellaneous features
which are very handy which are not the
no sense is there a principled
collection of these things these are
just things which are useful so there
are ways of defining sub grammars and
grammars for evaluation well not
evaluation context but grandma's for
contexts because we're trying to be
reasonably general hair stuff to
automatically generate substitutions for
free variables and binding and whatnot
support for this kind of lists all kinds
of extra ad hoc worry does it work I
tell you three small well three I tell
you small facts about three examples
built using this so one was by rockstroh
neatza who is a PhD student here and
went off and looks at a proposal for the
java module system before it had been
after it had been sketched but before it
had been finally stamped and he
formalized that I found some flaws in it
and discussed those I think they changed
the design a bit and then he
new design unfortunately we were
slightly behind the curve just in timing
right there standardization process and
the period interval in which they were
prepared to revisit their choices was
almost closing by the time that he had
got up to speed and really engaged with
it so otherwise he might have had even
more effect people talk about modular
semantics here so rock had three
calculate a core java language
lightweight java and a jar extended with
some kind of module system based on
their original proposal and a different
one with maybe a better on your system
he argued and you can see here the
overlap in the definitions his hot
definitions of those calculate so this
was almost entirely reused in that and
that was almost entirely reused in that
and likewise rather air kind of a
picture to be able to show the overlap
in the proof scripts of his Isabel
proofs that he did about those of
definitions so this is actual reuse
within a family of closely related
languages which is doable and valuable
let me look at the next example so
people have already mentioned it's got
antes o camel light so I was going to
print out a copy but apparently the
printers don't scale to this kind of
semantics see if this will work yeah
there you go lesson semantics
and now you know what I camel itís
wasn't that good so this is an
interesting scale because it is much
more than people normally address in you
know some conference paper or other it's
quite a lot smaller than all of Oak
camel or F sharp or what have you but
it's not sort of devoid of subtle tea I
mean if you can just sort of zoom in
here somewhere there's some some rule
some of these rules are quite boring so
by chance I've come up here with the
application role so you can hopefully
just about say the battery yeah this
looks huge is that because it also
includes some details of like the Delta
functions the you know the details of
arithmetic and things like that on the
whole lot I think this is what it is so
sorry so what did what it is is the
operational semantics and the type
system for ya so it's that much
operational semantics and that much type
system and it includes so all of the
core ml like stuff that you might be
expecting so the language includes
semantics for floats but this is not
including this these two pictures are
not including the details of the float
operations and all that kind of stuff
this isn't including the standard
library stuff no not this this is clean
and tidy people want JavaScript not
because it's big so sorry I lost my
chain absolve yeah oh yeah so so there
are some know you said not without some
candy Thank it so some of this is quite
straightforward
and some of it is a bit less so yeah
that's not what we wanted that's what we
want it so this is the kind of rule that
you might find in a nice efp paper or
whatever this is some kind of a typing
rule for records this is not very subtle
actually but there are you know there's
some stuff going on there are some
tuples of derivations and a bunch of
typing assumptions on fields and some
other kind of stuff so I don't want to
talk about that in any details I just
want to show you what you would have to
write e not to express that role and
it's that looks sort of the say this I
hope it looks sort of the same and now
looking sort of the same is what lets
you casually edit the collection rules
in an emacs buffer and email them to
somebody else and check them into a
version control system and read the diff
when somebody else changes it in a
version control system and all of that
kind of stuff so getting rid of the
listen tactic noise is more valuable
than what one line of imagined third
usage is this used by practicing
semantic cysts designing new things well
so Steve Eric and her colleagues
including Simon although assignment is
safely upstairs at the moment I'm not
sure Simon liked this part that's
definitely made him used ops to design
three generations of the Haskell core
lambda calculus I'm not going to tell
you anything about the Haskell call and
calculus but I'm going to tell you
something that Stephanie said about it
we had a little paper together in the
workshop on mechanizing meta theory here
is that little paper so it's definitely
wrote this side and we wrote that side
Stephanie said I plan to use opt in
every new paper that I write in some
form oh the tool has become an important
part of my design process and I've come
to rely on purpose of this part of the
talk is to explain why and she goes on
to explain that these these facts of
being able to easily manipulate the
definition and toss it around with
colleagues and get this basic sanity
checking very quickly we're just
invaluable to her alright so as a
language design tool it does actually
work as a semantics design tool does
actually work okay so good but art okay
there's a lot of butts so so one but
perfectly legitimate is that you might
choose especially now hello not so much
in 2005 to just work directly in or
hollow as well they've all got a bit
better we understand better how to use
them certainly doable you still don't
get all of those same things it's more
heavyweight you don't get that
production quality typesetting you have
to do your own encodings of something
think that Oh certainly doesn't do so it
generates this positive for symbolic
terms but it doesn't generate a
production quality partha or 3d printer
that you can use in a standalone
language implementation and it would be
great if we could figure out how to tell
it to do that and surely the user will
have to add a bit more information to
make that possible the type system when
I say type system so this language of
context-free grammars with sub rolls
there's a type system there's no para
met rissa tea in it and that turns out
to be surprisingly annoying because not
only do you want to write stuff in your
user-defined abstract syntax of the
language or defining sometimes you
mostly just write that stuff so for a
camel light you must you just write that
stuff and it's fine but sometimes you
want to write a whole pile of auxiliary
definitions in directly in higher or
logic or in some kind of functional
language and for that you really need
just a bit more power electricity
otherwise it becomes too annoying and
doesn't at all produce executable
semantics well clearly you don't have to
do that because I've just described
three very useful cases in which we
didn't do that at all and we still got
some benefit but it would be nice to do
that so now let me switch my balls
little snippet so this was motivated by
our experience doing work on sort of
standard programming language semantics
and after that several of us started
working on other kinds of semantics so
specifically machine semantics and on
the concurrency semantics that you find
in shared memory programming languages
and what we found then was that we were
not on the whole working with an
elaborate inductive syntax the
particular semantics that we were
building didn't involve that kind of
syntax at all instead what we were
writing was sort of standard hyerollah
logic functional programming and
inductive relation definition and we
wanted a similarly lightweight way of
doing that so we proceeded to invent
Scott Owens was in one of the main
initial guys in this another metal
language that so art is good for
user-defined context-free syntax and
inductive relations lem does not support
user-defined syntax in any way to speak
of and then lets you have rich
how many of pipes and functions and
inductive relations and in fact you can
tell what to generate them definition so
the two are somewhat integrated although
not as timely as they might be so it's
the same kind of story so you write some
definitions with useful stuff the
definition language it's sort of the
intersection of the definition languages
of those provers and then it's giving
you lightweight stuff it's giving you
type checking decent typesetting output
that insofar as possible preserves
comments and why its place and so on so
you're getting idiomatic readable
definitions in those targets and so my
camel boilerplate and in fact now
someone's working on the inductive
relation execution too so let me give
you two examples of usage of that so
this is sort of XR cartels work on
building a semantics for the concurrency
behavior of power and arm
multiprocessors so this is again this
kind of experimental semantics deal
we're not formalizing in an existing
industrial spec because the existing
industrial specs are nonsense and indeed
the architects don't always know what
they intend they know what they bill or
pretty well but they've always know what
they intend the architectural envelopes
that they specify to be so what we do
here we do another of these big loops we
generate tests fairly seriously we build
test harnesses we run tests on actual
silicon and also inside the processor
vendors they've run them on pre silica
news of this is found various more or
less serious bugs in the processors or
the pre silicon designs and then we have
a model which is a
moderately large scale spec not huge
written in then and we are executable do
we need to be executable here by golly
yes we need to be executable because we
need to compare what the model says with
what the actual hardware says for all of
those umpteen tales and tests and we
need to do that automatically so here we
need to be able to take the model take a
small test program and calculate the set
of all allowable outcomes and we also
need to have a good presentation and
have some friendly architect accessible
English version of the world also we can
argue with them about it because they're
not going to argue with us about the
details of the higher order logic rules
we need to talk with them and then we
repeat and then every so often when you
try to use these models for doing some
software or Hardware verification and
then repeat we provide them tasks so did
you say the hardware guys or the
hardware but the hardware says three or
four the hardware guys have a much more
interesting dialogue in which they say
things like okay I'm not actually going
to tell you this but a micro
architecture might be a bit like such
and such or they may say as far as I can
tell your model is consistent with the
external behavior of my hardware and
with the intention the architectural
intention of my hardware but it would be
confusing to tell my hardware partners
your model because the internal
structure is different that's a nice
thing when they say that all they say
yeah yeah i can get my head around that
all they say that's a good question I
don't know I'll go and ask my other
designer friends and see what we think
we you
to build they say all kinds of
interesting things but without you know
if we were just doing blackbox testing
there would be done hope we don't have
enough information about the internal
structure it's too it's got too much
internal states to infer that just by
experiment in a reasonable amount of
time yeah we haven't we depend on them
being somewhat informative no no no but
but by this whole iteration of
understanding how the stuff is actually
working understanding what kind of
properties they mean to provide
understanding what you need to know in
order to implement high level languages
above these things a lengthy iteration
making wrong model so we made I don't
know how many wrong models on the paths
to this we've used multiple different
version control systems does that help I
mean that's a measure of ok so so
executable excuse for sure
okay so this is the lem model which have
not shown you any of but I happily can
it's compiled by LEM 20 camel code
compiled by o camel toe camel byte code
compiled biochem ljs to JavaScript and
running in the browser it's a highly
highly trustworthy tool chain so I shall
pick from a library of tests a test let
me pick that on that's always good test
so there's a little test this is a
typical kind of a test it's a fairly
small one there's a power assembly
programming called a little assembly
vera gram for this thread and a little
assembly program for that threat and
then I might run that exhaustively or I
might run it interactively and this
although i'm not going to explain the
detail is the state of the abstract
machine of that model and these green
things are the system having computed
that the precondition for those
transitions is true in that state so
then i could say are why don't I take
that transition and the state changes or
I could take that transition maybe this
will be more interesting I could take
that transition and you can see I can
just sort of interactively walk through
that and explore what the model means
and the model is a bit subtle so being
able to do that is invaluable so we in
fact we need to do this interactively
and exhaustive lick in order to get
anywhere
okay another usage so mark batty is a
PhD in here bold and brave PhD student
embark on trying to make sense of the C
and C++ in currency model when you
started doing this they the concurrency
subgroup of the c++ standards committee
had been agonizing me on the concurrency
model for several years maybe five and
they produced some kind of prose draft
proposal in the way of standard
committees you know the kind of thing so
mark came along and said ha I shall read
this or at least read you know section 1
point 10 and chapters 29 and 38 which
with any ones that mattered and he
proceeded to try to formalize this with
several of other people involved too and
find some medium serious but not major
conceptual mistakes and proposed fixes
to those and traipse along to the
standards committee and explain those
fixes and you need explaining those
fixes with concrete examples that we had
run in some mechanized tool we had some
confidence in them was a fine way of
interacting with the standards committee
whereas dumping two thousand lines of
higher order logic on their heads would
not have worked I know that it wouldn't
have worked because even though so these
are very smart people who do know what
they doing in many many ways there now a
lot more about the languages and about
compilers than we do but if you
accidentally show them you know some
kind of an upside-down a then they will
accuse you of perpetrating Greek letters
and the whole I give you not the whole
Tony in the room it will go down and
cold and it won't work so well so one
has to be careful to sort of go to them
rather than vice versa so here we have a
model but it's not an operational model
it's what people call an axiom
etic memory model not to be confused
with an axiomatic semantics in the
conventional sense but here you might
take some example program I think my
internet might have gone away so maybe
this won't work maybe I'll pretend so
you take some example program and then
you run it I know it worked and it
calculates the set of all model
allowable executions there are eight of
them here each single execution looks a
bit like this it's a graph with a bunch
of reading right events and various
relations and then you can step through
them and see which ones are consistent
or not according to some rather cut and
then you might say oh this the coherent
memory use axiom is violated for this
example and then you might go away and
read thee as it gone I've lost it read
the coherent memory use axiom and see if
it actually says what you meant and so
on and then go back and iterate so it's
not always so so that's again sort of
automatic right we're executing the
model more or less as it is but you
can't always expect to do that and
sometimes much more cleverness is
involved so Francesco's a panade le has
just had a paper at PLD last week in
which he tests actual c compilers to see
if they do optimizations allowable by
this model and in order to do that he
had to do all manner of sophisticated
but first just instrumentation over the
compilers following some clever scheme
but also proving facts laborious Lee in
by hand about what kind of optimization
the model allows so it's not going to be
automatic time is ticking along so let
me go very quickly to my fifth little
case scalable language specification
what about scalable language
specification for machine languages x86
there's a good programming language
it is a good programming language
because it's on the whole very very
stable people are going to keep on
caring about x86 for a long time
unfortunately or are more power or any
of those so there's a lot of things that
are similar to what's gone before right
there quite large there's a lot of
contingent group really a lot if you're
going to do this at all you'd want to
make it available for multiple usages by
multiple users in multiple communities
oh it's just not worth the effort you
have to be able to test semantics guest
implementations and again it's a very
loose specs I can't really overemphasize
the importance of this enough if
everything is if you're in deterministic
land then you can just have specs which
are pure functional program is written
in almost anything doesn't matter and
you can do some of this testing stuff
but when they start to be more
non-deterministic in any of the many
different ways life is more fun but it's
different from normal languages in that
there is no there's a big syntax but
it's very flat no inductive structure to
speak of you can I think get away with a
first-order description of the behavior
of the individual instructions although
then you have to glue that onto the
memory model which is a whole other
story it's sufficiently big that it's
worth having domain-specific language
support for doing just this although
there are not so many processors that
you have to sell that to the whole world
they're not going to be very many
programmers in this meta language
hopefully so I think that we're doing at
load is in fact we were doing some of it
in small way in the very lunch break
here is defining a domain-specific
language actually defined in art because
that would be a preferred tool for
defining language it's like that and
compiling that via Len to glue on these
County models into multiple targets okay
so here are here were five little
snippets are really four different I've
told you about four different meta
languages and they're all quite general
purpose so none of these except possibly
the last time have any built-in
commitment to you know are you doing a
reduction semantics or a big step
operational semantics or a rewrite
system or anything like that they're
quite general purpose things but their
expressiveness is sort of tuned for the
application so here we seem to really
need all of hole for here in the common
case for programming language
definitions a bunch of inductive
relations is just fine here we really
needed a little bit more polymorphism
but not terribly much and in each of
these we're putting in a lot of work
into supporting the kinds of activity
that we do as supposedly working smart
assists and focusing on expressing the
stuff as cleanly as possible but paying
a lot of attention each other thing is
okay so before I stop um let me see how
I say about this nodularity I use an
interesting word here it sort of
suggests an analogy with modular
programming and we know you wouldn't
want to write programs as a big
monolithic pile of code that would be
bad like old-fashioned let's just think
for a second about the scale that we're
talking so I've said oh here's some
enormously big scary specs and so of
some of you in some of the earlier talks
and what's an enormously big scary spec
so
that TCP mole that was nine thousand non
comment lines of hall for and that was
enough to continue with several people's
attention for several man yes the that
power operational memory model has like
three or four thousand lines of limb
code so by the standards of the
semantics that you might find in an ACM
9.2 column paper that's quite big by the
standards of the pieces of code which
we're used to chucking around every day
that's quite tiny and that's on a scale
where it's not that painful at least if
you've got some tool support identify
the stupid errors which we do is not
that painful to go through and apply you
know search and replace operations and
cut and paste and generally hack it
around so for defining these languages
enough when working with a family of
related languages asrock did in LJ then
it absolutely identifying the modularity
which is there is very valuable but in
the grand scheme of things it's not the
same problem a software engineering it
is a different thing we would love to
have so if we could do if there was a
realistic prospect of doing modular type
soundness proofs in a way that would let
me reuse some substantial chunk from one
language to another that'd be lovely but
then I look at the o camel type system
and the Java type system and the lem
type system and the any pretty much any
other substantial type system that i can
think of and i don't see that much
commonality so i think we need some
other we need some other thing which is
not the software engineering notion
because we don't on the whole
want to compose these things from and
then only do proof or apply ad tools to
the results of those tools apply to the
components we have more freedom than
that because the scale is not on that
enormous and that so there's a lot of
places where you know I think one can
see that one can use a bit of semantic
engineering rephrasing one's definitions
in a more convenient style to pull out
the modularity that you may have with in
some particular family I think I have
gone long enough so let me end by just
putting that up there from 49 years ago
and asked you to contemplate how far
we've got since then if anywhere thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>