<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>eScience 2014: Welcome, 2014 Jim Gray eScience Award Announcement, Opening Keynote | Coder Coacher - Coaching Coders</title><meta content="eScience 2014: Welcome, 2014 Jim Gray eScience Award Announcement, Opening Keynote - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>eScience 2014: Welcome, 2014 Jim Gray eScience Award Announcement, Opening Keynote</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gQ3fIAUl078" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
like to talk about the Jim Gray a
Science Award it's hard for anyone to
work for very long in the field of e
science without knowing about Jim Gray
Jim truly was a leader in every sense of
the world he was at the forefront of the
field and of the area that we focus our
energy on and he was because of that
leadership we established a leadership
award award at Microsoft which we call
simply the Jim Gray II Science Award so
we established in this the year after
Jim Gray disappeared in the year two
thousand seven we established this in
2008 as a special tribute to gym and
we've had seven wonderful leadership
type individuals who have received the
ward over the last seven years and here
you see them all displayed we have one
of those with us anthony williams is
here and his name is spelled wrong on
the slight it's anthony and i call him
Tony he sitting here in the back you'll
hear him tomorrow in one of the sessions
and learn more about it so it really
it's my great pleasure today to announce
the eighth winner of the award look a
lot of applause here we come to Paul
Watson
it's always appropriate despite the fact
that probably most of you already know
Paul to say a few words about him and
these words here are displayed on this
slide it says a little bit about his
background it's very hard to capture
ones background in a simple slide so in
a few minutes what we've asked him to do
is actually talk about a very important
topic which is cloud computing in E
science and so that really I think
you'll learn more about him through his
talking then through all the words that
we can say but for the moment it would
be great Paul if you would come up here
we'd like to honor you
so with that let's welcome Paul
okay well thanks to Harold for for those
kind words it's a real honor and a
pleasure to be here to talk to you today
I wanted to as Harold said to talk about
some of our work on cloud computing and
we've worked on with clouds across a
wide range of E science projects but I
thought I might focus on one particular
project because it's a little different
perhaps too typically science projects
one that I spent most time in the last
three or four years and this is a
project about social exclusions so
social exclusion is one of the great
problems facing society today and we've
been looking at how we might address
this using these science techniques and
in our project we have a wide range of
different disciplines including
sociologists and if you talk to a
friendly sociologist and ask them for a
definition of social exclusion then they
will come up with something like this
about the fact it's a result of related
factors that prevent individuals or
groups from participating fully in the
economic social and political life of
society and that seems like a really a
theoretical definition but actually
affects a lot of people so in my own
country in the UK that's about one in
six people who are affected by social
exclusion and this is because it
includes older people with a range of
medical problems and perhaps suffering
from isolation it includes people with
disabilities and also includes
increasing the young people on the
margins of society and if you talk to
experts in this field about what the
effect of digital technologies have been
on social exclusion and they're very
clear and they'll say that well digital
technologies have had a real effect on
social exclusion and that effect has
been to make things worse and I was
surprised when I learned that but then
you look through the evidence and it's
actually hard to get a job these days if
you can't use computers and that might
be because you don't have access to them
it might be because you've never
developed the skills to use the
or it might be because you have some
disability which actually prevents you
from using computers and then there's
the fact that in order to save money
increasingly governments are moving
services that they're aiming at people
who are socially excluded they're moving
them online and so that actually makes
things difficult for those the large
proportion people are socially excluded
who lack the ability to get online and
access those services that's a sort of
catch-22 problem so there's all of these
issues which mean that people who are
socially excluded on reaping the
benefits that most of us are from the
march on of digital technologies and the
evidence is that things are getting
worse and so this is a rather dystopian
perhaps depressing view of the future
that over time people like ourselves
will pull away from people who find
themselves in this unfortunate position
and so a few years ago a group of us
decided to pull together a project and
have a look at perhaps different novel
ways in which digital technologies could
help people have found themselves in
this position and so this is the project
that Harold mentioned on the slides this
is the side project social inclusion
through the digital economy which is the
one I'm going to talk to you a little
bit about this morning and the aim of
that is to explore this question of how
can digital technologies transform the
lives of excluded people and communities
and it's a project that we do at
Newcastle in collaboration with Dundee
University in in Scotland and it's a
very wide-ranging project so it's about
20 million dollars so roughly at any
time we've got about 30 or 40 people
working on it across a wide range of
disciplines and sub projects and this
just gives you a flavour of even this is
only a small part of it but for example
if you look at the top left that's some
work that's been done in the project
trying to address the problems of people
with dementia so dementia is a growing
problem in society one of the problems
with dementia is people lose their
ability to remember how to conduct a
series of tasks or what they do is they
sell to try and do something big
halfway through it and then they can't
remember what the next step is and so
this was a this is work that we've been
doing where to see whether in the
kitchen whether you could have senses of
sensors around the utensils and the
cupboards and so on which we're working
out what people were doing and then when
somebody got stuck so we got to the
point where they couldn't remember what
to do next then could the system prompt
them neither audibly or visually to help
them to carry on and complete the task
they'd started the bottom left about
mobility so this is if you ask all the
people about their main concern they'll
tell you that it's mobility so the
ability to get around means that you can
go and socialize it means you can go
shopping it means you can see your
friends and family if you can't do that
because you lose your mobility then your
quality of life rapidly diminishes and
this is a particular problem for people
who live in urban areas who rely on the
ability to drive a car and there's a lot
of evidence there's people get older
they start to drive a bit more after
they've retired because they've got more
time and then suddenly there's a cutoff
and nobody's really sure why that why
that is you can interview people and you
get a whole series of results back so
we've got some work in the project with
transport researchers where we we bought
a car we instrumented it so every time
anybody turns the wheel I presses a
pedal we capture that information we
capture where they are on the road and
we also instrument the driver so we we
fit them up with a bio belt which allows
you to measure their heart rate which is
a good proxy for anxiety and then we
drive them around in particular
situations and we capture all of this
information or try and understand why
this why this is and Lolly the work
still ongoing one of the things that
we've discovered for example is that so
if you're in a country that drives on
the right hand side of the of the road
then turning left against oncoming
traffic is a real problem for all the
drivers and one of the reasons seems to
be that as you get older you lose your
ability to judge speed and distance and
so it's harder to work out whether you
can fit through a gap
the traffic that's coming towards you so
so this project starting to get these
sorts of insights and the idea is armed
with that you can then influence
policymakers who design Road layouts and
people who design cars in order to try
to make them better for people who are
older and suffering from some of these
some of these sorts of problems the top
left so we do work with musicians and so
this is trying to find ways to motivate
younger people to learn some digital
skills so this is getting them to do
some music making kids tend to be
interested in music so they make some
music the capture it and then they're
taught to use software to manipulate
that that music create new music from
the original sounds that they made and
so in this way what you're doing is you
teach them these transferable skills
they can apply in other areas which
hopefully help them to to get jobs but
you're doing it in it in a way which
where their self motivated you don't
have to force them to to do this because
they're so interested in music and what
you can do with big the software that is
provided and then the bottom one we do a
lot of work with people with Parkinson's
disease I'll come back to that in a
while and one of the problems with
Parkinson's disease is tremor sore
people suffer from shaking and the
question is well what can you do about
that so doctors will give patients drugs
but the problem is that how do you know
whether those drugs are effective how
often should they take the drugs what
strength should there be so this is a an
experiment with a sensor which measures
tremor in discrete wear and then this is
actually where the head-up display so
this is google glass we can do it
through a mobile phone as well which
then gives a prompt to thee to the
person suffering from parkinson's
disease to tell them that they should
take take a pill in a particular time
and of course what this means is that
they're not just taking pills at regular
times that are preset they're taking
them when they really need them and of
course the system takes into account the
strength of the medication that they've
taken on so and make sure they don't
take too much of the medication so so
this is for examples across a huge range
that we work on
in the project I thought I'd focus on to
just to to see some of the work that we
do it with cloud computing the debt run
oolitic so this is one on a problem that
a lot of medics have so if you talk to
medics then one of the things that
they'll say is that if they could
understand more about people's lives
then they could provide much more
effective treatment for them because
they could personalize that treatment
and there's a lot of problems that
people have particularly older people
with diseases like diabetes obesity
cardiovascular disease where the effect
of the disease can be mitigated if you
can know something about their activity
levels in their daily lives but the
problem is that at the moment what
happens is that the medic get somebody
into their office and they can ask them
questions that can set them how often do
you exercise how intense is your
exercise but of course as anybody who's
gone to and talk to a medic in some
cases we're prone to exaggerate mean
what we tell medics about how much
exercise that we we actually take and
even if we try to be accurate it's very
difficult to get an ideal picture of how
much exercise we have and whether it's
intense exercise or just light exercise
so the question was could we try and
shine a torch into people's lives so
that medics would know much more about
how much exercise that they were taking
so they could personalize the treatment
for them and so we've been doing some
work with sensors on this and you can
see here we're doing some trials in a
local hospital in Newcastle this is a
patient being with a sensor in the form
of it looks like a watch being strapped
onto their arm buyer by a nurse and I'll
show you a little video of this in
action so you can get some of the idea
of the information that you can get from
it oh oh I'll explain a little bit the
background to the video which is that
when I realize I wanted to do a video
then I talked to my research group who
are mainly in their 20s and their 30s
and I asked them well if we're doing a
video we need a model so we need
somebody who's getting on a bit might
need some help before too long and can
you think of anybody we could use as a
model in the
doing so one name kept coming back again
and again and so you'll see this when
the video starts so this is the
accelerometer its attraction
accelerometer measures in three
different axes acceleration strapped on
the wrist and here's the model coming
down the stairs at Lee at our University
and then going into the one of our labs
into culture lab and if we run this
again it's birds a bit faint actually
but underneath you can just about make
out the signals from the from the
accelerometer and if you can see the
green signal which is the one at the
bottom you'll see Peaks which is as I
walk down the stairs you can see every
step because this the green is down my
hand as I walk so measuring the swinging
arm but you might also be able to see
that the amplitude of the peaks changes
as I walk down the steps or down the
steps the amplitude is greater than when
I'm walking on the flat and so by doing
analytics so we've got people who can do
lots of signal processing then I'm even
tell the difference between walking on
the flat walking upstairs walking
downstairs as well as whether people are
sitting walking running and various
other sorts of activities in between so
what we want to do is capture this data
and we've been doing this over of the
last few years and then analyze it you
know students of techniques to try and
provide a profile of people's activity
oh it's also interesting the other
things that you can get out of this
information so here's another little
video so this is my wife were here she'd
say this is me doing something I'm good
at so I'm sitting down drinking tea and
reading a newspaper and when we got a
split screen in a second then so it
looks like you can't imagine any useful
information but if you can see it then
when I grip the cup then there's a real
spike in the accelerometer and then as I
lift the cup to my lips you get a
measure there's a little bit of shake
they're not too much but a little bit so
medics find this sort of information
useful because as you get older one of
the problems that stops you from living
a
healthy and happy life is if you can't
nebula object anymore so the ability to
grip is really important and it means
that you can cook and clean and
generally look after yourself so medics
are interested in how people's ability
to grip changes over time and then of
course back to the work I told you about
earlier of our Parkinson's disease
anything which allows us to measure
tremor and to analyze that again gives
us this ability to personalize the
treatment that we can make for people so
that's one project I've got another one
later but of course what we need to do
is to be able to extract that
information that's hidden in the data
from these from these sensors and given
the title of the talk and what Harold
said you won't be surprised to learn how
we want to go about how we've been
trying to store share and analyze data
over the past few years and so we've
been using cloud computing have a sense
it first came along that's a I think
that's a Microsoft cloud facility
somewhere in the Pacific Northwest and i
believe is about 200,000 computers in
that one facility which is about i'm
told six football fields in sighs i
think i got this from Dennis Gannon so i
think that means six american football
fields but they aren't a very different
i believe from the more standard
international soccer fields and so
you've got a hundred thousand machines
in one in one facility of 200,000 in
this case and what you can do is you can
go and grab them so you go of the
internet and you can grab these machines
processes and storage as and when you
need it for your for your research and
you pay as you go so the nice thing
about this is not it's not i think not
so much the technology but the business
model so the fact that you're only pay
for those resources that you use when
you use them and we found this is a
really good fit to the sort of research
that we that we doing the research that
goes on more widely any science and
there's two sorts of standard reasons
for this so this one is where you have
grown
in access to dead or a service if we go
back a few years to the grid computing
days there was lot of emphasis then on
people making available analytic
routines as a service and they'll quite
often put them on their laptop or their
desktop they would go to a conference to
present a paper on it it would get
people excited by it and then they would
say and here's the end point of this
service and people would go to that end
point and try to use it and the system
would just collapse so because it was on
somebody's laptop or desktop and it had
nowhere to scale up as the number of
users or the amount of data that was
processing increase but with cloud you
have this potential as you increase the
number of users you can grab more
resources and cook without additional
load and then the other one which i
think is a not just to match the way we
analyzed there and universities and
research labs but also matches the way
in which we think about science is that
is the bursting pattern so what happens
is that we're we get a dump of some data
we want to analyze it so we can grab the
resources that I needed to do the
analysis potentially hundreds or
thousands of machines do the analysis
and then stop paying for those for those
machines while we look at the results
perhaps write a paper about it and then
we might other examples might be an
event so let's say we have some sensors
so measuring river levels in a
floodplain what we might want to do is
when the levels get above a particular
height then we may want to trigger a
model which is going to predict the
effect of the of the rising river levels
over a over an area and effect on on
people and buildings and so we can do
that again we when the event happens we
can grab the resources we need in order
to run the models when we finish doing
that this might only happen every few
months or every few years then we don't
have to pay for the resources that we
need same with new algorithms so
somebody cycling to work on a on a
morning has a bright idea for a new sort
of algorithm they can come into work
grab the resources they need in order to
analyze the data sets that are available
to them in the cloud and see where their
algorithm really is better than better
than previous algorithms and so this
this agility this ability to grab
resources when when we need it I think
this is really has a transformational
effect on the way in which we do signs
but the question is can scientists
actually realize this potential that
clouds offer and I think this is a real
problem so our own work has been with
probably about 20 different scientific
groups and also some science companies
and one of the things that we find is
that there's a small group of scientists
with resources and skills are really
able to do the analytics scale build
scalable systems to so that as the
amount of debt they have to deal with
increases then the computation can also
scale out but these skills are very few
and far between not many groups do
actually have these sorts of this
resolve ability and the sad thing is
that there's this long tail of
scientists who are the people who could
really do with the access to resources
which clouds potentially gives them
because they don't need their own large
servers they don't need access to
national facilities they could just go
out and grab the resources that they
that they need but they're the people
who don't have the IT skills to take
advantage of those resources so again
this is another sort of catch-22 that
the people who could benefit most from
clouds are the people who can't benefit
from clouds because they don't have the
IT skills to do so so so people here for
example you're all in the head of this
of this curve but there's a lot of
people who would never go anywhere near
a new science conference you could
really benefit from the sorts of
techniques tools and techniques that
you're developing you just don't have
the skills and resources to to do so and
the reason for that is I think that it's
just it's just plain hard so I'm a
computer scientist working a computer
science department we take in for our
into our research group people have got
so masters occasionally undergraduates
in computing and really even people with
a master's degree in computing who've
done an undergraduate program and then
advanced program in building scalable
systems they're only just at the stair
where they find they're able to build
out these sorts of scalable systems it's
just so much you need to know and so
much experience that you need if you to
really provide efficiency in the systems
that you build and the reason is that if
you look at what we need any science
it's quite demanding so we need
scalability so all the examples are
showed on the previous slide of the
potential for clouds rely on somebody
who's able to build scalable systems so
as the amount of debt increases the
resources that you give this system
increase in proportion to that or that
can scale out as the amount of
computation that you're trying to do get
beyond the point where you can do it on
a single node and then security so for
healthcare applications such as the ones
that we do within the side project and
other applications where you're dealing
with a confidential date right or
whether some IP involved in it then you
need to build systems that are secure
you need to have control over exactly
who's allowed to see and do what with
the date or in the system and then you
need to build systems that are reliable
and so you need reliability because for
example if you go back to the to the
sensor example what you want to have is
the patient comes back into the into the
clinic after a week of wearing the watch
they're about to see a doctor say ten
minutes later and in that 10 minutes you
need to extract the data from the watch
analyze it and produce a report for the
medic so they can then personalize the
treatment for that patient if the system
fails or in the in that 10 minutes it's
it's down it's not able to do the
analysis then you've lost that ability
for the medic to really personalize
their treatment so trying to find people
who can build systems that are scalable
secure and reliable is actually I think
of very very challenging and when you go
to a cloud you're building from quite a
low base you can grab basic compute
power you can grab storage but it's up
to you to build these complex systems
above that and so we think this is one
of the reasons why a low clouds have
this potential to really transform a lot
of science their effects been really
limited due to the problem of a lack of
skill
so what we did a few years ago was to
think about a different approach and
that's really this one here so rather
than have everybody build from the
ground up a system and application for
their particular project to do whatever
they were to deal with whatever day they
wanted to deal with you whatever
computation wanted to deal with is it
possible to think about some middleware
so it's a middleware which does so
eighty percent of what people need to do
in these applications so the amount they
have to write is much more limited and
the complexity of what they have to
write the software that they create can
be more focused on their application
rather than generic things about how do
you scale up computations across the
cloud how do you build reliable systems
how to build security in your in your
systems and so about nine years ago pre
cloud we're in the middle of the grid
computing era we decided to try and
build out a system to do this and so we
took best-of-breed components that we
could find from around the gritty
science world so we took the the best
distributed file system we could found
we took the best metadata store we took
the best workflow engine and so on and
we integrated these together and this
was to be our platform and the idea was
that we would run this on our own
servers and therefore users could just
go online and use their capability which
we'd provided by already integrating
these systems together and have to say
that it was a complete failure it's one
of the biggest disasters that we've had
over the last 10 15 years and there are
a number of reasons for this one was
that all of these different systems were
almost completely incompatible in some
important ways so for example almost all
them have their own security model and
the security models just couldn't be
made to fit together and then there was
a problem of scalability most them
weren't actually particularly scalable
for example finding scalable work for
engines at that time was was very
difficult so as you increase the amount
of work that you wanted to push through
the workflow engine it couldn't increase
it its performance and the final problem
which wasn't one that we expected but
we'd started to work with some small
companies some starts and some SME who
had some science first product or
service that they wanted to to get out
there to customers and it's surprising
in industry and we found later in a
number of research groups that you're
not allowed to deploy software on your
desktop so almost all of these
components that we were integrating
together had some desktop client that
you had to deploy and people just
weren't allowed to do that or even if
they were they couldn't keep it
up-to-date apply security patches and
and so on so for all these reasons we we
had to abandon this this attempt after
about a year and go back and have a
thing about what we should should do
instead and so what we did was we
thought about and decide to build our
own system from the ground up by then
clouds had become available and so we
decided to build a cloud-based system
and so we had three design rules so this
is work that I did with with colleagues
huger Hyden and Simon Woodman over a
number of years and the three design
rules that we had would would be that
it's going to be an integrated solution
we'd noticed that three common things
people needed to do with to store data
to share data in a controlled way and to
analyze data so we were going to try and
build a system which offered those three
in an integrated manner and then we had
this real no software deployment by the
user so we didn't want this problem of
trying to get end-users to deploy
clients on their desktop or their laptop
and this was made much easier as it
happens because html5 was coming along
so for the first time we had a ways
through you could build very rich
webbing verses that people so people
could just go onto a web browser and do
the science and third easy easy to use
so this is something we always put in
everything that we try to do and I'm not
sure it's always very difficult to build
easy-to-use systems and I'll show you a
video in a in a little while so you can
judge whether or not we we achieve that
but that was one of our aims behind the
design because we were aiming at people
in this long tail not an expert who
could spend a lot of time learning how
to use systems who understood what was
going on behind the scenes we wanted
try and get the ability to do science
through their analytics out to people
who have not been able to take advantage
of advances in the field before and so
we built a system which is goalie
science central which we still is still
the basis for almost all of our project
and it looked something like this if you
if you take the lid off it so at the
bottom it you can deploy on cloud so you
can deploy it on your own resources I'll
talk a little bit later about why we
think it's important to have a common
platform you can deploy internally on
your own IT or externally on a on a
cloud and then you can upload data into
the system you can attach metadata to it
so that you can search for data with
particular characteristics or so when
you find it you can interpret it on the
left-hand side we have a workflow
enactment engine we have ways so you can
package up your services and load them
into the cloud for for execution and in
the middle we have ways in which you can
describe connections between people and
projects and that's the basis for the
security system so every time you put
anything into the system via a workflow
of service or data then it's private to
you but you can choose to share it with
individuals or with projects if you
choose to do so and the top is you can
see on the top right these are the
scientists who are using a web browser
to do everything that they might want to
do with this system but also we provide
an API so that means that you can write
a program which actually does everything
that you could do is a human by clicking
and i'll come back to why we've
discovered that important later so
here's a little video of it in action
this is my colleague Simon driving this
so what it's going to do is he's going
to upload data into the lease sign
central so you can see at the back this
is the debt of pain so these are all of
the files that he's either uploaded or
that other people have shared with them
so he's uploaded a new file which is
this one on the bottom left and what
it's going to do now is to have a look
at it so we version everything so you
can see there there's four versions of
these files you can always go back to a
previous version of workflows services
and data scientists think that's
that's absolutely vital for consistency
when they're doing long running
experiments there is viewing the the
data this is a csv file as far as we can
tell most of science goes on with csv
files and then here's the security so
he's clicking on this tab it's it's just
accessible to him at the moment Britain
and choose to make it available to a
colleague in this case his colleague
Google or to a project so he's uploaded
some data is made available to a
colleague and now what you can do is to
analyze it so on the left you can
hopefully see there's a set of workflows
and you can choose an existing workflow
and click to run it but what Simon's
going to do here is it's going to create
a new workflow so this this is where we
mainly this is where we were able to
take advantage of html5 so what happens
is on the left you can see there's a set
of categories for services and in each
category you can drag a service out
honor this palette and your web browser
if it's a service that has properties
you can set you can you can do it in
this we're here so what Simon's doing
now is to select that file you saw being
uploaded because this is a CSV load
service then he's going to do some
numeric analysis on it so he drags
another block any connected together so
and this way using a web browser you can
create a workflow and you can then once
you've created it you can serve it and
executed that will sit we'll see that in
a second we provide a basic set of
blocks but every scientific domain has
their own sorts of analysis services
that are important and people are always
coming up with new ones so we provide a
way for people to take code that they
have for a service and the package it
and load it up into the system so it
appears as one of those services in a
category on the left-hand side and you
can do that in Java or.net languages or
our is becoming very popular
particularly statisticians who we have
now using the system octave and a few
other languages as well so here's simon
i think is almost finished creating this
workflow this is building a model of
some data and is using a
neural network to try and model it and
so he's finished so he clicks to save
the workflow and now we can click to
execute and so this is executed in the
cloud automatically system grabs the
resources to do that and it's creates at
the bottom some new data which some can
click on and then you can view it so and
there it is so in the red if you can see
the red that's the model that's been
produced in the green is the raw dare
okay so so that's the basic features of
of eastern central so let's have a look
at using it for this for the the wrist
sensor application that we saw earlier
trying to understand people's activity
so this is the this is what's running in
the hospital at the moment so this is a
simple just a Windows application in
fact and so when the patient arrives
back in the in the hospital with a watch
the nurse takes it off them plugs it
into the computer types in a patient ID
presses a button and the dirt sort of
might will be uploaded into e-sign
central where it's analyzed and then
report is produced for the medic and so
here's the is one of the workflows which
we used to do that analysis and this is
two pieces of the analysis that we we
produce so we're still experimenting
with the medic sister what's the best
format to present them with its
information but quite like the one at
the bottom which is just a simple pie
charts or for the waking hours of the of
the patient this is what they they were
doing so split into different levels of
activity running at the top most of the
patients we we're doing trials on at the
moment are over 80 years of age so we
never see much in the in the running
category but you see quite a lot of
light activity walking as well and at
the top there's a more detailed sort of
analysis for the medic so we can get a
view for each day of the patient's
activity so although medics are
interested in overall information about
a patient's week like the pie chart they
also also have things they want to check
so one thing I've discovered is that
medics don't like it if people
lunch and then go to sleep apparently
that's very bad from the point of view
of people accepting from obesity and
diabetes so on this on this graph that
you can see at the top there's a time
line at the bottom this is from midnight
and midnight in in one day Wednesday
there is a couple of years ago this this
particular timeline and you can see you
might be able to see a red marks at the
top and that's when the patient was
sleeping and you can see one which is
just after lunch so I imagine that the
medic talked with a patient about and
try to encourage it we go for a walk
after lunch rather than fall asleep one
of the things that we discovered was
that provenance is really important if
you were to encourage people to share
their data so it's all very well it's a
big push in science for data sharing but
in general I think was Carol gobelin
earlier Jim Gray Adwan who said that in
her experience is like trying to get
people to share toothbrushes trying to
get them to share to share data and what
we found is that people would much
rather do everything from there from the
beginning again in their in their own
lab are using their own tools rather
than trust somebody else's data but one
way to get around that is to keep a
provenance trail so when you give people
data they can click on it and see
exactly how it was produced and who did
the analysis and that seems to be aware
to dramatically increase the amount of
sharing of scientific dead so this is a
for any date in the system you can get a
provenance report this is just a part of
it and it shows you this is for the pie
chart shows you all the services that
were involved in the generation of the
pie chart you can see which versions of
those services there were and if you're
really interested in more detail then we
store the debt or in a graph database
and we can generate these reports which
are these visualizations which are in a
standard or PM which show you exactly
how something was produced so you've got
a pie chart at the at the top that's the
yellowy orange Sun like circle at the
top of that graph and then the green
other services which were used to
generate that but
okay I'll give you one last example
because I think this is the way in which
we've seen that cloud computing is not
just changing the way in which we can do
analysis in science but it's also
changing the way in which we can deliver
services healthcare services so
healthcare services are under pressure
in most countries after the banking
crash is a shortage of money in the
economy and people are trying to make
efficiencies and one of the problems
that we find in lots of different areas
is that the limitation is in the medics
or the therapists who are trying to
treat people so stroke is a real problem
worldwide is about as it says 16 million
people a year suffer from a stroke about
three-quarters of those people live and
they need rehabilitation if they're to
recover as best as is possible and the
way this is done is the in the UK and I
think this is quite typical is that
after you've had a stroke you'll be seen
by a therapist it will go to your home
and have a look at you and work out what
the problem is the extent of the damage
and will then take you through a set of
exercises so they will do the exercises
with you and then they will give you a
piece of paper with a list of exercise
zone and ask you to do that until they
come back and see you again which might
be in a in a week's time and so the
problem is that well two problems really
one is that for anybody who's ever had
something like a bad back and gone to
the going to a physiotherapist or a
doctor and they've done some exercise
with you and they've given you a piece
of paper to take home with all these
exercises you have to do 10 of these 20
of these every day and so on then if you
like me then after a couple of days you
might do it diligently the first day by
the second day you start and lose your
interest but it's very boring and by the
third day you're only making a token
effort a lot of these exercises and
people who are supposed to do these
exercise when they've had a stroke
suffer from this same problem of the
boredom of doing these exercises and
then the second problem is that the
therapists have no idea about whether or
not these the patient is doing these
exercises and if they're doing the
exercise are they doing them
right and if they're if they're doing
them at all is it really causing an
improvement in them and so they have to
schedule their visits without any
knowledge of how the patient's doing if
they had that knowledge then they could
plan their time better they could go to
see those patients more regularly who
needed help and those patients who were
doing fine they could see them less
often and so that could be much more
effective in delivering their service
but they they have nowhere to know how
to do that so a group of colleagues
including neuro scientists and people
working on games as well as our cloud
researchers came up with a way to do
this a different way to do this so this
is a try you can see the video here with
a trial with a patient and what they're
going to do is to play a game so this is
a circus game so this is a patient who's
had a stroke and what he does is he
follows the movements which you can see
from a little avatar in a circle on the
right-hand side of the screen there you
can just see it now and if it follows
these movements there any causes the
larger cutter on the left to do things
like to juggle or two to do loop the
loops and he wins points and points are
incredible motivation for people to to
do things it's astonishing what people
do for points and it's much less boring
doing this there is just following a set
of exercises from a from a sheet and so
the patient's like this the therapist
like this because they find that the
patient's spend more time doing the
doing the exercises and you can use
techniques of games so if they're doing
well then you can move them up to the
next level where you might get them to
do things that are a little bit harder
and and so on so there is a doing some
juggling and so that's that's great to
the parrot so we could give this to
people they could take it to their own
homes and when the patient saw them
hopefully then these these patients
would have been much more likely to have
carried out the exercises but we wanted
to try and go a bit further so what we
do is even when they're in the when
patients are in the homes all the data
from the game is sent over a home
internet connection back to e-sign
central where we run various analytics
over it so we've worked with
mathematicians to try and design
in analytics which match the assessment
that the therapists do so what happens
is the therapist will get the patient do
a set of exercises and they will give
them a score so one score is called the
car high score which is a measure of
upper limb movement and we've devised
ways through workflows using analytics
so this is one of the one of the
workflows which can generate an accurate
car high score based on how the patients
are playing again and what this means is
that now wherever the patient wherever
the therapist are from a phone or a
tablet or a laptop they can go online
and they can get a dashboard which gives
them for all of their patients
information and they can click on a
patient so the dashboards that they're
at the top and this therapist is clicked
on one of those patients and at the
bottom what they seem there is it's a
graph showing that the change in the car
high score day on day for this
particular patient so they can see for
example that the high score increase
from day 1 to day 2 and then the patient
didn't play the game at all for three
days and then played it again with a
marginal increase didn't play it for a
day and so on so this allows the
therapist to decide how best to spend
their time so they probably be worried
by the the gap of three days when the
patient wasn't playing the game and they
might decide to go out and talk to them
about it which might result in them
switching the game are setting the game
to her to an easier level and so on and
they might start to get worried if even
if the patient was playing the game
their progress is measured by the car
high score it wasn't what should be
expected so this means the therapists
have much more flexibility in how they
spend their time and they can put their
efforts where they think it's going to
to matter the most the other thing that
we can do is as we build ups or in
trials with this at the moment as we
build it more and more information you
can do cross population analysis so this
is a graph about one particular game and
this tells you something about the
popularity of this game across the whole
cohort of patients so this says that on
day on week 1 then patients in general
played this game somewhere between three
and four times during that week and then
it tails off a bit and then it
dives down in and week fall so this
might tell you you might compare this
with another game that might show
actually this game is pretty good at
getting people interested or it's not as
good as another game but you can also do
things like judgment here would might be
that after day three people get bored
with this game so after week three they
get bored with it and so it's probably
then worth going and switching to it to
a different game for them to keep up
their their interest so we collect all
of this information there's all sorts of
different things you can do to measure
the effectiveness of the different games
to see how a patient who comes in with a
particular starting point a particular
level of problems from the stroke to see
which is the best particular game to
give them and to know what the
expectation is in how they should
improve all the time one thing that's
happening in the health service in the
UK which may be driven by things like
this is the payment by results is
becoming important so you get
third-party providers who can provide
treatments for particular conditions
like to say stroke and you don't pay
them upfront you pay them by results and
so one of the things that people are
thinking about is if you've got this
sort of information you could actually
measure the level of the patient when
they arrived measure how they were
improving over time and when they got to
a certain level that was governed by the
contract with a third party provider
then that could automatically trigger a
payment so that's that would be a real
change in the way that services
healthcare services were delivered in
there in the UK but it could be enabled
by this ability to capture detail
detailed information not just about
individual patients but across the whole
population of patient
let's imagine that the trial to go well
they seem to be going to at the moment
and we want to scale this out to perhaps
hundreds of thousands or even millions
of stroke patients in the UK around the
world then would we be able to scale so
we put a lot of effort into scalability
I said earlier this is one of the
problems of cloud cloud to give you lots
of nodes but it's up to you to spread
out your work across them and scale it
up as you get more debt or as you need
to do more analysis so this is this is
how we do it I won't go into detail but
what happens is that every time there's
a workflow needs to be executed goes in
a queue and you've got a set workflow
engines which you can run on a set of
cloud notes so you can have as many of
those as you as you need and what
happens is when they have when they're
running out of work to do they can go
and grab some more work from a queue and
do it and return the results back into
the into the system and so that's that
seems like quite a simple architecture
and it looks on paper like a lot of
architecture diagrams as if a work well
and be very scalable in fact we found it
took us about two years to tune the
system so we get these sorts of results
with it so now so this is typical to
this is a as a particular experiment
that we've done running on 200 nodes as
it happens in the Microsoft cloud and
we're getting about ninety percent
efficiency for scalability it took us
about two years to get up to ninety
percent efficiency working closely with
with people in the IG ot me in fact
they're Microsoft but the attraction of
having this cloud platform is that now
all of the projects that we run across
our platform benefit from the work that
we did we removed a lot of bottlenecks
in order to make this scalable and so
all of those projects now benefit from
that whereas if you're back to the
building individual applications for
individual projects and the danger is
that any optimizations that you do will
only benefit that one particular project
so I think this is another reason for
why it's a good idea to actually have a
platform which means that all the effort
that you put in through your projects to
add new features to improve its
capabilities every project benefits from
not just the one you're currently
working on so um so hope I've shown you
some things about some of the work that
we've been doing on on clouds I've come
back to in a minute about some
conclusions that we've drawn from from
doing this I wanted to just segue into
thinking about why I do this and why I
think it's good for computer scientists
like myself to to work on e science
applications and for this I need to go
back in time so Harold mentioned some
work I've done in in industry so this is
going back 20 years this is a machine
called the ICL gold rush mega server
which sounds like a 1990s name for a
computer i guess marketing people would
think about ones now and this was a
project that I worked on I started
working on this when I was doing my PhD
at Manchester University at a
collaboration with a local company ICL
and we started with as you do with some
at that time some sketches on
blackboards and eventually we built a
little prototype and it got to the point
where I CL decided that they wanted to
turn this into a product and so they
asked me would I move across and so
after five years into this collaboration
i moved to work for ICL witcher but I
work for for about another five years
and after about ten years of this
endeavor we produced a product which we
then released and sold around the world
and it was a parallel database machine
so at the time I sale was made for him
company and their men friends it was
quite expensive doing high-throughput
transactions on a mainframe and so they
wanted something faster and so we built
this parallel debt-based machine which
had all sorts of exciting features at
the time of it so it had 64 nodes in
those kamna c2 which run to spark chips
and i can remember that the excitement
that we had when we got our first
delivery of quarter of a gigabyte disks
that we could put into this into this
machine to really mean that we could
store enormous
vs. and process them and so I worked on
this for many years was the first one
that we sold was to Johannesburg City
Council and the part I had just fallen
and they had a they had a small computer
that they had their electoral register
on and suddenly because of the fall
apart I they had many millions more
people on their electoral register and
so they needed a bigger machine and so
the first one that we built went to
Johannesburg and after about five years
I CL i still had invested about 50
million u.s. dollars in this all the
time we had about 150 people working on
it and I was ready to move on and design
the next generation of it because I
don't know what anybody's work in
industry but often what happens you
release a product and then you're
excited because customers use it and
then customers go and ruin everything by
finding books and wanting support and
wanting extra features and so that's
where your attention then then turns and
I found that that was a much less
interesting part of working in the
computer industry then actually
designing new systems so I CL weren't
interested in immediately plowing on and
putting a lot of money into in the next
generation so I went back to academia
and I moved to New Castle and I can
remember my first day Newcastle I was
led into a small office and so I'd left
this company with his 50 million dollar
project hundred and fifty people work
with me I was on my own in this office
out and remember thinking what on earth
do I do now that was my very first
thought and then my second thought was
worth did I do this and for a while
things were quiet it was quite difficult
because you realized that you don't have
the resources in order to do the sort of
thing that you could do it scale
industry but I started so we try to do
some bits of work here and then we built
some models and we got a little bit of
money to build prototypes but really I
realized that the two things that I was
I was lacking weren't particularly
resources in general it was two things
so one was a source of problem so it it
i sell-- there's always people banging
on your door coming in from
banks and building societies and local
councils saying why can't your machine
do this more quickly or can't why can't
it do that or the other and it in
university then I'd lost those those
stream of people coming to me with with
these problems which I realized were
motivating the research that I was doing
and the other thing I'd lost was access
to data so in ICL we had all of these
banks and councils who would give us
examples of the data that they wanted to
process through our machines and we
could use those for valuation we could
see how fast we could run all of these
different sorts of debt based workloads
through the system and it was very hard
I couldn't get Darren and I can remember
end up writing program to generate dummy
dead it to process and it was all a bit
demoralizing actually and after five
years things were going weren't going so
good in it I went and giver I remember I
gave a talk at Essex University and
there was a professor there who are knew
very well and I give my talk and we went
at the pub afterwards and I was
explaining these problems to him and
what he said was that well you know you
need to get used to what you have to do
in in academia which is to work on and
he said ty solutions to type problems
and I can always I can always remember
that and so his idea was I needed to set
my sights lower start working with
smaller amount of data and working on
problems that i generated myself rather
than the real problems from the outside
outside world and I was rescued from
this by something that happened in 2001
and that was the UK science program so
in 2001 the UK government decided to
invest a lot of money any science and it
was conceived and direct LED and
directed by this fresh-faced young man
here who was then professor at Hampton
University who moved over to take the
lead in the project Tony here and I'm
delighted Tony's here today and me in
the audience and Tony's vision was that
it will be possible to transform science
by the use of advanced digital
technologies and the way in which he
achieved this was through social
engineering so trying to get computer
like me to work with scientists across a
wide range of different subjects and for
the first time in five years suddenly I
had lots of really interesting problems
and the problems were much more
interesting even than the ones i had
working in industry because it was more
exciting to work with say neuroscientist
trying to understand how the brain work
than it was to work with her with a bank
trying to run a set of customer
transactions through slightly more
quickly through there through their
machines and we were drowning in in debt
so every area of science suddenly was
generating data so biology was probably
one of the first ones was also astronomy
also had huge amounts of data which were
available you could grab it and you
could work with scientists to extract
lots of interesting things from it and
hopefully drive on the science as well
as providing interesting challenges to a
computer scientist like me so the this
program which was which atony conceived
was copied widely across many other many
other countries and then Tony continued
it when he moved to tomorrow soft and so
I think for me this was the real real
saving of my my career this this opening
up with this new world there was
possible to work with sign it's about
interesting things to do and lots of
data that we could work with them on and
them it was through Tony that I made Jim
Gray actually so so Harold mentioned gym
and I've known Jim because by reputation
because working on parallel dariush
machines Jim was considered to be the
main person in the database field at the
time and that was because very unusually
what he did was he matched the practical
so he really designed and built through
his career database systems which were
faster and better than previous ones
that had been been available but he also
could do the theory side as well so he
had an equal respect which is very
unusual for academics but also from
people in industry and got to know Jim
through Tony son when we went to his
house once for for breakfast and we were
sitting in his houses in San Francisco
looked out
the harbor and I can remember that he
didn't know how to make tea so Tony had
warned him that look Tony drunk coffee
that I drank tea on a morning and so Jim
had found his wife up she was out of
town with her friends and family and I
can remember that he was standing with
her with a kettle with with boiling
water in a cup and he had a tea bag and
a set of detailed instructions in the
stopwatch which he venues like Cohen out
a chemistry experiment to make me a cup
of tea for for breakfast and after that
he would do things that really surprised
me so he would send me long email to
give me advice and guidance on what he
thought I should be doing like giving
feedback and some of the papers I've
written and I can remember once for
example letter I was given a talk at
keynote at a conference and it was a
some work that I put more effort into
that I'd really put my heart into it
than anything else over the previous
year and so I wrote the paper spent a
long time writing it and then I gave the
keynote while i was giving the keynote
you could see it just went down like a
lead balloon you could see the
tumbleweed rolling along in front of the
stage as i talked about these these new
ideas and I was quite demoralized by
this and then about a month later I got
an email from jimson Oh he'd read the
pit we really liked it and he was
sending around to his friends and so
that was a huge boost and the mazing
thing about Jim I feel is that he was a
loser top man in his field very busy
person had lots of demands on his time
he was always people always wanting him
to go and get keynotes for example but
then he would spend time sending long
emails people like me who are nowhere
near in the same league as jim was to
give us advice and guidance in the field
and I think they're very few people who
at the top of their game in any
discipline or actually put that effort
into bringing on the next generation as
well as keeping their own career going
so that's one of the reasons why it's a
really a great honor to get this award
in Jim's name
so just to end with a few conclusions so
and I'm always reluctant and make very
sweeping statements but I do think this
one is true I do think that cloud
computing is going to revolutionize
e-science and I think that hopefully
through some of the examples that I've
shown you can see the benefits of this
the agility the ability to just go and
grab resources as in when you need them
not have to put in big grant
applications that take you to two months
to write in order to try and get the
compute resources that you need to try
something if I can just go and grab the
resources to do it immediately the
democratization of it the fact that you
don't need to be in a center with lots
of resources in order to be able to try
out your your ideas at scale they built
it to share so clouds don't they're not
the only way to share data and they
don't guarantee you'll share it's much
more likely i think that people are
going to build systems which allow data
sharing and sharing of computations if
they're in a cloud where naturally you
can share them among other people rather
than say if they're on your laptop other
the local server in your university and
i think we're starting to to see that
the guys who we work with in the medical
school at newcastle Mike Janelle
Martinez group who do the the movement
analysis work their aim is that this
should be a common repository which they
can make available to anybody can go and
get the data add their own services into
the system and and try the mountain ate
them available to others and the crowd
naturally lends itself to that sort of
that sort of approach and finally impact
so talking about the second example with
the strawberry ability shin the fact
that if suddenly we do decide we get the
opportunity to roll this out hundreds of
thousands of people we don't have that
thing that used to happen in
universities where you had a little
prototype that was running on a on a
small server in your lab and you it was
good enough to get a couple of papers
out of it but then if you wanted to do
something at scale then you need to
start again and build a scale was this
one entirely different sort of an
entirely different sort of architecture
I think what clouds do is going to make
much more likely that people are able to
take something which they've done
through their research and then to scale
it out to get it out there to benefit
society and a wider group of scientists
but I mean said that then there's all of
these barriers so I've mentioned quite a
few of them how to build scalable
systems the issue of dealing with
security problems of building systems
that don't fall over inconvenient times
and also auditability building system so
you capture what's being done so you can
get reproducibility something else
that's important in science and so our
think the thing that we've learned is
the cloud platforms we do think either
way forwards so rather than solve each
problem individually and build out a
siloed application stack for that i
would encourage people to think about
whether you can generalize what you're
doing and built and common services
which you can then apply across a wide
set of wide set a project and i've
talked about one of those e-sign central
which is which is ours and it's open
source you can download it from
sourceforge if you want to try or you
can go if you go on the web there's also
a version that's up there way you can
just register through the web and log in
and try and try it out but it's not I'm
not claiming it to the be-all and
end-all and what I'd like to encourage
other people do is to think about other
sorts of platforms ones that might in
the end be better than the ones that
we've we've produced but I do think that
this approach of platforms really is the
best way board and finally to come back
to what I said earlier that i really
think that II science is really
important for computer science that
generates all of these exciting
interesting challenges that computer
science can use to come up with new
research ideas and to test what they
what they've done my hope for the the
coming years is really that this vision
of e science which Jim Gray and Tony
have been propagating over the last
a couple of decades now that are really
continuing that new young computer
science will take up the challenge and
realize how it's a really good word to
find interesting problems to work on
which can have real benefits to advanced
knowledge and also help decide thank
okay i'm going to share myself to answer
any questions so any questions anybody
else Darren
yes
yes
yeah so we're at the stage where we can
do trials but we're not able to do it at
scale at the month across a wide set
patients so the way it works in the UK
so you can apply for ethical permission
to do trials on a small group of people
but it would be a set of Hoops you would
have to go through if you were to really
roll it out as a production service in
there in the health service and it's
forced us even in these trials to
address some issues but there are others
that we we leave for the future so one
of the issues that's really important of
course the security and I didn't have a
chance to really to talk about this in
detail but one of the reasons we found
an advantage you have a nice i internal
is something that's portable is that
what we can do is we can run it on a
public cloud like as you or Amazon or
open shift for those sorts of analyses
where there's no issues to do with
privacy or security for cases where
there are then we can run it on our own
internal IT now the trick and this is
where the our current researchers is for
it not to be an either-or decision so
it's it's quite a you know one of the
barriers to clouds is you get people who
say well there's some sensitive date an
application therefore we can't take it
out with public cloud will have to run
it internally and what we've been doing
is we've worked on tools so you can
describe your application the security
levels that you require within that
application for sensitive debt like
patient there and you can press a button
and what it does is generate a workflow
that spans both the private cloud and
the public cloud where we run besides
don't belong both and so then that means
that the secure data remains on the
private cloud and the day that's
anonymize but the debt way you really
need the scalability if the public cloud
can go out to there so that that's been
our main drive the bet we haven't done
yet is if you wanted to say go through
all the regulations that you would be
required to get it out there so in the
US then really that is a whole new set
of regulations that we'd have to go
through and that would require actual
resources to do that so we're just at
the stage where we've got an idea of
what would be required if we got to that
stage but we haven't done that yet
yeah so that's a good that's a good
question so the idea is are we going for
I guess are we going for throughput are
we trying to reduce the response time
for big workflow which might have
internal parallelism well we can do we
can do both so in the man what we find
is that although MapReduce is a big
hammer that people are hitting every
nail with almost everything we do is map
so there's very little reduce ever
involved so you get lots of debt into
sensor data from lots of patience and
you need to analyze it in they're all
they're all independent so we do lots of
maps and that makes it easy to spray it
out across a set of nodes in fact the
graph that I has enough time to go into
it but the graph that I gave you and
that nine percent efficiency that was
for one workflow and that was a workflow
that we use to do some some chemical
informatics with some cancer research
scientist to try and predict the
behavior of molecules and it starts off
and then there's points where you can
paralyze it because you get individual
data sets that you can explore in
parallel you bring it all back together
at the end so although it I think it
said it had half a million workflows
they were actually sub workflows that
was spawned off from the the top level
workflow so the way in which it works is
that you start your top level workflow
that can then create other work for us
which then go into the queue and then
those are those are scheduled and then
eventually you can combine them all back
together again so we've got a bit of
experience of of both and it in in that
case we've got good efficiency for that
the case of reducing response time but
as I say in the main what we find is
most people want inside what we do map
yeah okay
so
yeah sure so so the idea was that the
project I've talked about as a focus on
social exclusion and I talked about some
of the perhaps the more medical end of
the of the of the the projects that we
do within that but what about other
things in the two examples that the
gentleman gave so two one was about
traffic congestion and pollution and the
other one was about violence if we take
the two separately all right they can
both combined into one at times I guess
but if we take the traffic one then when
we've done quite a lot of work on
analyzing traffic flows in pollution in
in Newcastle so using very small cheap
sensors which are spread around the city
and combining that with the information
that you can collect from traffic
sensors which are usually loops wires
that are under the under the road and as
a result of that then you can start to
build models and you can start to see
what causes particular problems in the
hope you can head it off the the the one
thing that's interesting is that there's
always two ways in which you can use
their analytics so the first thing is to
understand which is what we can do in
that case but the second thing is then
to try what can you do about that and
that's why in our projects we have
experts in particular domains like
sociologists traffic experts and so on
because what you need to understand is
how policymakers could be influenced in
order to change what they're what they
do so when you get politicians to change
the way in which the road systems work
perhaps in five years time to have a
much more dynamic system of routing
people around said he's perhaps using
incentives so we have a congestion
charging in in some cities in the UK
could you have differential congestion
charging if people would be prepared to
go slightly longer route
which would reduce the levels of
pollution and so and I wouldn't
underestimate the the difficulties of
doing that trying to understand the
levers by which policy changes in trying
to trying to cause those to have an
effect and they seem to be different in
different domains as well so that's why
we try to hook up with experts in those
areas we do quite a lot of public
engagement so we get people in from
those from from the government and the
councils and so on who could potentially
have a have an impact on that and and
make them aware of what we're doing in
the hope that they will then take it
into account in the other area of
violence then that's a more difficult
area we've done aside from from this in
the same project we've done some work on
social media analysis which the police
are very interested in so trying to
understand before things happen what
what information you might get which
might lead you to predict that
something's going to happen in a
particular place but also afterwards
then analyzing what happened in order
that then you could in the future look
out for those particular patterns that
you've detected that's at a very early
stage and it's very difficult I mean one
of the problems you've got is a real
lack of information because there's only
a certain proportion of the population
provide information on social media and
it's not always accurate and one of the
things that used to happen was that
which was very helpful for that was that
almost all crimes for social media
automatically geotagged all of the
information and we had some riot in the
UK things two years ago three years ago
and they were the first riots that we'd
had in the era of Twitter and some
people were tweeting things like I'm
going now to set a light to this
building and in the in the tweet it
would be geo-targeting so the police
could just drive along and pick them up
when that when that happened but now
people seem to have become more savvy to
that and also a lot of
at the people provide clients for social
media the default setting is now off
rather than on for dear time you
mentioned that use when your team spend
about four years achievement if the
ninety percent cap scalability of your
system yes special years worth of effort
with some two years two years yes with
some organ experts in yoga we start off
at about forty percent gone up to ninety
percent could you mention one or two of
the problem that you face during that
time of how you fix it yeah okay so so
one of the so there's really two things
that we needed to do so one was the
scheduler so the scheduler wasn't
feeding the workflow engines at the
right red so I mean I've worked on power
machines route 25 years and it's always
the same problem so you know the problem
that if you have a central queue with
all your work in then when a work for
engine runs out of work do it takes a
while before it can get the next piece
of work from the queue and set it up to
run so then what you do is you go and
try to preload your work for engine with
with tasks to do so that they're sitting
ready and waiting but then if you do
that too much then what happens if the
cube root of central q runs out and
there's no work on some other work for
notes so getting there the balance right
there was one of the things that we we
played with the other thing was it's
quite complex to run a workflow in a
scalable web because what you have to do
is it's no good just so we can run a
workflow engine on each node but wait
and you can send the description of the
of the workflow which says run this
service and then send the debt this
service and run it but for scalability
what you have to do is to dynamically
deploy those services on that node with
the world for engine it's no good having
a service deployed in one place and
sending everything to it so what you
have to do is to have efficient ways to
package up services and efficient ways
to then dynamically deploy them as and
when when required and that's an area
that we're still
working on we think that developments
like docker for example are going to be
a real help in in that area but that was
the the second the second point in our
system that was a ball neck another one
ok
so the question is about data movement
so I'm at the moment we take a
relatively straightforward view so what
happens is that when we run a workflow
it's all run on the same node so we
fetch the input files and then any
intermediate files remain on that on
that note as they move from one service
to another and we have optimizations
where we can stream them through the
nodes in order to for example to try and
make best use of main memory where we
started to do some work is on where I
was saying earlier Darren's question
where we've got multiple e-sign Centaurs
running on different clouds and what as
I were the way I described it we
describe the application and we have a
tool which if you describe the security
requirements it works out how to divide
it up across these nodes now what
invariably happens is you get multiple
options in for a very simple workflow
and the question is which one do you
choose so then we have a system where
you can use cost models and you can do
it on pricing which actually takes into
account data movement as well indirectly
because of cause it costs you to move
data into a cloud but you can also in
your cost model you can have a policy to
reduce the amount of data that was
transferred between between nodes even
though it might cost you a little bit
more in price or whatever so you're
trading off well we don't we never trade
off security but you can trade off price
against performance if you want to do
that
yeah yeah so so at the moment almost
myself I'm sorry optimizations have been
on any sense out running in a single
cloud and so we haven't worried about
that but we've started in these cost
models to try and use estimates of how
long it takes to move there and one of
the one of the motivations for that is
that a large described patients going
into a clinic with a watch and then
there's a small number of gigabytes a
day it gets uploaded occasionally we
involved in remote studies and then
through the post will come ten terabytes
of data on a big disk which we then have
to process and that really is good
motivation for minimizing the amount of
data movement so it's in those cases
that we've started to do it but we have
a vehicle for doing it which is this
cost model mechanism in our in our tool
but i wouldn't say we've done a lot of
work in that area would be keen on
anybody who wanted to work with us on
ok so I'm sorry you were talking about
scalability and I was just wondering
about domain expert scalability so he
start up working with a group of experts
from other domains and all of a sudden
you realize that you've missed a facet
of that problem and in order to finish
your work you have to bring in other
people and then it may scale up to you
know whatever so is there any kind of
advice or expertise that we need to
handle this expert scalability yeah so
so that's a that's a really good zone
that's a really good question that what
we find is that if you look at what we
are we embed in the cloud 3 signed
central then we end up with these
workflows that when day arrives you use
the workflow reduce them to my analytics
and what we find is that most of the
people we work with a perfectly capable
of of loading some date when clicking on
a workflow to to analyze that data and
produce a result in a form that they
they understand and that's why we put
effort into allowing sharing of
workflows by individuals and and within
projects just what you find is that then
there's a next level there's somebody
who can actually create a workflow there
there are level of expertise above that
and they do that and they've got the
authority because of the project leader
they're running the lab to say to
everybody look whenever we get this sort
of data use this workflow because I've
blessed it the next level of expertise
is people who can actually write the
services that part of the workflow and
what we find is that so this is probably
this is probably going up by factors of
10 you know so for every for every
for every person who can write a service
there's 10 people who can combine
service into workflows and then there's
10 people who can use these workflows
knowledge to do their their analytics oh
so the thing that we found is to is to
not treat all scientists all users the
same and to try to work out where they
fit in this pyramid and to make best use
of their talents in that so for example
by if something the right a service
helping them to package it up and
putting it into the system so that
others others can use it people can
write workflows then making sure that
other people in the project aware of
this work for which they can use rather
than have them try to build a workflow
from the ground up themselves like
that's that's the approach that we we
found is most effective
so thank you Paul for an illuminating
talk and for setting the tone for the
rest of the things we'll see over the
week it was very well done thank you
very much now we're going to have a
coffee break in the next room and I
don't know if the next room means that
way or this way she's pointing over
there and remember when you come to the
next session if you're coming to the one
related to our workshop it's upstairs
and we'll see you there and see you at
coffee to thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>