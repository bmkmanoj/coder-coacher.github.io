<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Non-Convex Robust PCA - Part 2 | Coder Coacher - Coaching Coders</title><meta content="Non-Convex Robust PCA - Part 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Non-Convex Robust PCA - Part 2</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SAVYq4eNmnA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so in the last talk I gave a brief
interest to PC in this talk I will be
talking about a recent work on
non-convex robust pc i will explain all
these terms for the slide and this is
joint work with new engine sujay anima
and pratik all of them are different
places so towards the end of lost our
last talk we saw the problem where we
observe a matrix m where which is a
corrupted version of a low rank matrix
which is l star and then there is a
perturbation or annoys matrix between on
it with s star so the example the case
that we considered last time was that we
don't know anything about the noise
matrix there was no structure in the
noise matrix and somehow the best we
could do in terms of flow rank
projection was to get to recover the low
rank matrix up to some error which
depends on the norm of the noise in this
setting we as we consider a case where
the noise is much more structured in
particular the structure we assume about
the noise is that it is very sparse what
I mean is that each row or each column
only has a very small number of nonzero
entries and given this summation so the
part of the noisy version of the low
rank matrix L star we want to recover
the low rank matrix exactly so this is a
problem and robust PCA because if you
just do PCA will incur some error and
you want to do it robustly even in the
presence of some structure noise to
recover the low rank matrix L star
exactly without any noise whatsoever
it's a problem clear ok so let me give
you an application where this comes up
the application is foreground background
separation in the video so a video is
composed of frames just vectorize each
of the frames so make each frame as a
vector and then stack the vectors one
after another
we'll get a matrix where each column is
a frame and each frame has the
background component and the foreground
component the background component does
not change much over frames so this is a
low rank part of the video and the
foreground part is people moving across
and people moving across or objects
moving across or whatever they may be
they are usually sparse because you
don't have huge objects that occupy the
entire space so the perturbations in
this case are sparse and if you can do
the low rank sparse decomposition
successfully then given a video we can
do foreground background separation
which is helpful say for instance in
surveillance professors or something
like that okay so the natural
formulation of this problem is to write
the matrix M as l + s where we want to
minimize some combination of the rank of
L and the sparsity of s sparsity of s so
the 0 norm of s corresponds to the
number of nonzero elements in s we want
to write ms l + s where we want to
minimize the rank of l and the number of
nonzero sinus this is a non convex
problem because both the rank function
and the sparsity function are both non
convex functions and we do not know how
to optimize this efficiently so there
were these two papers simultaneous
papers by gender sacred natal and
candice at all which considered the
convex relaxation of this problem where
the rank constraint was relaxed to the
trace num constraint the trace norm
constraint or the nuclear norm is the
sum of all the singular values so if you
recall from the previous talk it is also
called the shat on one arm and it is
just the sum of all singular values
whereas rank considers rank is equal to
the number of nonzero singular values so
this is a relaxation of the rank
function the trace norm of L and the
sparsity of s is relaxed to the l1 norm
of s this is just as in compressed
sensing where the l1 norm is a proxy for
the sparsity of the matrix and the
constraint is the same in M is equal to
l plus s this is now a convex problem
and we can solve this efficiently by
efficiently I mean in polynomial time I
what these papers managed to prove is
that under certain regularity
assumptions on the underlying matrices L
star and desktop that M comprises off we
can indeed this convex program indeed
recovers the true L star and star
exactly so there is no error what cyber
if you do PCA without using the
structure of s we will incur some error
whereas solving this problem gives you
these matrices exactly without any error
firstly I should mention that this
problem is the problem of decomposing
the matrix into this low rank plus pass
is not even well posed in general for
instance so we have this low rent plus
parts so if the low rank matrix itself
is passed then we could write it as a
zero low rank matrix plus a sparse
matrix or a low rank matrix plus zero
sparse matrix so the decomposition is
not even well-defined so we could have
two different decomposition which
satisfy both of its satisfy this floor
angles parts conditions and the problem
is so it's not will post so the
regularity assumptions that make this
convex program work or even in general
make the problem will post or that the
low rank matrix is not spot or another
way to put it robust Lee is that the
lower the entries of the low rank matrix
are all well spread across all the
elements and the mass is all not
concentrated in just one element put it
another way so if L star is equal to u
sigma v transpose is the single avalue
decomposition of L star then each row of
you does not have to a large norm so so
in general each row of you can have norm
equal to 1 so let me give an example
so if you is so you consists of
orthonormal columns so if you is say 100
this is not incoherent because the first
row yeah
if the U vector is 100 then the first
row has a large norm and it's so when we
take u u transpose the resultant matrix
will get will be just one in the top
element and zeros everywhere else and
it's hard to distinguish this from any
sparse perturbations so what we assume
about the low rank matrix is that it is
not like this it's more like 1 over
square root n + 1 over square root n or
something like this this will ensure
that the mass of the matrix is well
spread across all the elements and even
if you put up only small number of
elements the remaining ones will give
you enough information to be able to
require matrix so that is the
incoherence assumption on the low rank
matrix the MU here is the incoherence
parameter and sparse matrix is degree
bounded what I mean by this is that any
row or column of estar has at most D
nonzero elements and so we consider the
deterministic setting and this means
that these nonzero elements could be
anywhere in the matrix they are not
random or anything of that song under
this under these conditions sukara and
zhang prove that the convex program has
a unique minimum and that unique minimum
is indeed the true decomposition l star
under star as long as the number of
spots perturbations d is at most n over
mu square R so let us just try to parse
what this means say we are looking at
very small rank say are equal to a
constant or indeed one and if the matrix
e is optimally incoherent then mu is a
constant as well in this case it says
that you could perturb almost a linear
number of entries in each row and each
column and you are still able to recover
the entire matrix exactly without any
nice word cyber you're introducing huge
amounts of noise but structure noise and
we are we can still recover the entire
matrix exactly without any noise
whatsoever so
yeah yeah the to decomposition is the
unique optimum of the program and this
is tied up two constants in the sense
that we can construct so there is a
constant C such that if these larger
than that constant times n over B Square
are then there exist two different
decompositions which satisfy both these
properties but are not the same so you
have no way to distinguish which of
these is the correct so the problem is
not even well posed ok so we have seen
that the problem is yeah yeah so I mean
so the definition goes why is VD what is
the cushion I don't yeah yeah
that would correspond to the background
being reasonable and not just one person
here one person there or something like
that so the background is somehow
occupies the entire scene and not just a
person standing here and a person
standing there
once we are
no no so if the so there is it if the
matrix satisfies these conditions it is
not necessarily true for every matrix if
the matrix have fatty decomposition
which satisfies these two conditions
then you will be able to record the
decomposition if there is no such
decomposition you can run the algorithm
it will give you something and then you
will realize it it does not mean
anything yeah so for one correspond to
the spa correspond to this pass
perturbations which means that you do
not have too much traffic in the video
there is only a small amount of video
and there is a background a good enough
background that okay so here we said
that the convex relaxation algorithm
solves this problem up to constant
optimally so what is there to be done
here so the issue in the convex
relaxation is that its run time
complexity is cubic in cubic per
iteration and it does not scale to large
sizes moreover to get an accuracy of
epsilon the number of iterations we need
to do goes as one over epsilon and this
really does not scale well for very
large size problems when you want good
accuracy in your result so whatever work
does used to propose a new non convex
algorithm which does not go through the
convex relaxation route but rather tries
to solve the original rankin sparsity
constraint problem and this algorithm
has a complexity of r square MN per
iteration so r is the rank here and
eminent NR the ambient dimensions just
doing a pca will take order of RM n time
so this complexity is just a factor of
our away from the complexity of standard
pca and usually r is small so this
should be a close to the runtime of the
lpcm algorithm and we are algorithm also
has geometric convergence what I mean by
that is to obtain an accuracy of epsilon
you need to do log 1 over epsilon
iterations of the non convex algorithm
that I am going to describe and the
algorithm actually probably recovers the
same pairs of l star and star that
convex methods to so we get the same
optimal recovery results but propose it
much faster algorithm than the convex
relaxation technique yeah yeah so so we
do not look at it in terms of how the
function space looks like or anything of
that sort what we can show is that
whenever you have a true decomposition
of L star and star the algorithm indeed
converges to L star under star so it we
are not making any claims about whether
whether or not there exists other local
minima saddle points or anything of that
sort
I mean the function we are essentially
trying to minimize is this rank of L
plus this paucity of s subject to m
equal to l plus s so let me notify the
basic algorithm at each iteration we
maintain an estimate of the low rank
part l and the estimate of the sparse
part s and each hydration consists of
two steps the first step we look at M
minus s if we had s exactly then M minus
s should indeed be Lauren but we do not
know s exactly so I minus s will only be
approximately low rank and not really
truly roll low rank so we project our
project a minus X onto the set of low
rank matrices so that is PR the position
under the rank are matrices we make it
we update l to be that and now we take
this l and look at M minus L if Elvis
exact M minus L should be sparse but M
minus L is not sparse because L is not
exact so we threshold all the very all
the small elements to 0 and only retain
the largest element which will make
ehhhh sparse sorry
yeah so initially the start with say is
equal to 0 s is all identically equal to
0 so we have a fixed initialization s
equal to 0 under these conditions it
will be unique there will be unique
decomposition satisfying both these
conditions because there are all these
algorithms which will recover that
decomposition and if there were two they
should recover both of them which would
exactly be the same there is a fixed
initialization as equal to zero for this
and for even the convex relaxation there
is a unique optimum ID l star and estar
so there cannot be two different else
aren't ester
right so we keep repeating both this
procedure both these steps till a fixed
number of iterations till we feel we
have converged and the main idea in this
approach is that instead of cracking the
l2 norm the operator norm or the flu
biggest norm as we did in the previous
example in the last talk that we the
dimension will track the L infinity
norms of L minus L star and similarly s
minus s star and show that they decrease
in each iteration yeah yeah r is the
rank of the underlying matrix so in
practice you could try keep trying
various ranks and see which you will do
a cross validation so you set a
particular are and then see how that
works if that does not work well then
you go to the next star and that is only
dependent because the number of
iterations you okay so this is a very
basic version there are further versions
which which depend on r square in a
particular way
the l infinity norm is because so when
you when we begin we know that the low
rank matrix has elements spread all
across the matrix but when we take the
PCA it is not if we just get l2 norms on
the error we do not know if the
recovered matrix the errors are
concentrated on just one element or they
are spread all across the matrix taking
just l2 norms will give you so just
taking l two norms will not give you the
fine-grained information about the
structure of the errors so it will only
tell you that the aggregate error is so
much but you do not know how the error
is distributed among all the elements
but when you take the l infinity norms
you get error on each of the elements in
particular and this gives you a much
fine grained information on how the
errors are propagating yeah so I will so
okay so what this means see this means
is that we set all the elements of a
minus L whose absolute value is less
than this threshold 20 and I haven't
specified what the threshold is /
delphos a later why will it be a column
it will be a matrix right no no
determinant so you start with s equal to
0 so you get an air you compute M minus
L it will be a matrix and you have a
threshold Zeta and you said all elements
of this matrix with value less than
theta 20 yeah
so that can be done by say the power
method or there exist very efficient
algorithm for computing the the top our
principal component of any matrix yeah
the lsr and estar satisfying those
conditions are unique because the
regularity conditions that I mentioned
these two conditions because so take any
such Elsa and estar at least from this
result we know that the unique optimum
of the convex program is going to be the
Telstar and estar if there were two of
them both of them should be the unique
optimum which means that they should be
equal so the analysis there does not
really depend on the algorithm it just
says that once you write down this
convex program we can write down the
conditions for the optimum so it will be
the KKT conditions the non convex
problem because we are projecting
directly onto the rank constraints we
are not trying to minimize the trace
normal trace norm we are just trying to
directly minimize the rank and the
sparsity so we are directly trying to
solve the non convex problem ok so let
me see
okay so the basic llama we use which is
very reminiscent of the last exercise we
proved in the last lecture is that if
you have a low rank matrix L star and
this pass matrix a star satisfying the
previous curve so L surrender satisfy
the regularity conditions and if L is
ranked our projection of L star plus a
star with each of the elements of a star
not being too large then we get an
infinity nom bond on L minus L star
which is one-quarter of the Infinity
norm of estar so just to recall the bond
we had earlier was that and compare that
to this where we have infinity norms in
both the places del maximum element in
the matrix maximum absolute value of the
element
okay so let's see how this proves the
lemma so we recall that okay so we will
show that if we have this step then the
next for the next step the errors that
will be incurring will decrease by a
factor of half so this is an identity
procedure what we will show is that in
each I direction will decrease errors by
half so s is said to be this thresholded
version of M minus L let me specify what
the threshold we choose will be it's
just one it's just exactly the same as
the bond we get on L minus L star
infinity once we set this threshold we
have to condition to guarantees one is
that the support of the sparse matrix
that we recover s is contained in the
support of the two sparse matrix estar
so this is because if a particular
element of s star bar equal to zero then
M minus L would just be L star minus L
and by this guarantee it's smaller than
one-quarter of whatever that quantity
and the thresholding operation centers
in 20 so the support of s is contained
in the support of estar so our
estimation of the perturbation never
goes beyond the actual sparsity and a
similar argument also tells us that we
indeed estimate a star up to an accuracy
of half of what we initially know it is
just using the thresholding the choice
of the threshold and the guarantee on L
minus L star to be smaller than the
threshold we can argue that the
estimation error error in estar is going
to be at most half of what it initially
was now in the next step we are going to
apply the same lemma 2 p.r of M minus s
right so this is a next step that we
estimate the low rank matrix L to be the
rank our projection of M minus s and M
minus s now is l star plus s star minus
s l star is the same low rank matrix s
star minus s has the same sparsity
pattern because the support of s was
contained inside the support of the star
so we have the same conditions on the
sparsity of s star minus s moreover we
have decreased the Infinity norm of s
minus s by half
right so we can apply this lemma so when
we applied this llama to the next step
you would have decreased the sparsity of
the the value of the spots perturbation
to half of what it initially was and
each of the citations then keeps
decreasing the error by half so yeah s
is equal to zero so that is why we are
doing PR of L star plus s star so it's
just PR of them so let us see why
proving something like this is not why
this does not follow from standard
results the sparsity of estar along with
the Infinity norm of each of the
elements actually gives us a bound on
the spectra operator norm of estar this
can be you proved just using the
questions inequality and then as we saw
earlier whale fin equalities tell us
that the spectral norm of L minus L star
is at most the speculum of Esther this
is indeed just the triangle inequality
as well at least in the first step and
similarly for the Frobenius norm but
obtaining bonds on the L infinity norm
are much harder and we require al
infinity norms here because there is
much more structure in the data that we
have to leverage if you are able to if
we have to get exact recovery as opposed
to only approximate recovery okay so if
there is anything to take home from this
talk it would be maybe the slide and
next so i will explain how we obtain the
l infinity non bond and i think this may
be useful in general context as well so
let me try to go go over the proof in
the rank one case so L star is used are
you use star transpose so L star is just
drank one under star is a spaz
perturbation matrix and we are
projecting it again to rank one matrix
which from which we get L and let us say
this is equal to lambda uu transpose
where lambda is eigen value and you is
eigen vector
what do we get from standard results
firstly the sparsity of estar means that
we get a bond on the spectrum of estar
say it's less than very small number Oh
point one this then gives us just using
triangle inequality and divis kahan that
lambda lies between point nine and 11.1
and you star transpose u is at least
point nine so u star and you are almost
aligned but we do not exactly know how
they differ in the L infinity nonsense
okay so we just write the eigenvalue
equation for the matrix U star you start
on both plus s stuff so this times u is
equal to lambda you this is just
eigenvalue equation so rearranging terms
gives us I minus s star over lambda
times u equal to u star transpose u over
lambda times u star but this is just
rearranging the terms &amp;amp; 0 we know that s
star is at most point 1 and lambda is at
least point nine so this matrix I minus
dis thing is invertible so we take it to
the other side and we expanded using
just Taylor series ok so there is the
same equation that we had on the
previous slide so the very nice thing
about this is so firstly it is not
explicit equation for you because they
you appear on the right hand side but
the very nice thing about this is that
the place where it appears on the right
hand side is just as inter product
between you start transistor and you and
standard results tell us how to bond
this quantity because it is just how
close you start and you are going to be
but apart from you star transpose you
everything else is explicit so the right
hand side does not contain anything
about you now the sparsity of estar and
in quadrants of U star so these are the
properties that gives you these give us
infinity norm bonds on a star to the P
times u star now all we have to do here
is apply triangle inequality and the L
infinity norm of you will be the this
will be a constant because numerator and
denominator are bounded and so the L
infinity norm of U star and a star U
star a star square u star and so on
and start to the pu star is decaying as
half to the P so all of them some
geometrically so just using this
inequality it is very easy to deduce
infinite amounts of you u minus u star
and any such quantity yeah yeah and we
had that assumption on d less than n / c
mu square CN over miss Ferrar yeah so
whenever I said yeah so we are using the
fact that d is less than n over mu
square R because you star has the MU
square R and so on so yeah just to
reiterate the only thing that we used on
the right hand side is from the
classical result is Davis kahan type
bonds where we get results on the
perturbation of the eigen vectors in the
l2 norm and use that and then further
structure on the matrices that we are
involved with and this lets us get
infinity compounds which are much
stronger than just l2 num bonds okay so
yeah this is a restatement this is the
same lemma that we saw earlier but there
is still one issue with this and the
issue is that we assume that the
Infinity norm here is smaller than some
quantity x sigma r and when we are given
a matrix we do not know how why this
should be true or how can we use this
right okay so i'll tell you what we can
do in the first step so L star is
incoherent so let me write
so L star was you star you Sigma V
transpose and so any element of L star
IJ equals the row here x sigma matrix
times the column here and this we know
was bounded by mu square root r over n
mu square root r over n times the
maximum element here is Sigma 1 so this
is less than or equal to Sigma 1 u
square r over n whereas what this
requires is we want the star infinity to
be less than mu square r sigma r over n
which would be much smaller than the
sigma 1 here so given the matrix we know
that the maximum element of the matrix
is bonded by mu square r over n times
Sigma 1 so we can threshold all the
elements of the matrix so that every
element is the maximum is still Sigma 1
mu square r over m in that case the
perturbations cannot be larger than 2
Sigma 1 mu square R over N and so that
is the case we have this general Emma so
okay let's say we have rank our matrix
but we are doing the projections only on
to rank K we require estar infinity to
be only smaller than you square r over n
times Sigma K and we essentially have
the same result as we had before plus
the error due to the due to the
remaining components k plus 1 to R so in
particular when k is equal to 1 all we
are assuming about estar infinity is
that it is less than mu square r 1 and x
sigma 1 which we know how to achieve by
just three holding all elements of the
matrix we do the projections on to rank
one that will give us this guarantee on
the rank one projection and right when
this X star infinity right when the
error is comparable to mu square R over
and Sigma 2 we will go on to the next
stage take k equal to 2 and apply this
do rank to projections and then apply
the same lemma so we keep refining at
each step getting one dimension at a
time
yeah yeah yeah actually we do not
exactly need to know though we do not
need to know the exact value all we need
to know is a bond and we can get a
estimate on the bond by just looking at
so we just need u square R Sigma 1 over
N and this indeed can be replaced by the
maximum element of the matrix so that
can be gotten easily just by looking at
the matrix okay so is this part here so
this was a direct rancor version where
the matrix was ranked are and we were
doing rank are projections this is a
more this is a final version of the same
lemma where we the matrix is still rank
are but we are doing projections only on
two ranking we have a weaker assumption
because you are and we are since we are
doing positions on to rank a but we have
this error incurred due to ignoring the
components from k plus 1 to R okay so
this is the algorithm I just described
so for k equal to 12 are we try to get
one component at a time and for each
component we do projections onto the set
of ranking matrices and the threshold is
chosen appropriately again depending on
the lemma that we saw earlier and for
each k we run this V ronde citations log
1 or epsilon at log 1 over epsilon times
and at the end of it we are sure to have
decreased the error up to epsilon or up
to the next last up to sigma k plus 1
then we go to k plus 1 k plus 1 prank
and then keep repeating this procedure
to rank R
so that is the maximum you need to you
may converge earlier as well but once
you so what this lemma tells you is that
you will decrease the error in L minus L
star up to this number you cannot go
beyond and if this number is my so since
your desired accuracy is much F salon if
this number is much smaller than Epsilon
you do not really care so once you run
long one or epsilon iterations either
your error is equal to epsilon or your
error is this much so long one or
epsilon is because we have this having
and if this is much smaller than epsilon
we only need to go to epsilon accuracy
okay so this was so algorithm yeah so
the final theorem is that if L star is
mu incoherent rank R and s star has at
most n over fight well mu square are
explicit constraint then in each
iteration of our algorithm the support
of st is contained in the support of a
star s t minus s star at the end is
going to be at most epsilon over N and L
t minus L star in each element is going
to be at most epsilon over L ok so let's
see how this compares to convex solvers
the in exact ALM is the fastest and most
widely used special-purpose convex
solver for this problem and what it does
is it tries to solve the convex problem
by trying to project alternately project
onto the set of onto the trace norm ball
which means that the trace norm is equal
to certain number number number and
again on to the l1 ball so it keeps
alternating between these two convex
projections the issue with this is that
even though the final matrix that we are
going to recovery is low rank the
intermediate I traits could be high rank
whenever you project onto the trace ball
and that is the main reason for the
growing complexity of this method so n
alpha so this plot projects the maximum
rank of the intermediate rate and the
x-axis represents the number of
non-zeros maximum number of non-zeros
per row or column so as we increase the
number of non zeros in our data matrix
as weak as we are corrupting our matrix
more and more the intermediate rank of
the ilm increase goes beyond 600 for
instance even though the two underlying
rank is just 5 so in most of the special
purpose convex solvers for with trace
norm minimization the biggest issue is
that the intermediate rates could have
very high rank which leads to both
storage issues and time issues because
you are having to do a much larger SVD
as opposed to our method where we only
do rank rsvd
okay and this is a comparison of speed
for alternating projection which is our
algorithm and in exactly LM which is in
the red the first plot shows time versus
the number of corruptions per row or
column and the second one shows the
show's time versus the incoherence
parameter and in both cases our
algorithm runs much faster than in exact
am so let me show you this example so
this is I think a shopping mall or
something like that and so the first on
the top left is the original video and
the bottom is the low rank component
which would correspond to the background
which is gotten just by doing PCA and
the second column so the top is the
lower end part record by our algorithm
the bottom is the spot spot record by
our algorithm the third column is what
is required by convex relaxation
background and foreground so the video
is maybe for a minute or so I'll run and
so just a vanilla PCA has much more
noise than either of these two
backgrounds
so and it's hard to distinguish between
at least I find it hard to distinguish
between these two but the only thing is
that our algorithm is sorting runs much
faster than the convex relaxation
technique I mean they are not exactly
low rank so they're so if you want
exactly rank maybe you need to go to a
much higher rank so this is by using
rank equal to 5 or 7 or something like
that a very small number okay so to
summarize what I presented is a simple
non convex algorithm for doing the
robust PC a problem the way the
robustness is getting rid of sparse
perturbations and the algorithm consists
of alternating non convex projections
non convex because we project
alternately onto the set of low rank
matrices and sparse matrices we saw that
our algorithm is factor are slower than
vanilla PCA this is because we have our
different rounds and each round just
does one pc so since there are different
rounds it's factor are slower than
vanilla piece here but it is much faster
than conduct the convex version of the
robust PC whenever the rank is much
smaller than the ambient dementia and a
metal bowl of some of this work is to
design faster non convex algorithms for
solving machine learning problems in
many cases in recent work we have seen
that people have come with convex
relaxation techniques for solving
nonconvex problems under certain certain
data assumptions on the input and it is
not clear if we really need to go to a
high dimension relax the problem into a
convex problem and try to solve it we
can indeed try to solve these problems
under the same regularity assumptions in
the non convex regime and in many cases
we have been able to show that this non
convex methods indeed recover the two
matrices up to
the accuracy as the convex relaxation
techniques yeah I think I'll stop there
Thanks yeah
no it is not constant so it will be
constant at least across a reasonable
period so it will be so you can see a
person coming back coming going away and
standing back again no no no so each of
these so this is a frame you put this in
a long vector and you stack all of these
one side by side so that will be your
matrix 1 matrix yeah and each column
corresponds to one frame the assumption
here is that the background frame does
not change too much over multiple
columns so the background frames
correspond to the low rank part because
they have more mass here whereas the
foreground part keeps moving is very
varying across multiple frames so they
are the sparse perturbations yeah yeah
yeah yeah yeah
I am only familiar with very little work
in this area but I know one work on the
thing called rook four REE prox robust
repro this is from America was money and
I was state okay don't know if I'm not
familiar with the work she talks about
online versions of these alkyl okay
instead of batch process okay and you're
not familiar than mine yeah maybe I
should take a look at thank you moot
question
yeah yeah yeah yeah yeah yeah yeah yeah
I don't know how this would work in such
a setting
for approximating the those non convex
functions like a zero loss function so
can you pinpoint some like papers that
like that very good for reading and like
about these non convex techniques like
optimization techniques you mean how to
chose a surrogate loss functions or
sorrow weight loss function generally
are a convex function yeah like as you
mentioned like there are non convex
optimization techniques that directly
salsa yeah yeah like the Amal problem
yeah yeah I mean this so the non convex
version has been very recent so I do not
know of a survey of any of those works
but I can send you a list of papers
which does non convex optimization for
the compression no I don't know if this
model is relevant for compression of
videos yeah sir so maybe once you know
the background part is low rank so it
automatically gives you compression
because m is n n square matrix whereas
the low rank part and the sparse part
have very succinct representations so I
but I do not know if people have used
this for compressing videos yeah that is
a good point
I am Not sure so this is a standard test
set which is available on the web I do
not know how they generated it and all
those songs so this is one of the
standard test sets that available for
background foreground separation so this
is available on emass website from UIUC
i can point it at this and many other
videos available for this task if there
are no more questions let us thank
runneth for encouraging us to think
non-convex we thank you with the token
of our appreciation</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>