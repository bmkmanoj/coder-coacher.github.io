<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning Course - Lecture 6 | Coder Coacher - Coaching Coders</title><meta content="Machine Learning Course - Lecture 6 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning Course - Lecture 6</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5HBz8rSsps0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so let's start any questions from
last time anybody tried the last
assignment that I gave me so what
happens when you plug in your current
density estimator into the K instead of
tea normal estimate into K means yes now
see if you want to get maximum mileage
out of this class you have to do work
you cannot the not a passive laughs so
if you remember what we wanted to do
last time is we wanted to say okay
suppose I have instead of so my P of Y
conditioned on X i am going to write
this as P of Y P of X conditioned on Y
over P of X ok and this if I have if I
have em samples okay and of my of them
comes from the Wyatt class then this is
just my over em okay and then P of X
condition on why I am going to write
this as summation over all I such that y
is equal to y que of 1 over R 2 DD rh to
the d I don't remember what notation
we're using so let's use H to the D K of
X minus X I over H
x and then this divided by 1 over n pi
ok and then p of x is nothing but 1 over
m summation over all i hope 1 over H to
the d we call it k of x minus XJ o.h
play so all we did is we substituted for
P of X we said this is our kernel
density estimate for P of X 4 P of X
condition on why we just look at the
points which are labeled y and
substitute their kernel density estimate
and for p of y we are just taking the
fractions okay if you do this then this
cancels out okay this cancels out this
cancels out and what you are left with
is basically summation of i why i equals
my of K times X minus X I over H over
some constant I don't care what this
constant is this constant is this it
does not depend on anything in
particular if suppose I tell you that
why is has only two classes right so let
us say if Y is just plus one minus one
ok so if this were they this will just
the two classes so if you just had a
binary classification problem then what
happens is your decision rule is if you
remember your decision rule is our max
over Y of T of my condition on X right
for you you predict with the class which
has the maximum probability the
posterior probability ok and so which
means in our case will predict if p of y
equals plus one condition on X is larger
than p of y equals minus one condition
on X
then you will predict plus one otherwise
minus one weight or i can write this as
if this minus this is bigger than 0 then
you predict this is plus 1 otherwise i
predict this is minus 1 so now what is p
of y equals plus 1 minus p of y equals
minus 1 according to this formula right
so according to this form life i plug
this in okay what happens is this is
nothing but you can write this as
summation of Y I okay k of x minus x I
over H over a constant is bigger than
equal to Z then you predict plus 1
otherwise you predict my ex is this lip
so if i had many classes then this is
the general formula i am specializing
this formula for two classes okay so if
I suppose I told you that instead of
multi class i just have want to use the
same formula for doing a binary
classification problem so if suppose my
why was just plus 1 or minus 1 then you
remember what was my decision rule my
mic a means for sorry my k-nearest
neighbor decision rule was i take p of y
conditioned on x look for all possible
why take the Y which has the largest
probability and predict that Frank so
that was my rule was all my x over Y of
P of Y conditioned on X so I find the
maximum Y which has the maximum
posterior probability and then I predict
that right so this is my prediction this
is my prediction my star okay now in the
case when you have just two classes plus
1 and minus 1 then what happens to this
arc max right so the AG mech simply says
that if p of y equals plus 1 is larger
than p of y equals minus 1 then you
should predict class 1 and otherwise you
should predict minus 1 or equivalent lee
i can write this as if p of y equals
plus 1 minus p of y equals minus one is
bigger than 0 then it must be plus but
what is this condition p of y equals
plus 1
means word I have to take all this
summation over all the plus classes and
I had to subtract from it the summation
over all the minus class okay but that i
can write compactly as summation over
why I times K of X i minus x i oversee
clear that's all i am doing and this you
can think of this as the weight that is
given by the IH point to the location X
correct so this is now this is nothing
but a weight factor right so this is
this defense what does it vary on or
what does this thing depend on this only
depends on X and X i clear so it depends
on what is the distance between X and X
I and so you can think of this as point
X point x is giving you a weight sorry
point X I is giving you a weight at
location X when so it tells you how
close or far away you are depending on
that this is WI let's see weight the
weight of the ayat point at location X
and you are just summing this weight by
multiplying it with liar so in other
words what you are saying is all the
positive points are going to give me
positive weight all my negative points
are going to give me negative weight if
the positive weight is larger than the
negative sum of negative it then it is
plus one if the sum of positive weights
is smaller than the negative it then it
is minus 1 so this is also you can think
of this as some sort of a weighted
linear classifier you are just simply
saying that every point gives you a vote
the word of that point depends on the
distance of that point from X so if you
want to predict at location X you see
who is close to you and what weights are
they giving you are the weights positive
or negative is if the overall sum of H
is positive in your predict plasmon if
node it's mine okay so now we will go
into the next part
and this is some part of it is going to
be fairly mathematical unfortunately you
cannot help that but the nice thing
about what we are going to talk about
today that provides a very unified view
of looking at things and it allows you
to do many many things in a rather
unified way you don't have to look at
individual distributions but you can
talk about an entire family of
distributions okay so that's so that's a
caveat so please pay attention because
the math is it is mad heavy but again
the math is important okay and again if
something is not clear please ask me
because I can I can go over it as slowly
as you want but really I want you to get
this if you get this you know the rest
of the course of sort of becomes much
much much more easier okay but if you
don't get this rest of the course is
going to be quite hard so stop me
anytime anything is not clear
okay so so far we did nonparametric
density estimation right so well we said
we do not make any assumptions about the
functional form of the density we are
simply going to do sophisticated
counting so it is going to take data and
we are going to let the data speak for
itself the other paradigm in machine
learning is you say I am going to assume
a functional form for my density okay
and then i am going to estimate the
parameters this is very different and
this is placing a very strong assumption
right so the assumption is it yeah they
are the underlying assumption is it you
are somehow assuming that you know the
functional form from which the density
was strong okay are you are you are
making a very strong modeling assumption
than that maybe it is drawn from a
Gaussian in stone from a poisonous storm
from some distribution this may or may
not be true but what you are saying is
assume that that is ground truth i'm
going to estimate my parameters okay so
that is your that's a fundamental shift
in thinking and in order to do that of
course you may have gone to courses you
may have done machine learning courses
in the past where they say okay you know
i am going to assume the data is drawn
from a gaussian and let's go and derive
the equations for a gaussian or they may
say well let's go and do it for a
burning and whatever right instead what
we are going to do is slightly different
what we will do is al introduce you to a
family of distributions and it turns out
that this family of distribution is very
powerful in the sense that almost any
name distribution that you can think
about belongs to this family okay so
we'll see a few examples of these kinds
of distributions which belong to the
family and then i'll give you unified
ways of dealing with all members of this
family okay that is what is a beautiful
thing about it so what is this firmly of
distribution that I am talking about it
is called the exponential family and the
functional for the distribution looks
like this so I am going to say P of X
condition so this is a parametric family
okay so which means it is always
parameter eyes by something so P of X
condition on theta is some P naught of X
times and I don't worry I'll explain
each thing in great detail so first of
all you have to note that this is a
parametric family of distributions and
this is the parameter what do you mean
by parametric family of distributions
which means think of something like a
Gaussian is a Gaussian has a functional
form you know the functional form and
the parameters are the mean and the
variance so if you specify to mean the
mean and the variance you completely
specified the distribution okay so in
the same way if you specify the
parameter theta to me or specify this
distribution this this parametric family
of distributions this thing Phi of X is
called the sufficient statistics okay
sometimes in machine learning terms we
all will also call this features so this
is simply saying that X could belong to
any domain it does not matter what
domain X belongs to and that is an
important distinction so X could come
from any domain fences X could be
strings trees graphs vectors whatever it
is Phi of X is a function which takes X
and spits out a vector ok so this vector
could be whatever dimension there is
another very important thing I all I am
just mentioning it here we will see more
about this when we look at support
vector machines finally not be finite
dimensional ok so that's one thing to
keep in mind this is a usual dot product
so whenever I use this ok in the course
this means it's a dot product ok right
now I'm just thinking of the Euclidean
dot products so just think of this is a
this is a vector in some D dimensional
space theta is a vector in some D
dimensional space and this is a dot
product between theta and ikes just the
normal Euclidean dot product which is
nothing but theta transpose Phi of X
which is defined as theta I
I effects okay but when we go to look at
support vector machines and other fancy
algorithms this will become a more
sophisticated product so I will leave
this notation here instead of using this
notation I'll use this notation this is
deliberate okay this is called the log
partition function the name for this
function actually comes from physics and
we'll see in a minute why it's a this is
a very very important function in sort
of you know it's the heart and soul of
the of the exponential family and the
log partition function as you can
imagine is a normalizing function what
do you mean by normalizing function you
know that this is a density right so
what is the property of a density the
property of a density is that so what's
the property of a density I do this it
should be one right because it is a
density it must integrate out to one so
what the log partition function does is
it ensures that this density integrates
out to one okay so if you do this
actually it turns out if you write this
expression it's fairly easy to figure
out what the log partition function is
because p of x given theta can be
written as P naught of X X 4 phi of x
times theta over Xbox J of theta DX
integral right but this is a this is
independent of X whatever is being
integrated is X naught theta so i can
take this out okay i can bring it to the
other side so this is nothing but expo g
theta or if i apply log-transformed on
both sides this is mine ok so my log
partition function therefore has a
functional form
now there are a few things to keep in
mind okay this well it looks nice and
easy that it has a functional form there
is a very big hidden danger or there is
a very big hidden problem in this
expression okay the big hidden problem
in this expression happens to be this
integral what does this mean it means
you have to integrate something over the
entire space okay suppose you are
modeling the space of all documents it's
mean you have to do this integration or
this space of all possible documents or
suppose you are doing this X is some D
dimensional vector which means you have
to do this integral over the entire
vector intense vector space okay of D
dimensions or whatever is the domain of
definition of X you need to do this
integral okay and this usually causes
problems this will we'll see a lot of
examples of where this causes problems
but they are also there also some very
nice properties of this function we will
talk about but I just want to point out
this okay so this is called the log
partition function and this is called
the base measure so the base measure I
am just going to for most part and just
going to assume is timed at lebesgue
measure so it doesn't it's it's a system
it's some some standard measure and I'm
just going to ignore it you can keep it
round it keeps on adding constants and
make the equations messy but for most
fun and just going to assume that this
is just a constant it is a base it's in
a big mission okay but in some cases
it's very important so this is
especially important when you are
looking at discrete domains for instance
that you don't have a lebesgue measure
and you have to work with some other
measure but whenever there is an
exception i will talk about it but for
most part you can ignore this just
assume that this is just some constraint
density okay so only otherwise all this
is telling you is that how does how do
you do sampling in your actual space
like if you are looking at this pace of
documents how do you do sampling in that
space so don't don't worry about us
follow okay and this scary distribution
family I claim
is a abstraction of almost any density
that you can think of any name density
that you can become more or less okay so
examples that belong to this family
include binomial i'll show you how the
buyer so Bernoulli by Bernoulli
multinomial beta gamma gaussian boy song
all these distributions belong to this
family what do I mean by belong to this
family which means that there is a
representation fee of X and there is a
parameter theta such that the
distribution the way you write it can be
massaged to be written in this form that
is what i mean by the it belongs to
exponential family ok so let let me go
and show you an example first very very
simple example of the Bernoulli ok so
what I am going to assume is my space X
just contains two points zero and one
just for heads or tails or 0 or 1 and I
am going to assume that Phi of X is
simply going to be X ok so this is this
this is my space that is my sufficient
statistics and now I need to estimate G
of theta ok so what is G of theta it is
the log of the integral so here this
space contains only two points right
which is 0 and 1 since it contains two
points I do not need an integral I can
just do a summation ok so it is a log of
P not i'm just going to ignore this is
the x of x times so in this case x is
equal to plus 1 or 0 times theta plus x
of one-time state correct so what is X
plus 0 times theta
right so this whole thing is just one
and what is xbox one x theta is just
exposed it right so this is going to be
my log partition function is log of 1
plus X puff theta and what is my density
going to be p of x equals one condition
on theta is just going to be expo theta
over 1 plus x 4 theta and p of x equals
0 condition on theta is going to be 1
over 1 plus x ok now if you give me any
Bernoulli distribution with parameter P
right I am just going to set this to be
equal to P and solve for theta which I
can always doing this is exactly equal
to one minus P okay so any Bernoulli
distribution can be rewritten in this
form okay similarly this is a good
exercise to try out I won't do the math
but again you can either look it up in
the book or try it out on your own if
you set Phi of X to be x + XX transpose
things one if you set your Phi of X to
be this and if you set your theta to be
the following thing Sigma inverse mu and
Sigma inverse okay if you set your Phi
of X to be this guy and theta to be this
it turns out what distribution will you
get will get a gospel okay and similarly
you can try for other things so there
are the multinomial by saw all of that
okay so the point here is while it seems
like we are doing a lot of work right i
mean this seems like why do we want to
why do we want to you know take a simple
distribution like the Bernoulli which we
all know and understand and love Zoe
well and try to write it in this very
complicated form involving X
shells and J of theta and all of that
what advantage do we get out of this
that's a question that we need to us and
we'll answer that can wheel answer this
question by saying that we can say many
things about the entire family we can
say many things about this entire family
of distributions in one shot we can
handle all of them in one shot we don't
have to worry about individual
distributions we do not have to think
about what happens for the Gaussian what
happens for the burden only what happens
for the poison i can give you statements
that apply for any kind of modeling that
you that you use so questions before we
go on and find properties about this
distribution discuss properties I mean
I'm sure that all of you are wondering
about why do we need this complicated
expression but that i will answer but
other than that anything about the
functional form anything that's not
clear okay there is one thing that i do
want to point out so what is it water so
fed up you have to understand this
two-step process in some sense when
you're working with exponential families
of distribution what is the two-step
process if you select Phi of X what does
it mean so if I fix a Phi of X if I tell
you that Phi of X is something what does
that mean that means that you are fixing
a functional form for the distribution
okay it's like saying if i tell you that
i'm going to use this as my Phi of X ok
then I am fixing a certain functional
form which means that I am saying that
now I'm going to work with Gaussian
distributions okay and having fixed Phi
of X when I selected theta what does it
mean I am picking a particular member of
that functional form right so saying I
am looking at the Gaussian distributions
how do you say that I am going to look
at dawson distributions you say that by
saying i am going to fix this Phi of X
there is an equivalent definition which
is you can tell me that I am going to
use this but
ecology of theta actually turns out that
equivalent under there are some mild
technical conditions about
dimensionality but if modulo that if you
either tell me J of theta or you tell me
if I of X they are one and the same you
are telling me what is the form what is
the functional form you are going to use
and then the challenges to find a theta
like right so that the theta fixes a
member of the family you see the
two-step distribution so you can say i
am going to work with gaussian
distributions you specify phi of x or
specify g of theta 2 minute now i say
okay now find you're working with
gaussian distributions tell me which
particular gaussian distribution do you
want to work with then you have to tell
me what are your parameters theta clear
so you have to tell me that i am going
to work with a gaussian which has say
zero mean and unit variance then you are
telling me that this is a particular
member right so there is a two-step
process so when you think about it in
this two-step process in machine
learning what happens is that fixing phi
of x or fixing g of theta is the
modeling part right so you are saying
that i want to use a particular kind of
distribution because I know something
about where the data came from or I know
something about the underlying process
that's generating the data does a
modeling part and then once you have
decided on the distribution family or a
functional form then finding the
parameter theta which say best explains
the data that you have observed is the
actual parameter estimation part okay so
there's a two-stage distinctions sorry
yeah let's Emily ma P will see many
different ways of estimating parameters
so this is where the sort of domain
knowledge in some sense comes in and
this is finding the theta is where you
are actually instantiate in the model on
the data that you have okay so you see
these two distinctions and that's that's
important
in any other questions yes the elements
of Phi of X he don't number they can
give you complete different X is a
vector ya know so what has that is a
good point so the way the dot product
here works is that you take a trace so
this is X X transport with Sigma inverse
so that is how you define the dot
product that is why I did not want to
write this just simply as a euclid n dot
product I wanted to have the flexibility
to just say it is any doubt product
because I mean you know if I want to be
pedantic and user use that then what I
have to do is there is an operator which
I would apply which is called the vac
operator so what it does is it takes a
matrix and flattens it out into a vector
and then I can do the same thing to this
no no this will come out to be the trace
so you can verify this so if you if you
take a matrix and flatten it out when
you take another matrix and flatten it
on then you get a place but in general
this is a good point right so in general
this can be an arbitrary dot product
right and that is another thing actually
we will see how to play around with so
this is another knob that we have to
tune or we have handle on is by changing
the dot products you can change the
distribution and the family this space
in which it is living and that gives you
additional handle that is basically what
we will do when we start working with
kernels okay any other questions yeah I
am for the most part I'm ignoring t-note
of X I am just going to assume that is a
lebesgue measure because we are going to
deal with continuous spaces but t naught
of x is very important for instance when
you look at a Poisson distribution
poison is on a discrete space then you
have to have a proper a proper base
measure so
that's why I am sort of sweeping it
under the rug but yes if you really want
to specify everything you have to tell
me what is the P naught of X what is the
Phi of X orgy of theta one of it but I
am just going to assume that it is a
it's a uniform loving marriages just
because otherwise what happens is in all
the calculations that I do they there
will be always a constant hanging around
and I just am lazy enough not to write
that so other questions so this is quite
this it turns out that these families of
distribution have remarkable properties
I mean there are amazing properties that
this this family of distribution says
and I just want to give you a flavor of
some of them so the first property that
I want to talk about and most of these
properties come from the from the log
partition function itself Oh in the
reason it is called the log partition
function is because exponential families
are also widely used in statistical
physics and this is an indie integral
overall energy states of a system and so
that is why it is called log partition
function the name comes from physics
okay so they were they used to use
exponential family quite a bit okay so
let's see some some remarkable
properties of this function J of theta
so the very first property of this
function is let me take the gradient of
this function with respect to theta so
if i take the gradient it turns out let
me just do this mechanical eng anything
and then you will see this so that is
again there's a cave yet whenever you
have integrals and differentials you
have to be very careful about when you
can you can swap the integrals and the
differentials okay so it so you are to
you have to either show something like
dominated convergence or you have to
show a technical property before you can
swap an integral and differential you
and just take an integration swap it
with a differential but for other
purpose and is going to throw caution to
the winds this is a mean it's not
entirely technically correct what i am
doing because i have to show you an
argument to show that I can do this walk
but I am NOT going to show you that
argument because i just want to show you
the intuition okay so if i take the
gradient this is simply going to be the
gradient of the log is nothing but you
get rid of the p note this is just x
and now this is where i had to swap the
integral in the differential but i am
just going to assume that i can do that
okay so this is just I take the I am
just applying chain rule right so I take
the gradient of the log is 1 over X so
that is one over this guy and then I get
the gradient of this integral of x 5x
times theta I am swapping the the
integral in the differential so I get
the integral of the gradient of Expo 5x
times theta again applying the chain
rule I get the gradient of x is just x
bit self the gradient of Phi of X sine
theta is Phi of X okay so this is just
simple application of the chain rule for
computing radiance and what is this
equal to this is Phi of X T of X
condition yeah and what is this quantity
anybody recognize this
anybody
this is the expected value right so this
is the expected value under the
distribution of fireworks so if you take
the first derivative of G of theta you
get the expected value of the sufficient
statistics under P of x given theta or
by the way they are called sufficient
statistics because like I told you if
you specify Phi of X you are specifying
the functional form so in some sense as
they are sufficient to fix the
functional form that's why they call
sufficient statistics the gradient of G
of theta gives you the expected value of
Phi of X under the distribution this is
why is is a useful result because
something funny something better happens
if i take the second derivative of G of
theta okay what do you expect to see it
turns out that you get the second
movement of the distribution okay and
then if you take the nth derivative of G
of theta you get the nth moment of the
distribution okay again this follows
this is something that I want you to try
so that's why I am not doing it but
again like I told you to get the maximum
value out of this course you have to go
and do some of these things on your own
okay it's not enough to just sit here
and look at the board you have to try
some of this so I won't do this but
something very fun fundamental happens
you get the second movement yet the
variance it is a variance under the
distribution of Phi of X and so on and
so far so you can show that if you take
the nth moment it will get the nth
moment and because of this property the
log partition function is also called
the generating function is called the
moment generating function
now why is this moment generating
property very useful to us so first
first of all this says it if you want to
get any movements of the distribution
just take the log partition function
just keep on differentiating and you
will get the moments okay but why is
this moment generating property of
particular interest to us is because of
this result this result is very very
significant to us so the the second
derivative of G of theta is the hessian
of G of theta right and the variance is
a matrix what kind of matrix is the
variance what properties of the variance
have symmetric and positive semi
definite or positive right so this
matrix is symmetric and positive same I
I write positive semi definite but could
be positive so this is as symmetric
positive semi definite matrix so if you
remember the review that pratik did what
is a positive semi definite matrix all
its eigenvalues are bounded away from
zero for a positive definite matrix and
for positive semi definite matrix they
are all non-negative so some eigenvalues
could be 0 but all of them are
non-negative now if I tell you that the
Hessian of a function is positive semi
definite what does that tell you about
the function anybody knows this the
function is convex so we will talk a lot
about convex functions later in the
course maybe even next week but one of
the neat properties of convex functions
is that so water convex functions
intuitively their functions which look
like this you look like a book okay even
in high dimensions whatever dimensions
you go to they look roughly like this
one of the neat properties of a convex
function is that there is a unique
global minimum so when you minimize this
function you'll always get a unique
global minimum okay we will talk a lot
more about the
properties of convex functions that is
sort of almost half the course will be
based on this but for now all we
conclude from here is it this is a
moment generating function the log
partition function is moment generating
plus for us the most important thing is
that it is convex and the reason we care
about it being convex is because as I
told you when you minimize a convex
function you will get a unique global
solution a unique global minimizer why
is that important suppose I give you a
function which looks like this nasty
function which looks like this ok if you
go ahead and minimize this function it
is very hard intuitively first of all it
is very hard because you know the
function itself rigo's and does all
kinds of things second of all it's very
hard to find out whether you are at the
global optimum or not but if suppose I
gave you this point this is a local
optimum you don't know whether this
local optimum is it or does the function
actually have a value below this there
is no way to find out but for a convex
function if I tell you that this point
is a global minimum you can immediately
go and verify that this is a global
minimum and we will talk a lot more
about why this is an important property
but for now for this lecture this is all
that you need the fact that for a global
for a convex function not only can you
find a global minimum but you also have
a certificate so you can somebody cannot
fool you if I tell you that the function
is convex and then here is the solution
or here is the minimum value of the
function they cannot fool you you can
immediately verify and say no you are
lying to me or you're correct if
somebody gave you a non-convex function
and said this is the local minimum we
all know where this is the global
minimum you are no way of checking that
ok so that is very important so for our
purpose therefore this is what we care
about the g of theta is convex why does
this matter to us because now I can make
some very very fundamental claims about
doing estimation or doing modeling with
the exponential families now let me show
you this if I gone question
questions comments say that I am doing a
fantastic job or nobody is following
this is in the parameter space right so
X is excess discreet but the parameters
need not be right so like faces here is
an error like we saw the Bernoulli was
an example right DS the space is
discreet because it's only 01 but the
parameters need not be discreet right
because the theta could be anything
because you see P was equal to 1 over 1
plus X 4 theta and so the range of theta
is from 0 to infinity other questions so
this is yeah it's a where you have to
make that distinction because everything
is happening in the in the you know in
the space of theta any other questions
okay so now let us actually see some
proper like I have been promising you to
show some amazing properties of these
functions I think I have just enough
time to do that so let us try to use
this motion so suppose I give you some
data it's our usual setting that we did
last time as well so I give you data so
i give you data X 1 to X I don't know
what to acquire I should use some
notation my notation may not be
consistent across lectures but at least
hopefully will be consistent with in
which case a given some data X 1 takes
em and you are going to make an
assumption that this data is drawn from
some exponential family distribution
okay and you're you know you fix up a
Phi so like I said before
we are going to fix a file you're going
to say I am going to assume that this
data was drawn from a distribution whose
functional form I know that is what you
are saying when you fix fine okay and
now my task is to find the best
parameter theta that explains this data
right so you remember the emily thing
that pratik was talking about so we are
going to do emailing with this
exponential family so we're going to
assume that this data is drawn from this
distribution and now i am going to ask
tell me what is the most likely
parameter that would have generated the
data that observed so i observe some
data tell me what is it most likely
parameter okay so for that i have to
write d so let me call this capital X so
I have to write capital X condition on
theta and because i have tell i'm
assuming the usual assumption that the
data is drawn I ID from this
distribution so this is just nothing but
a product P of X I condition and theta I
going from one time I had a assumption
which can also be because i am going to
assume this exponential family this is
nothing but i equals 1 to M of X 4 Phi
of X I times theta minus G of theta okay
and this can be written as expo
summation I going from 1 12 y of x times
theta the same times theta and instead
so this is the likelihood so I can take
the negative log likelihood right so if
i take the negative log of this
likelihood what do i get i get this plus
this this is my negative
so what am I saying if so what how do
you read this expression the way you
read this expression is suppose you are
assuming a parameter theta how likely is
it that this data was generated by an
exponential family with Phi of X how
likely is it that this data or senator
so say for more more concretely suppose
you are assuming that you are working
with a Gaussian say a Phi of X is X and
X X square then you are saying that
suppose if I assume that the mean of the
Gaussian is 0 and the variance this
month how likely is it that this 0 mean
unit Gaussian variance is to have
generated this data for every parameter
you can say that right so for every
Gaussian you can ask how likely is it
that this Gaussian generated this data
that I observe and what you are doing
and by writing the negative now what you
are doing is in Emily you will say among
all possible gaussians I am going to
select that Gaussian which is most
likely to generate at this data that is
a maximum likelihood estimation
principle so you are saying among every
Gaussian has a certain probability to
have generated this data right because I
can say however likely or unlikely I can
I can do this so say if my data was one
dimensional so if I data is look like
this if i gave you a Gaussian whose mean
and variance look like this this is also
likely to have generated this data and
if I give you another Gaussian say which
looks like this okay there's also a
certain probability that this Gaussian
generated this data so i am saying among
all possible gaussians find me the one
that most likely could have generated
this data you see the question oracle
and i can see because log is a monotonic
transform i can say if i take the
negative log take this expression and
minimize it maximize this probability or
take the negative log of this
probability and minimize it because log
is a monotone transform it doesn't
change anything
okay so now if I take this quantity what
does this tell me this tells me that I
have to minimize okay this quantity G of
theta minus 1 over m summation Phi of X
items i can divide and multiply by m
which is a constant so it does not
matter so i have to minimize this
quantity okay this we already know is a
convex function this is a linear
function of theta okay this is just a
dot product so it is a linear function
summation of linear functions are still
linear multiplying by constant still
preserves linearity so this overall
function is a linear function of theta
this is a convex function and subtract a
linear function if you know anything
about convex functions immediately you
know that convex minus linear is still
convex since this overall function is
convex what do we know about the
minimize minimization procedure we know
that the minimum exists the minimum is
globally unique current so we can and
actually in fact i will show you we will
have lectures on this an optimization
show you that finding the minimum is
also easy it's not just that it's not
just a theoretical exists initial
criteria not just saying oh there exists
a minimum you can find the minimum
easily ok so now think about what
something profound has happened what is
it that has happened I did not make any
assumption about 5x and I did not tell
you this is a Gaussian I did not tell
you it's a poison I did not tell you
it's a Bernoulli I didn't say any any of
that multinomial none of this for any
member of the exponential family this
result holds true ok
which means that whenever you do maximum
likelihood estimation with a member of
the exponential family there is a unique
global solution this is amazing if you
think about it right I give you some
data any arbitrary Gaussian I am just
going to say assume that a Gaussian
generated this data any arbitrary
Gaussian could have generated this data
infinitely many possibilities but what
this is telling me is that there is one
unique Gaussian right which will best
explain the data very very powerful
statement and it does not matter whether
it is Gaussian it is poison or it is
multinomial it is beta any of these
distributions you could assume that this
data was generated by any of these
distributions and there is always a
unique solution okay so instead of going
to 10 classes and attending all of them
and learning about each distribution by
doing a little bit of math you can do
this in an abstract way and this is a
beauty of the exponential sense this is
the beautiful thing about doing this in
abstract okay there is even more so let
me show you one property before we
stopped for today which is now let us
try to understand how does this minimum
look like okay what is the properties of
this minimum like what does this
argument are the minimizer what does the
theta that minimizes this expression
look like okay so how do i do then let
me take the gradient and site a 20 if I
take the gradient insid 20 where do I
get summation Y of X Y is equal to 0 or
this okay what is this quantity sorry
yeah but when you divide it by 1 over m
is the average right it is the empirical
mean of the sufficient statistics of the
data that you observed so you get x 1 to
x m data you compute the sufficient
statistics on them 1 over m this is like
the empirical average so what does this
tell you you try and what do you know
about this quantity this has to be the
expected value right this is the
expectation of Phi of X under the
distribution P of X condition on theta
so what this tells you is that when you
have found the optimum theta that theta
will have the property that the
expectation of Phi of X under the
distribution will match the the
empirical expectation or the empirical
mean of the data that you observed so
very pleasing proper diet sort of makes
sense it says it when you come up with a
estimate what you get as an expected
value of Phi of X in other words the
mean of that estimate is equal to the
empirical mean that you observed in the
data sort of makes it satisfying and
also I turns out that for a maximum
likelihood estimation in the exponential
family you can show rates you can show
that oh if I give you this much amount
of data then your estimates will have
this much probability or this much
confidence of being away from the true
optimum so you can you can give rates on
that however things are not all very
rosy with a maximum likelihood estimate
okay one of the biggest problems of the
maximum likelihood estimate is that it
relies entirely on the data in some
sense which means if you give me data
which has outliers immediately the
maximum likelihood estimate starts
having trouble I'll give you one example
and we will work this out in my next
lecture suppose let us say I observe
some data okay and all the observations
say i give you say thousand observations
which are all between zero and one okay
and i give you one observation which by
mistake happens to be 10,000 okay so
there was some matter in your
observation process or somewhere when
you were shuffling the data this happen
and suppose you are using a maximum
likelihood estimate say even with an
with a Gaussian what will be your
estimate for me and your estimate for
mean will be somewhere here since
completely bogus because one point
should not get you so far away from the
rest of the data but unfortunately that
is what happens with Mme and we will
talk more about it i will give you an
example we'll work this out and then i
will show you why emily has certain
problems and why you need to fix em le
in some sense there are things that you
need to do in order to fix em in so we
will talk about that in the next class
ok questions but just think about I mean
again the key takeaway from this classes
that yes it is painful to write things
as an exponential family sometimes it
takes a bit of effort good thing is that
a lot of this has already been done so
if you pick up any old statistics book
you know you can see a lot of different
name distributions that you know about
already written out in this form they'll
also give you what is the Phi of X what
is the G of theta for each of these so
that's actually a very nice thing so
things like Bernoulli multinomial
deliciously one notable name
distribution which does not belong to
the exponential family is this student's
t-distribution students T distribution
does not belong to the exponential
family the reason for it is exponential
family distributions always have very
very light tails they DK very fast okay
but students T has a heavy tail
distribution it does not decay very fast
okay so that's one of the limitations of
the exponential family but otherwise
there is a variety of distributions
which fall in the exponential family and
for all of them you can make these two
statements you can say that the log
partition function for all of them is
convex is moment generating and if you
do Emily with them you will get a unique
global solution and that global solution
will have the property that the expected
expectation of Phi of X will match the
empirical expectation these are the
properties of the exponential family for
every member of the exponential next
time we'll do emily m AP bezan estimates
conjugate priors will do a lot more with
them and we will also post the homework
hopefully by today i will post my pages
trying to get the data all in one place
and write do a write-up on what you
exactly needed ok questions wins in
solid go for lunch questions from Hydra
will open it early of the family's wish
list ok so that's a modeling question ok
so that's exactly where you know you as
a modular or a machine learner had to
figure out what what your Phi of X is
right so that's and that is also an
inherent issue in general with all
parametric families of all parametric
methods for estimating distributions is
you have to make an assumption you have
to make a modeling assumption that I
know the distribution family
so other questions okay so that caseless
yeah this equation sorry for this the
node should be posted on the homepage
for the second chapter so just look at
them maybe today morning or something
they would be posted so have a look at
other questions okay good let's break
for lunch each year microsoft research
helps hundreds of influential speakers
from around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>