<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bonsai: Reactive Tools for Data Science | Coder Coacher - Coaching Coders</title><meta content="Bonsai: Reactive Tools for Data Science - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bonsai: Reactive Tools for Data Science</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-pmHu-rsS-0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay thanks for coming everyone our
speaker today will begin Charlie Lopez
but first i'm going to ask adam kemp to
introduce himself and explain the the
context of the lab the set the which is
now for the signs what's the sensory
real explain their name at the lab and
everything and the move to london and
the sort of connections you'd like to
build with microsoft research well first
thanks to Dan and thanks for you guys
for coming how this happened was well
we've recently been tasked with moving
our research group to this new center
called the Sainsbury welcome center it's
part of UCL I don't have any sort of
this before but it's a research
institute for systems neuroscience that
it's a relatively undefined area of
neuroscience between stuff we do
understand at the level of neurons just
stuff that we weed lots to talk about at
the level of psychology it's essentially
everything in between which means it
could be anything this realization and
even in discussions over lunch is that
we have a lot of work to do in terms of
designing conceptual frameworks for how
we interpret neural data we found this
very fun but I think a lot of people
working in many different fields
including here Microsoft researchers
have discovered over lunch have a our
justice welcome to be part of this
conversation so I think at the beginning
of gonzales talk I just wanted to say my
name is Adam camp were starting I think
a very fun research group in London and
you should all feel more than welcome to
pester us because we will continue to
pester you for ideas about how to go
forward which is what I did a couple
weeks ago pestering Don and this is why
I Gonzalo is here so thank you and stay
in touch it's a privately funded new
Department of UCL specifically focused
so how that works I think is ideally the
best of both worlds and not the worst of
both we can independently do what we
want but the same time borrow from ucl's
infrastructure so it's the Lord
Sainsbury in the Wellcome Trust have
decided the brain should be understood
better than it currently is what exactly
they do they've left up to us
consolidated eyob
how we approach this problem to put it
into a broader context for me we are
specifically my lab is previously call
the intelligent systems lab and we we
are trying to figure out how cortex
works which is a the piece of the brain
which got very large in humans and
apparently is a quite relevant for
intelligence and in order to do that one
thing we realized in the context of the
many centuries of neuroscience
investigating what different parts of
the brains do if they used a behavioral
assays like an experimental environments
in which you can control the sensory
inputs and animal gets measure its
behavior outputs and look at what's
going on in the brain in between and to
do this they built environments that
were very well controlled where the
sensory inputs were restricted in the
possible behavioral expression as well
because that's all they could keep track
of they needed to have this control
otherwise I could interpret the signals
they saw it turns out cortex is not
required to do any of those behavioral
experiments which has been this awkward
dilemma that we've realized over the
last decade or so that the paradigms we
use in neuroscience actually don't
require the part of the brain we most
want to understand hence the desire for
starting a lab in which we're going to
need better tools for building more
complex paradigms that actually do
require this part of the brain that we'd
like to figure out and that's really
where you hire people with computer
science training and you task them with
make it easy for neuro scientists to
build much more complex interactions
between inputs and outputs to divine
experimental situations that require
neocortex and that's where it's all got
started I think he's on an amazing job
with this and in fact he'll discuss much
more about the use cases particularly in
neuroscience and I think towards the end
here I will be happy to answer questions
about the specific research we do but I
think it's worthwhile now spending some
time just talking about this framework
which has totally changed the lab the
way we do research in the lab as well as
this whole Institute now is actually
moving towards a bonsai based form of
the behavioral experiment so without
further ado thanks again to DOM to you
guys in dec examples I think so so can
everyone hear me okay yeah can you hear
me okay
yeah so it's actually real pleasure to
be here because I guess in the sense
this story is gross quite further back
in time and in a weird way all the
technologies you guys have been
developing here at Microsoft Research
and Microsoft elsewhere have quite the
quite deep embedding in what I'm going
to talk about so there's two parts to do
this so there's them what I'm going to
tell you about is essentially how we've
pushed these technologies to do the
kinds of assays that Adam was describing
that we want to do it require handling
these complex data streams and kind of
controlling them as well and then
there's a particular history to put it
into perspective because I could tell
you what bonds is for is essentially a
tool for the rapid prototyping of
systems that handle data streams but
that can be many things so to give you a
perspective on what I really mean I will
go through I want to show you a small
video of the kind of stuff I was doing
before i joined adams lab and so let me
show you that video just a second
so starting in starting in the late mid
to late 2000s I wasn't his company
called y dreams and they wanted
essentially to build build and explore
natural interfaces for humans
interacting with computers other than
using the mouse and keyboard essentially
and they wanted to have this touchless
natural interface with these digital
systems and I somehow ended up in this
research group where we were exploring
two different fundamental ways to do
this now I'm going to share with you so
essentially one of them is these physics
based simulations where you have virtual
elements and you sent that are running
in a simulation and somehow you find you
get humans inside that simulation in a
physically meaningful way so all these
little elements that are actually be
particle systems that can be whatever
you want and the humans themselves are
also an element if you realize here
they're all just we're still working
pretty much on the image plane so it's
all it to the kind of simulation
physics-based the human is inside we
were also interested in another paradigm
that is very popular now the augmented
reality where you take a virtual world
and you register it in 3d with the real
world and this isn't just an example of
how we were also exploring these kinds
of systems maybe you can't see very well
that was a story book for children and
this is just a playground that human
scale of essentially virtual avatar is
interacting with humans and at some
point to realize we could kind of bring
these two together and build 3d
registered physical based interactions
this is now using a 3d depth camera it
is sexually the Canasta which is now
connect to I guess so we were playing
with this back in 2010 building these
systems and once we realize
power of this really exploded and
started doing these experiments apply to
many different scenario so this is now
it also provided into our side projects
this is a like a dance performance where
particle systems kind of follow the
dancer around have different behaviors
this is still the same the exact same
principles another dance performance the
same principle actually same framework
it turns out in this company I was also
developing a framework for the principle
the framework are exactly the same it's
the idea is you have this physical
elements that somehow are influenced by
the human in a unified simulation that's
the that's idiot yeah I feel free to ask
questions by the way that's great it's
great here the dancers kind of
controlling the direction flight was
going and this was turned out to be so
general that we actually managed to
control robotic systems with this so
this is now again the same principle but
you realize we could if we can register
a simulation with the real world we can
actually use that simulation itself as a
mental model for a robot or now the
decisions that in this case is a swarm
of robots that we built our based upon
happenings of that are going on in the
simulation that is again the same exact
I idea this is just yeah robots this
isn't a bank we're a team of robots can
kind of serve as your virtual guide yeah
and then of course you can explode this
this is a interaction with an audience
it got it to cut them to play pong which
is smaller again back to the simple idea
but it's kind of finding ways to scale
it up
too real many different scenarios and
then now what this is turn into as I as
I've moved into this behavioral
neuroscience scenario that Adam was
talking about is this can actually bleed
through the way we build environments
for animals we want to move away from
the more restricted scenario of an
animal passively being presented with
stimuli to building an actual
environment that works in a
theologically meaningful way so that
turns out that also requires an
environment it is intuitive for the
animal to be in yeah so here the animal
has to collect these spots of lights and
order together every word okay so that's
kind of the trajectory and this is
really what what motivates so this is
these are the kinds of systems that I
wanted to make easy to make and it turns
out this is very useful for for
neuroscience as well and this kind of
like um just a panoply of different
different experimental setups people
have built with the framework that I'm
going to show so you tell you about
today so it goes M essentially used
primarily I guess still to follow
animals in real time as they move for
their environments but in many many
cases also to control the kind of
stimulus day you present this animal or
even to synchronize the that recording
of behavior with some measure of the
activity of the animal's brain in some
case but I think the best way now at
this point to really get you into this
is to to tell you about how what what
this thing really is I think that's
where I jump a bit into demo mode I'll
just show you so I will show you that
video already in in the bonsai framework
so this is this is what the framework
looks like it's the user interface it's
it's windows forums is great
but it's basically it works the
following way so you have I'll pull up
the welcome screen again and essentially
this is organized around the idea of
sources of data essentially data streams
that you want access to and really the
first problem that neuroscientists who
are not familiar with programming really
face is they want to interface with some
device in order to get the data that is
a mile produced by it and this can be
very complicated I mean if you ever ever
programmed the code to initialize and
kind of configure a camera to just pull
the images out it can be complicated
especially if you don't have an
engineering background but you just want
those camera frames or you just want the
data coming out of this amplifier or a
microphone or whatever but then once you
have so that's the first problem and
these are essentially what the data
sources you're colored in violet are
going to be represented so this firm
framework is based on them it's a visual
programming paradigm essentially and so
we have these data sources I was saying
and then you want to do something with
these data sources so you have a lot of
these transformation nodes that all to
kind of transform this data but then
what became most powerful is that okay
there are many it's easy to think of a
data processing system as just a
pipeline of data coming through but
really what happens in neuroscience very
quickly once the experiments get
complicated is that you're dealing with
a lot of heterogeneous devices so the
devices are not synchronized in any way
and they're working independently they
are made by different vendors by
different sit there are different
systems and you but you still want to
somehow combine their data together and
so the idea here this is where the the
idea of merging this with that we had to
kind of solve this problem at the
asynchronous level came came up and when
I was thinking about this is really when
I when I bumped into the reactive
extensions that was just coming out and
I kind of modeled the the
the problem around that so I'll tell you
outright that everything you've seen
here is essentially based on the on the
reactive extension so these data sources
I don't know if how many of you are
familiar with RX just curious k CB i'm
just kind of assuming everyone has been
and everything in a way no no no no but
I'm kind of assuming do you know about
it or should I feel like not not much at
all but two sodium you know about
essentially link is linked like a
familiar the the language integrated
query language in today that's okay yeah
so it's essentially the only thing that
the reactive extinctions have done is
essentially turn what were essentially
database queries into queries over data
streams so it's like you have a database
where you usually the database you
passively pull data from the database
and you kind of process it and the
reactive extensions you get data thrown
out at you and it turns out this is a
very convenient place for me to start
expressing this language and it's
basically what you're doing as you're
going as we're going to be building
these pipelines is that we'll be
building a query in RX and in Dominguez
in a quite literal sense meaning that
when I now I'm going to build a couple
of workflows with you let your data flow
diagrams it is absolutely do you you you
obviously look to them is it chose not
to use them so many many of them they
turns out they still don't work the a
synchronicity of these data streams are
still not a primary citizen this
framework so they mostly work on because
they do have a synchronous and parallel
programming but the way you wire up this
logic is still very much as a
synchronous flow of steps and what what
they do then like for example we have
lab view and simulink yes the way they
operate is the way the introduce
parallelism is you draw out a flow like
oh for processing that you want to do
and then they were going to analyze
syntactically the flow you've made and
then
they recognize point they automatically
infer for you points of parallelism and
you'll paralyze your processing that way
but then but it's not something that you
as a as the user of the language you're
not controlling at a fine level that
parallelism at listener or on most most
free on all the ones that I've dealt
with so far I've never seen it used is
that way yeah but you happy to get
feedback okay but I think it's easier to
just show how this feels like and there
are more more stuff that will become
hopefully obvious as we move along so
essentially what i did here what I was
showing in the video was add one of
these source nodes in this case is a
file capture that essentially provides
you a data stream into a file into a
movie file in this case and once once I
play so this node once I start playing
the the workflow essentially will make
all the work of opening up the file and
serving those frames that are in the
file and as an output and you can so
this is this window that shows up is
essentially a visualization into that
flow and one thing that will come out as
we will start playing with this data
stream is you can kind of visualize
everything that is going on in the
workflow which is something that is very
convenient if you're rapidly prototyping
these systems is you want to be able to
have a complicated pipeline you've just
created and essentially now inspect each
of the steps while it's while it's
running so let's add one of these steps
to kind of get a feel of what it what
this is about so now I've added a source
and now i will add a transformation to
the source and the first thing i want to
do is essentially just restrict you may
have noticed when the video there's
these dark bands here just want to get
rid of that so in a process is central
part of the video so I'll just add a
crop node
so now I'm going to crop the image and
here on the right hand side there's
properties you can you can use to kind
of parameterize the behavior of the node
so what I'm going to do now is I'm going
to open up the crop node so I'm
visualizing the data that is coming out
for the crop right now it's not doing
much but I will essentially use the
properties to define the region in the
image that I care about this case is
going to be this region so now the crop
is essentially showing up that region
only and now let's add in more stuff so
let's say let's try to extract some
information out of the out of this data
stream and one thing that is very
salient here is just the short of this
guy's very red so let's try and pull
that out so this is a possible piece of
information we might be interested in is
wear that shirt is in space and one way
you might do that is you okay let's come
up with let's say you split we split the
color so this is a color stream of color
images RGB it if you just split so the
split note that i just added is
essentially going to take all those
inputs input frames and give me now as
an output all the color channels that i
can pick here so one thing that is
important actually forgot forgot to to
explain this to you it's important each
of these data streams has an output type
because if you're if you're let's say if
you're accessing a camera or a file
you're going to get images with your for
accessing a microphone it's now in a
different kind of format or if you're
accessing your mouse or your keyboard as
we'll see later these are all different
data types if you right click on these
nodes these output tells you what the
type is so I'm essentially i'm going to
get so these are all just them a blue
green and red channels so i'm going to
take that red channel and if i play it
now so this is now the red channel of
the image
actually let's plot all of the channels
still read the green and the blue and
again stop me at any point if I'm not
being clear so these are all this is the
green blue okay so it turns out that an
easy way to kind of pull out that red
shirt is essentially to subtract the red
subtracted green channel from the red
channel sony one possible way there's
many so the way I'm going to do that is
now I have these two data streams the
red and the green string and have to
somehow bring them together and there's
operators that allow you to kind of
merge these streams together and they're
essentially all the reactive operators
so I'm going to use one that we make
some of you may be familiar with which
is zip I'm essentially going to zip this
data stream with this data stream and I
get a tuple of the two of the two pieces
of information in there and now that I
have this tuple I can just subtract one
from the other and essentially now I
have that kind of have a more signal to
noise a representation of that red shirt
yeah you can visualize so it turns out
it's there's not an image visualization
of that yet this is something that I I
will come back to this later is all the
system is extensible so I'm showing you
I didn't go into accessibility yet but
all of the components you're seeing here
are quite extensible so can not only
build your own data sources your own
transformations they can also build your
own visualizations you can pick from
different visualizations for example
here I'm showing this image as an image
visualizer but I can also show it as
text this is that that image just a text
representation of what that is we can
switch to an image representation yeah
but it could do other stuff with this or
whatever you wanted
so now let's complete just to our
extraction so I'm going to down
threshold issues a simple threshold to
really pull out make a category boundary
on what that sure it is so you can also
manipulate the parameters on lines kind
of see the effect that's somewhat decent
yeah it's good and now what we need to
do now is essentially define objects
from the image because what in the end
we want is the location of the of the
center of the red shirt for example so
I'm going to use there's a all the image
processing operations that I'm using
here are based on a computer vision
library called opencv some of you may
also know and essentially i'm going to
scan the image for regions of connected
components in the that are essentially
so the difference now is the what is up
with this operator is done is
essentially take the black and white
representation of the image and built up
a list of objects that are the connected
patches of white pixels and then I've
I've computed their statistical
properties in terms of the center of
mass the major and minor axes area and
so on its representation of that and
once we have that we can now let's say
we want the largest object because
that's essentially what we were after so
the most significant object inside there
it's a shirt it turns out another
feature is a bit you can be useful
sometimes you want to you may want to
see some of these some of these outputs
in context of some other outputs so when
one feature we've added in is the
ability to stack visualization so I can
stack that is
on top of this one and kind of represent
the what I'm what I'm doing in a context
of the original data you can also extend
that of course i means the we call the
mashups and then yeah okay so once i
have that i can pull up the center of
mass and add that to my stack of
visualizations and now have got a trace
of where the person is and of course i
can graph that so here the point of this
is the goal of the framework is not so
much on I mean people who are doing
computer vision this is like basics of
the basics like computer vision 101 but
the point here to emphasize is how we
wanted to make it easy for people to
essentially just get this basic things
up and running because it turns out
experimental neuroscience can already do
a lot of it with this if they have
access to it okay so that's for control
any questions so far if I move into the
city's kind of what I was showing here
is essentially data sources we can add
more so for example I could I could just
as well replace all the pipeline I was
processing here I could replace it with
a camera and have here a couple of them
it could presumably now use you and well
I guess we're tracking red object so
it's kind of biased but but you get the
idea i mean the idea is it should be
easy to exchange your source of data and
there's many more of these operators you
can play with and now we're going to
move next time we're going to move to
having effects we want these operators
to have effects on the real world but
before that there's any questions I'm
happy to to address them is it clear
ya know okay it's fine alright so let's
say now we wanted to have not just
processes data streams but actually
control them and for that i'm going to i
brought here a little setup we're going
to try and make work see how well it
works it's essentially an arduino that
mount camera on and maybe you can't see
it very well so I'll set up the second
camera okay get a good view of this
maybe this is enough let's try all right
okay so now i'm going to set up
essentially two things so one important
thing of the all of this being a
synchronous UK is you can naturally
introduce parallel data streams here so
now I've just simply set up to two data
sources there unconnected to each other
so you're just running in parallel so
one of them should be should be the view
of the camera let's try this one yeah so
that's the view or a little setup yeah
just make sure that lets people of both
cameras yeah this is the view from this
camera and I want to put view from
another camera yeah apparently isn't
like that all right
let's get to one and then let's get to
this one then and now what I'm going to
set up here is essentially I want to
move I'll be able to move these servos
in a way that will create some rules
that are interesting but first just to
check if this is all working you want to
set up here another data source so the
mouse if either the mouse source that
essentially gives me as I move my mouse
on the screen the coordinates of the
mouse I'm basically going to pull up the
X those x and y coordinates and use them
to drive another kind of node which we
call a sink so sink is now essentially a
side effect that is going to be caused
by the computations on the data stream
and the side effect can essentially be
your data going anywhere to put you to
have some kind of useful effect it can
go to a file on your hard drive you can
say video you can save data which is
what most scientists care about but also
you can have other external effects via
devices and one of them is the Arduino
so I'm going to be able to essentially
say send these two outputs to the server
so essentially going to be how familiar
are you with the Arduino also is another
question I should ask this is
essentially cheap microcontrollers that
are very becoming very popular to allow
people to just simply control some basic
electronic devices and the idea here is
you can expose to Banzai essentially the
layout of the pins in the Arduino and
you can set the state of each of these
electronic devices by sending commands
to the microcontroller and one of these
commands that you expose essentially the
controlling is pwm clocks that are
running in the india arduino and I kind
of define a protocol for the servo to
work and if you do that then if this
works so essentially now moving my mouse
and kind of controlling the pan until to
this camera which is okay okay so at
least the servers are working fine and
let's try now to do a challenge for that
I woodley wanted to get these to rank
let me unplug one camera and weeds
change ports
okay so that's one camera camera 12
right cool switchings four USB ports
works it's good all right so I have two
cameras working good so now what I want
to do is I want to use I want to trap
this object over here a little ball and
basically instruct the servos to keep
the ball stable the center of the of the
image so the idea to to do that is
essentially actually which one is which
this one this one all right so for that
we're going to use something similar to
the first principle I was showing you
before of segmenting color but now in a
slightly more sophisticated way and
basically so the images from the camera
communities RGB colour space and it
turns but it turns out you can do a more
robust segmentation of color if you work
in a different space which is hue
saturation value I don't want to go into
the details of how how this
transformation works right now but it's
essentially a different way to represent
color or you can isolate the hue and
saturation of the color you can isolate
that from changes in brightness it just
makes it more robust and then you can
threshold on that on that representation
which is what i'm going to do now so
this is representation in HSV space and
this will be my threshold and i'm just
going to find here where the ball is
really quickly it's good now i want you
to be saturated object it's good good
enough
that's my object so now once I again
once i have my black and white image i
can again detect the objects exactly as
i was doing before an isolate where that
where the ball is in the image okay so
now the idea is to simply use that to
drive the server and the algorithm is
quite simple but i'll use it as an
opportunity to illustrate another the
first layer of extensibility of a bonsai
is you can script directly in the
language this is turns out as your
coding up your own situation often comes
up because the power of this is that
there's a lot of operation operators
already built in but you kind of want to
do your own stuff so one simple simply
simplest way to do that is to just work
within a layer of scripting with rabbit
foot Python which actually because all
of this runs within the dotnet framework
is actually ironpython and you can
essentially yeah just define a single
function that will dictate how the value
that's coming in gets processed into a
an output and what we're going to do
here is essentially pick up our value
which is a position actually I need to
do that to make sure this works is I
need to from here I want to take the
largest component and I want to take the
center of mass and the exposition so we
were going to do this is we're going to
first take care of the x position
stabilize that and they don't stabilize
the y so now I'm going to add a
transform that now is going to work on
this floating point value this center of
mass of the object in the image and it's
going to turn that into a servo command
and the way we're going to do that is
we're going to compute an error term
it's just a basic negative feedback idea
and we're just going to subtract we want
to keep we essentially want the error to
be 0 when the object is at the center so
this is a vga image goes from 642 480 so
want to keep the object in the middle of
the image who we want if the if the
position of the point in X is 320 the
middle we want the error to be 0 so not
no command if it deviates one one way or
the other then we want it to be to do
something so now we take that now
there's a difficulty because these these
servos actually don't work with relative
commands you can tell them to just move
left and right you have to tell them to
go to a specific position so that I have
to do here translation step which e
which is I have to define a target which
is essentially where the servo is and I
essentially need to update that target
this is a global variable I'm just going
to set it up here and essentially update
the target with the error and I'm going
to normalize so the this is just a
normalization factor to account for the
fact that we're working in image
coordinates in pixels but I want to work
in degrees which is the units that the
servo understand so this is just a
scaling factor for soda small small step
size this doesn't blow up so the idea is
really I'm just computing an error and
appearing might is updating my desired
position based on that error so we can
actually visualize that before we do
anything to the servo so that's the
position of the server winx go of the
ball in X and that's actually maybe I
should show sorry before we go there let
me show you just the error
so the idea is now if I move the ball Oh
basically control so the ball is moving
but also is the error signal so and if
the ball is in the middle and the error
is close to zero if it's to the to the
right then the error is going to be
positive in the left is going to be
negative and the command is basically
going to reflect that so now if I just
send the target based on that to the
servo so so you guys can see what's
going on it zipit arrangement yeah
essentially the physical image
stabilizer this is just using negative
feedback but now it can't look up and
down so only the X so now to solve for x
should be similarly easy we basically
just take instead of taking the X we can
branch the output of this node and pick
the y coordinate and essentially we want
the same node so can just copy paste
from this script here and now the image
in Y is different so that's 240 is now
the center 240 and now let's see if this
works because I forget the sign I think
there's a sign flip here but we'll
discover that because the y-coordinate
is inverted like but let's let's find
that out yeah and maybe you're right
yeah it's right away I think I also
forgot to change the pin nice nicely put
didn't like that because didn't find any
object we could add a check here so that
if no objects are detected it should
take care of that but I think oops maybe
we should add a check so let's add a
check here the problem that is going on
is when no object is detected then the
position is not a number now let's just
add a check to make sure that this
doesn't happen so I'm going to define a
check by basically picking taking the
area of the object which if no object is
detected the area of the largest object
is zero and just check if that is
greater than zero then what I can do
once I have this boolean check I can
create a condition so this is another
type of node I didn't talk about yet
this is essentially is essentially a
where clause in a query it's basically a
filter for the for the data stream so
now when an output is coming here it
will be filtered by this condition so
only outputs that actually match the
condition will be allowed to go through
so now we shouldn't we shouldn't get any
more of these nan thingies and let's
kind of pick up something just yeah I
think I think it's kind of running away
from the ball so I'm going to shift I
think there's a city centers flips I
believe
slowly you can flip yeah let me reset it
first two so I won't update the target
so should be easy and I'll just reset it
to the stable position okay that's zero
it's good now the object is clearly
there let's try like that
yep
if you can see it from here actually you
should increase this guy sorry you can
see better what's going on yeah it's a
simple control system control systems
like this for quite a while but you're
you're so what was your kind unique
piece here little pieces into waiting
programming this and they've been able
to do that before perhaps it's that it's
the way in which technologies it's the
way in which you're given access to the
technology is though the fact that I can
just to gain access to a camera can just
tap camera and images are being spit out
immediately instantaneously and the fact
that if I want to do things in parallel
just add more of these pipelines and
it's more about the agility of building
this people have been building control
systems but maybe not in 10 minutes it's
the it's really because here that the
point is to be able to make this fast
because as an experimentalist there's a
lot of value in being able in an
afternoon to go for a bunch of different
solutions whereas if I have to take a
month to really try something out then
there's a decibel commitment and people
are kind of afraid is possible for a
long time it's quite remarkable nobody's
well these synchrony they have and they
have I mean there's lots of languages
even visual languages that allow you to
specify the skies of pipelines but to
use them and and to actually bring this
point across maybe this is not the best
way to do it but to put a synchronous of
these data streams in forefront and not
just to them that that's something that
it's much much rarer I have never seen
in it
system isn't perfect sorry this is a
place where we can see that they
synchronicity is at the forefront here
yeah well here's a good day to photogra
makes perfect logical sense yeah it
works in real time as very smooth yeah I
get you an example like we use this for
parties as well so I can maybe pull up
that example so yeah yeah let me show
you let me show you that so let's now
something that I an example that I
personally like so I'm gonna take this
camera there is now again not ah sorry I
can't really put two out let's take this
camera over here and had a threshold
operation so I didn't and actually I
lose I lose this opportunity to
introduce another concept so this is
just a thresholded version of of this of
this image but now what I want to do is
I want to so you see that these nodes
have properties here that you can kind
of manipulate on the user interface
online but work and one other thing you
can do is you can actually externalize
them into their own elements in the
workflow and you can write to them using
other streams so now let's pull up data
from the microphone so that's now
microphone data this is presumably me
speaking and i'm just going to build a
basic volume eater it's essentially just
a rectified version of this and let's
just scale that to something that so if
we she usually isn't good enough let's
see what the range of values is it's not
just a basic volley meter which makes
some sense I guess so let's feed that
into there so if this works
shouldn't here special is not so I get
into the sound yeah love you speak
special changes yeah another thing is
gently asynchronous thing that you were
saying was the important thing sorry you
you said something that that the key
thing here that is different to other
systems is that you're exposing a
synchronicity yes does this new example
show that no yeah yeah well okay okay we
can go over the it does show in a sense
because the updates to the threshold are
asynchronous to the video string right
so I'm updating the the threshold it's a
very simple one I grant you that but it
is it is an asynchronous update of the
of the parameter a more interesting
example that maybe you'll appreciate or
not is let's another thing that I that I
like is one interesting issue that I'll
give an example that people people use
quite a lot is when you're analyzing
let's say behavior or just some data
stream sometimes you have events that
are triggers into whatever data you are
pumping out and you what you want to do
is essentially to take a sniff it of
that data at the moment the event
happened and you may want to save like a
little window of data on aligned on that
event essentially for example and the
event itself may be asynchronous to what
you're doing so one way to do that and
this is I don't think we'll have time
maybe to explore this in too much detail
but one interesting way to work here is
to slice essentially the data stream
into little these little windows based
on some events so i'm going to use here
is an operator this is triggered window
where essentially i'm going to use a key
down event i'm gonna take a keyboard and
essentially take a key and use that key
so know what's going to happen he
is you take data stream and whenever the
key is pressed then you're going to
slice that data stream into a window
okay and now you have and each window is
essentially going to be also a sequence
of data that is going to last until the
next key press is going to happen so now
you have all these chunks now with these
chunks you can essentially do whatever
processing you want and essentially the
way we specify that and is essentially
through these nested notes so I didn't
talk about nesting yet but you can loop
computations into Internode's in this
case what I'm doing is this select many
node is essentially taking the each of
the windows that I've sliced and for
each window i'm gonna do a computation
so it's like it's like this workflow is
going to run for each of these windows
and let's say i want to i want to save
the video clip with that let's say let's
put it on yeah anything be here fine
it's greater videos click dot avi um
it's a to fix i'll count okay so
essentially what's going to happen and
let me bring you to that folder just so
you see this it becomes clear yes that
is aligned that with the event of key
press it starts David if you press yeah
you would see all these these things and
you could I mean it and then you have
all I mean the all the other examples
would use yeah so the other examples
would use would make use of all you
could use all of the reactive operators
as well to build more complicated
scenarios but the idea here is hi works
I what we're trying to do here is
essentially create a graphical language
or kind of express these kind of
complicated things in a compact way that
people with that are not like spend a
lot of time understand these
technologies could somehow into it at
and I mean I tried it to do use here
like simple examples but you could yeah
yeah any more questions yeah yeah we can
we can play that one for sure so it was
just the movies videos if I just play it
so now essentially yeah whenever I hit a
key then a new clip is going to start
and maybe I can even buy it here so that
I don't even know if this is the video
right yeah this is the one so that's the
key i'm pressing ok so now whenever I
had press the a key a couple times so
now if I stop that at least these last
few clips the first frame should be me
hitting the key it's blurry that's the
key press event essentially it should be
the same for all the other ones so they
all should start at key press
essentially yeah so we want up as a
sauce so all of when you draw these
things I presumably is a direct
correlation using the reactor exactly
exactly yeah if I could show a more
example like I could just show you how
you build one of these things which is
really quick it's like two minutes for
reacting further instruction exactly so
you're building a visual building a
query in the RX domain graphically it
actually turns out this is actually
compiled so then the way I do this is
actually I didn't talk about this at all
the code is generated so you're one of
the things that does seem to differ
conceptually bedside labview and others
is this is totally founded in unit r XY
efficiency
everyone is a very plain programmatic
yes textual programming models has been
quite a fine and then you've got the
visual everything of our arm exactly
labourers so it takes the rx1 ad and
basically builds rafiqul layer to that
answer any questions what sort of the
limitations do you have in terms of in
terms of expression yeah so every node
you put there is essentially an
observable so one of these are X like or
an innumerable if you want to think
about it and now what you're going to do
is all these nodes are operating on
these observables so they take as input
any number of observables and the output
one another observable so anything you
can express in that you can argue what
is the general generality of that but
you can keep extending it to whatever I
domain you want to or not I mean it's as
general as SQL I would say I mean we can
discuss what what is there but it says
general purpose so it's not tied into
any like video processing or audio
processing or whatever question so if
you're it does a static data from graph
right so what if you want these
structures a glass to change in my
program for them yeah so that's the most
interesting step in this then the way
the code is generated I didn't talk
about this at all but when you're
putting in these notes here you're
actually more technically you're
actually am your defining a build graph
so the this is actually going to do
cogeneration and actually you have
control over that so you can for each of
these nodes I can actually ignore you
can write models this will take me more
time to explain but you can actually
take you could take control over the
build and essentially do whatever you
want and specifically that and what
scenario that I find particularly
interesting for future applications is
it doesn't the co doesn't even need to
be technically run in c-sharp so I could
use to keep servable I don't know if you
know this interface is like like link
you can run queries it actually
in the database you can do things here
as well so i could take control over the
build and for example run code embedded
in the arduino for example yeah i could
push piece of code out to devices and
things like that because i have the
build graph in my in my hand and you
could have nodes that dynamically do
things like that my sisters yeah but you
have taught them underneath yeah can I
write and I have a shower c sharp F
sharp but of course I mean of course I
actually I was waiting for a while I
actually had to make it a say I toyed
with the idea of should I put fer
scripting or an hour Python scripting in
both probably yeah but a F sharp is them
I couldn't find that the time this was
like three years ago when I was making
these initial decisions I couldn't find
a good interpreter for F sharp that work
hosted inside the an application I don't
know if this has changed probably yeah
yeah there's a good trip for example as
a graphical of the environment but as
long as we hope the puzzle as renowned
as a machine Lonnie not yet web tool
Council there's quite a tendency to move
to use data frames the time series data
frames are as the intermediate sort of
format think it's possible I could be
incremental ones or so on but um yeah
where is I do you see at the end at some
of your things that were to cool off
they're here for example and I just
wonder if that is come up on your radar
that you know use it using using data
frames in our sensor that could be
income oh yeah this kind of intermediate
for presentations that you build within
language that there's rooms to be
deafening it's a it's a release not
using observables yeah exactly assistant
mark has around views decided friends
yeah wonderful I mean technically
technically i well know the answer is
not yet but one extension that I thought
it would be easy to do is just tie in
this because now it's observables
anywhere but could as easily be
innumerable said anywhere or any
combination of both of them or even
other things because but but the data
frames I haven't I haven't looked at yet
something to go yeah bye guys love it I
think Nick's off Alan fingers like that
just butter she wrote on for your fault
I didn't hey thanks room</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>