<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Capturing and simulating the interaction of light with the world around us. | Coder Coacher - Coaching Coders</title><meta content="Capturing and simulating the interaction of light with the world around us. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Capturing and simulating the interaction of light with the world around us.</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QpXyIbJsBw8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
start a pen so it's my great pleasure to
welcome when Sir Yakov here Wensley is
currently in the computer graphics group
in the eth zurich and it is PhD in 2013
at cornell university he has worked on a
lot of different things mainly in
computer graphics in realistic
appearance modeling in advance monte
carlo methods for rendering in fact you
see the author of a quite impressive
physical based renderer called me
toolbar and I'm really happy to have him
here and listen to what what you will be
talking about thank you so uh see if
this works ok so before I tell you about
my work I want to start off with a sort
of a bird's eye view of the whole field
of computer graphics so as I'm sure you
know the twist of graphics compared to
other types of cs subfields is that
we're dealing a lot with visual data and
often times that visual data comes from
a physical object and one thing that we
can do is to acquire this information
and then to fabricate a replica of the
object but another thing that we can
also do is to to process this
information so to transform it and to
display it and then turn it back into a
physical object and there are various
types of visual data and in this talk
I'll focus on what I think are the most
important three and those are raster
data so things like pictures or bitmaps
then at geometry information so triangle
meshes point clones and so on and then
there's also material data and that
characterizes the way in which an object
reacts when it's illuminated so let's
take a look at this pipeline that I
mentioned just before for these three
different types of visual information so
for raster data acquiring it is easy I
just take a photograph that usually
takes less than a second and then for
transformation thanks to a very fruitful
research over the last years we have a
very mature software market even
software that caters to non-experts that
can do
all sorts of transformations displaying
it is also very simple I need a computer
with a basic graphics card and the
screen and then fabrication if you want
to think about it that way means
printing an image and what's important
about this row is that all of these
devices are available quite cheaply and
so this this pipeline is just incredibly
common we don't even think of it as such
now let's look at geometry data here the
capture stage gets a lot more difficult
so acquiring this information takes
minutes to hours and there are a variety
of different devices that can do this
they range from structured light
scanners to a laser scanning depth
cameras multi-view reconstruction but
one serious issue of these types of
techniques is that materials they
interfere with the capture process and
so in this case for instance we had to
apply some drugs to the owl to be able
to get a decent reconstruction and
that's actually kind of tricky to remove
afterwards and then also these devices
generally just provide the geometry and
at the very most they provide a crude
approximation of the material such as
the overall diffuse color there's a also
big software market based on the field
of geometry processing but this software
is now much more complex and requires
years of years of training to be
proficient and for displaying this type
of information now we need a fancier
graphics card with 3d acceleration
fabrication has really gone through the
roof in the last year's thanks to a big
progress on this 3d printing technology
so here are just a few pictures I found
after googling 3d printed bunny and
those are just silly examples but the
point I'm trying to make is that the
moment that you have reasonably
reasonably cheap devices that can kind
of give you this end-to-end pipeline and
the software market and it allows people
to be very creative the example on the
bottom left is the bunny sprinkler that
choose out what are threads eyes so on a
more serious note oops
sorry about that on a more serious note
this kind of technology can also really
improve people's lives so for instance
to design custom prosthetics for for
patients now let's look at material data
so to capture the appearance of an
object basically we have to illuminate
it from many different directions and
then also look at the way it reflects
light as seen from different directions
and that's normally done with a machine
called a spherical gantry so this is a
very specialized device it's not
something that you can order in a
catalog usually you have to build it and
it's extremely expensive scanning this
information takes much longer now so
definitely on the order of days to it to
multiple days the moment that we oh and
I should also say these machines usually
because the measurement takes so long
you have to install them in a room which
has a dampening from vibrations and it's
also painted black to minimize the
effects of the environment so it's a
fairly laborious measurement setup and
then to take this visual data and to
turn it back into an image we have to do
a light transport simulation and that's
what's referred to as physically based
rendering and the computational costs of
this step are outrageous and something
ll get back to later and then
transformation and fabrication is where
it gets really sketchy this is sort of
the Wild West of visual information
processing pipelines so there are some
initial research but it's I would say
it's fairly crude so for material
manipulation for instance the state of
the art is that you have a model with a
lot of parameters and then you can tweak
some of them and you get a quick
feedback and then people are generally
pretty happy with that but this is very
alien to a person who works with
materials on a daily basis to think of a
craftsman or somebody working in the
fashion industry it's just not the way
they think about materials so if i
wanted to summarize my research goal in
one sentence then it's basically to make
this last information processing
pipeline work better on the order of the
previous two
I've been working on all aspects of that
and in this talk I'll mostly focus on
the display part so physically based
rendering and I'll also briefly talk
about acquisition and transformation but
no fabrication so just a bit more
motivation why do we care about
materials and correctly modeling the
physics of light transport one reason is
that the pictures you get out of it look
staggering and and and realistic and
with careful modeling we can make
something that's really
indistinguishable from photographs and
that's for instance why the movie
industry is interested in them and for
predictive rendering and industrial
design the goal is to know exactly what
something is going to look like before
we make it because doing so may be quite
expensive and for architecture it's
pretty much the same idea of just at a
larger scale and then for a cultural
heritage preservation the idea is that
and we want to completely capture the
visual data of an object because it's
fragile and it may not survive to the
coming generations and so all of these
applications share an extremely high
demand on the realism which requires
accurate modeling on the material level
so I mentioned before that rendering is
expensive and just to truly communicate
the costs of this step to you I wanted
to share a example from the movie
industry from the movie gravity so Tim
Weber who was the visual effects
supervisor on this show I said that if
they had just had a single machine to do
the rendering and they would have needed
to start at the dawn of the Egyptian
civilization to finish rendering frames
in time for the release of the movie now
if you're making a movie that will gross
five hundred million dollars then
obviously that's not what you do you
take some portion of that money and you
go buy a big a big cluster and and short
and time that way and but this also
means that this sort of technology is
basically unavailable to most people so
to understand why these computations are
so expensive I'll briefly recap the
mathematical problem that they solve so
here's a picture of a cornell box net
sort of a standard seen that people used
to reason about light transport and
let's suppose that we're trying to
compute the color the pixel color of
this black dot which is moving
and forth on the ceiling here well on
the right side you can see the
corresponding computation and
essentially what we have to do here is
to compute an integral of the incident
illumination as seen by the black dot
and a material term so this is what kind
of characterizes the way that this
surface here at the top reflects light
now this is a metal surface and what you
can see is happening here is that it
selects out a narrow portion of the
illumination which leads to this
concentrated reflection and it's also
dependent from which position i look at
this point and that's why it's also
changing here so normally the way that
we solve these kinds of integrals is
Monte Carlo integration and so the idea
is that we take this integral and
instead of evaluating the whole thing we
just replace it by one or more samples
so we pick a point somewhere in the
domain evaluate all the terms multiply
them together and then we just take that
as an approximation of the integral
let's let's say we just do one sample
part of the problem here is that we
don't actually know what the incident
illumination is yet so we can really
just sample this material term and so
let's let's do that let's put a sample
there then we also have to evaluate the
unknown incident illumination at the
same point and so how do we do that well
this this point here is actually a
direction as seen from this point and
it's looking at another position in the
scene and so tracing array into that
direction we can we can find where that
position is and then we'll have to solve
yet another integral as seen from the
perspective of the blue dot and so as
you can see this leads to a recursive
algorithm and and that's also what makes
it challenging to be able to terminate
here I'm going to need to find a way
away eventually to make it all the way
from the light source to F all the way
from the camera to a light source
through some number of intermediate
bounces and for challenging input scenes
this sampling problem becomes very badly
behaved so um ultimately what these
methods do is to construct the
trajectory of a light path segment after
which is actually a lot like they happen
in in physics and I'll refer to that as
the path tracing paradigm and there are
two main ingredients to build a working
system on this paradigm the first one is
the rendering algorithm and that's the
part which is responsible for computing
integrals and it reasons about light
transport and the second one is the
sorry is the physical model and this is
all about the material that things are
made of and progress in both of these
fields has been rapid but mostly
separate from each other so people
usually choose to work on one area or
the other but not both of them at the
same time and this is very much visible
in the standard pipeline that people use
for physically based rendering so we
have black boxes for the rendering
algorithm which is called the integrator
here and then for the materials and the
geometry and these communicate with each
other through standardized operations
such as finding intersections against
the scene geometry querying the material
models and then more recently it's also
become popular to use markov chain monte
carlo together with this then we have
yet another black box that controls
these other black boxes so if we put on
our C has c/s head then this is actually
a really nice architecture because we
have all of these well chosen
abstractions and then we can for
instance remove one of the black boxes
and remove and replace it with another
one and as long as it satisfies the
contract then we still have a working
system so it's that looking pretty good
from from that point of view but the
issue is that there are many situations
were complex interactions between these
different parts lead to poor performance
and as a field graphics has almost
codified this particular set of
abstractions with all its problems and
people rarely touch them because it's
just such a big work amount of work to
it to make substantial changes here so
the issues that people normally get when
when there's a lack of convergence are
noisy and unconverted renderings and I'm
showing examples for a few different
methods here and those images look
pretty bad a single frames but would you
have to realize is that this noise is
different in every frame so it's much
more distracting
and all of these methods because they
they ultimately rely on the path racing
paradigm they perform a random and brute
force simulation of light path and now
the funny thing is that nature actually
uses the same algorithm so on a sunny
day about two times 10 to the power of
33 photons hit every square meter of the
earth and bounce around happily and on
the other hand the fastest computer
systems can do only a few hundred
million raise per second so we are
really not going to win this if we'll go
the way of brute force computation and
so you know you how this traditional
architecture is ultimately designed for
this brute force approach and so you can
think of my work in this area as opening
up some of those black boxes and seeing
if there's something that we can change
about that and more specifically I'm
interested in developing models that can
describe complex optical phenomena to
give them the necessary smarts about the
problems they have to solve and i also
want to understand the interdependencies
between different parts of this pipeline
and to find more suitable abstractions
that avoid these low performance
situations and so I'll cover several
projects that fit into this rough
pattern I just described starting with
work on appearance modeling and then
moving on to the rendering algorithm
side and the apparent models particular
the project the first to describe fairly
specific physical effects that were
essentially unrendered well with
state-of-the-art methods in the sense
that you may have to wait a month or a
year to get even a single picture and so
the focus here is very much on
performance and the first topic I'll
cover is a specialized reflectance model
for rendering glitter glitter is a
phenomenon that characterizes many
materials in our world and when we look
at them under the Sun or a similar
source of directional illumination we
see many small glints instead of a
smooth highlight and what's interesting
about these materials is their behavior
when we illuminate or observe them from
different directions so here's an
example from the very common material
that we don't normally think of as being
particularly a glittery it's a injection
molded power adapter like I'm sure many
of you have probably at least five at
home
and as the diet source moves what you
can see is happening is that parts of
the surface light up and then they stay
bright for a few frames and then the the
highlight disappears again it seems to
happen in an almost random way now it
turns out that most rough materials
actually look like this when we
illuminate them with a small light
source like the Sun and the reason for
this effect is surface structure and
that that was real uh-huh yeah you can
do it on your own also it just take a
small light source and one of these
power adapters they have this grainy
surface and so you just need a small
light source and then they look
literally like that or take them outside
so so the reason for this effect is a
surface structure and that structure is
usually classified into three different
scales we have the macro scale that's
sort of the size that of things that we
reason about in our daily lives then
there's the meso scale and that's where
small surface variations and scratches
are found and then there's the micro
scale and that scale is so small that we
can't usually resolve things anymore
even though they may still have an
effect on the appearance now in graphics
we have very good tools for dealing with
the two extremes in the end that's the
purpose of geometric models and the
reflectance models but the major scale
is fascinating because this is where
materials and the geometry meet and we
can still see the effects of this with
our eyes and this is where glitter
occurs so how can we visualize things at
this scale suppose this is a tiny piece
of surface that's visible through just a
single pizza and our task is to compute
the brightness of that pixel essentially
what we have to find out is how much of
the surface is oriented in just the
right way so that it reflects light from
the light source for instance the Sun to
the camera so how do we do that well
it's going to depend on where the Sun
and the camera are and in reality of
course those are much farther away from
the surface then I'm showing here so one
option would be to use this path tracing
paradigm that I mentioned before and so
to do that we
basically be shooting many rays from the
camera and then we just count how many
of them end up at the Sun but the issue
with this is that it's a terrible
sampling strategy the probability to hit
the Sun under these conditions is about
10 to the power of minus 5 so to really
have a good estimate a reliable account
in the end we'd need to trace millions
of rays per pixel so this is not really
going to fly as a as a rendering
technique on the other hand we can think
about the geometrics situation here a
bit more so in I showed you the surface
where the Sun and the camera we're at a
45-degree inclination with respect to
the surface and under this situation I
think you can convince yourself that the
only places where light can actually
reflect are the ones where the surface
normal points straight up and and and so
the techniques that I'll mention now are
based on this concept that we can build
a sort of a database of where spatially
and directionally things are pointing on
a surface and to query that database to
prune down this big search space and
that we don't have to rely on this brute
force approach so we built two different
methods the first one is procedural and
it creates plausible clear effects using
a statistical model of microscale
roughness and the advantage of that is
that it's very fast and requires almost
no memory and the second one is
data-driven and that can be used to
visualize structure that has been
measured or computed in a simulation so
here's a video from the first model
which creates plausible structure and
this is a true pair of shoes illuminated
from the left side and what you can see
is that we recreate this effect of
glints appearing persisting for a few
frames and then disappearing again and
here's another example where we show the
effects of the parameters of the model
to achieve for instance a different
spread of the highlight
the data driven model is slower but it
can be used to render more interesting
effects such as cutlery with scratches
and dents and we can also animate this
major scale structure for instance based
on the properties of waves to create
realistic ocean views and what you have
to keep in mind here is that the
underlying geometry is just a plane so
everything you see here is happening
inside the shading model and just to
give you some idea of how long these
images take to render here's a metallic
test object rendered once with our
technique and then with a brute force
method and both images are noisy but the
noise on the left those are the glints
the actual thing that we wanted to
render and the noise on the right is an
performance it's a convergent artifact
of the brute force method now using the
order of convergence of these methods we
can estimate that'll take about two
years for the image on the right to
converge to the one on the left and
that's just for a single image and not
in an animation and so this kind of
database lookup approach is one way that
we can visualize certain effects such as
glitter but now I'll switch gears a bit
and tell you about another one and for
materials that consists of multiple
layers and this was published recently
at siggraph and the description of this
project really begins with a personal
anecdote um so in my spare time I
developed this mitsuba rendering system
which is basically a simulation
environment to compute realistic images
and at some point during my PhD I just
became really unhappy with the material
system in it so the thing that the part
that's responsible for describing the
reaction of objects to light and I I
just wanted to do to redo it from
scratch and so I implemented first the
really basic materials which are things
that conduct electricity and those that
don't and so you can use the ones on the
left to simulate metals and those for
non metals like glass water air etc and
I also made smooth and rough versions
and the smooth ones are called specular
that's a term I'll get back to later
again and
also had a basic version of a diffuse
material and then the next thing I
really wanted to add were plastic like
materials and those are characterized by
having both a diffuse reflection and
then also the reflection component from
a dielectric so how do we get that
normally or very often what people do is
that they make a blend so they they say
for instance let's take sixty percent of
the dielectric reflection and forty
percent of the diffuse and then we'll
just add them but that doesn't even make
sense you can't just add these models
like you would mix two two buckets of
paint so a more sensible way is to
devise a structural model of a material
that will give you this type of
reflectance behavior and then work out
on a piece of paper or so what what's
the resulting equation that we have to
simulate and so here's an example here
we have a diffuse base layer and then we
put a coating on top and so that will
give us this kind of plastic like
appearance so then because let's see
what happens when we shine lighted it
well a portion is going to get
specularly reflected and then another
portion gets refracted into this lacquer
layer and it gets diffusely reflected at
the bottom but now something also
happens that we didn't originally plan
for which is that as light reflects back
up from the bottom layer it can actually
happen that it doesn't escape
immediately but it gets reflected back
down and interacts with the diffuse
surface at the bottom once more and this
is actually what happens to over fifty
percent of the light that gets reflected
back up here so it's if it's a fairly
important effect now what we have to do
is to integrate over all possible ways
that light could take and that'll give
us some formula that we can plug into
our rendering system and it looks fairly
complicated here but it turns out that
when things are so simple I have these
perfectly flat layers perfectly diffuse
bottom layer perfectly smooth dielectric
coding then you can just do it on a
piece of paper and work out the equation
yourself and the effects of that are
interesting so
if you've ever put a shiny varnish on a
material like wood you'll have noticed
that it looks darker and more saturated
afterwards and the reason for this
effect is exactly what I showed you on
the on the slide before that light can
interact multiple times with the base
layer at the bottom which increases the
amount of absorption and if we simulate
materials using layers then we can
recreate these effects you can see that
on the right here so when I saw that I
got really excited and I wanted to put
layers on everything also on metals and
other types of materials in missoula but
the problem is it doesn't work because
the you can really just get an analytic
solution for the very very very simplest
case and so this next project was born
out of this search for a nicer solution
to this problem so this is basically a
system to arbitrarily layer anything on
top of anything and compute things while
accurately accounting for all the
internal reflections that happen inside
the material this is a sort of a
high-level overview of the system so the
user is presented with a graphical
interface to design stacks of layers and
to modify their properties and that
produces a structural description that's
handed in to a pre-processing phase that
brights a file with coefficient that's
read by a rendering algorithm and that
makes an image which is shown inside the
user interface and so on so Allah I will
start by explaining this portion here
that does sort of the the main
computation and and to motivated I'll
start with something much older and
that's glass plates theory from Stokes
and I'll just discuss the version with
one class played two to motivate what's
happening inside this algorithm so
suppose we have a flat glass plate and
we illuminate it from the top then some
portion of the light will get reflected
some portion gets refracted and then
like we saw before there's another
portion that kind of gets trapped inside
the layer and it just bounces back and
forth and after every bounce a little
bit can escape at the top and a little
bit can escape at the bottom now let's
suppose that we wanted to know more
overall what's happening here so let's
say we want to know the total amount of
light that leaves on top so we have to
add up
of these arrows basically and then the
total amount of light that makes it out
at the bottom well to do that we have to
yet we have to basically compute the sum
and so suppose that after one reflection
event the amount that gets reflected is
called our and then the amount that gets
transmitted is called T then to compute
this arrow well that's just our and to
compute the next arrow it's t for a
transmission then our for reflection and
then t for another transmission so if we
add up all of those arrows in this way
then what you can see is that we get a
geometric series and that's really nice
because whenever we get a geometric
series we can just directly write on an
explicit answer without having to do the
sum so even though what's happening here
seems kind of daunting we can actually
characterize it in a very satisfactory
way now with that example in mind let's
generalize a little bit more so before
we had a glass plate and we illuminated
it from the single direction and then we
had an R and a TI coefficient that kind
of describe what's happening now suppose
we have a thicker layer and instead of
illuminating it just from one direction
we are illuminating it from many
directions and because we want to
compute things will will discretize that
that incident illumination into some
vector of measurements and that layer
now it doesn't have to be made out of
class anymore can really be anything
let's think of it just as a black box
that's hit by some light now when when
light hits it then some portion is going
to be reflected and some portion will be
refracted we are transmitted and this is
ultimately a linear process and so what
it means is that we can describe
everything that's happening here as a
matrix vector multiplication so the
example the difference between what we
had at the bottom here is that we're
dealing with scalars and then at the
bottom we're dealing with matrices and
now we can ask a very similar question
which is if we have two layers and we
can fully describe what's happening
inside them using matrices and then we
just cannot pack them on top of each
other are there are also some other
matrices that describe the combination
of these layers with all the internal
scattering that's happening inside
between them and the answer is yes
and the equations look a little bit more
complicated but if we compare them to
what we had just now you can see that
it's actually kind of the same thing so
instead of a scalar division we have
matrix inversion the constant one gets
replaced by the identity matrix and so
on and then some things didn't quite
simplify as much because we're dealing
with non-commutative matrices but
basically yeah those those are the same
equations and they give us this very
powerful operation to combine the
matrices of any two layers and then
figure out what the combination of them
does and this is basically what the main
algorithm of this method does so it
takes a structural description of a
layer and then just walks down from top
to bottom and applies these equations
over and over again so this is the core
operation in an inner system that
assembles these layered materials and
these equations have been known for a
long time so we don't invent them but
they're usually not practical for these
kinds of applications because the
matrices get way too big too fast and
our contribution here is a frequency
space representation that makes it
possible to do all of this linear
algebra in a sparse way and to keep it
efficient and then we also show how to
take a variety of standard models that
people are used to in computer graphics
and to project them into into this basis
so that they kind of fit into this
funnel so there's lots of fun math in
all of these boxes but it's a little bit
too low level for this type of talk and
so I'll move on to the next topic now
for rendering what we have to do is to
satisfy the contract of one of these
boxes and for this particular
representation I alluded to and it turns
out that we get a lot of mileage from
SIMD and particular interest AVX
instruction set but otherwise the
rendering algorithm is is fairly
standard and then after rendering the
resulting image gets passed back into
the layer design interface and so here's
a short demo of what that looks like the
user starts by adjusting the roughness
of the top layer of a coded gold dragon
and then add some scattering particles
into this coating and then after a pre
computation and a brief rendering step
the effect of this change becomes
visible
and now I'll show some example videos of
the same rough gold dragon with the
clear coating and what I'll do is I'll
have two layers the coating and the gold
and both of these have a roughness
parameter and and so now what we can do
is to tweak these roughness parameters
in independently of another and so
here's what that looks like on the left
is the combination of those two layers
and on the right other layers rendered
individually and here you can also see a
structural description of what's
happening here so now you can see that
the bottom layer is smooth and I'm
changing the roughness of the top layer
so there are some highlights here on the
on the specular object that are
basically being added or removed here
but then what's also happening is that
the refractive coding blurs out the
reflection from the metal underneath now
let's do it the other way I'll have the
top coating be smooth and then I'll
change the roughness of the of the
bottom layer and now as you can see the
effect is much more dramatic and what's
happening here is that that the change
the it becomes magnified by the
refractive coating that's why it looks
so exaggerated and then when the bottom
is rough and we tweak the top roughness
the effects are much more subtle but
still visible and I'm excited about this
method because it allows us to explore
these types of interactions between
different layers that previously just
weren't feasible before in the sense
that you know you may have to wait a
week month or a year depending on the
parameters to render your image so in
practice people just never use these
kinds of layer combinations and this for
reference they took about eight minutes
per frame now I don't have time to go
into much detail but one thing we also
did was to investigate real-world
layered structures focusing on enamel
samples in jewellery design so we
manufactured a set of enamel samples you
can see one here and then put it into a
robotic country where we could move the
light source and then take lots of
pictures of the object at different
light source positions and that gave us
some measurements those black dots and
then we fit a low parameter version of
our model with with just three parameter
to it and then we manufactured a flower
art piece by 3d printing a wax model and
casting it into gold and then we coated
it with the same sets of enamels that we
had previously measured and that made it
possible to compare photographs and
renderings of the final object and here
you can see what that looks like so in
this particular object there's a color
shift depending on the thickness and
well the physical object has a sort of
handmade quality because it was actually
made by a traditional Goldsmith who
applied the animals with a brush so the
estimates on the thickness of some of
these are a little bit sketchy but
overall I think that we were able to
capture the salient effects here so seen
from a high level the layers of this
project and then the meso scale from the
last project those were geometric
properties and all parts of this
combination of black boxes had to work
hard together to solve the problem in a
brute force way and with our
modifications what's basically happening
is that they become the full
responsibility of the material system
and then we also already solve a part of
the integration problem in there which
makes it easier the last apparent model
I'll discuss is about rendering
volumetric materials such as cloth and
this developed over sequence of siggraph
papers starting out first with a general
framework for volumetric light transport
and then later more specific
applications to cloth rendering and the
motivation for this project was that
there are just many objects in our world
that reflect light in an interesting way
as a consequence of their internal
structure so light enters these
materials at some point leaves somewhere
completely different and the path that
it took depends on the geometric layout
of what's inside and these materials are
also in a sense fuzzy in that there is
no clearly defined surface now in
computer graphics if you wanted to
render something like that you'd use
normally a standard framework that is
known as volume light transport and in
this framework there is a density
function at every point in in 3d space
that roughly tells you how much stuff
there is and that works great if you
want to simulate steam or smoke or the
atmosphere of a
our body and Atmospheric objects but it
really doesn't work for these types of
materials that I just showed you and the
reason goes back to the 1940s where
these equations were derived in the
radiative transfer community and back
then people introduced some critical
assumptions to make the equations as
simple as possible and so in particular
they assume that the medium is invariant
under rotation but this is obviously not
true as soon as the structure of the
material is oriented in some coherent
way so suppose we are dealing with this
medium that's made out of ellipsoids
that point primarily in the upwards
direction and I'm visualizing
evaluations of the extinction
coefficient and that quantifies how much
light scattering there is and you can
see that clearly it's much more for
light that's coming from the left right
versus light that's coming from top to
bottom so this is a medium that violates
this assumption and so our work in this
area was to develop a new mathematical
foundation for light transport that
still works for these types of materials
and then we focus particular on media
containing fibers so I'll show you one
of the first visual results we got using
this theory and this is a scarf created
from a physical simulation by keller at
all and with a classical theory we could
only ever render diffuse looking fibers
but in the real world fibers reflect
more light into some directions than
into others and there are also chemical
processes such as Mercer ization to
further increase the shininess of fibers
and in this image we can capture these
sorts of effects after generalizing the
underlying theory and this result got us
excited and we wanted to test our theory
on other types of oriented structures
but a big impediment then was just how
can we get good data to feed into our
algorithms later we send tiny pieces of
cloth to the CT scanning facility of the
of UT Austin and they inquire they
acquired this incredibly high res 3d
scans for us using micro CT technology
so this is an example of velvet and then
the second one is for satin so once
we've scanned the particular cloth
sample it's possible to synthesize more
of it procedurally in a computer
simulation and use it as an input to our
method
and in this way we can reproduce the
appearance and rendering that convey
both the character and the detailed
structure of different we've types and
what's interesting and that was sort of
one of the main points of this project
is that we can scan different we've
types and then the typical appearance of
these materials just emerges
automatically so we no longer have to
develop a specific model for this type
of wave pattern for this type of wave
pattern we just scan it and and then get
the result we we hope for so this is an
example from from the satin this is for
wool gabardine and then this is for
velvet and we get this typical brim
highlight that well that has and
following this we began working with the
Rhode Island School of Design and
developed a method that could take as
input a weaving pattern for an
industrial loom and they were really
interested in that because they have
they actually have an industrial loom
and they have lots of students who want
to use it so especially as the semester
ends there's a long queue of people
waiting to produce their their results
and so if they had a sort of a preview
then maybe they could reduce the number
of false yeah mistakes so here's an
example of that for a jacquard brocade
alright so so far I've spoken mainly
about appearance models and I've focused
on specific materials in isolation but
materials also interact with each other
and that's an interesting and
challenging problem on its own so in the
last part of the talk I'll focus on this
aspect and the project i'll discuss was
published at siggraph in 2012 and it's
also the topic of my PhD thesis and the
observation of this project is that
whenever we have a light transport
simulation then geometry and materials
are highly related to each other and
they give rise to another high
dimensional geometry that I'll explain
now and the problem we try to tackle
here is how to find specular light paths
so those are light paths which go from a
light source to a camera and then they
involve one or more spit specular
interactions so an example of that would
be a reflection in a mirror or a
refraction in a water or glass surface
now the problem here is easy to see when
we want to render this kind of a
swimming pool scene so we can shoot a
ray from the light source that will
refract into the pool and then it'll
bounce around and eventually leave but
the problem is that to construct a
complete light path we have to exactly
hit the aperture of the camera or there
the eye here in this case which has
extremely extremely low probability we
can also do it from the other way so we
can trace from the camera side but then
as we saw before the probability of
randomly hitting the Sun is just low
it's a very bad sampling strategy we can
also try to come from both sides but the
issue is that we really want those to
meet in a single point so that doesn't
work either so this type of scene is
surprisingly difficult to render if you
want to get unbiased results and to
understand what's making this so hard we
can analyze it on a bit higher level so
let's look at this very simple scene in
flatland and we have a light source a
sensor and then there's a mirror at the
bottom and I'm showing one example light
path and because everything is so simple
in vertically here let's also forget
just the Y coordinates and then we just
parameterize that vertex by the
x-coordinate on the camera
then on the sensor and then on the
mirror so now for that to be a valid
specular reflection we know that the
inclinations have to match the theta one
has to be equal to theta 2 that's just
the law of specular reflection and with
our particular parameterization of the
light path we can also write that in
another way which is to say that X 2 has
to be exactly in between x1 and x3 so so
far that's quite tangible but let's take
a step back and think about what's
happening here in a more abstract way
and we have a light path with three
parameters x1 x2 and x3 so we can also
think of it as a point in a
three-dimensional space and then this
three-dimensional space is the space of
all possible trajectories that light
could take with one bonds so with that
in mind let's look at not a second time
at this equation and you can see that
it's actually a plane equation so what
it's saying is that those light paths
that we care about and which satisfy the
physical laws they all lie on a plane
which is kind of passing through this
space and so that's kind of interesting
more generally we can observe that
actually whenever we have specular
reflection or refraction the subset of
paths that we care about that satisfy
the physical laws they lie on a on a
lower dimensional sub set so we can
formalize this a bit so for both
reflection and refraction we can write
down a constraint which captures what it
means for a scattering event to satisfy
the corresponding physical law and so on
a life path with vertices X 1 X 2 and so
on that are always involves three
adjacent vertices and then generally if
you have light that goes into a glass of
water and then out on the other side
we'll have a few of those constraints
and so we can stack them up all together
into a function see that's kind of the
combination of all these constraints and
and then we want to basically set that
to zero and get what's left over and so
normally this is what we call the
specular and manifold and normally when
you do monte carlo rendering you just
throw darts on this manifold which can
be pretty inefficient in some cases and
what we want to do is to do
look a rendering algorithm that can
parameterize it and move around so we
really built into this at this geometric
knowledge into the rendering algorithm
and so I'll just show one brief example
here for a glass sphere and the mirror
reflection at the bottom so we can take
this function see that I mentioned
before and differentiate it and then we
get the Jacobian so i won't go much into
detail but basically what it tells us is
what happens with all the other vertices
when we wiggle around one of the
endpoints to first order so it's just an
approximation tells us kind of how to
keep that path in a valid configuration
and to make it usable we also combine it
with the second operation that's a
projection onto this manifold and so by
combining both of those two we get an
algorithm that looks a lot like Newton's
method it takes linear extrapolation and
production steps on this manifold and
then if we put those together and we can
modify this light path and it remains in
a valid configuration and here's just a
short video of what that looks like in
practice in an actual algorithm so we
still search with brute force for light
paths in these little squares but then
you can see that bigger portions of the
image suddenly light up they become
brighter because we were able to explore
the neighborhood of these light paths so
we kind of amortize the cost of this
brute force search and it works also for
rough and dielectrics but I don't have
time to go into that so this was really
a new operation and and that has been
adopted by the community and so there
are a number of papers now that use it I
think two or three more also this year
and I'll skip over the next project
because we're a little short on time
okay so I've presented several
techniques that basically changed some
of the abstractions of the rendering
pipeline to make realistic rendering
more efficient and the last change was
the most extreme because we created this
new tool of the specular manifold that
kind of sandwiches itself between all of
the other components so this is the
direction where I see a lot of promise
in the future for for solving some of
the fundamental problems that
still exist for this architecture so
returning from the pipeline from the
beginning it's clear that much remains
to be done so my dream is that at some
point we'll be able to capture not just
the shape but really the appearance of
objects easily in a few minutes and to
3d print them to create physical objects
again and I will also want to work on
these transformation tools to make them
more intuitive to people who are really
used to working with materials in their
daily lives so before i finish i want to
mention the me tube around our project
so this is a piece of software i wrote
during my PhD and it's a plug-in based
system so the idea is that you know
these rendering systems are very big and
so it's an impediment when you want to
work as a researcher in this area that
you have to build this giant system
before and so here with me tuba you can
just work on one plug-in and then the
burden becomes lower and it also has
implementations of many state-of-the-art
techniques which is convenient when you
want to compare against existing methods
I'd like to acknowledge on my co-authors
without whom this work wouldn't have
been possible and I also want to thank
my wife will easier who made many of the
3d models in this presentation and that
concludes my talk thank you so you were
talking about the sequence of total
internal reflections hmm the resulting
light that comes out comes out of the
physical object at a different place
that's right it does it does you want to
love that No so in this particular model
we assume that the layers are thin
compared to the scale of things we're
looking at so that we can remove that
parameter that's actually very important
just to reduce the dimension otherwise
the storage would become prohibitive so
what we are left in is a brdf which is
four dimensional that turns out to be
still too much storage so we then we
focus on isotropic brdf which are
materials that still look the same if
you rotate them around the normal
direction
so then it's just three dimensional and
even then even then it's challenging so
there's a lot of kind of technical
aspects of the representation we have to
go to frequency space and then exploit
sparsity and do a few more things and
then we can get storage under control so
during the spatial dependence i think is
a unfortunate not really feasible at the
moment with that type of method yes you
mentioned two things where and basically
artists have a more physical handle
things like the weaving pattern example
and also the coating example where I can
have a gold layer and then some coating
on the console layer and I can see that
the artists they really can manipulate
some physical sliders and feel very
satisfied about these physical controls
can you also imagine that they are
things we're going closer to the
physical detailed models is actually
hindering the artists other areas score
at odds with the concepts that an artist
modeller has in their mind so is it
always really that in going positive
physics is going to be more yeah
definitely how'd you addresses or could
you think about pressing so for instance
this layered model that I I mentioned
just now that's that's sort of the way
that I think about materials in terms of
their index of refraction you know
Beckman equivalent roughness coefficient
so those are also not parameters that
are very usable 22 I don't know a
goldsmith for instance so um one thing
you can do is repair a mattress
everything there are some some people
who have been working on that just
finding more intuitive parameters for
instance something that will let you
move between diffuse and specular
there's a brdf that's being used by
Disney that that does that and so they
they have still a physically-based
reflectance model underneath but all of
the parameters are exposed through some
nap taking as input artist friendly
parameters so that's that's an option
not sure if it's really usable for for
everything in general
definitely the case that the more
physically based you become the harder
you also make the the modeling for
yourself so these rendering systems they
really require perfectly meaningful
input so you mentioned earlier that you
had issues for instance where the
normals were flipped on some model and
then the rendering you get out of that
is just garbage so that's definitely the
case you have to really put a lot of
especially when you model things out of
glass there are changes of the index of
refraction the moment that your model is
not watertight or there's some kind of
transition of index of refraction
changes that doesn't make sense you can
actually add energy to your you can make
a little atomic reactor out of a glass
of water that like every time light goes
through it increases in energy and so
there are bits I find myself explaining
a lot of these things over and over
again to 22 people using the system and
maybe another interesting direction
would be just to figure out how to make
things more brittle and and yes um the
Saturn to me hmm look like it was
begging to move mom what is obviously a
whole different you know part of
computer graphics in terms of accurately
simulating the motion of art oh okay is
it completely orthogonal to what you
don't know do you think some of these
ideas could possibly be use in the
simulation of motion of these divine
scale um so at Cornell there's a big
effort or used to be a big effort on
cloth simulation especially these types
of woven pieces of cloth that's where
I'm this scarf example for example came
from it's not really related to light
transport I would say there are some
material parameters that could probably
go into into both models so if you have
something that's very fussy that will
have a visual effect but it will also
have an influence on the on the dynamics
of the material in terms of the actual
simulation that has to happen
I think it's rather different one is
yeah one is the steady-state simulation
in the in the light transport simulation
we really don't care about time because
light is moving so fast we can just
remove that variable oh yes so I'll just
give you a brief idea what what is going
what's happening here so this is
published by now sorry so the so I'm i
mentioned that rendering is very
expensive and part of the problem here
making my waist early okay part of the
problem is that the amount of pixels
that have to be rendered it's just very
large and this is becoming worse and
worse with the increases in pixel
resolution so in the movie industry 4k
is now becoming very predominant nobody
can render 4k images it's just too
expensive then you have to do it for
stereo and now there's also a trend to
move to higher frame rates so from 24
frames per second to 48 so there's just
no way and so how can we how can we deal
with that well one thing that people
often do is to use the coherence in in
this iterations to reduce the burden so
because all of these frames are so
similar you could just render a subset
of them maybe every fourth one and then
reconstruct what's missing in between
and and you could render the images at a
lower resolution and then extract the to
do some kind of up sampling and then
also you could render noisy frames and
preprocess a post pro system with a
denoising filter and you can also do all
of these four things at once now the
problem is that there are certain
assumptions of these 2d post processing
algorithms that really don't work
anymore the moment you have renderings
with a realistic light transport
simulation and that's because each pixel
captures many different effects so you
might have an object that may be going
this way in screen space but the object
is specularly reflecting
and the reflection is actually going
that way and maybe it's not just
reflecting but also refracting and you
have yet another thing that you can see
in that pixel which is a refraction that
that's moving yet another way so you
have all these conflicting things that
are reduced to 2d and then you cannot
process them with I don't know yeah
basically any kind of state-of-the-art
technique so what what we what we did
here is to instrument a rendering system
so that it generates many different
images that separate out all of those
different effects that are happening so
you can think of it as generating some
kind of a tree that you can see here and
all of the leaf nodes are images and
then the tree nodes are operations of
what you do with these images so maybe
you add them or you multiply them and
the the important thing is that all of
the leaf nodes capture such specific
physical effects that you can now
suddenly run to the post processing
algorithms on them and then we also
determine where it's something that's
visible in one frame goes in the next
frame so here's just an example so we
start with something that's very low res
and noisy and subsampled in time and
then we extract a fluent animation out
of it so it kind of seems like magic but
what is hidden here is that we render
out to a huge amount of information so
our images have 100 channels or so we
started initially with images that were
1 gigabyte in size or so later we
admitted to restrict ourselves a bit
more and so one challenge here is also
that some of these refraction paths can
be quite elaborate so here you can see
these refractions through of us and if
we just interpolate the motion based on
the object of what's visible in screen
space we get all of these artifacts and
so using similar techniques to what I
mentioned before with this speculum
manifold we can actually compute the
screen space motion of specular paths I
think that's called specular flow and
buy some communities and so
that's basically what this project does</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>