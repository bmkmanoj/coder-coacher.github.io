<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sampling Techniques for Constraint Satisfaction and Beyond | Coder Coacher - Coaching Coders</title><meta content="Sampling Techniques for Constraint Satisfaction and Beyond - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sampling Techniques for Constraint Satisfaction and Beyond</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bRQ6m2iheQo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
great um so I'll be talking about simply
techniques for constraint satisfaction
most of the techniques that I will
describe will be on set but these
techniques can be extended to be on set
and other domains this is joint work
with super thick soccer Bertie Daniel
Fremont and cyndi staysha from UC
Berkeley and Moshe body from rice just
brief motivation in this 21st century
one of the things we design very complex
systems and few of the things all of us
are concerned is that how do we
guarantee that the systems indeed work
as we desire and this leads to a lot of
work in the verification let's start
with a simple motivating example so my
let's say they have a design a simple
circuit which takes two inputs a and B
this is a toy example and sees the
output so that sees some function of
envy in this case how do we verify well
one of the ways that if you can try for
all possible combinations of a and B
then it's going to take long time and
what this has led to is a dominant a
paradigm based on simulation so what are
in the last two decades what is dominant
paradigm is that you take a hardware
design and you try with a test vector so
you have some vectors that you try and
these test vectors should represent
different test scenarios and you hope
that you will catch the bug you have
some criteria some clue coverage
criteria you say that you know if I have
tried five percent of all possible test
vectors then I'm happy or you know five
days I'll be happy so in this case the
first question comes how do we get these
constraints these constraints come from
designers from past experience users
your designer might say that I don't
have a lot of confidence when you know
be lies between 100 and 200 so this is
your one of the constraint here from
past experience you found some bugs or
your users have some constraints so in
this case one of the
that we are interested in is that given
this set of constraints we want to
sample the values of a and B in this
case which satisfy this a set of
constraints and what kind of sampling we
want well initially let's start we want
uniform sampling and a y uniform because
we want to ensure that we are not
missing one of the tests kasturi really
so we in cases where we are interested
in covering all possible cases we don't
want to miss one of this a possible test
vector trivoli so to summarize we start
with set of constraints in our case we
assume we can convert this into a set
formula and now the problem reduces to
that given a set formula can one uniform
the simple solutions because enumeration
is a harder problem we are not we are
interested in coming up with techniques
which do not require enumerate all
possible solutions and of course
whatever techniques that we want to
propose should scale to real world
problems this is what we call it
scalable uniform generation of set
witness witness signifies a satisfying
assignment and we want this techniques
to be scalable this is certainly not a
new problem and it has lots of
applications in other areas for example
sketch by synthesis which have been on
based on templates there has been a lot
of work recently that can be benefited
from scalable uniforms in this and then
there are other problems in other
disciplines I will just illustrate how
sketch based synthesis can use a uniform
generation so in what we do in this case
play synthesis recently we come up we
have some sketch which is a template of
the program and some correctness
condition there are large space of
program that satisfy this correctness
condition and usually the goal is to get
an optimal program optimality can be in
terms of running time memory and one of
the technique is that if we uniformly
sample from the space of programs we
actually have probably stick guarantees
that we will end up with the optimal
program
in expected number of iterations I won't
go into detail that how it can be in
other areas but let's start with the
I'll introduce the sampling techniques
motive through the uniform generation
and then I'll discuss that how these
techniques can indeed be extended to
other problems like model counting and
by sampling and all of these techniques
actually depend on hashing so I'll have
some discussion of fashion and what I
think our future interesting directions
from this work cool so uniform
generation uniform generation is again
not a new problem there since 1980s
there have been lots of work in theory
and practice to categorize the work from
academia there are two lines of work one
is theoretical work but the focus is to
get really strong guarantees so the
foundational work by Bella record rich
and pet rank in 1998 where they are
actually able to give the guarantees of
uniformity so what is a uniformity
guarantee you want to ensure that a
probability a sample is output should be
equals to 1 over the total number of
samples so they can give the guarantees
of uniformity but unfortunately it does
not scale beyond formulas of 20 to 30
variables certainly not what we are
looking for on the other line there I
have been I work on coming up with
heuristics where there are very big or
no guarantees but these methods can
scale to large formulas one of a notable
work has been from Excel was X are
simple prime from gum several and Selma
from Cornell in two thousand an industry
has its own techniques to categorize the
techniques again on techniques that a
theoretical guarantees there are
techniques based on BDDs so you can
construct a binary decision diagram from
the set of constraints and now finding a
uniform solution is just doing a random
walk over BTD well if one can construct
we can get good guarantees it is are
known to a blow up after
the formula goes beyond a few thousands
of variables also there are techniques
based on introducing heuristics in the
set solvers so what you are other people
do is that in the set solar when you are
trying to find a solution you can have
introduced some randomness and then hope
for and that the distribute and that the
samples are uniform these techniques
have absolutely no guarantee some of the
techniques have very weak guarantees
although they have strong performance so
what we are interested in is coming up
with techniques that help strong
guarantees and strong performance and
that has is precisely our contribution I
will introduce our algorithm unison with
a strong guarantees of uniformity and
scales to formulas with hundreds of
thousands of variables so how do we do
it well let's assume the circle to be
said of all possible assignments and the
dots here represent a set is every dot
represents a satisfying assignment so
here's a wild idea if I can if I can
partition this space in such a way that
every cell that is defined by these
lines has roughly equal number of
satisfying assignments if such a thing
and be possible then what i can do is
that i can pick a random cell and pick a
random solution from this cell so if all
the cells have roughly equal sized then
for every solution has roughly equal
probability of symbolism looks like a
good idea but the problem is that how
can I partition without knowing the
distribution of solutions and the answer
to that comes from the techniques of
universal hashing proposed by Carter
Backman in 79 further work by sip sir
and 83 so I won't go into too many
technical details but a brief overview
of what Universal hashing does so
traditional understanding of hash
functions is that we met from one space
to other space will call it
from 0 1 raise to n 2 0 1 raise to n so
we have to raise to n elements and we
are mapping to to raise to M cells so we
know that if there are the inputs I have
our random from the space then all the
cells where I mapping will have roughly
equal number of elements in expectation
what Carter Backman sure is that you it
is possible to come up with hash
functions for which you can help the
guarantee is that no matter whatever the
distribution an adversary or the person
who is giving us these elements choose
all the cells are still going to be
roughly equal in expectation however
that is still not good enough for our
case because for example so few more
notice since F is the formula and i will
be using RF to denote the solution space
so in this case and that's the other one
is 0 so in this case if this is how my
hash function is dividing then the
expected number of elements in a Cell
our RF by two but I might end up
choosing this cell where I will not have
any solution and this is not good enough
for what I was proposing earlier and
turns out that our people realize its
problem in other discipline RF is the
set of also listens for the formula f
and so f if the formula and i will use
RF to denote the set of all cells so
this motivates us to come up with hash
functions with stronger bounds on the
size of the cell how many elements can
be in a cell and this led to a lot of
work these are am the problem arose in
other disciplines of this led to a lot
of work on a higher Universal ax t hash
functions so we will I use h NMR to
denote family of our universal hash
function so we started with the
universal but now we want higher
universality I'm not going into too many
details but what are our Universal had a
hash function means is that higher the
are there are stronger guarantees
the range of the number of solutions in
a set so we are trying to minimize
higher moments in a variance or a higher
moment of course there are no free
lunches as you want more guarantees you
you will have to construct these hash
functions and otherwise universal hash
functions requires a polynomial of
degree r minus 1 so there is attention
things that are scalable so we want
lower complexity and for that we'll need
lower universality but we also want and
stronger guarantees and again I idea of
using hash functions is not new Bella
record its patron use this idea and
their paper so we said well let's just
implement the paper so what was the idea
you start with the solution space which
NM and is the number of variables in the
formula and we are talking about a
boolean formula so we have any Universal
hashing so Bella record bgp uses an
universal hashing and then what it
checks is that it checks whether all the
cells are small or an equal so these
reasons are specified by here so there's
a formula and conjunctive with the hash
function so a hash function so
additional constraints so since I'm the
algorithm required that all the cells
should be small once you can ensure and
all of them are equal you can pick one
of the cell well when we try to
implement the first question came that
how we are going to be able to encode
polynomial of degree n minus 1 we are
interested in formulas of degree
thousands so encoding a polynomial of
degree 999 and giving it to set server
and asking for solutions way too much
what we have been able to show is that
this can indeed Boettcher just using two
Universal hashing or three universal
so the three universal hash functions
are simple exhaust that I will be
discussed later the second person earth
that while it can be formulated in NP
query if there is a cell which has more
solutions it is again not feasible so we
ask can you just randomly pick a cell
and make our decision based on that cell
if that's really small enough can be
something more about this first look for
let's say would progress it hash
functions satisfy any diversity what I
don't understand is I will try to make
the hash functions aha so yeah again
there is attention if hide the
university it gets harder to complete
will cryptographic hash functions be
good enough for you my depends we know
that some of them are for wise fibers
independent so like tabulation best
etching is for wise independent the
universality and universal hash
functions of I think the only technique
we know is coming up with these
polynomials which are also related to
characteristic polynomials am but they
require degree of n minus 1 so you want
well that was the idea but anchor um we
want stronger independence but we don't
want to encode polynomials of degree n
and what we have been able to show you
that you don't have to go that far you
can actually achieve this using very
simple hash functions so coming back to
what I was describing is that the btp
are required to check if all the cells
are small and equal
yeah yeah I'll discuss that so what we
can say that we can actually just pick a
random cell and make our decision based
on just picking a random cell and well
we don't get uniform generation weakened
slightly weaker guarantees but i'll
discuss later that these are strong
enough for all practical applications so
we get guarantees of near uniform
jenison that i will discuss later so
basically this the cell can be specified
by a random constraint and the original
formula goes so the sets all in part is
done here ok cool so what this has
allowed us to take a method that could
scare that could work only for few tens
of variables and take it to thousands of
hundreds of thousands of variables so
coming back to the idea there are two
questions one how large these cells
should be and what what I mean by the
size how many solutions are cells should
have the second question is at how many
cells should I partition into so to edit
the first question few things I know if
the cell is too large then I'll have to
actually enumerate the number of
solutions that that's what we started
with that once I figure out a cell I'll
list all the solutions and pick one of
them randomly if it is too large then it
will be hard to enumerate if it is too
small you can see that the variance can
be high if the cell size is just one
then the variance can be zero some cells
might not have a solution and it can be
I also the guarantees that we gave our
near uniformity basically approximation
so the more you want the closer to
uniformity you want the cell would be
lots for example if you want true
uniformity then you would just be cool
in the whole solution space is yourself
what if they have said that the region
without regards to the formula like that
what you have your technique what you
there is a without considering the
popular we also inside the region you
could reach another constraint and then
go to San yeah so the constraint the way
you specify the cell if because we have
to come up with these constraints of
hashing constraints if if we had to
encode an Universal hashing then it will
be formula and you know a degree of n
minus one polynomial encoded into set so
that's where the polynomial come from
don't
footings picked up
because what we need is a radical
straight through partition the solution
space then this will pick up on those
attending
and picking something
also need these guarantees encrypted any
solution impactor
is the same as
additional property okay so what we have
been able to come up with a I'll use
pivot to denote the size of cell and
pivot is of course inversely
proportional to epsilon which I'll and
discuss later which is the approximation
guarantee and so for typically the pivot
comes down to weigh 40 and so what we
are interested in in a cell that has 40
solution atmos for deserves okay so let
me discuss how does the algorithm look
like so we stop so the next person or
that how many cells they should
partition into and for that we have a
linear search algorithm and that can be
optimized later I will discuss that
later but let's start with on the idea
so we initially give the formula to a
set solver and we ask if the number of
solutions are less than pivot to do that
if you can just enumerate solutions
given a set solar you can block a
solution and ask for an exhalation
because the pivot is small it is
possible for most of the SAT solvers if
the answer is no then you put one
constraint and it partitioned into two
cells you pick a random cell and you ask
again and if the answer is now you keep
doing until you reach a point where
because you keep partitioning so at some
point here is a point where the number
of solutions are indeed less than people
in that case you enumerate the solutions
which you have already done because you
are using assets over and you select a
solution randomly from sets up from this
cell we are not making any assumption on
the set solar as long as it's a complete
solar and gives a solution we are not
making any assumption on the randomness
of sex oh yeah yeah well because we only
choose one so the whole idea is that at
every point when it didn't you know the
number of solutions were greater than
pivot I actually partition into further
I said you know this the partition into
two is not a good idea so let's partisan
two more so I actually partisan 24 I'll
show XR constraint they actually
partisan you know M constraints
partition it to raise to M sets go from
one go back to previous life then go
from or constrain we're actually no
solutions are more than the pivot they
added workers saying you are not
actually finding the partition is
speaking you party is figured um well
actually it can be shown that you can
just work with this partition oh yeah
sorry yeah you can you just more
constraints all you say and so it did so
you can do either case we initially
started with saying that you have to
pick the whole thing randomly but you
can actually this partition it yeah
that's what the our proofs depend on but
we don't check further at this point
what we say that yeah this was a aya and
there will be a theoretical guarantee
that religion has less polymer solution
and Ally the disability without X quite
possible that you know you do this and
there are regions will sue ya lo and
improve so that we can actually
experience in practice is actually much
better than it originally and it is
simulated yeah the article get pleased
that when you pick up one of these using
these three person hash functions pick
up one of them and find it is small
probability that
samples not uniformly distributed
stressful okay and later able to talk
about how to get what your distribution
research yeah yeah so what does this
give in theory well for uniformity we
are interested in a solution Y for every
solution Y in the solution space ideally
probability of Y Z output would have
been won by RF in our case we give lower
and upper bounds and these look very
similar to the PEC bounce except the
constant is 6.84 instead of one but what
I want to state is that this is strong
enough it guarantees that you are not
going to miss a solution trivoli is
actually independent of the number of
solutions of the yeah so so the number
of iterations that you would take to
actually reach the fixed point like
where the resolution is less than pivot
would be very high if there are large
number of solutions to the formula so
why not make the favorite size actually
dependent on the number of solutions or
maybe it's related somehow to the number
of switches so that you reduce the
number of iteration well the first thing
is that you don't know the number of
solutions have absolutely no idea about
the number of solutions the second you
can make the pivot large and it will
come down quickly but when you make the
pivot large there is a trade-off because
now you have to enumerate more solutions
so to give historical context the BGP
algorithm also had the concept of pivot
but it was n square in the number of and
where n is the number of variables so if
you're a thousand variable formula you
have to enumerate a million solutions
just not feasible it's a probabilistic
algorithm so in theory we can prove that
the algorithm will succeed with
probability 0 point 5 2 because it is
possible that by the time you have
partition there are no solutions in the
cell users and in that case we declare
it it has failed in practice the success
probability of all the experiments we
have run so far is a far greater than
point I am very close to one
so of course we are still in the same
complexity class they make polynomial
number of calls the problem is in PPP to
NP and our algorithm is in the same
complexity class okay so how will we do
when we go to practice so we wanted to
first evaluate how good are we in case
of uniformity so we took a small
formulas where it is possible to
enumerate all so listen so that we can
have an ideal simpler so in this case
I'm plotting graph for a formula that
has 2 187 variables 1200 closes and has
16,000 solutions so what we did is that
we had their ideal simpler and over
sampler and we wanted to see um this
graph says that the point here employees
228 solutions were generated 150 times
so so XX so a point here I guess if you
go by the water the point here implies
it say that 228 solutions were generated
in 150 times so what we need to be
render these algorithms for into 10
raise to 6 times a lot of times and then
we found that how many times a solution
is generated and they I you can prove
that if you start with a uniformity this
actually comes to a poison curve this is
how it should look like so basically the
x-axis consists of all the solutions
right the y-axis consists of frequency
bodies no nah it's not a point let us
say take the point to 28 1 so it means
that there are 128 distinct solutions
each of which appears hundred and fifty
nice
so I did
all of the solutions so if I did not
tell you that which of the Earth's
correspond to our simpler and the other
one is to the ideal simpler I guess you
would be hard-pressed to answer and in
this case yeah the yellow one is over
simpler and the purple one is the ideal
simpler uniform simpler we also tried
with other matrix like ll 1 the norms
and in all the metrics that we have
tried so far the distributions for the
bet all the benchmarks are
indistinguishable from uniform ok so how
will we do in run by running time so I
am presenting for a subset of formulas
these come from different verification
scenarios and on the y-axis the time is
presented the purple corresponds to X
are simple prime
across all networks what do you define
benchmarks where you work I know so far
yeah I mean we couldn't try for all been
fun because we have to run it with a
situation so far we haven't had and the
thing is that because the theoretical
guarantees in pectus there has a lot of
work on X or they are far more powerful
than three in the vicinity yeah so this
is what I was showing here they um you
um uniform simpler yeah no actually we
don't know because we are repeating at a
lot of time so all so this also maybe
the interesting point
so there is no solution which is left of
that is forbidden where is when we write
a similar example
those backup solutions just felt left
out what is the bias from the solver so
those solutions never got generated so
how about those curves look like suppose
you two are curve for example in this
solution so well in this video there's
uniform it occurs in that case you could
see that the solutions were never
generated in this case it will be this
some of the solutions will appear more
times and there will be less number of
solutions that get up there yeah but
there are no points below what are the
money but as over there you find that a
large simple solutions are actually the
exuberance and
does the
but there is equal foreign matter
everything's in CNF x yeah um all the
discussion that i'll be doing it's the
sienna well we use so we use a venir a
SAT solver in all this technique so as
long as your sets Oliver is complete it
can give you a solution when you want
the algorithm doesn't care
oh yeah so for this because we wanted to
run a lot of times and get some say
compared with it we can scale to the for
the latest of the largest we have been
able to scale our 400 400 thousand
variables for 80,000 well again it's
this curve is every simple for unison
gets generated in point point zero two
seconds of point to say very I mean we
run it on the cluster and didn't have to
worry at all about this so this is very
easy formula okay so then how will we
compared with X are simple prime that is
the solver that has some very big
guarantees compared to unison then on
the solar like busy pv says very strong
guarantees does not scale cannot solve
any of the four been swung that i am
representing here so there are some
solutions that were never generated so 0
they're not in its slides oh yeah I mean
um if you use ll 1 it's very very
different you have the melon seeds so
that
so the other plot that we did was to
actually seeing that are all the
solutions so there we plotted the
different solutions to lightbox
reversing their existence is one point
is one solution of resolution number 60
and then we plotted how many times so
that's why it is that a uniform on the
other side it should be straight from a
gap but using finite number of samples
in every state in the limit so so so we
basically get it clustering around that
line but we'll be running source sample
premises funny thing we notice so the
slide is your ideal line there was a
group above it I backed up a bit to the
band below it and all the x-axis which
were representing the individual
solutions there was an entire empty bag
which means those solutions never
generated
so we we did it well you represent both
it just both are you know overlapping
with each other so because these
distributions you use you know ll 1 or
whatever was in our first visual so I
mean this is a question which we do not
know how to exactly answer only measure
the Udacity in a practical experiment
noted and noted this everywhere it's
basically a sufficient condition this is
actually all of these are various
perspectives there's about viewing
uniforms happen yeah that's what my
confusion were very small no no again
and you know we have the clarity so I
really with a practical experiment
okay um cool so how will we compared
with X are simple Prime the purple
represents the time taken by X are
simple Prime the blue is by unison this
is log scale and you can see even well
that's the best algorithm out there to
compare against and so I won't call
better your answer was no reason for
using one exactly not as possible yeah
so we can yeah answers are correct
dances example like those ones ever
gives wrong yes no it generate but it
never said that is it is not uniform
this yeah but ever solutions is because
the problem is not about the solution
the problem is the conditional
distribution is wrong yeah so when I
three eyes of the coordinate you even
then you know exhaust happens you say
you don't use time by going to a
hydrogen yeah yeah wait so now I
introduce our techniques here in uniform
the nation and I will discuss that how
we can use this to do solve more
interest other interesting problems like
model counting and when instead of
uniformity you are interested in some
distribution or solicits so what I will
be discussing is approximate model
counting so what is model counting to
define we are given a set formula and RF
again denote the set of all solutions so
the problem of sharp set which is
counting the number of solutions ask
what is the cardinality of para for
example if F is a or b RF is 011 011 and
the number of solutions that your give
is 3
and Sharpay is the class of counting
problems for which for the problems for
the decision problems in NP so why are
we concerned about this application this
problem well it has lots of interesting
applications for example Bayesian
inference can be reduced to model
counting problems decreasing multi is
increasing there are lots of reductions
from interesting applications to model
counting so now it looks pretty obvious
what do I am going to do if I can
partition if all this cells have roughly
equal number of solutions I can actually
pick a random cell and my estimate would
be the number of solutions in this cell
multiplied with the total number of sets
and no nothing maybe outside is nothing
for now
and the strength two instances in a
particular for a particular choice of
random constraint some of those cells
any empty but these are random things in
the expectation
so supporting was that sleep up there
together
yes I'll show later that I every X bar
is one cell and mr two days to him so in
this case again going by the idea we
will keep putting X our constraint will
keep checking if it is less than pivot
the moment it is less than pivot you
count the number of solutions in the
cell you know how many acres you have
put so multiplies with tourists and this
gives an estimate with some confidence
if you want to amplify the confidence
the usual probabilistic techniques you
can repeat the algorithm t number of
times where the T depends on how much
confidence you need and then we take
median of these values and it can give
the excellent data guarantees so what
does this current is look like we start
with the algorithm takes formula f you
can specify the tolerance we say that
how far the approximate count can be
from the exit count and the confidence
how confident you want to be so we have
the theoretical guarantee that if the
counter returns account see then it lies
within the tolerance epsilon with
confidence delta that you specify and
again the algorithm lies in the same
complexity class b p p 2 and p it is
polynomial with respect to the
parameters given access to a set solver
ok so how will we do in practice we
wanted to compare with the the exit
counters so a kesha instead of our exit
counters or what we did is that took a
set of benchmarks we listed the runnin
time on y-axis these are the benchmarks
so what we say are there have been lots
of development in exit counting it can
indeed do actual counting for lots of
for the last class of formula but then
it starts steaming out and what we are
showing here is that the em algorithm
approximate can handle problems that lie
beyond the range of exit counters so if
you have an exit car if you have a
problem you can if you cannot do exist
counting
your best bet is to approximate counting
if you can't do that then you go to
methods which has no guarantees this is
by henry paul behm Henry calls tianshan
latia how how do you say I am
for a given for morning morning poison
pills so I am not aware of the tool but
these experiments are first set so I oh
I am well they have the other so by the
way the latest tools from their group is
sdd which you can also use to do
counting and be sure I understand yeah
well sir it in the last leg is when you
take the median of the traffic circle
sense content ether medium usually one
is the particular average white where is
the median so honestly I don't know why
it is better but the proof is very
simple if you take the median but I i
would guess that you can yeah I'm to the
proof
rather try this median actually we knew
that exactly so basically if it is wrong
then you know that half of them have to
be wrong and just a simple combinatorial
argument you can it yeah okay so how
will we do in practice well we took
benchmarks we rent the exit counter and
within our experiments with setting
epsilon to be points and five which
loosely says that allowed error in 75%
what we observed in practice is that
first the Delta was set to be 0 point 9
but we observe that in all cases the
count is actually within the one in 2
1.75 and over 1.75 so these two curves
the purple curls are lower and upper
bound of the 1.75 factor that you can
obtain from exit counter cash a more
interestingly the error that we get is
mini religious four percent which again
says our guarantees are far more
conservative than in practice so we we
were allowing up to seventy-five percent
but it's actually the count that we come
of it's very very close to the exit
account ok so now moving on can we take
these techniques to cases where you are
actually not interested in uniform
sampling but you have some distribution
our assignments and you are interested
in sampling from the distribution given
some constraints so extending the idea
what I want in this case is that all the
cells should have equal weight so what
we assume here is that for every
solution there is some weight assign it
can be a black box or you can specify
according to different distributions and
now what we want is that
resale should have roughly equal weight
which we are shown that this is actually
possible they are the proofs and the
techniques extend the idea instead of
counting now will be summing up the
weights and using the similar ideas so
and then we can pick a random cell if we
want to simple then we'll simple this in
this cell will simple a solution
according to its weight if you want to
count then we will count the weight and
multiply it with the total number of
cells so moving on you one might be
interested yeah yeah actually the proofs
go through the idea of more technical
details but I think we can take that off
sign but yeah
but now the number of
solutions
because they not eat of the week then
you would have 1.3
yeah so is actually so now the sizes of
these random cells approving AF not be
as small as the earlier I have a little
bigger and factor which equals be right
here so it is the earlier Siewert
multiplied by the ratio of maximum and
yeah and I but then you can do log
reduction on that so yeah I guess okay
so now one other interesting problem is
that i am actually not interested in
sampling over all variables maybe I care
only about few variables or I'm
interested in counting unique solutions
to the subset of variables for example
if this is my solution space for a
formula of ABC variable so I have three
solutions 0 0 1 0 0 0 and 0 1 0 but if I
ask how many solutions are unique
solutions to and B then there the number
is only two so not three and actually
it's interesting that we can solve this
case more efficiently sort of violation
of freelance theorem but we have to hash
only on the variables over which you
want to project so doing that your hash
functions are actually more simpler so
we can solve this case in general the
techniques would X and you would have to
enumerate you would have to say ok I
have to put blocking close on these
variables but we can solve it for
efficiently
as you go and do a quantification first
no well yeah
and similarly you can do sampling you
can give the guarantees that you know
for every solution okay so the whole
talk I kept mentioning hashing hashing
and that's where the real meat lies so I
have brief discussion on this so what
are the hash functions we use we use
this XOR best hash functions what we
were trying to do we we had n variable
formula so we are trying to partition
the 2 raise to n space into 2 raise to M
cells so if our input variables are x1
to xn how we construct the hash function
is pick every variable with probability
half and XOR them and equate that to 0
or 1 so that is one constraint it
divided into two cells if you want to
divide into so if you recall in the
algorithm we had to keep dividing so
every time we had an extra constraint it
divides into you know two times the
number of cells we have divided so mxr
equations divided into 2 raise to M
cells so how does our formula look like
we have the initial formula and we have
X or constraints turns out that crypto
people from Krypto yeah so you have
again yeah you can prove their
international space and so and you are
doing anything so equations yeah others
what what XR equation is so how do you
click one x our equations look at all
the variables
take this
the subset of this exotic I see and then
equal in 201 with embroidery
this
events
since the nice part about this is there
was all this I constructed this is a
direct and Isis constraint divides this
place into exactly two ops 2 equal size
Charles because for everything which
makes him see you you will have another
thing that should be
okay yeah if you take any assignment of
religions which make 60 just put up with
random ups so that sorry twenty20 I've
got this is also a movie to select a
monitor yeah for hashing itself you just
give it time hey I answer that you're
saying that I want this parity to be one
it was also injected with the origin
you want to pull you naughty one hasn't
came on thought we said to be zero arwa
sign this
that is a hash function whoo it's a
forward one makes a sort of you choose
the 0r what you are choosing one of the
two buckets yet also yeah
yes you are choosing one
so we are secretly make the selection of
which bucket list absolutely so that's
some of the random choice of the pocket
very much choice of the same you should
let me just define a hashing function
yeah so just that its own constraint is
the hash for my point so yes exactly
randomizing that we are randomly
choosing a copy so if it is it is it
constraint right right now I have
original formula and I write a more such
obvious yeah and you fix those formulas
once and for all or do you keep
regenerating the smaller like you're
having so we can put the first one
random is intuitive it comes into pools
and by this 01
so we come visit more than 40 solutions
moment to see this 40 of solutions this
is not good now we add another
consideration fresh and let you keep
this guy thing oh yeah you have oh yeah
so that also allows for the increment
because we independently chosen these
supposed to be honest all i have is that
you know when you do this to themselves
now to me it seems like you know some of
the self or some of those cells will not
intersect with the formula right the
glass is like some of the cells could be
empty we have a specific case for a
specific insurance right but the house
like that has no spicy money happen
because these are being randomly chosen
there is no preference why a self should
have only everything's in it is as life
require everything's as it is
yeah I that
okay so while CNF plus X ours and there
is actually a efficient set solver
crypto mini-set no no and in case of
creek so crypto mini-set people have
observed that if you encode exercise in
to CNF then it doesn't work so well so
what this nice solar does is that it is
almost like an SMP solver because X or
by themselves is a very easy problem
Gaussian elimination p time problem so
I'm and then it I make PS almost like an
assembly solver and it can actually saw
snakes yeah yeah yeah I don't understand
why yeah just remember them to sailor
and this here is a serious problem solve
it
and she said either X or constraints can
do Gaussian elevations of course there
is no questions it is doing ocean
English figuring out to be stalking yeah
so although adding on that y XR we know
there is a for resolution based systems
we know there is a exponential lower
bound for X oars so the sets always
always mess of a lot of things but they
do resolution so that is one of that can
be one of the potential experience one
thing that it's all huh yes excellent
system that was something ya see
constraints same thing so Consuela
quality so yeah but this is a bit do it
did it figure out the right we were
figure out that we can randomly choose
the sale and circulated they also choose
these kinds of hash functions of X on
the answer this is actually exactly the
same hash functions but they do not make
you the properties of these I see
so the video is not complicated
parameters to be provided and provided
the right parameters then you give
something otherwise yeah okay so one of
the observation is that what is the
average length of Atkins xur constraint
if I have n variables than average
length is n by 2 in practice there have
been other experiments on Xers and we
know smaller Texas better the
performance so one of the question was
that how can we shorten the xr's I'm not
going into too much details but what we
came up with the concept of independent
variables what it means is that in a
solution space if you assign values to
some of the variables addict assigning
values to some of the variables
basically called as independent
variables can uniquely determine values
for rest of the variables so for example
if you are trying to end called a circus
equal to a or b then the independence of
what is a and B because assigning to NB
you know see but assigning to B and C
you actually I cannot uniquely determine
what will be value of it for example
this can be one this can be one then you
don't know whether H 0 1 m in theory
finding independent variables
complexities do we are working on it but
in practice it is easy because all the
auxiliary variables that you introduced
in any encoding they are dependent
variables so we hash only on independent
variables the technique works as as long
as there is a superset of independent
variables and this results in two huge
speed up so we're wrapping up what are
where do we go from here the first thing
is all the techniques that I presented
our onset domain bullying domain can we
extend these two more expressive domains
like smt CSP I think there are two steps
one it's efficient we'll have to come up
with efficient three universal 03
independent hashing schemes just
extending the bitwise to a smt you can
get the guarantees but then you lose the
whole advantage of SMT solvers because
it will force you to bit blast to start
with I would say that you know bit
vectors let's just start with modular
bit vectors and then we can extend
beyond that and the second thing is that
in our case we had crypto many set which
could handle the CNF plus X ours very
efficiently I think you extend this will
need to come up with solvers which can
also handle the hash function that will
come up with a very efficiently so we'll
need similar solvers for other domains
the other thing is we have been using
CNF plus X horse it turns out that we
don't know a lot about what happens when
you mix ian of NX horse for examples
basic questions like phase transitions
so we know that if you take a random CNF
formula there is a phase transition if
you take a random XR there is a phase
transition what happens when you have
some predefined space and you are adding
random constraints if you know more we
can actually modify our algorithms to
take advantage of such cases the crypto
min said has more like a lazy approach
it tries to do Gaussian elimination at
some point it does a bit blast but at
very in a higher depth so more has to be
certainly explore it until what point
one should do Gaussian elimination when
should one convert into and of course
the one way to speed up the whole thing
would be can we come up with shorter
xers there was recent work from airman
at all the short that if we knew more
about solution space shorter experts
would have been fine but again we don't
know about so listen Spence so can we
come up with can you theoretically prove
that even shorter exes are fine
I also say there are some interesting
connections which go back to and the
paper bivalent Jeremy valent 40 any
seminal paper which reduced very simply
to counting so what the guarantees that
we had was 6.84 and it is certainly
troubling because why can't we get to 14
almost uniformity that was described by
zero valent buzzer on a defector was one
what gets more interesting is that we
actually get the approximate guarantees
if you remember the vector was one so
what Jeremy Lin was Ronnie showed is
that if you take it if you want a almost
uniform generator you can make
polynomial number of calls to
approximate model counter and then you
can indeed get almost in the phones in
listen and vice versa if you want
approximate model counter you can use
almost uniform generator so what we know
is of course we can use the approximate
model counter to get near uniform
generated so what we are doing are very
similar techniques for both cases but in
in case of approximate model counter
where we are not using near uniform
generated entirely as a black box so
what needs to be shown is that how a
much uniformity does one need to reduce
to my approximate County so this is the
part that of course you can get knit
since near uniform generation looks like
a weaker property then almost in the
function listen you can do the first
step we don't know about the second step
so what needs to be are done is to get
more explore and the connection between
near uniforms innocent almost uniforms
in this these are the work and on the
collaborators with which all of this
works were done thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>