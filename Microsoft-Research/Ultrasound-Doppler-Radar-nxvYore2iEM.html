<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ultrasound Doppler Radar | Coder Coacher - Coaching Coders</title><meta content="Ultrasound Doppler Radar - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ultrasound Doppler Radar</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nxvYore2iEM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay good morning everyone welcome to
this talk it's my pleasure to introduce
super did Krishna Rao he's been with us
as a summer intern over the past 12
weeks working on ultrasounds he's a
master student at the booster you new
booster polytechnic institute of
robotics engineering and without further
ado floors yours thank you Hollis very
well a good morning everybody the audio
team dr. David had come and markeith dr.
Yvonne and and honest and everyone so
basically today I will be presenting to
you the work that I did with my mentor
hanas Gampa at Microsoft Research
redmond washington in the last 12 weeks
so we attempted to basically develop
hardware and algorithms for ultrasound
doppler radar and the and the primary
objective was basically imaging range
and velocity profiles of objects of
interest in the field of view so let's
let's see in a more formal way what the
objective was so given a system we
wanted to image and estimate right
around the system that is basically a
360-degree horizontal field of view in
the field of view we wanted to estimate
and measure and estimate targets
position and velocity relative to the to
the user and why would anyone do that
probably for such situations where in
with increasing technology or with more
number of disabled people we need more
intelligent systems to
be actively sensing the ever-changing
environment for us and this is one
typical case wherein a user is happily
listening to some music while walking on
a busy road that's that's definitely
going to come in the in the very close
future and he need not really worried
about the vehicles approaching him
because such a system aims to warn him
about these these these targets so the
underlying principle for this system was
basically Doppler effect and time of
flight and just to give you a quick
illustration of what Doppler effect
really captures is basically when you
actually transmit a signal there is a
relative there is actually a change in
frequency when there is relative motion
between the source and the receiver so
in this case the source would be the
user the the object would be probably
the car or the other target and again
the receiver is present with the user so
let's let's take a look at why we chose
ultrasound to to achieve this task why
is and why not and firstly ultrasound is
very low power consuming and since it's
not very high frequency it's it's about
40 kilohertz high frequency components
are not pumped into the circuit and so
this this is basically why the
electronics is really cheap when you're
designing an ultrasound system and small
factor small form factor is is a great
great advantage because it offers a
great choice for mobile devices because
as you can see in the in the later
slides a transducer an ultrasound
transducer is millimeters or it's really
small so even after using probably an
array of transducers the form factor
will not have really blown up
and most important thing again is it's
outside human perceptible range so if a
device is if the device is being used by
the user it won't really interfere with
his normal day-to-day activities well
works very well in indoor outdoor
well-lit and you don't need light so
it's as against any other for example
cameras or something so it doesn't need
illumination it's active sensing and it
works equally well indoor and outdoor
and array signal processing basically
can be leveraged to get a 360 degree
field of view which we have in this in
the system and the the responses of
these transducers and receivers are not
super directional so we need beamforming
to really improve the spatial angular
resolution of our sensing and we can
leverage active time-of-flight sensing
that is the that is basically how the
transducers work to get a range or depth
estimate and again this is not really
available in passive sensing devices
like cameras and to cap it all basically
we we attempt to use Doppler effect to
get even velocity profiles of targets in
the field of view and this is every
frame so this is not across several
frames or or it's not so every frame we
have an estimate of the velocity and the
distance that was the attempt and so the
immediate question to ask again is why
not use cameras well we don't get depth
information and also we need
illumination because it's not active
sensing and okay yeah we get depth
information using stereo cameras but
wine why not that so they require global
shutter and this often shoots up the
price of the cameras and the entire
setup and stereo vision cameras require
precise alignment and calibration this
is firstly this is silly time-consuming
and even after a sufficient calibration
the depth depth resolution of the depth
accuracy is not really fantastic so too
yeah one might say to avoid these
calibration issues why not use a
commercial stereo camera setup like
bumble bee or a source they actually
require a very computationally intensive
processing algorithms like sum of
absolute differences or sum of squared
differences and these often require
nessus it ate a GPU and that again
shoots up the power requirements and the
cost and again no 360 degree field of
view well to get 360 degree field of
view we can use lidar but they are
expensive and power-hungry how about
connect it connect manages to solve many
of these issues apart from 360 degree
field of view but well 36 watts power
consumption and it requires a fan to
keep it from overheating keeping all of
these in mind we decided to use
ultrasound and let's see more about that
so we actually have to be objective and
take a look at the other side of the
corn also so limitations and challenges
of ultrasound the i guess the list is
pretty long you know compared to the
previous slide so challenge is that more
than a lot more firstly calibration is
still required and it often demands
anechoic conditions a researcher who
wants an off-the-shelf researcher who
wants to use such an array or such such
transducers can't really get access to
costly anechoic chambers that that's
still there and sensor responses are
basically very wide angle and this kind
of affects the spatial angular angular
resolution so that necessitates in a lot
of signal processing and frequency
responses are again frequency and
temperature-dependent
because they're piezoelectric
transducers reflections are one more one
more challenge because you have
multipath multiple and specular
reflections again that necessary it's a
lot of signal processing so these so we
are moving closer and closer to some of
the major issues that we experienced so
basically maintaining good
signal-to-noise ratio achieving
practical frame rates and making sure
that the device operates over a wide
wide range so some of the constraints
that that control these these parameters
are basically power pulse width and
attention attenuation of ultrasound and
air so power basically is we ultimately
increasing the power would increase the
distance and the range but that would
again result in overheating so one can't
really crank up the power after after
some point of time beyond a point and
pulse width improves SNR but again kills
both frame rate and range and and of
course we have more than three DB per
meter of attenuation of ultrasound
signals in air above 100 kilo Hertz so
these are so basically it calls for some
sort of an optimization between all of
these so these are these are basically
the questions at hand how to increase
SNR achieve good directivity high frame
rate and prevent overheating and again
estimate velocity and in a single shot
that has not really been done other way
around telex knavish bands basically so
what's the range but during the same
printer is 40 45
that's can go 5 is 10 meters 110 meters
ah that's brilliant we won't further
than that yeah I see okay so I'll just
walk you through the organization of
this talk very quickly so we we dive
into we present a literature review and
then take a look at the problem
formulation and give a small background
about Doppler proper effect and how we
can use it and for signal design and
estimation and then we will conclude the
talk with presenting the current
approach progress to date and results so
one can actually notice that right
starting from early 1950s to 2012-2013
and now 2015 researchers have invested a
lot of time and resources on trying to
make use of ultrasound so it basically
tells a story because man man has not
yet rejected ultrasound because for all
these years continuous progress has been
happening and more and more applications
have been coming up more recently you
can you can see that ultrasound imaging
was actually considered for HCI tasks
like guest gesture recognition activity
speaker gender age gait recognition all
of these smart smart home environments
so ultrasound has stayed with us for a
long time and micro Doppler signatures
these are particularly interesting
applications wherein you basically so
every small parts of a moving object
results in a Doppler shift so and that
is quite unique that's what the
researchers have reported and probably
feeding the signatures into a deep net
deep neural network can help us learn
learn the signatures and do some
recognition and from a range detection
perspective again
work has been happening since 1997 until
until 2014 and in fact the last the last
one that you see here that was the was
the prototype developed by the previous
intern and we made made use of it
sufficiently for our preliminary
evaluation and testing and this was a
get carried out under the same same
group under dr. Gunther chef so let us
see let us take a look at the problem
formulation that is the doppler radar
like i mentioned before this illustrates
the concept of doppler radar to be to
get more technical to be more formal
received signal is basically stretched
and compressed when the target is
actually approaching or receding and
ultimately this is what we aim to
measure so typically atone for a pulse
or a job is emitted to insana phi the
surroundings and we make use of a chop
signal that goes from 38 kilo hertz to
42 kilo hertz and reflection of a moving
target at a velocity V is basically
represented by sr of tea and you can see
that the signal is actually stretched
and delayed so the stretch gives us an
estimate of the velocity of the object
delay gives us definitely the distance
basically the time of flight and these
stretch factors are calculated as a
ratio of c plus v by c minus week where
velocities direction matters the plus or
minus and time of delay is basically the
time required by the ultrasound to reach
the required range and get back so
essentially we are trying to estimate
these two factors that is the stretch
factor and the time delay to determine
the range and velocity so particularly
from the literature review this but this
one particular paper
give us some insights about signal
design and estimation so they proposed
something called as the wideband cross
ambiguity function which basically is a
2d 2d coupling it is basically a 2d
representation that couples the delay
and stretch factors so effectively as
you can see that it's basically the
cross correlation between the
transmitted the received signal and time
delayed and stretched version of the the
transmitted signal and some some points
to be noted is basically for all
practical purposes the integral is over
minus T by 2 to plus T by 2 basically
because our system is a band-limited so
we don't need to do that do the overall
integral and like I mentioned it's
basically a 2d representation of the
correlation and the ultimate task is to
estimate these two coordinates in that
2d representation here so some insights
from the same paper about signal design
we see that lfm stands out as it gives
better resolution for estimating the two
parameters this is the Doppler stretch
domain that is the time delay domain and
you can see that the Gaussian signal
would be very very ambiguous as against
lfm some more results in favor of lfm
from the from the literature we see that
we get better resolution using the lfm
so we implemented the signal basically
what you see here the time domain
version the frequency domain basically
the frequency sweep sweep from 38
kilohertz to 42 kilohertz and the pulse
width is about 5 milliseconds
some of the parameters that that we
considered while designing the signal to
be transmitted is basically we are
sampling at 192 kilohertz our center
frequency is about 40 kilohertz because
our transducers resonant frequency is
basically around around that range
around 40 kilohertz max range we are
considering is about nine meters
currently this can this can be changed
and pulse width is about five
milliseconds so one interesting
interesting thing to note here is
sequence width here it's 10,000 76
samples that basically these are the
samples that are that represent the
duration the sound takes to it basically
do a round trip to the max range and
sequence which basically decides the max
range and all of this is basically
dependent on the speed of sound so
taking a look at the implemented pulse
train that we are pulsing out from the
thank you it kind of does you see it is
thank you thank you so looking at the at
the pulse train being transmitted so
this is basically this represents one
block we have eight channels and
basically a channels of transmitters and
receivers and 20 frames so 12 that goes
up to 20 and we make use of play graph
to basically interface with the sound
card we generate the signal and MATLAB
we interface with the device using play
graph to play and record the signals so
going into some more detail into the
estimation framework that is the
wideband cross ambiguity function I just
want to remind you that it is basically
a 2d representation of the
and the received signals so if you can
see here so this is actually the stretch
factor like I mentioned before we
decided to go for the following stretch
factors based on these constraints we
decided to basically measure velocities
of objects from 20 plus or minus 20
milliseconds so the meters per second
extremely sorry for that that is about
40 meter miles per hour and directly
deriving from that from the formula we
get the stretch factors and with regards
to the number of stretches that
basically decides the resolution of of
this ambiguity function that you saw so
for for a sample for it for example for
a stretch factor for the number of
stretch factors of about 31 this is
illustration of the stretch del FM you
can see that across 31 stretches it is
stretched in time and implementing the
ambiguity function and testing it for
lfm for the ideal case that is 0 Doppler
shift and zero delay this was the
response that we got and as you can see
it's pretty pretty high resolution in in
this domain that is in the stretch
factor domain and we tested we tested
our initial algorithms on the previous
prototype that was available and built
in this group and this is how it looks
basically you have an array of
transducers microphones and testing on
this set up gave us our first light that
is some results as you can see the the
plot to the left is basically the cross
correlation that gives us the
measurement of the distance so you can
see that there is a strong peak at
around around point 6 meters and you can
exactly see that
this cross here which represents the
maximum of this function is again sent
is again at around that range that is 0
point 6 meters and one more thing to be
noted is the speed since this is both a
single short estimation of both the
velocity and the delay the range so the
speed is more to the negative side
because the object was really
approaching the setup in this case so
basically what I mean is let us assume
that well this is actually the setup
that we were considering so there was a
connect facing the setup and this 2d
this this plane surface was moved
backward and forward in one dimension
across this dimension and this
represents the depth map of the connect
so we we just measured the the we
monitored the depth value at the center
of an roi that represented our object
and that is how we cut this plot so if
you can see the overlaid data of the
connect depth estimate and our our
systems depth report reported by our
system you can see that the slope which
represents the velocity is quite
consistent of course there is there is
an offset because there was a bias in
the in the arrangement between the
connect and our set up that can be
corrected for so diving into the
hardware design basically we have
piezoelectric transducers that we use
and this is how the prototype looks as
you can see the it's basically a low
form factor device of about fifteen
millimeter diameter and height of about
100 millimeters 110 millimeters and we
have this receiver array the transducer
array here
so taking a look at the system
architecture we have this basically is
the system architecture we have our
microphones we have em preamplifiers we
have we make use of our Emmy to
basically mix and the mix the signals
that are sent and received and it's
interfaced to the system the computer
through a firewire and that is going
into again a preamplifier and there is
our speaker array so this is this looks
more more intuitive than the previous
one the previous slide so basically this
is our test and calibration setup we
have our device and for calibration we
made use of B and K receive a microphone
and as you can see this is the previous
prototype that was used so taking a
quick look at the I agree that we made
use of that was developed by dr. Mark
Thomas so we make use of certain regions
of of this a GUI to actually send send a
send a test signal at around 40 kilo
Hertz and then we measure the responses
we update the gains and this is
basically the impulse response recorded
and the magnitude response to the right
so how was that how was this actually
done so to calibrate our to calibrate
our speakers we actually kept this
microphone at 1 meter distance and we
think through all of our speakers and
the response is recorded here where
basically what you saw for one channel
that is and to calibrate the receiver
setup we used a transducer from this
setup we again pinged and then we
measure the responses using our
receivers and we recorded the impulse
responses
so these are some of the directivity
patterns that we that we observed from
the measurements for the transmitter
they they don't look really directional
do they yeah so they don't that's that
was not unique yeah so not there that
although they're directional too they
are not really directional for about 15
degrees plus or minus 7.5 degrees which
we are looking at and you can see the
transmitter frequency responses one more
challenge that we faced by the way this
is after calibration you can see that
they are not very well very well matched
so this was something that kind of
created some delay you know another
thing you know no progress and but of
course you can see that the resonant
frequency is somewhere close to 40 kilo
hertz yura work area is between 45 and
50 yards yeah this is where they much
huh in their fourth of the variation
office yeah so the receiver died
activity pattern looks something like
this again they're quite quite
directional better than our transmitter
more uniform like dr. Vaughn said this
is staggering from the right that's the
natural or entity of Americanism
self-care and then only it or your
frequencies but we're talking sir you
just said the canvas it's made for the
true that's here so this but the one can
note that the receiver so this is
actually a the transmitter that we used
from the the previous configuration that
you on the developed by the previous in
turn you on Doc managed and these are
the receiver responses so they're all
quite quite well matched that is that is
good better than better than this so
this out all this necessitates
beamforming that's that's the long story
short all this necessitates beamforming
because we need better spatial angular
resolution and we made use of the be of
GUI tool developed here again by dr.
mark thomas at the audio audio and
acoustics research group at MSR and this
is basically used for generating the
beamforming waits for example this
represents the presents the the method
we use for beamforming mvd are closed
form and this shows the this basically
shows the array setup for for our
speakers and this is basically the metal
hit or basically the dimensions so we
can see the direction the directivity
pattern and this is exported and stored
for later use which we will see and same
same goes with receiver beamforming
weights we use them video closed form we
evaluated a media are closed forming a
closed form method and this is what it
looks like beautiful yes he is
omnidirectional so that's that made us
go for some some other methods of beam
forming that we will see in the in the
near future because firstly our setup
was our transducers were not really
omnidirectional so these are the results
from the previous two GUI slides that
you saw this is for the receiver looks
looks pretty good but still has a lot of
side lobes that that really catch know
many reflections and with respect to
transmitter beamforming I don't need to
say much it's it's not really it's not
it's not very great so all this actually
made us choose a different beam forming
weights estimation framework that is we
did it through numerical estimation we
had our we had our impulse response a
caustic channel impulse responses and
this was actually the desired beam
pattern that we were targeting for the
receiver and for the speaker and we
actually did numerical estimation we
basically fit the curves to get this so
we are forcing based on the measured a
strong yeah okay so as you can see the
MV dr is represented by green desired is
blue and curve fitted responses in the
red so it is quite evident that the red
ones perform much better than the green
ones that is MV dr classical estimation
of the weights same goes with the
receiver responses pretty pretty good
the red ones perform pretty well and
these are all the 24 beams by the way
yeah so for beamforming we are making
use of 24 beams rotating at step step
size of 15 degrees from 0 to 3 45
degrees and these are all the 24 beams
that you see so taking a look at the
flowchart of this entire system this is
basically a recap of what we saw we have
the ir GUI tool we generate the acoustic
channel responses we get the v design
and generate the desired pattern i
should probably mention a little bit
about that so this was basically the
signal like i mentioned that generates
this pattern so we generate that signal
the that pattern and then we take the
pseudo and the basically multiplication
of the pseudo inverse of the acoustic
channel matrix that is basically H
which basically is acoustic channel
response and the desired pattern so this
gives us a curve fitting and ultimately
we arrived at the beamforming waits for
this particular recording so if we do
another recording for another
measurement of the acoustic channel
responses we need to regenerate the
weights right so once we get the
beamforming weights let us again
traverse from here we have we generate
the eight channel 20 block signals in
MATLAB we played through play graph and
then we segment the received signal into
a matrix of four D matrix of it's
basically in the frequency we converted
to frequency domain and so n FFT number
of mics that is eight number of speakers
that is eight number of blocks that is
20 in our case so it's basically a 40
matrix and we perform band pass
filtering on the received signal to
basically reject all the all the noise
and signals beyond our frequency range
of interest and after that we do
beamforming for the transmitters so we
have the weights from here the received
signal and then you basically do offline
beamforming this was this was something
that was observed in the previous
internship basically by the previous
intern he observed that leveraging the
LTI linear time-invariant nature of the
system he basically suggested that
transmitter beamforming also can be
performed offline which significantly
improves the the frame rate and then we
perform microphone beamforming as you
saw before and then we perform a matched
filtering to basically again get rid of
unnecessary reflections and noise
basically we perform a background
subtraction which you would be seeing in
the next slide so some preliminary
results from from this new device is
something looks like it looks something
like this the raw map of 360 degrees
across 24 beams and we do post
processing that is to remove the noise
by using a knife approaches is basically
background subtraction so you can see
that I play a quick video here so this
was walking detection test to see if the
system really detects a person
approaching and slowly walking past the
device and how long exactly thank you
harness among so much clutter so
basically our our environment was quite
cluttered ideally this this should have
been done in the anechoic chamber but we
received the device towards pretty much
the end of the internship about when
when about 3324 about three to four
weeks were left and they were heatsink
issues and we had to install heatsink so
the device was actually going back and
forth from the hardware lab to us so
with all this and so that kind of didn't
let us move the entire set up to an
anechoic chamber and make better
measurements well amidst all this
clutter there is some happy news still
so you
so you can see that for the further
testing that the walking testing this
was the response this was the response
for the walking dist walking detection
you can see of course some more
post-processing is to be done we are
currently just using knives background
subtraction we can apply we can make use
of particle filtering Kalman filtering
or any confidence based tracking
approach to basically fix the fixed
object of interest actually in Vegas you
see video before I oh right right you
want God you want me to play the video
again before that you have the raw
videos right it's right there no that's
not so this is actually how it looks
before background subtraction you can
you can notice that there are responses
all over now there are later again so
there are responses pretty much
everywhere so finding finding a major
source of reflection is pretty hard
especially given given our cluttered
cluttered environments like as you saw
here pretty pretty much several several
objects right around the device are at
the same distance so that that is why we
get that's why we get so this naive
you're calling a nice form of background
subtraction and it's only working
because the sensor is fixed correct so
what is a sensor we're moving as if you
would be much more sophisticated
tracking
probably an optical flow or tracking or
particle filtering something like that
so you say reflections here in show the
positions you need us what about the
speed can you detect the speed of the
moving copter can use this fafsa help
could you please come again the speed of
the moving object so from the videos we
saw it's just reflections right which is
this does the time delay yes how well
the speed detection worker so we we were
battling downs issues with the device
until yesterday so we probably in a
couple of days we might we might be able
to really find out how the ambiguity
function behaves on this new device that
is because technically our another best
criterion for filtering case okay
everything subtract everything that
doesn't move right and leave only the
moving objects which have speed above
the certain threshold out to provider
yes much clearer image eventually that's
exactly what let us make use of because
they basically remove they have a
threshold velocity ranges and any object
that is not moving in that range of
velocity is rejected as a noise so
that's what the even the radars do here
so all of all the post process a lot of
post-processing is remains it is yet to
be done and we'll figure out soon how
the ambiguity function works you know
probably a couple of days you have one
called a not a couple so sincere thanks
to my beloved mentor hanas Gamper he
helped me a lot and
none of this would have really happened
without his guidance and support and dr.
mark thomas he gave us several insights
during the project to debug several
issues with the hardware na for the
algorithms the GUI tools that we saw the
calibration the impulse response
recording the beamforming tools all of
them were developed by dr. mark and
thank you everyone for your continuous
encouragement and ever smiling approach
and sincere thanks to dr. David hecka
man basically this this project came
into being because of him so he wanted
this device to be prototype and tested
and the algorithms to be built so thank
you so much and special especially the
hardware layer lab members know Alex
Chang and jason goldstein they really
helped us through in and out issues of
the hardware right from development
testing all of those that's about it
thank you so much any questions so back
to the big formal synthesis for both the
loudspeaker array and microphone array
here you guys use the phase information
did you get any phase information or it
was completely ah yes so that the team
matching actually uses for face and
baggage
but the question is whether we should
perhaps ignore the face in some regions
violent use for the whole what a whole
360 degrees I mean the phase response of
the transducers yeah another not here is
that technically what is interesting to
see is the multiplication of those two
this is the joint activity pattern of
the transmittance and the ratio that is
your being which actually brings another
interesting idea can we do a joint
synthesis of the transmitting and
receiving being formulas in a way to
maximize the directivity we don't hear
that if we have a big sidewalk of the
transmitted transmitter if we do have a
lot there from the ratio there was a
time when we were deciding what the
geometry repeat we were maybe angel eyes
have transmission received transmitter
receiver and arrange them in a way so
that we could try to control with
aliasing artifacts it cdz 02 to create
two in any place it is yes there's those
rates of submission look straight
forward and the other advantage is that
by keeping a separate the microphones
can be cramped into yeah 15 millimeters
diameter which your transition tells us
eventually will really a single your
microphone Gillespie career another kind
of another thing here is of course them
so we actually measure the info
responses in the setup he was showing
with truncation of the impulse response
obviously going to Danny my chamber
might help for calibration purposes and
if we want to move away from the
resonance that perhaps we will also get
so pretty much reward a softer
calibration you would range is slightly
above the resonance frequency for 20 or
50 45 to 50 compels you equalize it that
did the transmitters you here today bc
the response is hot here this is your
work area it's not going to get hurt
it's here yeah yeah we'd is definitely
take that into account and retest any
more questions so I guess me I didn't
quite understand what is the spatial
resolution of objects tent we're talking
mostly about cars and people or I walls
or anything smaller than that these the
water Hayden sure what is the spatial
resolution Oh what you mean the what
sighs what sighs objects can you detect
like I do you have to look at something
the size of a human and larger or can
you look at smaller objects so if you
know several factors come into play like
the range at which the objects are
because that decides the resolution and
a direct straightforward answer a frank
answer is we still don't know because we
got the device just about one of three
weeks back and to get the device setup
calibrated and do get some initial
results itself squared challenge but I'm
assuming so are at least the map
resolution is 15 degrees as you saw and
with regards to how many bins of those
map a real world object occupies we
still need to figure that out we still
need to test that and 120 clicking
that's one sample things do you smoke
weed 80 George came this is your
resolution in this respect you
if I cling good way to lose our killer
that's it I it is let's do you put 40 45
till your chips they can catch an insect
so it's possible yeah banging by the way
the brain of a but is not that much more
computationally powerful than the other
computer so advantage against the path
has a step off the path and the insect
are suspended in air the same well if
you attach the device to a drone well
how are the drone itself might induce
some web defense and true are the
obvious next step we're trying to
improve yeah we could we do we do
certainly take it site so this is this
was a setup that was necessitated by
mostly time constraints but if we were
to take this outside and and repeat the
experiments that I think we were killed
off a lot of it we will get rid of a lot
of reflections and then would get
perhaps a better idea of how this might
perform so priceless yeah and we if it's
an open space we will definitely not get
such responses no completely no there's
pretty much an object right around the
system at very close range is hand the
the problem is the human being is just
one of these at the same range so we
don't really know and also the the
signal-to-noise ratio is not very great
in such a cluttered environment you
don't know whether the human being is
reflecting or if it's hitting the human
being hitting a closer object and then
coming back by then the human being
might have passed but you don't even
know the neat open space amount of my
choosing behind you which is enough
large school right basically to be
equivalent of open space except the
floor yeah right okay if there are
always very good to work you made the
progress in the direction thank you
thanks for building on top of your
housework and hopefully next year
another in the rest of this right
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>