<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Faculty Summit 2016 - Machine Reading for Science and Society | Coder Coacher - Coaching Coders</title><meta content="Faculty Summit 2016 - Machine Reading for Science and Society - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Faculty Summit 2016 - Machine Reading for Science and Society</b></h2><h5 class="post__date">2016-08-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vY-CUiWTMak" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everyone and welcome to the
afternoon session on machine reading and
society my name is Chris quark I'm a
researcher here in the natural language
processing group at microsoft research
redmond and i'll be your session share
for this afternoon so we this one and a
half hour session will feature three
speakers presenting on scenarios
techniques and the broader societal
impacts of endow in computers with the
ability to read to extract facts from
natural language and convert them into a
machine-readable format and as we shall
see machine reading is poised to
transform the way humans work by
illuminating and consolidating the
world's information our first speaker is
Oren Etzioni who leads the Allen
Institute for artificial intelligence
his career includes both entrepreneurial
successes leading several companies to
successful acquisitions as well as
academic honors as a professor of
computer science at the University of
Washington and triple AI fellow one of
his papers in particular popularized the
term machine reading and helped form a
community in this important research
area today he'll be discussing some of
his recent reached research topics at AI
to welcome foreign thank you Chris for
the kind introduction hyphen for
organizing the panel and also Chris even
gave me his laptop so I can try to do a
demo it so it's unfamiliar technology so
we'll see we'll see what happens let me
jump right in because time is short as
Chris mentioned I lead the island
Institute fry which was founded by Paul
Allen he previously founded this other
company thing anyway in one of his
lesser endeavors he founded the Allen
Institute for AI we're now 60 people and
as I always like to mention we're hiring
so please keep that in mind AI tues
research methodology is to focus on
grand challenge problems we try to
define ambitious goals and at the same
time have measurable milestones we focus
on research task and goals rather than
on specific mechanism
which makes things interesting for us
and our mission is is AI for the common
good we're trying to have a high impact
on society through AI research and
engineering was real pleasure hearing
bill gates in the other session he
mentioned of the huge potential benefits
of a I also talked about the risks as
footnotes I think he presented a pretty
balanced read discussion my favorite
part though when he says that's the
stupidest thing I've ever heard I feel
like okay my I can end my career and I
have heard him say that live anyway so
so what's the societal problem we're
trying to solve it starts with what i
like to call moore's law of scientific
publication somebody in a recent book
pointed out that the number of
scientific papers published has doubled
every nine years since world war two in
our own data on computer science we see
this exponential curve so this is
computer science papers in our corpus at
symantec scholar over the last 40 years
and you see it's almost a perfect fit to
an exponential so so that's a problem if
you're trying to do academic research
right there's no Renaissance men and or
women anymore it's impossible and so
let's say I'm trying to get up to speed
on deep learning and I type that into
Google Scholar which is the engine that
most people use these days it very
rapidly comes back with more than 3
million results so more than ten years
ago when Google Scholar started the fact
that it was fast and increasingly
comprehensive was it that's no longer
the case we just have too much
information it's still wonderful to have
it as a resource but we need some way to
be more selective right to be able to
home in on the key results and that's
the focus of our work it's not just that
the number of papers is so large it's
also the number of citations each of
these papers might have
a popular one might have five hundred a
thousand ten thousand citations each of
those citations will have several
hundred citations the citation closure
is easily tens of thousands of papers at
which point you give up and you know you
just cannot do a comprehensive search in
a given area so our idea is that we can
leverage AI techniques to combat this
kind of information overload and yeah we
introduced the term machine reading back
in 2006 and tried to galvanize interest
in using the techniques that people have
been working on for a long time
information extraction etc etc to try
and do something more more ambitious and
two noteworthy things about this concept
as we defined it about a decade ago is
one that we're trying to do it with a
minimal amount of label data we all know
the phenomenal success of deep learning
techniques when you have a large amount
of label data when you don't have a lot
of label data which is often the case
when you're trying to analyze scientific
publications right you can't just you
know hand them off to mechanical turk
and have people label the data life
becomes a lot more difficult in likewise
when you're trying to analyze arbitrary
topics let's say you have a hundred
million PDF files across a wide range of
sciences and you want to figure out
little things like what are the paper
saying well you need to have very
scalable techniques to do that so so
where does that lead us it leads us to a
an engine which I'll demo for you
shortly called semantic scholar it's as
part of you know it's a non-profit it's
not a product that we're trying to
monetize AI to the Allen is to do it as
a non-profit Institute and we're just
trying to build a better more selective
engine to cut through the clutter
semantic scholar is currently only
focused on computer science papers but
we're rapidly expanding it we're going
to launch a neuroscience
later this year and moved to additional
scientific areas I'm curious how many
people have heard of semantics collar
before today Oh made my day and how many
people have actually used it okay good
good and how many people are using it
regularly okay our statistics thank you
Doug you know what are what our students
for so a significant percentage of my
PhD students are are using it on a
regular at a regular basis so hopefully
I can persuade you to to use it more
frequently in in this talk I do want to
acknowledge some of our most notable
collaborators including Doug who is our
regular user know whose whose work with
us on a number of aspects of it I'll
talk about in a little while Lee Giles
who's here in the back ng tank I don't
know if he's here but we've we've
benefited from collaborating with a
number of teams and they've been very
generous with both their software their
insights and the data resources so what
are the unique capabilities of semantic
scholar why should you use it there's
actually a lot of little things but let
me just mention three things first one
related to this issue of cutting through
the clutter we have as I'll show you in
a second facets that allow you to home
in on results so you may start with
three million results on deep learning
with a small number of clicks you can
end up with three or four papers that
you can analyze we provide overviews of
the papers via key phrases that's that
are automatically generated we identify
the datasets used in the different
papers which again helps to guide your
search another thing that we focus on is
which citations and references are
actually influential so I mentioned you
have this paper it's been cited a
thousand times you really can't afford
to look through each of those more
recent papers to find what's what's the
relevant recent work what we do is we
analyze the citations automatic
and decide which citing papers were
really influenced by by the current
paper I'll show an example of that and
that often results in a compression of a
factor of 10 or 20 in the number of
papers you have to examine and then the
last thing you know the number of actual
papers any of us even the most voracious
reader the number of papers will read in
our lifetime is limited right and at the
same time the number of papers published
is growing exponentially so part of the
idea of semantic scholar part of my
hypothesis is can we avoid reading the
paper completely can we home in on an
appropriate sentence can we home in on a
figure or table so that we don't have to
read the paper and again we provide
mechanisms to do that simply to enhance
your efficiency as as a researcher so
let's go to the demo which requires me
to see what I can do here all right so
that went well
the demo is going really well but you
you can't let see thank you okay so this
is what it looks like let's do the query
i mentioned deep learning is one of our
examples and there's all kinds of little
goodies here for example we we note when
papers are mentioned on twitter and
social media and we throw those up on
top we also give you a little escape so
if you're not happy with what you see
here with one click you can get to to
google scholar and look at what's now
3.66 million papers so what part of my
argument to use why you semantics collar
because you can always easily escape to
a more familiar ground if you need to
but more interestingly okay I've got
these millions of papers let's say in
deep learning I really want to see
yoshua bengio s work on deep neural
networks so with two clicks I've gotten
a two to four papers right that's that's
a much more manageable set that can also
as I said restrict by the data set use
the publication venue and then when I
look at the paper itself I have a whole
bunch of information about it so we have
these automatically extracted key
phrases we've extracted the figures and
tables so you can quickly cycle through
and see the different figures the remind
yourself of something have you read it
before you can look at the results use
their easy to share via social media so
a lot of of helpful information and then
this paper is from 2009 so if I want to
look at more recent work look at papers
that cited it and again you in our
corpus this paper has 852 citations but
we've been able to analyze this and
identify 41 papers that were really
strongly influenced by this paper and if
i want to see why this is ranked as the
1 most strongly influenced i can look at
these citation excerpts and see quickly
what paper a saying about paper be so
I'm just giving you the flavor of all
the different things you can do here and
all of them are based on machine
learning machine vision techniques etc
helping you guide your way through
through the paper we also don't just
have you look at the citations you can
click and look at the references and
again out of the 206 references that
this paper makes we've identified 28 as
ones that the paper really builds on and
again we tell you why so hopefully that
gives you a flavor of part of why yeah
if you want a switch back thank you
Chris okay why I'm I'm excited about
this engine and another thing I'd
mentioned is where we're very responsive
you know the old line Avis you know we
try harder if you see something you
don't like or something you want to
improve send me an email we're
constantly improving it over the last
few months since it's been launched our
traffic has been growing nicely i like
the shape of the curve it's still of
course pretty small through in the last
five months or so a little bit more than
half a million people have have gone
through the system but the number is
growing growing rapidly let me give you
a quick pic peek under the hood this is
the work that Doug Downey and his
student and I did together we looked at
this problem of Ken you automatically
identify key phrases that describe the
article if you have a controlled
vocabulary that's not too difficult but
if you don't write if the papers and an
arbitrary topic and you want to allow
arbitrary phrases to describe it it's a
harder problem here's how we formalize
it you want to identify k phrases that
best describe an article in actually
building on that then you want to have a
set of phrases that actually describe a
set of papers right and that's those are
the facets that are
showed you on the left that allow you
for example to pick DNN as a way to cut
through all the deep learning papers i
won't i won't go through all the details
here obviously we need to consider which
phrases are relevant and informative at
the same time their coverage there
redundancy we formalize all these things
in a integer linear program and we've
we've gotten some pretty good results I
want to mention also that the data set
that we've created unlike Google Scholar
is available to the research community
for research purposes again just send an
email it's on our FAQ and we'd be happy
to share our data with you so here's an
example of some of the fun topics we've
we've addressed we asked the question
can you predict citation rates in the
future right we're all aware of you know
kind of this endless fascination we have
with citation rates H indices etcetera
this is joint work with Luca ways in the
UW Statistics Department and make a long
story short we found out quite
remarkably that we're able to develop
learning models they're able to predict
H indices ten years into the future with
less than twenty percent error on
average so here's here's an example the
green line at the bottom is is our
method and likewise we're also able to
predict individual paper citation rates
quite accurately even ten years into the
future which is a little bit scary but
again just an example of a fun thing you
can do with our with our data okay so
Chris showed me I have less than five
minutes left where are we going and I
would summarize that quickly by saying
we're trying to get more and more
ambitious about what we can extract
automatically from the different papers
for example we're looking forward to
being able to type in something like
information extraction the name of a
subfield and getting a very
comprehensive overview that tells you
okay here are the different
tasks where this technique was used here
are the different accuracies that were
achieved on what data and by the way
this is not something we can do today so
this is future work and this is kind of
a a mock-up or maybe and we want to be
able to tie in the particular results
were describing to particular paper so
when we tell you a number it's easy with
one click to get the provenance of that
likewise if I type in a technique like
Gibbs sampling I can start to see the
different tasks that this technique was
used for maybe the size of the circles
represents different areas and how
heavily I was used in a different area
and so on and so on and again have an
easy way to find provenance thinking
even further ahead we're going from this
and this may be a nice prelude to hoi
Fung stock removing from computer
science and neuroscience to the
biological sciences and i wanna i always
like to put up one of my favorite lines
from eric Horvitz which is he says it's
the absence of AI technologies that's
already killing people through error
specifically preventable medical errors
right so one of the reasons that I'm
excited about this project and the
future capabilities is that doctors and
medical researchers are using very
primitive tools whether it's pubmed or
i've even heard stories of people
looking up doctors rather than just
people looking up information in the
emergency room through google scholar ok
that really does not make sense we need
more powerful technology for these
people must have already said that we
have a moral imperative to work on AI
because we're going to be saving
people's lives so important thing to
remember and then last thing from the
semantics color vision and of course
this is way beyond what we can do today
but it's exciting to think about can we
using machine reading analyze different
papers and synthesize information across
the different papers and actually
generate hypotheses and potential
experiments
and go from search engines to AI based
discovery engines that maybe can help
find answers to science thorniest
problems thank you very much we have
several minutes for questions the person
in the back thanks for the interesting
talk so one of the reasons I still go to
Google Google Scholar is the coverage of
all the other tools to don't have in
contrast what do you think is the best
way to increase the coverage of the of
the articles in the other datasets crawl
baby crawl so let me answer a different
question than the one you asked because
hey I think that coverage is a little
bit illusory right it's absolutely true
that google scholar has this phenomenal
coverage right and it feels good right
the type in a query and see that there's
millions of papers and thousands of
citations but how many of those papers
do you actually end up looking at I
would argue that it's at least worth
considering whether you're more
effective with a tool that is admittedly
less comprehensive but gives you a
stronger ability to navigate the the
paper space I don't have the data or
user study to prove that point but at
least it's worth thinking about thank
you just follow up on that so the
different representatives of different
websites are here do you think it's
useful to get a coalition of sharing the
data it's been between these scholarly
website and then have a open access
repository that everybody can confess
from and build and build their value on
that then that is such a great idea I
think we should have a workshop on this
tomorrow
sorry there's be thinking so Alex wait
is organizing such a workshop headed in
that direction I do think that something
along those lines would be fantastic and
more generally allowing open access
would go a long way it is surprisingly
challenging when you get to the details
but absolutely I think that's a great
idea and I think other people do too and
other people that i mentioned from
microsoft to sites here to RNA minor
have shared data with us and as I
pointed out our PDFs are completely
freely available for research purposes
so we're starting to move in that
direction so you mentioned ILP briefly
could you comment a bit more on what
kind of mathematical optimization
problems you would like to solve and I
imagine even the problem you have the
street here could be cast as some kind
of an optimization problems so can you
comment a bit on how advances in
optimization technology could help
further this vision well what what we
find is that actually both with
balancing these different constraints on
the key phrases you know redundancy and
coverage and inform them this and
actually a number of other problems that
we've worked on at AI too we have found
ILP to be very helpful but the the hard
part is modeling the problem in the
constraint language once we've done that
and that's something that we iterate on
successively we just use commercial
solvers so I don't know if I'm
addressing a question but from our point
of view the problem is much less in
optimization technology let's leave that
to unblocking on the name of the
commercial solvers but it's really hard
to go from a domain to modeling it
appropriately in the in the constraint
language that's something that is
currently done manually and it's
surprisingly i think finicky is is the
word
a couple of questions about the
prediction thing that predicts citation
and influence what features are useful
and would you please consider not
putting their online at the risk of
having self-fulfilling prophecies well
so a fair point so we we're not planning
to put that down line that's a research
enterprise I should also mention the
while our error rates are very good in
aggregate for any individual person who
may be concerned there can be quite a
large large error so it's not it's not a
right we're not attempting to reduce any
one to two to a number the the features
that we use there are kind of the
obvious ones including venue hama you
know the the previous right is the kind
of time series right so the previous
excuse me rate of citations and you know
things like that we've also played with
all kinds of things like the number of
authors the length of the title we've
looked at a wide wide variety of
features I think it is interesting just
to see to play with this data but we do
have to be cautious about right citation
rates and how they're used in you know
hiring and promotion decisions let me
just end because we're probably out of
time with one plug for in an imperfect
world which citation world definitely is
there are there's currently one choice
which is the number of citations or it
can be summarized by h-index but it's
based on the number of what we call raw
citations this is very very noisy easily
manipulated by self citations right all
this has been documented when we look at
influential citations it's far from a
perfect measure but I think it's quite a
bit better than raw citations because
for example we don't count self
citations we don't count throwaway
citations where somebody sites your
paper i'm in a long list of ten other
ones right we count citation is
influential if based on a machine
reading of the text it did actually this
does seem to the
paper a influence paper be so I do think
that this citation count world will be
better one if we start adopting
influential citations and potentially
even improve on that metric as well
thank you very much thank you pardon me
just a moment while we switch to the
appropriate pc all right so so our next
speaker is my colleague quite fond kun
these are also a researcher in Lannister
language processing group he received
his PhD from the University of
Washington under Pedro Dominguez and
since joining our group peace he's
eagerly pursued this agenda in applying
machine reading to cancer genomics and
precision medicine so please okay thanks
a lot thanks Chris and as you can see
later in the talk a lot of this is
strongly with Chris and other colleagues
so I'm very happy to have my talk
actually right after orange who would
introduce a machine reading which he
pioneers and also he have the last line
multi-whating the medicine that's saw
the area that we're kind of focusing
application on so I sauron mentioned one
of the key mission of machine reading is
trying to recognize important entities
and relations on text so for example
given a biomedical piece of queueing
piece of text from a biomedical
literature you might want to extract key
entity such as protein processes and so
forth and then more importantly you want
to understand the complex relations that
are expressed in the text so for example
protein can regulate other proteins and
and then but in turn they can be
controlled by some other protein in the
context of some processes like cell
suicides and so forth so there has been
a tremendous progress in a pass on
machine reading a lot of them have been
focusing on news wired tags and web
orange pioneer a lot of the work in
those to me and there is a pretty
powerful insight for this kind of me
where because a lot of the facts that
you care about their there was a lot of
redundancy for example I effect about
celebrity entity often they have been
mentioned a lot of time and they can be
leveraged to actually to actually the
tool to aid the machine reading so
relatively simple method can
go a really long way in this kind of
domain in contrast in the domain such as
I health care in solid more specialized
domain there is a characteristic is
actually quite different for example in
biomedicine what you care about probably
out so that the cutting-edge kind of
finding which by definition are not
mentioned a lot of time so there's a
very little redundancy to leverage but
on so this basically actually present a
lot of a great opportunity to to to
actually develop more sophisticated NLP
methods so one of the things I already
mentioned in this talk is sort of like a
common bottleneck for supervised machine
learning paradigm is that you need a lot
of sort of the training example to in
order to teach the machine to be able to
learn how to extract of America from the
text so it's all arching theme for our
research is actually how do we overcome
that bottleneck by leveraging this new
paradigm called distant supervision
essentially it's trying to leverage the
related structured data that is
available trying to get information from
there and I will showcase a few example
later on about how a bunch of those
challenges that are naturally stem from
the precision medicine domain but also
potentially applicable to other high
very worthy cause so building on all
these advances so still saw the snippet
of what we have been done so far is a
centering around this so-called litterin
project where we were able to extract a
lot of orders of magnitude more
knowledge compared to
traditional methods and more recently we
have been starting to expanding from the
Machine reading angle and trying to
build up a clinical decision support
which on one hand input on the genomic
data and then on the other hand
suggesting a potential decision helping
decision making for cognition and then
keep continuously learning from
experimental results and some of our
preliminary cell shows that by
incorporating this kind of knowledge
from literal and other knowledge bases
we can potentially help building good
models to predict cancer drug
predictions so I will start by kind of
motivating this cancer precision
medicine domain and then go over the
machine reading work and and and and
then so talk a little bit about this
more recent cancer work so many of you
have heard about this gray really
exciting kind of disruption happening in
the genomic work so in 2001 when the
Human Genome Project concluded we spend
billions of dollars to sequence
essentially a couple genomes nowadays we
can we can actually sequence millions of
genomes and and each of those genome
actually costs less than a thousand
bucks and if you look at the trend the
the growth current growth rate which is
the red line it's actually substantially
outperform the Moore's law and actually
in within a decade we will be able to
actually sequence the whole planet
everybody and it's not just DNA we can
measure we can also measure messenger
RNA we can measure methylation histone
modification and even to a lesser degree
the protein and so they together
actually give you a pretty good
multimodal can measurement about sort of
the the cellular software that govern
now health and disease of course the
unfortunate fact is that this is
basically at the machine code level
right so unless you can translate them
into some sort of actionable space like
the disease diagnosis and drug
development otherwise they will be of
little value by themselves so one of the
area obviously with a great value
proposition is the precision medicine
and
so the dream here is that rather than
focusing on developing those block
busting drug that is applicable to a
general population let's try to find out
what what went wrong at the individual
level and then develop jargas
specifically target them so cancer is
the particular attractive solar disease
group for this for this kind of vision
because by the time the tumor is form is
completely governed by the genetic
mutation accumulating the tumor genome
and it it is account for almost a
quarter of the deaf in us right now and
also as a population age the people who
be inflicted we will grow substantially
so here is a sort of what success might
look like right so so this this in
unfortunately individual has this late
stage melanoma that has already spread
and normally he wouldn't sawai for very
long but by taking this drug that
specifically target this mutant his
mutation is the tumors mutation in the
bureau of gene the cancer almost
miraculously melt away in in less than
four months so so this is exactly is all
of the dream scenario we want together
with precision medicine unfortunately we
are not quite done yet because cancer is
a has the help of natural selection just
like HIV and eventually figure out a way
to get by the drug and and then and then
the cancer comes back so in order to
actually fully kind materialize the the
potential of genomics and precision
medicine we're basically finding
ourselves in in sort of a kind of a
disruptive moment where there are two
palahniuk that that that are standing in
the way so one problem right now is that
cancer is driven by this complex
landscape with hundreds of if not
thousands of mutation in each tumor and
and and and in the solid get involved in
like Coney dog or the 20,000 gene so now
in principle we have the measurement for
all those 20,000 genes but the question
is how do we find the one that are
responsible for the particular tumor so
there is a if you think about sort of
all the knowledge
that we have already understood about
those 20,000 g nobody can memorize even
a fraction of them but more importantly
those kind of knowledge are not
independent they are actually
intertwined with each other because gene
directly each other and so so there is
an even bigger challenge how do you
integrate all this knowledge to do
automated reasoning so let me give you a
concrete scenario to think about this
problem so nowadays a major cancer
hospital already as routinely sequencing
cancer patients sequencing that humor
and then trying to make best decision
how to treat the patient and what they
do to deal with this in carrying
information below is that they put a
bunch of specialist in the room trying
to form this so-called molecular tumor
board and basically a bunch of
specialists trying to kind of
collectively so the manually going to
either pump map as friends and so forth
to figure out how to make sense of those
hundreds of mutations what are the
significant one and and what can we do
about them so as you can imagine this is
a very hard to scale and for example
Memorial Sloan is actually going to
sequence tens of thousands of tumors but
on the other hand they're boring
basically meet every week and can only
review a few cases every week so you're
talking about a very big gap here on one
hand we have this cutting-edge
technology that can give us this
measurement but on the other hand we
cannot actually affect it we use them so
what we really want is to actually be
able to identify some of these
repetitive processes in the
decision-making process and then provide
support to to kind of improve their
productivity so one thing that that the
geneticists and an oncologist spend a
lot of time on is so I understanding how
the gene interact with each other so you
can basically think of the pallid as a
big gigantic knowledge graph right so
the notes are basically a gene and the
gene products and then the edge of hyper
edges are basically how they relate to
each other for example one gene can turn
on energy in but in hip again and I gene
and so forth so it's a very library
programming here
and if you look at all kinds of complex
disease cancer diabetes etc they all
stand from kind of perturbation of
multiple pathway genetic pathway so here
are the sort of the famous cancer whore
marks so basically each of them you can
think about it as sort of a specialized
skill that cancer has to acquire so
cancer has to acquire all this case you
set in order to become a fatal one so
for example they need to grow a lot of
blood vessels they need to wrap okay
indefinitely they need to convey the
tissue boundary so to my gray and
metastasize so each of this kind of
module actually can be Porter in
different way they break down to smaller
modules and so far and the whole process
go recursive so by the time when you get
onto the gene level and when you look at
a statistic they become so convoluted
and that's why it's very hard to just
look at simple gene-based statistic and
try to make sense of what are the
driving mutations moreover even if you
figure out here is a driving pathway
driver pathway and you even if you
figure out okay there is an effective
drug that blocking this a driver pathway
cancer is incredibly robust because of
all those eternity pathways and can
actually get invoked to compensate for
the loss of the origin of halfway so
this is why cancer kind of why relapse
occurred so the so this sort of
motivators kind of the Machine reading
agenda of it is trying to attract a lot
of those knowledge which has already
been known but they are just scary in
the millions of papers so Oren talk
about like in computer science so we
probably think there are lots of paper
for us to kind of keep chart on suffice
to say that in biomed they have a
slightly bigger problem so every minute
I'm speaking here there are two new
paper attitude is biomedical repulse
repository and every year they add like
a million new papers so it's definitely
impossible to try to kind of read them
manually what we really want is to
actually automatically scan all those
tags and then build up this kind of
knowledge graph and which can
then be used for automatic reasoning so
what you want to do is convert those
taxes using earlier and extract all
these compacts a relation among the
entities so 11 problem for doing this
and in general like this could be my one
single slide to tell you why NLP is hard
the problem is that people are so
creative in figure out different way to
say essentially the same thing so here
is basically a few ways to say the
simple fact that tp53 can inhibit
another gene called bcl-2 so in a small
couples like involving a couple thousand
abstracts if we sit down and look at the
words that have been used to denote this
kind of negative regulation event you
can already see a extremely long tail so
if you do it in a standard machine
learning way then the first step you
will have to do is to get a bunch of
expert who know about the domain and
then start to annotate example like this
so we start from text and then you
specify painstakingly specify okay here
we're here where the entity sets and
here where the relation sees and and how
their argument structure and so forth so
this is incredibly expensive and and and
and now tomorrow if you have a if you
today if you have a couple sly talking
about like blood cancer and so far and
tomorrow you desire I want to do it in
the lung cancer in other places then a
lot of terminology will change and a lot
of the things that may not be able to
transfer so so the so the challenge here
is of course it's not just for this kind
of machine reading domains general for
supervised learning in general but it's
this problem with particular heart here
because this kind of tax is not easy to
you know just send them to mechanical
turf or average laypeople dnot also
often use have seen this kind of complex
regulation event they involve complex
sim semantics rather than a simple spam
on our spam kind of a notation so the
question is how can we kind of overcome
this bottleneck so an overarching kind
of theme of our research have been
always been trying to
whether we can see some sort of kind of
a freely available resource that is all
related and trying to co-opt them to
give us a supervision signal so let me
give you a very simple idea using a sort
of a simple scenario so because of this
half we are very important nature
basically mobilize the editors at one
time to try to collaborate with NCI the
National Cancer Institute and try to
manually read papers and put them into
database so this is an incredibly
high-quality database but unfortunately
the nature creditor gave out a couple
years because the effort is just too
much but the database is freely
available and and and we can try to see
if we can make use of them so in the
database of long it will probably tell
you for example each row it would have
your particular relation let's say that
the furrow says that tp53 negatively
regulate bcl-2 as you saw earlier so
knowing this fact along doesn't give us
a annotated example per se because it
doesn't help us where does this vacuum
from what is what where is the tax and
how is it mentioned there however if we
if we if we try to look at the West
among of the texts that we can get the
unlabeled tax in in the pub map and then
we can start to notice that okay I
already know there is a relation between
bcl-2 and tp53 and then there is a
sentence where this to entity actually
cooker so I can start to kind of
speculate that maybe this sentence is
talking about this relations so I can
start to kind of treat this as a
potential training example that talking
about this fact and obviously you don't
stop there right you don't just using
your existing knowledge base to annotate
the sentence what you actually do is to
treat them as potential positive
examples and then you can use them to
throw to your favorite machine learning
algorithm to train our extractor and
then you can now attract it fax from new
papers and extracting new facts
involving entity that are not even
specified
in the in the database so this paradigm
is known as a decent supervision and it
has has been sort of a very kind of a
promising paradigm to get over this kind
of annotation bottleneck so we have
sucessfully apply this to sort of do a
pas map scale extraction so I told you
earlier that nature editor they
basically spend two years to extract it
about 10,000 unique gene regulation
event to put in those databases by
bootstrapping from from that we were
able to scan the the palm app basically
it within a day and we we got millions
of unique new regulations and we have
already find a bunch of application
including a reason kind of breast cancer
study that that is is we will be soon in
submission for that so so so far we
talked about this distant supervision
and a lot of the existing work are
focusing on the classic simple
qualification scenario where you
basically have an instance and you want
to say okay is this a positive
regulation is this a negative regulation
or so forth but in reality you often
have tax that actually tell you much
more than that you not only that there
is a regulation event involving these
two protein but also it tells you that
this regulation is controlled by yet
another proteins so so we basically
develop the first approach that
generalizes distant supervision from
doing classification to actually be able
to extract this kind of complex
knowledge by automatically learning a
semantic parcel so one of the
encouraging initial result we got is
that by doing this we evaluate on a
particular day set where you can also
actually chain up a supervised approach
because it has fully labels so we can
actually compare how to have to a
supervised approach and of course we are
not we don't name it actually
getting at this the state of the art
simple I approach but we were actually
very competitive against like average
supervised approach and and we are using
much less information in in the
supervision there and if you're curious
basically this the kind of that the Jews
of our approach is like we start from
the knowledge base with some of the
known facts and we start with the
unlabeled text and also the syntactic
analysis like dependence tree so
essentially we basically imagining that
the semantics and notation the semantic
parts that represent those complex
knowledge is actually always out there
they are just like the latent and
notation that you don't observe in your
training data so but what we can do is
that we can essentially model that as a
hidden Markov model so essentially that
Z is like the hidden represented hidden
the latent semantic stay and then you
can start to model the emission
probability like coming from symantec if
you are in this this semantics day you
can you may be able to generate this war
and this dependency and then you have a
transition probability going from one
semaine de ce to the trial semantic
state and so one thing obviously you can
do um to recover the grammar and and
also the semantic parts so one twist
here is that how do we use the knowledge
base essentially what we do is to
incorporate as basing on lie prior to
score all different kind of semantic
parses so if they are compatible with
the knowledge base then then the the
prior like it a little bit better so
this is effectively the virtual evidence
that pineapple first proposed by Judy
pro so this is how we actually be able
to learn the semantic parts with without
direct supervision so so far we talked
about the extraction and we're limited
to explicitly stated facts but also but
often this entity have relation like
genes in the same family tend to have
similar relation and there are if if we
know that gene a irregular Jim be Jim be
recovered in seed and then they probably
they see and so forth so you can imagine
encoded this up using my first order
logic and probabilistic and using
something like market logic to do the
reasoning but the catch is that so this
this kind of approach is very general
we're powerful but the catch is that the
inference is a sharpie Hari is very
expensive so more recently we have also
exciting to explore another alternative
direction which is that we try to aim at
a less powerful less general kind of
reasoning but it can be done lightining
leaf as basically a matrix operation so
essentially we are trying to map each
entity and relation to a Wechter and
then given that embedding then you will
get a score you can score arbitrary
relation candidates triples but so the
key thing is that how do we how does
that embedding come by and we are using
a different kind of distant supervision
here trying to push up the score for the
gnome relations and so forth so there
are some promising results and and more
recently we have been looking at a
different type of knowledge that other
than just a Djinn Djinn interaction we
are actually trying to look at how about
how have a ginger interaction for
example depending on the mutations
dallas on the ALK gene a patient's tumor
may be susceptible to quit certainty or
not so we want to kind of understand
that and there are explosive result but
because of the sequencing and so forth
there are a lot of paper and new paper
coming out with the findings but the
state-of-the-art approaches basically
some oncologists trying to read review
papers and then curators knowledge base
and one of the popular source is this GD
KD and it has about 160 unique
interactions here so suffice to say that
we try to kind of a tackle this approach
one of the challenge here is that we
need to model actually the discourse
because a lot of this knowledge actually
if you look at where they occur that the
author may say that we try to start it
is the drug but then in the following
sentence you may say that
if person have this particular mutation
it may be responsive or not without ever
mentioning the drug again so we actually
need to expand our reading from a single
sentence which is predominantly what
existing work have been focusing on to
actually cross sentence and so we
develop first approach that try to apply
this general decent supervision to this
kind of problem and we get a couple
other magnitude in increase so we are
putting all this knowledge in this
literal database and which is on as we
were so it's available for both browsing
and web services so we'll just quickly
in one minute talk tell you a little bit
about a use case in basically trying to
use this knowledge to type prioritizing
the drug combinations so so we are in
collaboration with with the nikon
sensitive for the bill mmm project so
here's a problem is that all this kind
of relapse and recurrences this
basically motivated direction to try the
thing about not putting just a single
drug which may be very precise maybe
having very little side effect but on
the other hand it's very easy for cancer
to get around them so instead let's try
to think about how about putting
multiple drugs and so to sort of
preempted the cancers exclaiming route
the challenge here is that we already
have hundreds of cancer drugs and and
and even if you just think about
pairwise combination there were there
were hundreds of thousands of them and
so so the problem here is that how can
you choose a subset how can you help
oncologist to pick a subset of combo
there are particular promising and then
to do experiment with it so in a
nutshell what our approach is doing is
that we we have a fabulous of how drug
acting on a particular targets but we
are trying to augment that with the
knowledge the networks that come out
from literal because the key is to
understand that when you target a
particular gene what is the ramp
in the rest of the network and that
really determined the toxicity the side
effect and also whether simple
compensation station pathway exist and
so forth so I will skip to sort of some
of the kind of the highlight is that
when we incorporate the literal
knowledge base we were able to actually
predict the effect the frequency of the
drug in the single jar case where we can
get a pretty big improvement so so the
next step right now we are working on
Weibo Hsu is trying to actually put that
into practice with drug combinations so
amazingly DARPA actually some of you
might have heard yesterday that they
actually have this program called big
mechanism that actually have a lot of
over a lab with this dissipation and we
are power team that is led by Andrey
rosovsky who is actually sitting right
there and so another thing so so far
I've been talking about cancer just to
wrap before I wrap up I just want to
point out that there is sort of an even
bigger kind of precision medicine agenda
that incorporate all disease school for
example chronic disease which currently
caused a more than eighty six percent of
our healthcare costs and and there is a
lot of information in the electronic
medical records that could be actually
co-opted to use to actually improve to
tackle that problem and there is
actually almost a perfect parallel with
the with the literature case so here you
often have lots of information that as a
spell in the in the clinical notes that
are almost impossible to capture them in
structure from a priority by doctors so
naturally you want some sewing machine
reading to convert them to structure
form which can then be used as sort of a
big time series then you can start to
kind of predict sort of disease
progression and so forth and and I just
heard that Chris Rea have already done
some exciting work in there
space we might be actually hear some of
that so so that's the end so I want to
thank all my collaborator obviously
without them I I cannot do any of this
chris has been collaborating with me
from day one on this agenda and also and
raised very close collaborator and so
far so thank you we have time for a few
brief questions between speakers
are there any publications about the
DARPA big mechanism work where we can
see about advances in machine reading
and knowledge representation yeah that's
a good question so I can't comment on
the overall program because it's still
kind of relatively new I think we have
we have a few work some some of this
work that you have seen actually is
basically within the scope of the
program and Andre and I also have some
work that it are in the pipeline so
that's a work that Sullivan our in our
in our group but I'm sure other kind of
team that they also definitely have have
a bunch of work that in is in the
process</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>