<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Energy-efficient Scheduling in the Non-clairvoyant Model | Coder Coacher - Coaching Coders</title><meta content="Energy-efficient Scheduling in the Non-clairvoyant Model - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Energy-efficient Scheduling in the Non-clairvoyant Model</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eb4odgbZ-M8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hello it would be fun it's my great
pleasure to welcome back Dave malia who
is a professor at Duke University Dave
Mele has had a long association with da
microsoft research being intent here
multiple times and a postdoc and now
visitor and he's going to tell us about
energy efficient scheduling in the non
clairvoyant model and now most of the
work was done actually when all the
authors were here when they pulled I was
a postdoc you see was vista and z was an
intern and this one the best paper award
spa so overall it's been a very
productive effort from all of us so over
to devaglia thanks nickel so as the
convention this is joint work with yossi
Azhar and two authors who are in the
room Zi Huang and nikhil they value all
right so this talk will be about going
green and what we mean by that is we
would like to compute in an energy
efficient manner now there has been
growing interest in energy efficiency in
computing both in the theory community
which where energy is now increasingly
being treated as a first class resource
in addition to traditional resources
like space and type and also in practice
because there are several applications
such as those in data centers where it's
important that algorithms are aware of
the energy usage of a particular
scheduling plan or a particular
execution in fact scheduling jobs on
data center on machines is one of the
key problem domains where energy
efficiency plays an important role which
is exactly what we will be talking about
today and there are two kinds of two
paradigms of energy efficient scheduling
that have been popular and these two
paradigms come from the two dominant
application domains where energy is an
important resource
they represent sort of the two extremes
of computing as we know so one domain is
this our data centers which is sort of
the highest or the largest scale
computing that that we see today and
they're the dominant technique for
managing energy is what are known as
power down schedules so what that means
is that at certain points of time you
shut down some subset of machines in the
data center allow them to cool and and
restart them after a while there has
been some grow a growing body of
literature which talks about algorithms
for power down scheduling but what we
are going to talk about today is at the
other end of the spectrum on handheld
devices where again energy is an
important resource and there the
technique for managing energy is
somewhat different because you can't
quite power down devices so instead what
are used are known as speed scaling
processes so what does this mean it
basically means a processor that can run
at various energy levels so either you
can run it fast and then it it processes
jobs faster but it consumes higher
levels of energy or you can run it
slower and then it consumes less energy
but what you will take a longer time to
complete the same job okay so this is
what we will talk about today in
particular we are going to look at what
is one of the most basic problems in
this domain so you have a time axis and
jobs arrive over time this is standard
scheduling now what do the jobs what are
the jobs characterized by so jobs have a
release time at some point of time you
get to know that the job so the job
arrives in the system and you know that
the job needs to be scheduled so that's
the release time of the job jobs have an
important score which we call density
and you'll see why it's called density
soon it also has a volume this is the
total amount of computation that the job
requires and we will define the weight
of a job to be the product of the
density and the volume okay the product
of the importance and the total amount
of processing that you need to do
so these are the parameters that define
a job for me now all these jobs come
online or come over time and they need
to be scheduled on a single machine so I
just have one machine I have a bunch of
jobs arriving the jobs need to be
scheduled on the machine the key feature
is that this machine is speed scalable
so I can run this machine at various
different speeds and depending on how
fast I'm running the machine i will
consume a certain amount of energy and I
will complete the jobs at a particular
time okay so what's the problem then the
problem is to decide two things first if
I have a bunch of jobs that are awaiting
processing I need to decide which of
them I should process on the machine the
machine can process only one job at a
time okay so that's the first select
that's the first selection that I need
to do and then once I have selected the
job to be processed on the machine i
need to specify the speed at which i
will run the machine so remember the
machine is speed scalable so it can run
it various different speeds and i need
to need to tell you what speed the
machine is running at at any given time
so these are the two things that the
scheduler should tell me and the way the
speed shows up is is that it impacts
both the energy and the processing time
in particular the rate of decrease of
the volume of the job that's remaining
that that that's being currently
processed and the volume that remains
decreases at a rate that's equal to the
current speed okay so if I am running
the machine at let's say speed 10 that
means that at for every unit of time the
remaining volume decreases by 10 units
so that's the natural definition for
speed and how does this how to speed
influence energy well the influence is
through what's usually called a power
curve okay so this is just a mapping
from speeds to energies or speeds to
instantaneous powers and typically we
will assume that this power curve is a
convex curve it's a small polynomial
function of the speed so about s s cube
or s squared some somewhere in between
here is the setup clear
okay so so this is the setup now the
question is what am I trying to optimize
so at a high level it is clear what I
want to optimize I want to optimize two
things first job shouldn't wait in the
system forever so I want to optimize
delay or latency and second I should not
be running my machines at crazy speeds
so I also want to optimize energy so at
a high level these are the two things I
want to optimize energy is is quite easy
to quantify so it's just the area under
the power curves so at every instant of
time you are running your machine at a
particular speed that translates into a
particular power consumption according
to the power curve and energy is simply
the area under this curve but how about
delay how do we quantify delay the
standard way to quantify delays by using
what's known as flow times so what's
flow time it's basically just delay but
x wait so you take the difference
between the completion time and the
release time of a job this is the total
time that the job spends in the system
and you multiply it by the weight of the
job and that's the flow time of the job
you sum it up over all jobs that's your
want if that's your quantification of
delay now for the purpose of this talk
though it's not strictly required for
the result but for this talk it will be
convenient to introduce a slightly
different notion of flow time it will
make our life a little easier it will
make things geometric we call it the
fractional flow time of a job and here's
here's a way to understand what this
means take every job think of it being
split up into a very small jobs let us
say our eps Alon volume the resilience
of such jobs now for each of them you
compute the flow time according to the
formula that I that you had earlier so
each of these this in small pieces of
the job get completed at a certain point
of time you subtract the release time of
the job from it all of these jobs are
released at the same time when the
entire job shows are you take the
difference and you sum it up over all
these small pieces okay so that's the
fractional flow time of a job and in
this talk I will restrict myself to
fractional flow times rather than going
to integral proteins okay all the
results though generalize
so one way to think of the fraction of
fractional flow time as i said is is
that is the sum of flow times of these
infinitely many parts but an alternative
view is that in fact it's the integral
over the weight of the total weight of
all jobs left in the system at any point
of time so if you think about it at any
point of time which are the jobs
contributing to your flow time which are
these infinitesimally small jobs
contributing to your fractional flow
time it's exactly those jobs that are
still in the system that haven't been
processed right so this is simply the
integral over all the remaining weight
over the entire times time axis so
therefore it can in fact be drawn as so
if I if I draw the curve of the
remaining weight with instantaneously
and take the area under this curve
that's in fact the fractional flow time
of the job so these are the two things
that I want to optimize and as is
standard in the literature what I'll
actually do is oh by the way so
sometimes the fractional flow time
worked also increase because new jobs
are released and therefore the remaining
weight increases and what I do as
following standard convention is that in
fact I will optimize the sum of these
two quantities so summing these two by
the way you can all the results hold
even if you scale these in whatever way
you want okay so you can change the
scale on the tube and take any linear
combination so can you also see give you
a ball no that's that stronger that's
funded by oh yeah yeah that's not here
no that's not clear
yeah that's not real you will see where
so it's all it's almost so the part that
we are doing is going to give you by
criteria like bounds but then we will
piggyback on something that was
previously known and there the bounds
are not quite by criterion eastern
castle in order to the ranch so you will
come out of all this yeah yeah so our
jobs created yes case of preemption is
free and allowed okay so that's the
problem again any questions about the
problem okay okay so what was known for
this problem well what was no one was in
what's known as the clairvoyant setting
so in this setting when a job arrives
all the parameters of the Java revealed
ok so when when you see the job you get
to know its density its weight its
volume all the parameters and for this
setting there was a series of works
which I won't quite go through but the
key one that we will really use is the
following observation it says that if I
have a bunch of jobs arriving over time
the right thing to do is to use the
following two routes so remember why do
I need two rules because there are two
decisions I'm making with job should I
process and at what speed so the job to
process is given by this hdf rule which
means highest density first so among all
the jobs that are awaiting processing I
look at the job which has the maximum
density that's the job i should process
ok this saw is sort of intuitive right
because that's the one which will
differentially add the most to my weight
so that's the job I process and at what
speed should I process it well for that
the first rule says that you should set
the power the instantaneous power to be
exactly equal to the total remaining
weight of all jobs in the system ok so
that's the p equals RW rule
and we'll see why this is this gives
certain power people remaining milk so
take the entire remaining weight jaja SI
units goes out be missing the Sun right
so the units are already matched in the
some yeah okay and and what bansal
channel and proves sure was that this
these two rules in conjunction give you
a to competitive algorithm okay so the
total value of the objective produced by
the algorithm is at most twice that of
an optimal solution but they do not give
any the comparative ratio is not in
terms of each individual objective if it
were then what more it was suggesting
some kind of my criteria approximation
would have been possible epsilon is
where is epsilon showing up I think it
shows up in pink
I could you remember where epsilon
scales up yeah I don't remember throwing
up to a power function and they need you
kind of make it konkan our compacts and
then do a kind of Oh approximation of
vessel so that it's like maybe strictly
convex or so it's left and i'm gonna
start at the poet said had a terrific
yeah the candidates now maybe the number
of particle calls into the power
function something like this yep okay so
that's that's the rule that was known
okay so how does this algorithm work so
if you think of drawing the power funk
but the power curve over time so let's
say I have only one job at the beginning
so I set my power to be equal to the
weight of the job like that's the total
remaining weight and and then this job
keeps getting processed at some point a
new job is released at time are two now
immediately I increase the bump up the
speed of the machine right because I
want to keep this I want to maintain
this rule that the remaining weight is
equal to power instantaneous power and
also I if the second job that was
released as a higher density job I in
fact preempt the first job I take the
red job out of the system I put the blue
job in okay as you can see the blue job
has well maybe you can't quite see it
but the blue job has a steeper slope it
just indicates that it has a higher
density job that's why the weight is
decreasing faster and then once the blue
jaw finishes I resume the first job okay
so this is the kind of power curve that
the clermont algorithm gives us question
is what do we do if we don't know this
the remaining weights okay so we don't
know the weight of the job which means
that we can't quite run this algorithm
because even at time zero I need to know
the total weight okay so before I go on
let me mention that the kind of so there
are two two different models in which
you can think of non clairvoyants you
can either think of knowing the total
weight of the job when it arrives
or the total then or the density of the
job when it arrives of course if you
know both then you know all the
parameters okay and the one that we will
consider is the one where we know the
density so we know the importance of the
job but we don't know the total weight
of the job and therefore we don't know
the volume either now there has been a
little bit of work on in the other
setting where you know the weights but
you don't know the density is when
they're right and there are some results
but these we won't concern ourselves
with this okay so this is the algorithm
in the clairvoyant setting now one thing
to note here is that the area under the
curve represents both the parameters
that we are trying to optimize so if you
if you remember what I so I i drew these
two curves one was the energy the
instantaneous power curve and the other
was the remaining weight curve and I
said that the two objectives are in fact
the areas under the two curves now here
the two curves the algorithm has been
chosen in a way such that these two
curves are exactly the same because
power equals remaining weight and
therefore the two two objectives are in
fact the same area under this curve so
what these guys really showed is that
the area under such a curve is at most
the optimal value whatever is the
optimal objective and therefore they got
their two approximation now there's one
property of this curve that we will use
a lot and let me just State this
property at this point it won't be clear
why this property is important but we
will slowly see how it can be used and a
property of this curve is that if you
simply take one of these curves okay so
if you just take the red curve for
instance what does the curve depend on
it only depends on the density right if
I tell you the density then you can draw
the red curve or if I tell you the
density for the blue curve you can draw
the blue curve so if I draw such a curve
and look at the two areas the area under
the curve an area above the curve then
just simple maths shows that the ratio
of these two areas is above is
between two and three and crucially is
independent of the weight or the
densities so this curve of course
depends on the density if the density is
higher than the curve is steeper right
because the remaining weight drops
faster but irrespective of what the
density is and therefore irrespective of
what the actual slope of the curve is
the ratio of these two areas turn out to
be identical and it's about two or three
and in fact what this means is that this
also holds differentially it means that
if i draw the if i start the curve not
from zero but from some other point and
draw a little bit of the curve then the
again if i take the two sort of
horizontal and vertical segments and
compare these areas so that ratio is
also exactly 2 plus 1 over alpha minus 1
and we will call it the differential
University property this is a property
that we will need in the rest of the
term so this at this point this is just
a property of a particular differential
function ok good so let's go back to the
uniform density case so ultimately we
will look at arbitrary density is jobs
having various different densities but
let's start off by just looking at a
situation where every job has the same
density ok so this is how the
clairvoyant algorithm looks now what do
we do for the non clermont algorithm so
let me simplify even further let me say
that there is just one job in the system
there's nothing else at time 0 I get one
job and then I don't get anything more
so this is what the clairvoyant
algorithm does it's just this curve now
it's not clear what I should be doing in
the non clairvoyant case because note
that i do not really know w one at time
0 so I can't quite simulate this curve
in the non clairvoyant scenario instead
what we do is we flip the curve around
so what I can do is I can simulate this
curve in Reverse by simply setting my
power to be the process to it rather
than the remaining weight ok this is
just a mirror image
the curve on the left but why is this
any good so what was happening in the
clairvoyant case the flow time and the
energy were both equal to the area under
the curve for us well we have flipped
the curve around so the energy still
remains steady under the power curve
energy by definition is the area under
the power curve and therefore the energy
consumption is the same in the two
algorithms the problem is with the flow
time but then if i look at any time
instant the process to wait is the
height of the curve so the remaining way
it is the remaining it's the part above
the curve curve and therefore the eight
if i integrate over time then the flow
time in fact is the entire area above
the curve rather than under the curve
but then our university property says
that these two areas are not very
different that's exactly what the
University property was saying so if I
simply use the property as is i will get
as i will get a comparative ratio of
about 20 k for the single job case for
the very simple single job scenario any
questions about this one sorry well this
is not even single densities just single
job no job is being released in the
interim okay so let's make the next baby
step let us have two jobs again uniform
density won the second job is released
at time are two now life is not so easy
ideally what we would like to do is to
again flip these curves around and
similarly and run an algorithm which
which basically flips the power curves
around but can we do this not quite
because we don't know where to break on
the right right so in particular we
don't know the remaining weight of the
first job when the second job was
released to know this I have to process
the entire first job otherwise they do
not know the weight of the first job and
therefore I do not know where I should
break that curve and start again so this
doesn't quite work instead of that let
me try to play around
these segments on the curve so what I do
is I take out the blue segment firstly I
think of this clairvoyant algorithm as
the last in first out algorithm so f
note that all the densities are equal so
it doesn't really matter which job you
are processing however for the sake of
forking for easier explanation let me
just think of this as a last in first
out algorithm which means that the blue
job once it arrives at r2 preempts the
red job and that is the one which is
being processed and then the red job
resumes now let me take the blue job and
move its curve to the end okay so the
there there's no semantics here I am
just taking a part of the curve and
moving it somewhere so this is the curve
on the left and we will try to simulate
this curve in Reverse rather than the
original curve okay so what do i do I
keep running the first job till it
completes and at this point I want to
start my blue job at this height w 1
prime but what is w 1 prime it only
depends on the value of W 1 that's the
remaining weight when the second job was
released in the clairvoyant algorithm it
does not depend on the actual weight of
the second job it just depends on the
weight of the first job so that's
something I in fact dunno in the non
clairvoyant setting since I have
completely processed my first job the
red job okay so therefore I can simulate
the blue curve in Reverse at the end of
processing the first job but why is this
any good yes I have flipped it around
but I have changed things changed the
order and maybe that's messing up with
my proof to see why it's good let's do a
simple analysis so what are the
remaining weights at any point of time
in the algorithm in the clay non
clairvoyant algorithm well till time are
to the remaining weight is just given by
W 1 minus the processed weight at art to
the second job is released so the
remaining weight goes up by W to
and that's how it remains still the end
of the first job sadlier okay so now let
me split up the clairvoyant areas into a
and B this is sort of the natural split
and the nonfluent areas into a prime
which is for the first job so remember
for non clairvoyant you have to account
for area under the curve as well as
above the curve so that's a prime and
then B prime are the remaining weights
due to the second job and the energy due
to the second job so those to the
vertical and the horizontal segments are
B prime so a prime I can charge to a by
just using my University property that
was what i was doing for a single job so
charging a prime 2a is just same as the
single job I simply do that so a prime
Annie are gone I somehow need to be able
to charge be primes the two segments
that have drawn as B prime to be so let
me move the B prime up and join it with
with the other segments and move the be
back to its original location ok so now
note that what is V prime what is the
horizontal segment in B prime it's in
fact this segment the segment that I
called be extended to the end of the
algorithm these two lengths are exactly
equal it's the total time minus R to the
release time of the second job and now
what should i be using well I will just
use my differential University property
now so I want to charge the two segments
the horizontal and the vertical segments
to the area under the curve which is
exactly what the differential property
was doing for me so simply use my
differential University property and
charge be prime to be and therefore I
can infer that for two jobs I still have
the same ratio it's 2 plus 1 over alpha
minus 1
alright so where is all this going right
so somehow we have to turn this into a
general analysis until this point it's
been playing around with pictures I
cannot do that in general so this
geometry has to somehow we convert it
into an algebraic definition so let's
try to do that for that we will
introduce a somewhat new idea that we
call incremental analysis the idea is
that instead of trying to predict what
octaves or even what the final
clairvoyant algorithm is doing we will
try to compare ourselves the non
clairvoyant algorithm with the
clairvoyant algorithm on what we call
the current instance and what is the
current instance it's simply for every
job I simply take the weight of the job
that have already processed that's all I
know in the non clairvoyant setting I
don't know whether the job job has more
weight so i define my current instance
to be the processed weight of every job
and now on the current instance there is
some optimal clairvoyant algorithm
that's what I want to compare mine on
clairmont algorithm to okay so I want to
compare the objectives of the non
clairvoyant and clairmont algorithm for
this instance now what does this mean
inductively what is going for how how do
i maintain this property this invariant
that the non clermont algorithm can be
charged to clairvoyant for the current
instance so what's when I can go from
time T to T plus DT my current instance
changes and since the current instance
changes by clairmont algorithm might
completely change from some curve it
changes to a very different curve
somehow I need to account for that I
need to I need to be able to lower bound
that objective the change in the
objective in the clairvoyant setting and
charge the additional objective that I
incur in the non clairvoyant setting to
that increase in the clairvoyant case
okay so that's the general plan this is
the general plan okay okay so let's
let's see if this this is able to solve
our single job case first so what is
this thing what happens when I check it
when I
change from an instance with weight W to
an instance with weight W plus DW in the
current in in the single job case but
basically what happens is the time T
equals 0 axis shifts to the left right
so I had weight w1 so I was starting my
algorithm with power w1 and running on
this curve now my new weight is w1 plus
DW so I will start with w 1 plus DW and
again I will follow the same curve
remember these curves are only defined
by densities it's not dependent on what
the actual weights are it's really just
where you start that's defined by the
actual weight so that's the extra part
that I am getting on the left in the
clairvoyant objective what about the non
clermont objective well I am getting
exactly the same shape but on the right
now because I flipped it around so now i
can use my differential University
property to claim that the two try two
rectangles there the horizontal and the
vertical segments can in fact be bounded
by just the vertical segment that's
exactly our differential in University
property ok so you can use differential
University universality to claim that in
fact you are maintaining this invariant
that you can charge the non clairvoyant
solution to the clairvoyant solution how
about two jobs well as from time 0 to r
2 when the second job is released its
behaves exactly like a single job it
does nothing different now at our to the
second job is released but nothing
really happens in our analysis why
because the non client algorithm
continues to process the first job and
therefore the current instance has 0
wait for the second job it hasn't even
started processing it doesn't know any
weight so really till the end of
processing of the first job in this
analysis it is a single job scenario now
once I finished the first job the second
job starts getting process and then you
get back to the setting that we had
earlier so basically this this red curve
in the clairvoyant algorithm gets
shifted to the right and this blue curve
shows up in the middle
and and the blue curve also shows up at
the end there and again the differential
integrality property basically exactly
say is that the B primes can be charged
off to the B this is what we did earlier
as well and this keeps happening right
so the red curve keeps getting shifted
and the blue curve keeps getting bigger
all right so so then what is the general
algorithm so this this is the kind of
analysis that we want to do so what
should be our general algorithm well
firstly we want to process jobs in first
in first out orders so if you look at
this picture the red job is completely
processed before I even touch the blue
job so clearly I am processing in fifo
order in the non clairvoyant algorithm
okay so I want to process jobs in FIFA
order and how do i set my instantaneous
speed so one way to think of setting the
instantaneous speed is that I want both
these algorithms to end at the same time
I don't want my clairvoyant algorithm
the non clermont algorithm to lag behind
otherwise my analysis will go for a toss
so I simply set my speed in a way such
that the non clermont algorithm doesn't
lag behind it finishes the same volume
of job that the clermont algorithm would
have finished okay and that gives me a
unique speed and that's how I set it and
therefore my analysis works all right so
that takes care of the uniform density
case and that's pretty much what I want
to talk about today I'll tell a little
bit about the arbitrary density scenario
the difficulty with arbitrary densities
is that it becomes messy and this
geometric figures don't work anymore but
i'll give you a little bit of an
intuition as to what goes wrong and how
how we are able to fix it so what goes
wrong well but you know is this you know
this sensitive run its release okay
so what goes wrong well again we imagine
there are two jobs and the second job is
at a much higher density than the first
job the blue job is has a higher density
than the reg okay so when the blue job
is released it preempts the red job in
the clairvoyant algorithm it has the
highest density first rule and then once
the blue job completes you resume this
venture now what options does the non
clairvoyant algorithm have well one
option is again try to mirror these two
red and blue curves as it was doing
earlier okay what's wrong with this
densities loss which should be the right
way right so the blue guy has high
density and that guy is made to wait
from r2 when it's released till the end
of the first guy the red guy okay this
clearly is suboptimal I won't be able to
charge this additional time that it's
waiting so that's bad so it seems like I
should really move the blue guy up front
because it should be processed
immediately on release so this is what I
should be doing but there's a problem
with this as well at this point I don't
know enough about the red guide about
the red job to be running my machine at
high speed so I'll end up running the
blue job over a slow part of its curve
right because I haven't even seen the
rest of the red job so there there's
this two conflicting pools one requires
the analysis sort of requires me to go
in first in first out order but as mo it
said intuition suggests that really the
highest density job should be processed
earlier so the way we resolve it and
this is not very unique this is sort of
the first thing you would think of is to
take the jobs and group them in
densities some kind of geometric
marketing now within us the same bucket
you process jobs in FIFA order first in
first out orders so these are jobs with
roughly the same density
and across buckets you use the highest
density first rule so if some job has a
much higher density than the current job
being process it will priam the current
job but if the densities are close it
won't connect thing to do however this
sort of messes up the entire analysis
and in fact it messes up the definition
of instantaneous speed as well so
ideally what should we want to do for
setting instantaneous speed remember
what we will be doing earlier we wanted
our non clermont algorithm to come
finish as soon as the clairvoyant
algorithm finishes so we don't want an
enclave and algorithm to lag behind now
it turns out here we want need a little
more because if we are exactly finishing
at the same time as the clairvoyant
algorithm there are easy instances that
you can construct where the adversary
will force you to lag behind at some
point so in fact you need to non
clairvoyant needs to have an advantage
over the clairvoyant algorithm in this
case ok so in particular we want the
completion time of the clairvoyant
algorithm to be a constant times more
than the current time which is the
completion time of the non clermont
algorithm on the current instance right
by definition non clever algorithm
completes the current instances current
time but clairvoyant we want it to be
slower so we want to run our machines at
faster speed now one way of ensuring
this is to say that I'll make sure that
at the current time if I simulated the
clairvoyant algorithm for every job it
would have had a constant fraction of
the job still left to process right that
would have ensured that the clairvoyant
algorithm let's say will run for double
the time as it turns out this is not an
invariant we can maintain for every job
but we can sort of maintain this on
average so we can do some kind of an
amortized analysis and while this is not
true for every job we can show that if
this is true for most jobs or if this is
true on average even that is sufficient
for the final ratio that we want ok I
don't want to go into details of this
let me just state that a competitive
ratio you get for non-uniform density is
a sum constant
but the dependence is not good on Alpha
so remember alpha was somewhere around
two or three so so let's not too bad but
the de from a technical perspective we
are not sure whether the dependence
should be exponential or whether you can
probably get metal dependence okay so
let me end by mentioning a few open
questions the first one is not really an
open question but while the analysis for
the uniform density case as I saw can be
sort of pictorially represented it's
pretty easy you can do it in page or to
the proof for the non-uniform density
case actually takes about 40 pages 30 or
40 pages okay so there is something we
are not doing right there there's
probably as much simpler proof there or
maybe a different algorithm our
dependence on alpha is not great in the
non-uniform case it's about 2 to the
Alpha which might be improved it might
be possible to improve and what's
probably the most interesting thing here
is can be used this notion of
incremental analysis somehow so I think
this is somewhat new in in in in in
terms of the available techniques in
online algorithms so typically online
algorithms don't work with and a change
with changing instances it's just one
instance that they think about so maybe
this is useful elsewhere thank you
questions so as you can you go back 30
its own oh yeah hello the next one
tourism so what hole is it written out
what is the dependence on Alpha it's
sound exponent I don't remember to to
the Alpha I think the base is worse than
to you okay but some is e to play some
pasta be on some other but it is it
could be out like 10 or so it's possible
that you can get alpha squared or
something something in fact for the
clairvoyant setting well even some more
general scenarios you can get polynomial
also dependence
for single machine rates single my shoes
too but for more general general
settings in health in English in sisters
do you know any lower bound oh oh not
that I'm aware of no a lower bound
suggesting that it's harder than
clairvoyant I think the clairvoyant case
might have known as alpha changes you
can should get harder or could it be
constantly develop alpha so let's for
the clairvoyant case file you want is to
ride in a bit above yes but for the
novel in one case by alpha Chi's yeah we
are not here in fact the racial for
uniform density improves the alpha
increases actually approach to if you
have large alpha but the improvement is
not significant it goes from three today
yeah for the DNA government openness 1
over alpha minus what the treasure for
an entree one setting as well yeah just
for you only for the non polymers out
there is to it the other one does not
have a dependence on but hear you sing
it was as well I goes from three to ten
years but in this case here it's just
for this kit is growing yeah probably
saying it could be okay but it is all
good script would be we don't have a
good big three yeah we don't have been
dependent LOL alright</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>