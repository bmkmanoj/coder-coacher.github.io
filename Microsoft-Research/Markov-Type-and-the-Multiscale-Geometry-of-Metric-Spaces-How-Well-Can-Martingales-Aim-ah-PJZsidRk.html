<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Markov Type and the Multi-scale Geometry of Metric Spaces -  How Well Can Martingales Aim? | Coder Coacher - Coaching Coders</title><meta content="Markov Type and the Multi-scale Geometry of Metric Spaces -  How Well Can Martingales Aim? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Markov Type and the Multi-scale Geometry of Metric Spaces -  How Well Can Martingales Aim?</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ah-PJZsidRk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you
okay so good afternoon welcome everyone
so today we have a double feature at
three-thirty Jeffrey agreement will be
speaking now it's james lee will tell us
about Markov type and the Milky
multiscale geometry of metric spaces how
well can martingales aim ok so thanks
for coming so the talk today is going to
be about a problem kind of the
intersection of probability and geometry
of metric spaces let me begin by just
giving you a sort of a conjecture that
well the main leads which is the main
object and talk or at least a special
case of the main thing so this is a
conjecture of 0 actually ok I'll write
the conjecture but then i won't right
below it just in case the screen of nor
uhaul Paris then Shrum and Scott
Sheffield from 2004 ok and the
conjecture is the following so one has a
first of all the planar graph the graph
can be infinite or finite you'll see
that for this particular stating of it
doesn't doesn't really matter you have a
planar graph I'll use D Sub G to denote
the path metric so the shortest path
metric on the graph and then you also
have a reversible Markov chain whose
state space is a subset of the vertices
ok so this is some reversible Markov
chain on V and and it's it's a it's on a
finite its support on a finite subset of
the vertices so this is why that sort of
doesn't matter if the graph is finite or
infinite a I'll tell you the quantifiers
here in a second so you have this this
Markov chain and it's important that
this Markov chain is started
according to the stationary matter so in
fact this is going to persist you at the
top the marketing is started at
stationary and now the question one asks
is what's the average rate of drift of
this Markov chain so look at the
expected distance in the graph starting
at time zero going at time T Square this
and then compare it to so on some kind
of statement like this compare it to the
distance that the chain goes in one step
okay so this is the kind of statement
one wants the expected distance squared
x 40 steps the most constant times time
times the expected distance after one
step okay so so okay so what's the I
didn't want to okay so the point is that
see here should be a universal constant
independent of the chain independent of
the graph just you know see should be 10
or 100 ok this is this is the conjecture
that there exists such a constant such
that for all planar graphs for all
Markov chain supported on the vertices
start a stationary do you have a sort of
you have this kind of drift that you
would get for instance is from simple
random walk on the patch right on the
line yes have to be restricted to the
edges of the graph no no the most of the
Markov chain the only way the graph
comes in is through the path metric in
this in the whatever in here so it
doesn't have to welcome just a general
vitak that if you can then also the well
the the metric is certainly given by the
edges of the graph it's the path metric
so you put some weights on the edges or
you don't have to wake up you but it's
the support of the work of train could
be any subset that couldn't let
contagious shove it and how the three
metrics because you measure the distance
in terms of the geometry on the ground
so so but still happy it's a he have an
arbitrary finite metric space you can
just put it past between each pallet but
it's a plane i'm flying at you
there's a restriction on the crab okay
so let me give a couple of examples just
to demonstrate some of the subtleties
here so okay so if for instance if you
just considered some n by n grid and you
did simple around the walk then no one
is surprised by the fact that this holds
okay yeah after after T steps your
distance you expected distance to be
about square root of T away from the
starting point but suppose you the
random walk didn't sit on the edges so
you could do things like make larger
jumps okay then if all the jumps were
the same length then you also wouldn't
be surprised because kind of just by
rescaling it again would be a random
walk in the plane and the same thing
with hope so it's actually get a little
more difficult if you think what happens
when you know the sometimes the jumps
could be small and then every once in a
while there could be a huge jump and how
these there's sort of you can't kind of
rescale to get a random walk sort of on
the plane again legit or service again
you can't rescale to get a sort of
simple random walk and this is already I
mean there's a simpler proof of this
there's a fairly easy proof that it
holds for this graph just because the
subset of the it's by lucious to a
subset the plane but already this is you
know it's it's not so easy to analyze
that from first principles if you have
when you have the jumps being allowed to
vary in size okay so that's one example
to keep in mind and another example is a
look at the complete binary tree so if
you weren't paying very careful
attention the definition you might think
that this is violated for the complete
binary tree because if you start your
random walk at the root then simple
random walk will of course go at linear
speed until you know for the height of
the tree okay but the point is that this
this Markov chain is started
stationarity and in the complete binary
tree almost all the stationary measure
is that the leaves at which point the
random walk doesn't go anywhere to sit
to the leaves right so okay so these are
two examples to keep in mind so now
first let me tell you where this notion
comes from and then I'll in a little bit
get into the proof so it goes back to
this definition of something called
Markov type two to Keith ball from 90 to
define mark of type is so a metric space
this is a metric space has mark of type
p so it's a some number greater than
equal to 1 if ok so there are a lot of
quantifiers but it's not so okay so here
are the quantifier so okay so if there
exists some constant C such that and
I'll remove these quantifiers for most
of the talk so but so okay so that for
every finite state space so this is some
finite state space for every reversible
Markov chain on this state space ok
let's start start at stationarity for
this and for every time we have this
kind of dependence ok now i have this
map oh I guess yes okay for every I'll
explain this in a second okay so take a
finite state space take a map from the
state space into your metric space take
a reversible Markov chain on this date
space and then ask for something like
this to hold ok so it's the same thing I
wrote before for planar graphs except
now I have
I put a power P instead of the power P
equals 2 and here i had the the markov
chain walking around on the graph
whereas here it walks around on some
auxiliary state space and then there's a
map from that space into the graph this
is the right definition because well
it's i'll explain in a second it's the
one that has the correct applications
it's also sort of composers very nicely
but for most of the talk i'll actually
emit this map f and just pretend that
the random walk is take is walking
around the metric space sort of i don't
know of any you know I it could actually
be that if you that you could remove
them actually I suspect it's probably
not true for some trivial examples but
so for most of the talk I'll admit the
map app but this is the proper
definition in other words the the Markov
chain has more state than just where it
is in the metric space it also could
have some auxiliary state sort of coming
from this mess say it again you seen
that hey yeah so uh yeah Oh No so seed
see could depend on the metric space so
but C cannot depend on any of these
other data ok so see is it probably so
the weather metrics you know this p is
the mark of type of the metric space the
best people which this holds and there's
this constant also is a property of the
metric space and let me okay so where
does this concept come from personally I
mean it's a fairly natural you know wait
to talk about the drift behavior sort of
on finite subset of a metric space okay
but it it's inspired by a sort of
something in the in the linear theory so
in the end of the geometry of banach
spaces so okay so suppose I have a
normed space X and then I have a bunch
of vectors in XX 1 2 to the X n ok so I
could consider a random some that looks
like this where the epsilon eyes are IID
random signs so I take I take sum norm
space take a bunch of vectors and then
randomly thumb up the vectors with with
signs and now
okay so i could for instance measure you
know okay let's just use the square for
a moment i could for instance measure
okay so this is a red comes I'm going to
random walk in this linear space I can
measure the distance I go after sort of
n steps according to transm walk and I
could compare it's a two to something
like this okay so this is where you can
think about this as kind of this is the
length of this is the sum of the lines
of the squares of the individual steps
of the walk okay so if a Banach space X
satisfies this for some constant C
independent of the choice of vectors an
independent the number n the space is
said to have type 2 okay and there's a
sort of similar notion of if you reverse
the inequality you get the notion of co
type and it turns out that sort of this
these this parameter sort of type and
these parameters type and coats I mean
if you change the content to to a to a
power P then you get type P and code
type P actually tell you a lot of
geometric information about the the
Banach space here in okay so okay so let
me give you an example and I'll tell you
why Keith prove this so for instance the
following is known as the maury
extension you know so take take two
banach spaces x and y and then consider
the following type of question okay and
let's actually also fix some subspace of
X and then consider the following type
of question I have a bounded linear
operator from a sin to Y so I said
linear operator with bounded operator
norm and I'm curious about when I can
actually get an extension of this
operator to the whole space X okay such
that it remains bounded so so this one
is bounded and I want this one to be to
be bounded as well okay an extension so
it's an extension in the sense that if I
restrict the new map to the subspace and
it's equal
old map okay so you can ask sort of when
do such extensions exist and more
improve that such an extension always
exists just given the behavior you sort
of just given information about how
random walks behave in x and y so if X
has type 2 okay so if X satisfies an
inequality like this and why has code
type 2 and for any bounded linear
operator this extension always exists
and this gives kind of extension is a
very powerful thing you can you can you
can do a lot with so what keith was
trying to do is actually something on
the face of it seems kind of ridiculous
is there's this a sort of beautiful
history in the nonlinear geometry of
banach spaces of linear notions okay so
this is I mean I said it in words but so
this is a bunch of linear notions I have
linear spaces and subspaces and linear
maps as having nonlinear analogs uh
let's sort of by all accounts see one
shouldn't expect to have except for the
fact that we happen to live in a world
that's nice enough that somehow this
kind of thing works out and so what what
keith was trying to do was to give a
nonlinear version of this expansion
theorem just in the category of metric
spaces okay so so what is it these
things mean in the cattery of metric
spaces let's just suppose we have two
metric spaces x and y and now we'll have
and okay so now we'll have this s but
now s is just a subset of X so I'm
subset of X and instead of a bounded
linear operator will have a Lipschitz
map
from this subset into why and now the
extension question is whether this map
extends does there exist an extension of
this map to the whole of X okay which is
which is Lipschitz so for instance it's
a classical result that if X and wire
Hilbert spaces the result of cures Brown
that such an extension always exists and
in fact you can take the Lipschitz
constant of the extension to be equal to
the original Lipschitz constant here's
what here's what Keith proved that if X
has mark of type 2 and Y has ok so Marco
code type to Marco top is not defined
but you can just eat this is already
interesting if Y is just a Hilbert space
so hey so this is hold for wiping a
Hilbert space so XS mark of type 2 and y
is the Hilbert space or has mark of code
type to which I'm not going to define
for various reasons then this kind of
lipsticks extension always exists ok and
I'm just trying to give this to to get
the set incorrectly if you were really
sort of one knows going on they always
exist in extension such that the new
Lipschitz constant is is is the most
some constant times the old Lipschitz
constant and that constant depends on
just on this constant in the mark of
type definition so this point is it's a
quantitative relationship as well ok so
this is you know sort of you go from
looking at the geometry of spaces in
linear spaces into in terms of these
linear random walks there's this
metrical definition and there's somehow
this beautiful it's you know extension a
analog of the linear extension theory to
the nonlinear setting ok so I mean this
is just sort of one example of the
applications of this thing yeah the
shoes a metric space is not off my top
two ok good
so let's talk about so far Denis I
didn't give any example of next places
that are mark of type 2 so okay so so
what did Keith so okay yeah so there was
one problem with Keith paper it's a
beautiful paper he proved the following
theorem he proved that Hilbert space so
L 2 has mark of type 2 and in fact with
constant C equals 1 and the definition
there i guess i should mention by the
way that mark of type 1 is trivial so if
you take p equals 1 then this follows
with with constant one just by using the
triangle inequality linearity of
expectation and then the fact that
you're at stationarity means that every
step of the walk is actually distributed
the same as you know sort of FZ 0 SZ one
ok so so P equals one is trivial just
follow from the triangle inequality
right so--okay so the yeah this theorem
seems great the only problem was that
this was the only the hilbert space was
the only space for which keith was able
to prove mark of type 2 so if there was
if this was really a nonlinear sort of a
hardcore nonlinear generalization of
type and co type than you would expect
that you know there would be other
spaces besides just Hilbert space but
had this notion ok so I'll come to you
I'm coming to your question ok so so now
let me mention sort of so this was the
situation until this work of nobody
written your names elsewhere so now this
will be MPs s right so they proved two
things first of all they proved that LP
4p bigger than 2 has mark of type 2
which is which is like a very comforting
thing because those spaces have linear
type 2 so if this was really a nonlinear
generalization and you would hope that
when you restricted a linear category
you sort of get the linear theory back
so so that's great and they also proved
that trees have markov type 2
and for I guess for especially for you
vol it's a very nice kind of so this was
in in the Indus insertive in the
functional analysis setting this was
kind of the most important open problem
after keys work to ought to to resolve
the Marco type of LP 4p bigger than two
I believe that they started actually
just working on the problem for trees
and it turns out that actually trees are
a bit harder in some sense then lpp
bigger than two so once they solve the
tree problem sort of this problem fell
rather ok immediately ok so there are
other spaces of markov type too what's
an example of a space it doesn't have
mark of type 2 ok so so l1 it let it
even say it I'm different l1a has only
trivial Markov types only only mark of
type 1 it has nothing better than one ok
so why is that so justjust take the ok
discrete hypercube sitting inside l1 and
just takes the standard random walk so
simple random walk on on the Hamming
cube right and now it's it's it's
straightforward to see that say the
distance from your the starting point to
your ending point after say n over 3
steps this is this is at least an overt
n with high probability alright so this
immediately shows you that this space
has no nontrivial markov type because
after n steps it's gone distance n so
certainly will not be the end and of
course every every step goes distance
only one so the right hand side here is
one the left hand side is after T steps
but for T equals n over 3 is gonna be
like n it grows linearly it doesn't grow
at and sort of any slower than that ok
so this shows that l1 has only trivial
mark of time ok and I guess it's a good
point to sort of point out one other
thing about Markov site now which is ok
so let me let me do the following so
both x and y are metric spaces let me
just write down what it means to be
people people know but this sort of the
map map from X to Y is x lib sheets okay
if it preserves all pairwise distances
so all right I'm going to cheat a very
small amount here okay so so let's say
this a map f is x lipschutz if it
preserves all the distances or to a
factor of see this this quantity them
will call the pileup the sort of the
infamous see such this hope so called
the biologist distortion of the mapping
and one thing that's fairly apparent is
that this mark of type is the bite of
cheese in variant which means that if I
have a metric space X that embeds into y
by Lipschitz Lee and why as mark of type
2 then x also has mark of type 2 okay
and that's just because if you look at
the definition if I can change all these
values up the constants and the you know
the statements is still true okay as
long as I changed about the universal
constants not depending on the chain so
if I can embed one space into another
space and the in a pile of sway in
the new space has mark of type p then X
inherits this from the new space okay
and in fact in a quantitative way right
so if if I could in bed one space to
another with distortion okay then you
would have the sort of same kind of
inequality okay I guess all right
okay this doesn't matter because which
is the constant of raising to therapy
but if I want the constant to change in
proportion to the distortion I guess I
should put a power P here but the point
is its quantitative so if if the map has
distortion d then the new then then sort
of X will inherit mark of the mark of
type of why but now with the constants
for where this content is grown by a
factor of d okay so in particular you
can conclude for instance from this
argument that the Euclidean distortion
so the the distortion required to buy
Lipschitz we embed the N dimensional
cube in the hilbert space grows like
square root of the dimension okay so
there are many ways to prove this it's
been known for over 50 years but this
this gives you an example of how mark of
type comes into play also when you think
about the sort of by lucious geometry of
metric spaces okay this is a very simple
example but by studying random walks you
can sort of really compare the geometry
of two spaces using this kind of notion
okay good okay so now let me return to
the conjecture so so i'll prove i'll
prove this to you now well in a moment
but different from keith's profile I'll
follow sort of MPs s and then you'll see
why the second sort of the subtitle of
the talk comes into play why why
martingales become a key object of study
here okay so ask me any questions if you
have any at the moment all right okay
alright so ok so now coming back to the
planar graph question why hasn't sort of
the theories of our answer this question
well one reason is that we know that
plant that there are planar graph
metrics which do not buy Lipschitz embed
into hilbert space and they don't buy
the puce embed into l p4p bigger than
two so you can't use any of this sort of
embedding machine we talked about to
solve this question let me tell you one
thing we do know about planar graphs
obviously we know a million things about
planar graphs so let me tell you a sort
of one kind of embedding we know they do
have and then i'll tell you our main
result so ok so one more notion of
embedding and then we'll move on to
marking accounts alright so this is a
notion of a threshold and then so so we
have again to metric spaces x and y and
we'll say that X threshold embeds into
why ok so if there exists a constant K
and now this threshold embedding is
going to be a collection of mappings not
a single mapping so it's a family of
mappings from X to Y the family is
indexed by some non-negative real number
I guess it's a family on one Lipschitz
maps which satisfy the following if the
distance between X and Y and X is at
least towel ok well if I had a single by
Lipschitz map and the distance at least
I'll then if this into the image would
be at least tau divided by some constant
times some constant but here the sort of
only one of these maps is required to
notice this so in this threshold
embedding
okay we have this kind of condition
these are all supposed to be towels even
all right okay so X thresholds embeds
into y if there is this a family of one
Lipschitz maps basically one for every
scale of the space so there is no Matt
there's no global map that sort of can
get all of geometry right but at scale
towel v sub towel reflects the geometry
at scale towel up to a constant so if
the distance this is for all x and y you
next but if the distance in x is at
least I'll the distance in why should be
at least Talib to a constant factor in
this case this case the universal
constant okay so this may look a little
bit strange it's there are plenty of
examples for instance even the complete
binary tree old result of organ shows
that the complete binary tree tis not
embed into any hilbert space but it does
threshold embed into hilbert space if
you want to see i can i actually i won't
do it now but if you want at the end or
offline i can I found at least a fairly
simple description of the threshold
embedding of the complete binary tree
into hilbert space if you want to see
one of these things looks like and
actually especially in a number of
applications in computer science
understanding relationship between these
threshold embeddings and by lipschutz
embeddings was very important okay so
you know sort of there's this ongoing
theme in in many parts of metric
geometry of understanding sort of when
the ability to control a space at every
scale uniformly it sort of implies the
ability to control it somehow at the
same all the same time all the scale
simultaneously okay so this is the
notion of a threshold embedding and
here's just a I shouldn't call it a fact
it's not obvious a theorem that sort of
planar graph metrics threshold embed
into Hilbert space so you can take that
so I'll add the word uniformly
so uniformly here means so you can
interpret this theorem in am always if
you take an infinite planar graph metric
it threshold embeds in Hilbert space if
you take a collection of finite planar
graph metrics they all thresholds in bed
with the same constant K the constant is
uniform doesn't depend on the ok so and
I won't go into it now but actually
along with this conjecture they asked
about other spaces like doubling spaces
you know other even things sort of like
hyperbolic spaces spaces of bounded
nagada dimension okay that wasn't
specifically asked there but the point
is that all of those spaces admit
threshold embeddings in the hilbert
space okay so sort of it's not just the
planar graph question although that's 1
i'll focus on at the moment okay so
that's so this is one thing we know
about painter grass and now here is the
main here's the main theorem which i'll
try to present in the remaining time if
a metric space threshold embeds into
Hilbert space then this space inherits
mark of type 2
ok so for by Lipschitz embeddings it's
straightforward but it turns out you
actually need much weaker control on the
geometry to get mark of type 2 okay so
just the existence of a threshold
embedding is enough to get mark of type
2 and for any sort of enthusiasts or
junkies you can you know you can sort of
generalize you say if you threshold
embed into AP uniformly smooth Banach
space then you have mark of type P so
there is sort of a generalization beyond
Hilbert space but this is sort of the
most interesting thing because it has
applications it shows the planar metrics
and doubling metrics and sort of
hyperbolic spaces and so on they all
have markup side too ok all right so now
I want to give you some idea of how this
theorem is proved and now ok so now
we'll go to probability all right wait i
thought it was clearly define i thought
that was for universal see them yeah so
this is ok yes so i guess this is
something people do quantitatively in
other words the the mark of type
constant only depends on the content in
the threshold embedding and so the fact
that this is uniform it does imply that
you get uniform constants okay you're
right so it's weird if you it'd actually
does imply it just by itself you can
actually take all the planar graph
metrics and just and just put them all
in one single giant planar metric space
you don't even care if it's separable
and I mean it so if you can just and
then and then the fact that I just have
it for a single graph means the whole
with uniform constant but ok all right
so now let's start with something easy
let's see I've have 20 minutes is how
long have I lysol ok let's prove this
fact that was first proved by Q Bob will
prove it in a more complicated way that
the rail line has mark of type 2 ok this
is our goal uh ok so recall what happens
we have this we have this Markov chain
which is sort of has it state spaces
finally many points on the real line and
this thing hops around and we want to
prove a something like this with P
equals two ok so how can you do it well
there is one situation on the real line
where we know we get some kind of
behavior like this and that's if we had
a martingale right so suppose that we
have some some real valued martingale
then by just orthogonality of martingale
different sequences this the expected
distance square after T steps is exactly
the sum of the distances squared in the
individual steps okay so at least
martingales satisfy you know the kind of
growth that we're looking for okay so
this is not a martingale but maybe its
kind i mean it's it's a you started
stationarity right so you know if you
run it for long enough it doesn't go
anywhere it's this you know the sort of
the center of a mass stays at the same
point because you started stationarity
ok so the way to bring Martin golds into
the fold here and this is what NP SS did
based on some work of lions in Zhang
Zhang TJ lions ok so let's try to
convert this this this Markov chain into
a martingale tts Zhang
okay let's let's try to convert this
chain into a martingale so all right so
here's that ok so the first step we can
do pretty easily all right we'll start
the martingale at zero good now we're
the next step we get into trouble so
let's let's sort of try to define the
different sequence okay so so if the
world was sort of as we might hope and
expect we would just define our
martingale difference to be the
differences of the values in the chain
you know and that you know if this was a
martingale we'd be done so we'll be
really happy okay because if you know
then this would this would hold that
would immediately give us our inequality
of course it's it's not so okay let's
just add the appropriate correction term
write it for this to be a martingale we
want that the conditional expectation of
this is 0 so let's just force it to be 0
okay so the difference is here are given
by how the how our chain behaves minus
the defect of our chain from being a
martingale okay okay so let me tell
there is a good there's good things and
bad things the good thing is that we can
control by the way tell me if I I won't
write any lower but tell me if I start
through it the good thing is that we can
control the increments of this
martingale in terms of the increments of
the chain so this squared is that most
sort of twice the exhalation of this
squared plus twice expiration of this
squared so sort of all right can write
something like this and then because
we're at stationary this is four times
the do mine is at one squared okay so
I've given you some giving you a
martingale at least I can bound the
differences in terms of the differences
of of Z so all right the problem of
course is that what I really want to be
able to do now is say well if the
martingale doesn't go very far 40 steps
then the chain also didn't go very far
40 steps this is problematic because in
every step this martingale picks up this
sort of extra Croft that sort of from
the non martingale mess of Zen
so now here's the beautiful step I mean
that we have the reverse with Markov
chain at stationarity so we've been a
little bit sort of like we broken
symmetry bye-bye only tracking it from
the beginning right because it should be
the same run backwards in time so in one
sense i'll say the idea and then we'll
write it down but the idea is now to do
the same thing would have another
martingale that tracks the Markov chain
backwards in time and then by the magic
of averse ability when we take the
difference of those two martingales all
the crap will cancel out and will you
know sort of when they you no one is
going forward and that time one going
backwards I'm kind of when they meet
every this extra stuff cancels out okay
so what do I mean so here's here's our
backwards in time martingale if it
starts at time T and then the
differences are given by t minus s minus
1 i'm just trying to the transformation
is just t minus this
okay so the point is here's another
martingale with respect to the backwards
filtration and in and it satisfies the
same argument gets sort of it satisfies
that it's increments are bounded by the
increments for for Z and now here's the
cal state as a lemma but if i look at
dead s plus 1 minus z is minus 1 this is
exactly this minus b plus 1 okay so this
is my claim that this is equal to this
no no good okay it's not divided by 2
uh-oh ok let's check it uh so so for n
what do we get we can just plug it in I
mean you know it's not divided by two we
get this forward back for the
mathematician but you have it the time
it makes difference by two on the left
yeah so there's a gap of 2 on the left
that's the right Tony discrete case
there's a parity issue so there's a
different phone continues continues in
what you have to divide by two right in
the discrete case which is squeezed it's
a it's actually little uglier because
you have this this gap but okay so you
can just put here's the okay we'll just
do the calculation so you get Z X plus 1
minus z s minus expectation of this
conditions on this and here you get okay
so here we get z s minus 1 minus z s so
again you so you take t minus this and
you get s minus one and you get minus
expectation of this ok and now ok now
we're subtracting this from this so this
cancels okay here we get the right
things that s plus 1 minus it s minus 1
okay zed s cancels and now we have a
verse ability the expected value of Z s
plus 1 conditions on jess is exactly the
same as the expected value of Z s minus
on condition that is s so those that
cancels as well okay so though so that
sort of the differences in Z up to this
annoying parity issue can be represented
by not it's it's not a single martingale
but it's the difference of two
martingale different sequences and so
what this implies okay so there is a
parity issue that i'm going to not gloss
over because it's messy is that we can
write z t minus a 0 as a difference of
two martingales 80 and BTW where these
the increments here are are bounded sort
of the squares of the increments here
are bounded by what they were in in zip
a and B are almost MN except for the
fact that you have to correct for the
you know after the correct for the
parity issue okay so then this
immediately tells us what that expected
value z t minus 0 squared is that most
well twice expected a t squared plus
twice expected VT squared but now these
are martingales and they're different
sequences are bounded by this so
immediately we get okay there's some
constant times T okay all right so there
we proved so that's the end of the proof
that the real line has marked off type 2
okay and it was it was by taking our
Markov chain decomposing it into a
difference of two martingales and then
using just a straightforward bounds for
the marking girls okay so two things to
observe first of all the proof didn't
have to be on the real line actually the
one of the beautiful things about the
prove is that it only uses addition and
multiplication it only uses addition and
subtraction okay it doesn't use
multiplication at all so even if they
use multiplication it would still work
it would still go from the real line to
Hilbert spaces but since it uses all
addition and subtraction actually you
can generalize it to arbitrary normed
spaces with the cavi okay so there's one
part that this is the part of the proof
that uses multiplication actually the
idea that martingale different sequins
are orthogonal that actually involves
inner products so this is what fails in
a general normal space but everything
else carries through okay and actually
witness machinery now for instance if
you have if you can control martingale
different sequences in LP spaces and you
can prove that LP has mark of type 2 for
people get into which is what MPs s did
so now let me try to tell you the
difficulty that arises improving the
main theorem here ok all right so we're
going to follow this formula but then
we'll get stuck ok so if if we have our
space X and we said suppose we had a
suppose we have a bye Lipschitz map from
X into l2 I can kind of you know sort of
the point is that now I can all right
and I also have my my Markov chain
taking values in X I can use this ok so
we have to again you have to believe
that everything I said here works if you
replace our by Hilbert space you know
just go through and check ok it does
work I mean it's all it's you know
there's only addition and then maybe
like the Pythagorean theorem ok ok so we
can so we can we sort of take our by
Lipschitz map we would write the this as
a as a difference of two martingales in
Hilbert space and now just apply
everything as before and we would sort
of conclude that ok so what do we
conclude expected value of this is at
most some constant times T
okay times this this is this is all in
Hilbert space this is the l2 norm and
now use the fact it's by Lipschitz on
the right here you just replace this by
you know okay what's going on in the
metric space and on the left here I
claimed the same thing replace it but
what's going on in the metric space
there's a reason I'm doing this you know
okay I've already claimed that by
Lipchitz preserves the so the property
but so okay so I mean here so you would
first map your space in the Hilbert
space you would write sort of the Markov
chain under this map as a difference of
martingales and then proceed as before
the problem now is that we don't have a
buyer she's in bedroom you have a
threshold embedding so we only have
control sort of for every scale he has
use a different map which means that for
every scale we actually get a different
martingale right so now if my threshold
embedding I unfortunately of course the
race the definition I'll recall in a
second but if I look at sort of my map
for that that's able to control scale
towel this is now all sort of i can
again write this as the difference of
two martingales but the martingales
depend the martingales depend on the
scale of the mapping that i'm using okay
alright so now let's try to use that to
prove it and then you'll see where the
main difficulty lies and i'll tell you
how to resolve it and then we'll be
we'll be done okay so this didn't make
sense let's just okay here's our setting
we have a metric space X we have this
Markov chain Z and we have this family
of mappings from X in the Hilbert space
so these are all Lipschitz and they have
the property that if the distance in x
is bigger than tell then the distance in
the hilbert space is bigger than how
divided by some constant ok and now
let's try to use this to prove some
bound on
expect this value of this so these are
we care about we're trying to prove that
the existence of this bedding gives x
mark of type 2 so we should be able to
prove enough about on this thing okay so
now here's sort of i'm just going to do
kind of the most obvious thing i want to
control this but i can only you know I
can sort of only control one scale at a
time okay so let's first just write this
in the following way okay okay let's
sort of write out the expectation
squared in terms of the tail so now at
least I know sort of i can write this in
terms of the events that this thing is
big okay that's going to allow me to use
my mapping at scale lambda to say
something about something in Hilbert
space right so now I can just say this
is that most probability okay so now i'm
working at scale lambda and have a k
here ok so now here i've just used the
property that when this is big and
implies that this is big okay when this
is bigger than lambda plus this one's
bigger landover k okay now i know that
for every lambda alright I've used am
using lambda so let's put the lambda for
every lambda I can this is a mapping
into Hilbert space so I can write it as
a difference of two martingales for
simplicity in the sake of time let's
just pretend i can write as a single
martingale i just cut down the number of
terms by two so let's just use a so we
can now bound this by probability that
this martingale ok now it's indexed
okay good this is where I want it to get
ok so now i sort of i can bounded by
this kind of weird thing I'll this is
very strange because well first of all
it's not even necessarily measurable but
ignore that for the moment this is very
strange because at every point in time
I'm considering a different martingale
okay so I mean if these were just you
know if these were arbitrary and
martingales or even martingales with
just bounds on sort of their their total
l2 norm or something I would be out of
luck here the only real benefit i have
now is that I know the way these
martingales were constructed they all
live on the filtration that follows the
random walk around the metric space so
they're all defines work with respect to
the same filtration and that brings us
to the following I'll state a theorem
now and then you'll see how this
sort of basically I have some kind of
bound on the increments of all these
markov chains but there's a bunch of
them and they could all use those
increments in different ways so this is
where the part about martingales aiming
comes into play sort of okay so yeah so
let me write down the here's the theorem
okay so i have um and I'm just abstract
about what we know here so we have some
common filtration of our probability
space and we have some random variables
alpha T which are adapted to the
filtration and now consider say a family
of martingales let's index them ok I'll
index them by ok some index at I ok so
this is some family of martingales and
what I'm going to say is that ok and
they're all day sorry all the
martingales are we just back in this
filtration I can bounce all of their
different sequences uniformly in terms
of alpha ok so I have a bunch of
martingales and what I know about them
is they all sit on the same probability
space and I have this this is this is a
random variable but has the same random
upper bound and all their differences ok
least random variable is the one coming
from the random walk this is essentially
how far the random walk goes in the
metric space and that upper bounds how
far all these martingales can travel ok
and now what I want to be able to say is
the following ok so this is the
assumption then the integral ok should I
all right i'm gonna use why instead of
lambda good ok of this
times the supreme over all these
martingales I'll take the worst possible
tail and let's okay so here's what i
want to say i have this integral which
is it's the same in to go here i have
just replaced lambda x y just so that
there's naught lambda here is tied to
the martingale as well here I've taken
just a supreme over all these possible
martingales if i took the soup outside
then there would be an obvious volunteer
if the soup was outside then this is
just what's inside is just the expected
square of this that's what's inside if i
take the soup outside it's the soup of a
bunch of over a bunch of martingales but
the integral is just the expected square
and then as before we can just bound it
by okay t's 11 yes okay so if i take the
soup outside then i have the bound that
i can just bound this is just the two
norm of the martingale is just the
expected value squared and i can bound
it by the expected values of the
increments ok so the novelty is that the
Supreme comes inside but it's still true
with some universal constant see out
here ok so this is in some sense at
least for here I've done it for real
diet martingales this is maybe the main
technical step to analyze martingales in
the real case and ok so i just wanted to
sort of express the difficulty of what's
going on here again if you get rid of
the soup and you had a single martingale
then this would follow immediately from
this is just the expectation squared and
you get it immediately from the fact
that the expectation squared and most of
some of the difference is squared but if
I allow you to take the soup it's not
clear at all sort of why these
martingales can't sort of each one try
to aim for a different point in the tail
right like suppose I even only have two
jumps you know I have a big jump I take
with some small probability and a little
jump that I take most of the time so
okay so now my martingale scan at every
step i have a whole family they can do
it they can take either of these two
jumps so i can consider for instance the
martingale that always takes the small
jump to the right and the
big jump to the left well I can consider
a martingale that picks uniformly at
random and sometimes takes a small jump
to the less and sometimes to the right
or the martingale that you know goes the
other way right and the question is
whether sort of if I give you some tail
some tail down why to aim for can you
can you sort of conspire so that your
martingale manages to sort of use all
its l2 norm just to hit that particular
value of y ok so well the answer is no
ok so so if there's uniform control even
if you take the soup over all these
things ok so this was the sort of how
well can martingales aim not that well
ok so like they can't the a martingale
can't conspire to use its all of its to
norm to sort of just managed to get to
why ok I mean they're subject to the
same difference constraints all right so
this is the main theorem what's the
proof well it's essentially due to work
of Burkholder and Gundy from the 70s
although that took us quite a while to
realize so if you don't I mean they have
some very beautiful techniques for
analyzing martingales using very clever
stopping times that allow you to prove
this kind of result ok so let me know if
it's not clear I'm gonna end is just a
second ok but this is the main kind of
thing that comes up here when you have
these thresholds embeddings now for
every scale this different martingale
and you have to somehow control them all
uniformly and it works but it's you know
it's perhaps counterintuitive yeah we
both interviews because it's not the
standard so it's not just ok so it's odd
actually didn't appear in his book it
does appear in a survey paper in like
around section 11 ok what's the
ok so here I'll state the the serum
alright so let's see so I have a
martingale and I want to make sure and a
neck in this case I need some lower
bound on like this ok ok you need some
lower bound on some kind of thing that
says you make a move often enough ok so
this is a real delight martingale I have
some bound like this and then let's
define the square function just to be
the sum of the squares of the increments
ok so this is sort of what I know about
the martingale and now here's the claim
alright so let me let me write it down I
guess I should do this did you introduce
yeah ok so i'll write it down and then
i'll say what it what it says ok so for
any sufficiently smooth function one has
the following kind of control okay so
this is the maximal process associated
of Martin guess I have a martingale I
look at this I look at its maximum value
so I'll say what smooth means in a
second but for any sufficiently smooth
function the maximum value the the fee
of this maximum value is controlled in
terms of fee of the square function okay
sound of a very strong inequality and
what kind of functions can you use so
this holds as long as sort of fee is
doubling in the sense that I guess when
I ok so there's quantities quantitative
argument here you assume fees doubling
with some constant lambdas when I double
the argument the value goes up by a
factor of lambda that gives me some
constant c lambda such as this holds ok
and the interesting thing is i mean you
can't use this to bound the tail
necessarily because you can't use cut
off functions but if you if you look at
this integral you don't really need to
be able to bound the tail precisely you
all need to be able to bound it up to
something which is sort of up to third
order because it's sort of a integral of
computing something a second order so so
just by making feed drop off cubicle e
that's enough to write which is going to
be doubling so if it features the cubic
function you'll get something doubling
that's enough to control this integral
ok this is the burkholder gundy theorem
it's it's beautiful and it somehow has
to sort of cope with this and it does
using using using magic so I mean it's
it's sort of like right well
burkholder's like you know has a survey
we has all these techniques and there's
sort of this you know it's a really
magical stopping time argument that gets
this to hold ok so I have to end now so
I just but I did want to sort of give
one open question so ok so this was a
question about whether or not
martingales can sort of aim you know use
even subject to some bounds kind of aim
to be at a particular point let me just
say to sate another question that came
up in some other work with you've all
which is also about how well martingales
can aim and in this case better than we
could have liked so it's just as sort of
like as a you know as a way to see the
deep richness of even very simple
martingales here's a consider the
following class of marketing else so I
have a martingale on the real line and
the martingale can just do the following
at every point so actually suppose it's
a is a monkey on the integers and every
point it can go plus or minus one or you
can go plus or minus two these are the
options that the martingale has to
itself okay sorry can just go left or
right or we can go left two steps and go
right two steps okay and it's you can
even assuming it has it's sort of
Markovian so it doesn't it just makes a
decision based on the integer at that
okay so every integer it goes plus minus
1 or plus minus 2 and now you can ask so
it's a whole family of martingale so you
can choose the rule however you want
suppose you try to choose the rule to
sort of so that you land at 0 as often
as possible okay you always have to move
so you can't just sort of stay still you
have to stay moving but you choose the
rules so that you want to maximize your
probability of being at 0 ok so the same
we have a guess on what the upper bound
should be so now I want to say that no
matter what rule you choose again your
rule is just at every point and you can
choose differently at every point but
you can go plus minus 1 or plus minus 2
that's all you can do ok and now I want
to give you an upper back I want to give
an upper bound on how well you can hit
zero so how well your martingale can aim
for zero does any have a guess
well that's definitely a good / bad one
but no no so you're turning around and
fixed it right foot for fixed end but i
but but but but you can but it's an
asymptotic problem so I don't if up to a
constant I'm happy with what your video
you know what and you're trying to talk
too much you do know what I'm trying to
optimize yes so okay so so so costilla
said this is sort of the obvious
conjectures I have and in fact I believe
all my lived with this belief at least
for probably for six months but this
should be the right answer that that I
mean this thing is look it's plus minus
1 or plus minus 2 I mean sort of it
can't I mean the best you can do is
something like this now the truth is we
actually don't know what the answer is
but there is reason to suspect that
actually the right answer is that you
one can achieve something like something
significantly better enter the half
minus epsilon so there is some rule
which does much much better than
standard random walk okay so computer
simulations have borne this out and
there is a there is a differential
equation which the jet which suggests
has happens at what's up if doesn't was
big I know it's like yeah it's not like
a little epsilon it's you know I mean 0
epsilon comes in various sizes right but
I mean no no you get the and there's I
mean there is a there is a differential
equation which suggests yeah what is it
oh that I don't know right I can go a
picture of the rule for you which is
just that as long as you're within a
certain area you just do plus minus 1
because you're trying to stay near the
origin and then once you realize you're
getting screwed and you somehow sort of
like gone wafer then you want to be you
just desperately start plus minus doing
to try to get back to the origin there's
kate you need to find
and you're closer than root k then you
walk plus minus one you're further than
we can you walk by and and actually we
can fill this for variants of this
instead of tough nice one of those do
you do you stay at zero with some high
probability and the first one then this
this can be too well I'm going to stay
there with high probability I mean stay
is every step we stay in place of
probability point done and you walk
trust nice one you do not approving with
equal zero or just end with the power
yeah nice this is operably to you but
not for this platform so there is that
there is a differential equation that
sort of Charles smart sort of analyze
which is able to but it's not sort of
the the corresponds when the continuous
and discrete case aren't able to touch
events that are this is not fine and
find that sort of being at zero so so I
well now I guess we should ask you've
all about his proof so let me stop the
top thank you
it is it a combinatorial hype argument
or like I mean just analyte you have an
exact ruling you would by hand okay good
any questions
sir that is a conjecture for the the
best power is another based on the
differential equation in this
so somehow the differential equation is
the best thing for the continuum is it
or is it just about
even there it's even even there when you
analyze it with two different rates the
power doesn't come out very cleanly she
just responsible papers Joel McHale are
indicated for the continuous case good
good what's the power he just all the
purple for some power this is bound
donor yeah so yeah well as far as I know
and even what you've all says for this
is the best known that you can okay
there's a question waiting proven upper
bound of some power that's that's bigger
that is possible but I guess you're
saying you know a lower bound that beats
this that's what you really want can you
know any are you ask me we know any
strategies at least this one I'm asking
whoops you can be sort of you can beat
the trivial bouches one so does go down
like a power law there is an upper bound
to the power cut the patterns are hot no
please yeah we wouldn't and you know
some of you know you know they
low-balled on the player how fun only on
the power of their something yes it is i
mean if you always take one is our noble
but lumen presents an upper bound on the
probability of the upper bound
probability so there is an upper bound
which is no 1 over 1 over m to the
points to the point 1 okay
for that so so take the time to do
happen it's not so this is no time to do
you try the most interesting is to put
the lower bound on dates which shows
that the upper bound cannot be one of
them which they can do apparently for a
slightly different model but the point
is it martingale somehow even though
they seem very trivial I mean like this
one is almost like some other is still
very sophisticated behavior so just one
call me with them said but you know
purple the guardian qualities many
people were all used to seeing them
where Phi is a power so let you bout the
power of a martin gilbert power square
function and that doesn't need this
pesky condition in the second love line
but somehow for this competition was
important to have five that are more
general than just power long and this is
so you like this for one tennis we're so
used to refer the gandhi oh BTW charge
us for our furlough that was not good
enough and and I guess yeah this
condition doesn't necessarily hold for
marking girls but sort of so what we do
is we actually just you know give them
on you a little kick in it case it
doesn't satisfy this and then that can
be absorbed the square function
together
comments or questions ok so we adjourn
everybody</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>