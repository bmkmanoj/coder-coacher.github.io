<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Asia Faculty Summit 2012: Keynote: Human Computer Interaction Research at Microsoft Research Asia | Coder Coacher - Coaching Coders</title><meta content="Asia Faculty Summit 2012: Keynote: Human Computer Interaction Research at Microsoft Research Asia - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Asia Faculty Summit 2012: Keynote: Human Computer Interaction Research at Microsoft Research Asia</b></h2><h5 class="post__date">2016-07-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Kk9y9pFCmqM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you want and welcome faculty from Asia
it's especially great to see some of my
friends collaborators professor Jung
from Japan professor general from Korea
and especially my former student
professor su enjoy who's now a very
successful faculty member yourself I
guess that's the biggest reward for us
educators is to produce successful
people so I being at Purdue for 14 years
and decided recently to join Microsoft
to change the world so to speak in the
only way I know how which is to put tech
haptics technology into the hands of
millions of people but today my talk
will focus on the group on managing
become a computer interaction group at
microsoft research Asia so these are my
guys tell Xiao you are actually here
somewhere over there Dara edge Sergio
palette own know and Jay have a jolly
goo is Cody itani I think we'll agree
that computing devices are becoming more
capable of sensing an acting now
computers know where I am where I'm
looking at ok hear my voice understand
what I'm saying and now can't even feel
high touch the computer screen through
my finger gestures instead of focusing
on smaller faster and cheaper which a
lot of people are doing about there was
while endeavours human-computer
interaction is about making technology
more accessible and finding introduces
for them interesting uses for them so
the ultimate goal is interact naturally
and gracefully was voiced just to touch
etc at the Microsoft Research Asia the
human-computer interaction group is a
very diverse group we have computer
scientists but have technologists like
me who's now focusing on haptic
technology and we have a signers
industria interaction designers we like
to think that we create the census
devices interactions Anderson
that will drive the next generation of
natural user interfaces and reshape
human activities for the better so I'd
like to give you some examples sort of a
glimpse into the future of HCI by
showcasing some of the excellent work
we're doing so the first project is
called a peak control it's using a
handheld projector for direct control
physical devices through a visible light
and it's led by calcium the idea is that
instead of having this visual display of
controls and you move your mouse to
click here and there what if you take a
projector and move that projected image
onto the things that you're trying to
control we are introducing p control
which enables the user to freely control
various physical devices using a hunter
projector and inexpensive sensor units
to control a device the user aims the
handle projector at it costs the
projected interface component over the
sensor unit and issues the command by
pressing a push button controlling
another device is as easy as pointing at
it
we use an up the shell handle projector
free push buttons are added to enable
basic input on the projector the first
button activates input on the target
device while the other two buttons allow
switching between protected interfaces
the sensor unit comprises a photosensor
an indication LED and an optical
diffuser in our prototype setter it is
connected to an Arduino microcontroller
while in the future it may be integrated
into various electronic devices the
image contains both the visible
interface for the user and the embedded
control information for the sensor unit
by changing pixel brightness at a high
frequency different areas can transmit
different temporal codes within the same
interface these coats can either send a
control command along the user to
activate various functions in the
interface so you get the idea what you
do is you take a handheld projector your
point you project and you control their
objects we use the off-the-shelf
handheld projectors and low-cost photo
sensors and then the control signals
actually embedded in the projection so
when you press that button down and
point the project onto our object the
control area is actually sending binary
code so wherever the photo sense that
happens to land in that area picks up
that code and we use the peer-to-peer
communication so we don't really rely on
in a Wi-Fi or any other connectivity
around us and so this is sort of an
example of a creative use of technology
and I like to move on and give you a
sort of different project called a micro
mandarin that's led by general edge here
the idea is that despite the best of
intentions would have a lot of
foreigners in Microsoft Research Asia
you think you're going to come to China
you're definitely going to learn their
language because you can be universal
environment and all that but you'll get
this right and you who has the time to
take a language lessons every day and
all that but if you're busy we can do
what's called a micro learning we can
let you you know cat one of your catch a
break you can do a learning on your cell
phone and what's more if you we know
that you're going to the bag and we're
going to give you flash cards that
actually contain only the vocabulary
that's relevant for your next task
because then it'll encourage you to
actually speak when you go to the bank
and you know half the vocabulary that
you need so let's take a look at chu
liuxiang show 18
so now he's heading to the subway
station and the appropriate words about
train fare and all these things come up
so the way it works is that we use for
squares to sense where the users located
and then a pop screen comes up and say
can you confirm you're there in that
area and then just pick the nearby
venues so you know I'm going to a bank
i'm going to a cafe and once you pick
that an appropriate subset of flash
cards will come up and then you notice
the user is looking at it and flipping
around and also there is separating on
that button that lets users say yes i
use that phrase today not having used it
today so it attracts how much the person
uses the language so a user study was
conducted lasted for weeks long there
were 23 peoples located in Beijing and
Shanghai in these people basically were
given two ways of learning one is the
micro learning or context based learning
is what we call it and the other one is
the more traditional frequency based
learning so this is having flash cards
based on how often these words are being
used and then it's everybody use both
methods and who did a comparison so we
found was that in terms of on the left
side context based learning Michael
learning really fits a very busy
lifestyle studying context relevant
words really encourages their use on the
spot on the other hand this is not like
context alone is better than the
traditional frequency based learning but
rather it really depends for example for
a beginner who really needs to mask
Master a basic set of vocabulary this
you prefer frequency based are grilling
to get a bit of basic stuff down and
also some people just really want to
improve their language skills not
particularly use it in the next five or
ten minutes and they still like the
frequency based one so you notice that
you know this is a piece of work at
that's based on theory and also contains
a bit of sort of design intern
how you would do the software of the
interaction so let me move on to an even
more design based project called the
reaction media are led by Sergio Paula
Tonio here the idea is that our first
reaction to something is the most
precious right you watch a video and you
you know you watch a movie and all that
you have your first reaction to
something and it's precious and when
people not co-located and when you want
to community that first reaction to
someone you love your friends and all
that what would you do you know how
would you do that so here's a here's the
idea we love spending time with friends
but we are not always together
thankfully today we were all well
connected we share texts music photos
and videos among all these media it's
video that captures the most videos
carry our memories stories and emotions
with online video sharing today our
reactions the stories I left behind
while in real life facial expressions
are the key to communication so what if
you tell the whole story let's begin
with reaction media this is check he
goes to the computer to check his email
he receives an email from Mary about
reaction media when he opens it he sees
that Mary's inviting him to check out a
video in the corner Jeff sees Mary's
photo he enters reaction Mediacom where
after logging in he can watch the video
Mary st. Mary's reactions are played
inside the video clip and chip blobs at
her familiar but unusual facial
expressions meanwhile his own reaction
is being filmed he can click to view it
along with his friends reactions when he
clicks further he can see more people
and is able to choose anyone's reaction
to play inside the main window if Jeff
finds his own reaction or other people's
reactions interesting he can attach it
to the video and share it with his
friends his friends will receive an
invitation email just like the one he
got from Mary eventually chef gets to
know his friends in a whole new way for
example he learns that Mary loves baby
so much and Tom always eats
meka what kind of video is watching Jeff
becomes more and more open to show
himself and starts to visit reaction
Mediacom every day always curious to see
his friend three actions by inviting a
chair japan's himself getting closer and
closer to his friends reaction media
works which are because it's richer not
only does it enhance the videos he
watched it also gives him a chance to
truly know people all he needs to do is
just watch and click so here's an
example of how we actually do
interaction designed to enhance
emotional connections between people and
that leverage fact that everybody uses
online media these days so you start
with a vision of what you want to
achieve in this case is emotional
connection you create concepts and then
your prototype in this particular one is
the video prototype and then you go on
to do the software prototyping and they
put it on the real world so this is one
aspect of the design we do and another
aspect a small focus on industrial
design shall we do is a young researcher
we have in our group whose background is
an industrial design so in javis work he
would start with a scenario in the using
her face and this particular example
were looking at link wireless headsets
so blink is this wonderful program that
we use with ain't microsoft to
communicate with colleagues not just in
Beijing but everywhere around the world
in this is a depiction of some of the
pain points that you might say that when
you use link to communicate here's a
pariah to hear somebody who's microphone
is not properly plugged me so as she's
talking the voice is leaking and the
people behind him the co-workers just
want to kill him there's you know in the
bottom middle is that you know you get
connected the first thing you say is can
you hear me can you hear me now isn't
there a better way to start a
conversation so you decide you sort of
do that analysis of your users
and look at all these things and then
you do what's called a snare bass to
sketch so here every panel Joey has
spent yourself a different problem for
example what do you do if you on the
phone sell my tree forget to charge up
your battery and the upper left what
would be the form factors that I really
make a link wireless has that really
ultra-mobile here's another sketch tower
has done actually it's a laptop cover
for students that fit their lifestyle
for example you can see on the upper
right there's a handle so when you you
know have backpack on everything you can
pick up your laptop easily in the middle
you can see the clock and the email show
up so you don't have to really open up
the cover in order to get a quick
glimpse of sort of what new email is
coming and all that again these are a
lot of concept drawings and then you
work out the mechanical details and then
you do the physical mock-up and
hopefully it will lead to some product
someday so we're getting into more
hardware related designs now so which is
a good time for me to describe body
spill let x co gia Tommy so I think the
video speak for itself so let's watch we
developed a wearable acoustic sensor
called body scope the chest piece of a
stethoscope is attached over the
microphone embedded in a modified
bluetooth headset it is designed to be
positioned on the side of the users neck
in order to amplify the sound inside the
body and eliminate external noises the
body scope sensor is intended to be worn
around the neck this device supports the
classification of sounds recorded near
the throat rather than what we might
normally here
so here's Koji eating and drinking as
part of his research to some quite
different ring we will now show you some
of the sounds recorded by the body scope
the sound recorded by the Bioscope is
shown in the spectrogram when the user
is sitting we can see weak signals below
70 Hertz this cycle consists of two
parts so we believe that it is the heart
sound
when the user is taking a deep breath
the body scope can sense the sound of
inhaling and exhaling the good idea
you'll get different signals when you
laugh when you speak when you whistle
and you're saying and all those things
I'm so body scope is a wearable sensor
that records the sound in the users
throat area and I can recognize
different activities right now it's got
recognition accuracy about eighty
percent in their lab and a bit lower in
the world so the last one I want to show
you is my own project haptics this one
is about providing sliding and a cooking
feedback on otherwise smooth piece of
glass to facilitate a facilitate typing
this particular case the trivia honey
sound tailed calvini down in this
demonstration we show a new haptic
display that delivers touch feedback to
a finger on a touchscreen we provide
different feedback depending on whether
the fingers sliding on the screen or
clicking an object on the screen two
scenarios have been developed in the
first scenario we show a button clicking
demonstration the second demo shows a
thumb typing scenario the first scenario
is operates sliding and clicking haptic
feedback into a thumb typing scenario
the left thumb moves up and down along
an arc in the lower left corner of the
touchscreen this is mapped to three
letters in the vertical alphabet display
the user selects desired letter with the
right thumb three buttons can be found
at the lower right corner of the screen
clicking on the top button selects the
top letter clicking on the middle button
selects the middle one and so forth this
way the left one does the positioning
while the right thumb does the selection
the locations of the arc for the last
bone and the three buttons for the right
thumb can be felt as a different texture
by side
the thumbs on the touchscreen a key cog
feedback is triggered whenever the right
arm taps are one of the three buttons
the haptic display shown in this demo
provided so the nature of haptic work is
that unless you feel and touch it you
don't really know what I mean so I
brought the hardware and tomorrow I'll
be the demo fest and I'll I welcome all
of you to come and give it a try so I
hope that giving you a bit of flavor of
the kind of work we do whether
interdisciplinary each project may seem
very different but they all design very
specifically to explore a particular
survey area at the intersection of many
different sort of disappoints and we
believe that this is very important
because it's in those areas that were
very likely to bring true innovation to
human computer interaction thank you
very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>