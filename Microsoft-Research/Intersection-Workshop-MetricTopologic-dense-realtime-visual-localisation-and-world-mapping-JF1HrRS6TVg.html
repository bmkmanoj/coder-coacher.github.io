<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intersection Workshop - Metric-Topologic dense real-time visual localisation and world mapping | Coder Coacher - Coaching Coders</title><meta content="Intersection Workshop - Metric-Topologic dense real-time visual localisation and world mapping - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Intersection Workshop - Metric-Topologic dense real-time visual localisation and world mapping</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JF1HrRS6TVg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
good morning and thanks very much for
the invitation to come and speak here so
I'm actually from the cnrs permanent
researcher in based in the sophie aunty
police in South of France and I'll try
and make a bit more of an effort to to
speak to integrate into the different
aspects of the other sessions basically
I think there's a lot of overlap so in
this talk I'll speak a little bit about
3d modeling and I think there's a lot of
overlap with the computer graphics
community but i'd like to try and look
at that from a robotics perspective and
try and rethink some of that i think a
lot of work has gone towards in versing
the computer graphics pipeline for
computer vision without rethinking it
for our our particular applications and
in terms of interaction i think that the
what I'm going to show is more sensors
that are going to be going moving up
into the environment so we're no longer
looking at a static sensor that's just
observing someone interacting with a
computer but it's actually the computer
that's going to be out interacting with
the world so a lot of this work has been
done in collaboration with several
students and also researcher from inria
in so fiancée police Patrick Ruiz so
basically a lot of my research since
around two thousand five has been
looking at this kind of core problem for
a lot of robotics applications in that
we want to know where the robot is
positioned in space and to be able to
know where it's positioned as it's
moving through space we need to also
have a map so this is this chicken and
egg problem that we you we need to solve
simultaneously and we wanted to be able
to do that in large-scale invar
with very complex in geometry so we
can't easily start applying lots of
different types of features planar
objects lines etc but we want to be able
to just have some sort of general
representation and importantly for
robotics is be able to do that all that
in real time so our solution has been to
go towards more image based approaches
or sensor based approaches so we we want
to stay as close as possible to the
sensor information so that we can
compare that with the online sensors of
the robot so this kind of deviates from
these 3d graphics models where we have
texture maps 3d models with aliasing
effects and trying to cater for all
different viewpoints rather we're going
to start looking at maps that are
dependent upon the trajectory or the
given set of trajectories that the robot
can undertake to to navigate through
that environment and we're going to
deviate also a little bit from a lot of
thrust in the computer vision community
towards feature based approaches so we
strongly believe in these sensor based
approaches so we call that direct
approach where we minimize directly the
sensor information or the image
intensity and we've found that to be
very robust statistically and very
precise and it allows us to avoid all
these feature extraction errors and
feature matching errors because we don't
have to do that anymore and we're
actually getting feedback from the
global information towards this kind of
low level stages of the processing and
we'll try I'll try and show a little bit
how we treat the the different
criticisms of these direct approaches by
looking at how we can model dynamic
changes in the environment such as
illumination changes and how we can
develop a generic model for any type of
a sensor so just to give a first kind of
overview maybe a little bit technique
but the whole rest the rest of the
approach is kind of based on this so
back in 2007 we first published some
work on
localization and mapping so basically we
have a set of photometric color images
we have potentially depth images so we
also we didn't have access to the
primeSense Center in advance like dieter
but we were already working on stereo
systems so we can we can consider a
stereo system to be RGB d system and so
we have this novel view synthesis which
you're probably most of the people in
computer vision community are well aware
of and we like to work a current image
using knowledge about the pose so six
degrees of freedom pose and the the
depth of the points so that we can walk
that to a reference image so instead of
having a 3d model we're basing
everything with respects to the sensor
information and most of the time we'll
just take the first image to be the
world reference frame so we have just
the incremental estimate so we're
looking at nonlinear iterative
estimation incrementally so where most
of the time looking at very small
increments between two time instants at
45 frames per second so we can assume
that there's very little movement
between the different the different
images and we build up a global
representation of that by optimizing a
robust optimization criteria and by
defining this robust optimization
criteria with respect to all the
information in the image and not just
certain features or patches we can get
some nice robust statistical properties
through an M estimator so this is
basically the stereo configuration we
had to look at modeling that through the
quadra focal geometry between a
reference stereo pair and the current
stereo pair so we have a whole set of
relations that allow us to describe the
transformations between all the images
and we found that the the base relations
that were the most important was going
from the left reference image to the
left current image through a trifocal
tensor so the quadra focal decomposes
into trifocal and we end up with two
trifocal to enter
is that will transform left and right
images onto one another and so basically
we're defining a plane in the right
image through a corresponding point so
this is for the stereo case we have
correspondences spatially for dense
matching but we don't have
correspondences temporarily and we don't
keep the we don't keep the feature so we
will be considering directly the
intensities so this is just basically a
geometric working function for the
stereo pair which depends on certain set
of under known pose parameters and this
is just some of the video videos that we
were doing back then so here we have
very accurate visual odometry and six
degrees of freedom you can see the
stereo pair being used for the estimated
six degrees of freedom odometry and we
can see that we're able to do a full
full tour of around the bath of around
200 meters with only around two meters
drift at the crossing point and we have
mobile objects we have flickering lights
which are all able to be successfully
handled by the by the M estimator so we
can see here just a little bit of
information on the M estimator so we can
see down the bottom the different parts
that are being rejected from the data so
we can see that there are some parts of
the scene that aren't visible from left
to right cameras we have moving cars and
we can see that the DM is jump every now
again so this is when we're changing
reference image periodically based on
robust statistic criterion that will
detect when the robust variance gets too
strong so we kind of with with the
arrival of the kinect well to looking at
applying these techniques to the kinect
so i guess you're all aware of the
geometry of the connect so i'll just
move on to this kind of based
optimization which we call direct
iterative closest point because we still
want to maintain this direct
minimization criteria based on the
raw sensor data but in a lot of these
devices we're on the other to obtain the
the world depth also so we can start to
consider these sensors as providing both
intensity image and depth image so we
minimize directly the roar intensities
worked from the current image to the
reference with the reference image and
also do the same with the depth image we
actually have a warping function for
this depth image and then we have this
scale factor that allows us to take into
account the the relative uncertainty
into the two sensors so we had different
sets of choices for the depth warping
function because we have all these
inverse compositional or Ford
compositional approaches which allow us
to compute precompute different parts of
the estimation problem before hand and
make the estimation more real-time so in
this case we are able to reuse some of
the data that some of the computation
that was made for the intensities by
doing a Ford compositional approach and
so we end up with the estimate of the of
the poses and as I'll show later also
the depth so this is just a picture of
this closed loop non linear estimator
estimize a esta magic it's my estimation
process and so we can see that we're
actually closing the loop the estimate
error on the sensor data so we're not
introducing any open loop error on the
features or on the feature matching
process and so with this we've kind of
saw that we got some really good results
using the whole image using robot
statistics on that I want to push that
further so we started looking at having
a sensor based approach which was even
more dense we wanted to have full
spherical panoramas but we need this
depth image so we started working on
building this kind of ego centered
representation which is a set of spheres
kind of like what we'll find on google
street view but we depth dense depth
images on the sphere
years and using that to perform
navigation and localization so each each
sphere is composed of the photometric
component and the depth image and we
also have a saline see map which tells
us which pixels are more interesting to
constrain the full global problem so
basically the the classic sensors that
we have to be able to do this all try to
aim at having a common central center of
projection so we went out and expressly
built a system with bass lines between
all the cameras so this was our first
system we had quite a lot of difficulty
because we have diverging angles of the
cameras and when we try and warp all the
difference of the six cameras around the
ring onto a central sphere but there
were missing depth we couldn't walk the
photometric information onto the sphere
so recently we we have developed a new
system that has has just a single
baseline and basically it's a set of
three stereo pairs back-to-back looking
outwards which gives a lot better
information so this is just a video of
the the output of this sensor so we can
see where obtaining nice 180 degree
panoramas and we have quite dense depth
information on the bottom so once we
have this sort of dense sequences in the
environment we can start to to look at
using that for navigation first I'll
just talk a little bit about how we
position this views in the environment
so basically we have this direct
estimation approach for the entire
sphere so we just minimize directly the
intensities and estimate the poses
between the different spheres and we
just like to determine where we need
another sphere so we do that by looking
at the median absolute deviation of the
air distribution and if we detect that
that's getting too large then we
we place a new sphere so yeah for the
moment we've just looked at this kind of
very efficient criteria but I think we
have some more work to do to work out
how to optimally place these spheres in
the environment with respect to
occlusion boundaries and things like
that that are not handled by a sensor
based approach that but that are handled
by classic 3d model based approaches but
we have the advantage that's for our
applications we have this photorealistic
rendering of the environment so this is
this is just a video of this
representation so we've got the spheres
in red we're just choosing the closest
sphere and to the to the character
position and we can generate noble
viewpoints locally around this
trajectory which is sufficient for
navigation applications we can see if we
come out of the graph we lose a lot of
the precision but we only need that
locally around the trajectory and and so
we can start to see a lot of
applications for for gaming or or
robotics or augmented reality and
because we have this sort of compact
representation kind of Google Street
View style we can start to look at also
having compact packets that can be
transferred onto mobile devices for for
navigation purposes as well so I talked
a little bit before about how to handle
dynamic changes within the environment
so often we'll have an expert system
such as this that will go and map the
environment for us and we'd like to use
a low-cost sensor online to go and
navigate and localize ourselves with
respect to that model but often if we're
outside we have a lot of illumination
change from morning tonight or through
the seasons so basically we talk about
model-based approach in the case where
we're localizing respect to this
previous model so we have all the depth
maps we have the full environment
reconstructed and reference with respect
to a world reference frame
but the problem here is we can have very
very large changes with respect to the
current online sensor but we also have
the visual odometry approach which just
computes these parameters incrementally
over to time instances and there we can
have very little change between two
images at 45 frames per second so we
just simply combine these two approaches
so what we do is we simultaneously
estimate an error with respect to the
model and ayres back to the visual
dmitri and we can also take leverage of
the depth maps computed for the model
for the visual odometry part because we
know that that's correct and we just
introduced the intensity component of
the visual odometry into the system so
it still stays very efficient in terms
of real time estimation so this is just
a picture of this duel estimation
process is probably more interesting if
I show you the video so here we have
quite a few different scenarios that we
have tracked we have so this is our
tracking system we have the reference
image in the middle at the top and the
associated depth map we have the current
image that we want to register with the
reference one so this is just the first
part here is just one increment so we've
purposely introduced a large
displacement between the two cameras
even though in typical situation we are
much closer you can see that there is
quite a large domain of convergence we
can see the the robust weights and air
on the left so we can see at the end of
the minimization with because we have
very large illumination changes we still
have residual error with respect to the
model-based approach so that actually
gives us a dense illumination map pixel
by pixel that with the change with
respect to the model so this is this is
the online tracking system here we're
building up a model of the environment
so we can see the different reference
images or spheres presented in blue and
we have the six degrees of freedom
relationship between the cameras
and then we do this tracking in real
time so we're just selecting the closest
the closest reference image in red and
performing minimization with with this
saturations so they're actually bay
window just behind the camera so there's
a lot of daylight coming in even though
it's an indoor situation and so we
tested global illumination change where
have inverse of gradients in versed in
the image and we also have tried local
spot changes and local parts of the
scene so we can see that we're getting
close to millimeter accuracy with the
stereo system we also tested changing
the calibration parameters on the camera
because we are quite robust we have this
closed loop estimation process so we can
d focus we can change the aperture size
which changes a lot of the illumination
in the scene and we can see that the the
camera pretty much stays still in the
aperture change but when we actually
changed the focus of the camera we can
see we're just playing with the depth
the z estimate of the of the pose so
it's just giving the precision but with
a lot of the applications for robotic
navigation it doesn't matter too much if
we are not metrically accurate at
globally as long as we have some sort of
closed loop estimation locally that
maintains a correct estimate so this is
just it just keeps on going on with
different types of illumination change
so this is some of our work on the on
the kinect sensor for doing direct ICP
so it's quite similar to the work with
microsoft and imperial college on on
doing the kinect fusion but here we're
using our ego centric approach so we can
see the raw image here being integrated
over time so we're actually creating a
volume as they do but we're just keeping
these reference images and then
propagating the the integration over
time and we can see that we're able to
maintain a quite high resolution
estimate of the of the depth and the
intensity maps of the obvious een on the
top left is the the normal of the
different surfaces so we can actually
get a lot of very interesting
information out of out of this and I
have just another small video here with
the kinect so once again using these
direct approaches minimizing both
intensity and depth so we can see we've
got some problems here with the
automatic gain on the sensor we don't
have access to a lot of the internal
workings um so we have to put up with
these intensity changes or do blending
techniques to do with that so quite
similar to some of the work bye bye
dieter folks also so here is just our
virtual navigation through these
environments so this this large scale
here has only median filtering on the
depth parameters to be able to get to
get to filter the fields of the scene
so I don't know if I've time I've just
got a small video here of autonomous
navigation so this is what basically we
have outdoor situation here we're in the
center of town in coma from on a
electric vehicle and we're using this
egocentric representation to teach the
robot a given trajectory that we'd like
it to follow so this is the learning
phase we just drive around manually and
gradually build up the model so we
really had a lot of problems with with
the illumination from Mornington I we
also had we also saw that our salience
II selection criteria that actually
looks at not only the the gradient
within the image but it's at the four
jacobian of the warping function to
select the most interesting information
was able to to work quite well because
we had a lot of points along the square
at infinity which would have been
selected by a kind of classic Harris
type gradient in the image detectors and
we would have lost all the information
that would have constrained the
translational component so this is just
building up the model I just kind of
skip along so we had a one kilometre
round trip around the around the square
in fully natural environments and then
we use this in an asymmetric kind of
configuration so we have this this model
that's been built up and then we just
used a monocular camera to localize the
robot with the respect to that and to
determine its trajectory and so this is
just a video of the of the autonomous
navigation so you can see that we have a
we also have a laser sensor but all of
these approaches are uniquely vision
we're not introducing any inertial
information or anything else into the
estimation process and the the laser is
just there for security purposes so
someone walks
of the vehicle it will stop the vehicle
and so it's it's able to handle all
these these largely dynamic changes in
the environment with pedestrians triumph
changes in illumination etc and and so
it doesn't matter too much that with
actual global model we actually had
quite a fair bit of drift with actually
respect to the ok we learnt the path
through the through the gate so we
forgot to open but um yeah so this it
doesn't actually matter that the global
model isn't metrically precise as long
as the locally local part of the
minimization maintains the same image
working so we can actually have this
metric topological representation that
allows us to navigate even with
non-metric global models and so that
kind of concludes my presentation so
maybe it has some ideas for throw it to
to rethink the this kind of computer
graphics pipeline I spoke with a few of
the graphics people earlier and I think
they're still even looking into these
type of image based approaches also and
also for for doing localization and
mapping I think that even maybe with
these kind of direct dense approaches
its may be possible to work will work
towards higher level information so
trying to use these direct approaches
for semantic high level seen knowledge
may be there there's a way to rethink
that so that we don't have these huge
feature descriptors but we maintain
somewhere in the representation this
direct roar information that allows us
to have a real keep touch with the world
and the real sensor information
so that's great thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>