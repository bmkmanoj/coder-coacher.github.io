<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Northwest Probability Seminar 2014 - The role of compactness in large deviations. | Coder Coacher - Coaching Coders</title><meta content="Northwest Probability Seminar 2014 - The role of compactness in large deviations. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Northwest Probability Seminar 2014 - The role of compactness in large deviations.</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ak4Fdr9kBuk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay good afternoon everyone
so we're especially delighted to welcome
this year's Birnbaum Speaker Barrett
reservoir dan has transformed quite a
number of areas in probability and just
to give a small sample this the dunce
covered on theory of local times in
general his work on large deviations
that we'll hear something about today
hydrodynamic limits one of my favorites
is his introduction of the
self-intersection local time and I could
go on but instead let's have the speaker
tell us about the role of compactness in
large deviations right now thank you
very much well it's so pleasure to be
here so there are many problems in
analysis where we use compactness of
space in some routine manner and
sometimes it's not there and we'll start
there to work harder and the hard work
takes different forms so we'll look at a
couple of examples of various types
things that we do to avoid the issue and
then I'll give one example that leads to
some interesting calculation so what is
a large deviation theory one of the
things we want to do is to analyze
asymptotic behavior at certain integrals
these are integrals of the form
exponential of a function with a large
parameter multiplying it times certain
family of probability measures PN and
then you want to look at the Tilted
measure which you throw in the density
to the n FX is the radical derivative of
PN and the normal
by the expectations so that queuing is a
probability distribution now which
somehow tries to maximize half you know
now the pin itself is has its own
behavior it's sort of it's not uniform
and it decays locally exponentially
different rate at different points so
actually there is a fight going on
between the function f which is very
which wants to be large but the cost it
has to pay for picking a point X as I of
X so this fight has to be resolved and
one can conclude that Xion is
asymptotically like n times exponential
of n times the supreme FF - I that's one
of the results a large deviation theory
which is not easy result to prove so
long as you can interpret what DPN
decays like exponential minus I of X you
have to give a meaning to it
because the left hand side is a measure
defined four sets and right hand side is
a function defined on points so that has
to have an interpretation and one of the
things we want to prove with the
interpretation is we want to show that
the measure queuing concentrates at a
single point a provided the variational
problem is a unique solution if at some
point a F minus a as a maximum strictly
larger than all the other values then we
want to show that the Quian measure
concentrates at the point a so now what
do we mean by this expression that DP +
behaves like exponential minus IX well
we are interested in lord of pn and we
want lot of pn away normalized by 1 over
n to behave like infimum of I of X or
the same even this cannot be true for
all sets because
if you look at a single point set i
could be finite at that point and PMF a
usually zero for single point sets so
this again has to be interpreted
carefully and the interpretation is you
get a lower bound for open sets and a
upper bound for closed sets just like as
limit theorems in probability now for
open sets it's easy because you know
usually these rules are obtained for
small balls by some tilting argument you
make law of large numbers for the tilted
measure to look localized at that point
X and you figure out the cost of tilting
and it gives you the exponential lower
bound so the lower bound if you get for
a small ball that's enough for any open
said you can just optimize the location
of the small ball and then you get the
inequality for open sets the upper bound
is for closed sets and how do you obtain
the lower bound how do you apply the
upper bound usually in probability what
you do is use chebyshev's inequality to
get the upper bar right so you use the
inequality the probability that some F
is bigger than L is bounded by U to the
- daniele times the exponential of n
times L like it share not bound type of
thing
and then if you have this a nice
function you can find a ball around some
X of radius epsilon contained in this
half space if less than or equal to L so
that gives you an upper bound for a
small ball and the upper bound or a
small ball is basically e to the minus n
times f of x times the exponential the
expected value of the expression and the
expected value of the exponential
perhaps grows the exponential of some
constant which depends on F so in the
end the best possible upper bound you
can get is a decay rate which is the
best possible temperatures in equality
you can dream of
but this is only a local bond for a very
tiny ball and and and you cannot control
the size of the ball because there's a
soft argument in some sense it depends
on what the soup is attained what kind
of a bad after you have and what do you
do you use the inequality which is
simple inequality the probability of the
union of two sets you can bound it by
the sum and you can write whether twice
the maximum the beauty in large
deviations is by the time you take the
log and divide by L the two goes away
once the two goes away then you can
cover a compacts with a finite number of
these things so immediately you get an
upper bound for compact sets so you need
finite covering so you can estimate the
probabilities in question for compact
sets the rate of convergence would be
okay this ace is compact we're done this
problem is solved often these pages are
not compact then you have to do
something else what are the things you
could do well you can find a big compact
set and show that what happens outside
is not relevant now in product theory in
order to show that something is not
relevant
you have tightness estimates you show
that the probability of a compliment can
be made arbitrarily small here we are
interested in large deviations of later
TN so we had to show that outside the
compact said the probability decays
faster than any exponential you can
think of so basically the constant of
decay has to be very large outside a
compact set you can sometimes do that
and that's called exponential tightness
estimates that you derive or you can
compact over the space
in any space can be compactify eat I
mean account your theory main points and
topology but then that is its own
problems because now once you come back
to pay the space it's not clear whether
your function extends to the
compactification or not and then you
have to get your local upper bound for
all the points you have added in the
compactification so you have to have
normal as a concrete compactification
not an abstract compactification in
order to do it or you can modify the
problem you can just if it's interesting
just estimating it you can simply say
well this integral is less than or equal
to some other integral somewhere else
which happens to be a compact space then
you can do that and all of this have
been try all just for Brownian motion
and just for the local time in different
context you can do different things the
Brownian motion local time is just a
random measure which is the amount of
time that the Brownian participants in
the various sets a and the space of
measures is now the ambient space
because you think of LT a as a random
probability measure so you even family
of probability distributions in the
space of measures and Rd and this is
where you want to do a large deviations
and it's well knows what the rate
function is locally there's no mystery
because you know it's just the it's a
reversible process standard computation
that the local rate function is just
directly form of the square root of F so
the question really is how do you handle
the non compactness here the first
attempt is to estimate for example the
firemen cost formula
integrals like this for pretentiously
that go to infinity as X goes to
infinity
now if the Brownian path wants to go too
far we kills it so it's clear in this
problem you can handle the non
compactness because the non compact part
just doesn't count
and and the asymptotic rate is just the
infimum of the integral it's we had the
supremum because of the minus sign and
so on you try to Racine phew one can
also technically compactify the space
well the easiest compactification one
can think of is the one-point
compactification so just add a point and
infinity and say my space now is compact
and the rate function for the Delta
measure at infinity is 0 because the
Brownian local time un surely wants to
get away to infinity so the
neighbourhood of infinity really means
most of the mass is very far away and so
it's clear that the rate function at
that point is 0 no everything is
compactified but then the price you pay
is yet to assume that the potential V
goes to 0 or has a limit as X goes to
infinity then you can subtract the limit
and it's just basically going to 0 so in
the class of functions thatis that you
can handle gets reduced considerably
because of the compactification you used
another thing you could do is you can
replace Rd with the tourists you can
project the Brownian path that's
wandering in Aldie the Brownian part on
the torus and with the function you are
trying to estimate is larger on the
torus for the
selected path then for the original path
then if you try to estimate it on the
torus it's good estimate on the old
space and the torus is compact so he
estimates vote for the Taurus so you can
sort of do the problem for a large
Taurus maybe and in the end let the
radius of the Taurus go to infinity
there is no trouble interchanging the
two limits because you have actually
domination in one case and that has been
tried too and that works for the problem
of Wiener sausage because if you project
the sausage the sausage in the Taurus a
smaller volume then sausage and Aldi
okay so problems like determining the
range of a random walk and things like
that
projecting on the Taurus is a good
method of compactification because the
projection of overestimates so now the
problem I want to look at is the
following thing on the local time in D
dimension you look at this double
integral of the potential 1 over mod X
and you want to analyze this well here
what works is replacing Brownian motion
by Austin moon by process because also
ruinberg process is strongly recommend
and you can get super expiration
estimate or what happens outside a
compact set so you can do that and and
you can show by because the function 1
over mod X is positive definite the
integral is more for also known by
process there it is for Brownian motion
so comparison works here it's not quite
satisfactory for the following reason
when you modify these problems like this
use methods comparison and so on it is
good enough to estimate the partition
function ZT and how it grows
but if you really want to prove that the
measure QT defined by this tilting the
new measure if you want to prove that it
is I think I made a mistake here there
is no Manohar T because I use the local
time already okay
there should be T in the scaling so you
cannot analyze this by modification
because then a double limit is involved
and it's hard to control things so what
I'm going to describe now is some joint
work with the cherries you look at you
is a postdoc at Munich so let's look at
this local time and you have a function
of the local time that we want to
analyze but this function is translation
invariant because if you translate the
measure mu it doesn't change there are
examples of this the examples of such
functions are our function which is a
function of two variables or you can
take functions of K variables which are
invariant under rigid translations and
then you integrate with respect to the
product measure of the Meuse and these
are functions that are invariant with
respect to translations so they have
this property and let's suppose these
functions are tame that whenever the
separation distance between a pair of
points goes to infinity these functions
vanish
so they're basically supported on points
x1 through XK which are tightly bound
together if they sort of drift apart in
the function vanishes our function 1
over mod X 1 minus X 2 has that property
and then we want to analyze the
expectation of something like this
so now we have to go to the caution
space because it's a problem on you know
if you want to you know the problem was
the translation invariance so the
natural space on which to do the last
deviation is the quotient space of
orbits of measures translational is this
space is still not compact okay so the
question is what what is the
compactification of the space you can't
get away with one point compactification
here because because the orbits then all
the orbits collapse because at infinity
everything meets you know one point
compactification so you can't use that
so you need some other compactification
so the question is how to compactify one
point contracta fication is not suitable
it's not translation invariant so so
let's take a function f which is
translation invariant continuous and as
this property of decaying when the
distance is go to infinity and look at
all the all such functions not only for
1 K but all the various values of K
functions of several variables for all
values of K and then I look at these
linear functionals lambda f mu which is
the integral of this function against
the mute it just elementary point-set
topology these objects for each F are
bounded and despairs have K are all nice
separable spaces because the space of
functions of this type is really a
function of K minus 1 variables that
vanishes at infinity so it's a nice
separable Banach space so a countable
number of separable Banach space so it's
enough to choose a countable dense
subset that works for everything and
then by diagonal is a
in procedure I can choose the
subsequence that all these things
converge okay so a countable set is
enough and also one thing interesting
that's to be noted is the FK minus one
in some sense a limit of F K because if
I take a function of K variables define
it as a function of K minus M variables
multiplied by a function V of X 1 minus
X K although this function fee has to
vanish at infinity for this to be well
defined in our class but I can take the
limit as fee goes to 1 if you take the
limit as fee goes to 1 it's not a
uniform limit but it is a point twice
bounded limit it's okay for integrals
you know because they can use the
bounded conversions theorem so then the
integral FK with respect to the product
measure will converge to measure of the
whole space times the K minus 1 so the K
minus 1 dimensional integrals are
recoverable from the K dimensional
integral by a limiting procedure that so
now what I want to do is choose the
subsequence that all these limits exist
I can always do that so now I have a
comp activate the space in principle but
I have to know what is this linear
function lambda f ok the answer is very
similar metric between two measures
probability distributions mewhen and new
to you look at this lambda f j mu 1
which is a k dimensional integral of f j
k depends on J because different
functions discomfortable sequence will
be in different dimensions and you
compute the integral take the difference
and you know as usual trick do you do
for a control product or metric spaces
you put some weights in French so that
the series converges
nothing special about it and you try to
complete in this metric which is really
saying that anything that converges for
each function should be some limiting
object one way of choosing the weights
is just usual choice so here is the
answer the answer is the limit consists
of collections of orbits mu J mu tilde
and their total masses of all the odd
bits see the total mass doesn't depend
upon which measure you choose in the
orbit so these are sub probability
measures each orbit will have some
probability and you add them all up you
get something that's less than or equal
to one there's the limit and then the
representation in terms of the limit you
compute the same integral as you did
before for each member of the orbit and
add them all up for the different orbits
the intuition behind it is very simple
how can a sequence mewhen fails to be
compact the whole mass can go around
away to infinity that and then the mass
can split into two pieces they're
running into infinity in different
directions that can happen or you can
have a Gaussian distribution with a very
high variance so the mass just goes away
so in the first case the orbit actually
converges to a different orbit there are
bit correspondent in fact the orbit
doesn't change at all because I have the
same you here in the limit I get two
pieces mu1 and mu2 of mass 1/2 H but the
two pieces are identical the same
one-half mu or now mu appears twice
the third case it just becomes dust
so the compactification consists of
various pieces that you can pull
together which are this sets of all bits
and then something that's disappeared
into dust that's why the total masses
when you add up you get something that
adds up to P which is less than or equal
to one okay and the same orbit can occur
twice so you have to count you have to
list the orbits with the understanding
that you can list the same thing twice
of course it's possible that the other
collection is empty which means
everything went to dust so for every
function if your linear function goes to
zero or you can just have a finite
number of odd bits or you can have a
countable number of odd bits it mass is
getting smaller and smaller and adding
up to something less than or equal to 1
so that's intuitively the
compactification and you have a metric
there which is you look at lambda off
side which is this integral summed over
all the orbits and the metric is the
same metric except instead of a single
measure mu now you can do it for each
member of the orbit and add them out
it's clear D is a metric but if you want
to show it's really a metric you have to
show that the distance is 0 only if the
two orbits two sets of our bits are
identical you have a problem here
because you have one set of our bits
here
another set of our bit
there and here are some objects which
are equal but the objects are collective
objects and now you have to match Arbit
with orbit if you want to show the two
sets of orbits are identical so there's
a matching problem to be done so let's
not let if you take a function of two K
variables which consists of a function
of K variable repeat it in two blocks
but the blocks linked by something and
the link the linking object can tend to
one and then by dominate convergent
boundary convergence theorem what you
will converge to is some squares for
these functions so that means if you
have integrals identical for two orbits
then their squares are also identical
and you can repeat it for any power and
so all the powers are the same that
means you can identify individual pieces
so you would think that now that you
have identified individual pieces then
you are done but not quite because all I
know is that any integral appears on
here appears on this list but for
different functions it may appear in
different places I have to locate them
all in the same place that requires an
argument
so for each chef okay I will come to
that in a minute but what happens if you
know that even if you succeed in doing
it what you know is the multiple
integral for each F is the same for two
different orbits then you are to
conclude the two orbits are the same we
measure what do you mean by that so the
real problem is to show that if you're
two measures mu and nu which satisfies
this relation for every function FN FK
and for K bigger than or equal to two
then mu is a shift of new that's what
you have to prove okay how do you do
that well let's go they take the
characteristic functions so the
characteristic function is a you can't
use the characteristic function at all
arguments because you had to be shift
invariant that means you can look at the
product provided the sum of the T is a
zero that would be a limit bounded limit
of functions that we admit okay so you
have to carry streak functions Phi
inside with the property that their
product are equal at all arguments when
their sums are zero then is one an
imaginary exponential of the other well
first it the easiest charge to take two
T's is take TN minus T then it tells you
free T times V minus T is C T times C
minus T we know that V minus T is a
conjugate of V T so it tells you their
absolute values are equal then you write
phi t SI t times Chi T then you would
like to prove that Chi T is an
exponential you take two of them it said
now take three
and that tells you kaity 1 plus T 2
equals KY t 122 but then that's only
four T's that are not 0 Phi T shouldn't
be 0 with 0 then Chi is not defined but
near the origin they're not 0 so and
away from the origin you can prove that
Chi and T is kaity to the nth power
because n+ T's and 1 minus NT work and
that concludes that one is a shift to
the other so now we are faced with the
problem of matching individual ones that
is for each f is lambda mu occur
somewhere on this list so let's take one
orbit and I compute for their orbit the
bunch of numbers lambda of MU all these
numbers are going to come on the other
list which is a collection of orbits
somewhere but I want to show that they
all occur in one orbit and then I'll
match these two orbits peel them away
and repeat it again then I'll be able to
match the two orbits so the question is
bear category theorem helps
no yes you can I can always let f10 to
one because one is translation invariant
and can be a limit of these things so
that means the collection of total
masses are the same for the two are two
sets of orbits so it tells you that the
size of the two orbits are the same and
the total masses are the same so in
particular if you give me the mass there
can only be a finite number with that
mass so if I know lambda F mu for all
the air for one mu I know definitely
know the total mass and there are only a
finite number of things at the other
side with which I could possibly match
so I consider this set of all functions
in sum of K such that it coincides with
that new so there is a set of functions
in F K for which the integrals match
those that I get for a new on the other
side these are closed sets and their
union is all of FK by Bayer category
theorem one of them as an interior but
once two linear functions are green open
said they are the same
so that's the end of that okay and then
you have to worry about what happens
different Kay but notice I said that AF
K minus 1 is a descendant of FK so there
are only finite number there I have a
matching for every K eventually somebody
has to match infinitely many case and
then I can come down and they'll match
all the case ok so that one of them s
you have all kinds of very large values
of K and you can come down ok
now if you want to show that the object
I have is really a compactification then
I have to show that the original asset
was dense that's not very hard because
if I look at this what I have in the
completion I have a bunch of measures
with masses adding up to some number
less than or equal to 1 so take these
measures put them in space but split
take compact subsets of these with
outside of which the mass is very small
the countless approximate and then
spread them out as far as you can and
it's clear that that's going to
approximate this because the your
visualization of object that
approximates this is just different
pieces that are very far away from each
other so so you can embed your mu and mu
2 UN into a single measure with pieces
that are very part of a so it's clear
that the words no space is dense here
but then it's not
we didn't really prove the
representation theorem if I remember
correctly I just mentioned it I didn't
prove it I mentioned some consequences
of the representation I prove the
uniqueness of a representation and so on
I never prove the existence of the
representation and that is the crux of
the compactification I need to show that
if you give me any sequence of measures
I get some money abstract from them
these pieces okay and you use
concentration functions to do that so
you define the concentration function of
a measure to be the supreme um how much
time do I have to plenty I won't need
that
so yes concentration function to do this
is different you know you look at these
all of radius-r translate it around take
the maximum probability you can recover
from it and that's the concentration
function of radius R and any probability
measure by just the tightness argument
if R goes to infinity this goes to 1 now
if you give me a sequence I can choose a
sub sequence that converges and and the
limit QK as K goes to infinity it's
monotone as the limit that limit did
that be 1 it's something that's less
than or equal to 1 in some sense you
should think of Q as the biggest
recoverable piece and all these
calculations are depend only on the
orbit doesn't so it's it's done on the
quotient space already
if q equals 1 that means after some
translation they measures that
themselves are tight so the sequence
after translation converges so they are
big converges to a single orbit if Q is
0 the probability for any ball goes to 0
no matter where the ball is
that means the measures converge to real
dust and in the limit that corresponds
to 0 orbits and the total mass is 0 in
that case if Q is in between you can
recover a big piece along because the
concentration function gets values close
to Q so you can put it back and you
learn something which is mass almost Q
well make it at least Q by 2 you can
almost get Q but it worked a little
harder but you don't need to do that
take it at least mask you by 2 but then
grow it as big as it can that particular
collection you bring it in here and then
you see what the limit is as you
increase radius okay you have collected
this biggest piece it's not one it's
something less than 1 and then you look
at the measure that's left over that
left or measure can't be anywhere near
the vicinity of this measure because if
it we're in the vicinity of this measure
your radius by increasing more you would
have recovered more the fact this is the
maximal amount you could recover means
the rest of the mass very is very far
away and then you remove this piece and
work with the rest of the mass and you
remove piece after piece after piece and
then you can show that that's a
subsequence you have to do
diagonalization million times at the end
you get a sub sequence that converges
and the convergence is in the metric
that we have so this is the
compactification that you do
repeat and exhaust so now we had to
worry about doing large deviations in
this pace okay first there is some the
orbit is not compact because its orbit
by all of our D so a neighborhood of the
orbit is a lot bigger in the
neighborhood of the original measure
because here to tell you all the
translation but what saves us is a
Brownian path is not going to wander too
far you know in time T typically it
wanders a distance routine if I give it
range T then I am in the large deviation
range if I give it distance T Square
then I am beyond large deviation so I am
willing to say my Brownian path is
allowed to wander a distance T Square so
the local time is essentially going to
be limited to something of volume T
Square which is polynomial in volume
that's going to affect the exponential
rate by an extra polynomial factor which
I don't care about so the fact that they
are built is big doesn't matter so I can
limit myself to a definite sized orbit
there is a way of doing it
you define a certain function C the way
you compute large division rate function
is unity s have some integral which you
can control the integral you control
here our final cost formula when prÃ³xima
tells you how to control integrals of
explorations in fact if the exponential
is a deformed laplacian one-half the
plus e nu
over you with a minus sign then by
martingale property you can this
integral exponential of its integral is
bounded by one basically so there are
lots of integrals that you know of
exponential Fineman cards type whose
expectations are bounded by a constant
so you use all of them and look at the
best you can do and that's the large
deviation upper bound that calculation
gives you gradient of square or f-18 and
that's it's a simple calculus of
variation so that's what we need to do
except we can't use one function we have
to use a function here the function
there a function here to capture all the
bits and pieces so you have to take the
supremum or all possible functions over
all possible locations but a little
perturbation doesn't change much because
these functions are all continuous so
there is no problem in having a little
variation but then your volume is only T
squared so you can do it by a polynomial
number of these things and and you know
it's just a mess to write it down but
there are so many parameters you know
and you look at this integral the
expectation of the exponential of this
without the 1 over T is bounded by some
number and then you look at the supremum
of these things that's the lower bound
you can get that's the upper bound you
can get and these integrals are all
bounded by Fineman cars formula small
variations change very little and you
don't have to shift by more than a
distance d square the Super's or
polynomial many sets of AI and and if
you do the infimum you can separate out
all the components and then what you get
for the rate function which is some
other reproach
in the end we haven't really discovered
anything new we have just justified the
compactification and pretend that the
space is compact that's all so now you
go back to the problem in question so I
want to look at this function this
function has a singularity that's an
additional headache
but one our mod X is not a bad
singularity in three dimensions and
usual estimation methods tell you that
you don't have to worry about the
singularity you for all practical
purposes you can pretend that's the
bounded continuous function similarity
is not so so if you do the variational
problem now you can do it in our space
and you know F is an l1 function you can
think of it as the square of an l2
function and then one over eight grad of
square F becomes additional a form and
different components have different
functions and so the variational problem
so this is just what I get by doing the
variational problem in the compact of
headspace and one can then prove that
the variational problem is attained at a
single fee of unit mass which is unique
up to translation so there is a unique
maximum for the variational problem on X
tilde and that tells you the mass under
QT concentrates in a neighborhood of
that orbit the uniqueness of the
variational problem is proved earlier by
Elliott leap in some other context so
that's definitely useful here so the
model of the story is in the end you
didn't need all this right but you
needed it in order to be sure you didn't
need it thank you
questions opponents the British thing
would these precautions is so
unnecessary had they not been taken
to this racial problem that you serve
the Lord addition for the specific
potential so it had some a physical
origin yeah it's I think garden poor
along problem which is question of
electrons sort of reacting with the
field that they created themselves
because I know that you know if you just
fix one variable and just look at the
potential as a function of the other
variable thing the concentration of that
along the intersection property the
random walk so this seems to be tightly
related any other</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>