<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Predicting Travel Time Reliability using Mobile Phone GPS Data | Coder Coacher - Coaching Coders</title><meta content="Predicting Travel Time Reliability using Mobile Phone GPS Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Predicting Travel Time Reliability using Mobile Phone GPS Data</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mS7S6kz3xac" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I'll be talking about some work that
I did while I was at Microsoft Research
in the fall for sabbatical joint work
with Eric Horvitz moyes hash goldschmidt
and a number of other folks at MSR I'm
just just taking a position as the head
of the marketplace optimization team at
uber and also i'm a professor at Cornell
so I'll be talking today about methods
for predicting the reliability of
driving time on a road network and the
main applications are in the area of
mapping services like Google Maps ways
and bing maps and also for decision
support systems for fleets of vehicles
okay you can think of ambulance fleets
or fleets of delivery vehicles or ubers
vehicle fleet for example so these
mapping services as you're as you're
probably familiar with provide
predictions of driving time from an
origin to a destination in a road map
and they give that as a deterministic
prediction of driving time okay so just
a single number here 18 minutes to go
from to go for Microsoft's AI
headquarters to downtown Seattle they
they use these predictions of driving
time in order to recommend a route that
has low predicted driving time there's a
large number of companies other than
these mapping services that use these
driving time predictions however for
example any kind of delivery service
like Amazon UPS or FedEx plus the
ride-sharing services like uber and its
competitors trucking companies like
con-way freight and more but these
deterministic predictions which of the
current state of the art are never
perfectly accurate right and in the
travel time context the sources of
uncertainty include uncertainty and
traffic conditions unexpected weather
conditions and also the fact that we
don't know the precise scheduling of the
traffic's the traffic signals
if we do have accurate probabilistic
forecasts of driving time for arbitrary
routes in the road network at arbitrary
future times we can use that in decision
making right we can use that in decision
support systems for fleet vehicles we
can use it for routing and a more
risk-averse wife instead of proposing
the route that has the smallest expected
driving time the current state of the
art we can think about proposing the
route that that minimizes say the 90th
percentile of driving time so a more
conservative route selection we can also
attempt to report the driving time
reliability to a user for example we can
give them arranged for any x or if we if
we think that giving them a single
number is important in terms of the
simplicity of the user interface we can
choose how conservative that number is
so instead of just giving them the
expected value which is the current
practice can give them the 80th
percentile we're probably not going to
try to explain that that's the 80th
percentile but the way that it would be
experienced is a more conservative
prediction of driving time once you get
there in fact this comes next to some of
these fleet vehicles like delivery
services there's actually a long time
that's actually the delivery time at the
end and they have to come up with
methods to predict not just the driving
time but then also the last step where
they're actually delivering the package
data that I'll focus on is location data
for users of mobile phones we have had
access to anonymized windows phone GPS
locations we focused in particular on
the Seattle metropolitan region this
source location data for mobile phones
is now quite ubiquitous there's many
many companies that have access to large
volumes of this type of data and also
it's the only source of information
that's currently available that provides
near comprehensive
Bridge of traffic conditions on road
networks additionally there's increasing
evidence that we can accurately estimate
traffic conditions using only this
source of data so that's why I'll focus
so much on this mobile phone location
data here I'm illustrating the windows
phone location data that we were working
with I've aggregated into small spatial
grid boxes and colored according to the
average measured GPS speed so orange
here is slowest average speed and blue
is the highest speed and you can see
that we're able to discover that
highways are faster than driving in
downtown Seattle for example so the way
this works is that on a windows phone
for example when an application on the
phone accesses the GPS information the
operating system also has access to that
location reading and that's why
Microsoft for example has access to
large amounts of this GPS information
but that application I can be for
example a mapping application like
Google Maps or ways or bing maps on the
phone the time between the measurements
is typically between 1 and 90 seconds
for the data source that will focus on
and in particular now notice that the
the mobile phone is not necessarily in a
moving vehicle when we get a reading
right the person might be sitting in
their office or at home and so we have
to actually isolate the periods of time
in which the the person's taking of a
trip in a motorized vehicle and so we
have a set of heuristics that we use to
do that they appear to be fairly
accurate for example we seem to be
eliminating almost all of the bike trips
and and running trips and so on by using
just a simple set of heuristics to try
to isolate these these periods of time
when people are actually driving once we
do this we have 150,000 trips in the
Seattle metropolitan region in our data
set that have distance at least three
kilometers so here's a few examples this
is in the Redmond region and I'm
focusing on three particular trips and
here we have one vehicle going south on
Highway four or five and then two
vehicles Black is a second trip and
green is
third trip and the points are each of
the GPS readings along with an arrow
showing the direction and speed and this
vehicle has gone north and then headed
east and then doubled back on itself for
example so the goal here is to use
location data from mobile phones to
predict the probability distribution of
driving time on an arbitrary route in
the road network at an arbitrary future
time okay the challenges here are that
we have a very large number of possible
routes in in a road network of a
reasonable size so say in the Seattle
metropolitan region we can't possibly
enumerate all of the routes in the road
network and certainly if we wanted to
focus on trips that have taken exactly
the route that we're interested in there
would be very few or no trips that have
precisely the route of interest okay
although there may be many trips that
use particular roads that are included
in that route so we'll try to use that
idea okay and of course driving time
depends on the time of the week the
traffic conditions and lots of other
effects yeah so are the our method does
not do real-time prediction and there's
actually a practical reason for that but
what we do take into account is we take
into account weekly cycles and
congestion so that is the biggest effect
in this in this type of data but we're
not taking into account we're not
adjusting that according to our
real-time information about traffic
conditions and that's basically because
the windows phone data is not at high
enough volumes to to be able to do that
for individual roads you could try to
get some sense overall of say the entire
the downtown Seattle region or something
but you don't we don't have enough data
to really be able to focus on and get
great information about real time
traffic conditions on individual roads
if there are some companies that do have
access to data at that scale but but we
did not the statistical approach I needs
to be able to give informed predictions
for parts of the road
work they don't have very much data okay
like rural areas our approach captured
again captures weekly cycles and
congestion levels and it it needs to be
extremely computationally efficient
because this needs to work on
continental scale road networks we also
have to accurately capture the
probabilistic dependence between driving
time on different Road segments in the
same trip and this is important because
it turns out if that there's a strong
positive correlation between driving
speed on the first road and in your trip
and the second road in your trip even
after controlling for a for observed
traffic conditions and so on okay and
that's because for example some drivers
just drive more quickly than others
there's also effects due to the fact
that we don't perfectly observe traffic
conditions so the effect is that if we
get high measured speed on the first
half of the trip we're likely to have
high measured speed on the second half
of the trip and we have to take these
sorts of effects into account as we do
prediction in order to avoid under
predicting the variability okay to our
knowledge were the first to provide
accurate predictions of reliability of
driving time for complete and
large-scale road networks like the ones
that are used in in mapping services so
again the data sources is a location
pings from mobile phones we have to
convert before we can start fitting a
statistical model associated with roads
we have to match that to the road
network so the first step is a
pre-processing step where we are
estimating what route was taken by the
trip in the data set and how much time
was spent on each Road segments in that
trip okay so when I say link here I'm
referring to the link in the road
network which means the road segment
between two intersections okay so
there's lots of different methods that
can be used in this step but the upshot
is at the end of the day for every trip
and let's index the trips by little I
for every trip in the historical data
set we have the estimated route which is
sequence of networks I network links so
Road segments in that route and then
also the distance that we traversed on
each each link in that route okay at the
in the first link of the route and the
last link of the route we may not
reverse the whole the whole link okay so
that's why we need to estimate the
distance traversed on each link and
finally we need an estimate of how much
time we spent on each link in that in
that route ok so then from here we're
going to go on and pretend as though
these estimates are precise ok you might
ask well why not take into account your
uncertainty from this stage as you do
your estimation in the next stage the
answer is that we have done that in a
formal way it didn't help much in terms
of accuracy and it's also quite
computationally intensive so it doesn't
scale well to these large road networks
so the method that all proposed is
called trip it's actually short for a
travel time or liability that's a type
of travel time reliability inference and
prediction and we take the the driving
time of trip I on the cave segment of
that trip and we model it as the ratio
of several factors in the numerator is
just the distance traveled on that link
so all we're doing is converting from
time into speed here so focus on the
denominator here which is our model for
the driving speed on this particular
Road segments ok we model that speed as
the product of two latent factors the
first is a trip level effect and the
second is a link level effect so what
we're doing is we're decomposing
variability into variability that occurs
at the trip level and variability that
occurs at the local or link level ok
we're so I'm going to make some very
specific parametric assumptions here the
model is actually a little bit more
flexible than this but for for
concreteness we can say for example that
II I the trip
trip level speed effect has a log-normal
distribution this turns out to be a
pretty good assumption for the seattle
data and and that this this random
variable capturing link level
variability also has a log-normal
distribution once we conditioned on an
unobserved congestion state for that
link okay so we assume that the
congestion state takes one of a finite
number of values for the seattle
analysis we actually just use to
congestion states which works quite well
congested uncongested but conceptually
speaking you could certainly have more
than two but conditional on the
congestion state and on the link we
assume a log-normal distribution for
this random variable with an unknown
mean and variance which are allowed to
depend on what link is that which road
segment are we talking about and what is
the congestion state but so this is
almost the entire model all we need is
the model for the congestion states
right these unobserved congestion states
queue and we use a model which
incorporates probabilistic dependence
across the links of the trip okay so in
particular we use a one-dimensional
Markov model for these congestion states
so the the probabilities the parameters
of that Markov model are going to depend
on what road it is and also what time of
the week it is okay so this is how we're
capturing weekly cycles and congestion
levels okay so we take the the week and
we actually manually been into a set of
time bins that define things like
morning rush hour evening rush hour late
night and so on we allow these
parameters of the markov model to depend
on the time bin of the week so there's
some initial probability of congestion
and then some probability of
transitioning between congestion states
and these probabilities and transition
probabilities are allowed to depend
again on what link it is and what time
of the week it is
this model implies a normal mixture
model for the can for the log of the
driving time okay and that's important
because when we look at the data we see
a bimodal behavior and here I'm showing
I'm focusing on the five road segments
in the network in the Seattle network
that have the highest volume of data
okay the 4 i've actually gotten the two
highest volume on road segments that are
highways and then the two that are
arterial roads and then the highest
volume major road just for diversity and
you can see that the histogram is the
the log of the driving times for evening
rush hour okay and even when we restrict
to a particular time of the week like
evening rush hour we still see this
bimodal behavior in driving time so it's
very important that we capture that in
our model and and we seem to be doing
that effectively so that the curve here
is the estimated density from our model
in it and it roughly matches the
distribution of the observations yeah
that by would ah maybe as being related
to the fact that it's people trying to
turn right and the right plane is backed
up it has a nice little point and
actually that that is part of what's
going on here and it well no no no just
not that into you often have that like
Madison Avenue will be backed up and
everybody's trying to get an i5 is
backed up right leg you're going
straight then you can go through so so
yes if in the simplest case where you're
you're at the road segment that's just
before an intersection part of this
effect is definitely due to whether
you're taking a left or going straight
and you can min you can actually
redefine the road segments to account
for the fact that you're turning left or
going straight or going right but in
fact this doesn't fix this problem
entirely Co somewhat surprisingly and
the reason is because I in fact this
this effect can back up for more than
one Road segments so if you think about
if you're on highway 520 going west in
in redmond and and you have the option
of going north on 405 or south on 405 so
in other words highway coming into an
interchange with another highway you
have kind of backed up traffic
conditions due to the fact that you're
trying to take an exit half a mile down
the road and it's very hard to kind of
distinguish that manually happened and
thus account for this multimodality in
an explicit way okay so so um the we do
computation so so we have to be very
careful about how we do computation in
this setting we want to make sure to do
shrinkage in all of the important ways
right so that means that we want to try
to well okay so we need to we need to do
some shrinkage which means we need to do
some some bring in some bayesian ideas
but we can't run a huge Markov chain
here so we do maximum a posteriori
estimation and we do it very carefully
so that we get an expectation
conditional maximization algorithm that
has closed form updates in each
iteration we really need those close
form updates because we have to be super
super efficient when we do that we get a
very effective algorithm but that means
that for example we have to be careful
about what we are maximizing over and
what we are integrating over so here we
instead of just maximizing over the
conditional density of the unknown
parameters given that the observed data
we actually include these trip level
effects in that vector and we maximize
over them instead of integrating over
them and that allows us to get these
closed form updates and it also turns
out that there's not as much uncertainty
about these quantities as there are
about these so that it's a pretty good
approximation so in practice as we think
about computational versus and
statistical trade-offs in in accuracy I
think often the way that we account for
this in practice is we pick a
statistical model where we can do
efficient computation rather than trying
to fix the computational method after
we've specified a too complex
statistical model so again a big picture
we use expectation conditional
maximization and we we do this in such a
way we get nice closed form updates and
the estimation time is just 15 minutes
without utilizing parallelization the
algorithm can be paralyzed in a very
effective way we haven't yet done that
but even with that we're able to do very
efficient
estimation step and then the prediction
time has to be basically 50 milliseconds
or less at least if we want to use this
for mapping services in practice and and
we do hit that bar so it's about 15
milliseconds for prediction you might
argue that we don't have to worry so
much about the estimation time that it's
okay that the estimation time is a
little a little longer than this but I
would say no it's important to keep that
the the estimation time down as well
because in practice as we're trying to
improve the model if we have a very long
estimation times it makes it difficult
to to iterate on the on the quality of
the model yeah exactly it's by the truth
it was wrong anything as computer
subject once I was full hmm oh yeah
she's a poor background it's actually
really fast in 15 gonna say yeah but how
long is it 15 answers oh uh the trips
are typically I don't know how many on
average but some think something like 20
links on average these are not very long
yeah yeah okay so just to give some
results this is again the Seattle case
study I'm focusing first on three
particular routes in the road network
that happened to have lots of a high
volume of data just to see what happens
and but again our goal is to do
prediction for the entire road network
which is a much much harder problem and
here we basically get the distributions
are roughly correct okay we have to we
capture the long right tail in driving
times this is in that on the test data
the curve is the predictive density
again just restricting to evening rush
hour we get the variability about right
in the centering about right we capture
by modality in in the cases where it
occurs and you you know these
predictions are they perfect their no
they're not as good as if we were to do
a nonparametric estimation focusing
exactly on this particular route but
again the problem really isn't a much
larger scale than this and think that
what we're trying to obtain is a really
effective engineering solution here not
not a solution that's perfect for a
small number of high-volume routes right
so i would say i would argue that this
is this is quite good for our purposes
um so just to compare to some on the
entire test data set which is 36,000
vehicle trips in the Seattle
metropolitan region we compared to the
prediction method that was developed by
Microsoft called clear flow it's used in
bing maps and also to a simple approach
based on linear regression of the whole
trip driving time on several explanatory
variables including the route distance
the law the time bin and so on so very
simple linear regression on the whole
trip travel time so here I'm showing the
empirical coverage of predictive
intervals on the x-axis the theoretical
coverage on the y-axis the empirical
coverage so what percentage for what
percentage of test trips did the actual
driving time fall within say a
ninety-five percent interval or a ninety
percent interval and so what we're
looking for here is we want the curves
to follow the y equals x line and
there's only two methods that do that
successfully one is trip our method and
the other is this linear regression
model and it gets good coverage
basically because it's modeling at the
whole trip level it gets the amount of
variability about right so that's why it
looks good according to this measure but
all these other approaches like
Microsoft's method clear flow really
Quenneville really dramatically
underestimates the amount of variability
in driving time that's because it's
assuming independence of driving time
across the links of the trip okay um and
but it's also important to notice that
even if you have two methods that have
good coverage of predictive intervals if
you have one of the two methods has
narrower predictive intervals on average
then that method is taking better
advantage of information in the data to
do to give a precise prediction right so
this numerical measure is looking at
what's the average width of the
predictive intervals again for different
coverages for the predictive interval so
95% intervals over here for example and
basically our predictive intervals are
about fifteen percent narrower than
those from linear regression I'm that's
because we capture some the features in
the data more
tively like this by modality also
looking at the the accuracy of
deterministic prediction so we still can
get a single number for driving time as
a prediction and in in this case what we
do is we take the geometric mean of the
distribution because these are heavily
right skewed I mean we look at the
accuracy of those deterministic
predictions on the test data so
basically all of the methods do about
the same except that linear regression
does considerably worse it's quite a
rough method again but our method gets
about ten point four percent error for
deterministic predictions okay and due
to random variability in driving times
again you can't get 20 for these for
these predictions right clear flow
Microsoft method does slightly better
and the reason is because clear flow is
designed for deterministic high quality
deterministic prediction and it uses
hundreds of explanatory variables that
we're not yet incorporating into our
model so it's definitely something that
we want to do in the future but it's
interesting that despite the fact that
clear flows using hundreds of
explanatory variables that we're not
taking into account we get almost as
precise in term in terms of the
deterministic predictions as clear flow
oh I do just want to point out a fun
tidbit which is that after we started
publicizing the our method in January
Google Maps introduced a interval a
range of driving times on the Google
Maps interface so if you go onto google
maps web interface then you type in an
origin and a destination and then
specify a future time then it'll give
you a range for driving times now
instead of a single number to conclude
we've introduced methods for a
probabilistic prediction of driving time
for arbitrary routes in a road network
and shown it has quite quite accurate
predictions and I'll just stop there
thank you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>