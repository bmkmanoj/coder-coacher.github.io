<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Empirical Inference for Intelligent Systems | Coder Coacher - Coaching Coders</title><meta content="Empirical Inference for Intelligent Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Empirical Inference for Intelligent Systems</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-djRQacYkes" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
then our office director I saw Bennett
that you also had a short time at
Microsoft Research in the Cambridge lab
at the past for a while and he's gonna
share his thoughts on machine learning
thank you very much
can you all hear me so I'm very happy to
speak after Tony Hoare its first of our
great honor it also reminds me of my
time at Microsoft I was a very young
researcher he was already a Turing Award
winner and he was knighted during the
time I worked for Microsoft and he
nevertheless always had time to give
advice and I very much remember my
conversation with him and the influence
they had on my later career even though
I was probably just one of many people
that he talked to him it's also very
nice to speak after Tony's talk because
I think my talk is somewhat
complementary and maybe together we
cover two of the important directions in
computing if we think that the important
directions might be number one deduction
and number two induction so I will try
to say something about induction today
and you will also see that maybe they
reflected in the style of my talk
induction is something much messier than
deduction you get these beautiful slides
white background if you only the minimum
number of words you will see something
very different now but I hope you'll
find it interesting so I will talk about
empirical inference and I work at the
Max Planck Institute for intelligent
systems and it is easy to our long-term
goal is to understand the organizing
principles of complex behavior so I have
a few videos here you can see a white
blood cell in pursuit of bacteria he's
here frog trying to catch an insect up
here what else you can see a human doing
something which is called parkour
which is quite impressive motor behavior
and and finally you can see this group
of humans engaging answer in a complex
joint B or complex collective behavior
performing a double chorus Bar cantata
it's actually interesting there's
interest story it's an interesting story
about this
apparently Mozart when he visited
Leipzig went to see a performance of
this cantata singer Tim ham and he liked
it so much that he wanted to get hold of
the score which he didn't manage so he
just sat down in the evening in a pub
and wrote it down from memory so humans
can do impressive things and we are
obviously very far away from doing any
of this using artificial systems but we
tried to do some simpler things and use
machine learning to do these simpler
things and this is an example of was
something we can do this is when you go
to a shopping website so suppose you're
looking for this object which is called
at last circular cutter so it's a cutter
that you attach to a glass and then you
can make a nice round cut so if you go
to Amazon you shop for this then from
past purchases of other customers we can
learn that people trying to buy this
kind of item might be interested in
other things to go along with it and
then we can make suitable
recommendations and this is a live
example well not live but it's a real
example from the Amazon website so what
Amazon turns out recommending is this
thing called the balaclava down here a
baseball bat brand named hooligan and
this one is a little bit more curious
this is a Batman rucksack with bat wings
now a little more seriously or more
abstract if we think of this problem of
empirical inference or drawing
conclusions from empirical data and we
might think of the example of scientific
inference so suppose we are physicists
and we have measured two observables x
and y and we find the observe the the
measurements to lie approximately on a
straight line this case we might
hypothesize that there is a linear law
underlying the data but already like
it's noticed that even if we have random
data so likely
I was thinking of taking a quill pen
dipping it into the ink and then shaking
it over paper even from random data we
probably can find some slightly more
complicated mathematical expression that
explains these data points which then
begs the question would we call this a
law or when would we call it a law and
like this already has the right
intuition and thought that we would only
call it a law of nature if it's simple
in some sense this of course implies the
question what do we mean by simple and
what kind of guarantees can we make and
this is something that studied in
statistical learning theory nowadays so
a number of people especially
mathematicians have thought about this
problem for physicists this problem was
much easier as exemplified by Rutherford
who once said if your experiment needs
statistics you ought to have done a
better experiment so the science is in
the data and how you generate the data
and what you do with the data should be
should be a trivial so we don't believe
this anymore nowadays so let me show a
second example this is something take
for biology or more precisely from
perception so if you look at these
handwritten digits you have no problems
recognizing the digits or assigning the
images to the correct class label but if
I now apply a fixed permutation to the
pixels so the same permutation in each
image then these these patterns
representing the images suddenly look
much more complicated and you'll find it
harder to distinguish them even though
maybe you can see that the 0s tend to
look different from the from the threes
and mathematically if we represent these
images as vectors then the sequence of
the pixels doesn't really matter as long
as we perform sort of standard geometric
algorithms on these vectors so for most
learning algorithms these images might
be the same but to us as humans the
problem suddenly looks quite hard and
and that seems to be a contradiction but
of course the answer is that the
original images only appeared easy to us
because we've been trained on this kind
of data all our lives so in fact in
reality maybe both problems are
difficult and our brain can solve the
first problem because it has been
trained all our lives and extracted
statistical regularities and in fact in
the worlds of Horace Barlow the famous
neuroscientist the brain is nothing but
a statistical decision organ now I
showed you an example from scientific
inference one from perception
it looks like perception is hard science
is easy but in fact science can also be
hard if we look for the right problems
and nowadays people are solving problems
in science using non-trivial inference
and this is an example from
bioinformatics so here the task is to
classify a human a human DNA sequence
locations into the certain classes that
are biologically relevant I won't go
into the details and the interesting
thing about this program is that we have
huge training set 15 million examples if
we only use a few hundred or thousand
training examples in our performance
it's essentially chance level so we
might as well guess so if we look at
this data set as humans we would not see
structure it would look random to us but
if we use these machine learning methods
and I won't go into detail now then
using a sufficiently large set of
examples we will do very well and this
thing will not look random at all we
will have found some structure in the
world so we have methods nowadays with
which we can find non-trivial structure
that's not obvious to humans so in this
sense we have already achieved
superhuman intelligence if you want in
some very restricted domains and these
domains are characterized in this case
of these these hard inference problems
as one might call them typically
characterized by high dimensionality
complex regularities little prior
knowledge by which I mean we don't have
an explicit mechanistic model that
explains the data and as a consequence
of the need for big data sets and
computers and methods to process these
data sets automatically now one of the
crucial problems of machine learning is
referred to as generalization and I want
to give you a little bit of an intuition
why this is a non-trivial problem so
suppose you have observed this simple
number sequence here and I asked you
what's the next digit anyone would like
to guess 11 that's a very good guess so
there's a very simple law that explains
my
be 11 and this sequence even has a name
I can explain to you afterwards if
you're interested for the lazy Couture
sequence and but 12 is also fine in the
case of 12 we have this simple law if we
want 13 we could say it's the T Bonacci
sequence each number is the sum of the
three previous ones if it's 14 we could
say actually this is the set of divisors
of 28 it stops after 28 and there's
another set another nice continuation
the legend according to a former poster
combined the next sequence should
actually be 1 because obviously this is
a decimal expansion of Pi and E
interleaved and you don't even need e if
you want you can also just look up where
do these voltages appear in the decimal
expansion of Pi and you find it's in
position sixteen thousand nine hundred
ninety two and then we can look up how
the expansion continues there there's
even a website the online encyclopedia
of integer sequences where you can enter
these digits and you get meaningful
continuations and you find more than 600
hits last time I looked so obviously if
we ask the question which continuation
is correct or generalizes there's really
no way to tell and philosophers call
this the induction problem now we can
try to ask a slightly easier question
and which is what statistical learning
theory does this is more of a
methodological question we don't ask
which law is correct we ask what kind of
method should we apply such that we
typically come up with laws that
generalize so that's such that we find
laws that will probably do almost as
well in the future as we have done in
the past and this problem also
corresponds to a well-known problem from
philosophy this was called the
Atlantians problem or demarcation
problem by papa and it was attributed to
cont
by papa and it's the problem what
separates physics from metaphysics so
what kind of methods should you be using
such that you can call yourself a
physicist rather than a meta physicist
so this problem was formalized in theory
of machine learning or statistical
learning theory starting with a with the
simple task of pattern recognition by
people mean the case where we have some
inputs and we want to assign them to
classes plus and minus one and the
people who pioneered this direction
above Nick and Joe where've a plague and
showing Yankees both during their PhD
thesis in Russia at the Institute for
control science of the Russian Academy
of Sciences and they looked at this
problem where they they assume that we
have these observations each of the
Bayesian is a pair of an input and the
class never past minus one we assume
they're generated independent and
identically distributed from sum under
low underlying probability law so this
is an underlying random experiment and
our goal now is to infer a function from
these observations a function which will
also take this input points from this
domain X and produce it outputs plus
minus 1 labels a function which will
minimize the expected error and the
expected error is computed by taking an
average or expectation with respect to
the underlying distribution which we
don't know we only have data from this
distribution an average over this
function T or this function L which is
called the zero-one loss function if you
think you look at it for a second Y
takes values plus minus 1 f of X takes
values past minus 1 so this can be
either 0 or 2 and we divide by 2 so this
quantity is 1 if and only if f of X is
not equal to y 0 otherwise so its
expectation is the expected error what
data generated by the same same unknown
underlying regularity we can't compute
it because we don't know this
distribution so what we need is an
induction principle to find a function
that both not exactly minimize this risk
so-called risk but with that will at
least get close to minimizing it and the
duction principle that vapnik germany
studied is called empirical risk
minimization it consists of replacing
this quantity that we would like to
minimize with its empirical counterpart
so this is the same quantity but only
evaluated on the training points this is
also called the training error
empirical risk and now we have to
minimize this quantity over a class of
functions now the question is whether
this is consistent so consistent means
roughly speaking it statistically to the
optimal answer in the limit of
infinitely many data points and the
turns out the answer is no without
additional assumptions and the
additional assumption that we need is we
need a restriction on the size of the
function class so it turns out that from
the law of large numbers we can easily
prove that this quantity for any fixed
function this quantity will converge
towards this one in probability but if
we have a large set of functions it
turns out even if for each fixed
function here this empirical risk or
this training error converges to the
true error in probability this doesn't
imply that the minimizer of the training
error or the minimum of the training
error which is what empirical risk
minimization tells us we should be using
converges to the minimum of this other
curve so it turns out this is exactly
related to the difference between
point-wise convergence and uniform
convergence that you might know from
mathematics and I won't go into details
on this but the main message is whether
the minimum converges depends on how
last these function classes if the
function class only contains a one
function then the law of large numbers
is enough but if we have a huge function
class it turns out we have to worry
about the complexity of our function
class into this and the public
Germanicus came up with this notion of
vc-dimension which is a measure for the
richness of a function class and it
turns out if this VC dimension is finite
then empirical risk minimization can be
proven to be consistent so to work in
the in the limit no matter what the
underlying probability distribution is
so that's maybe the most important
theorem of statistical learning theory
and the VC dimension is a combinatorial
quantity I'll just briefly say it but it
doesn't if you don't follow this it's
not important at this point it's defined
to be the maximum number of points which
can be classified in all
simple ways using functions from our
class so for instance if if we use the
class of linear functions linear
classifier so separating datasets into
two classes with a straight line it
turns out if we have three points we can
separate them in all possible ways two
to the three possible ways using such
functions once we have four points we
can't do this anymore
therefore the VC dimension will be three
and more generally in a d-dimensional
space is the VC dimension using this
kind of construction or using this
function class will be a d plus one so
in D dimensional space is d plus one but
it turns out so that's in a sense if we
would like the VC dimension to be small
if we want to learn the smaller the
faster we can learn the fewer data
points we need at the same time we want
to work in high dimensional spaces in
this case the VC dimension grows with
the dimensionality but they are
interesting cases that are nowadays used
in machine learning where we try to
separate data points not just by any
straight line but one which will induce
a large margin of separation in which
case one can prove that the the the VC
dimension will not grow as fast so it
turns out this case it will be inversely
proportional to the square of this
margin of separation so to come to the
conclusion of this part of our learning
theory and the kind of results that one
can then derive in terms of VC damage in
the kind of generalization error bounds
or methodological prescription if you
want the kind of mounts looks like this
so the bounds will tell us that the test
error of our system will be upper
bounded by the training error plus some
confidence term with high probability
where the probability we could for
instance address to 99% which case we
have this Delta equal to 1% which
appears in this formula in this
confidence term that characterizes how
much the training error might mislead us
about the test error that we haven't
seen yet the confidence term that
depends on this VC dimension end on the
sample size the number of observations
that we have seen so if we have seen a
lot of observations we might be able to
learn
to use a large VC dimension and still
have a small confidence term and and the
interesting thing now is if you think
about applying this so suppose you try
to learn something that cannot be learnt
let's say you take the telephone
directory of Cambridge and you try to
learn a mapping that will take us and
input the name of a person and produce
as an output their telephone number so
we believe we can't really learn this we
could only memorize it there's no unless
people that I'll use some kind of
algorithm to make up the telephone
number so suppose this is really not
possible then we would find to learn
this mapping well with a small training
error we would have to use a huge system
a huge neural network with millions of
parameters or maybe an actual lookup
table which is also very complex and
mapping in this case we might have a low
training error but this quantity will be
very large so we won't be able to to
guarantee that our system will work in
the future a new person moves to
Cambridge we won't be able to predict
their telephone number on the other hand
if we use a small system such that this
quantity is small we will find that we
won't be able to actually explain the
given data to find it to find a Mak
mapping that will correctly reproduce
all telephone numbers of people that
already have a telephone connection in
Cambridge so this this trade-off between
number of data points and complexity of
the function class and this is a caste
into these kinds of inequalities and
that's one of the crucial aspects of
what's what's happening in machine
learning and in one way or another
we'd using a regular regularization
theory or complexity of function classes
or Bayesian piles one way or another you
have to take this into account when you
do machine learning now what's a second
aspect that's interesting that I want to
spend a little bit of time talking about
is this method of using kernels to
construction classes so I told you it's
important to use function classes whose
VC dimension can be characterized or can
be controlled and kept small at the same
time we want function classes that are
rich enough to explain real world data
and there's this nice trick using
kernels and the idea is to
the data into a higher dimensional space
and in this space to do something linear
and in the linear case we know how to
control the VC dimension by maximizing
the margin of separation so in this
trivial example here we might start with
two-dimensional data the true decision
boundary might be this ellipse here so
we want to separate blue from red now
suppose we map our two dimension points
into a 3-dimensional space
by this three-dimensional mapping so we
compute all monomials of degree three
then it turns out just since this is an
axis aligned ellipse this separation
boundary will be a straight line would
be a hyperplane in this 3-dimensional
space
and moreover it turns out if we look at
this mapping if we took two points X and
X Prime and both map them both into this
space and it turned out we could rewrite
the dot product in the three-dimensional
space as the square of the dot product
in the original space so this that
seemed like a funny coincidence but it's
actually a very nice coincidence because
it means in this case since we since we
know how to compute the dot products in
this three-dimensional space as a simple
function of something done in the input
space we might not have to carry out
this mapping individual
three-dimensional space in the first
place so maybe between two and three by
mentions that doesn't matter but in in
fact the way people use this the space
on the right hand side might even be
infinite dimensional so we could do a
similar kind of trick for infinite
dimensional data and for it turns out
there's a certain class class of kernel
functions which correspond to dot
products in associated spaces these
spaces are called feature spaces or
reproducing kernel Hilbert spaces and
whenever we use such a kernel we know
that actually we're doing nothing but
computing a dot product in such a linear
space that's very nice because in these
Hilbert spaces or dot product spaces we
can construct geometric algorithms we
know how to compute angles distances etc
so these functions these kernel
functions K are called kernel functions
and the correct class
functions that one should use is called
positive-definite curls they're well
known from approximation theory I won't
go into the details there's a simple
definition of what's the positive
definite kernel and it's a reasonably
input symbol to prove that a kernel
through a function of two arguments
which is symmetric a kernel is positive
a definite if and only if there exists a
map into a Hilbert space that says that
the kernel corresponds to the dot
product in that space okay
so the most famous use of these kernel
methods is support vector machines
support like the machine you have a
classification problem with two classes
in the simplest case you map them into
high dimensional space compute a large
margin separation and if we do this then
we get a nonlinear separation in the
input space and depending on what kernel
we choose the current we determine this
mapping depending on what color we
choose this nonlinear separation will be
more or less nonlinear in the input
space and here I'm actually turning up
the non-linearity by choosing a Gaussian
kernel whose width I am turning down and
you can see I'm getting more and more
nonlinear decision boundaries here so
this is also an example of a nonlinear
decision boundary which is linear in
some other space but more seriously so
these are further examples of nonlinear
decision boundaries or nonlinear
surfaces which all corresponds to hyper
planes in a suitable Hilbert space so
one can do various things with it in
this case we actually work together with
computer graphics people to construct
this morphing algorithm another
application of or another algorithm that
can be performed in terms of kernels is
called kernel component analysis so in
this algorithm we are not trying to
classify data but we are trying to look
for variance in the data for structures
or principal component analysis is a
well-known method from statistics that
looks for direction of large variance
and it turns out if we look for these
directions in the hilbert space
associated with a kernel then these
directions of larger
various variants and correspond to these
nonlinear features of the original data
and in this case we have a data set with
three clusters it turns out that to the
first two directions separate the data
into the clusters so this first
component separates this cluster from
this the next component then separates
this cluster from the union of those two
because this separation has already been
protected out it's in principal
component analysis so we just need to
care about separating this from the
Union and then the higher-order
components look for structure within the
clusters but also in an interesting way
so for instance this this one looks for
a structure in this direction and this
one looks for a structure in this
direction etc in the number of other
methods for a dimensionality reduction
can be viewed as special cases of this
so in the next few minutes I want to
show you a few examples of machine
learning and then I will move into the
second part of the talk where I will try
to motivate that actually machine
learning based on statistical
information is not the full story and in
many cases we should be thinking about
where this statistical information
actually comes from what underlies
statistical associations and this will
lead to will lead to a discussion of
causal learning so first a few examples
so this is an example of a project we
did together with people in medical
imaging it's a nice example because it's
now it was licensed to Siemens and it's
incorporated in a certain type of
medical scanner here the problem was
that so we worked together with a
colleague who was working on a pet and
mr scanner so scanners led to positron
emission tomography and magnetic
resonance at the same time so this is a
non favorite problem because you have to
construct detectors for pet that work in
high magnetic fields and but from the
medical point of view it's should be in
many cases preferred to a traditional
scanner which would combine pet positron
emission tomography with computer
tomography which is an x-ray method
x-ray is of course a much higher
radiation damage so traditionally these
two methods were combined which had the
advantage that one could come
what's called the attenuation correction
using this CT signal so the the pet
signal gets attenuated as it passes
through matter and it turns out that
this attenuation can be very well
predicted from the CT image so if you do
these things together then you can
normalize the pet image correctly which
means you can identify a tumor no matter
whether it's right in the middle of your
head or close to the end this wasn't
possible anymore using magnetic
resonance PET scanner when in this case
what we did is we used a set of training
data where we had taken both mr and CT
images for patients a set of training
data to learn this regularity to learn
the mapping from mr to CT so to give an
NMR image constructor synthetic CT image
and then we can use this CT image to
correct the pet image with almost the
same results
so here's another example this is using
something called reinforcement learning
which is a little different from what I
told you so far but it's also a
data-driven learning method so in this
case we have this little Japanese game
where you're supposed to catch a ball
with this Cup and in the first image you
saw our PhD student teaching the robot
arm roughly what is the movement and
then afterwards the system first system
tries to reproduce this movement and
observes what the ball does what we then
feed back into the learning algorithm is
the distance of the ball as it falls
down passing this plane a horizontal
plane defined by the position of this
cup The Closer this distance is the
better is the current movement and
therefore the system tries to adjust its
so-called policy or it's both motion
generation method such that this
distance gets smaller and after 15
trials it's already significantly better
and so this is now the final system
which i think is after about a hundred
trials and a hundred trials is similar
to what it takes a human if you have
never played this game before
but the difference is that if you've
done it the first time you
reproduce it whereas this system once it
has learned it can almost always do it
so here's a second example of a learning
system so in this this is an example
from image processing so here we have a
an image sequence taken let me pause
this taken through the exhaust of the
air conditioning system in our building
of a distant chimney and the raw
sequence you just saw so what we now do
is we try to explain this raw sequence
as a underlying image latent image that
we haven't seen involve with something
generated by turbulence and using the
assumption that we think that the
underlying image is stationary and you
will now see the reconstruction in the
middle and actually you can see that the
reconstruction already after 13 images
which is where I post it is quite good
and it gets a little bit sharper here at
the end it's essentially as sharp as the
ground truth image here oh it's the same
thing again okay so this was the raw
sequence and now you can see in the
middle the reconstruction getting
sharper okay so now tell you a little
bit more about kernel methods and what
people do with kernel methods nowadays
so remember kernel methods we were
saying we have which use a kernel this
kernel generates a mapping of our data
space into an Associated reproducing
kernel Hilbert space in which we can
perform some simple linear methods now
let's assume we want to construct the
simplest possible learning algorithms so
this is an example that we did for a
book that we wrote about kernel method
so I was really thinking what's the
simplest you could do in this case what
we did is we took two classes of data
points we mapped them into the feature
space so let's say this is already the
image in the feature space now and now
we will assign a test point which could
be anywhere to the class whose mean is
closer so we compute what it means and
then assign this to the class whose mean
is closer obviously this induces a
hyperplane decision boundaries to the
set of all points that's closer to here
than to here is actually this half space
from the other half space by this
hyperplane and if we just substitute
this into the decision rule so we
compute this distance in terms of dot
products so it can be written in terms
of kernels compute this distance is
again expressible in terms of the
products then we get to a decision
boundary which looks a well-known to
steady stations because what this
decision boundary corresponds to is it
does a dentist's equal density estimate
of the one class is a thousand windows
estimate you put a little window
function or a little kernel on each data
point and use some of the data point
series like you're smoothing out the
data points to get a density estimate
and it computes a density estimate for
the other class and then it essentially
looks which class is more likely to to
perform the classification now that's
interesting because this means that we
have now represented each data set each
data set somehow as a density estimate
and actually it turns out and this was
area of active development in the last
ten years that it's very interesting to
think about such mappings of data points
not just individual points but sets of
points into repeated kernel urban spaces
and to think about what kind what kind
of information these mappings retain we
can ask what kind of information do we
retain if we map a set of points into
such a feature space and it turns out if
you use a linear kernel we just retain
the mean because we're mapping each
point into the space and then computing
the mean so that's now out our
definition of the map for a set of
points that's trivial so if you use a
linear kernel we just remember the mean
if we use a polynomial kernel of degree
and it turns out what we retain is
exactly the first end moments
statistical moments of a data set if we
use what's called a strictly positive
definite kernel and one example of this
is a so-called Gaussian kernel it turns
out where with no information so this
mapping is injective even though we map
to one element of our earth which is
available space we can reconstruct all
the points that have contributed to this
map from this volume element in
principle and we can generalize this to
distributions so rather than mapping a
set of points we can map a whole
distribution and the way we do it is
we sample points from the distribution
map each of those points into the
repetition kernel Hilbert space and then
compute the expectation so the average
over this sampling procedure again it
turns out if we use a linear kernel then
trivially we just retain the
expectations that it means we represent
the distribution by its mean of course
that loses a lot of information if we
use a polynomial kernel we represent a
distribution by its moments up to a
certain order and if we use what's
called a characteristic kernel and again
a Gaussian is an example of this then we
retain all information so we can now
think of our distributions and whatever
we're going to do with them and we can
think of them as elements of this
Hilbert space and we can perform linear
algebra on them and one can do various
interesting things with this and this
has been a very active area of research
in recent years we can first we can re
we can view interesting special cases
known from statistics so this thing here
is called the moment generating function
it's a special case of this so-called
kernel mean map for a particular type of
kernel we can express things like
independence test so if we have two
variables x and y and we want to test
whether they're independent we can do
this using a geometric construction in
such a Hilbert space we can also do
things like homogeneity testing Bayes
rules we can give a physical
interpretation related to physics to the
physics of wave optics and found over
diffraction and finally one interesting
example that I wanted to talk about for
two or three minutes is related to
probabilistic programming and computing
of random variables functions of random
variables and I think this is maybe it
of interest because it's also related to
programming or maybe to the future of
programming in some respect so in this
case so I told you we can use this
mapping here to represent a distribution
in a Hilbert space and then perform
things on this distribution in that
Hilbert space now let's assume this
distribution is the distribution of a
random variables so you have some
quantity
an underlying experiment and every time
you perform the experiment you get a
slightly different value so it's not a
fixed value but it's a distribution now
suppose you have functions that are
defined on the values that your random
variable can take so functions let's say
you have functions or operations on data
types you have strings integers etc and
associated functions that take these
data types as inputs and produce other
data types as output but now you want to
lift these operations two random
variables taking values that are of
these data types so now you have a
random variable that takes values that
are strings or graphs or whatever and
and you want to lift whatever you can do
with strings in graph to distributions
over strings and graphs it turns out
that's something you can do using this
method and essentially to do this and I
think I'll skip a few details because we
lost a bit of time and what you to do
this what you need to explain is given
an element of a repetition countable
space which in the general case takes
this form here plus limit points but let
me not talk about this so in the almost
general case an element of the
revolution curve space takes this form
and here's another random variable which
is reproduced represented like that
you'll now have to explain and show how
to compute functions on these random
variables and this is the formula which
i won't go into details on but maybe I
should just remind you computing the
distribution of a function of random
error this is non trivial so if you for
instance if you had two random variables
that are both Gaussian and now you
define a third random variable which is
the sum of those two then you know that
the distribution will be the convolution
of the two original distributions which
is again a Gaussian but that's about as
much as you can go
analytically so in general if your
distributions are not Gaussian or if the
function that you are applying is not
just addition but something complicated
there's nothing you can do analytically
so this I think is an elegant method for
this kind of problem and we've only just
started working on this and I should
also point out
what's what's nice and what makes this
fit nicely with programming is that we
can define kernel function not just on
vectorial data we can define in
principle kernel functions on any kind
of data we just need to prove that this
positive definite is property holds true
so there are kernel functions for
Strings graphs lists you name it so for
various types of data types or or images
DNA sequences so I think this is an
interesting direction to think about in
the future so with this I'm going to
move to the last part of the talk which
is about causality and I think I'll try
to keep it relatively short just
motivated and then show you one
application which is related to
causality which I'm quite excited
excited about so I will motivate it
again with a shopping example from the
website of Amazon and this is someone
else found this a nice example the
example is someone shops for a laptop
rucksack so it's a rucksack with a
special padded compartment for a laptop
and then Amazon recommends that the
person should buy a laptop to go with a
rucksack and probably these purchases
are statistically upended it probably
happens significantly often that people
buy both at the same time but
nevertheless it seems seems to be stupid
to recommend to someone who's looking
for a rucksack that he or she should
also buy a laptop because intuitively we
would think that the buying the laptop
is the cause for buying the rucksack but
not vice versa
now both things contain information
about each other so both purchases
contain information about the other one
but the reason for this is that so the
course contains information about the
effect because the effect is controlled
by the course in some sense whereas the
effect sarita yes the the effect
contains information about the course or
the other direction because it carries a
footprint of the course so we have it's
clear that there's information in both
ways and if we measure it statistically
this is it's called the mutual
information and it's a symmetric concept
so it's just the same number no matter
which way around which seems strange
because from the course and point of
view it looks like the reason why a
contains it mention about B is something
else the other way around so it seems
like there is actually a directionality
there that's lost once we start talking
about statistics so from this point of
view or from the point of view of
causality statistics is an epiphenomenon
there's something more fundamental
underlying statistics and this is
causality and of course there are many
more layers below causality so the
causal description is from my point of
view one layer below statistics but
there are many further ones and maybe
towards the end you get to couple
systems of differential equations and
maybe maybe at the end you have the
Schrodinger equation or even even
something more fundamental but my goal
now is to tell you a little bit about
just one step down from statistics so
what generate statistics and just to
throw a motivate a little bit more the
difference between statistics and
correlation you all know this example of
the storks so we looked at the human
birth rate in European countries and the
frequency of storks we see a strong
correlation there's a nice paper about
this the countries with high birthrate
tend to have more storks but of course
now if we wanted to intervene in such a
system so if we were suppose you're a
politician and you want to have a higher
birth rate in your country you won't do
this by making sure you get more storks
so we wouldn't believe that this kind of
dependence also makes the prediction
about interventions so it's it's an
observation purely observational
dependence which holds true in what we
call an iid settings or independent
identically distributed data but it
doesn't say anything about the effect of
interventions so statistics doesn't
predict the effect of interventions but
there is a relationship between
causality in statistics in the first
person who I think understood this was
this physicist and philosopher
Reichenbach he wrote a book called the
direction of time and he formulated the
common core's principle and this
principle says that if we see or if we
encounter two observables x and y that
we find to be statistically dependent
then there must be a third observables
that that causally influences both so he
says that no there's no statistical
dependence without
underlying causality this means in the
generic case we would have said Causley
influences X&amp;amp;Y so these arrows are
causal as special cases that might
coincide with X or Y in which case we
would want to get one of these graphs
now fighting back goes on to say that
this quantity is said then screens x and
y from each other in the sense that
conditioned on that the observables x
and y become independent so that's maybe
not so easy to understand but it's
actually almost equivalent to this model
that might be easier to understand and
this is maybe the current gold standard
for how people think about causality
it's called a functional causal model or
a structural causal model or a nonlinear
structural equation model and this is
interesting it's also somewhat related
to programming I think even though
people usually don't think about it that
way so the idea here is we have a set of
observables x1 through xn they are
embedded in a causal structure described
by a directed acyclic graph again the
arrows in this graph represent direct
causal links and on each observer we
further assume is a function of its
parents in this graph and a variable UI
which is not shown here so these
variables ui now we assumed to be
independent random variables these are
sometimes thought of as noise variables
or a unexplained variables that's why
they're called you everything else is
deterministic so these functions are
deterministic the only source of noise
are these variables you and this
so-called structural equation is
actually I wrote it as an assignment
because it's not really an equation
we're not allowed to solve for the
right-hand side but this equation says
if we change some values over here
we know how this one would change but if
we change this one it will not affect
these variables it will only affect
those variables that have X I as one of
their parents so this is an assignment
not an equation and it turns out that by
feeding in these independent random
variables these noise sources that you
each know
from the graph or the graph structure
entails a certain structure of the joint
probability distribution so I told you
this is the sources in toys everything
else is deterministic but since these
are noises we get a joint probability
distribution of all variables and it
turns out that this distribution carries
the footprint of the graph structure and
and this is described by what people
call the causal Markov condition this is
also as an aside a so-called graphical
model but actually and every graphical
model can be written this way but this
is a little bit richer description than
a graphical model this contains
information that a graphical model
doesn't contain anyway so this a brief
summary what now people what it was that
caused the the central question of
course the inference now is suppose
you're given this joint distribution can
you recover the graph structure for me
so can you infer what is called what is
effect what are these errors just from
observational data it's a complete
knowledge of the distribution and it
turns out even complete knowledge is
often not enough so it turns out that if
we have at least three variables we can
test for such structures using
conditional independence tests because I
told you in the previous slide this
structure together with the independent
noises implies certain conditionally
independent statements and this works
only for three or more variables because
every conditional statement connects
three variables now it's interesting to
think about what do we do we have only
two variables and that's a problem that
we thought a lot about in the last six
or seven years and we have developed and
analyzed various methods that can handle
the case of two variables it turns out
we have to make additional assumptions
so it turns out the three variable case
can be done using this assumption of
Independence of the noises I talked
about before in two variable case it
turns out we can make an assumption of
independence of noises and functions or
independence of inputs and mechanisms as
we like to call it and then solve that
problem but I don't have time to discuss
this now
so just to flag what kind of problems
are people interested in and now I'll
think I'll go to the last last
application because I would like to
reassure you about that and this is an
application in the field of exoplanet
search so here in this application we're
working with data from this Kepler space
telescope or satellite circling around
the Sun so this is a telescope that has
been staring at the same patch of the
Milky Way for about four years taking
pic taking a sequence of pictures half
an hour of half an hour exposure each
and this is how the telescope looks the
field of observation is close to this
constellation cygnus one image looks
like this so this is a set of CCDs
imaging chips and we're interested in
150,000 stars in this field and what
we're looking for is transits
so a transit means we're looking at a
star which is occasionally partially
occluded by its planet whenever this
happens we get a slight dip in the light
curve this dip is quite small because
planets are much smaller than stars so
for instance the earth is about 1% of
the diameter of the Sun so 10 to the
minus 2 so the area is about 10 to the
minus 4 of the Sun and the signal is
roughly a 10 to the minus 4 signal in
this case and as seen from space the
earth transit would last about half a
day it would be visible only from half a
percent of all directions now we found a
lot of planets using it but nothing
quite like an Sun because of this it's
difficult to find to begin with and it
it only produces one transit per year
and we have only four years of
observations so this is our problem and
it's especially hard because we have a
lot of systematic errors because the
spacecraft changes the spacecraft has to
stabilize itself very precisely the
stars have to be very precisely on the
same pixel by the even if we shift by a
fraction of a pixel it makes a
difference and it has to compensate for
things like radiation pressure fluids
from the Sun etc and so these noises are
bigger than the actual signals we're
looking for and this is the kind of
thing that we know when
to model causally and the way to do this
is so our data so this is the true thing
out in space the let's call this
variable Q that's what we're interested
our quantity of interest we can't
measure directly what we can measure is
these observed light curves and these
are our light curves of pixels belonging
to the same star so the Stars smeared
out over a number of pixels and you can
already see that these pixels behave
slightly differently some of them
decrease others increase the scale is
blown up of course and the reason for
this is that the star moves around a
little bit on the CCD just by fractions
of a pixel and so what we measure is
actually not the star itself but we
measure some derived quantity that's
influenced by the star and influenced by
these noise sources now reconstructing Q
from Y is impossible but an for but
fortunately we have a little bit more we
have lots of other stars we have this
whole measurement field and we have the
light curves of the other stars as well
and the idea now is that these other
stars are also affected by noise and
they are partly affected by the same
noise processes because if you look at
these curves you see some similarities
like this one looks quite similar to
this one even though these stars are
probably light-years apart out in space
so we have a situation where if we have
a joint noise source which we'd like to
remove we know that it effects different
variables and we want to reconstruct the
underlying hidden quantities and this we
call these related quantities
half-siblings because they share one
parent and the problem is similar to
something we call the milkman problem
which looks as follows
so in the milkman problem so you know
there are these and statistics they may
not be true but some people claim that
as much as 20% of all children might not
have the father that they think is their
father so in this case in England then
such a child is called the milk and the
milk man's and so now the problem is for
us we have a set of children and we
don't want to find who's the milkman
child but actually we want to
reconstruct how the milkman looks from
the set of children without having seen
the
so this is obviously a non-trivial
problem and but you can understand the
intuition probably so X the other stars
know something about our star of
interest because they are affected by
the same noise
moreover X is independent of our star of
interest because the star of interest is
light-years away so there's no way this
star would directly cause the influence
the other stars that influence X or the
systemic effect of the instrument and so
therefore if we try to predict our star
of interest from the other stars and
remove this prediction we will only
remove that component that's due to the
joint noise source so that's what we
want to remove and we can formalize that
we call this a half-sibling regression
because this quantity R there's the
condition expectation it's also called
regression in statistics so the best
explanation of Y in terms of X and then
average the expectation I mean I think I
should come to the conclusion but I just
wanted to tell you that one can prove
some results about this and I think it's
quite nice what one can prove if we make
the assumption of additive 'ti so we
assume that the unknown star has is
affected by the noise in this additive
way so it's some non-date possibly
nonlinear function of the noise but it's
added on if we moreover assume this
causal structure which as which implies
that X is independent of the Q and if we
finally assume that in principle the
information about the effect of the
noise about f of n is there in the other
stars so there exists some function
precise at this such that this equality
holds true we don't need to know this
function so if we assume all this then
it turns out that by subtracting the
regression so this is the regression
from the observed Y we recover a
quantity Q hat which we can prove is
equal to the unknown star Q minus its
expectation so this means we can
reconstruct the color the star up to its
expectation and this is an equality of
random variables so that means every
time
and we make an observation the
probability that this quantity here this
is a random variable and this one so
this is the one that we can reconstruct
this is the true one that we don't know
the probability that the values that we
generate differ is zero so this is as
much as you can ever hope for in
statistics and we can generalize this we
can weaken some of these assumptions but
I think I will I will skip this I'll
just show you the kind of results we get
so this is a typical results that the
astronomers get so that's the light
curve they they also have a pipeline of
pre-processing and trying to remove
systematic errors and this is the kind
of result that we get and we have a lot
more about it I don't show you so much
but I should also say that exoplanet
search is not just about removing noise
but it also it's about finding these
tips so this is an exoplanet here but
that's a very easy one finding them
integrating information over multiple
periods etc this is involves a lot of
other things but we have also done that
and produced a list of exoplanets
candidates and some of them have already
been confirmed and I think maybe at this
point I should I should stop I was also
thinking of involving you in a
discussion about the future of AI we can
do this afterwards or in the break if
you still want to discuss and but I
think I should conclude now and thank
you for your attention
colonel you retain all the information
of the observe data is that a problem
space-wise - or is that a problem is
that a problem space-wise as you're not
storing anything specific it's usually
considered a feature the Gaussian kernel
that we retained all the information so
in principle and I suppose it depends
what you want to do with it exactly but
but in principle it's an advantage that
you retain all the information now you
can approximate things and if you
represent them with limited accuracy
then in the end you might lose
information anyway and if you have
finite accuracy you may not be able to
distinguish between a kernel that's
characteristic such as a Gaussian our
kernel that's almost characteristic or
if you take a Gaussian and you limit it
to compact support or something like
this you would get something that's
almost the same maybe up to a massive
machine precision but mathematically is
no longer characteristic so this is the
tape or the mathematical analysis it's
it's nice to have this but of course it
would be interesting to propagate this
through and think about what happens to
all this if we represent things we find
out accuracy
always been a fool join us for dinner
tonight and possibly at the dentist as
well so what</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>