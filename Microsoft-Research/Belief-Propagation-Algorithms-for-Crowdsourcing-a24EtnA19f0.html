<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Belief Propagation Algorithms for Crowdsourcing | Coder Coacher - Coaching Coders</title><meta content="Belief Propagation Algorithms for Crowdsourcing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Belief Propagation Algorithms for Crowdsourcing</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/a24EtnA19f0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so let's start so today our machine only
seminar speaker is a Chan deol cha is a
from PT is a PhD student in user elven
computer science department and his
research interests focus on machine
learning and these applications Chan
received MSRP a fellowship in 2011 and
the same year he did an internship with
a jump higher and the Christmas today
he'll be talking about crowdsourcing
chapter thank you thanks Danny full of
introduction Oh could you heard me could
you hear me ok so this is a joint work
means my friend Jim punk and my advisor
Alex Isla it was presented he nips last
a year so far sourcing is the process of
outsourcing the problems you want to
solve too cuz of peoples and recently
has been become a very powerful approach
for solving problems that computers
cannot solve alone and using a human by
by harvesting human human intelligence
and it's also very powerful to gather
information and data form cause of
people usually can do a lot of powerful
things however crowdsourcing can be also
backfires if you don't cheat them very
carefully because humans tends to be
very unreliable and they are very
different from each other so different
people may have different opinions it's
a problem how to denoise the labels form
clouds and how to aggregate the
appearance from different peoples so
today I'm going to focus on the
consensus algorithm for this crowd
sourcing setup so to be more specific
assume we have a set of images which
have unknown two labels binary labels
for example in this case baby you want
to I identify whether there are tax in
the image so assume there are too many
images so we hire several people we open
a call on one of the crowdsourcing
platform such as Amazon Mechanical Turk
and hire seller workers so now each
worker is assigned with the subset of
images and each image is again labeled
by multiple workers doing this can make
sure the redundant information to help
the accuracy so but because the workers
are very unreliable so they may have
different labels even worse for the same
image the problem is how to aggregate
these noisy labels together estimation
of the unknown shoe label di so the the
criteria function here is to minimize
the bitwise average or just the expected
number of mistakes you can make the
number of wrong images because the
workers are very diverse so I measure
the diversity using this reliability
measurement qg which is just the
accuracy that the probability of for the
workers to give the correct answer now
we have experts which have very high
accuracy qg close to one we have
spammers they are lazy people that tends
to just give random answers ignoring the
problem yourself so they have qg
approximately two half and also we have
adversary's that tends to give opposite
answers for reasons for all sorts of
reasons so there are many algorithms for
doing this problem orbit a naive
algorithm is just to do majority voting
so so here because the labels are plus 1
and minus 1 so i write it as a sum and
the same formula so but this problem is
definitely not very good for counting
the diversity of the workers because it
treats all the workers and
other labels uniformly so in this plot I
show one of the results of one of one of
my experiments so this black line is an
Oracle lower bound that you can get if
you exactly knows the accuracy of each
workers so this is what you can get if
you have some Oracle knowledge and
majority voting is here so this is the
average so that's a very large gap that
you can close by potentially combining
diversity of different workers
resurgence all right like the Oracle the
world ah so it's it assumes the qg is
exactly known to you but you also pick a
distribution over the ques uh so so once
you know the exact manuals you know it
doesn't matter the distribution or the
disputed this plot change wouldn't it
the distribution oh yeah sure sure this
is a one example this is adjusted at Dom
institution I was like a bear for this
is just some more oh this is some
article simulation just to show some
concept but I will show more experiments
later the actual what the true Oracle is
within number works well yeah I know in
this case yeah I go definitely go back
to this plot later so another algorithm
is a iterative algorithm proposed by
David kaga save on all and over action
so so this is a quite interesting
algorithm it's basically a weighted
version of majority voting so here each
label lig is weighted by a confidence
level ygi it's a real value and this yg
I is iteratively calculated by these two
linear updated rules so basically are
intuitively you can interpret this
linear updates as sort of passing
messages between the workers and the
tasks and this algorithm is very
interesting actually oh I will refer
this algorithm as Kos for convenience in
this talk so it's very interesting
algorithm people actually can show some
optimal properties on the sample
complexity in asymptotic case
but however it doesn't perform very well
and sometimes in practice and you may
wondering how how people derive this
algorithm actually that's no derivation
for this algorithm in the paper they
just write this algorithm follow some
intuition of brief obligation but
there's no solid connection there so
it's curious to understand what this
algorithm is exactly doing and so we can
improve it and understand the advantage
and disadvantage of this algorithm so in
this talk I'm going to present a more
general algorithm that actually unifies
both majority voting and QF's so both of
them will be an extreme case of our
algorithm with some special assumption
and out by smooth mp3 majority voting on
the qsr we can actually get much better
performance so blue line is our
algorithm and the red line is Q s I will
show more details on this platter and
our algorithm have a principle
derivation another set of algorithm I em
algorithms there are a lot of young
algorithm studying form on 1979 to very
recent papers and these algorithms are
basic basically its usual people will do
so basically they will build a
generative probabilistic model on how
the labels are generated and then they
will estimate the parameters of the
label using maximum likelihood buy em
algorithms cheating the true hidden
variable as the true labels and hidden
variables and then they go back to
estimator the labels with the parameters
they just estimated by maximum
likelihood so there so the problem for
Yemm is that there are so many models
you can use from very simple model that
use only in polluted distribution and
confusion matrix in in 1790 and two very
complicate models that counts all sorts
of factors and use multi dimensional
representation
very recently so the real question is
how to choose these models so we have
simple model and complex model which one
we should use there's a trade-off here
and also giving a model how could you
influence at decoder the labels so Yemm
has this two-step approach where they
fester estimated parameters and then go
back to estimate the labels so is this a
optimal way to do it in the sense of
minimizing the bitwise error rate so if
it's not how much improvement we can get
by using another different efficient
algorithm even the model is the same and
also we can show that yeah and what also
was the connection between em algorithm
and the algorithm like majority voting
and the qos I should before so this
paper world tries this walkabout tries
to adjust these problems by with rigor
biting the crowdsourcing problem into an
inference problem of graphical models
and use infants techniques such as
pre-publication mean field so so some
background on graphical model here so
graphical model is a high-dimensional
distribution special high dimensional
distribution whose probability is a
product of many local factors each of
which which is a society here depends
only on a subset of variables so this
structure can be represented using a
factor graph you here where the cycles
are variables the squares are factors
and they are connected if this variable
are involved in this factor so this is a
very simple example and now give me a
graphical model we want to a use your
problem is to calculate the marginal
probability which which require of
single variables which requires to sum
over or marginalize over all the other
variables and this is a difficult
problem usually it'd be hard because you
need to sum over X
financial terms assume d here a binary
variables so there are many
approximation algorithm for doing this
including briefed obligation and mean
field so so bleep obligation are
proposed by user pearl and many other
people works by approximating the
marginal distribution pz with the
product of a set of functions m here
this function can be interpreted as
passive messages because they are
calculated iteratively by this to update
one of them is sort of you can
understand it as passing messages from
variables to factors another is passing
messages from factors two variables so
the detail of this algorithm is not of
important for this talk but just
remember they have this innovative
message passing style which is sort of
similar to the qsr algorithm and
introduced Lee earlier another another
set of hours are arming fuel algorithms
which also approximate the marginal
dispersions but it works by
approximating the Joint Distribution pz
the whole the oddities with a free
independent model so and you do this by
minimizing the ki raat KO divergence and
usually this problem can be solved using
coordinate descent and it's very
efficient usually so they are more
background in in my jordans book or
corners Merlin's book so now we need to
build a graphical model for our
crowdsourcing problem to do this we
start from the simplest thing that we
can do so we assume the workers the
labels of the workers are generated by a
very simple polluted distribution
basically lig is the label of worker I
gave to a image
is correct equal to the two labels e di
if whisper vqg otherwise it's wrong
because it's binary and now here qg is
the accuracy or worker g and i assume is
drawn from some prior distribution so
all the workers will have the same prior
distribution but they have different
john of qg so now are we can calculate
the posterior distribution to join the
posterior distribution of the two labels
E and reliability q which is
proportional to the product of the the
prior distribution times abolutely
likelihood term so so here here this DG
here is the total number of images
labeled by worker g and c g is the
number of correct images are labeled by
g among all these DG images yeah yeah
yeah so we assumed equal difficulty for
all it's the simplest model you can use
now this 3 g.r is the number of correct
images so it's actually a function of
the two labeled Zi and also the noisy
label ll g so now given this model you
can figure out what's the optimal
estimator for the two labels e you
actually should maximize the marginal
poster distribution of each di
individually this will be exactly
minimize the expected bitwise overweight
and if you do this it requires you to
calculate this module distribution in
which you have to marginalize over the
reliabilities and then sum over all the
other are two labels so this is a
difficult inference problem because it
requires integration
in summation over high dimensional space
this is the world we are going to use
influence algorithms and in question for
these slides observe papers yeah so
hyper parameter are hidden in the prior
of the qg and you somehow you can see
the qg other prior so you can also add
flyers on the on the DI but I didn't do
it just for simplicity right now it's
uniform so so now we can we can
calculate so now we can actually faster
integrate over the continuous variable
q.q other reliabilities so we can get
the marginal distribution over the two
labels on me so the two labels are
discrete variables is good for
prefabrication Elysium and this ink and
this ink integration can be actually
exactly calculated by pushing the
integration inside to the products so
now the integration are only one
dimensional integration and it's easily
to calculate either by numerical methods
or close the form solution so now we can
actually define this one dimensional
integration and some local factors IG
which is a function of CG but remember
CG is a function of the two labels see
GI that connects to worker G so overall
we can react to actually write this post
air distribution of Z as a product of
some fact local factors which has
exactly the death the graphical model
form so for what the prayer you in 20
you don't need to do so yeah you can you
can see at arbitrary prior so you can
still integrate over this thing yeah now
we can actually transform this sort of
bipartite assignment graph of
crowdsourcing to
standard a factor graph representation
where the variables are treated other
tasks or images and the factors are the
workers so the idea here is that the
workers each worker actually introduce
some sort of correlation on your
posterior distribution of the label of
the images that they labeled so if the
worker is very accurate is very good
then all the labels should be consistent
with his labels so this is sort of
correlation so now we can run a standard
pre-publication on just the posterior
distribution of PG so so we have this
local factor sigh here so what the ship
it looks like so it's an integration it
has this integration form which depends
on the definition of the prior qg so if
qg has a flat prior so the reliability
can be uniform between zero and one then
you have this symmetric and the convex
shape so work on both end when the
workers are either perfectly wrong or
perfectly correct it provides a lot of
information so it has a high value and
when the worker have like half of
correct it's very random and it's
basically provide that information so it
has no values and in question so here's
some other fire so in this case you have
larger probability to be Q larger than
point five so there are more experts so
in this case or correct comes more
information than all wrong so here's
another prior so we have half of the
spammers and the half experts here's
when the when the workers are
deterministically equal to some Q larger
than point five in this case the the
factory is actually a straight line
would you expect sort of when you didn't
show which is
are you being just this one without this
is it so actually you can guarantee for
arbitrary choice of qg this thing is
always always have like positive second
derivative second in the sense of finite
finite difference so you always have
this yeah yeah yeah yeah you can
actually show that it could be flat but
you yeah so spandy mentally it's so if
you look at the form of this shape
fundamentally something close to the
entropy so when you have deterministic
venules has two more o negative entropy
sorry yeah great when you have a camera
but but oh well will you get a little
expert so in that case you will be flat
off its that's not the red light yeah
yeah yeah yeah so so remember i have
this thing so basically the value of q
well well decide the slope of the land
so if it's the mena middle it's just
flat yeah so now you can just run the
standard prefabrication algorithm so
this is just form don't bother the form
but essentially we will decode that shoe
label CI by maximizing the marginals we
care from the algorithm click rates
you're doing new people
yeah it's approximation so you don't
know whether it'll convert uh actually
yeah I don't know so but in this pub
this problem is somehow easy because if
you have a lot of workers the
distribution actually concentrate around
the true value so that's not a problem
here but yes it's always a problem for
loopy publication so now what's
interesting is that because the labels
are binary labels so you can actually
transform this the binary distribution
into a log log odds ratio so and also
the same for the messages now both the
messages and the marginals are right as
real venues and then we can transform
the whole mess repassing algorithm using
this log odds for and we get something
very similar to QSR algorithm so
basically we have the same formula for
estimation labels again a weighted
version of majority voting linear
updates from task to the workers but
based different of a different message
update from workers to task so for kill
s this is adding an update but for our
algorithm we have this nonlinear sigimor
sigimor function whose form i will not
define it in this park but basically
it's some symmetric and monotonically
increasing function it's a sigmoid
function row statuary at large venues
and you can be calculated are very
efficiently wins odg log D G square so
DG is the is the number of images
labeled by waka G so it's only slightly
worse than kill as whose complexity is o
TG so the idea he is that using this the
sigmoid function is somehow more robust
it doesn't goes to infinity as the
linear update so this is the actually
important
accessing our of the stigma about it
it's not all this one oh yeah so it's
actually uh so yeah it's complicated
because Sigma Sigma is a function of
many variables but Sigma is a symmetric
function so so this is just a iterative
plot where you treat all the inputs are
equal yeah so it's yeah a special case
by in general you can prove its
symmetric monotonic increasing in enjoy
it has this trip why is little different
as the key go back page
right so actually I saturate but this
doesn't this doesn't say that the yep
and the marginals is just that the
precise the messages are saturated yeah
so somehow you because in this algorithm
you actually put the prior so because of
the prior they don't say yeah they
somehow the probability upon you from 0
now it's interesting to look at special
the algorithm when we take some special
price so if the workers have this prior
a deterministic prior equal q equal to
some large value larger than 1.5 then my
algorithm reduced to majority voting not
surprising and it's not good because
this prior doesn't count any diversity
nor their anniversaries in for the
workers okay yeah any basic so once a
larger the employing five if it's a
small employer five didn't you get some
sort of like oh yeah yeah function in
this case so um step function so
actually it's a uniform function so
every time the sigmoid do every time you
update you will set this one equal to
uniform then you go back this update you
will get a majority voting so now what's
interesting is that if you take some
prior called hot Empire in this case the
workers reliability equals to either 0
or 1 with half probability then in this
case my algorithm reduce to the qsr
algorithm you educate the sigmoid
function reduced to the street line and
this pry is actually very very special
it's a limit of the beta 0 0 so so in
objective statistics people many people
discuss this prior because it has very
nice properties that matches basic
statistics through frequent list for
example if you do a bayesian inference
over this prior you get you will exactly
get a maximum likelihood estimator so
however this prior is also not
reasonable in practice because it
actually counts too many adversary's in
practice you cannot image you have a
prior like this so basically you have
almost as many as adversary's as workers
and you don't have any thing between
them
this is a very extreme and because my
algorithm actually can pick up a cherry
prior so you can think about pick some
more reasonable priors like this so we
have a reasonable amount of adversary's
but not over dominate and the in
practice this works way better than
posso having a very blessed of it for
the birth yeah I try that I have to try
that so we can actually um so it depends
actually so but actually so yes in Joey
has this shape but because but sometimes
he has some pig so it's not smooth as I
tried because wheel data is not that
large so you cannot see the effect but
the point is that I try different wires
and I even try to learn the priors but
actually the priors the the choice of
pride actually doesn't influence the
actor is that a lot once you have some
general shape like this so i will talk
about this in the experiments yeah
so for the body class so basically this
is a painter prior for multi-class you
have to issue a prior then you have like
multinomial do your part I think similar
in the factors oh but its conjugate so
yeah oh yeah you don't have to then yeah
integral won't be high dimensional if
it's not conjugated bravely but it is
yes virtually okay yeah sure so now I
talked about how to use bleep obligation
to do the influence here we can use me
feel algorithm also and you actually
connects to the yam algorithm so what do
we do here is a slightly different so we
have this joined the posterior
distribution over Z the shoe label and
the reliability q now we just
approximate the whole distribution with
a special fully independent model a
model whose probability is just a
product of some local probability new I
over di and some new new new g / qg so
both mu and nu are probabilities and
then we minimize the KO divergence so we
can use according to descent algorithm
to solve this we update mule with fixed
to the new and update you know and
what's special is that this new function
distribution is a continuous
distribution so you cannot update
exactly but you can actually show that
at optimal point the new is always a
beta distribution so every time you just
need to update a keep trees of the
parameter of a beta distribution that's
fine and that's the exactly the optimal
solution so and then what you can do is
that you can actually approximate the
update of new with some one order
approximation so the reason I want to do
this approximation is because then I get
a much simple update so this is
approximate after approximation way out
we have update form you and the queue
queue now the queue distribution
actually you need you to keep cheese of
the beam value
of qg so alpha beta are the prior so
here I assume the reliability have a
prior of beta distribution alpha beta
other coefficients in the prior so now
what's interesting is that if you go
back and derive the a.m algorithm on the
same model you find that you get almost
the same update except that the update
4qg the Alpha is replaced by a 4-1 so
that's only one small difference but
actually this small difference actually
make a lot of sense because it is
essentially at one smoothness so you
will make the always a robust so in
general if you pick a uniform prior in
that case alpha and beta equal to 1 then
this update is likely to to exactly up
cuties likely to update 0 or 1 if that
happens you can check the yam algorithm
it will then just stop at this point and
will never update so in many cases using
a other one smoothness is actually good
thing to do so and the difference
between this alpha and alpha minus 1 is
just the difference between
marginalizing a beta distribution or
maximizing a beta distribution curative
Lee so our algorithm can have some
several different extensions because
it's sort of derived using a generative
model so you can think about different
models before I use this very simple
parolee model what the labels are just
generated from a single corn veloute
likelihood so q g is the accuracy here
but you can actually extend it to a more
complicated oh that essentially have
this confusion matrix structure so in
this case depending on different value
of di the probability of correctness is
different why is sensitivity or not is
specificity
and this model it is actually very
important in many practical data sets
because usually we have this position
requires your work if the label is plus
1 then decorous it could be very high or
very low and this model can capture this
i will show you experiments on that
later and also since we are able to
marginalize over the parameters we can
actually do model selection by just
comparing so if we have saved these two
models and we want to decide which model
fits the data better what we can do is
to calculate the marginal distribution
conditional model Warren model to and
compare which marginal marginal marginal
likelihood is larger and also we can
incorporate item features expert labels
all sorts of things you can think about
so now I'm going to present some
experiments starting from some
simulations examples and then to the
real data so this is a figure that issue
earlier so basically the assignment
graph is a random bipartite graph and
then the workers reliability are drawn
from this very simple or prior where you
have half experts half spammers then and
then I increase the number of workers /
images so as the number worker increase
the accuracy general decay so our Q s is
generally better than majority voting
but it actually performed badly is when
the workers are the number workers are
small so now if you actually use PP
algorithm with the uniform prior you get
a slight improvement remember kill s is
actually also PP algorithm but wins beta
0 0 prior so the only difference is just
given choice or prior including the
majority of OT now if you use a
p paid her to vampire or you get much
better so what this means is basically
saying because the troop I actually
don't have any other recipes so in this
case you can't let other bursaries it's
reasonable to get much better and here
is what I if I use the truth prior
because I know it in my PP hours so in
some sense this is a what you can do in
the best case impractical because you
never know the true this purchase and
and actually the pp 215 is very close to
the true distribution so before I said I
tried say different priors and I even
try to learn the prior but actually it
doesn't matter in this case because it's
already very close to the true
distribution so um okay
so also I have something else so now I
was around the e/m algorithm and
approximately feel algorithm so their
performance is slightly worse than the
BP but not very significantly so owning
a minor improvement so here is the same
data but now I change the number of
images / workers wins the fix the number
workers per image so you can think about
if you fix number workers and only
change the normal images for workers
what you can do is to get more accurate
estimation or workers reliability and
because majority voting doesn't use any
reliability information so it's always
flat and all the other algorithm
actually gets better when you have more
images basically you change the workers
more but majority kill essays was and
then the other I was an astronomer
clothes also close to the true
distribution without the task difficulty
yeah I remember this wrong Oh journal so
you are a model but minus the test yeah
yeah so I forgot that paper but this is
a one-point model without difficulty but
wins the prior so here's when I changed
the change the data priors so before I
have this model now I increased
gradually increase the percentage of
anniversaries so now i actually have
elderberries so as the anniversary
increase majority voting gets worse
because you know its majority voting but
all the other algorithm actually gets
better because as you have more other
Vasari's that reversibly actually carry
some carry some information now if you
can detect them you can flick the labels
and you can get a character label and
this is what this algorithm to but
what's interesting you can see the year
maoism which is this light back curve is
actually much worse than the approximate
mean flow approximation which is
essentially different from erm algorithm
by adding alpha one and it's actually
get worse with the that when when there
are a lot of number for anniversaries so
this is because this numerical stability
issue okay back up will ya oh I didn't
say I didn't separate service generally
generated in Europe yes oh yeah okay
yeah so here's a real data set so it's
an image of data set wins the set of
birds so we have two tablets of birds
here and we want to identify which is
which so it's a binary classification
problem 100 images and 40 39 workers so
here we are because originally they are
like it's a fully connected graph so we
subsample the number workers and what's
interesting is that
actually all this algorithm EMB POS I
was the majority vote in this case and
they are even worse than the algorithm
proposed in the same paper we're in de
at all 2010 that use a multi dimensional
representation model that model is sort
of complicated because has difficulty
all sorts of things but it's get much
better than majority voting so what's
the reason for this because for for this
sweet curves i'm actually using the one
coin model if i use the two coin model
but one the same apart inference
algorithm I immediately get as good as
burning the ode to claim 0 is the
confusion matrix model so depending on
yeah sorry symmetric yeah so this is the
natural of this data set because I
forgot which but while the bird is more
difficult to identify than the other one
so they also talked about this thing in
their paper so basically for this data
set the most important factor is this
two coins assumption other than that
anything like multi dimensional
representation or difficulty Sims
doesn't help at least according to this
result yeah I have I think it's around
the same it's just too many pipes i
dunno i'm sure
I have to see something that I'm
confused by trust me loving that you
don't learn as they as they label but if
they're not sure so you are saying which
part is unwritten with this or the task
you dispense it's apparently a thing for
a different ability to process for
Arlington parcels oh yeah sure sure so
if you're sure it's locked in Atlantic
and get it when I'm easy it must be the
other guy so you have to judge learn as
they charged but you're getting keep
that hurt but it's all they will see the
joint anything are they true thing are
they getting feedback on this data set
or it was the data set just bulk label
without feedback that it is um so what
do I mean by feedback so also you mean
the workers are like shops or random
without feedback to the to the since
where the workers given any error
correction information as they were
labeling yeah I don't know so it's just
the thinnest at that big gave me a fire
that's operating the Bayes error rate
let's assume people are then it's not
necessarily true that the the optimum
confusion matrix in fact is is symmetric
depending on the distributions so they
could be being the humans could be being
Bayes optimal and still have an
asymmetric confusion but presumably they
get feedback they have an incentive to
go towards optimal right that right or
even if they're trying to be I'm just
being generous so you can actually plot
what the empirical confusion matrix look
like in this data and it's actually a
symmetric so
so and then we tried another natural
language of datasets so in this case we
have a set of sentence and in which we
want to rank the temple orders of two
works so in this case John fell and Sam
pushed him pushes the fast so it's the
answer so and now in this data set
actually story is different because if
you try the 1.2 coin model it's most
likely the same you get the same
performance except qsn majority routines
much worse so yeah sorry not know
features just as Cygnus just the problem
see yourself no new features interesting
change the killer is people from the
Surrey their approach is the best yeah
at least was so yeah farm Siri its best
assume a lot of things and so they
assume there's the number workers and
number images has to be very large and
graph has to be generated randomly and
more importantly they assume the model
is from the one point model which is not
reasonable and even even the assumption
are all correct this still perform was
in case where you have small number
workers so they call this phenomenal
phase transition they actually discussed
this problem so yeah I think
fundamentally is because they use this
hot Empire they don't add any smoothness
into the algorithm that's the problem
to summarize I have this used this
graphical model methods to solve
crowdsourcing problem generalized Kos
majority voting and connects to em
algorithm with me few methods some
insights here so so fast all vote or
choose the prior is actually critical so
majority voting kill as PP they are
different they are just that the same
algorithm wins different wires and also
sometimes for EMR wisdom you can also
see improvement when using this
different players so another thing
choose the model is really critical so
one point model chacon models sometimes
can make really difference so the
fundamental question here is to how to
do model selection on this problem and
now infants problem sometimes matters
sometimes doesn't so BP is sometimes
slightly better than yam but sometimes
the same so I wouldn't say this is a
critical factor but it's something that
you can should be careful and finally
this related work of me minimax mean
principal algorithm is another approach
that also use a very simple simple
unifying principle by Danny jang-soo
mate and E so it's interesting to say
searching for space doesn't help
we don't have massive which which into
your whole approach yeah it's not yeah
we don't have to
even that doesn't have it's just the
same coin same two coins for everyone
yeah yeah oh yeah sorry I didn't kill
you really hard anything crooked run
around this whole world makes our job a
latrise to see this so I can't really
created this a new people so far too
detective Perez being a lot is a very
shitty not that I said they were skins
they were the skinny as a gamer a parish
he's a march force of maturity version
ok this is look at Vassar idea even days
ahead of Sarah you can see that which
everybody will come worse but in there
that aside judika Kony is a very strong
but given approaches it's very bad he's
much more than okay that's possible
actually so so if if the workers have
very close reliabilities then you can
actually show majority voting is the
best thing you can do so probably that's
the reason in their search the workers
how bad you can't front very different
oh well that's that's good advice strong
huh and the day autoworkers about it and
get him guess I don't know I guess so so
this is go ahead of Sarah uh-huh why coz
IM oven Frederick reason and the
stronger car were kind of warrnambool
wait ok Hannity the stronger builded
that the labels mmm I'm not sure if you
add some prior to smooth the the whole
or the workers whether you can get some
attract some generous if you is can you
to generate dennys mall back into your
mom uh I think yes um no I don't have
model I just have like I said well yes a
way to take his thing and chop out
item difficulty and chop out only have
two classes does it match up in some way
you act immunol framework if we have
your comrades 44 know what I'm look I'm
here the farmer eww our approach we
review is true yeah the humor approach
you do it is key yes right if we remove
the second of the secretary from Davis
is bad about dominance getting to
approach and our program is becomes told
my cooking right but so there's not way
to make it reduced it is they so so so
yeah sports a bit of a copy the new
cheeks it's a bit of every skin you to
pray put it proud proud
unpregnant trucks may see a certificate
which doesn't naturally happen in max in
it doesn't naturally come out of Max it
the Beijing up I think so in your
framework there Arthur you have this
regularization which is sort of like
prior and by its diff Empire it's like
putting some like Gaussian prior over
the exponential parameters instead of
beta penalize the furniture Asia
computing cloud from the implication
yeah I think yeah I think the difference
is is that so here I'm saying taking
different models you can do different
algorithm to decode the solution even
wins the same model assumption so yam is
one of them you can do something
different I think this is how these
axioms
very very nice weekly accident and that
about the least consistent isn't unique
is it falling for the accents you have
so the question is what happens when
when your system is axial to push it
down sir what do you mean by action well
they're not all objectively isiliye
create another particle cutie sex okay
yes so just kill them in the hole the
times even Isaac doesn't depend on the
person I can honor models yeah yeah sure
Chuck times and so that thing all from
this or this they were consistent was
the question is if you have sent
requests what's your and is it also
secondary status reckon so so they have
this faster principle and then basically
it's used to construct a model right so
the in the minimax formula you have this
you have to choose the sufficient
statistic to to match and this effect
lee lee is choosing some exponential
family model on so it's I think it's
basically selecting the model but wins a
more intuitive way yeah just said that
his work is just using davids candies a
confusing racial tomorrow
part of your pretty proud on the comfort
of my cheeks you put a banner proud get
admissions and then an Olympic in a full
of busy you friends using video
propagation all new fail area before
before a child of work or out of work
just-just-just either poland estimation
is again that's like she's a difference
and given her little data you have it
make sense to integrate you know I think
consider as a price of quiet Union for
years a better prior to one that is
assumed that the most kind of good guys
as a matter that's constant in the visit
it was a real date huh that's why I
chikuwa is impersonal waterful very good
okay yeah yeah sure yeah so basically I
tried different priors once they have
this like decrease in shape then it's
generally ok does it matter to use beta
2 131 or 10 10</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>