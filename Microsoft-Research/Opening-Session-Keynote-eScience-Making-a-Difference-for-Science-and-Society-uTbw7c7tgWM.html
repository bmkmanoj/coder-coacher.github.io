<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Opening Session &amp; Keynote - eScience, Making a Difference for Science and Society | Coder Coacher - Coaching Coders</title><meta content="Opening Session &amp; Keynote - eScience, Making a Difference for Science and Society - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Opening Session &amp; Keynote - eScience, Making a Difference for Science and Society</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uTbw7c7tgWM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
it is my pleasure and an honor to
introduce Tony hey um who was a
professor at Southampton in parallel
computing and from then on was the
starter and the founder of e science and
the UK and then he went to Microsoft
where has been for the past 80 years
conducting and fostering
multidisciplinary research and one thing
that I just told tony is that whenever
anyone asks me what I science is about
and to give a good definition I always
use his definition which I hope he will
tell us about in his doc thank you Tony
and after your your talk i will start
giving the word to the public to ask
questions okay thank you very good thank
you
yes it's a great honor to be back here
and thank you very much for those kind
words so today I'm going to say a little
bit about what he Sciences I have a lot
of connections with Brazil when I was a
professor at Southampton my research
group was actually at one point known as
little Brazil because I had large
numbers of students from USP in Sao
Paulo we were very good at football and
they also cooked me a feijoada I
remember and because I was the boss they
gave me the pig's ear which was very
flattering and the other connection I
have I supporters soccer team a football
team called Southampton and our gift to
the world actually from Southampton is
Brazilian soccer because you all know
charles miller who actually founded
brazilian soccer came from southampton
he was trained at southampton so but let
me get to the topic of the meeting so I
this is the sort of outline of the talk
so I'll give some background to why I
believe e sciences is so important so
relevant and how science is changing
I'll give some examples of e science for
environment in genomics and in health
care which shows that it these will
solve the problems we have the potential
to solve the problems of the world to
actually help economies and and really
create innovation and then specifically
look at some of the projects we're doing
in Latin America some words about open
science and then some conclusions so
these are just some of the examples of
recent publications which stress the
importance of a big data and big data
comes from all sorts of sources from
satellites from from high-throughput
genomic machines from sensor networks on
the ocean floor from accelerators like
they have in Geneva at CERN so huge
amounts of data and in all parts of
science it's not always going up to
petabytes but it's it's certainly going
to an order of magnitude more data than
scientists have had
deal with and so this is in science and
the similar phenomenon is happening in
industry and so if you look in a popular
search engine called Bing maybe not so
popular in Brazil but if I look up data
scientist in bing this is where it gets
it says data scientists the hottest job
you haven't heard of what is a data
scientists data scientists is the role
of the future so data science for both
scientific investigations and for
industry is now an important discipline
and training people to be data
scientists yes we train them on
scientific problems but they can apply
their skills in industry and so just to
pick up one thing this is from the
wikipedia article what is data science
and you see it involves lots of
different sets of skills and and I would
probably include other skills they're
like data curation data management a
little more explicitly that may have
there but but the data science involves
a whole collection of different types of
skills so when I came to Microsoft I
first met Jim Gray from Microsoft in in
in 2001 when i started doing eat arms
and we continue to debate until i found
it easier for me to join microsoft to
continue the debate and Jim had a
slightly different way of phrasing it
but it's the same message and so this is
Jim's way of explaining the changes that
are coming about in science he pointed
out that the way we've done science
historically we've had experimental data
you know measuring the planets and so on
we've had people like Isaac Newton
Maxwell Schrodinger summarizing these
experiments in in laws of nature which
are mathematical and so the ways we've
actually explored nature have been with
both experiment and with theoretical
analysis of these equations which you
could then make experiment predictions
which you could then test and those are
the two if you like the two basic
paradigms for doing science
what Jim pointed out was first of all
about in 1950 onwards we have computers
and computers enable us to explore
different aspects of science that are
inaccessible in any other way so for
example if you want to know about galaxy
formation if you want to know about
climate change the only way you can do
quote experiments is by simulation and
so simulation you need to change
students to know about numerical methods
about algorithms about parallel
computers about languages programming
languages and so on different set of
skills than theory and experiment so Ken
Wilson Nobel Prize winner in the 1970s
call that the third paradigm Jim
introduced the idea now that we need
another set of skills so it doesn't
replace the previous three we need all
of those skills but we need another set
of skills we need students and engineers
and scientists who understand how to
deal with data data intensive science is
going to be with us for a long time and
it's it's as we talked about with the
instruments with the sensor networks
with satellite surveys with enormous
accelerators and so on neutron sources
and so on would generate huge amounts
more data than we've ever had before and
just dealing with that data we need a
set of tools and technologies if we're
doing a pharmaceutical company they have
their biologists in one building their
genomic asst in another building maybe
in the different country they need to
collaborate you have distributed data
you need to manage you need to
manipulate so the set of tools and
technologies to support data Federation
and collaboration is what I call e
science and it's to enable you to do
science in ways you couldn't do before
better different and faster is how I
usually discover usually describe it and
and the fact that you hone your your
tools and technologies from computer
science on to those problems means that
you improve your computer science as
well so
data analysis data visualization data
mining and eventually disseminating and
preserving some of these things so these
are the the four paradigms Andy science
is the sort of set of technologies
applying computer science to scientific
problems this is another slide from Jim
Gray who alas is no longer with us
pointing out that we've been known for
some time that in chemistry there's the
traditional chemistry but there's also
computational chemistry there's
traditional biology and now
computational biology but there's now
also bioinformatics camo informatics
there's an informatics aspect too many
many fields I've seen even archaeology
and astronomy are talking about the
informatics dimension and there you have
the problems of managing lots of data
how you organize it how you share it how
you visualize it how you interrogate it
these are problems that we need to
understand and and and train students
and faculty in researches into doing
this and these skills are obviously of
relevant to industry so changing nature
of research the models are very
complicated multidisciplinary T is
fundamental large time scales large
spatial scales data sets can be real
time so that the US are doing an ocean
Observatory initiative they're putting
fiber-optic cable down on the floor of
the ocean near Seattle and that will
stream in data 24 hours a day 365 days a
year and that changes the way we do
science because instead of just getting
a small amount of data we now have a
flood of data how do we find significant
signals in that data and it may not just
be nicely structured it can be all sorts
of unstructured data like the web and so
on how we actually mine that data and so
distributed organizations virtual
organizations if you like you i want to
share data from here with a researcher
over there but not allow everybody to
see it when
need to do it securely and share data
with whom we want to and management and
social challenges come about because
instead of a professor publishing a
paper with just two of his students now
it may be a paper with ten professors
and 20 students and that's a change a
cultural change so I really think this
is a unique time it's an opportunity for
computer scientists to work with
scientists to solve some of the grand
challenges facing humanity and in so
doing when doing a science in the UK I
find initially there was a very great
misunderstanding in that the scientists
didn't know what computer science was
about they thought computer scientists
just wrote programs and computer
scientists didn't understand the
complexity of the scientists problem and
you needed to work together to talk to
each other for you know six months or so
before you understood each other's
problems what what one could bring to
the problem so it really is an important
and long-term investment to get
scientists and computer scientists
working together but I do believe that
that's the way we can solve some of the
problems and so I'll give some examples
of that so there's a couple of books
which you can download they're published
under a Creative Commons license one is
some essays on the fourth paradigm where
my role my co-editors christine
tolerance do at ansley did all the work
my role was merely to bully the authors
computer scientists and the scientist
not to write a dry technical scientific
paper but an essay which could be read
by a large audience so we asked some
scientists and computer scientists to
get together and to visualize what their
field would look like five or 10 years
from now and that's what this this essay
contains and you can get a copy on the
web and similarly we have in science at
Microsoft it's it's examples of e
science examples of computer scientists
were
with scientists doing innovative
exciting things and so if you want to
find out more about those i invite you
to download them we have some copies
here and I have some copies to give to
my colleagues so this is the the fourth
paradigm essays and this is the science
it Microsoft some examples from genomics
using machine learning the environment
and health and computer vision examples
of e science so let me just give some
example so you'll see exactly the scope
and breadth of the sort of things you
can do let's look at the environment
genomics and health care so in fact I
won't say very much about computational
ecology and environmental science it's
an exciting area and particularly
relevant in Brazil because you have the
rainforest in the Amazon and a huge
range of different regimes and fauna
here that are vital for the world what
is the impact of the decisions the
policies that were makes we really need
data to inform those decisions and we
also don't know enough about the global
carbon cycle and and that's the sort of
topic that drew pervez in his keynote
after this lecture will be talking about
so this really is an advertisement for
you to go and listen to Drew who will
tell you in more detail the sort of
things that they've been doing the tools
they've been producing which we hope
will be easily usable by scientists
around the world without having to know
all the details about relational
databases and and so on so I recommend
that one goes and listens to drew
genomics and personalized medicine so
what the promise of genomics we all know
the human genome has now been sequenced
and we hope we will actually learn from
that to understand the causes of
diseases to help you diagnose the
disease to find out who's likely to get
it to find out whether this drug or this
drug is suitable for your genetic makeup
or whether you'll get a bad prediction a
bad reaction to that drug to
personalized medicine will become a
reality it hasn't become a reality yet
but this is using machine learning in
genomics is a key area and I'll just
give one example here so we sequence the
human genome what we're now trying to do
is genome-wide Association studies where
you take a whole population and you look
at all the the different defects and
varieties the snips that you have and so
this is these people are healthy and
they have this particular genomic
sequence these people actually unhealthy
and they they differ in this particular
area here but but this person has the
same defect but is healthy and this
person has no defect and is unhealthy so
you have actually two that there are
subtleties in actually understanding the
genetic causes of disease and you need
to have a large population now the
difficulty for doing that is that you
have typically a control population and
then you want to to look at your
patients and to analyze them but there
may be different genetic makeups in the
control population which may be
different in different places and they
can confuse the analysis there may be
false correlations these people may be
related they may come all from the same
village or whatever and have the same
genetic correlations so you need to take
out spurious correlations and to do that
you need to do a very sophisticated
machine learning algorithms and that's
what David heck erman and his machine
learning group in in Microsoft Research
have been working with researchers to do
so they've developed an algorithm called
fast lmn standing for fat factored
spectrally transformed linear mixed
models doesn't matter what it is but but
the important thing is to show the power
of computer science that before this the
run time increased with the cube of the
number of patients
that you were trying to look at the
number of people you're doing the study
and because it was very expensive in
computation it actually meant you could
only do small samples of a few thousand
by actually doing a new algorithm which
grows linearly in the number of of
people being analyzed you can actually
go up to hundreds of thousands of people
and that means you have a much bigger
signal and you can get much more
accurate results so changing the
algorithm can have a huge impact and
enable you to do analyses that you would
not be able to do so this is they put
their algorithm to the test and this is
a the code name in Microsoft was called
Moondog it was using cloud computing and
the Wellcome Trust data which had
genomic data for for a thousand people
and covered seven common diseases and so
what they did was look at pairs of
correlations of defects these snip pairs
and there's about 60 billion of them and
to do that on one processor would take
you about a thousand years and generate
20 terabytes of data so instead what
they did was choose use 27,000 computers
up in the cloud and analyze that took
just 13 days so they were able by using
cloud computing its computing resources
far beyond what a research group in
medicine could do they can't have a
27,000 machine at their in their in
their laboratory but they can use these
cause up in the cloud and from that one
of the latest results is that this pair
of defects this particular one here is
linked to coronary heart disease and
that's that's just been published in
scientific reports so it's an example of
the sort of things you can do with new
algorithms and understanding new
insights about disease so that's an
example in genomics this is another
example which is relevant certainly to
Brazil which I learnt as one of the the
largest
national health systems so this is a
health care challenge this is a
quotation about the u.s. so people who
go into intensive care into hospitals it
turns out when people come out of
intensive care one in five of these
patients is call back into intensive
care and that's a very expensive thing
it uses that the facilities it actually
it it would seem to be unnecessary and
so if you could reduce the amount of
readmissions it would save in this case
it says for over the whole of the US 17
billion dollars a year so if you could
reduce the amount of readmissions when
you've had somebody in intensive care
you let them go and if you could predict
you know what the probability of someone
coming back was and then give extra
extra support for people who are more
likely that would save money and that
was the challenge that our machine
learning group under Eric Horvitz in
Microsoft Research did they looked at
hospital data they had access to
hospital data which which covered an
eight year period and a whole variety of
things not just the reason that the
patient went into the intensive care
unit but all sorts of other activities
that the hospital had records about that
patients so taking all that information
into account 25,000 variables they were
able to make predictions as to the
likelihood of that patient being
readmitted into intensive care and so
this is the classifier they produced
these are false positives these are
genuine ones and what you have to do is
to reduce the false positives if you say
everybody is likely to come back that
doesn't help at all you have to get a
prediction of ones that are genuinely
more likely to come back and to check
that and so that's what they produced
and they checked it against real data
and this is the real data which shows
very similar to what I showed that they
can the various different test subjects
they get a predictor that they can
predict with some likelihood
that this patient will be readmitted and
if that's the case then you can give
extra follow-up you can give extra
medical support in the home to prevent
them coming back and that has now been
translated into a product it's now part
of what used to be Microsoft amalga but
that's our hospital system information
system which is now a separate company
called carradyne but this this
readmissions manager is actually a
component in that and there is some real
data so it's now live you can look at
the people that they admitted they
discharge the name and so on what the
problem was you have all this data and
you can actually go and check out the
experiments so this is initial pilant
this is one month later and what you see
is that the total readmission rate has
reduced by three percent in just a one
month after deploying the system and
that's for this particular Hospital over
a million dollars of savings so that's
the sort of power of machine learning in
healthcare it really can make a
difference if you include all these
things so that's why it's relevant to to
almost every country and of course by
trying on machine learning on this
particular problem we get better
algorithms which we can try on some of
the ones that Microsoft's products care
about there are other opportunities for
machine learning in healthcare it's now
sadly hospital you go into hospital it's
a very dangerous place you can get an
infection and you can even die from
these things so this is particular one c
difficile this is again trying to
predict the likelihood of infection in
hospitals can you do something about it
so this is one of the problems that
they're now working on so these are are
some of the challenges that machine
learning is taking in healthcare there
is another grand challenge which you see
around you in South Paulo and that's an
urban city problem Sao Paulo has over 10
million people and
or if you include the surrounding towns
and all sorts of data you get traffic
flow energy consumption environment
economic and so on and a huge amounts of
data how can you optimize these and how
could you build a new city which
actually did this better so these are
the sort of challenges that we see not
only in Latin America but also in China
India and in other parts of the world
urban city planning urban analysis of
optimization of resources and so this is
a problem that we're now beginning to
look at both in China and in the utin
the USA how one can actually optimize
cities the IT systems of cities how can
you make use of all the information to
make sure your systems are robust
dependable and efficient so I was then
going to give a few examples of the
projects that we've been doing in Latin
America itself so the first one I want
to talk about is is live and ease and
there's a video which I hope will show
later about preserving a medical as in
America's wildlife and this is concerned
with rare species so working with the
lack seer virtual Institute which which
is based in Chile getting tools to find
out the distribution of these endangered
species but they're in danger than
they're rare and therefore they're not
very often seen so this is if you like a
crowdsourcing technology so if you are a
tourist there and you see a rare animal
or a rare bird you make a photograph and
of course it knows exactly where it is
location and you can submit it and you
can help increase the datasets and get
more data on these rare species so this
is preserving wildlife one picture at a
time so live Andes is a project that is
we hope now to extend to a broader area
but it's a very exciting project using
crowdsourcing to help give
data on and endangered species this is a
project we have in Brazil it's about
sugar cane and you all know it came to
me as a surprise when I first came to
Brazil that the cars run on ethanol
mainly although you now have gasoline
but in ethanol is still a major
component for the Brazilian economy
provides eighteen percent of its energy
so the question is can you make variants
of sugarcane that can grow for example
in dryer hotter conditions this climate
change that you can you go in different
parts of the country than we we do at
the moment can you make it more
efficient to produce ethanol and so on
so to do that you need to understand the
genetic makeup of sugarcane and it turns
out that that's a very difficult problem
because we all have a chromosome from
our parents and we have two chromosomes
two of each chromosome in our body so
wed diploid sugarcane can have as many
as five to sixteen copies of these
chromosomes in there and therefore
sequencing and building and that up is
very complicated because you don't know
which chromosome copy it came from and
because it says very high hetero
zygosity what it means that each of
these copies of the chromosomes have
slightly different variants slightly
different defects and they matter and
that's why the the properties of
sugarcane are what they are and so
assembling that is a very difficult
challenge so nobody's actually done the
genome assembly for sugarcane yet and so
we're working with Klaus Sousa from USP
and other researchers in Brazil to
sequence the DNA of a particular
representative strain of sugarcane and
the goal is to as I said to get better
drought tolerance better disease
resistance and a more efficient ethanol
production so where are we in that
project well as of this month
we've been analyzing the data from
Blauser at USP and we have various
versions of the data a version which is
if you like only one chromosome the mana
ploy but what we need to do is the
polyploid which has large numbers of
base pairs and what we're trying to do
is assemble these to understand the
whole of the sequence and what we've
managed to do at the moment is is do
okay it's probably as good as most of
the plant genomes that have been
sequence but it isn't really good enough
yet and therefore what we need to do
it's because it's really like a jigsaw
puzzle you don't know whether this bit
comes from this this copy of the
chromosome or this bit and you really
have to to work extremely hard to put it
all together so they're trying on that a
new hierarchical approached to get much
longer fragments of genomes which you
can then assembly and we hope to have
some really exciting results before the
end of the year so not yet solved but we
think that's an exciting project and
we'll see what will what will come of it
in the next few months and again it's
using techniques for putting things
together from using computer science
techniques to make these more efficient
this is another product which is
important to Brazil this is actually in
Colombia light in the coffee growing
area of columbia as opposed to the drug
cartel area in colombia and this is a
center for computational bio informatics
up in the highlands in colombia and
we're working with them and a number of
other uses of supporters and what
they're trying to do is is to develop a
whole set of resources for biological
data for health systems and they also
have computing and scientific computing
so putting all this together they hope
to be able to solve some of the problems
to make sure they understand how best to
optimize and look after the coffee
regions but also other areas and so they
can offer consulting and and training
for four people there so this is the new
center in Colombia which we've been
collaborating with a couple of products
to finish up with these are projects
that we've been doing with our lack seer
collaboration this is a national
multinational latin collaboration
centered in in Chile but it involves the
searches from different countries and
this is looking at at frogs monitoring
frogs to see as an early indicator for
ecological stress so if you want to know
if the if the climate change is having a
bad effect or if logging or whatever
actually finding a frog population and
monitoring the effect of that frog
population is one way of doing it and so
this is a sort of schematic of the
approach that they're taking this is the
distribution of amphibians and what
they're trying to do is get a
representative frog species to actually
give them some indication of the health
of that ecosystem and this is using GPU
chips gpgpu chips to accelerate image
processing visualization for a number of
applications so if again if you can make
use of these chips it's a very effective
and cheap way of doing computation and
so there's weather forecast
visualization trying to recognize
textures in motion for water fire smoke
and so on looking at high resolution
tomographic images looking at fracture
asst for osteoporosis that's one of the
applications and lastly looking at noise
reduction algorithms for medical images
to try and get better medical images
in order to to be able to observe
features in that and general image
recognition you see here on motorways so
GPUs provide a very effective way of
actually doing better image analysis and
that this project is looking at that and
it's done with alexandra dl from uva so
i should say something about the agenda
that is now you see in in the USA you
see in the UK and in europe and many
places around the world the belief that
you need open data and open science to
in order to help us solve the problems
of the world most efficiently so the
data life cycle so just acquiring the
data is only the beginning so you can do
it from the supercomputer simulation
from a sensor network you get all this
data what are you going to do with it we
may need to share which may need to
collaborate it may some of it be held
here in são Paulo other parts may be
held in New York and hadn't putting this
together combining them these are
collaborating and then you visualize
that data you need sets of tools to do
this and then of course you need to be
able to analyze and data mined to go
from data to information to knowledge
turning data into insight and finally
you need to publish that you need to put
it out there so people can get it people
can see your your your paper so open
access to publications is I think a key
part of this but also you need to give
people access to the data on which you
came to your conclusions so
dissemination both of the paper and of
the data and how you share this and then
you spend a lot of money to get some of
this data we can't keep all the data
everybody creates but you need to decide
which of the data you want to keep which
you want to preserve and it's it's not a
trivial way of doing it if you have for
example many scientists store data in
spreadsheets well I can tell you in
in 10 years time the version of the
spreadsheet program that they used won't
exist the version of the operating
system probably won't exist and the
hardware so how do you actually preserve
data in a spreadsheet that you can still
make sensible use of in 10 years time
these are challenges for computer
science so just some examples this is an
example we did with the Astronomy
community it enables you to look at
whole lots of data you can look at the
night sky as you've never seen it you
can look at the visible but you can also
compare that with the ultraviolet and
the gamma-ray or the infrared and the
x-ray and radio waves so you can look at
all these different spectrum and and it
there's a feature of worldwide telescope
which i think is going to be important
in this data rich world that we're
coming and it's the ability to tell
stories with data so what the worldwide
telescope tool tells you is that you can
actually have an expert astronomer talk
for example about star formation and
they can show that this data set in the
ultraviolet and this data set in the
visible looking at this galaxy and that
galaxy and they can take you on a tour
through all these data sets and come to
your conclusion so you can understand
how they reach their conclusion and that
capability for these tours through the
data is important because other people
may come to different conclusions and
you need to allow them to make their
tours through data so you can imagine
for example with climate change it's
quite controversial so scientists can
show how they came to their conclusion
by showing how this data set and this
data set and this data set come together
but other people can take different
routes and come to different conclusions
so world wide telescope the ability to
do tours is extremely important and
yesterday Rob gave
a session tutorial on lay escape which
instead of looking at the at the sky you
turn the telescope onto the earth and so
this is if you like this is plotting
seismic events and mr. Wagner had a lot
of trouble convincing scientists that
tectonic plates was relevant but
actually if you just plot on the globe
the seismic events you can see the
outline of the tectonic plates extremely
well this is the ring of fire around the
Pacific here and this is the
mid-atlantic spreading of the of the
ocean plates so you can see these
instantly you can actually go and look
do models this is the Japanese tohoku
earthquake and this is somebody's model
of of the playtest is being subduction
zone going underneath the Pacific plate
and you can actually go and look at
these models and and do a tour of these
models so these sort of ways of handling
huge amounts of data you will need
sophisticated visualization tools I'd be
talking about science but it's also true
increasingly in the arts humanities and
Social Sciences and so this is a project
we've been doing with Berkeley and with
Moscow State University it's an open
source project school chrono zoom and
here you see you're putting you can go
from seventeen point eight billion years
or you can look at Earth's history or
human prehistory of human history you
can compare all sorts of different types
of events on the same plot you can zoom
in zoom out and this is a sort of
visualization tool that will be suitable
for certain types of applications so
this is one that's being used at the
moment by the so-called big history
community which is looking at how
biological evolution how civilizations
how geological evolution how they play
together and so this is one example of a
visualization tool when I came to
Microsoft in 2005 Bill Gates gave a talk
at the supercomputing conference this
was a slide that he
and it's still relevant today it shows
that really we're approaching a
different type of publication because
just having the paper publication is
fine and still useful there's no doubt
but actually more so if you can have
interactive data you can go from the
publication to the data you can go from
the simulation the authors plotted this
but if you can go and see the computer
program that produced it you could
change the parameters and so on you make
the data available to other people so
it's reproducible you can have different
ways of collaboration the documents are
dynamic and so these types of
publications and new types of of
publishing are going to be extremely
important and we're all familiar with
the the academic review process but
there could be other ways of doing it
there are different metrics that you can
put on and you see organizations like
the public library of science instead of
having a citation impact for the journal
they give you now what you more
interested in how many people have cited
that particular article in a journal and
so they're different ways of measuring
impact and other people take into
account blogs and Twitter feeds and and
all this sort of stuff so there's a
revolution going on in scholarly
communication this is an example of of
that revolution and again it involves
all sorts of different technologies and
computer science and scientists need to
collaborate to make that happen I talked
about curating data in a spreadsheet
well this is a tool that's an open
source tool that you can either use as a
service from the cloud or you can
actually plug into Excel and it enables
you to to curate to add information to
add metadata to your spreadsheet so you
can other people will know what the data
represents what the calculations in each
box and collin represent so it's an
example of a tool that you need to deal
the data life cycle so I've only talked
about the visualization and this is a
little bit about curation but I've also
talked about the machine learning data
mining aspects and so there's a whole
lot of tools that you need to have to
look after data throughout the data life
cycle in 2003 many funding agencies got
together in Berlin and this was the
Berlin declaration to promote the
internet as a functional instrument for
global scientific knowledge base and for
human reflection and they felt that open
access was extremely important and it
was not just the publication of your
paper but also original scientific
research results raw data and metadata
source materials digital representations
of pictorial and graphical materials and
also multimedia material so all the
stuff that goes around your paper at
least some of that needs to be made
available to other people and in order
that it can be used by other people you
have to put some metadata with it so
this is an education process for
scientists because previously we have
not done this as a community very well
people who do the work of looking after
data it's it's a form of publication in
biology it's increasingly a form of
publication of datasets people need to
be able to get credit for publishing
data and you need to be able to cite the
people who made the data set and so
their organizations like data site
orchid is is to distinguish who the
researchers are and i work in korea and
many the searches in career up called
Lee or Kim and you need to know which
Lee in which Kim is the one and so
having a unique identifier that's what
orchids about data site is about citing
publications and then in order to make
data different data sets usable there
of different ways of doing that you can
make very complicated ways of
interoperability this is a very simple
protocol called the open data protocol
which is based on HTTP and Adam and it's
now in the Oasis standardizations buddy
supported by people like IBM SI p and
Microsoft and others and so this is at
least a step towards data
interoperability that I can use this
data set and combine it with this data
set to produce new knowledge and so it's
one of the sort of things in this open
data open science agenda that that are
beginning to change the way we do things
and this is the last slide from Jim Gray
but this is the vision that I have and
that I think we should work towards that
in the future you will have all
literature available online because open
access is coming and will will be
available so I'll be able to see your
paper and I think go and from your paper
I can find the data you used I can
combine it with my data make a new
discovery publish it and so on and so
having worked with researchers around
the world what I see is that we're not
very efficient as a global research
community at doing our research we
discover the same things many times over
if we could make the results and the
data easily available so everybody could
see it you could increase the the
velocity the scientific productivity you
could solve some of the problems in
healthcare disease environment that are
facing the world you could really make a
big step forward for science and society
so a vision of if you like a global
scientific library which consists of the
literature and the data that you can
combine and use is a great vision and
this was a vision from Jim Gray so
lastly let me sort of sum up with some
of the things that I think will be in
your future machine learning I'll talk a
little bit about visualization
technologies
and then end with some words about cloud
services because i think these three
technologies are going to be there in
the future so machine learning I've
given lots of examples of machine
learning so I'm not going to go into
detail but just to say that you know
we've known for a long time computers
are good for storing and computing and
and looking after huge amounts of data
but the number of papers being published
her minute nowadays are overwhelming the
ability of scientists to keep up we need
help from our computers and so we like
computers to help with the automatic
acquisition aggregation correlation
interpretation discovery organization
analysis and inference so they know what
we're looking for and they can infer
information for us and we can then go
and use that information so machine
learning technologies semantic
technologies of all types will enable
this to take place so machine learning
of all types will be here in the future
understanding medical images this is an
interesting problem for computer science
that a human being can immediately
distinguish the rib cage from the
arteries in a medical image can you
teach a computer to be able to
distinguish between a heart and the
liver and a kidney we can do it can we
teach a computer and so that's what this
project led by antoni Oh criminy Z and
his researchers in Microsoft Research
Cambridge are looking at and so they're
looking at all sorts of exciting things
including an analysis of brain tumors a
particularly unpleasant brain tumor
which is called geo blaster blastoma
which is it is very aggressive and
usually fatal but can you assist the
surgeon in doing operations and so using
the Kinect sensor you can actually as
the surgeons operating they can see
exactly where where the tumor is so you
can actually go in on a mock-up of the
patient's head you can actually go in
and look at the brain
look at X to the tumor and actually
isolate where you should be doing your
operation so these are augmented reality
applications that you can actually use
with your vision technologies and these
type of technologies I think will become
of increasing importance and then let's
we heard yesterday that there's a new
Institute for natural disasters in in
Brazil so this is an example that we did
in Europe about fires and some years ago
on Greek islands which are relatively
small there were these disastrous it got
very hot and very dry and then there
were these high winds and fires could
really have burnt up the whole island so
there were very great dangers for the
Greek Islanders in those days and so
this is an example of how you might
prevent that so this is trying to take
all the possible data you can about the
island and about the Forrest's the
vegetation and so on so you have a
weather model you have real-time
information you know about the winds you
know about the forecast you know about
the vegetation the topography whether
it's a mountain or valley satellite
images road networks the history of
fires which is an urban area what's an
aggregation Illaria and so on so you
have all these different data sources
and you wish to combine them into models
so this is a model that's now deployed
in the Greek island of lesbos and it
gives you a visualization of the fire
risk so this is based on the five day
weather forecast and it tells you what
the the fire risk is in a color-coded
way here and so that's using the weather
forecast you can look at it based on the
current weather if the winds are high
what is the most likely areas where you
need to have your your farm and deployed
just in case this is if there is a fire
what does the wind which way is it
likely to grow fire simulation and
mapping and so these are some of the
things that you can do using if you like
the data fusion of all these different
data
putting visualization putting analysis
and machine learning together to build a
model which is useful for disaster
recovery and prevention and so again
this uses the power of cloud computing
to bring computing power to bear on this
problem when you need it and this is
Microsoft's cloud it's called Azure and
that's what we've been using in many of
these experiments and it can be used for
various services it has there's some
faculty resources you can go and look at
and as of this year besides supporting
windows vienna's virtual machines it
also supports Linux and it also supports
an open-source implementation of the
Hadoop non-relational database MapReduce
algorithm and so it's now a real useful
resource that you can use for applying
to a whole variety of problems as I
tried to show from genomics to the
environment so a concluding slide I
think machine learning is a skill that
computer scientists need to work with
scientists to educate them you need
scientists who know which computing
program is useful and which which
particular algorithm for machine
learning is appropriate for their data
and collaboration with the computer
scientists is a useful way but you can
also train people people who are
environmental scientists need to be
trained and understand some of the the
technologies and computer science
visualization technologies I showed you
some examples of that the zoom
technologies how you tell stories feel
like interactive storytelling with data
I think will become increasingly
important and I also believe that in
order to be able to deal with
unpredictable and amounts of computation
cost-effective computation both for
research and for business
okay it's a clearly a message from bill
gates how do i get with outlets in it
okay thank you very much okay thank you
so uh hello oh we have time for a couple
of questions well as moderator sex part
of the moderators task to ask the full
questions and um I'll do I'll start and
then people will ask other questions
I'll follow the technique that I fall
with some of my exams that I asked a
couple of questions and the student
chooses which one he or she wants to
answer so I alas get two questions and
you choose the one you prefer it to
answer first one is arm it you talked
about lots of exciting directions to
advance science through collaboration of
computer scientists and scientists of
other domains and it made it sound very
smooth and very easy and things appeared
out of nowhere I never said it was easy
no but you didn't but but that's the way
okay perceived it so what kind of
barriers are talk to talk about a few
barriers and the other question would be
is there anything that you'd recommend
scientists from other domains to learn
from just to study in computer science
so they can explain to computer
scientists what their needs are so that
we can understand them faster and better
okay okay so the foot the first one no
but let me try the first one that
there's a project where a researcher has
a tower where they they measure water
vapor carbon dioxide temperature and so
on and they measured every six hours and
they put the data into a database not at
its flat files it's stored so this is a
project it's called the ameri flux
project and there's these towers in
fields all across America about 140 of
them and they're actually a bigger one
which is actually around the globe but
let's just talk about the American ones
and obviously if you want to find out so
every six hours you get a small amount
of data saved in the file and if you
want to get em a week average to this
particular tower in this particular
field it's very easy the professor tells
his student to go and get that and the
student spends many hours getting all
the data together adding it up and and
doing stupid things that they shouldn't
have to do and so what we've actually
done is take that data and put it into a
database hiding the fact that it's a
relational database and sequel and all
this sort of stuff and putting a nice
interface that even the professor can go
and actually allocate and find out
exactly what the the average for the
week is for this field and of course
when you publish now with all this data
available instead of just publishing
from one tower in one particular place
it's more useful to do it from a region
and but that involves a cultural change
instead of the professor and is to
graduate students publishing a paper
with three authors if you take a region
with ten towers then you have ten
professors and twenty students it's a 30
author paper and that's a cultural
change in the publication and credit for
that field and this is a sociological
problem and so these things take time
physicists now published with hundreds
of scientists on it but again that's
been a change over the last 20 years so
I think there are these cultural
problems which are as important as the
technical ones but by making the data
available you can do more useful things
and and also removing the burden for
students to have to do stupid things
that should be easy I think personally I
think students need to be looked up
Thank You Andrea I am Andre a Eureka
thanks for the presentation I have a
question concerning our curriculum in
computer science because when you go to
the first part like I go beyond
competition okay and sometimes I think
our curriculum eat too much is too much
algorithms anything changing and not so
much so much that the center okay so do
you think we need to review that or
thinking about the new curriculum to do
specific kind of computer science guy
yes I mean I do to it familiar with
algorithms as as if you like the heart
of computer science for shortest path
and all this sort of stuff yes and
that's what we teach but but we also
teach things like neural networks which
are not conventional algorithms and they
learn from the data and they evolved to
do pattern recognition or whatever
they've been trained to do and those are
not conventional algorithms so we do
teach some of those but but you're right
I think we do need to think for example
if we were doing a one year master's
course for scientists what would we
teach them we should teach them more
than just algorithms we should teach
them a whole set of technologies in
machine learning and they are all about
data how you learn from data and I think
one of the major insights in computer
science it is his Bayesian networks the
fact that and there was this big debate
among such as tacticians because there
were the people who believed but they
were called the frequentist that you had
to take data many times and take an
average that was the only sensible thing
to do and then there were these people
called Bayes who believed you could make
a guess and that's not very scientific
but from that guess you could improve
with more and more data so in vases
paper for example the example he gives
is of a newborn baby
who doesn't know whether the Sun will
come up next morning so he takes a bet
it's fifty percent likely it'll come up
fifty percent it isn't if it doesn't
come up he's going to call up with the
black ball and the white ball if it
comes up and he puts them into a bag and
the Sun comes up so it puts a white ball
in because the Sun came up and after a
hundred days in that bag you have 99
right balls and one black ball so your
initial guess which was 5050 has now
become very much weighted towards the
Sun coming up and it's about learning
from data and that's a very important
part and bayesian methods and bayesian
networks are an important part we should
teach people how to deal with
uncertainty and i think that's an
important part and it isn't the
traditional algorithms I agree okay we
just have time for one last question
which is over there I mean dhruba here
and professor from the Department of
Computer Science I had a question
related in quest of my colleague you
have present does the last talked and he
your last line is constantly future I'd
like to ask first importance money
Master under such that what's the
importance when worthless of
developmental mathematics we have talked
a lot about technology computer science
yes absolutely no again when I talk
about the fourth paradigm it does not
mean that experiment competent theory
and computation are not import the
course they are and similarly one of the
fundamental bedrocks of computing is
mathematics so mathematics is important
and the example of Bayesian networks is
where I think is one of the places that
mathematics can make a difference but
there are other places that mathematics
new mathematics can play a role
absolutely agree so mathematics is going
to be fundamental in doing this data
analysis data mining and we need people
who are educated both in mathematics and
computer science and science and that's
the challenge that we have to have as
professors in universities that's what
you need to create these new data
scientists which can go into science but
also to industry and help the country's
innovation the gender and business so I
think you can do both but mathematics is
a kip-up key part okay thank you let's
thank attorney for this very
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>