<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Crowd-Powered Systems | Coder Coacher - Coaching Coders</title><meta content="Crowd-Powered Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Crowd-Powered Systems</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9vC3g2-uoVk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
sue and I have the pleasure of
introducing Michael Bernstein coming out
from MIT working with Rob Miller Michael
really needs very little introduction
he's been out here a couple of times for
internships working with four or five
different groups he's now working with
MSR New England so he's got plenty of
experience with us you know Michael's
amazingly decorated he's good his
multiple best papers pet best paper
awards and has a fellowship in one of
ours I'm the MSR graduate fellowship and
he's going to tell us about his work
over the last couple of years combining
human and machine intelligence so
Michael thanks great so yeah I'm excited
to talk to you today about crowd powered
systems and crowd powered systems are
interactive systems that are going to
combine human intelligence from crowds
of people collaborating online with
machine intelligence and to set the
stage for why do this might be a good
idea i'll start with the word processor
and that's because the word processor
might be the most heavily designed
heavily used interactive system ever and
like most interactive systems it tries
to support a very complex cognitive
process writing and it does so by
helping with some really complex
manipulation tasks you can think of by
now how we've got some relatively
efficient algorithms to help with layout
we can build language models to help
with spelling with grammar but at some
level what we really have very little
support for is the core act of writing
itself or maybe even editing think of
questions like expressivity word choice
or even situations well like this so how
often have we all been in a situation
like this where you have an hour or two
before deadline a strict word or page
limit and you're a little bit over
length I think we've collectively burned
a few more cycles on fixing this kind of
situation and we'd like to admit now
historically when you're in this kind of
situation you would turn to other humans
in particular editors if you were
published author you could turn to an
editor who would help you shorten your
text who would collect things that
Microsoft we're
didn't catch for example spelling or
grammar errors and in the sense they
were really a core part of the writers
toolbox but this has really never been a
part of the toolbox that we could make a
permanent part of our software because
if you wanted to do that you would have
needed these editors to be available at
large scale really at any time and that
just hasn't been possible but today we
do have tons of people online taking on
some really impressive tasks this is
known as crowdsourcing and you know even
with in areas related to computer
science we're using crowds to help
collect data for machine learning
algorithms we're running Studies on our
systems even social scientists and
economists are behavioral economists are
running large-scale studies using
crowdsourcing platforms or folding
proteins we're even writing collectively
an encyclopedia and this isn't a new
phenomenon in fact it goes back to the
1700s when the British Royal astronomer
started distributing spreadsheets for
the calculation of nautical see charts
through the mail and it reached its
original height in the 1930s when a WPA
project hired 450 so-called human
computers which is actually the source
of the term computer that we use today
but what I'd like to point out is that
this lineage of distributed human
computation has really acted as a batch
platform that is you take a lot of your
work you push it over the wall you wait
a while hours days and then eventually
you bring it back and run some analysis
on it what I'm going to talk about today
are ways in which we can turn
crowdsourcing from a batch platform into
one that supports interactive systems
that is rather than having a single
human editor help you out with a
situation like this where we're stuck
between what the user is willing to do
and what the system can support what if
we had tens or thousands of individuals
all look at your document so just ways
that it could be improved or shortened
we could algorithmically start to
identify the best of their suggestions
and then give you access to them in an
interactive system that's what I mean
when I talk about a crowd powered system
interactive system a user interface that
supports that that combines machine
intelligence through whatever we can
design or AI with crowd intelligence
now let's say that we thought this is a
good idea you're going to run into a
couple challenges when you try to build
these kinds of systems the first one is
quality a few weeks ago I asked a
thousand people online to flip a coin
and to type h if they got heads and t if
they got tails so hopefully in this room
would be about 50 50 turns out on the
internet there's actually about a two to
one ratio of heads to tails and this is
not exactly that the Internet's a biased
coin it's that people are trying to
optimize for money they start
satisficing they try to generate
randomness and when people are trying to
generate randomness they do so and
non-random ways in fact you might notice
that this doesn't actually add up to a
hundred percent that's because fully
seven percent of the respondents didn't
type h or t they actually went outside
of the grammar if you will and typed out
the entire word they misspelled it or
they wrote the enigmatic f and these are
these are the kinds of interesting
challenges that we need to face when we
start talking about integrating crowd
contributions voluntary voluntary or
otherwise into software systems because
algorithms may not expect this a second
challenge is going to be speed or
latency for building interactive systems
we expect them to react quite quickly
and crowd sourcing just does not react
to that quickly in fact when it first
came out people were very excited about
it saying that it is extremely fast and
then pointed out that it was 48 hours
before they got a response in fact in
fact some folks at UC Berkeley ran a
survival analysis model and found that
the half-life for responses in these
kinds of systems varies between 12 hours
to days roughly depending on how much
you're offering in a paid crowdsourcing
context we really need to cut this down
by orders of magnitude if we want to
have interactive systems so today I'm
going to show that we can in fact create
these crowd powered systems that we can
overcome these challenges with quality
with latency and embed crowd
intelligence into our everyday
interactions and that in order to do
that I'm going to introduce several
computationally motivated techniques
that help crowds accomplish these tasks
that they wouldn't otherwise be able to
accomplish again overcoming these
challenges of quality and of latency now
I'm going to focus for most of the talk
on paid crowdsourcing I'll come back
near the end to talk about how we can
use other kinds of crowds to take on
lots of
that paid crowdsourcing would never be
able to do but for the moments you may
have heard of Amazon Mechanical Turk is
perhaps the most popular paid
crowdsourcing platform on Mechanical
Turk there are millions of tasks that
are that are done things that look like
this label an image transcribed a short
audio clip for small amounts of money
usually on the order of a few cents and
people do a large number of these and
hopefully make up a reasonable amount of
money if you look at the population on
the unsystematic and across a variety of
indices gender education income it
mirrors the overall population
distributions suggest that you really
have some relatively educated
individuals on these platforms who are
looking to supplement or completely
replace their existing income we're
going to use paid crowdsourcing to
explore this notion of crowd powered
systems I'm orienting my talk around two
main systems the first is Soylent it's a
word processor with a crowd inside which
will hopefully convince you that this
entire idea is worth pursuing and second
is adrenaline which takes these concepts
and makes them happen in real time quite
quickly i'll start with Soylent Soylent
is people it's a word processor that's
recruiting crowds as core elements of
the user interface now I'd like to point
out before I start that I'm actually not
the first person to come up with this
name you may know one Danielle Fisher
who handed the name off to me at some
point what I hope you take away from the
section is that we're really embedding
crowd contributions as a core part of
how this system works and that we're
going to take something that crowds
aren't potentially very good at and
decompose it in such a way that we can
actually focus individuals efforts and
get higher quality responses so rather
than tell you about Soylent I'm actually
going to show it to you so this is
Soylent running on the Soylent paper
which is a little meta but let's this is
exactly the situation we pointed out
earlier where we're a little bit over
length let's say that we've decided this
conclusion is a bit too long rather than
shortening it myself I'm going to
actually ask for some help I'm going to
push it off to one thing that that
Soylent does called shorten when I when
I push this Soylent to push a bunch of
tasks off to mechanical turk you don't
to pay too much attention to the details
here but you can see that workers are
marking up the text they're making some
edits they're doing some votes when it
all comes back we've collected all of
these suggestions that the workers made
and we can start to put them all
together what you see here is on the
left my original text and on the right
everything that has been marked up
anything that's underlined in purple
here is a section of the text that has
been marked up as being short nabol a
patch we call it and you can see that
with every patch has a number of
different options that have been
suggested as potential rewrites that are
short that are shorter we can then
consider the space of all possible
paragraphs order them by length and I
give you a slider such that when you
drag the slider the text rewrites itself
and become shorter or longer or really
anywhere in between and hopefully when
you're done your text is now on 10 pages
this shows me the new kind of
interaction we can build by talking to
crowd intelligence but we can also talk
about how we might support existing AI
systems crowd proof is as a crowdsourced
copy editor effectively it's going to
find errors that Microsoft Word didn't
and indicate solutions and give you
plain English suggestions of what's the
problem you can see here that the crowd
has suggested that this paragraph has
two potential problems with it you can
see that they've explained in one case
the sentence too long in another another
case here there's actually a parallel
sentence structure error so introducing
is the correct way of putting it the
interesting thing about this second one
is that this error got past I think
eight authors and six reviewers before
crowd proof caught it before camera
ready deadline the reason is that this
is at the bottom of page five and by the
time we're reading and getting to the
bottom of page five our eyes are getting
a little bit tired but crowd members are
coming in with many different
perspectives and they're perhaps even
seeing this text as the first thing they
see so there's a lot of different
there's a lot of different reasons here
why we can that was not grab power why
we might actually want to draw on the
crowds here now we can also talk about
how crowds might support natural kinds
of input to a system in particular I
tend to write like this I leave my
my citations in brackets like this and I
need to come back later and fill in and
write out of bibliography in particular
if I'm using something that takes in big
tech as input I need to go find that
metadata let's say I wanted to ask for
help with that we can push out to
something we call the human macro not
everything can be proofreading or or
shortening this allows you sort of
open-ended requests so in this case I
might just ask for help finding the bib
tech for the citations and brackets and
rather than writing this myself I'm
actually going to show you what one of
our one of our user study participants
created you tell us a little bit unclear
you can located these by Google Scholar
searches and clicking on bib tech not
the clearest thing in the world but
we'll go ahead and paste it in we can
say how many people we want to help how
much we want to pay them and when it
comes back you'll see something it looks
like this where they've gone out to
google scholar figured out what we meant
and brought back the VIP tech that in
short is silence silence goal is to
reach out to these crowd contributions
to create new kinds of interactive
systems interactive set up shortening a
new kind of interaction so aid with with
proofreading supporting an AI system and
open-ended requests like the human rack
probe find me a figuring because it's
the internet they'll find you cats these
are the kinds of things that we think
are possible when you engage with crowds
as a core part of interaction now if you
were to try and build a system like
Soylent you might come up against some
interesting challenges as I pointed out
earlier in particular we've worked with
a lot of Mechanical Turk workers on
these kinds of systems and we simply see
that roughly a third of what we get back
or thirty percent is just not something
you'd want to show to a user
particularly not a user who might be
paying for such a system and we need to
actually deal with this quality issue if
we want to make such a system really
large scale deploy able so why is this
happening i'll explain it through a
couple of personas i took this paragraph
off of a high school essay website it's
a really horrible paragraph I've just
underlined a few of the issues with
here and we asked Mechanical Turk
workers to in an open-ended sense just
edit it make it better proofread it and
you see two kinds of personas here one
we would call the lazy worker this is
someone who was trying to optimize for
money and send a clear signal that
they've done the work but do no more
than they really need to so given this
this really error-filled paragraph a
lazy workers going to do something like
this that is they made a one character
change to the word comradeship to fix
the spelling and it's not surprising
they did that because it was the only
word that was underlined in their
browser as being misspelled but they
made a clear edit on the other end of
the effort spectrum there's the eager
beaver this is someone who's also trying
to give a signal that they've done the
work but they sort of go too far they go
outside the bounds of what the system
might expect given the same paragraph an
eager beaver will make some some good
fixes but then they're also going to
insert new lines between every sentence
which is not something I personally
wouldn't consider an improvement to the
text and these personas are not specific
to mechanical turk you can think about
say wikipedia where you have some
workers who are working quite hard to
try and make edits but sort of getting
reverted because they don't know the
rules you have other people who are just
getting by and in my in my opinion a
state of programming with crowds in the
loop is still very early days and in my
view it's sort of similar to before we
had patterns like model-view-controller
and so on that started to codify best
practices such that you could get
reliably better results so our goal here
is going to be to start thinking about
what such design patterns might be in
the crowd computing space I'll introduce
one that we've that we use in soylent
called find fix verify the notion is
that this design pattern is oriented
toward these open-ended problems like
the things that Soylent tackles I can
contrast it to a closed-ended problem
something like a multiple choice test
where you can upload some small
percentage of ground truth and use that
to compare and figure out whether the
work is good here there's a huge space
of possibly correct answers what we're
going to do is we're going to decompose
this very open-ended problem into into
three stages which are slightly less
open-ended and give workers more
direction I'll explain it through an
example we use this both with
reading but and shortening I'll show you
with shortening here rather than having
workers directly edit the text we're
first going to have them just find areas
of the text that can be edited we're
going to effectively get a heat map over
say the paragraph we're going to look
for independent agreement across workers
to certify an area of the text is a
patch and we're going to send each patch
out in parallel to a fixed stage in the
fixed stage we're going to show what
each another set of workers exactly one
of these issues and ask them to fix it
that is to shorten the text to fix the
typo depending on the on them on the
text we're going to collect a bunch of
these suggestions randomized their order
and put them through a verify stage the
verify stage is going to try to enforce
some invariance here basically we want
to make sure that we're not changing the
intended meaning of the text and that
we're not introducing any new style or
grammar errors anything that survives
the verification stage we can finally
pass back to the application logic and
in particular here create something like
shorten okay so why is this a good idea
in particular why would we split fine
from fix why not just let workers go in
and sort of improve the text well one
major reason is that we're actually
taking advantage of these two personas I
introduced in the fine stage we can
force the lazy workers to find two or
three different errors in the text which
gives them a lower bound that they can
understand and we can understand and in
the fixed stage we can point these lazy
workers at a problem that is perhaps not
the easiest one and they can't sort of
get away without having fixed that
particular problem so by giving them a
more specific task we actually end up
producing higher quality results at the
same time we can focus the eager workers
on a task that we really want
accomplished right now and help
hopefully keep them from going too far
off the rails it also allows us to group
the suggestions so we can get that drop
down if you've ever passed out a draft
and then gotten a bunch of different
edits that are confusing but we know now
is that given a particular problem these
three edits are all different ways to
fix it so you don't have to actually do
that merging yourself the notion behind
the verify stage is that we get higher
quality output by putting these work
and productive tension with each other
so you have one set of workers whose
goal is to try and suggest options and
another set of workers whose goal is to
consider critically whether that those
are correct I'd like to point out that
well well this was somewhat early in the
space there's really a growing
literature much of which was done here
in fact that's playing into this larger
space of what happens when we combine
crowds and algorithms yeah I verify read
in a more general rank which would allow
you to say show two possible solutions
to some of them haven't think the better
one so it's actually agnostic with crowd
proof we just try to pick the M best
like oh this is the one best because we
want to make one edit right with with
shorten we actually just want to filter
out anything bad and sort of get a
general rank because we just want to
continue we want a set of as many
options as possible but you could
imagine doing a rank and then you'd have
to assume where some cutoff would be but
certainly there are many different ways
you could run a verification stage this
is just one way did that address your
question okay come back later if I'm
still being confusing uh we wanted to
know whether this works in particular we
wanted to know three things uh how has
the quality does this look like how long
do you have to wait and how much is it
going to cost so we can throw a bunch of
input texts at soylent in particular
here's shorten here are five different
input text we gave it ranging from
techcrunch HCI papers OS papers and my
personal favorite a rambling email from
the Enron corpus and feed it through
Soylent and get edits that look
something like this now across all of
these texts we see that we cut about
fifteen percent of the original
paragraph length on average what this
means is that you can take an 11-page
draft of a paper hold constant the title
figure all that boilerplate run it
through shorten and get a 10 page paper
back without having changed any of your
core arguments so why does this work how
does it work workers tend to avoid any
sort of technical content they don't
understand and instead focus on wordy
phrases in particular in this case the
phrase are going to have to can be
shortened to have to without changing
the meaning of the text too much
and these are exactly the sets of
phrases that we can start to collect a
corpus of and train a machine learning
system to take over much faster and for
free just have the crowd take over a
verification stage now they make more
complex edits as well for example
merging sentences this sentence now
reads the larger tangible bits project
comma which introduced the metadesk and
two companion platforms okay but this
does not always work here's some
interesting ways in which it fails one
is that workers are not a member of your
community of practice that is they're
not experts and they might mistake their
expertise there's a signaling phrase in
academia we say in this paper we argue
that workers just find it boring so you
may disagree legitimately so expertise
is one issue another one which is
endemic has to do with parallel ISM that
his workers in one patch can't see what
the workers and the other patch are
doing so in this case you have to list
items and they cut the main phrase from
one and the parenthetical from the other
which leaves the resulting sentence
somewhat meaningless if you wanted to
fix this you would neither need to talk
about enforcing global constraints which
is what how she and Eric I've been
looking at or starting to merge patches
when they get too close together ok so
we across these three stages are really
recruiting hundreds of people for each
of these texts costs about a dollar 25 a
paragraph if you're willing to wait
longer can get down to about thirty
cents and from there you can start
talking about optimizing and sort of
indecision in theoretical terms trying
to minimize the number of workers you
would need for each stage to optimize
some global quality constraint now how
long you have to wait there are two
types of wait time in Soylent the first
is between when soil and asks for help
and when a worker says okay I'm going to
help you out and now if you sum the
median find the median fix and the
median verify you see about this takes
about 18 and a half minutes now can take
much longer this can stall but roughly
you're looking at about 20 minutes the
second way time has to do with when the
work between when the worker says
they'll help and they actually complete
the task and again if you sum the
medians this is actually much faster
it's about two minutes now the second
half the talk i'm going to show ways
than which we can get that 18 and a half
minutes down by several orders of
magnitude
but but you're looking at perhaps in the
limit about a two minute wait between
when you ask for help and when so I can
get back to you so we can do the same
thing with crowd proof we can give it
lots of different input text bad
Wikipedia pages text which passes words
grammar checker and then at the top
which is an essay written by a
non-native English speaker I'll focus on
that one you can see some of the edits
that it makes word by itself finds about
one-third of these errors crowd proof
finds about two-thirds of them and
interestingly they find different errors
which is to say if you combine them you
get about eighty-two percent coverage
when it finds an error crowd proof fixes
it about ninety percent of the time so
when does it miss most commonly when
there are two errors in the same patch
and the lazy workers come in and fix the
obvious one but don't notice the more
detail a subtle one so the same
processes happening over and over human
macro same thing you can see the exactly
the text that i pasted in earlier other
things like finding figures changing the
tenths of a document I'll focus again on
the first one the input text looks
something like this in fact if you're
familiar with the literature you know
that this is an incorrect citation
Duncan watts is one person not to but
the workers still manage to figure out a
noisy input and a noisy command and get
the correct answer now there's no
verification stage here which means that
the thirty percent rule comes back so we
see that about seventy percent of the
time these things are pratik are perfect
and about ninety percent they have the
right idea but some subtle error in them
so far I've introduced you to Soylent
which has introduced this new space of
interactive systems that are powered by
crowd contributions and in order to do
this I've introduced the fine fix
verified design pattern to you which has
started to focus these contributions to
address questions with quality yes and
on what percentage of the workers end up
actually contributing it is the survivor
verification
what percentage of things like if you
have a lot of lazy workers who end up
giving you totally useless things so
what you're asking is is there a high
correlation if you give me bad things
now will you give me bad things later so
yes my sense is yes I don't have numbers
for you there certainly there's a power
law of contribution in most of these
things such that a small number of
workers are actually producing a large
amount of your results there are lots of
folks who think about sort of this
global quality management crowd flowers
a good example of someone who maintains
sort of across many tasks and I think
the first thing you'd want to do is
start to build up us a better notion of
reputation either as the platform like
Mechanical Turk or oDesk can do this
better than any individual requester or
as a requester we can start to get
feedback from the user saying I like
that edit that edit was really bad and
we can sort of propagate backward yeah
your views are what's there yo and wider
wider so often I don't have deep
knowledge that is I think you would want
to spend more time talking them to get
the sense but there's sort of this
notion that you're over compensating
sometimes this has to do with if you
have an interesting task or it's the
first one you sort of don't know the
task parameters yet or the boundary
conditions I think that's one thing
that's going I think often it has to do
with them overestimating their abilities
as well like yes I'll just insert new
lines you know not a good idea yeah yeah
digital so say that incentive a factory
proponent
no in fact that winter mason and Duncan
wats paper demonstrated that paying more
gets you more work that is faster but no
no no increasing quality in general
that's that's what we see you need to
design your task better to get higher
quality results should i think is as an
HDI person that's nice to hear that
means that i can actually have an impact
ok so at this point i'll turn to
adrenaline which is going to take these
ideas and push them into the real time
space the reason we want to do this is
that the kinds of applications we can
build are really constrained in a very
deep way by latency now Soylent was one
of the first crowd powered systems I can
actually turn to the broader research
literature that has started to explore
the space much in a much broader way
that even I could alone showing that
it's it's useful for design health and
nutrition robotics vision many other
kinds of things but fundamentally all of
these applications are constrained by
the same limit as Soylent which is that
sort of 20-minute wait time in fact the
best result we've seen in the literature
comes out of Jeff Biggums group at
University of Rochester which is able to
get one response from a worker about 60
seconds after you ask and that response
isn't verified because it's a singleton
and if that's the best we can do we've
already lost because usability
psychology has demonstrated that users
will only pay attention to it
interaction at max for 10 seconds
they'll lose the flow so what we really
need to create our on-demand flash
real-time crowds that's our goal here
we're going to pick one motivating
application which is going to be
adrenaline a camera that is for novices
sort of built into your interior into
your cell phone and what it's going to
do is for the kinds of situations pushed
out beyond what Mike's been working on
we try to sort of find aesthetically
subjectively the right moment to take
the photo so it's sort of the moment
camera up the crowd powered we want to
do this in real time because ever since
the introduction of the digital camera
it's become a core part of our photo
taking experience that you take the
photo you see the result we can take
another one you can share with your
friends we don't want to go back to an
era where you have to develop your film
overnight so this is the kind of thing
that
that it only looks like you can see that
they're capturing a video of people
doing high fives there one of them is me
and as of right now we just made a
request to the workers to help us choose
the best frame they're going to poke in
along the bottom there start exploring
the space and very quickly they'll focus
in on the final frame now so just a few
seconds later we have a final frame here
are a few other kinds of pictures that
adrenaline takes you can see people
trying different angles different kinds
of poses action shots like people
jumping off of a bench or people just
being silly and hoping that the crowd
will will pick a cute moment and again
we can collect data from these kinds of
systems and start to train more
automatic ones as we push out so if we
want to create adrenaline we need to
solve two problems and these correspond
to the two wait times I pointed out
before the first is how we get the
crowds there quickly and in order to do
this I'm going to introduce a new
recruitment approach for crowdsourcing
that we call the retainer model the idea
behind the retainer models that we're
going to ask workers to come and sign up
before we need their help and we're
going to actually pay them a little bit
extra I while they can go do anything
else that can work on other tasks they
can check their email they can chat but
as soon as we have a task for them
they've sort of implicitly agreed to
come back when we have a test we just
pop up a simple javascript alert brings
their attention to that to our browser
tab and we go from there so does this
bring people back quickly it's an
empirical question in fact in the space
of HCI kinds of questions is one of the
most measurable so we ran a study on
Mechanical Turk counterbalanced across
days of the week times of day and what
we did is we had people sign up for a
task and then we called them back so I'm
going to draw a graph here on the x-axis
you're going to see how long it took
between when workers saw this dismissal
and when they click the ok button
started working on the y-axis you're
going to see a CDF what percentage of
all of the workers click the ok button
at least that quickly they were
randomized into different wait time
buckets so if the workers weren't
waiting very long you saw a curve that
looks something like this they're
waiting a little bit longer a curve that
looks more like this what you can take
from this is that if the workers are way
under Phi under 10 minutes you get about
half of them back two seconds after you
ask and in fact you get about
three-quarters of them back three
seconds after you ask now what happens
if they've been assigned to wait longer
now you see more attrition but in a
separate experiment we found that if you
offer a small bonus you can take a curve
that looks something like this that is
like a twenty-five percent chance of
this worker coming back and push it all
the way back up as if the worker hadn't
been waiting at all so we changed the
incentives we changed the behavior so I
notice I said that we paying about a
half cent and have sent a minute into
this sort of the expected wait time so
it costs about thirty cents an hour to
have someone on retainer now even just
with this where we can create some real
time kinds of applications we built one
called a bee that sort of crowdsources
traditional kinds of instant votes so if
I want to know which which tie to wear
today or which of these two designs
people like better over here on the
right you're gonna see a go button I'm
just going to click the Go button and
then we're going to replay a result from
one of our studies so this is something
that took 20 minutes with Soylent 60
seconds with Jeff Biggums work and we
get five votes in about five seconds so
this is the kind of thing that the
retainer model can do crowds in two
seconds and traditional crowdsourcing
kinds of tasks in about five seconds and
that's great if what you're trying to do
is choose between two photos but if
regulans not in particular it's trying
to choose between say a hundred or more
photos all at once so what happens now
is that the workers arrive quickly but
it takes them a long time to actually
shuttle between those last few frames
and choose the best frame so how we're
going to help them find that decisive
moment how do we help the workers work
together in order to overcome these slow
work times and we're going to take
advantage of one notion here which is
that we really have created synchronous
crowds for the first time you can assume
that all of these crowd workers are
arriving at once not sort of arriving
and leaving at you know at individual
whim as you usually have on Mechanical
Turk and we can start to think about how
we could get them to work together
particular I'm going to claim that if
we're smart about this we can get the
crowd to work fast
collaboratively than even the single
fastest member of that crowd the way
we're going to do that is through a
technique we call rapid refinement and
in a continuous search space like what
we have with adrenaline the notion with
rapid refinement is to look for
agreement early as it's starting to
emerge before people would have made
their final selection and use that to
reduce the search space quickly and
focus everyone's attention so I'll
explain what we mean with pseudo code
here on the left you see what the server
has on the right there are three workers
who get initialized to random positions
in the video the server sees all of them
and we're going to start looping until
we get down to a single frame we're
going to look for agreement we're just
going to say how many workers are within
a particular region of this video right
now there are none I'm going to wait
until there's a certain amount of
agreement say two-thirds as the workers
start navigating through the space
there's still no agreement eventually
you'll see that two folks do come
together indicating that they are
interested in the same region rather
than immediately jumping forward we're
going to make sure it's not a false
positive so we're going to make sure
they stay in that region for two seconds
they do we're going to certify this as a
refinement reduce the search space so
those folks who agreed can stay exactly
where they were anyone who disagreed
won't get paid yet and we'll get
reinitialized to a random new part of
the video and we're going to keep doing
this again and again until we get down
to a single frame this is how rapid
refinement works i'm going to show you
exactly the same video i showed you
before just focused on the bottom part
so you can see that workers arrive and
very quickly they're going to start
agreeing on it that central region we
can have a refinements then an
overlapping another refinement down to a
single frame so just a few seconds now
what came out of this that is does this
work do we have some sort of quality
time trade-off happening here and when
we have low quality results what's going
on assuming see that there's an
underlying assumption that there is a
single best place because you have
multiple carrier functions yes so if
there's a five more distribution
it is that generally days after day
video is taken with one kind of optimal
place we assume that there's one intent
but you don't actually the nice thing
about crowds is that they're sort of
their many of them so what you can do
although we don't do it in the current
implementation is just fork right so you
can you can imagine these this as
populating some probability distribution
and if you see two peaks you can just
sort of put one set of people over here
and one set of people over here or just
focus on one for now and then you can
wheel around when you have more time to
explore the second one and I'll point
out another reason in a minute why that
might be a good idea yeah more quick
question what what's the final dollar
amount that you were paying for fine yet
Frank when you're at I'll show you in
just a moment so yeah cost is going to
be another elements here so we actually
had 34 folks I think from our University
come in and take video photos from this
and we produced five different candidate
frames from each of these each of these
input videos one of these frames was
generated using rapid refinement as I
described here a second while I was
effectively a ground truth we had a
professional photographer come in and
choose that best moment and third was a
off-the-shelf production level computer
vision algorithm for choosing aesthetic
or representative frames within video
you can think of this effectively as
what YouTube what does when it chooses a
single frame to represent your video the
other two techniques were more
crowdsourcing oriented generate and vote
looks a lot like fine fix verify we call
people in off retainer they nominate
frames we call people more people off
retainer and they vote amongst those
frames generate one just takes the first
response we get in generate and vote
that is the fastest member of the crowd
as soon as they produce anything we just
take it so we can measure two things one
is sorry three things cost latency and
quality we'll start with quality so we
can have these these people rate on a
nine point likert scale how much they
thought that the photo was was what they
were looking for that they liked it and
what we see is that rapid refinement
tends to do statistically better than
computer vision which chooses a
different moments and statistically is
indistinguishable from the photographer
due to large variance now typically you
see something that looks like this
where the crowd chooses something in the
same general area but not exactly the
same frame on the top row they were
actually just one frame apart sometimes
you see something like this though where
you notice this is a bad photo that
guy's eyes are closed its blurry so what
happened here we actually had a false
positive you had two workers who were
interested in nearby regions of the
video that didn't overlap but they were
close enough to each other that the cyst
that the system thought they were look
interested in the intersection snap down
and they were left with a region of the
video that had nothing good so if you
wanted to catch this you would need to
sort of notice thrashing behavior and be
able to pop back out and explore a
different area ok so here I can answer
your question about cost rapid
refinement was about twenty cents a
photo and then it went up from there so
in what I would hope you would take from
here is that rapid refinement was
actually not only the fastest mmm but it
was the most reliably fast as
statistically had the least variance
which I would claim is really important
for interactive systems you don't want
something that reacts quickly sometimes
you want it to sort of be reliably
reacting quickly and really what's
happening here is that we're pulling up
the tail sometimes you really do have
fast individuals but sometimes you don't
and rapid refinement can identify that
longer tail or that M sorry it takes
that longer tail and pushes that
probability mass to the left generating
vote still performs in under a minute
which is much faster than Soylent and in
fact matches the quality of the
photographer which we thought was pretty
cool yeah confused to have to second the
second row there is it seems like I mean
generate one if you're using the same
for to your scheme
when it seems like that should be pushed
even further toward zero because it's
the very first person in response to
anything in the upper case you're
talking about Minnesota's Canada so so
it is pushed a little bit closer farther
than zero if you want to like take your
eyeglasses you can see that it's
actually sort of one unit left they
still have to scan the entire thing it
takes them some time to get called off
retainer and we know we you were just
taking the first one so we have five
people on retainer we call them all back
and we only pay attention to the first
one so sometimes you randomly don't have
a fast person in your crowd so that's
really what's happening here okay so we
make a few trade-offs one strength is
that we actually get fast preliminary
results so within that ten-second
boundary we can return something to the
users and that happens on average within
that first refinement happens within 10
seconds we also don't need a separate
verification stage because verification
is effectively built into this algorithm
we're looking for agreement as we go but
we do sacrifice some things we're
sacrificing some amount of quality as we
saw to get to get this speed trade off
you can think of this roughly like
randomized algorithms where you're
getting a not potentially optimal result
but something that's much faster so you
have you have this trade-off now and
more importantly in my opinion is the
fact that we're actually stifling
individual creativity in the system and
this is not just adrenaline in rapid
refinement fine fix verified as the same
thing most crowdsourcing systems all
have this regression to the mean
effectively happening imagine you were
the photographer in the crowd you would
have no special ability to actually pull
the crowd toward what you know to be a
good result if you want to push forward
in this I think you want to start
talking about automatically identifying
these experts as we were talking about
earlier and giving them a privileged
position within the app within these
systems in these algorithms now in terms
of generalizability rapid refinement we
think really applies to sort of single
dimensional continuous search spaces
largely just within photography you can
think about brightness contrast color
curves these kinds of things so by
combining the retainer model and rapid
refinements we're able to execute these
really large searches in a human
perceptual space within about 10 seconds
and this allows us to turn around and
start asking these same kinds of
questions about say creativity support
kind
of applications this is Photoshop let's
say you were creating a poster for a
rock concert and you wanted to have a
band of skree Torah a crowd of screaming
individuals in the audience this puppet
warp tool allows you to author control
points sort of like Takeo agora she's
work and you can drag it now let's say
we call people off of retainer and make
and say make that person look excited we
can have a bunch of individuals do that
manipulation we can take all of their
suggestions draw them back into a layer
in Photoshop and produce something that
looks like this so in particular with
about eight workers on retainer you
start getting feedback in a couple
seconds you get your first figure and a
half a minute and we went out to several
hundred figures and kept getting new
ones every three seconds on average so
we think we've really closed the loop
here and connected this back to a
Productivity style creativity support
desktop application that's allowing you
to sort of draw on this crowd
intelligence as as you need for things
that perhaps you would never think of
now back off here for a moment and point
out that the retainer model has started
to systematize the recruitment process
for crowdsourcing we're changing that
recruitment process and by systematizing
it we can actually begin to model it and
ask what happens when we go from having
say 20 people on retainer to huge
numbers I won't go into too much detail
here but it turns out that you can cast
the retainer model using queueing theory
that is it this is just a formal
framework that allows you to sort of
understand that if workers are arriving
at some rate you can recruit new workers
tasks arriving at some rate and you can
recruit new workers at some other rate
ask questions about how long is the key
with the line and in particular this is
an MMC CQ which is to say we have C
workers on retainer and if we have any
more than that number of requests we're
just gonna give them a busy signal so
now we can ask what's the probability
that when I need help that is there's a
task there's no one left on retainer to
help me we can derive this from Erlang's
loss formula in queueing theory it's
this pie of see you see here as a
closed-form formula you can also ask
what's the expected number of workers on
retainer which gives you a better sense
of cost you can then plot those two
things against each other and treat it
as a minimization problem you can say
how few workers do I need on
gaynor to quote to have some guarantee
of service like a 1 in 10,000 chance
that when someone wants to help there's
no one left to help them this has lots
of other applications you can think
about asking you can model what happens
when you share retainer pools across
applications you can ask what happens
when you then start routing tasks to
workers to avoid starvation or you can
do what we call predictive recruitment
or pre crewman which is this notion that
if we if we know that the UH navarrete
task is going to arrive within the next
10 seconds and that workers will
maintain their attention for up to 10
seconds we can actually recall the
worker before we have the task to show
them a loading screen for a moment and
when we do that we actually see that we
can get feedback in just a half second
it really starts to blur this cognitive
boundary between me pressing a button
and seeing feedback as sort of
cognitively part of that of that action
so you can push farther on this but I'll
I'll back off here we'll say at this
point I hope I've convinced you that we
can create real time crowd powered
systems and that we can introduce
techniques in order to support these
things like the retainer model and rapid
refinement so yes question you created a
model of human behavior by I given
people these incentives to stick around
and wait for your response and so you're
basically one economic entity in the
system who's done this and the question
is right at justice in any type of
arbitrage system what happens when
everyone else starts running arbitrage
right this is exactly what do the same
right this is exactly why we want to
start asking and modeling what happens
when we combine retainers across
requesters what what I want to put
forward is that the platform could
actually support this you can imagine
having two sets of tasks there's the
real-time tasks and the non real-time or
batch style tasks and you could sort of
agree to follow like in the sense of
Twitter like I like that request or that
kind of task that kind of task that kind
of task and there's give them to me as
they come otherwise I'm going to start
picking up tasks and then the system can
actually consider this a space of
everything I've signed up for its space
of everything else people have signed up
for and the tasks that are coming in and
then route them to actually sort of keep
a a globally optimal solution so it's
definitely possible right now on
Mechanical Turk to do that kind of
arbitrage right I'm trying to push
forward in
say how would you design the next
platform to avoid that kind of problem
but your concern is absolutely valid
given where we are now we have an office
fire I'm assuming a wet monopolist fire
that is there's one system making the
day
that handles all the requests out to
the workers uh I mean it's I wouldn't I
wouldn't call him a napolis buyer but
you could you can imagine that way its
platform support or you could you could
be like crowd flower a middleman right
where I will help you get real time
workers right and you just sign up
through me and all and all I'll help you
there but yes effectively we're talking
in that case about what happens when we
centralize I think if you start
splitting and everyone's competing for
real-time workers it would work just not
as well right for exactly the reason
that you that you that's your intuition
okay so across these two systems I hope
hopefully I've convinced you so far that
we can create these crowd powered
interfaces these interactive systems
that are supporting the kinds of tasks
that we could traditionally not support
with computing systems sort of this line
between user and system now there's a
third dimension effectively of crowd so
that we can create these interactive
systems that embed crowd intelligence
and that in order to do that we can
actually start to look toward
computationally motivated techniques to
help the crowds accomplish these tasks
now at the beginning I promised that I
would push past paid crowds and I'm
going to do that now there are actually
many different kinds of crowds out there
on the web we can pay crowds we can
create new kinds of crowds we can mine
the activities that crowds have already
have gone and taken upon themselves and
I just want to give you a brief tour
through sort of that bigger space
because I actually like to play across
all of them with several citations to
work i did here actually i'll start with
designing new kinds of social computing
systems but in particular if you wanted
to create a crowd that never existed
before this is work that tends to appear
at HCI conferences like KY and whist as
well as social computing conferences
like I see wsm our goal here is to
create new kinds of social systems that
never existed and understand how to
design those systems now I'll give one
example this is work that I did with
Eric and desney and Greg and several
others unfriend sourcing the notion here
is that we may actually want information
that a generic crowd would never know so
in particular if I wanted to know what
to get desney for his birthday
mechanical turk would have no idea Yahoo
Answers has no idea but people in this
room his social network really do and by
creating incentives over the social
network we can encourage people i should
say Mary was also involved in
this work sorry this is what happens
when you're on the spot the to encourage
them to share these tags many people in
this room in fact we're some of collabos
biggest users we got tens of thousands
of tags of on thousands of individuals
and a follow up we created a system
called feed me that starts to
effectively learn models of people's
interests by by riding on this activity
of people sharing interesting news with
each other and when we do this we can
create systems like this we can route
questions like does I you I research
tend to appear at the whist conference
these individuals are tagged with both
kinds of with both I you I and whist but
they never had to sit there and tag
themselves with their interests we were
taking advantage of the fact that there
is a power law here that we can take a
small number of individuals who are
really active on these social networks
and spread out their their interests and
activities such that it's to the benefit
of everyone else in the social network
we can also ask what happens when we
take unusual designs in in the space of
social computing systems particular you
may have heard of 4chan or /b they
created the Anonymous hacker collective
which you may have heard of it's sort of
heterodoxy community to say the least
it's a an unusual place in the internet
I don't recommend checking it during the
talk now they make some really
interesting decisions one is that by
default all posts are anonymous two is
that they don't keep archives it's not
go global in fact when new content comes
in it pushes off older content so we we
we got five and a half million posts
from this from the site and simulated
the dynamics of the site to ask what
happens in a large-scale online
community when you have anonymity and
ephemerality as core design tenets we
saw that the median thread lasted just
five seconds in the intentional sphere
of most people that is on the first page
and was pushed off completely from the
site within five minutes in fact we also
found that over ninety percent of the
posts were made completely anonymously
and we found some interesting ways in
which it was there was a suggestion that
these exact decisions of anonymity and
ephemerality or what we're leading to
for chan's ability to drive internet
culture if you've ever seen a lolcat if
you've ever been rickrolled you have
experienced the output of 4chan
so we can think about these kinds of
questions of how to design these online
communities as well we can also talk
about mining what crowds have already
done this is again work that tends to
appear at HCI conferences like Cayenne
whist I'll focus here on some work that
I did over the summer with Sue and Jamie
on tail answers that's right I missed
another one and Eric thank you I'm good
for it all right so so answers you may
be familiar with or one box is something
like this when you query for whether in
addition to the organic search results
you see something like this which is a
result that's been designed specifically
perhaps kind of sad in the case of
Boston for that kind of query but we
don't have any kind of response for
something that is a much less common
kind of query like what are the
substitutes for molasses which we know
to be actually collectively quite common
that is there in the tail there is a
large number of somewhat popular queries
so what we created with something called
tail answers where we can again augment
these these organic search results with
a direct response telling you exactly
what you would replace molasses with in
fact we can create hundreds or thousands
of these through an automated process
answering questions like how long does
all those stitches last the story of the
invention of the light bulb how to turn
up the volume on windows XP many others
that are all sort of collectively
somewhat popular and I'm sorry in an
individually somewhat popular and
collectively quite popular so we really
do turn to crowd data to make this
happen we can look for for search trails
like like Ryan has been exploring where
we identify where people start searching
navigating through the web and find
pages where they have an unusually high
probability of getting to that page and
then ending their search session if we
combine that with looking for the
Canaries in the coal mine a small number
of searchers who use question words in
their queries like what is the average
body temperature of a dog we can start
to identify web pages where people are
finding concise informational answers to
their to their needs we can then use
something that looks a lot like fine fix
verify to extract that content from the
web and promote it into a direct
response so really there's a broad space
here of crowd powered interface
and crowded powered systems we can talk
about how we might pay people how we can
create new kinds of crowds to collect
information that's never been collected
before how to look to what crowds have
already been doing so my goal really in
the large scale is to integrate social
and crowd intelligence directly as a
core part of interaction as software and
of computation more generally now
focusing just in the paid crowdsourcing
space there are a lot of ways to get
there we want to think about how we
integrate crowds with machine learning
for one that is we can already now start
to deploy these systems collect the data
and train better machine learning
systems but we can also then take these
machine learning systems and use them to
make the crowds more effective for
example entail answers we found that by
using an open information Strack shins
open open information extraction system
we can actually just have the crowds vet
the answers which ends up being much
faster and cost less we can also think
about what happens when you say start
treating these workers as say like
stumped learners in an ensemble we want
to think about the platform how would we
change odesk Mechanical Turk many of
these systems TopCoder we've seen what
happens when there's a small scale like
hundreds of individuals online at once
what happens when everyone is is a
contract or effectively that when we
have hundreds of thousands of people
participating how do we help them
develop expertise and notice when they
have the expertise how do we help with
lifelong learning what is what did
benchmarks and complexity look like in
this space if you come up with a better
algorithm how do we actually compare and
understand the ways in which it is
better and at a high level we can start
to talk about ways that we can combine
machine and social intelligence to take
on these really complex or high level
tasks think of sort of the big questions
helping you write a lecture write a
symphony big questions now this work
opens up many cans of worms now we don't
have enough time to really get into an
in-depth discussion here but I want to
give you a sense of the kinds of
questions that I think about and that I
think are important in this space first
is that we have this sort of returning
notion of scientific management how do
we think about contract ethics in this
space how do we make sure that people in
expectation make a living wage when
they're doing piecework what happens
when your software has goals and dreams
that is there are individuals
participating as part
of this system and you want to support
their interactions socially want to give
them the opportunities for career
advancement these are all important
parts and I will art and I would argue
will lead to a better results if we
think from that platform side finally
we've complicated notions of attribution
should we've had sons of authors on the
silent paper on the flip side if there's
an error who's now at fault so these are
just a few of the issues that I think we
need to push on so in the meantime
people have picked up fine fix verify to
start doing things like image Sigma
cakes image segmentation like you can
see in the upper right there authoring
maps they've modeled it using formal
crowd languages it's been integrated
into coursework at several universities
and more broadly again well Soylent was
one of the first crab powered systems
I'd like to point out that there are
lots of these systems that are really
gaining traction in the research and
practice space things helping with blind
individuals translation databases it's a
big space and I hope you'll come play
with me in it so I hope I've convinced
you in this hour that we can in fact
create these crowd powered systems that
are going to enable experiences that you
wouldn't be able to accomplish with just
machine intelligence but nor could
crowds on their own perhaps do them
either we're creating a symbiotic system
that actually plays to both of their
strengths and more generally I hope I've
convinced you that computation can
become a critical component of what's
known oh it's the wisdom of crowds so
I'm part of a small crowd of
collaborators my closest mentor is Rob
Miller and David kharghar at MIT a
variety of researchers across many
institutions including here graduate
students undergraduates and many others
so thanks to all of them and at this
point I'd be happy to turn it around to
discussion and questions thanks
yes just your observation of the workers
out there
kind of return for our parents yeah I
don't even have a sense what the total
population
hopping over times
spread out right so I think this will
continue to be a question the late the
most recent information I've seen has
sort of moving east I guess would be a
good good characterization so there are
more workers in India than there used to
be I think the model you want to keep in
your head here is that people in the US
are using it to supplement their income
in India Bill theses actually do some
really great work at MSR India looking
at ways in which people are actually
replacing their income entirely in ways
in which you can say you cell phone
platforms are give people cell phones as
a way to actually start expanding this
that's a great question Amazon doesn't
say my estimate would be tens of
thousands are signed up but perhaps
hundreds maybe low thousands online at
any given time that might even be an
over estimate I think that these
platforms have a large space to grow
yeah I mean I said five million tasks a
year that's actually that so if you
think about who's actually using these
largely it's researchers and and
companies like crowd flower that are
using it to like verify business
listings and so on part of what I do my
role here is in expressing the much
broader space that crowds could really
tackle and by doing so I hope that will
push open the boundaries of what can
happen and then you look to things like
odesk where there's real expertise like
PE I've hired music engineers to help me
create a song for a chi madness last
year lautech people mathematicians all
exist on these platforms so you'll start
to see continuum from mechanical turk
which is homogenous and sort of generic
intelligence out to things like odesk
where there's real expertise
promise to come back to you in case I
didn't if I wasn't cleared are you happy
I'm so good question nothing was why
live verify rather than rank so verified
would seem to return a boolean and
language would assume that it takes an
input of multiple things we didn't give
the best or continue
so and just rang it so i guess i was
saying my be specific when you could be
gentle are you absolutely you could
Eric's done some thinking on how to do
generic sorting using crowds that would
be another way you could do this that's
effectively rank right so now you have
sort of noisy comparators effectively
you know really what we're getting is a
histogram that is for each for each of
those pieces of text we're getting a
number of votes that it's bad so in a
sense we can get a noisy rang from that
and it's just a matter of what you do
with it but you're right that you could
push out more generally and consider the
verifies is a notion that's semantics
right that means like we're trying to
effectively get a notion of what's good
and bad but you can imagine rank being a
better term for that if you wanted yeah
so one thing people often ask about
crowdsourcing you know what wit domains
can you apply to it I think you get a
lot of really complain
you could talk a little bit about
generalizing some of the things you
talked about like task decomposition to
find things verified to give a few
examples that can apply you notice has
decomposition the key to through game
domain how might that play into cash you
might try and do an advanced this for
the system yeah so I think about this in
the following way right now I mean I can
view this as a as a limit like theorists
want to know what are the limits of this
but you can also view it as a research
challenges you know what can we engineer
in the future this sort of a weird space
in the middle there one thing that
crowds are currently quite poor at is
anything that requires high level
knowledge that is if you wanted to
actually have your entire paper
shortened you probably would there's
this orthogonal element in which we give
every paragraph and have it shortened
individually but when I shorten the text
it also happens by me saying this
section just feels really wordy or I
could just cut this paragraph entirely
and if you want to make those kinds of
assertions or imagine you wanted to
build a crowdsource personal assistant
someone help you order pizza reserve
rooms set up meetings they would need to
have some globally consistent knowledge
of who you are and that you don't like
anchovies right how do you build those
very large-scale kinds of pieces of
understanding across lots of distributed
test seems like a very hard problem in
particular you can think about you know
how much time it takes someone to get up
to speed on a task with in comparison to
how much it takes them to actually do
the work so it takes me forever to sort
of figure out what it is I need to do
and get the expertise and read the text
and then I sort of hit yes or no it's
not right now good match for
crowdsourcing but we don't have a good
sense of that curve and sort of how
quickly drops off that would be a great
thing to do Eric so while HD topic to
cover
which particular challenge really
I think that start pushing out bigger
exactly what I was but I was suggesting
you start pushing out from sort of not
toy systems but things that are taking
on simple tasks two things that really
start solving complex interdependent
problems is I think really hard and
really exciting if we can make it happen
jobs well I mean I think about whether
you could start attacking that through
sort of sampling based approaches or
whether you could actually create mini
management structures within the crowd
to start taking on those kinds of things
I've no clue whether that will work but
that's sort of what's exciting about it
certainly i also think really at a high
level pushing at bringing crowd data
into interactive applications is a
hugely underexplored space in HCI right
now and I'm perhaps preaching to the
choir here uh-huh but I really do think
that that's another thing that has huge
legs that we can push on yes Robin is it
just salt persistence right yeah do you
have any thoughts on top
like an illicit word spell checker I
know it's not as good as editors but I
know that it works right yeah this club
power systems who knows maybe the course
I'm not gonna be as good as my great
point so you know another way to put
this would be if i run crowd proof three
times I'm gonna get a bunch of different
suggestions right I may not get the same
things twice that's a hard problem um
but I think that is something we need to
start addressing I pointed out that
reliability in terms of latency was
really important but you're absolutely
right that in addition we need to think
about sort of reliability in terms of
repeatability great point I have nothing
to add but I think that it's important
yes good our systems you already shown
my at least one or two ways
a whole bunch of ideas place
or other kinds of desktop applications
at the same time I think one of the
things that those supper systems on
YouTube is there
perhaps you're not trying to have this
lotion we're trying to construct
something we're talking about what
you're going to do against your pedigree
and so the software systems are nice
because they're kind of like private to
you and they're reliable in that way
what do you think about how do you have
the crowd as a consultant under NDA or
how do you have a minus E or I'm you
prevent we got here for the publishers
sort of confiscated things yeah so honey
prevents your stunt your creative works
from being ripped off and released
before you release it like if I just
publishing its paper by the way if
you're in the business of making a
creative work that's right you don't
want to have the first chapter edited by
the crowd and then released online
that's right so what you start to see
already is that companies are getting
contracted crowds not for mechanical
turk but under NDA several companies are
doing this so you can sort of have a
sort of your own on-demand crowd that
you size dynamically as you need based
on the needs of the enterprise but yeah
you could also think about like
homomorphic crowdsourcing right what
would it mean to actually take it and
reliably obfuscate the critical parts in
such a way that the work can still be
done but but also yeah thinking you were
sort of pointing towards what happens as
you go off the desktop and what's
getting crowd-sourced is stuff about you
where I am at any given point there
we're just starting to see people think
about this I think Jason Hong is
starting to think about it for example
in his students but it's going to be
it's going to be a fun ride for sure at
the very least
questions
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>