<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Emerging Issues Privacy Summit - The Algorithmist:  Ethical Considerations in Data Use | Coder Coacher - Coaching Coders</title><meta content="Emerging Issues Privacy Summit - The Algorithmist:  Ethical Considerations in Data Use - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Emerging Issues Privacy Summit - The Algorithmist:  Ethical Considerations in Data Use</b></h2><h5 class="post__date">2016-08-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/56RvG0U-MJU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
today's discussion is really a part 2
part 1 was something will we cover some
information at the big data summit last
about six months ago so I want to do as
a brief refresher on that because I
think it sets the stage for a lot of the
other things we're going to talk about
today at that summit we talked about the
idea of this measuring life of data
fication of everything and in this
scenario we talked about how there was
some some researchers who were able to
through the inputting 360 different
sensors within the seat were able to
identify each unique different users so
apparently the way we we say it put
enough sensors there is uniquely
identifiable and so what that led to was
this idea that a seat is no longer just
a seat it becomes something very very
different and enables all sorts of
different uses so what we're starting to
see is taking all aspects of life and
turning them into data talked about
collecting big data these idea is that
we don't know the future value of the
data we've heard that earlier today and
we can't connect the dots we don't have
and the feeling is that their
traditional requirements driven
collection fails so we collect
everything and we keep it all at least
those were kind of the key points noted
by the CTO of the CIA and his
presentation at Giga own I bring that up
because i think that there's some truths
here in terms of how we think about big
data but falling back to what blaze was
talking about earlier some really really
interesting implications about what this
means talked a little bit about the role
of the algorithm and how that's growing
interesting article about how we're
training
drones are now enabled to chronic Oh
make decisions interesting quote there
from rear admiral on their drone program
the other quote here is from google and
talking about how they were making
decisions as to what articles would
appear in Google News interesting point
here I think is no humans were harmed or
even used in the creation of this page
again I think that sets the stage for
some very very interesting questions we
also talked about what we must do
great quote from a I found this really
fascinating book you mentioned last time
as technopolis so once the technology is
admitted plays out his hand does what
it's designed to do our task is to
understand what that design is we need
to before we admit that technology we
should have our eyes wide open and
asking all the right questions so why do
we need to keep our eyes open because I
think we all know data is just starting
to play out its hand
a couple of great examples we're seeing
it all over in terms of there is all
sorts of opportunity identifying areas
where if there's going to be an outage
we can identify it earlier through the
through this coat massive collection of
data and creation of these new
algorithms the other side we can now
using using different algorithms
identify what might be a top hit on
billboard we might even be able to
design a hit just simply based on the
algorithms this situation here was that
they designed these algorithms and were
able to apply it to thank Norah Jones
first album before she released it and
they identified it was going to be a big
hit and nobody else thought it was going
to be a big hit well something was right
here of course on each of these there's
a downside think any of you familiar
with the Terminator films very familiar
with Skynet and the implications of
where things could go another example
one day something if the marketing
department walks in says hmm we want to
figure out if customers pregnant you
mean she didn't want to know kid could
you help well yeah can and I think most
of you are familiar with the target
example and it turns out that really new
parents are fantastic it's a great
opportunity a whole new buying
opportunity whole new stage of
consumerism and there are points in
people's lives when those routines
change and when there's an opportunity
and data can enable us to make great
things happen or maybe turn out a little
bit creepy for those of you know the
story or don't know the story the
situation here was the target was able
to identify potentially people who may
be pregnant they then would send coupons
vouchers to those individuals and in
this case they sent it to this
dividuals father who apparently did not
know that this this individual was
pregnant another example the ability to
identify when there are these high peak
emotional moments in gameplay and to
deliver that advertising when those
individuals are most receptive to the
brand message wonderful opportunity here
as it turns out microsoft has actually
filed a patent on this i don't know
whether this is actually been approved
or not and again in these situations i'm
not saying good bad i'm simply saying
these are examples of where things are
going with our ability to collect this
data and it raises these questions and
why we need to keep our eyes wide open
so how do we keep our eyes wide open
well there is an existing framework our
values standards of business conduct
laws policy the stand mps so we have the
corporate values set of beliefs that
help govern our behavior integrity
honesty open expect full i think you
guys are all familiar with these we also
have the standards of Business Conduct
and those reflect our commitment to
ethical business practices and all of us
need to know those values the standards
of business conduct in laws regulations
and the policies that apply so for a lot
of us here that's a Microsoft privacy
policy and then the standards that go
along with them so that's great that's
very very helpful the standards are very
very helpful in a lot of situations
policies are very helpful in a lot of
situations but in those scenarios that
we just went through how do we begin to
draw the lines around what's really
acceptable a lot of times those laws
aren't there already those standards
aren't there already right now we can
ask we do ask is it consistent with our
business values does it meet the
business objectives is it legal
is it permitted under the notice and
consent we've heard a lot about notice
consent what that makes happen or
doesn't make happen and then is it
compliant with our standards as I said
even if you're following the law you can
do some stuff that makes your stomach a
little bit uncomfortable that's quote
from the target employee who's the data
analyst in that case so what we end up
doing is saying well okay yeah it does
all these things but is it creepy which
this then brings us back to the values
the the high-level corporate values
again and really understanding what is
creepy I think we all know we all have a
slightly different variation on terms of
where where's that creepy line what does
that mean what does it mean across
different cultures so we need to ask
what is the ethical thing to do really
what is creepy I'd say there's creepy
plus there's the choices we make about
who gets to participate so in terms of
what offers we enable in terms of on
what devices are different services
available to and who actually can afford
or use those different devices we are
making all kinds of decisions about who
can participate we're also making
decisions about users and how they're
defining or expressing their identity in
this case Mark Zuckerberg saying you
know if if you have more than one
identity then you know you lack
integrity well as we heard earlier is
his identity the same when he talks
about his business as he is in terms of
protecting his information we're also
making decisions about who gets what in
this case orbitz had done some analysis
and determined that Mac users actually
we're willing to spend more and
frequently did spend more on certain
travel turns out they were
I'm buying buying a top-level room
sources the medium level rooms and they
were willing to pay more so they charge
them more we're also making decisions
about you what users will do but they
would come in the future in this case
the target is a great example you think
about education or any sort of other
opportunities that come from Big Data
we're also making decisions about what
opportunities users have to alter the
decisions that are being made Kate
Crawford who's an MSR researcher very
interesting paper on really big data and
due process what sort of rights and
opportunities to individuals have to
change the decisions that we made the
offers that are being made the things
that are being said about them that are
then changing a lot of their experiences
so we need to establish a common
vocabulary and mechanisms to identify
and address these issues so and I'm
running out of time but there's a couple
of potential paths forward like present
here and this afternoon we'll have an
opportunity to dig in these into these
further so we can map our values
policies and standards against different
scenarios so we've got our values we've
got our policy we know some different
scenarios that may not be covered in the
standards and we can begin to say well
do we have some gaps here in that case
again for discussion purposes only in
some cases maybe we could say we need do
a better job of requiring teams to
identify what is the benefit that's
being offered here benefit has a key
component is a key component of what
regulators are thinking about in terms
of the sufficiency of the notice and
consent we can also talked about are we
respect we talked about contextual
integrity in terms of respect are we
respecting the context in which the user
gave us that data terms of the
perspective are there situations where
we actually need two more formally say
we need to require a different
active on these types of data uses and
control are we really providing enough
control to those users about all these
different potential uses and
implications for their lives or maybe we
could just add to our current policy
we've got these the policy that's
largely aligned with Phipps maybe we
just need to add something that says
it's align with data ethics say we take
steps to ensure that our collection use
the data is in keeping with our values
and foster and then from there we can
develop a whole set of standards that go
along with that we can also establish
new mechanisms may be super great
because Janice is going to talk about
this one and a couple others in
particular maybe we need a
cross-functional review group who can
really dig into these and provide more
input more guidance in a different
perspective maybe there's an opportunity
to identify different questions that we
could ask the right stages and the data
life cycle that are really going to
drive people to ask these right
questions and come up with some
different solutions I don't want to get
into this but this is it covers a lot
more information but if we looked at
those different points in the life cycle
there's all sorts of opportunities to
ask you know do we want to do we really
want to collect this data do we need to
collect this data the implications of
collecting this data or even telling
users that we're going to collect this
data you may have a chilling effect or
may have other implications that we
really aren't comfortable with or our
users aren't going to comfortable with
in terms of our expression of rules and
we're tagging the data the decisions and
how we express those rules and what we
mean in the wordings words that we have
also have implications that we need to
be aware of in terms of how long do we
store the data and who has access to
that data and there's a whole other set
of questions around the modeling of that
data and maybe we just want to create
some inferences around sexual
orientation or we want to create
inferences around any
of different things we should be asking
certain questions at those different
points and we've gone through and
there's all sorts of different questions
we can dig into these this afternoon and
time it's really about trying to prompt
teams and individuals to expand their
thinking about the potential harms
potential benefits of their use of the
data in this case what we did was try
and map some questions to the Universal
Declaration of Human Rights and some
some analysis the Ryan Calo isn't out
the u-dub did around objective and
subjective harms that could impact
individuals again as I mentioned maybe
we need some new standards they require
analysis and benefits the require
contextual integrity you require
analysis the harm so next steps we've
got a working group session your input
incredibly valuable really appreciate it
and then what I'll do is for those who
are interested in stead of a follow-up
session because where we want to go
right now is identifying those scenarios
identifying potential gates and testing
of prompting of questions or other sorts
of analysis that we can do with a
specific team different scenarios
because what we don't want to do is
drive ethics applies to relate to
everything that we do we're not in a
situation right now to drive the ethical
framework for all of Microsoft well we
want to do is drive value to those teams
we're trying to make right decisions
right now and get them get the services
and products shipped and then moving
forward before you know it will be able
to address ethics and our algorithms
it's a little bit of a joke coming but
it's there's the opportunity there but
it's not today today we have to take
these initial steps with that what I
want to do is introduce Janice because
Janice is actually working on these
initial steps within MSR
hi I'm Jess I i am the privacy manager
for Microsoft Research and this is going
to be more kind of like an overview of
how ethics works at least in a research
setting so in terms of Microsoft
Research we really believe in doing
innovative and cutting-edge research and
I really believe that we're enabled to
do that with this outer ring of making
sure that we are doing things in an
ethical manner as well as in a privacy
protected manner and so the reason why
this is a big deal in research is
because not so long ago researchers used
to do really really horrible thing in
the 1930s there's this thing called the
Tuskegee syphilis experiment where we
basically studied we wanted to study or
the government wanted to study how
untreated syphilis would affect rural
African American men so they found 600
people the majority of whom had syphilis
and they just said hey we're going to
provide you with free healthcare hooray
they did not tell them that they were
sick they just said that oh I think you
have bad blood unfortunately this study
span did 40 years and 10 years after the
inception of the study they'd pretty
much figured out how to cure syphilis
but they prevented people from actually
getting treatment nor ever even finding
out that they had this disease and so of
course that led to many bad things
similarly in a university setting Yale
there's this famous experiment called
the milgram obedience experiment Milgram
wanted to see how people dealt with
people in positions of authority and
whether they would just agree to do
whatever someone who was in authority
would tell them to do and so they
solicited participants one of they
solicited participants and so people
would show up in a room they'd be like
oh I'm here for study draw straw oh I'm
going to be the teacher you'll be the
learner so the teacher is on no this is
left and then the learner and all the
learners unbeknownst to the people who
are selected as teachers work this is
played by an actor
and so in the getting-to-know-you phase
or in the waiting phase they'd be like
the actors say oh you know that's the
only kind of funny this heart condition
I've been sick what's going on and they
would say hey we're going to have you
answer questions every time the learner
gets something incorrect push this
button to shock them so there's this
machine that was supposed to be
delivering electrical shocks and they
just move it a tick every time the
person got something wrong and so it's
clearly marked on the machine there's a
dangerous sign as well as an xxx death
horrible do not do this sign and during
the session they the learner would start
screaming and pounding on the walls and
then just become unresponsive and the
participant would be like oh I'm not
really comfortable I kind of want to
stop but there'd be a person in the
white lab coat saying no please continue
and they would keep on telling them to
continue up to three times and for the
majority of the participants they just
kept on going at the end of the study
they just said thanks for coming see you
later here's your you know use your gift
certificate or here's now here's some
candy bye and this caused significant
psychological harm to the people who
have participated because they really
thought they never realized that the
person wasn't being shocked and that
they didn't actually just kill someone
right so this was one of these things
that was really bad and this spurred a
change in laws and the creation of
what's called institutional review
boards so now institutional review
boards are required anytime there's
research that's been funded by federal
departments or agencies and there's a
whole set of regulations and rules it's
outlined by the Department of Health and
Human Services one of the other things
that came out of this time period were
reports kind of talking about basic
principles so the most well-known report
is the Belmont report coming out in 1979
and so these are kind of like the basic
things that you should think about in
terms of ethics you have to respect
people make sure you're protecting their
autonomy that you obtain and
informed consent and that you're
truthful about what's going on as much
as possible you figure out the
beneficence in terms of do no harm to
people on weigh the risks and benefits
and finally the end making sure that
procedures are administered fairly so at
MSR we do have an ethics program and our
focus is really to have people consider
the ethics in their research and we have
an ethics advisory board on to discuss
those thorny issues and questions and
it's made up of researchers from all the
labs around the world that we have and
people can go through several options
for an ethics review they can either use
their collaborators university or
medical ethics review board they can
hire an external one or they can go
through our process and for us in terms
of a computer science type of world
these are the kind of high level best
practices I try to convey to our
researchers that they're dealing with
people like having people come in or
even online they have to obtain informed
consent if they are planning on using
deception because in some cases you do
have to deceive people so that they
don't act differently than what they
would normally do that you should
debrief them afterwards about the true
cause an example of this is if you're
trying to study how people look at error
messages you can't tell them hey we're
studying error messages because then
they're going to pay really careful
attention to those error messages and in
terms of privacy I really want to make
sure people are aware of the source of
their data but they've notified people
properly in terms of data collection in
use they're storing and sharing the data
correctly and they're protecting it as
much as they can and I'm just going to
jump to one final scenario because I
think this is the really interesting one
in terms of Twitter so we have this
issue where there's a lot of data that's
available online and the terms and
services of these online web sites like
Twitter or other social networks not
including Facebook they just say hey you
are aware that you're posting your
information online we're not liable for
what happens afterwards right if other
people see what books you like to read
or for other people see how the
locations you're going to it's up to you
to really protect your own privacy
right and so that means researchers from
all over the world are just going out
and scraping data and creating these
large data sets and here's one such
example of research that can be that is
interesting can be done in this case is
related to Twitter communities there's
researchers in our silicon valley lab
they want to analyze the Twitter fire
hose or all the different tweets that
were coming out and they found that they
could algorithmically identify groups
loosely organized around hashtags so in
case you've never seen Twitter people
often add little number signs or a
hashtag to help other people search for
tweets in this case oftentimes if you go
to an event or a conference or
everyone's going to see the same movie
they use a hashtag so you can identify
self identify like Oh i really liked
Hunger Games or oh I'm at the privacy
law scholars conference plsc and so they
found that in addition to these one-time
only based on the event hashtag things
happening there's people that have
actually used hashtags to organize like
weekly communities our weekly discussion
groups and in their presentation of the
research one of the big teams saw that
they done this and said I wouldn't be
great if when people were doing searches
in bing that they could also see a
Twitter group that would be associated
with their search term in this example
someone has searched for dementia and
there is an all-timer support group that
meets on mondays at 6pm to 8pm and you
know we can figure out how to identify
and scrape these chats and then figure
out what the main topics were and then
maybe categorize them and love people to
see archives alright so that means oh
now we can help people find these people
to talk to you in case they want support
for Alzheimer's and so this is one
example of what this is just a mock-up
of what that would look like right so i
think the difficult thing is is that in
terms of legality this would be fine
to do right because there's nothing if
you are using the public twitter api and
you are collecting tweets and doing all
this investigation and analysis on your
own you could say hey there's a travel
group that meets on wednesdays there's a
another support group that meets on
thursdays now let's surface these groups
unfortunately people don't really have a
good understanding about the gutting of
as we've mentioned before context is
very important so if I'm part of an
alzheimer support group I'm thinking I'm
just chatting with these people about my
problems and all these feelings I have
and how I'm so upset about my
grandfather having Alzheimer's I don't
necessarily expect that a search engine
is just going to make this public for
everyone to see and as the moderator of
such a group maybe I don't even want
other people to be able to see this who
aren't part of the community and so one
of the analogies that we like to use is
you could be up like an an a a group
that meets regularly you go and you get
together you go to the park it's a nice
sunny day you sit under a tree you
expect the conversation you have is
private even though you are in a public
space you don't expect that anyone else
is typing everything you're saying and
putting it replace somewhere for people
to see forever and so what happens with
Twitter's like it's become like a
microphone is in the tree actually doing
that recording and broadcasting for the
world all right so we're still trying to
figure out what is the best way to deal
with these ethical issues because on one
hand there's legal on the other hand
there's ethical and unfortunately in
terms of ethics it's not there's not
really like a hard line rule just like
in privacy right in privacy we talk
about that creepy feeling same thing in
ethics like we could do this but should
we and other issue is related again as
we've mentioned big data where before
especially in computer science maybe we
were studying error logs related to the
system not people but now people are the
data and that shift of doing that
understanding of oh I'm studying
facebook data other Foursquare social
networking location data it's getting
that my shift under having people still
understand this is about actual people
still it's not just anonymous data so
these are some of the issues that we're
grappling with we're going to be
creating kind of a more concrete review
system as well as a submission process
with peer reviewers and I'd be happy to
hear about the needs from other product
groups and maybe we can have a Microsoft
wide system instead of just the research
specific system or just a you know other
types of platform specific systems
Thanks thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>