<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MSR Vision Faculty Summit - Adventures in the world of 3d Computer Vision | Coder Coacher - Coaching Coders</title><meta content="MSR Vision Faculty Summit - Adventures in the world of 3d Computer Vision - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MSR Vision Faculty Summit - Adventures in the world of 3d Computer Vision</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/f0rBj-nIkKw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
and now for the second session George
will be talking about his adventures in
3d computer vision okay thanks for
inviting me yeah pologize for apologies
for the title serious case of writer's
block i think so the work i'm going to
be presenting is have been part of it
has been done while i was in toshiba
research in cambridge university and at
the moment i'm i'm i'm working at Aston
and the outline of my talk is going to
be I'll begin by giving you kind of my
turf point of view on what's happening
in the world of 3d vision based on my
experience in the past few years and the
particular what are the key emerging
technologies emerging applications sorry
and would which of the technologies that
are out there are kind of maturing and
what are the limitations and then I'll
briefly kind of sketch some simple ideas
that we have been pursuing lately
hopefully to try to overcome these
limitations okay so let's move on
there's an ever growing need for
photographic 3d models and this was
definitely true when I started my PhD in
2002 but it's even more to now and I
think the reason is because technologies
are getting better and better and I
think people just want more out of them
and a 3d model is of course just a
digital copy a real object that allows
us to inspect details measure properties
on counter physical properties and if
you want we can also reproduce using a
different material okay so that's
basically what this is about we are
looking to extract these types of models
and again when I started my PhD I think
all of us thought that the big
application domain was cultural heritage
preservation and the things that go
around it digitizing collections of art
works for museums and galleries and
stuff like that and while there still is
needed significant number of projects
that are doing certainly that I think
it's becoming more obvious that
the real sort of target group for our
systems is the entertainment industry
and actually the fundamental user group
and you know there was a big drive to
cross the uncanny valley in face
animation and this was driving a lot of
the lot of the progress but as well as
developing all kinds of 3d assets for
Hollywood or games the games industry
related to that there's also a growing
need for the web 3.0 type applications
any particular big scale City modeling
and people are trying to create
life-size computer graphics models of
urban environments that's you can
actually come from navigating this is a
really cool system by the way if you
come visit it I strongly suggested it
then at the same time there's a drive to
bring this technology's closer to the
home user and I've always believed that
this is something great in the night
example of this system has been
announced in Cambridge actually where
you just capture photos of your face and
body upload them and they will create an
avatar and that lets you try out clothes
virtually and stuff like that so all of
that is so all of these are actually
real really happening applications as
opposed to think that sometimes
researchers tend to think might
materialize in the future and they never
do these are actually things that are
happening now and I think it's important
to ask wise vision such a successful
sort of approach delivering 3d scanning
solutions as opposed to other ones and
because I actually believe that this is
actually one of the best examples of
computer vision in the real world right
so why is it successful well I think the
reason is because if my take on it is
because the traditional approach starts
from the real world and then you have to
invest a lot of time and effort to get
measurements you know laser scanners
spectroscopy all kinds of stuff like
that to measure shape physical
properties then you feed this into a
model you know compile all these numbers
in the model and then you use machinery
in computer graphics ray tracing these
types of things to finally generate
some visuals for what you want to model
okay so this is what the user actually
gets at the end and I think what vision
does is effectively it's taking a short
cut out of this whole approach and
basically you start from the real world
but now you just take photographs it's
quite simple but this is really what it
is and then your model is effectively
trying to match the appearance of those
images with the visuals that you will be
selling to the user at the end okay so
by construction these models are
basically designed to be photorealistic
because you need to try to mimic reality
there's a plethora of different types of
Technology exploiting all kinds of
visual information to extract shape
however I think some of them are quickly
maturing into real engineering systems
and now i'm just going to sketch the
tool that i think are kind of good
candidate or that the first one is of
course the photometric polarimetric sort
of domain surface capture people like
Paul tyvek image metrics all the light
stages that has mostly found use in in a
high-end film industry type applications
and the results are very very high
quality but the complex the setup is
typically very very complex and most of
the time very expensive which I guess is
kind of good if you're developing these
systems but not very good for adoption
okay and on the other opposite end of
the spectrum there's a very low-tech
solution which is multi-use theory based
around these multi-view stereo
technologies and this is based on
capturing 10 to 100 high red stills
which makes it quite cheap lightweight
and easily deploy the indoors or
outdoors and now we actually have a set
of actual software systems that have
been developed starting against with
josue system who I guess started this
whole vogue by releasing stuff for
people to publicly try out an experiment
and play with and this caused all these
other drives for commercialization and
you know software ization let's say and
in fact the system will you develop with
neil
carlos roberto myself has also seen a
few converters you know commercial
implementations little bit let's say
I've been involved with one for the past
few years but I think have been others
as well the system by the imaging group
in Paris has actually been licensed to
Autodesk and they're giving it out for
free and there's also slightly more
professional looking ones like the
software from RG soft called photoscan
and you can actually downloading a trial
version and have a look at them and all
of these things are actually you know
really delivering the results and there
are the router at the moment however any
any practitioner anybody that has ever
tried it will tell you that the key
limitations you know there are quite
serious limitations which a thing is why
some people are giving the software away
for free and one of them is the fact
that yeah we seem to be stuck to this
idea of of high-resolution stills which
I guess you know other sensors are
presenting very interesting
possibilities for example video or death
sensors of any opening form there was
even a workshop at the last nice
assyrian on this type of thing and
another limitation is the type of object
the type of scenes that can be handled
this is quite a big one most people you
know their biggest annoyance is the fact
that it only works on you know well
textured granite looking objects it will
never work on things that are uniform so
for many people that's that's a killer
that's a killer limitation finally the
word the world also tends to deform
quite a lot so you have to help you
somehow need to be able to do something
about these informations either capture
them or you know being variant to them
of some some way so the rest of my talk
will briefly describe three simple ideas
which we have been putting together
hopefully trying to alleviate some of
these limitations and starting with
video why would you want to do multiview
stereo with video the answer to my mind
is the fact that it gives you immediate
feedback as opposed to you know the
offline approach of taking the sales and
the feedback tends to lead to better
model
and the same time is also very passive
in shape on the other hand to do it you
need online camera pose estimation which
nowadays is sort of readily available
but you have a real-time online
algorithm and the key requirement there
is interactive rates and you need to be
able to cope with loads of data in the
sense of only looking at the data once
and not not not being allowed the
multiple pass the multiple passes in the
data okay so that's that's how video
system can scale otherwise it would
never scale so what we did is was
actually know the simplest possible idea
which was to try and treat pixels inside
your video sequence as depth sensors so
you know you freeze a particular frame
you consider a pixel and a 2d box around
it and then with every incoming frame
within every incoming video frame from
that point onwards you just do a search
along baby polar line that comes in
compute an ncc score or something like
that and whenever you find the local
maximum that score just treat that as a
measurement coming from this end okay of
course you know you only need to store
this little patch so it's quite scalable
to keep these sensors growing okay so
hopefully the idea is that you know get
enough measurements that somehow at some
point the correct depth behind this
pixel which is what you're trying to
measure is going to be put it's going to
pop out okay and it kind of does if you
actually look at it you know this is
kind of very hard way we type evidence
but if you look at what happens if you
have a well textured pixel you get two
types of measurement you either get
something which is close to the correct
depth or something that is completely
uniformly random based on you know
outliers on failures in ncc matching of
some sort and of course if you have
untextured or occluded points as
expected you know you get a perfectly
flat almost perfectly flat histogram
nothing kind of stands out okay so
that's that's kind of where we started
from and if this type of fixture lends
itself to naturally
a simple Gaussian plus uniform mixture
model for these measurements and here
that is the depth that you're trying to
infer x is the inland ratio which you
can also infer and you just bundle it
all up in a Gaussian plus uniform
distribution and we normally fit these
things the data using something like
a.m. or you know similar-looking
algorithms but remember we cannot do it
because we need to have a one-pass so
not allowed to multiple passes so what
we end up doing the end is sequential
inference and so you basically start
with a posterior at time T you have a
likelihood of measurements at time T
plus 1 and then you get multiplied if
you together and you get the posterior t
plus 1 okay and these are two DS of
functions right because you have depth
as well as the in high ratio okay so the
question was what form can come this
posterior take so you always have the
choice of having trying you know trying
a closed-form solution but that's
intractable of course because then the
components of the mixture model just
grow the nonparametric approximation we
know with histograms is just to memory
intensive we will never be accurate in
practice so what we end up doing and
there's a variational argument in the
paper which you can look at we end up
modeling it using a Gaussian x beta sort
of distribution and the Gaussian is for
the depth and beta is for the entire
ratio and the key thing is that this
only needs four numbers to represent the
posterior and again you know for the
same reasons you can't really do the
full variational thing so what we do is
we just do a simple moment matching
between this Gaussian times beta in this
distribution which is not a Gaussian
times that is something different okay
so we just moment match in the first two
moments and it seems to be working quite
well so I can show you here so in the
top I have the the nonparametric eight
which is kind of like my ground truth
here in the middle is that is the power
trick approximation and this is just
histograms so in the left you see the
successful case where it does work and
you can see that after a while you know
they converge to each other and
basically there and in the failure case
for some reason you know if you because
this is a proximate right and it doesn't
it's not currently to work it won't work
so you won't get the correct depth and
inline ratio but it's actually not that
bad because it converges in 20 in line
ratios of position which means that you
can discard the sensor as being bad so
it's a nice way to prune away this this
this whole sensor so now we can build an
app working on top of this simple idea
and what you do is you initialize a
bunch of pixel depth sensors the way
that I describes then you measure you
take a measurement you updated
posteriors you remove the sensors who's
in line ratio drops below a threshold
and at the same time you convert into 3d
points sensors whose variance in depth
draw drops below a threshold then at the
same time they're still considered to be
in Liars in liar generating sorry and
the final step is you just replace the
stuff that you removed or converted into
points you replace them with new sensors
in the current frame okay and this is a
video of how it works so you can see
that the red points are basically the
means of the posterior and the silver
ones are when they actually get
converted into 3d so the queue
requirement is that it's interactive and
you can actually see that you get
intermediate representations of the 3d
model as it goes on and if you spot
something going wrong somewhere you go
back you try to fix it and it's very
perfectly passive perfectly cheap and
the the dots are because at that point
we haven't bothered with actually
implementing it using a slam system I
think now it has been incorporated in
the system
so this is I mean instead of video I
think you can replace pretty much other
depth sensors i think is this is it's to
me it's quite evident that we need to
stop looking at we need to stop over
fitting on the concept of high-res
stills it's not necessarily the best
solution okay we did an evaluation that
basically shows you know various things
as the previous speaker said won't
bother you with that the really really
important thing is to have an evaluation
with with Richard which you know we need
to coordinate at some point is there
some results let me also point you
towards work that is done by my
colleagues in the shiba they are using
the system now to do recognition three
lyric video object recognition and I
think this is going to be more and more
important as we move on because you know
we need to start building higher level
models instead of just a soup of points
or triangles moving on to the next one
as I said you know big grief that people
actually have is the fact that these
methods don't really work with texture
less objects and that's that's true even
for them for the video one so if you can
see here for example thing over the desk
all soup can it's completely empty you
know the regions that happen to have no
texture in so so what do we do so when
we first saw these types of results so
this is when you apply Yahoo's method on
on this plant data set okay and because
there's no texture on the leaves etc you
know it looks horrible and so what we
thought was okay why don't we just try
the simple try shape from silhouettes I
mean even that could be better would be
better than bringing that junk and it is
true because silhouettes can handle a
lack of texture and they can also be
used by other multi-view stereo methods
as well as kind of as an augment as an
improvement step let's say so if you
have them you owe me you should always
use them the problem is that images in
the real world are too many and they
have all kinds of clutter in the
background which means that it's
actually quite difficult to do a pair
image sort of interactive segmentation
because there's just too many images in
these collections hundreds of them or
thousands so bounding boxes brushes I
mean although that is kind of difficult
so our task is basically let's do it
let's try a new it completely
automatically ok so I'll give you a set
of calibrated post calibrated images of
an object and you just give me
segmentations in all of these views and
now that I think about it I think this
kind of thing might be quite useful for
for recognition as well as
pre-processing steps so how would you do
that our previous attempt was using well
the simplest constraints that you can
you can think of which is the first one
is silhouette coherence this concept
that visual how projections have to kind
of maximally fill the silhouettes so you
need to definitely need to use it as a
very powerful constraint also use we
kind of introduced this fixation
constraint this is purely because you
need to decide on what it is that you
want to reconstruct so we just assume
that it's going to be what the camera is
looking ok so if if your object of
interest is is interesting then you know
you fixate on it and there's also the
appearance consistency which is standard
foreground background sort of color
models very similar to the ones that
grab cuts is using but there are
limitations and the key limitation is
that this foreground backgrounds of
color model is actually not separable in
space for many times in many many
situations one example is this the head
of this horse here which you can see
zoomed in and if you plot the the color
foreground background likelihood it's
totally almost blending into the
background right this is almost like
camouflage and because because the fact
that the horse without the shed is quite
consistent as well this algorithm quite
silhouette consistent these algorithms
tend to chop the head off so then we
thought of quite a neat little idea i
think which is this idea of using a week
stereo formulation which is just
treating the whole thing as a as a sort
of giant mrf problem ok we use super
pixels in order to make our life a
little bit easier so you take you
quantize into super pixels then you do a
labeling
each super pixel in your sequence as
foreground background where the unary
term is quite 34 which is just the color
model that you have obtained in a
previous adoration and the pairwise term
encourages pixels to have the same label
if they have similar color if they obey
the bipolar constraint and if other
super pixels vote for the same depth and
we plug all of this thing in an
iteration where in each other asian you
enforce the silhouette coherence and you
keep refining the color models okay this
is the kind of liquid week stereo
emulation and i'll just briefly describe
how how the algorithm looks like so okay
in the beginning you you do the super
pixels you consider one super pixel it's
almost invisible here and the optic ray
behind it you project it on neighboring
images and then you what you do is you
just accumulate evidence for for
similarity so it's like a weak form of
stereo formulation and then and then
what you do is for these locations that
are seen to be you know could have quite
a few votes in terms of the depth you
just linked them okay so maybe in a
little bit more clear detail this is my
super pixel this is the neighboring
image that the waterline you calculate
one sort of set of consistency scores
then you get another image you do the
game oops
you do it again then you just kind of
merge these so yeah so these are the
histogram versions of the same thing and
then you just combine the histograms
effectively every every image is voting
for four deaths and then in the
positions that are sort of seeing to be
to have a high high number of votes we
just linked your original super pixels
so if this guy is foreground then these
ones are also kind of encouraged to the
foreground and at the same time you can
use this to filter out things that are
not you don't have much support so you
can increase your you're positive so
it's a it's a almost like the classic
sort version of cereal only that you
know you have reduced it to super pixels
and you don't really care about the
actual depth of the end you just care
about foreground background so yeah yes
it is the same copy paste tail okay hope
is not an infinite loop
yeah and this is what what actually
would happen if you didn't have this
depth consistency thing you would be
linking this super pixel against well
the correct positions on the head of the
horse as well as other ones that happen
to have the same color okay so this is
with appearance only and if you include
the depth you just eliminate all these
other positions and you just keep the
one you want and of course it does it
does the horse quite well but most
importantly for me at least you know the
classic before and after shots we can
now get the plants quite nicely and one
thing I need to mention here is that
this is actually surprisingly quite fast
so matlab unoptimized stupid code is
about a few minutes basically like two
or three minutes per sequence so the
whole sequence thing was something like
40 images and use yeah it was doing it's
taking basically about seven seconds /
graphic adoration and it converges in
about five five or six iterations so
quite it's quite faster much faster than
multi-view stereo which is talking about
hours using and the final a little bit I
won't talk about actually I'm going to
rush through this I'm afraid is face
capture so we have this technique few
years ago based on color photometric
stereo and you know you just take an
image of an object and you illuminate it
with different color lights and the idea
is that you know you can invert the RGB
triplet with and get surface orientation
okay so that's what we had a few years
ago when it was working on kind of white
bits of cloth basically constant albedo
so now yeah so the key requirement is
calibration when you're doing
photometric stereo you somehow need to
estimate lie directions and intensities
because of the berry leaf ambiguity and
you can either use mirrors which we have
tried or you can actually use example
objects of the known geometry and in the
case of color photometric stereo what
you need to do is basically what we had
were doing before is just putting
the object you want the cloth in a
pattern and just observing the
variability in in color the problem is
that we faced is you cannot do that
because it's kind of painful to put the
skin on the template so what we end up
doing is using an example an example
version of the face of the geometry of
the face using simple multi-user very
simple motion in front of the system and
I guess the hope is that the you know
some parts of the template are going to
be good you know they have the correct
position in normal but we don't know
which is which so it's kind of like a
typical chicken and egg problem and this
basically lends itself to robust model
fitting so basically the interesting
thing here which I thought was quite
cute was that it ended up being fitting
into a 2d homography because you're
feeling from the video times normal
space into RGB triplet space and of
course you know we have two decades
worth of research on 2d homograph a
fitting and you can use your favorite
ransacked to do it yeah one thing I
didn't mention is that okay the key
assumption is monochrome olicity the
fact that the face is basically
different shades of the same sort of
color yeah and you know the lips are not
obviously fitting that because they're
purple and before he added cetera and
this is what the system actually looks
at it looks like at the end so the
reconstructions of the face so you can
actually see the low frequency error but
photometric stereo shows and a lot of
that stuff we have actually kind of
fixed but don't really have much time to
talk about us okay so I just wanted to
leave you with a few concluding thoughts
in particular deformable surface
registration is the real kind of problem
that you get on top of scanning and I
think maybe the next year the next
speaker is talking about that so yeah
the in terms of the week sort of shape
from xid I think it's generally quite
applicable and nothing I'd like to see
efforts that are more kind of quick and
dirty providing quick and dirty
solutions rather than doing the full
thing because that's quite useful and I
cannot iterate more these importance of
building higher level models that you
know for most users it's actually
useless to give them a soup of triangles
what they really want is you know this
is a door this is a wheel this is isn't
that okay so thanks and can take
questions if we have time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>