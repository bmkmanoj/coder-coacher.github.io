<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Robust Spectral Inference for Joint Stochastic Matrix Factorization and Topic Modeling | Coder Coacher - Coaching Coders</title><meta content="Robust Spectral Inference for Joint Stochastic Matrix Factorization and Topic Modeling - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Robust Spectral Inference for Joint Stochastic Matrix Factorization and Topic Modeling</b></h2><h5 class="post__date">2016-06-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TWym45U98T0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged public showing internet or
public display is forbidden and may
violate copyright law
okay what welcome to this lecture we
think Monty flying all the way from a
canal to come here for this week so he
would pop up to us about our topic
modeling and this is Ruby that are just
paper he will present upcoming nips so
we get a preview of his work he's a
extremely productive researcher and our
intern and we appreciate that he spent a
lot more with us okay thank you but
thanks for the introduction so today I
will present robust spectral inference
for joining stochastic matrix
factorization and also for topping
modeling this is a joint work with David
bindle and David meme no professors in
Crennel university before going into the
outline I will briefly talk about what's
the difference between several learning
models so in machine learning in order
to learn the model parameter we have
roughly two different method one is
likely based training we first chooses a
pro pro likelihood estimator of course
there are a lot of different estimators
like pseudo likelihood or maximum
likelihood map likelihood and then we
form a likely to function in terms of
the model parameters and then find the
best parameter usually via optimization
but there are another class of method
called method of moments so which is
first relate a population moments to the
model parameters and then estimate
population moments via sample and it
usually involves solving multiple
different equations so if i compare this
two different approach to learn
parameters the solving likely you'd
usually uses optimization which
requires multiple iteration that makes
the algorithm usually slow whereas
method of moments we are solving
equations closed-form equations and
speed is relatively fast and in terms of
the estimation quality unless the likely
function the design is strictly convex
it's not always up to mow whereas the
method of moments it's statistically
consistent what I mean by statistically
consistent is if we get more and more
sample it always converges to the right
estimator but whenever there is a
mismatch between our models and the real
data likely based method has intrinsic
power to manage model mismatch but it's
unclear in the method of moments so one
of the biggest focus of this talk is how
to handle model mismatch in a specific
problem related to topping model and the
method that I'm interested in this paper
is using both matrix algebra and
probabilistic inference together in the
same framework then so I will briefly
explain the latent Irish lay a location
so we have two hard parameters for a
multinomial distribution and for each
document and end position we first
sample the topic from the topic
distribution of that document and based
on that topic b sample the word and this
is how the topping model explained the
document generation if i propose the
corresponding view in terms of matrix
factorization it looks like this so we
have a word distribution for each topic
and topic distribution for each document
and
and because of this is a distribution
this matrix is a column stochastic which
means the sum of every column entries
are equal to one so let's say we have
currently only one document in the
right-hand side and this document the
first document it is the first topic is
the biggest topic which affects a lot
then what we are expected to observe in
terms of the word level is the
duplication of these words first word
and fourth and fifth word and if the
second document contains the second
topics and forth topics a lot and then
the expected document looks like the sum
of these these these words and these
these these words and but in reality in
reality always the model is not
coinciding with the real data so we are
observing these noise in here now I'm
explaining the event space for join
stochastic matrix factorization so in
comparing to the previous lda model now
we have a pair of topics and pair of
warriors here so these pair of topics
are sampled from topic topic
distribution and then based on those
pair of topic we are sampling the words
and words are of course related to the
word topic distribution in here so the
corresponding matrix vectorization view
look like this so now we have a topic
topic matrix the sum of all entry will
be equal to 1 so this is a joint
stochastic matrix and we have more
tapping matrix here and transpose here
what be
observe is here this is a word word
co-occurrence matrix or just empty so in
reality what we observe is containing
some degree of nuances like this so the
goal in this paper is to decompose what
we observed into this fashion okay so i
will explain why we are using
second-order method rather than the
first order method so the benefit which
is proven in aurora at all 2012
unfortunately the first order statistics
which is simple word occurrence in each
document is far from the ideal stuff but
fortunately the second order word word
co-occurrence matrix converges well to
the ideal word word co-occurrence this
is proved mathematically so the goal is
to decompose the newest observation of
our co-occurrence matrix into those be a
B transpose and what is called inference
here is to recover the word topping
matrix B and in order to do that we are
going to utilize something called
separuh bility assumption what I mean by
that is the typical non-negative matrix
factorization the goal is to minimize
the difference between C and B a B
transpose usually in terms of frobenius
norm but here just merely doing that
will not give us a great result and
usually it produces unrecognizable
topics it's because there's no identify
abilities guarantee in some sense this
is a probability assumption which I'm
going to explain in the next slide will
guarantee there was identify ability to
this problem so again just minimizing
see NBA
the difference between CNB a B transpose
is not the goal of this paper so what is
separable ax t is a probability
assumption each topic a has a specific
anchor word as K which is exclusive to
that topic what I mean by exclusive is
described in here so when that topic is
given we have both positive probability
to observe that anchor world whereas
given different topic there is no prob
ability to observe that specific word
this implies not every document about
topic a must contain that anchor word
but every document which contains that
anchor word will tells a list something
about that topic then what i mean by
anchor word defined in the previous
slide correspond to this red blocks here
so this red block is entirely dedicated
to the second topic and this intellij
decade to the first topic because there
is no probability of seeing this word
fifth word given another topic so we
have three anchors in this picture this
assumption is valid is something valid a
lot oh so in real data it's of course
not always valid but without this
assumption as i said the decomposition
doesn't have an identifiability so
usually all these on supervised learning
we would like to identify the mixture
different mixture topping mixture for
example in this problem that those
mixture are not sufficiently separated
to each other without this assumption
these will group some different
publix it why oh no this actually
separate different topics more
distinctively the Viper set assumption
doesn't hurt then it's possible that you
see the same ankle what gives several
different topics and those all those you
are different hobbies value me gustas oh
it's different cause topic is not
because ur top it is an hidden variable
so if we have this assumption what we
would like to do in the inference is to
create topic which satisfies this
assumption as much as possible as we can
and because these anchors are exclusive
to one topic it actually tries to learn
topic as separated as possible topic is
now we can observe yeah so if I reorder
these and push all these red blocks into
the above and reorder then it means B
matrix contains diagonal matrix in here
which is called d in this notation so
that basically this decomposition will
be re-written into this block diagonal
matrix form and there's already one
interesting correspondence so if i see
this first blog after the renumeration
then the da d transpose will correspond
to certain matrix of the co-occurrence
matrix so this is those types an
extension of the original topic model
the two branches oh is it at the same
type of model of the different women
it's not actually exactly same topping
model Kozma we no longer have the prior
for the word topic matrix part but it
actually subsumed some degree of those
all LGA different models so you will see
I want to open this something so can't
there be to anchor words in the same
column to anchor wars in the same column
in here yeah I know there is no way to
get there was two same anchors in the
same column cause and covered assumption
means if inserting rows there is only
one activated cell yeah and the number
of those rows is also hyper parameter
what I mean condition says if you have
to be the only non-white thing in your
road doesn't say anything about your
column so I don't see why you can't have
two red ones you could have a the last
row could be identical to the next to
last row oh so again this is not we
can't observe this is not we can observe
be so we what we can observe is only see
then as an user input for example in
this picture user oh I'd like to learn
topping model with topic 3 number of
topic 3 then we are going to decompose
this matrix assuming there is three
certain blocks in a ok yeah so it's just
another requirement for anchor word uh
it's not another requirement that's how
the inference goes in terms of the
directionality now you will see the
details go ahead so put it another way
is that me even if you have like to like
a world with thing ok just ignore what
just for me really just using one uncle
word that's enough ah no cuz if you'd
like to separate the data set into 10
different mixture you need 10 and coors
basically that's so because you are
assuming you have a diagonal which
it's a top yep which means for each
column the only I have one as well yeah
will permit is a is possible that for
some topical Muppets or some topics you
have more than one and go boy that is
not allowed in this model yep so every
topic must have only one anchor word so
in 2014 there is an extension
theoretical extension so every topic has
multiple different n koers I haven't
seen any real implementation of that
paper but there's an extension yeah so
if I have an anchor word this is the one
of the main inference how to find the C
bar IJ is a row normalized version of
word word co-occurrence matrix so now
it's conditional probability given
observing one word what's the
probability of observing another word
and because of this equation in the end
these relation will behold which means
so some row it rose in the row
normalized co-occurrence matrix will be
convex combination of certain rows
corresponding to anchor words and these
the coefficient the sum of this
coefficient will be equal to 1 and these
corresponding to the our topping matrix
that we've seen in the previous slide so
assuming we somehow know the anchor word
the rest of the inference is just to
figure out these coefficient so assume
we somehow learn those anchor word and
then to solve the previous equation is
just solve non-negative list clear its
simplex constraint there will be many
different method all right all use
exponentiate a great gradient and there
is also active
méthode and this is easily
parallelizable for each word that's one
of the biggest benefit and then the
inference becomes very simple Bayes rule
so this will be one entry in that B
matrix I k entry and once we know these
convex coefficient based on these method
the all these entry could be rewritten
based on Bayes rule so finding anchors
really matter so at the beginning a
rattle in 2012 they try to solve a lot
of LP basically pick one row in the
co-occurrence matrix and see whether it
could be reconstruct it could be
reconstructed by all the other rows
which is pretty exhaustive methode and
it empirically doesn't work at all and
then people developed or use QR with
rope avoiding which is very famous
method in that matrix algebra so pick
one extreme point in those row
normalized co-occurrence matrix as
initial anchor and projects every other
point rows down to the orthogonal
complement of that vector and choose the
farthest point and repeat this process
again and again until we find K and
curse of course these k anchors will
never recover the rest of the rows
perfectly and what we are doing is just
approximately find best k rose and this
is a greedy met though so it's not even
the best but something we could do in a
manageable fashion so benefits of this
encourage algorithm is as you could
imagine the every inference process is
deterministic so there is no random
initialization goes or yeah there is no
funky behavior at all
and so once we construct this noisy
co-occurrence matrix based on the real
data and then we no longer play with
these documents at all these are the
deal mists of Statistics then we need
for the inference and then we buy
produced an anchor word which is
exclusively dedicated to each topic
which might bring some interpretability
so what's the problem then this always
happen in machine learning so then real
data never follows the model so in
reality this co-occurrences with
railroads makes sparse row so if there
is a very rare Wars happens one or two
times through the document the
co-occurrence with that words is
extremely rare so which makes sparse row
but in terms of the matrix algebra those
rows look like very strange point or
eccentric point of that Corcoran space
secure with row pivoting algorithm
prefers to select those rows so anchors
are selected as two rare words and the
co-occurrence with those anchors become
noisy statistics and even worse the
co-occurrence exists between those
anchors is usually diagonally dominant
which means so one anchor word
correspond to one topic if this becomes
diagonally dominant we cannot capture
any interaction between topics this is a
serious problem so as a result all those
previous work they usually manually
crowd use manually crafted document
frequency cutoff which means oh they
just set some threshold like if in order
for this word to be an anchor word it
must happen a list in five different
document ten different document
sometimes 100 different document and
they measure the held out light clean
again while in order to measure the hell
Dell likelihood we need to finish the
inference process entirely and then oh
it doesn't look like it and said change
the threshold and measure he'll they'll
likely hood again it's extremely painful
process and even with the document
frequency cutoff and the Luren if the
number of topics is very small they
learn garbage topics and cannot capture
interaction between topics at all
because of this problem and even if K is
high the topic quality is poor so this
is discovered recently caused the
original two papers they all use the
synthetic data set which follows the
model pretty well but in reality in the
real data oh the topic quality is very
poor and comparing to probabilistic
inference like hip sampling the entire
inference quality is far inferior so i
will show you some sample topic ah
sample n koers with the original
algorithm which is called greedy here so
this is New York Times corpus message
which is popular in the scene and as you
could see none of the anchors are yeah
understandable and so we had a toy
experiment in previously MLP paper we
just compressed down this co-occurrence
matrix using either PCA or kissed a
casting neighborhood neighboring
embedding and we realized it gives us
much better and cord and even increasing
the held out light cleaved but still we
cannot explain well why this work that
explaining those is one of the purpose
of this new paper so I will show some
visual example of those anchored
so this is a 2d or 3d projection of
small Yelp academy corpus so these is
illustrating the corcoran word
co-occurrence base and the anchors
corresponding to the vert vertices of
the convex hull because as you might
remember the goal after getting anchored
the goal is to learn all those
coefficients to express all these words
by the convex combination of the anchors
and interestingly these n koers
corresponding to certain topic of the
anchor of the Yelp and we can also do 3d
projection while it looks messy there is
homeless and enchilada here so what
we've done in the new paper coming for
naps is we study extensively what's the
structure mathematical structure of
co-occurrence matrix e so while i
skipped and tally this has a lot of
probabilistic and statistics structure
you could see in the paper but would i'd
like to articulate in this presentation
is the geometric structure of the c so c
must be low rank and at the same time EE
non-negative the w non-negative matrix
is a category of matrix which is entry
wise non negative and positive
semi-definite so and by definition see
must be join stochastic which means the
sum of all entries is equal to one and
the lower rank is document is believed
to be generated based on small number of
topics so see you need to satisfy this
for different structure at the same time
on top of these probabilistic and
statistics structure so of obviously the
seam us to most satisfy a lot of
different condition at the same time and
this proof very rough proof YC must be
the positive semi-definite
I will skip it so seems to be this this
is and in order to to make C satisfy all
those condition we perform alternating
projection so projecting see down to the
low rank positive semi-definite space
first and then projecting down to the
cone of normalized matrices then cone of
the non-negative matrices and repeat
this process again and again so the
first projection is easily achievable by
the truncated eigenvalue decomposition
so rather than doing the full AG in
value decomposition which is clearly
painful if the vocabulary size is high
we just use for example by using power
method just get the first biggest k
eigenvalue and reconstruct see based on
that i invent and the projection
orthogonal projection to the cone of
normalized matrix is given by this
briefly the intuition is the sum must be
equal to 1 and the measure compute the
difference between the current sum and
the ideal some and get the average and
to revise average and do basically
penalize a reward based on that and the
non-negative matrix cone it's it's
pretty easy so I will the algorithm
alternate these three projection and
again and again until conversions
however this alternating projection
algorithm no longer guarantees the
convergence to the global optimum it's
because what where is the first two
cones normalized matrices and non-
matrices are convex cone but the
positive semi-definite with low rank is
no longer convex rare as simple positive
semi-definite is a convex cone
however ap instead enjoys local linear
conversions so there is a mathematical
proof in my paper but roughly speaking
only about the intuition so the set of
rank K matrices still forms of not bad
shape which is a smooth manifold and the
intersection with the convex cone and
smooth manifold is still smooth manifold
in almost everywhere sense and so so
long as the our estimator is not too far
from the convergence point it is
guaranteed to be converged and this is
one of the theory by Adrian Lewis 2009
paper so the rectified and chord
algorithm that be proposed in the paper
is consists of five to five different
procedures first construct the noisy but
unbiased estimator word word
co-occurrence and then rectify that by
using alternating projection so that C
will satisfy all the structure of the
idoc and then find the n koers in the
rectified co-occurrence recover the word
topic matrix by a probabilistic
inference with Bayes rule and recover
topic topping matrix a base down the
block matrix diagonal decomposition you
see in the earlier slide so it contains
pretty simple procedures and this all of
these procedures are deterministic and
that's of course clear benefit of this
method so
I will assure the result so how the
rectified co-occurrence looks like so
this is a two-dimensional visualization
of the co current space that this is
from the original algorithm by the way
the data set is nips data set and the
goal is to file find the five different
anchors of this data set so each dot
correspond to word and this is a word
word co-occurrence space and if I you
run the original greedy to our Withrow
pivoting it choose five different eggs
vertices like this and whereas if I
rectify the space like this and then if
I choose five different anchors on the
rectified space it corresponds despot to
these five points and if I map these
five points into the original space it
looks like this so as you could see the
coverage is much higher and it can
explain the rest of the world as a
convex combination better so this is a
typical topic result so this the data
set is nibs and again I have five
different topics so the original
algorithm Aurora at all if I run the
code based on his algorithm and their
algorithm this is the result topic so
everything looks like new your own layer
hidden and basically each topic agrees
with the Uni Graham distribution of the
corpus and in general in the word of the
topic modeling if the topic in firms
does not work well topic simply mimics
the Uni Graham distribution that's
typical behavior and if I run the gibbs
sampling
it gives pretty good topic one is about
the neuron cell topic or the control
theory reinforcement learning object
recognition or speech recognition neural
network stuff regualtion approximation
step and if we run the code in a
logarithm it's pretty similar to the
probabilistic lda even with this small
number of topic so if you actually
increase the number of topics
drastically like 200 then still in there
is a chance to cover all these vertices
eventually at the end so the topic
inference quality becomes better but if
the number of topics is not that large
enough there is a large amount of
chances that the original algorithm
fails because of selecting two eccentric
words rail wars which is not
statistically stable and this illustrate
the topic topic interaction which is the
second part of the inference so this is
coming from the original Aurora at all
methode so what they did is while I
haven't described after finding the word
topping matrix which is B they
multiplies the pseudo inverse of the be
to the left hand side and the right hand
side of the co-occurrence matrices of
course right inside the transpose of the
pseudo inverse matrix and then it gives
this result of course this is wrong
cause some entries are even negative and
there's no way for the probability
becomes negative and some entries even
beyond one the sum is close to one
that's that's because it's algebraic
properly satisfy that and this is
another method that we are using in our
paper by multiplying those diagonal sub
matrix to the left hand side
than the right-hand side of course that
is so simple met though then there is no
way that the original author didn't try
that method but actually if we try the
original that multiplying diagonal
matrix method to recover the topic topic
interaction without rectification it
will look like this so it's entirely
diagonally dominant because of the
reasons that I explained before again n
koers is likely to be selected as very
rare roars because of that the
co-occurrence between anchor and anchor
are extremely rare and statistically
it's not a good statistics at all and it
makes this diagonal matrices which
cannot capture the topic interaction at
all whereas this is the result from a
rectified and chord algorithm it
captures the topic interaction pretty
reasonably if you actually match one
topic with the previous topic in here
that we learned this values are these
values turn out to be reasonable and
this plot droves the overall quality yet
just one big image so we are not only
testing our method in the document
collection which are nips and New York
Times we can also do this model for the
movie and some data in the movie data
the word correspond to each movie and
document correspond to the collection of
movies that each user observed and in
the song data the song each song
correspond to word and the playlist and
it's broadcast station plays frequently
will be the document and then I we
currently have six different measure the
recovery indicates
how well those rows corresponding to n
koers reconstruct the other rest of the
rose and the approximation error is the
Frobenius norm difference between the
original co-occurrence matrices and the
decomposed vectorized matrices so
basically the approximation error is the
traditional measure to a traditional
metric to measure how successful the
matrix decomposition czar and the
dominancy is how those topic topic
matrix is diagonally dominant and
specificity is how much each topic is
specified from the uni gram distribution
and the similarity is how well each
topic is separated to each other and
clearance is a well which topic coincide
with the document so while this graph
may looks a little bit messy the the
thing that we have to focus is AP and
Gipps and the original baseline methode
so if you see the original base line
method for example in approximation the
approximation is pretty I but the AP or
our alternating projection method or the
gibbs sampling method the errors are
pretty low and those behavior agrees
over across different data set and also
in the recovery arrow as you already see
in the word word co-occurrence figure
after the alternating projection the
recovery rate is becomes far better and
if you see the another measure the AP
and gapes follows pretty similar
trajectory it's comparable to each other
which means we finally achieve the
probabilistic imperil result to the
probabilistic inference if you see the
original base line method they are all
far from there was probably stick
inference or AP method right a criterion
to judge which which topic center yeah
oh yeah that that's one of the question
which pops up always in talking modeling
so this is unsupervised clustering that
there's no clear way to judge which is
better so people use to introduce all
these different metrics like that I
suggested and also do the human
validation at the same time yes a few
little break yep so basically you see
this it's not that hard and some people
some researchers designed the matrix
messes up some on other metrics based on
this result how well how much each word
frequently occurs across different
topics like that but those are usually
subsumed in the metrics that I
illustrated in the plots so in other
task oh you know I saw that you can use
for st yeah perplexity perplexity izle
hell del like we're measuring yeah we
are we actually did that while I didn't
include in this paper yeah so it because
the reconstruction a recovery error
drastically goes down the hell they'll
likely increased allowed so the
conclusion
so we study the various mathematical
nature's of the co current statistics
and this might be exciting cause as you
as all of you know the word embedding
stuff they are all based on the word
co-occurrence while they are not coming
from this topic modeling assumptions but
of assuming there are some clusters of
words in the word code in the natural
language we there might be exciting
mathematical structures which is
desirable for a certain embedding of
course that will be becoming different
based on which task we are tackling and
we develop a principled way to rectify
the loose no easy co-occurrence rather
than exploring the documents the cut off
again and again exhaustively and based
on this method we can learn the quality
topics even if K is very small you
already seen k is equal to 5 example and
another example which is in our paper is
in the movie data k is equal to 15 and
if you run the original incurred
algorithm I think Pulp Fiction appears
across every every 15 topics as a top
top movie and the second it's I forgot
the name of that movie but those two
movies always toddlers across every
different topic while yeah what we've
learned has exciting cluster like lord
of the ring cluster and Star Wars
cluster and Walt Disney cluster and if
you actually run the gibbs sampling
method they gave pretty comparable
result and then we correctly learn the
topic interaction in a stable and
efficient manner and as I said we
achieve the comparable result so these
talk are based on these two papers which
I published
here and it's gonna coming soon in this
year and while I haven't prepared the
slide we are doing several exciting
extension in multiple different fashion
so as one of you might one of you might
already realize so the topic modeling
contains two different inferences one is
a word topic inference which is of
course the main inference so how which
topic is represented by the distribution
of words that part is included in this
algorithm but the secondary inference
which is what's the portion of topics
for each document that part is entirely
missing in this algorithm so that is
currently ongoing experiment and
interestingly the all the original
authors in Princeton like San llevara
wrong go and and on Kron we try and all
of the authors in Cornell are
collaborating each other all together
for those and another exciting with
extension will be like anchor topic in
for author topic inference so one so
rather than viewing all those thing as a
hidden variable let's say each author of
the document sometimes document has a
footprint of the authors for example all
these papers have a collection of author
and the another assumption which as one
more layer in the generative story is oh
this author has are interested in la
blah blah topics so author has a topic
distribution and based on that the
observed words are decided basically
some authors are interested more in the
topics for me I'm interested more in
probabilistic method
and then based on that the word that i'm
using frequently will be decided for
example like map estimation or bayesian
those crew is another ongoing extension
and an entirely different field like
privacy issue or so the basically the
co-occurrence matrix c is large if the
vocabulary size is just 10,000 it's
going to be 10,000 by 10,000 matrix and
save that in the memory is painful but
usually the natural language vocabulary
is 100,000 so how to store and do the
rectification step without explicitly
storing those all entries is exciting
question and even more I mean how to
store that efficiently and do the
rectification without violating the
privacy and those are all exciting
extension for and the future work yeah
so again wrapping up the presentation so
this is a méthode combat this is a new
inference combining the probabilistic
method and the spectral method and so
after forming the co-occurrence matrix
which is a second order moment matrix we
do we find the anchors on that matrix
and then based on that anchor verse all
the inference process is deterministic
and transparent and the exciting part I
think the you can take to your home and
for the future work so it is pretty
susceptible to the sample so if the
sample is not enough the estimation
result is pretty bad and also the model
mismatch
there's no intrinsic ability to handle
the model mismatch however if those are
solved the result from the method of the
moment it's easy to compute efficiently
and if we plug that result in to the
original probabilistic inference for
example get sampling needs a burning
process we need we no one knows how many
iteration we need to run at the
beginning to get a good result but if we
actually plug this result to as an
initial value to the gibbs sampling it
shows amazing result which even never
appeared in 100,000 of iteration which
is very exciting so usually all these
like lead fortune in the likely based
method there are no never convex and
there are a lot of modality inside up to
there and of course we cannot find the
good initialization point cause the
parameter parameter space stays high
dimensional and this is a really good
way to give good initialization so
combining those two methods seems highly
promising and other people's work there
are several other variation rather than
doing the second moment matrix some
people use third order tensor which is
word word word co-occurrence and those
tensor decomposition stuff is also
another way to do the topic inference
while they need to assume each topic is
independent to each other but yeah
that's another direction in which is
done by anand kumar yep so this is the
end of the talk
the Christians pretty short yeah I'm
finished in 30 minutes good yeah thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>