<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Revealing the Invisible | Coder Coacher - Coaching Coders</title><meta content="Revealing the Invisible - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Revealing the Invisible</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ey17mI3PS6w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so it's my pleasure to welcome and
introduce of fritto Jerome who's a
professor at MIT and he almost needs no
introduction he is one of the superstars
in graphics with an amazing number of
papers at SIGGRAPH and SIGGRAPH Asia in
recent years he's one of the founders of
the computational photography symposium
and conference he has really strong
influence in those areas and today he's
going to give us an overview I think
several different areas of his work
getting my team so welcome for you thank
you it's pleasure to be back and York
siz one of the few people who can
pronounce my name correctly so oh it's
good to be here
yes today I'm going to show you a few
overviews of recent work we've done and
then I'll spend more time on an area
that we're pretty excited about which is
to use computation to reveal things that
are hard to see with the naked eye but
before this let me show you a couple of
things we we've done in good old image
synthesis in compiler for image
processing and something about online
education and video lecture authoring
for a few words about a global
illumination it's still not a completely
solve problem simulating the interplay
of light within scenes is still
computationally extremely challenging
and anything we can do to make it more
efficient is is really needed and with a
number of co-authors from Finland a yaku
legend and being the lead one we came up
with this new technique that makes a
better use of the samples that that we
use to sample thy transport and we build
on this technique called metropolis
light transport that seeks to place
sample light rays proportional to the
light contribution it's an old idea by
Eric Veatch from a while back it's
pretty good at it the only problem is
that a lot of areas of a light space
contributes a lot to the image but that
kind of boring very so much going on and
so you shouldn't need that many samples
to to compute them efficiently in
particular Union it seemed like there's
this whole area is very
uniform and you shouldn't need that many
samples to compute it so in state what
Yakko and the guys came up with is this
idea of something according not to the
image contribution but according to the
contribution to the gradient of the
image and you see here this is our
sample density we really focus in these
areas where things matter so here I
visualize it in the space of the image
but under the hood everything happens in
the space of light paths so you know
light paths could be something that goes
from the light bounces of the screen on
the ground and then on you so it's in
this complex abstract space that we have
to sample according to the gradient of
the image and so whereas and so this
path space for a very simplified version
could be shown as this you know maybe
this is my receiving surface this is
where I want to compute light and this
is my light source so for each point
here I want to take into account the
contribution from all the points here so
abstractly I can show it as a 2d space
where this is my light these are my
pixels or my surface coordinates and in
regular metropolis you know because
there's a clear area this whole area of
light space doesn't contribute to the
image and this one contributes a lot so
with regular metropolis your samples
after your Markov chain process would be
distributed roughly like this and then
you just count the number of samples for
each column and you're done you get your
approximation of the image so instead
what we're doing is we're sampling this
space according to the finite difference
contribution to the gradient of the
image so instead of having one sample we
get like pairs of samples where we look
at the difference and we place this
proportional to the gradient so we get a
lot of these in this area of the
simplified light space and not so many
of these in this area and the cool thing
is that not only do we have a placement
of the samples that's more according to
where things actually happen but in
addition in areas like these all these
samples do tell us that there isn't
anything happening that the gradient
should be zero in all these areas so not
only do you get a better representation
of places where things change but you
also get more information
that you know even though you're
sampling distribution is low the
information that you have is pretty
strong and you know that you should
reconstruct something pretty smooth so
the basic idea is pretty simple it gets
fairly messy as you do it in the general
path space with you know path of
arbitrary lengths and etc and if you
don't pay attention to the math this
would be an example of some of the math
the main math you need to pay attention
to
instead of approximating ground truth
images like this you're gonna get a
result like that because some way of as
a Jacobian that creeps in and you're not
if you don't pay attention you won't be
computing the integral you think you're
computing and I'll refer you to the
paper if you like math integrals and
changes of coordinate and if you do the
right thing and take this term into
account you actually get the true
approximation and so this will be
presented at SIGGRAPH bye bye Yakko this
seminar now no transition because the
next topic is completely different
something have become very excited about
is the potentials of online education
and digital education in general and I'm
very interested in this style of video
lecture that you probably saw
popularized by the Khan Academy where
you see a virtual whiteboard lecture
where you see the writing going on as
the person is narrating what they're
doing a lot of people find these
compelling I won't get into the debate
of whether that's the best format but
certainly there are a lot of people who
wish we could generate content like this
and we don't have the talent of Sal Khan
to get it right and in one go because
the sad truth is that the authoring
software to do things like this or I
mean it's not that they're bad is that
they're non-existent I mean people just
take some screen capture software and
whatever drawing program they like and
they'll love people like Microsoft
Journal or Microsoft PowerPoint but the
bad news with these things is that you
have no editing capability if you get
anything wrong you essentially have to
restart your a lecture from scratch and
so I claim that this is very similar to
typing text on a typewriter so sure I
mean with a typewriter you can correct
your mistakes you can restart the page
from scratch or maybe you can scratch
fing and then write the correct word
later but it's really like the Stone Age
of authoring capabilities and we've all
grown accustomed to being able to edit
text in a very non sequential manner I
mean the order in which you're going to
type the letters really doesn't have to
be the final order of the text you want
to you know insert sentences delete some
correct some word maybe even reorganize
you know put this paragraph in front of
that other paragraph and so all these
modern tools really make the creative
process the authoring process very non
sequential and so because I was so
appalled by the state of the art of
tools out there I decided that I could
do better yeah and so I decided to
implement my own software and this is a
short video that I'll show you where it
can do authoring handwritten lectures
with current approaches is challenging
because you must get everything right in
one go it is hard to correct mistakes
content cannot be added in the middle
you must carefully plan how much space
you need and audio synchronization is
hard because writing tends to be slower
we present Pentimento which enables the
non-sequential authoring of handwritten
video lectures Pentimento relies on a
new sparse and structured representation
that builds on space-time strokes and
adds discrete temporal events for
dynamic effects such as color changes or
basic animation we also decouple audio
and visual time and use a simple
retaining structure based on discrete
correspondences interpolated linearly
this makes it easy to maintain and edit
synchronization let's look at an
authoring session with Pentimento we can
record strokes over time with the
standard pen interface the lecture area
is the white rectangle in the center if
we run out of space we can keep writing
in the red safety margin stop recording
and edit the layout using a familiar
vector graphics interface however our
edits are retroactive and affect the
stroke from its inception the lectures
temporal flow is preserved and the
equation looks as if it was written with
the correct size in the first place we
continue our derivation
but we decide that we went too fast and
that an extra step might help students
remove this equation down to make room
for the new step using another
retroactive edit we move the time slider
back to where we want to insert the new
content we press record and add the new
line we perform or layout refinement and
complete our demonstration in this
scenario we have focused on visuals
first and we now move on to recording
the audio we first make the audio and
visual time lines visible we proceed
piece by piece and selecting the visual
is the part that we want to narrate we
press the audio recording button and say
our text the audio gets automatically
synchronized with the selected visuals
we proceed to other visuals and record
the corresponding audio we can also
select in the audio timeline and record
our approach relies on discrete
synchronization constraints between the
audio and visual time which are
visualized as red ellipsis in the audio
timeline we can add constraints by
selecting the visuals moving the time
slider to the audio time where the
appropriate narration occurs and using
the timing menu or a keyboard shortcut
here we set the end of these visuals to
occur when this audio is heard we can
also drag constraints to change the
audio time or the visual time and the
visual timeline and the main view
reflect the change we can also delete
silence and the visuals get sped up
automatically once we have recorded the
audio we realize that the derivation
could be clearer if we replace the mean
meal by E of X we first make space for
the change we select the strokes we want
to replace and press the redraw button
we write Y of X and the timing of the
new content automatically conforms to
the old one and preserves audio
synchronization we also realize that
some of the narration doesn't have
corresponding illustrations we make
space and use draw to add visuals
without affecting the audio we derive a
fundamental identity for variance
variance is usually written Sigma square
it is defined as the expectation of the
square difference between X and its
expectation we can distribute the square
which gives us the X
of x squared minus 2x e of x plus e of X
square we use linearity and take
constants such as 2 and a of X outside
of the expectation we get e of X square
minus 2 e of X G of X plus G of X square
we clean up this a of X G of X and get a
of X square minus 2 e of X square plus G
of X square we now cancel one of the two
negative a of X square with a positive a
of x square and we get the final
equation variance is equal to e of x
square minus e of X square ray tracing
is a fundamental computer graphics
algorithm it allows us to go from a 3d
scene to an image the scene is
represented digitally for example a
sphere is encoded by the XYZ coordinates
of its center and its radius this is the
topic I tried to make one with I started
my drawing too big I didn't have space
it was a disaster that just didn't retry
until a viewing direction and a field of
view our goal is to compute the color of
each pixel the algorithm is as follows
for each pixel in the image we create a
ray from the viewpoint to the pixel then
for each geometric primitive in the
scene we compute the intersection
between the Ray and the primitive and we
only keep the intersection that is
closest to the eye once we have found
which primitive is visible at the pixel
we need to compute its color which is
called shady we take into account the
position of light sources and cast
additional raids this rule here was
right and then I did the visual based on
the audio head to Manta was gonna do it
in whatever order you want your lectures
on a variety of topics that include
probabilities barcodes Magellan's voyage
diffraction computational geometry and
many others
the executable is included in the
submission as well as a quick manual
thank you and so I'm hoping well I'm
having to spend a lot of my summer
debugging this thing and getting the UI
usable and hopefully by early fall I'll
be released free open-source blah blah
blah I use it for my class I've been
using in in lecture a number of extra
bits and pieces that I modify to make it
usable in lecture I've been enjoying it
I don't think the students I mean
enjoying it as much because teaching new
content with a new tool where you spend
your brainpower of thinking oh is it
gonna crash did I screw up this part of
the code it's maybe not the best idea
but but it's been kind of fun to to use
a Wacom tablet in the extra resin and
the blackboard or a small tablet PC
screen another completely different
topic maybe I'll try to go even faster
on this one because I think that
Jonathan is going to come to MSR to give
a talk soon and and he understands all
this a lot better but just as a some
advertisement for his talk so this is a
compiler that we created to get really
high performance image processing the
two people who really made it happen or
Jonathan Megan Kelly who's a grad
student finishing was me and Andrew
Adams who who was opposed to work with
me and is now at Google and the goal
really is to get high performance and
image processing and we all know that
these days you can't get good
performance without parallelism in that
parallelism is hard to achieve in both
the multi-core and the simile aspect are
really tough but equally important is to
achieve locality meaning that you want
your data to stay in the various cache
levels as much as possible and this is
equally if not more difficult to achieve
and the combination of these two makes
writing high performance code really
hard and usually you have to play with a
trade-off between a various aspects you
know locality and powers or more the two
big goals that you want to achieve and
very often the price you have to pay is
that you're going to need to do
redundant work and in image processing
that's
typically that you organize your
computation according to tiles so rather
than computing the whole image for each
stage of your computation you know stage
one whole image Stage two whole image
you're going to merge the stages and
computed tile by tile and the price you
have to pay so this maximizes locality
and parallelism but you usually have to
do redundant work at the boundary of the
tiles and usually we tend to think of
performance coming from a good interplay
between powerful hardware and good
algorithm and that these are the two
knobs that we have to make our
computation as fast as possible and for
most of us which are software people so
all we can do is write a good piece of
software but we think that it's useful
to split this notion of a program into
two sub notions one of them is the
algorithm itself and given an algorithm
given a set of automatic computations
that you want to achieve essentially
there's a big question of the
organization of this computation in
space and time and the best the best
choice will give you the best trade-offs
so what do I mean by the separation of
algorithm and organization of the
computation where we can start from
something very simple which the compiler
people have known and exploiting for a
long time just look at this very simple
two-stage blurrier so this is a three by
three box filter the first stage is a
blur in X double loop on the pixels
blowing X is just the average of three
neighbors actually it's the sum here but
it's the same and then we do a blur in Y
so you know this is one piece of program
that does this computation but I can
swap the order of these two loops the
4x4 Y it's still the same algorithm the
computation is just organized
differently but in this particular case
I think you get a 15 X speed-up it's
messing up my my slide so just because
you get much better locality by doing
the loops in the same order of things or
store so this is pretty well known that
the order of the loop can be changed and
most decent compilers will do it
but if you want to get high-performance
image-processing we want to take this
notion of separating the algorithm from
the organization a little bit further
because if you look at an actually
high-performance version of his 3x3 blur
it might look like something like this
so believe it or not the same algorithm
that we had before I still hidden here I
don't expect you to understand what's
going on here you've got some Cindy
stuff or pairs of loops I've turned in
two for loops because things are tile
they're going to maximize this locality
and the main message is that Avis code
is really ugly and how to maintain and
that the changes are pretty deep and
global it's not just that you're going
to optimize your inner loop and write it
in assembly and all that it's also that
you really are reorganized your
computation and that your whole code is
affected which in particular means that
it's very difficult to have a library
approach of this problem because the
library can optimize every single stage
of your pipeline and then you put them
together but for actually really fast
code you want optimization that goes
across stages of your pipeline and
libraries don't naturally do this and
this code by the way gives you another
order of magnitude speed-up so by
swapping your order of the loop we got a
factor of 15 and here we get another
factor of 11 so you know if your MATLAB
programmers who think that all you have
to do to get fast image processing is to
curl it in C++ well it depends if you
could this c++ yeah you'll get really
fast image processing but it's through
orders of magnitude faster than a naive
well a very very naive C++ the authoring
of the loop everybody should get this
right and so this whole reorganization
of computation is actually hard both
because the low-level mechanics of it
are difficult I mean again this is
pretty ugly code you need to change
things that many level of your pipeline
but it's also hard just at a high level
because you don't know what the good
strategy might be and if you're a
company like other
you really and I'm sure Microsoft has
people like this too you know you have
people who will spend months optimizing
one pipeline trying to paralyze it this
way trying tiles here maybe a global
computation there but you know it takes
them a month to come up with ones tried
to implement one strategy and so maybe
you're going to try a different strategy
if that one doesn't seem to be the best
one maybe if you have a lot of time
you're going to implement a third
strategy but by then you just have to
ship the product and you're going to
stop and so this is pretty high pretty
tough to come up with the best the best
option there's and so he lights answer a
compiler or languages answer is to
separate the notion of algorithm which
in practice we encode in a simple
functional formulation so here you've
got the blur exam no why just put it in
terms of the output of his blur X
function is this as a function of its
input then similarly blur why has this
expression and uses blur acts as input
it's very simple and this algorithm will
not change as you try to optimize its
organization and for this we have a cool
language where you have simple
instructions that allow you to specify
things like tiling parallelism SEM D and
things like this and I'll and and what
the schedule does is two things for each
pair of input and output function so for
example blur X and blur Y it specifies
the order in which you're going to
traverse the pixels for this function it
also specifies when its input should be
computed so blur Y needs blur X to be
computed so when are we going to compute
lurex
are we going to compute it all at once
for the whole image are we only going to
compute it for a small subset of pixels
around the pixel that we need and these
are the two big high-level decisions
that you have to make for each input
output pair and Jonathan will tell you a
lot more about this but
thing is that it's not just a random set
of small you know tricks to optimize
your thing you really get a nice
parameterization of the space of
trade-offs and the space of schedules
along various axes you can specify as
you know the granularity at which you
compute things on the one hand people
this and the granularity at which you
store things and we showed that all
these points in the space correspond to
different trade-offs that might be
valuable for various algorithms and
that's an animation and just to you know
give you a teaser so first of all this
is the C++ code I showed you before this
is the corresponding halide program
these two pieces of code have exactly
the same performance and by the way
halide is embedded in C++ so it's
reasonably easy to incorporate it into
your C++ program we use embedded it's an
embedded language and to give you a
sense of the kind of performance that we
that we can get Jonathan spent his
summer at the Derby last year and he
took one of the stages in the camera row
pipeline the one that that shadow
highlight clarity it's a local laplacian
filter algorithm that was developed by
Sylvain Paris and the Adobe version was
implemented by one of the really strong
programmers I mean to give you a sense
this guy's in a team that has only two
people it's him and Thomas Knoll the
inventor of Photoshop so he's really
good and he spent three months to
optimize his code his code was ten times
faster than the research code that he
started with but that took him 15
hundred lines of code just for this
stage of the pipeline
so Jonathan spent the summer there we
implemented this algorithm and optimized
it with halide and within one day he had
code there was 60 lines instead of 1500
lines and that run actually two times
faster than
the Adobe code that we started from and
I think that's even cooler is that our
language targets not only x86 but also
GPUs and ARM cores so you know in no
time just maybe you want to change the
schedule a little bit because the
trade-offs are not the same you can get
the GPU version yes
are you constrained by the layout you
know experience the layout of the input
data is not that critical we because in
prize fun very very much yeah I mean
disappointed I mean I had a master
student that I told you know go work on
the layout and you know let's also allow
people to optimize via layout and so
FAR's come back and saying I get no
performance gain we can talk about this
and you should talk about it with
Jonathan kind of the intuition is that
you have enough intermediate stages and
the prefetches and the cache systems
especially on the NEX 86 are so good
that as long as you do the granularity
of the computation itself right the
layout in the cache is going to end up
being right and things are going to work
out ok it was kind of surprising yeah I
was ya know I had to find another master
subject for his guys a little bit of a
surprise yeah yeah so go see Jonathan's
talk whenever he visits the language is
open through us at halide laying that or
something like this the documentation is
still you know it's it's a research
project we're hoping to create a bunch
of tutorials with Samir so that's a
little easier to to pick up and there's
a lot of enthusiasm about it at Adobe
and especially at Google well in
particular as a monster and who's the
guy who created Lightroom and Adobe and
now moved to Google is very excited
about it and has been contributing a
variety of things including a JavaScript
back-end which I don't completely
understand but he wanted to have fun but
he's also contributed a lot of exciting
stuff and we're still working on the
compiler and we're gonna make it more
and more useful hopefully
all right so now I come to the actual
chunk of my talk that's going to be
reasonably coherent and I want to tell
you about a whole research area that my
colleague bill Freeman and I are very
excited about which is to use
computation to reveal things that are
hard to see you is the naked eye I think
that in general this is a topic that has
been excited for centuries in science
and engineering and scientists have
developed lots of tools to go beyond the
limits of human vision you know starting
with telescopes microscopes and things
like this x-ray and if you're interested
in the area I have a keynote talk I gave
last year where I put together a lot of
these tools from outside computer
science for most of them and it was
quite exciting to to discover some of
them that I didn't know about so if you
want to look at it go go see my slides a
lot of them are really fun especially
like the stuff that takes phenomena that
are not visual in nature and make them
visible but the particular sub area I'm
going to talk about today is looking at
videos where apparently nothing is
happening but in fact you have a lot of
changes in motion now just below the
threshold of human vision so all these
pictures are actually videos and you
can't see anything moving but that
doesn't mean that there's no signal
there's nothing happening show me this
person is alive so he must be you know
breathing and his heart is beating and
you know you can't tell from this basic
video but we've developed techniques
that you can use to Optifine what's
going on here and reveal things like
these phenomena so here we reveal the
reddening of the face as blood flows
through it so with each heartbeat
there's a little more blood in the face
and get a little bit redder to give you
a sense that's if you have an 8-bit
image it's maybe half a value but we can
extract it and amplify it and show you
things like this even when your eyes are
still they have microseconds and my
tremors and we can amplify these
structures that look still are actually
swaying in the wind etc etc and so we
started embarking on this journey a
while back and 2005 we published this
work called motion magnification where
we took videos as input with some motion
really hard to see like this beam
structure here is bending a little bit
when the person is playing with the
swing and with all technique we were
able to take his very small motion and
amplify it and the way we did this is
use standard computer vision techniques
and image based rendering ideas we took
the video we used motion analysis we
analyzed feature points and actually DD
algorithm that surely you the main
author of his work developed to analyze
motion is quite sophisticated and quite
robust to things like occlusion and
given these trajectories and we do a
little bit of clustering to extract
different modes of motions and practice
we want to amplify this red segment and
we do various things like add vector the
motion vector further doing a little bit
of texture synthesis to fill the holes
and at the end we get these beautifully
magnified videos and we were quite
excited about these results but
unfortunately at the time the work
didn't have as much impact as we hoped
for partially because this technique was
quite costly we're talking about hours
of computation to get these results and
the algorithm was sophisticated enough
that if you didn't have it solely you
next to you to make it run and it was
really hard to use to the point where we
are whenever we wanted to compare to
this algorithm for a new work we've been
unable to rerun the old code as sad as
it might be and so this is partially why
we developed a new much simpler
technique which we call Alerian video
magnification and that was presented at
SIGGRAPH last summer so this is work is
a number of people the three main ones
who made
happen how you was a master student with
me at the time
Michael Rubinstein he's a superstar grad
student whom I believe Microsoft should
hire hopefully if you guys are smart and
Eugene she who was a former grad student
working at quanta research and then a
number of faculty members who gave
opinions and well no we actually I
should say this is a project where I
feel everyone in the list contributed at
least one equation actually did some
work so and in order to understand the
difference between this new work and the
old work we did a motion magnification
we need to borrow metaphors from the
fluid dynamics community and where they
make the very strong distinction between
Lagrangian approaches and oil aryan
approaches and what they mean by this is
that in the lagrangian perspective you
take a little piece of matter a little
atom of water and you follow it over
time as it travels through the medium in
contrast or Alerian fans just take one
block of space at the same location and
look at what are coming in and out in
this local position so this one is a
fixed frame this one looks at moving
frames and of course our previous work a
motion magnification was essentially a
Lagrangian approach where we would look
at each of these pieces of the scene and
see how they traveled through the image
and then we just make them travel
farther and in contrast the work I'm
about to introduce just looks at more or
less individual pixels individual screen
locations look at the color changes at
this location and amplifies them and the
basic idea is really really simple you
just look at the color value at each
pixel you consider it as a time series
so this is my time axis this is my
intensity axis of my red green or blue
axis I mean you know very standard time
domain signal processing I can do
whatever temporal filter I want
so typically we extract a range of
temporal frequencies so if you're
looking at the heartbeat
be round one hurts he will take you
Optifine these time frequencies for this
pixel and for all the other pixels in
the independently and then you just put
them back in your video when you're done
in practice it's a little more
sophisticated by than this not by a lot
actually I mean we just add a spatial
pyramid on top of this and that's kind
of about it and it's the really the
basic principle is just independent
temporal processing in each pixel a
little bit of spatial pooling to reduce
noise a little bit of pyramid processing
to be able to control which spatial
frequencies are amplified but that's
about it
and so I'll first show you how it can
amplify color changes which is
unsurprising since that's kind of what
we do maybe the more the more surprising
aspect that we really did not expect is
that it also amplifies spatial motion
yes but from the visual domain to get
the information or the signal is not
always decoupled from some noise and
some artifacts that may be due to some
other processes how would you go about
filtering them and getting the real the
child can move and you know The Fixer
difference could be because of that
rather than is not real well ask me
again at the end of the presentation if
I did an answer but the main and the
main thing we do about noise at least in
the first version of the technique is
just spatial averaging spatial low-pass
I'm not saying that you couldn't do
something smarter but that that's all
we're doing yeah all right so yeah as I
said the basic color amplification
technique is pretty simple you take your
your time series typically especially
for the heartbeat we do a pretty strong
low-pass on the image and I told you
that the amplitude is less than the
values so you need to average a number
of pixels before you get enough signal
compared to your noise but yeah you just
choose your frequency band in the time
domain and just amplify and you get
these cool results actually that that's
how the project started we were working
on a heart rate extraction something
similar to where the new Kinect has what
our colleagues at the Media Lab at devil
and we needed a debugging tool to
understand what we were analyzing so we
decided to just visualize these color
variation changes suctioning kind of
cool it works on regular video cameras
you don't need to do special acquisition
setups we were even able to play with
footage from the Batman movie and verify
that this guy has a pulse that you can
extract the heart rate so again we're
not the first ones to do this we believe
we do it better than other people but we
we did some validation at a local
hospital and at least four sleeping
babies our technique works as well as a
regular monitor which actually I was
surprised to discover that it's also the
fact that regular monitors are not that
good at extracting heart rate which was
surprising to me but anyway and so as I
said we mostly developed this technique
as a debugging tool for heart rate
extraction and when we looked at the
first videos we processed it was like
this is weird this person is moving a
lot compared to how much he was moving
in the original video what the hell is
going on and we really had no idea was
really an accidental discovery and so we
went to the blackboard and try to figure
that to figure out what happens and
that's what I'm going to explain in a
minute so you know the fact that we can
make things move in what seems to be
kind of a like Ranjan aspect that you
know these pixels are moving farther
away we need to study how local motion
relates to intensity changes in order to
understand why our simple technique
actually does amplify spatial motion and
so let's look at what happens at a pixel
when we have a translating object so
don't be confused my horizontal axis is
now spaced so this is the x-coordinate
in my image and the vertical axis is
still intensity so here I have a very
simple case where you know my intensity
profile happens to be a sine wave and
it's moving to the right this is the
next frame in blue right and I've got my
velocity DX over DT here so now we're
interested in how a single pixel changes
and nervous translation so this
particular pixel happens to become
brighter
this one is becoming darker so obviously
the intensity variation depends on the
location but we want to understand how
it relates to spatial motion and it's
kind of obvious if you're looking at
this diagram how this little horizontal
edge relates to this vertical intensity
difference you've got a triangle here
where the missing edge of the triangle
is actually the slope of my intensity so
it's essentially the image gradient and
so if I have an object translating with
this horizontal velocity the amount of
vertical intensity change is going to be
proportional to the slope of the
intensity the image gradient so if you
don't like diagrams and you're more of
an algebraic person when way of looking
at it is we're interested in the
temporal intensity derivative di over DT
and you can argue that di over DT is di
over DX times DX over DT so that's the
gradient that's the velocity and this is
something that's really well-known in
the optical flow community that's how
you know your lucas-kanade a your own
Schrank algorithm extract velocity given
intensity changes so of course in our
case we don't know this we couldn't know
this but we don't care about it all we
do is we take this intensity change
that's visualized vertically here and
make it even bigger so let's see what
happens when we do this we take this
intensity change and we magnify it and
we do the same at each pixel so in this
case here the intensity change is
negative and we make it even more
negative and you see in this image that
as we do this it looks like we
transported the sine ways the sine wave
further to the right and again if you
are not a bright person we made the eye
of a DT bigger by a factor alpha which
kind of if you have the same di over DX
the same image gradient suggest that
there was a DX over DT of velocity that
was bigger
and let me oh yeah it's yes so in the
paper we have the revisions that that
relate your spatial frequency is the
velocity amplification factor and we
also look how this compares to the
Lagrangian approach it's kind of
interesting because the the sensitive
the sensitivity to noise is not the same
so in some regime one is better in some
regime the other one is better let me
show you a quick demo so this is the
same well this is a Gaussian berm in
this case and you know it's moving
horizontally right but now what we are
studying is actually the vertical
changes so these are my very my
intensity changes and if I amplify them
vertically you see that it looks like my
gosh and mufasa
to the right and it's not a perfect
approximation I mean you see that we
overshoot here not surprisingly this is
the area where the second derivative of
my function is pretty high because
fundamentally this works only if my
local derivative model my di over DT di
over DX is valid and services when your
first order Taylor expansion is a valid
option but since we're interested in
very small motion that are impossible to
see to di this is precisely the regime
in which this matters and this kind of
explains I mean a number of people had
proposed to do temporal processing and
pixel values but nobody had apply it I'd
applied it to very tiny motion and this
is where this amplification of spatial
translation actually works should say
this is a visualization that was done in
undergrad Lille Sun and these kind of
visualizations are great summer projects
for a beginning undergrads
so I liked a little bit our processing
is not purely pixel based I mean as I
kind of said we first do a spatial
decomposition and we do the processing
independently on each scale of the
pyramid and some scales might not be
amplified because we know that the
approximation is not going to work and
so we can Optifine motion
is baby breathing which is Michael
Rubinstein's baby and you see that the
special motion is really amplified you
also see a little bit of the
overshooting when it gets too bright
here and this is the same thing as with
the Gaussian bump you might have seen
this one and you'll skip it I like this
one because it shows that you can do
different temporal processing to extract
different phenomena so this is our input
video it's a high speed 600 frames per
second guitar because we want to capture
the audio time frequencies and if you
are me fie frequencies between 72 and 92
Hertz you see the motion of the top
string and if you choose a different
frequency band 100 to 120 you see the
second stream because this one is an a
versus this one that was an E so you
have these degrees of freedom to choose
which temporal components you're going
to amplify well I think this video is
broken but I'll show it better later as
I said we did a study that compares the
Lagrangian versus the illyrian approach
I'm sure that least with a simple model
and for simple cases that you know
there's a regime where the Alerian is
better and one where the Lagrangian is
better yeah it's actually kind of cool
how some components of the noise get
canceled with the oil Arian because in
the Lagrangian version you're computing
velocity from pixel variation and then
you're creating pixel variation from
these motion vectors and so you end up
accumulating error along the way and
thanks to our colleagues at quanta
research we have a web version that
people can use and I've nothing in this
room it's as critical because we also
have MATLAB code and this is essentially
a one-hour project to reproduce but it's
been very useful for people who are not
computer scientists who have been able
to you to try out our technique for
their application so a number of people
have posted YouTube videos created with
our code as someone who has been using
it for pregnant women belly
visualization it's it gets a little
freaky and very alien like yes another
one
you see very well to use some video
stabilization because we amplify any
motion we see so you probably need to
remove camera motion somebody else has
used it to visualize the spatial flow of
blood in the face by using a color
grading after our process and it's not
real science if you don't have a guinea
pig and turns out some people did apply
the method and the guinea pig and so I
don't remember how he said it this is
the first oil aryan magnified a guinea
pig in the world and actually when my
colleagues who does biology in Stanford
is interested in using pretty much
something like this to look at the
breathing of some of our lab mice to see
what's going on with their cancer
research and to be able to tell earlier
whether something is an effect or not so
we've been pretty excited about
especially all the interest that we've
gotten from people in a lot of different
areas but we are still a little
frustrated with the amount of noise that
we end up for some of these videos
because of course you amplified pixel
variations it's not just the signal that
gets amplified and so in order to reduce
noise we came up with a new technique
that will be presented at SIGGRAPH this
summer and that was developed by Annie
Awad wah and Michael Rubinstein and
still in collaboration is Bill Freeman
and in order to understand the
difference with the old as in 1 year old
version you have to remember what I said
that essentially our layering in
perspective it works when you have a
first order Taylor expansion that's
valid and essentially it assumes that
locally the image has a linear intensity
with respect to space so if your image
is a linear ramp things work perfectly
unfortunately a noise gets amplified as
well as whatever linear ramp you
actually had and B especially because we
need to use multi scale processing with
the pyramid in practice the band of an
image pyramid doesn't look like a bunch
of linear ramps it looks more like a
bunch of local sine waves because that's
what been passed at so
and the kind of artifact that you see is
where this first other Taylor expansion
breaks and things like here where you
get really overshoot or undershoot same
thing we saw is the Gaussian and so we
created a new technique that instead of
assuming that images are locally linear
ramps they are locally sine waves which
is great if you want to do multi scale
processing and the good thing with sine
waves is that we know how to shift them
we have the wonderful shift phase
theorem that tells us that if your image
undergoes translation the only thing
that happens to your Fourier
representation in your sine wave
representation is a change in the phase
of your Fourier coefficient so we know
how to translate sine waves so all we
have to do is come up with local sine
waves so in practice we do use steerable
pyramids which are essentially you know
local wavelets that look a little bit
like this very similar to get Gabor
wavelet but the isotope of theable
pyramid that we use is actually
complex-valued so most people use image
pyramids that are real valued but we can
get complex valued pyramids that have
both an odd and even component to them
so just like your Fourier transform is
not a bunch of signs it's a bunch of
complex exponentials complex values
terrible PR I mean give you both a real
and an imaginary part which allows you
to get a local notion of phase which you
can then use for processing and for
amplifying local motion and they are
beautiful for you the main constructions
and obviously Yahoo we give some of the
details in the paper and you can look at
Josi man Shelly's webpage for more
information and so whereas the previous
approach used laplacian pyramid and just
did a linear amplification of the time
variation we use this terrible pyramid
that has both both a notion of scale and
a notion of orientation and that instead
of being real valued is complex valued
and we turn these complex numbers into
an amplitude and a phase and the
processing that we do is on the face so
we take the local variations of phase
and we Optifine
which really directly turns into
increasing the local spatial
translations and it works a lot better
than the old technique so in red is the
old technique so let me play it once we
have a Gaussian burn that's moving to
the right blue is the ground truth we
aren't increasing the motion Green is
our new approximation and red is the old
ones let me start it again the beginning
works fine the old one breaks pretty
quickly you see this very strong
overshoot or the new one that's better
longer until eventually things go a
little crazy especially when you get
phase wrap arounds but you're roughly
speaking that the new method tends to
work you know four times better meaning
you can use an application factor that's
four times higher and the second big
improvement is that it reacts much
better to noise so is the old technique
with noise you just amplify the noise
linearly at least the temporal component
that you've selected and so you get
crazy noise like this and you you so
effects like this in some of the videos
I've shown even in the baby one there
was a lot of a lot more noise with the
new technique we only modified the local
phase so we don't amplify noise
amplitude we just shift noise around and
so the local amplitude of noise stays
the same it just moves a lot more so
that the noise performance is way way
better and you should hopefully see this
and these results this is the old
version and this is the new version and
you see that noise performance is
significantly improved you don't get you
overshoot around the area of motion the
kind of artifact that you might get is a
little bit of wringing these are other
results as the old versions the new
version same thing here you see that the
nose performance is really dramatically
improved and by the way we try to apply
the noise into the old technique in some
cases it helps a lot but in some cases
it it actually hurts way more than than
it helps so that the new version is is
way more robust to noise and we can
amplify changes that are as tiny as the
the small refraction changes when you
have hot air around the candle
so in this case the small changes in
index of refraction due to a temperature
caused small shifts in the background
and we can visualize this and occupy it
and give you a sense of air flow and
we're currently working on techniques
that would be able to extract
quantitative information from this and
give you a velocity information so in
the SIGGRAPH paper I've got a lot more
information including how to play with
the space over completeness trade-off to
get a bigger range of motion we have
some ground truth comparisons to
physical phenomena that are ten times
bigger and we show that the technique
gives you something reasonable and so we
encourage you to go see the talk at
SIGGRAPH that Neil and Michael will give
and if you want to try it the webpage I
mentioned now has the new version and
I'll show you the beginning of a video
we created for the NSF that demonstrates
this she'll show you different pieces if
I can get yes to keep the intro mostly
I'll show it because the explanation
uses this drug pulsates through video
offerings the subtle breathing motions
of the baby
these changes are hidden in ordinary
videos we capture with our cameras and
smartphones
given an input video for each pixel we
analyzed the color variation over time
and amplify this variation which gives
us a magnified version of the video
so this is a case actually where because
my handwriting is terrible I first did a
version of this this mini lecture
actually I used a lot of resizing in
spatial layout as you can imagine and
then I asked one of my students that has
a much better handwriting to just you
know select the stuff and rewrite it and
because I have this redrawing tool
although the audio synchronizations was
preserved let me show you one of the
cool results like so this is a high
speed video of an eye that's static but
even when they're static our eyes move a
tiny little bit and we're hoping that
this might be useful to some doctors
when a person fixates at a point the eye
may move from subtle head motions or
from involuntary eye movements known as
microseconds such motions are very hard
to notice even in this close-up shot of
the eye but become apparent when
amplified 150 times and one final very
brief mention of something that will be
presented at CBP avi samara we have a
new technique with guha balakrishnan and
jong-gu tag that analyzes beads from
video but instead of using color
information we use motion information
I'll show you why that is videos we
demonstrate that it's possible to
analyze cardiac pulse from regular
videos by extracting the imperceptible
motions of the head caused by blood flow
recent work has enabled the extraction
of pulse from videos based on color
changes in the skin due to blood
circulation if you've seen someone blush
you know that pumping blood to the face
can produce a color change in contrast
our approach leverages that perhaps more
surprising effect the inflow of blood
doesn't just change the skin's color it
also causes the head to move this
movement is too small to be visible with
the naked eye but we can use video
amplification to reveal it believe it or
not we all move like bobbleheads at our
heart rate but in a much smaller
amplitude than this now you might wonder
what causes the head to move like this
at each cardiac cycle the hearts left
ventricle contracts and ejects blood at
a high speed to the aorta during the
cycle roughly 12 grams of blood flows to
the head from the aorta
by the carotid arteries on either side
of the neck it is this influx of blood
that generates a force on the head due
to Newton's third law the force of the
blood acting on the head equals the
force of the head acting on the blood
causing a reactionary cyclical head
movement to demonstrate this process we
created a toy model using a transparent
mannequin head with rubber tube stands
for simplified arteries instead of
pumping blood we will pump compressed
air provided by this air tank and I can
release the air using this valve now
watch what happens as I open and close
the valve once a second similar to a
normal heart rate ready here this motion
is fairly similar to the amplified
motion of real heads that we've seen
before we exploit this effect to develop
a technique that can analyze pulse and
regular videos of a person's head our
method takes an input video of a space
here in this most of the components of
standard units lucas-kanade ii tracking
a little bit of pca a little bit of
extraction the cool thing is that at the
end including gender they were able to
get not just their heart rate they also
get individualized location which gives
us it produces similar beat variations
this is a histogram of beat land
exciting result shows that we can
capture more than just an average I'm
told that this has diagnosed this
application don't ask me too much I'm
nothing like you belong down from the
back of a sub and unlike the color of
vs. we get hot straight from the back of
someone's head or in Halloween
situations so really the the thing I'd
liked I'd like to emphasize is I think
this whole area of reviewing invisible
thing using computational tools is very
rich and I think that in vision and
graphics we really have the right
intellectual tools to make a lot of
things happen and I encourage everyone
to the research in this area thank you
you're welcome to leave the we'll stick
around for questions answer questions I
sit where I work
so my question for the bus from head
motion is how much cost are you do
regular motion against how regular
regular is motion working yeah so I mean
this is the thing we're trying to test
you know how far we can go in people's
activity so if you're like running on a
treadmill it's not going to work typing
on the keyboard seems to be fine and
then we're trying to find where the
exact limit is so me actually the
biggest motion that we have to fight
against this breathing for for people
being static the second part my question
is how much prior information do you
need to get that cheats in like do that
you specify a band like you didn't know
you're there again the band is pretty
broad I think these days we just specify
if it's a if it's a newborn because we
have a heart rate is so much higher I
don't remember what band is using but I
think it's like point five Hertz to 2 or
3 Hertz something like this it's
reasonably broad yes so did you
accelerate your video amplification
language yeah no I've hired a student to
do this is we want to rerun the
real-time version on the mobile device
and so yeah we want them to do this the
face based version is a little more
tricky average partially your degrees of
freedom in which terrible P I mean you
use exactly and this probably it's
actually a more general issue for the
compiler where so far we've assumed that
the algorithm is fixed but we all know
that when you try to optimize your
processing you might decide oh you know
I'll use a cheaper version of the blur
in order to get the performance that I
want and and this is exactly the kind of
stuff that's going to happen I think was
the pyramids work have you done hundred
comparisons against the previous work
better or comparable yes so we've been
comparing with cholera and sometimes
it's better sometimes it's worse and
we're trying to come up with you know
the best of those it looks like the the
motion is less sensitive to noise
because as long as you have strong edges
that move you you really need a lot of
noise before you mess up the notion of
an edge but that at reasonable noise
levels it's less clear which one is
better
yeah yes that question could you also
like if it's the case that sometimes one
spiders
currently I have a student who's
supposed to be working on it I haven't
seen any result
you try this on emotions yeah that's
crazy stuff if I mean for like motions
trial
yes so the larger the motion the less
you can amplify the high spatial
frequencies so usually the way we get
away with motion that that's too big is
we just give up on the high frequencies
but then it's at some point you get so
low frequency that nothing much happens
the face based one can go a little
further retyping videos as well because
a lot of retirement dollars doesn't
optical forward-facing tracking yes I'm
interested in this course I'm interested
in this too
yeah we want to try absolutely yeah I'm
the face based method is very
interesting because it's halfway between
something like Ranji and then something
or Larry and I think there are lots of
things that you usually do with
advection that might be interesting to
do with this technique because it's more
direct and so I think as a result it it
will tend to be more robust
that's the
yes sounds like you've got some magic
experimentation with things that have to
be seen at all
have you looked at all and taking
something like the laser balance off
with the far away window and turn it
back into an audio signal have you
looked at doing that bit just the
visible light up with my high-speed
camera or window and I can do the audio
telling me I have both sides
yes we're interested in this we've made
some early experiments especially the
Lucia so there was a blueish membrane
that we have that's a rubber membrane we
started playing with you know
loudspeaker and trying to record the
motion and getting some audio back but
it's very preliminary at this point yeah
we were very interested in this
alright in the Sun and the stars
which can only be recorded by cameras
and so we detect that motion in Sun and
stars in cameras red signal is hopes so
have you tried magnifying that I guess
no we haven't tried that
ya know again we should try
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>