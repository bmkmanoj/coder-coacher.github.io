<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session: A Reduced-Dimension fMRI Shared Response Model | Coder Coacher - Coaching Coders</title><meta content="Oral Session: A Reduced-Dimension fMRI Shared Response Model - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session: A Reduced-Dimension fMRI Shared Response Model</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/n5Bc9qdlsn4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay it's my great pleasure to chair
this session and to introduce camera
chen he was going he's going to talk
about a shared dimension fMRI response
model Thanks hi this is cameron chen
from princeton university today i'll be
presenting our paper a reduced dimension
fMRI shared response model this is joint
work with janice chan yarra yarra yarra
hassan james XP and Peter Ramage so for
those of you not experts of fMRI here's
a one slight introduction to fMRI fMRI
stands for functional magnetic resonance
imaging fMRI data is four dimensional
three-dimensional distribution of
oxygenated blood in vauxhall space and
then over time so the time unit is
called key are generally ranges in few
seconds and the spatial unit is voxel
each FM right image typically is at the
order of tens of thousands voxels so
what's a problem we're trying to solve
here modern fMRI studies of human brain
use data from multiple subjects using
multi-subject data is critical in two
aspects first for a scientific reason we
want to learn how our brain work instead
of just how one single brain works
therefore whatever we have discovered
has to be generalizable to multiple
subjects second for a statistical reason
we can't gather enough data from one
single subject in order to increase
statistical sensitivity we need to use
data from multiple subjects however this
is not a trivial task a key challenge
for aggregating data across subjects is
to overcome enter subject variability in
anatomical structure and functional
topographies more importantly even under
ideal anatomical alignment we still have
problem differentiating functional
topographies across subjects so before
introducing how we are going to solve
this problem and equally important and
meaningful problem is how do we know
that we have done a good job in
aggregating data across subjects since
we don't know the ground truth of how
brain works
one way of doing this is to form it as a
machine learning problem so ideally if
we have a good way to aggregate data
across subjects we should be able to do
cross subjects experiments given data
from training subjects there are two
types of question we can ask first a
prediction question can we predict the
brain response of a test subject or a
classification problem given bring
response from a test subject can we
classify what's a stimulus in this
research will be using classification
approach as a way to evaluate the model
that we have developed what we would
like to do is to do classification by
using other subjects data as training
and classify and left health subjects
brain your response so the high level
idea of our approach is to learn subject
specific functional topographies why is
learning subject specific functional
topography is important because
anatomical difference can be adjusted
and there have been researches doing
this however even with perfect
anatomical alignment functional response
of an identical stimulus between
subjects are still very different we
also want to learn shared feature shirt
features are a time trajectory that
captured the shared dynamic across
subjects so if we are able to learn a
useful subject specific functional
topographies from these subjects voxel
space to share feature space then there
are two things we can do we can map all
subjects respawn from the voxel space
into shared feature space and then
conduct analysis in the shared feature
space or we can do cross subjects
mapping by taking subject one response
map it to shared feature space and the
map it to subject to tweak spawns so we
are going to utilize some very
interesting data sets for doing this we
use fMRI data collected while subjects
receiving stimulus so we specifically we
use temporarily synchronized
naturalistic stimuli for example two of
the data says we have our subjects
watching movie watching sherlock holmes
and indiana jones the other two datasets
are subject listening to audio listening
to a forrest gump an audiobook the
reason we are using temporarily
synchronized natural stimuli there are
two reasons using
naturalistic stimuli evoke a wide range
of response for the model too simple and
using temporarily synchronized stimuli
allow us to use time as anchor for
learning shared response so our model
falls into the big umbrella effector
model factor model is a very common
model in machine learning community
probably most of the people in this room
has seen this so let me use this as an
illustration to define some terminology
so the blue matrix is fMRI data which is
voxel by time the green matrix is voxel
by feature and the red matrix is
featured by time so one way of
interpreting this model is saying that
at a specific time point the fMRI
response that we observe can be viewed
as linear combinations of functional
topographies driven by a set of features
the columns of green matrix are
functional topographies and the column
of red matrix are featured what we are
interested is learning what is shared
across subjects so following similar
idea we first form join data matrix by
concatenating the data along the voxel
dimension there are uncertainty in the
data but there's also something that we
can control due to the usage of
temporarily synchronized stimulus we can
assume temporal consistency across
subjects to form the joint data matrix
so apply a factor model with the
composer joint data matrix into a large
viewing matrix and a red matrix so
following similar idea to interpret this
at a specific time point all the
subjects brain response can be viewed as
linear combinations of subject specific
functional topographies driven by a set
of shared feature and we design the
model with the expectation that the
feature will be shared so I'm going to
explain the mathematical model in a few
slides before that here's an
illustration of the generative approach
so motivated by the previous slide we
hypothesize there's something shared
which is the shared features in the
feature space and there are also subject
specific functional topographies with
the shared feature and subject specific
functional topographies we're able to
synthesize shared response and what the
model is doing is minimizing the
difference between the observation
and the synthesized shared response so
for those of you have been waiting
here's the math so the shared response
model is a latent variable model st
capture zshare elicited response at time
T X I T is the observation of subject I
at time t wi SE functional topography
for subject I and row I square as the
noise level for subject ice data xt
follow a prior distribution of
multivariate Gaussian and given st the
observation xit is also from a
multivariate Gaussian with the
corresponding mean and variance we also
impose an orthonormal constraint on the
matrix WI so that WI lives on Stevo
manifold so what's the reason of
imposing orthogonality constraint there
are two reasons first for robustness
using orthonormal columns of WI at least
robust result we have tried dropping the
constraint but at least you significant
decrease in performance second using
orthonormal WI it preserves temporal
geometry we do not want to shuffle data
across time however this is unsatisfying
from a neuroscience perspective however
because of the neuroscience
interpretability but this is a
mathematical model it works very well
but there's still room for improvement
leading to better neuroscience
interpretation so because of the nature
of skinny matrix WI is doing feature
identification with dimensionality
reduction at the same time so we have
this derives a constraint en algorithm
to estimate w on Stifel manifold we can
also use default standard c4 manifold
optimization approach but empirically
using constraint EMA algorithm is a few
times faster than standard Cecil
manifold optimization approach it's also
important to point out current state of
the art model for aggregating data
across subjects is called hyper
alignment which is proposed in 2011 it
is an analysis model different from our
approach a generative model the key
difference lies in whether we are
conducting the minimization step in the
feature space or the voxel space there's
actually a huge difference and the
detailed analysis are in the paper so
our main contributions is developed this
simple
model with a desired characteristic
leading to robust and state-of-the-art
performance we will demonstrate how this
model can be useful in various different
experiments so after fitting EML
constraint IM Algrim through this model
there are three main products that we
get the first is a set of shared
features and second is subject specific
functional topographies and third is
subject specific response the shirt
features can be used as template when we
when there's a new subject new subject
introduced to match up the new subject
with the existing set of subjects that
we have with rich stimulus data we
expect to learn subject specific
functional topographies that would be
generalizable to same subject but new
type of stimulus and with subject
specific response we are able to
decouple shared and individual response
so we evaluate our model with various
data sets the four datasets Sherlock
greater forest and audio book is
collected at with different mi machines
at different Institute's with different
subjects different processing protocols
at different brain regions and different
number of subjects number of actual
number of TRS so we first evaluate the
generalization to new subjects with
experiment timesegment matching what we
want to do is given a segment of
movie-watching response from a test
subject what we want to predict a time
point of the segment while using other
subjects data for training because we
designed the model so that it preserves
temporal geometry so that this
experiment is possible so the way we do
this is we left our subject m and we
take the first half of the movie
response from subject 12 subject n minus
1 to learn subject specific functional
topographies w 12 w n minus 1 and then
and the shared response which plays a
role as template and then we match up
the first half of the movie respond from
the subject m to the shared response and
learn subject specific functional
topographies for the left out subject m
then we take the second half half of the
movie response and transform the second
half of the movie response of all
subjects into the feature space using
the functional topographies learn while
using the first half of the movie data
so this is a result
the red bars are SRM the blue bars are
hyper lemon yellow bar I see a green bar
PCA and the left most bars are
anatomical alignment SRM l performs all
computer methods first why it does it
work better than hyper alignment there
are two key reasons first SRM is
adaptively aggregate eight aggregating
data based on the estimated noise level
four subjects with higher noise actually
plays last role in the estimated shared
feature and the low dimensional
representation is a way of the noising
then why I sorry I'm working better than
PC and I see a PC and I see it doesn't
have the notion of shared while what
they are looking for it's actually look
optimizing for variance or mutual
information it doesn't necessarily lead
to shared information the next
experiment we try to evaluate whether we
can use SRM to generalize to new subject
and very testing the stimulus so in this
experiment we use radar data sets which
consists of ten subjects watching movie
and watching a several images so we
first following similar approach using
the movie data to learn subject specific
functional topographies while lefting
out the testing subjects and then we put
we transform the image watching response
using the function of topographies learn
with the movie data and we conduct
classification trying to predict the
left I'll subjects stimulus while using
other subjects data for training so we
can see that SRM outperforms oh the
computer methods but the most
interesting part is SRM outperformed
within subject classification within a
subject classification is taking the
same subjects data for training and
doing testing on the same subject well
SRM is using other subjects data for
training and doing testing on the Left
I'll subject so why is there why as I'm
working better than within the subject
classification as a key reason is
because SRM is actually utilizing a in
this case a subjects data well within
subject classification is only using one
subjects data and this is a
demonstration that SRM is able to
effectively aggregating data across
subjects and to leverage the larger data
set so to my knowledge this is the first
time we're able to achieve higher
prediction of performance then within
subject classification
asian the final experiment what we are
trying to do is we are trying to
classify mental states we use audio both
datasets which consists of 40 subjects
listening two identical narrated story
we separate 40 subjects into two groups
and two groups receive different prior
context before they get into the MRI
machine and this leads to different
interpretation of the story so what
weird what I want to do is we want to
predict the prior context of the left
I'll subjects so we hypothesize that
there are three parts of the response
within the subject they are shared by
all response they are assured within
group response and there are individual
response directly taking a left I'll
subject and conduct classification we're
able to achieve predict above change
prediction accuracy indicating that the
information differentiating both group
it lies in the observation that we have
by applying SRM across both groups
together we identify shared by all
response and doing classification this
part leads to roughly chance level
prediction accuracy and this is expected
because SRM is estimating what is shared
and when we're identifying was shared
among both group it shouldn't be
informant informative and
differentiating between groups by
removing shared by all response what's
left as shared within shared within
group and individual these are
informative leads to above chance
accuracy and finally using a structure
way of applying SRM we are able to
remove shared by all response and the
visual response keeping only the shared
within group response and thus leads to
significant increase in prediction
performance suggesting that we SRM is
able can the couple shared and
individual respondents to tick to detect
through specific response so in
conclusion we have developed SRM which
is a very simple model and it has
achieve state-of-the-art performance
using multi-subject data demonstrate
higher sensitivity it also outperforms
within subject classification and lead
to low dimensional representation of
brain response SRM candy couple shared
an individual response and this allows
detection of groups
response and recent extension of SRM we
have been working on a kernel as version
to unlock information in higher order
statistics and we have also developed
information theoretic based SRM so thank
you all for listening i will be at
poster 23 tonight please visit thank you
thank you very much are there questions
okay thank you very much for presenting
of a new way of analysis so I have a
questions how do you set the featured
dimension and how stable were the
features for the different sets of a
stimulus see you used me that you used
and there was it possible to interpret
individual features after fitting yeah
so the first question is how do we set
the future dimension we use
cross-validation second question is how
stable are the features across different
stimulus so actually we are you for each
different data sets is completely
different subjects so not quite sure how
can we properly compare the functional
topographies between very different
subject and very different stimulus so
I'm not exactly sure the answer for that
if sorry was a last question up with the
interpretability yes so because of the
earth or support saganaki constrain its
I would say like we have been looking
into the brain map that we presented in
the and the voxel space some of the
feature are in trouble but most of them
are not and we are working on some some
way to tackle this problem thank you
very much Thanks ok I think in the
interest of time Thank You Cameron and
the next speaker can come up thank you
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>