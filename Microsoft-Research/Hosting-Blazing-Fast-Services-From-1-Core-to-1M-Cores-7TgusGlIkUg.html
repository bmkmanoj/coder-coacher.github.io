<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hosting Blazing Fast Services: From 1 Core to 1M Cores | Coder Coacher - Coaching Coders</title><meta content="Hosting Blazing Fast Services: From 1 Core to 1M Cores - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Hosting Blazing Fast Services: From 1 Core to 1M Cores</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7TgusGlIkUg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so as we all were all experienced enough
researchers to know that when someone
like me gets up and starts talking that
I'm actually presenting work that was
done by other people and I just talk so
it turns out if he's in the room if you
have any questions one of the students
who was on this project amar you might
direct all questions to him instead so
I'll start out with a pop quiz what's
that people in this room can't answer
that Oh wrong answer I updated my slides
just for this talk that is a Microsoft
data center in Dublin normally I show a
google data center because that was when
i made the slides but the microsoft
one's really pretty so it turns out that
when you build a data center in dublin
you get to run your air conditioners on
completely passive non chiller no
compressors mode ninety-five percent of
the time which is one of the reasons
other than taxes and politics and all
sorts of other considerations that cause
you to build a data center like this in
dublin because it saves you enormous
amounts of energy so over the years I've
come on a kind of this three leg model
of data centers and energy if you've got
a thousand watts of servers sitting in a
data center somewhere the first thing
you do is you stack a bunch of chillers
on top of it like we just saw in order
to make them not melt and you add UPS's
and power generators and things like
that so that they keep running when bad
things happen and all of that is just
overhead it's not going into the
computers that we know and love now in
the bad old days and those generators
and chillers and all that stuff might
have drawn something like what I have
there approximately a factor of two as
much as the computers themselves did so
the industry got together and it said
hey we need a metric for this will call
it pua and high pua is bad and it turns
out over the years these numbers have
come crashing down if you are talking to
the players who really know what they're
doing so Microsoft and Google and Yahoo
and Facebook these days have achieved pu
ease of like ten percent overhead 1.1
fifteen percent so they only spend about
fifteen percent of their power doing
that otherwise unnecessary junk now the
prob there's the problems not gone
because if you go down the street to
your friendly neighborhood
whittler financial corporation who does
not have the resources and clue of a
Microsoft their pu ease are still
embarrassing and someone ought to help
them do something about it but it's not
really a technical research problem at
this point it's a people problem the
second problem in power is
proportionality if you've got those
thousand watts of servers and you run
them at twenty percent of their peak
load they may draw fifty percent of
their peak power you look at menu say
well that seems a little bit wrong and
well yes it's wrong but it's an
engineering challenge my view on this is
people like Intel endelle and folks have
been making slow progress on this but
when when academics come along and say
well you know I've got a great solution
for you we'll just turn off the
computers when you're not using them
people at places like Microsoft and
Google kindly take to us academics aside
and they say we have to pay for those
computers its capex your color you're
telling me to waste my investment so if
there's anything revenue positive you
can do with those computers instead of
turning them off figure out what to do
with them and do it i don't care if it's
running steady at home or something like
that but do something instead of turning
off this million-dollar pile of junk so
i've been spending less time thinking
about proportionality on what we've been
spending with all of our brain cycles on
for the last couple of years is thinking
about energy efficiency why does it take
a thousand watts of energy to get done
the job we wanted to do in the first
place well can we do to make it take
less so there are many approaches to
solving this but there are two things
that we've looked at a lot the first one
is that gigahertz is not free if you
want to run faster it takes you more
energy per multiplication / addition /
basic operation this graph is a lie as
many such graphs are so take it with a
grain of salt and the y-axis is energy
efficiency higher is better this is the
number of instructions that you can
execute per Joule of energy the x-axis
is speed these are turtles these are
Lamborghinis and you'd love to be on the
very upper right of that plot you're
super efficient and you're super fast
and you'll note that no one's there the
reason it's a lie is I gave everybody an
ethernet interface that draws some power
so that these little 8-bit
microprocessor look like dogs because we
want to be programming dogs because
developers actually costs money too but
so the interesting thing about that is
there's a very definite sweet spot in
energy efficiency and you are all
carrying a device that operates in the
sweet spot of energy efficiency in your
product in your pocket 1.2 1.6 gigahertz
depending on how much cooler you are
than I am a gig of ram things like that
and there's it's not coincidence that
your cell phone operates right there so
that seems like a great way to save some
power will just run on slow processors
there's a second reason which is very
familiar to all of the database people
and all the architects in the room
things like the memory wall and the
increasing gap between our ability to
read things off of storage computers
keep getting faster and faster and
faster and faster which means that when
they have to go to memory if they're
really bored and so we start adding all
sorts of magic caching speculative
execution prefetching yada yada yada and
all of this stuff makes it so that we
are not crippled by the memory wall that
doesn't actually make our processors any
faster it just makes them consume a
little more power and then the second
problem is that transistors at their
heart they look like little capacitors
and they behave like little capacitors
and when you want to transistor to flip
you put a bunch of charge on one side of
it so that a bunch of charge carriers
diffuse into that channel and if you
want that channel to flip faster you put
more charge there so from a fundamental
transistor physics of perspective it
actually costs you more energy to flip
your transistors faster so you can
either be patient or you can be energy
efficient but you can't really be both
and so from our research perspective
we've come up with two very cheesy
please forgive me sounding pillars that
we look at for building energy efficient
systems the first is that gigahertz
hurts and the second is that remember
memory capacity costs you this actually
gets back I don't think you on this here
to Young's comment in the earlier
session about bigger and bigger and
bigger memory buses so if you want more
and more more and more memory on your
system you put bigger and bigger and
bigger buses those buses do you take
more energy to get stuff across them so
if you didn't pay attention to anything
I just said cell phones are really
efficient big really really really fast
server
are substantially less energy efficient
so we've been building these things and
that is and what we affectionately call
a fast array of wimpy nodes or a fawn
this is one of our more recent
generation ones I like I you know it
we're software people not hardware
people as you can probably tell so this
is a cluster of 88 nodes with 1.6
gigahertz dual-core Adams not that
dissimilar from a cell phone and a gig
of DRAM each but unlike your unlike your
pocket we give them very large amounts
of very fast flash memory and we've been
asking ourselves how do we build big
data intensive process and systems using
these fundamentally energy efficient
systems and what lessons do we learn
about the world and life from doing that
so I don't think I saw Kathy in the room
so I can freely misquote her at my
discretion so Kathy Alex been looking at
hpc for a long time and she observes
broadly speaking that every time you
threw a factor of 10 more processors at
hpc people they had to toss out a whole
lot of what they've done before and redo
it in order to extract even more
parallel ISM out of the code so that
cluster that I just showed you of 88
nodes if you're thinking about it if
they're like eight times wimpier than a
normal node you're probably running 10
times more of these things in order to
get the same amount of work done so what
I just told you about isn't free either
it is a way of trading massive amounts
of developer and algorithmic pain for
energy efficiency so whether or not you
actually like this approach in practice
is a very good question we can ask Vijay
if he wants to really build it into his
data centers or not the answer is
probably no and so I affectionately call
all of this the fon quad of pain so
because you have to build bigger
clusters you have to deal with more load
balancing you have to extract more
parallelism because you're using wimpy
nodes you have less memory capacity on
each node and you find yourself with
this burning desire to both avoid the
quirks of the hardware and take
advantage of any opportunities it offers
be that you know simdi units or other
thing anything that's anything you can
do to get a little more speed
eat out on the combination of all of
that makes my grad students absolutely
miserable and it's great because they do
the programming I don't i am but
unfortunately for all of us this is not
just masochism this is in some ways a
fundamental consequence of the hardware
trends that we've been seeing over the
last 10 years so this thing is our very
very familiar and well-loved Moore's Law
it continues we're continuing although
I'll biet at a slightly reduced rate and
to get a factor of two increase in
transistors per dollar every these days
about three years awesome but that's
Dennard scaling that was our ability to
drop the voltage as we shrunk the
process and that's stopped and the
consequence of that is gigahertz
increase has effectively stopped gone
are the days where we thought we would
have six gigahertz processors on our
desk it's kind of gone gloop because now
where everybody is trying to figure out
optimized designs that are a little
better here a little better there but
basically there's a pretty solid line
there that just we're not going to go up
to six so I argue that all systems are
going to face this challenge over time
and so one way for academics who don't
have the budgets to build clusters of
20,000 nodes to kind of get an edge
ahead and anticipate some of the pain is
to voluntarily embrace it by picking
weird quirky hardware and doing some
research to figure out how to actually
make it useful so we started this out
with a key value storage system key
value storage systems are ridiculously
popular these days if you look at
something like Facebook when you
assemble your main facebook page they
used to send my sequel queries to a
back-end my sequel to server this was
when Facebook was still quasi legally
running off of a server in a Harvard
dorm and that does not scale very well
so in order to improve that they stuck a
memory cache in between their big
servers and their big my sequel servers
that absorbed all of those hips so each
of those little sequel queries actually
gets executed as an individual key value
cray hey is the answer to this query in
the cache yes or no and it turns out you
don't just do one sequel query you do a
heck of a lot of sequel query
somewhere between 100 and a thousand
queries individual key value queries are
executed just to assemble your main
facebook page for you okay so these
things are very performance conscious
people care a lot about latency a lot
about throughput a lot about cost and
their random access and randomness and
computers not so hot very efficient for
sequential processing but when you start
to do randomness particularly with disks
but even with flash life starts to get a
little miserable so two thousand eight
ish we built this thing and this thing
did not use those shiny nodes I showed
you it used these 500 megahertz embedded
communication motherboards that we found
from some dude in Switzerland who was
manufacturing them and they had
extremely high tech for gigabyte compact
flash cards we thought we were pretty
cool and the goal was to build a system
that had substantially better energy
efficient performance in terms of
queries executed key value queries per
Joule of energy and it turns out that it
was hard it was a lot harder than we
expected and required a lot more systems
in algorithmic engineering and that has
been the story of our last five years so
wimpy GPUs limited amounts of DRAM as
you're probably all aware by now flash
is extremely poor at small random writes
it's very good at bullocks sequential
writes it's quite good at small random
reads but it's absolutely atrocious if
you want to modify a bit here and modify
a bit there and of course when you build
a big cluster out of these little
Wimpy's you'd like to make sure that
life doesn't go to you know where when
nodes come in depart so as one example
of the kind of solutions we had to to
create for this or adapt given that log
structured file systems were known for
20 years the way we chose to avoid
random writes was we took a
straightforward log structured approach
so we had an in flash data region we had
an indie Ram hash table and whenever
we'd put some data in there we would
just stick it in the end of the data
region and update an entry in the hash
table telling us hey key k1 is stored at
this particular location on disk so that
we could read it randomly but all of our
rights were sequential
and we looked at that and we said this
is really cool except we've only got 256
megabytes of memory on these nodes to
store that index and we're competing
with say the operating system for some
of that 256 megabytes and so we came up
with a new hash table design we called
partial key hashing and the idea took
advantage of the properties of flash we
basically created a hash table that
could be wrong but it wasn't wrong too
often and if it was wrong we just had to
do another read off of flash so one in a
thousand times we had to do two reads
but most of the time we sacrifice and we
sacrificed that small probability of
being wrong for saving a lot of memory
in the deer am based hash table and then
it turns out we've continued this line
of research over about three or four
iterations now where we came up with a
cuckoo hashing variant of it that saved
even more memory and then of course you
know this is the days of parallelism and
concurrency so we also developed a
concurrent version of that actually a
lock free optimistic concurrent version
of that that we published just last or
just this year i should say 2013 um so
it worked in 2008 it was 6x more
efficient than a comparable conventional
approach if you did this again these
days modern CPUs have become more energy
efficient I don't think you'd see six I
think you'd see two to three times more
energy efficient from using a really
efficient processor versus a
conventional approach and we were
actually able to use it to create you
know fairly good high-performance
reasonably predictable key value storage
services and then time did what it does
and it moved on we move to atoms and
solid-state ramada inside with
solid-state drives and if you look these
are the improvement numbers in CPU in
memory and in both the speed and
capacity of the flash drive and one of
those numbers is very different from the
others so while we got six to eight x
faster and six to eight X more memory
flash drives took this amazing leap
during that time period and they got
between 30 and 60 times faster and
bigger and so we thought we had been
extremely clever in our memory efficient
partial key hashing design
and this forced us to throw it all out
and go back and redesign something that
used one-eighth as much memory as our
previous design in fact our new design
didn't even have enough memory to store
a pointer in memory and to the thing
where the thing was on flash so the
story thus far when we started out with
fond yes and fond kv we moved forward to
atoms we designed the system called
silted SOS p and in order to do that
that very memory efficient thing we land
somewhere that as a systems person I
would never ever ever have expected to
land we published a paper at Allen X the
algorithm engineering and
experimentation conference and I am so
not qualified to do that it's not even
funny but I've got really smart students
um so he on tagged came up with a scheme
for external hashing right external use
a little bit of memory figure out where
to read the thing on expensive external
memory that turns out to be the most
memory efficient practical external
hashing scheme known to humankind and I
recently heard from conversations with
some folks at MSR Asia that Bing
actually is using not for external but
what is the currently known best
internal memory efficient hashing scheme
known to humankind a system a scheme
called CHD for some of their indexes and
so there's an enormous amount of
opportunity for these kinds of advanced
very but practical focused algorithmics
that are kind of at the cutting edge of
conferences like Alan X to really have
an impact on Big Data Systems these days
in a way that I'm not sure I appreciate
it until we got our feet wet on it so I
told you we were able to build well
performing systems using these Wimpy's I
lied again I I seem to lie a lot i'm
sorry on the way I lied is of course
when you have a system like this with
little atom CPUs and solid-state drives
you obviously cannot handle your entire
workload using one of them so you put a
bunch of them together and as I said we
have 88 of these things at the moment
and you use standard consistent hashing
techniques hash key figure out which
back end it goes to send the query to
the back end send it back to the user so
this is great
and you think how does my systems
performance scale as I add more and more
back end nodes and the answer is well it
depends so our goal we'd love if each of
those things can handle 10,000 queries
per second right the system should
provide an aggregate throughput of
880,000 queries per second we can scale
that back right sles are important in
these systems we could say let's get an
SLA of 850,000 queries per second right
so the red line is the scaling
performance as we increase the number of
nodes this is throughput if the workload
is completely uniform this isn't even
uniform random this is round-robin
uniform among keys we know are designed
to hit all of the backends so if you
want to lie you can just show that graph
and say my system scales perfectly
unless you expose it to real-world
quartz a real-world query might look
something more like the blue line which
is a Zipf distribution heavy-tailed so
on a heavy tailed distribution this
cluster of hours achieves something like
one tenth or one twentieth the
performance of what we would like it to
and if you're if you've got an adversary
generating your queries in this case
it's the lady gaga work 'let all queries
go to lady gaga no queries go to Dave
Anderson which is as it should be but
the through point is exactly what you'd
expect it's the throughput of one note
because all of those queries are going
to one wimpy backend so our system had
this extreme weakness under not even
horrible like real workloads and so we
looked at that we said wow that's that's
really unfortunate is that the end of
the world of the line for the fon
research and it turns out the answer is
well it was a little bit and because we
had to bring in some actual fast
high-powered hardware so we added a
cache to the front end that was doing
the load balancing and the big question
is well how big does the cache have to
be if you've got to cash all the items
in your back-end it's not going to be
very useful but it turns out there's
this really cool duality between load
balance or load imbalance and cash
ability if all of the queries are going
to lady gaga your crash has to have one
item and you've slammed everything out
of your cash if the adversary has
randomly distributed
queries over everything well your cash
is useless no matter how big it is but
that's okay because the workload can be
handled because it's perfectly uniform
and interns that the answer is 0 of n
log n the actual answer is about six and
log n where n is the number of back ends
so if you've got a hundred backends you
actually only need to store a couple of
thousand items in your front-end cash
and you can guarantee basically perfect
load balance or you can guarantee that
the load is so imbalanced that your fast
cash is handling everything and with
only a couple thousand items in your
cash you can fit that cash in l3 cache
on a big beefy microprocessor and so
where we ended up was with a system that
used one big beefy in the way that a big
beefy works best it's not even touching
DRAM and a whole bunch of Wimpy's who
are the ones who are sitting there going
okay slow flash please satisfy my data
requests and once you do that life is
really good so the blue line is then the
zip distribution and you can't really
tell the difference between perfect and
adversarial because they're basically
overlaid on top of each other
adversarial it's just a little bit worse
because the worst case is you do it
randomly and our perfect case was as I
said round-robin and basically the worst
case becomes the best case just with a
small cash have I exhausted you off okay
so I am where Catherine please
how do we prove the number of items so
remember I'm faculty mice I don't do the
work so the answer is there are actually
two things there it's just my name on it
so there there are two aspects to it we
we did it as a competitive analysis and
basically we said adversarial analysis
so we did it at the same time figured
out what is the worst ever sir or the
best adversarial strategy and what is
the performance of that and what is the
bound and it turns out that and the
adversarial behavior is the thing that
picks Lady Gaga and the power of that
ends actually at about less than n log n
because at some point you start forcing
the adversary to distribute the queries
over a suitably large enough but the
thing that actually causes the N log n
is standard balls and bins cashing if
you're just doing random balls and bins
into these backends um then you're going
to heaven n log in or a login / log log
in imbalance and that's the thing that
actually causes the problem and so we
end up having to take n log in in order
to compensate for the imbalance due to
just standard hashing so the adversarial
part dropped out early enough that it
was it was tractable
correct so now we can talk about how to
add multiple front end caches which we
haven't quite figured out and there are
a couple of really fun questions about
extending this like if you have better
load balancing schemes like if you
duplicate all of your items in the back
ends once I think we can do more to
actually reduce that to actually n log n
over log log in but it's like a factor
of three savings so I I can't bring
myself to care too much ok so where I
just took you on that whirlwind of the
last five years of my life is that we
ended up in the system that looked kind
of like this we've got this silt thing
in the back end using a very memory
efficient index to figure out where
stuff is on disk here's my third lie of
the day the algorithms that we developed
for silt are not fast enough to run well
on the Wimpy's and so we're still trying
to overcome some cpu bottlenecks in
front of silt we've got an insanely fast
cash where we prove the size of the
cache necessary to guarantee load
balance so that we're able to have a
system that can create an SLA equal to
the scaling of the number of wimpy
backends as random fallouts of this um
we ended up inventing the first multi
reader parallel cuckoo hashing scheme
that preserves the memory efficiency of
cuckoo hashing there's actually one from
Microsoft that was earlier than the air
but we use less memory and we had to
come up with this this Alan X thing for
the best known practical external
perfect hashing so this has been a an
absolutely wild ride for us and it's all
the fault of the architecture people so
if i had one takeaway it's that as
systems and database people and things
like that there is an enormous amount of
value in talking to these architects
early on and saying so what is it that
you people are going to be doing to us
in 10 or 15 years and how can i how can
I figure out how to anticipate and solve
those problems ahead of time and the
drawback is that the problems cut across
every part of computer science and under
humankind it is architectures it is
algorithms at systems this is some of
the hardest
programming that I and my students have
had to do and we would love more help
from that and we're just starting to
bring in machine learning but incredible
fun stuff questions I tired you all out
I'm done there's a remote audience who
wants to hear you so I think you'll
focus on the storage Ohio intensive
workloads so from the key value and the
later were closed so have you tried a
little bit more applications in the data
center and see how your how the phone
nodes can work with other big servers to
to a for example so changing old
video-on-demand system something like
that how can work in a real data center
we've looked at a couple of other
applications so the the Corwin's we've
looked at is we just use search as a
proxy for a large number of applications
and it turns out searches one where
there's a really fascinating kind of
pattern of interaction where if all else
is equal wimpy nodes are the same factor
of two to three more energy efficient
for doing search except that the
efficiency lines cross radically when
workloads drop out of say the l2 cache
in the wimpy but they're still fitting
in the l3 cache in the beefy and then
there's another drop when you leave the
main memory of the wimpy and have to
involve a network but you can still fit
in that terabyte of memory we were
talking about buying in a single server
and so there are these stair step
functions that meet that make the
analysis really complex for real
workloads because it depends on where
you fit in that space we also saw we
also looked at the problem of pattern
multiple pattern matching as you might
see in things like virus scanning or
intrusion detection and prevention and
that was another one where we were able
to craft a solution for the Wimpy's that
was substantially better but to do so we
also had to go back
to the algorithmic drawing board because
the conventional multi pattern matching
solutions used too much memory to fit in
the Wimpy's so once we got things to fit
in memory it was fine but we actually
locked up our cluster by running every
node out of memory once trying to run
grep so the bottom line is there's a lot
of efficiency to be had but almost none
of it is free and it's taken anything
from recompiling to completely riorca
tackling the software systems so we know
each other also you know it's a moving
target it isn't it isn't moving target
how does that you get to the point where
now you're not as efficient given oh
yeah we've looked at that what that
point is well sorry which at the point
at which what is not as efficient and
essentially to keep your going after
trying to conserve energy used or energy
with the lower processors but you know
as a as for example next item a vehicle
which is going to mention that but we
always have the generations of service
that we can improve or any of the height
in servers that we move them back they
become very very powerful they're
probably much more powerful than the
wimpy notes that you have Oh although
you might be surprised right a cell
phone is an incredibly capable device
but I think the answer for the long term
is that in the long term what I've been
doing may become irrelevant but
hopefully we'll have created solutions
that work in the weird world that we're
going to end up in the long term because
right for a while when gigahertz was
doing that there was a very easy
definition of wimpy it was like you know
cut the gigahertz by a factor of two and
cut the issue by a factor of two and you
just track it but now with the gigahertz
leveling off and we're seeing this very
slow increase in things like the
gigahertz and the capability of cell
phones and as that creeps up I think the
big difference will be things like cache
sizes and core counts and as soon as the
the difference between a wimpy and beefy
is a cache size in the core count and
it's less about the gigahertz then you
have actually lost that fundamental
gigahertz based physics reason for the
energy efficiency but you've also got
you know 48 core cpus that you have to
program and so everybody else has
embraced our pain anyway so okay our
next speaker is Ethel Mahajan Ethel is a
senior researcher in the ability
networking group in micro research and
he has he graduated from University of
Washington and has won several awards is
a very young career and he's essentially
going to talk a little bit about
utilization of the bandwidth between
data centers so and in st ends and
software-defined networks and things
like that so very topical Thank You
Victor so in both Dave's and VJs talk we
heard a focus on efficiency and this
focus on efficiency is actually not
accidental it's not accidental because
any we are reaching a point where
competitive competitiveness as a company
or anybody depends on how efficiently we
can operate our entire infrastructure
and this project that I'm going to talk
about is one example of what kinds of
things we can do in driving efficiency
and what kind of clean slate and
complete completely new systems we can
build in trying to get there by the way
this is this is kind of large project
we've been doing for the last couple of
years it involves a lot of people
including some interns were visiting ER
like chia from you and visiting
researchers and of course like Vijay and
Ming are also sitting here and there so
what's the problem so the piece of the
infrastructure I'm going to focus on is
what we call the inter DC wyd rio
networks so this is the network that
connects basically our five nines data
centers so are really big data centers
it's a global footprint and we are
talking about essentially capacity along
each of those links running anywhere
between 100 gigabits per second to
terabits per seconds worth so you can
imagine this infrastructure is highly
critical for services it's highly
critical because performance offer cloud
services depends on it but at the same
time the reliability depends on it
a customer right has to be replicated to
another data center so this network is
used both for user facing traffic as
well as other traffic in there because
these pipes are so thick and they run
across continents and across oceans this
whole infrastructure is really really
expensive which I can give you the exact
numbers but it runs easily into like
hundreds of millions of dollars
amortized card costs annually in there
so we have this so what's the problem
this is the problem the problem is that
the current technology stack we have for
operating these data set into DC
networks it's highly inefficient and i'm
not talking about efficiency of eighty
and ninety percent this is actually even
the busiest of links on this network go
under forty percent average utilization
and our most links are actually even
don't even talk about it like ten to
twenty twenty percent or so forth so
we've invested you know basically
hundreds of millions of dollars and we
are not able to use it at all because
given the technology stack so why is
this happening so one reason it is
happening is that the way these things
are by the way this is not a Microsoft
centric talk talk to anybody who
operates a big IP backbone and they'll
tell you that efficiency is a big
problem average utilization on the
network is a big problem so this is
basically a problem with the technology
stack itself not in the way we run it so
why is this happening so one reason why
this is happening is that in current
technology stack services that use the
network they can send traffic whenever
they want wherever they want however
much they want so basically you as a
service let's say a search service or
replication service you send whenever
you feel like you know it's a good time
of the day the result of that is that
the network as a whole goes through
periods of peak utilization and really
low utilization so this graph is
essentially normalized usual ization are
one of our major data centers and what
you see there is that you basically peek
around something this is by the way a
day long graph on the x-axis so you peek
around some point and then you
essentially are less than me less than
less than a factor of two for most of
the part so what this means is that
because people are sending wherever they
want to send we have to provision for
peak capacity we cannot because we
cannot drop traffic so now you might say
like what can you do about this so this
thing is actually not fundamental
so when I talked about like our services
it turns out that all traffic is not
created equal so we have traffic that
you might in simplistic terms are call
background traffic so this is like Rep
large replication jobs and at the same
time we have traffic that you might term
as non background traffic which is like
interactive traffic or traffic with SLA
on the order of seconds or minutes and
so forth so as it turns out you have a
mix of bagging on and on background
traffic and overall this emerges but if
you had the ability to adapt your
background traffic you can actually
reduce the peak to mean ratio as which
you can basically now use the spare
capacity for sending more traffic or you
could actually provision even provision
thinner pipes in there so that's where
we kind of had it so we have the ability
because we control our systems end to
end this offers us a unique opportunity
to try and adapt this traffic to the
demands of non background traffic here's
the second reason these things are
working at all here's the second reason
and the second reason is that the way
resource allocation happens in these
networks today it's local and is greedy
so give you an example if a flow comes
from A to E you figure out a path that
you can route it on so this is mpls for
people who know what I'm talking about
so you know you get a path from let's
say a flow wants to go from A to E you
figure out the shortest path in the
network that can accommodate its traffic
so you pick that one if nother one comes
in basically C to D because the link C
to D is taken what it will do is
basically take a highly local view of
the network and then it will find the
shortest path that is left over if
another flow comes in from C to G what
will happen is that it cannot find it's
because C to G direct link is taken it
will find a roundabout way so in these
numbers we can run both like simulations
as well as do some theory and we figure
out we actually lose like over twenty
percent of our utilization because of
these weird effects of local greedy
allocation but thing is like V
controller infrastructure which is this
unique ability we have and if we had a
global view we could actually do a much
better job at allocating how traffic
flow is routed so that's an example of
it's a much better flow which frees up
actually not only the paths are shorter
it has also freed up a lot of capacity
on the links that did not exist before
so with those two observations
essentially the goal was to build are
what we call Swan it's a software driven
van our goal is to build highly
efficient and we are putting numbers are
they in this by high efficiency I mean
that we should be able to run the
network with an average utilization that
goes beyond 95% so we are talking
average across peaks and valleys across
the day ninety-five percent that's why
we like to go so it turns out right like
getting high efficiency in terms of peer
utilization is trivial we have lots of
services that could eat up the network
in no time the trick here in this design
tends to be can you actually achieve
efficiency while retaining flexibility
and what we mean by flexibility here is
that traffic that is high priority
should be delivered first and traffic
that is low priority should be delivered
later and so what we do here is
basically take take a basically a survey
of the kinds of traffic we have and
partition then mark Streit priority
classes and then we do max maximum
fairness within a within a class so
these two key design elements that are
part of one should not surprise you they
are basically squarely targeted at the
reasons behind inefficiency that we see
today so we are going to coordinate the
sending rates of services and we are
going to do a global central resource
allocation so how many people here know
about as the ends okay so about thirty
percent so I'm going to do a quick in
case you don't know what as jeans are as
unit is this kind of newish sort of a
paradigm in networking stands for
software-defined networking the way
networks are built today is that we take
these big beefy routers from cisco yeah
yeah yeah yeah
yeah so you can do I think you can do
you repeat the question because yeah so
the question is if you have a historical
profile of our services or no generating
traffic can be account for the
historical profile and set aside
capacity yes I think with so on we will
be able to do that and and basically
that whole mechanism is called
calendaring so we can provision
resources ahead of time yeah no no yeah
okay so sorry so the network's today are
built using you take big beefy Cisco's
and junipers and what happens is that
these routers are not only very
expensive the control plane control by
control pain i mean the bits of code
that computes how flow should be routed
it sits on the routers themselves so we
as a service provider we don't control
it it sits right there on board and the
data plane if you want to control
essentially how your traffic is out it
you have to figure out a way to reverse
engineer the logic and figure out how
can i configure the cisco router such
that the flows patterns that emerge are
basically consistent with what i want so
tends to be it this whole thing if you
try to drive it towards high efficiency
it breaks down really really fast
because you don't have the ability for
direct control and that's what s deans
essentially offer you what happens with
SD ends is you use commodity streamlined
switches so take merchant silicon by a
switch from a restore point or somebody
like that and on top of that you run
your control plane on a server you
centralize the control plane you can
write it yourself and then what do you
do is you directly configure the data
plane of the switches which determines
how flows and packets are routed
directly on the switch itself there so
that's basically the promise of SD ends
and that's why you see a lot of momentum
towards this paradigm because it gives a
lot of control to us to have efficiency
and flexible infrastructure so with this
in mind this is what basically a 10,000
foot view of so on we have essentially
services generating traffic
a half service brokers so a service is
essentially a collection of hosts they
have a broker and we have network
composed of commodity switches and they
are like network agents between the Swan
controller and the switches what the
service brokers these are
representatives or services what they
tell us is how much traffic I want to
send between let's say CL and New York
and at the same time we are getting
information from the network agents we
say ok what is the current topology like
how much traffic is flowing on the
network of what so you got your state of
demand so what I'm going to call this
like the demand matrix and that's kind
of the network state take these two
inputs and you basically based on your
policy compute how much bandwidth should
go to each service so we've got hundreds
of services and hundreds of data center
pairs so we do this allocation for every
particular thing every particular pair
and we compute the exact network
configuration that can satisfy the
demand matrix modulo our policy so the
interesting bits here is that every five
minutes we reprogram the global network
so this is this is very this is very
exciting from a systems perspective so
think of like a network with essentially
a global footprint every five minutes we
completely reprogram it to match a
demand and we see substantial gains in
efficiency from being able to reprogram
the entire network really really quickly
so in fact like every five minute delay
loses two to three percent in
utilization so forth so we'd like to do
develop mechanisms that can reprogram
the network based on the current demand
really quickly so we kind of build this
thing and by the 11 some peace is like
the hardest part here in terms of I
think that that does not scale very well
is rate-limiting so what we do is push
all the rate-limiting down to the edge
so the rate-limiting is like after
services get a bandwidth allocation that
rate needs to be defined what that does
for us is that what we don't need on the
switch itself is any capability to rate
limit traffic all rate limiting it would
be done there so the switches can be
very very simple as a result so I'm not
going to go into too much detail about
the design challenges but as you might
imagine while the conceptual viewpoint
is very simple to get this going on that
scale we need scalable mechanisms for
computing bandwidth allocation and
configuration we need
work with limited switch memory one of
the one of the one of the reasons those
commodities switches are cheap is
because they don't have a lot of memory
so can we work within that to actually
get the routing we want the third one
avoiding congestion that's the one I'm
going to talk about real quickly here to
give you a sense of the new problems
that emergent emerge as we try to roll
out this system and of course like
resilience to failures and bugs there's
a paper coming out you can look at the
details or talk to one of us if you're
curious how we do that okay so what's
the problem with congestion during
updates and this by the way this this
whole problem emerges only because we
are centrally controlling the network
from top this does not happen in
distributed protocols but distribute
protocols have other issues so the
problem the following let's say you have
your network currently routing the flows
as on the left and you want to route the
flows as it's shown on the right now the
thing is like making atomic updates to
let's say r1 and r2 which have to
determine how flows are routed it's very
very hard so what might happen is that
you know one of the flows might move
before the other and when that happens
you basically will have congestion in
the network so think of it this way
Manhattan gridlock everybody's going
clockwise now you want everybody to go
counterclockwise can you do that without
actually dropping cars that's
essentially the problem in there so how
do you do that so and this is this is
where the nimbleness of the network is
guaranteed so what we do is we create
what we call congestion free network
updates so this is a guaranteed
mechanism to change the network from any
state to any state without creating any
congestion and we do that by creating a
congestion plan or an update plan so in
this case an example plan is that okay
don't move entire blue flow at the same
time move half of it then move FB and
then move FA and what I missed out here
is that the capacity of each link is 1.5
times in there so this kind of works so
that's the idea so think of in the
Manhattan grid log example we have a
shoulder so to speak and that shoulder
is essentially used in this case the
shoulder is 50% and that shoulder is
essentially used to resolve the gridlock
the interesting things here is that the
capacity we provision in the shoulder
let's say it's s let's say ten percent
the number of steps we need
a congestion plan is essentially 1 minus
s so that's a result we can prove in
fact we can actually find in most
situations we find like the plan is only
one or two steps we have some optimal
methods of computing the steps and it
between one or two steps we can change
the network state from any state to
other and this is where car gets
interesting because we have background
traffic the shoulder thing I talk talk
about you don't actually have to lead
that shoulder capacity vacant just put
background traffic on it and the loss
rate for that traffic we are less
sensitive to so in a sense we are using
the roads all the time except you know
some cars we just nuke when we need the
capacity for that's now I'm going to
like ask actually I mean to do a kind of
quick demo of the prototype that's
running in a building not far from here
before we conclude
so this is the topology so we build as
one system and currently it's being
deployed in a test that that mimic the
Inga datacenter topology of Microsoft
this is a part of that so we're on
deploy the servers into four locations
and this test back consider of eight
switches and we also these the server's
generator power traffic between all the
data centers you notice every data
center actually have two switches for
redundancy reason so every link between
two data center actually composed of two
physical links so we're ass free type of
application traffic on the test bed so
first one is high priority interactive
traffic which are very late in season
sensitive and notice that they are very
spiky we cannot predict how the demand
is generated the second type is elastic
traffic they are less latency sensitive
but still they have some deadlines to
make and the last one is we called
background traffic they have very low
priority it's entered their best effort
whenever their bandwidth we want to give
them so um
control4 so we show the grass here so
the grass on the left this is a real
time graph as you can generally see the
graph is being updated on the right so
the left graph shows the utilization of
all the links in this test back there
are four days ago we notice as ABCD so
there are three different colors on the
links the rel red color means the link
is highly utilized and green means is
low utilize and blue means is on medium
utilized as you can see most of the
inter data center links are red which
means they are fully utilized and only
the intra data center links are agreeing
which means they are not fully utilize
which is what we expected but what is
more interestingly is we show on the
right graph so there there are three
three times theory graphs shows the
allocated the traffic agreement
generated by the application and how
much man who is the education actually
are obtains so for the interactive
traffic even though they are very spiky
so because of the scale on the y-axis I
cannot show it's spiky but indeed they
are pretty spiky the interactive traffic
always obtain the bandwidth request but
further our elastic traffic as you can
see when the demand grow and increase
and decrease the allocation closely
follow the actual demand and then in the
bottom is a background traffic as you
can see when the when the elastic the
traffic it demand or drop we give the
free bandwidth to the background traffic
but as the interact elastic traffic
demand grows and we take away those
bandwidth from the background traffic so
this is also what we want to achieve
just just move to the slides just one
okay
so I'm also just going to show that we
can achieve very high utilization and
satisfy the SLA of the application but
we can also react to failure work at
eight so just now I inject a failure by
turning down one link the team or a 1
and D 2
so as you can see we quickly within one
second we are able to detect this
failure and remove the link between a 1
D 2 from the centralized traffic
optimization and you can see even after
the failure does all of the link can
still be fully utilized and what you can
see on the right is even after the
failure the interactive traffic is not
impacted the only the bottom the
background traffic there are demand
actually they obtain less bandwidth them
before so so what was clear from I think
hopefully from Ming's demo is is a
basically failure resilience and the
flexibility of so on what was what may
have not been clear is that how much
clothes are we getting to optimal so
this is a graph taken from real data
over our entire network and what we see
is that OH on essentially achieves like
ninety-eight percent of what an optimal
method can achieve like that's basically
not worried about congestion updates
that not worried about memory
limitations that's not worried about
resilience to failures and whatnot and
this is essentially what we are kind of
trying to build these days to summarize
essentially I think the key thing events
want like I said it yields a highly
efficient and highly flexible one is
based on coordinated transmissions
across services coordination is global a
highly centralized resource allocation
mechanism and we carefully manage how we
transition from one network state to the
other I want to leave I'm going to leave
you guys with the pub that has started
with essentially I think one of the
things that is key and very exciting as
a systems person right now is that
essentially efficiency is really key as
we build out this really large cloud
services we need to figure out ways to
make them efficient and there's a lot of
work there's a lot of opportunity here
in in both academia and industrial to
how we actually make this large cloud
scale services efficient one of the
other actually exciting thing for me as
a researcher is that because we control
these things end to end there's a lot of
opportunity for clean slate thinking so
in swan for instance we are changing
this way
chase we are putting in some new
algorithms we are changing the servers
were putting great limited we can do
that and so can like as academics as
well I would actually like to encourage
you to have essentially figure out ways
to run the entire infrastructure
introduce eyranne is one instance of it
but there's a data center network their
server efficiency and whatnot all of
those and you can do that while being
not worried about what the current state
of the art is but because hey in two or
three years will roll over our entire
infrastructure you know if so much don't
have the capability of switches don't
have the capability we can do that so
I'd can't encourage you guys to actually
think in that direction there with that
Island thanks any questions okay we're
gonna have mics and also please
introduce yourself so all right cool
mark return you can University of
illinois-chicago two questions first is
what's the overhead of the
software-defined networks what do you
mean by overhead so soft where do you
put it on top of hardware right so if
you did not use it and you ran the
system on top of hardware then you
wouldn't actually have to do additional
writing in your software-defined Network
in the control plane um so it turns out
I think so words like we can actually
use just one server to do global routing
and I didn't talk about the methods we
use to computer allocation if you're
talking with a computer overhead we can
actually use just one server I think the
thing we have more worried about is
actually resilience of centralizing
computation and not the overhead
overhead wise we can actually deal with
it because centralized computation as a
whole it's simpler than whatever the
distributed network is trying to achieve
by talking to each other so the key
there is like like as I'd like to just
because you did in fact overhead is in
our view is besides the point what's
more important as we centralizes can we
get the same level of failure resilience
and that I think we have some techniques
we can talk about that but the other
thing is an open area in a lot of ways
as world moves towards as the ends what
is the failure story look like to give
it kind of insight in the sense like in
the networking world they used to be
this thing called faith sharing right
like the control and data plane used to
fetch here so if your control
fails you can assume data plan fails so
what has happened now in SD ends is that
control plane and data plane are not the
same so now you have to make intelligent
gases around what happens when the
control plane fails is the data plane up
or not so those are the kinds of issues
we are kind of working through right now
and there thank you and second question
is about historical data I do plan to
incorporate it in here your decisions
that you make in the control plane I
think what we I don't know if he'll go
historical or not because i think
there's so much flux in our services
that historical trends may or may not
predict the future i think what we are
seriously thinking is the thing of
calendaring where services tell us not
what their current demand is but they
tell us what their future demand is
because services developers who are
deploying the service they have a better
idea hey you know i'm rolling out a new
feature next week and my traffic is
going to go up so this is this is the
thing that really bothers like the
operators which is like this unannounced
changes because i think if history was a
predictor of future we'll run a very
very smooth network right so that's what
i think we will bring in some explicit
signaling from services to actually plan
ahead in there but but history i think
may or may not be a good predictor so we
will probably stay away from that
complexity we just wanna move from APL I
you talked about this already but I have
to wonder about this assumption of
centrally allocating resources and
scheduling flows and stuff like that
isn't that the Achilles heel of all of
this it seems like an inherently
unsolvable problem what if part of the
network gets disconnected from your
central resource allocator it center etc
let me know what you can do it um it's a
ready real worry I should acknowledge
that first one of the saving graces
though is that even if the control plane
in this network's failed the data plane
continues to function as is so we go
ahead we program the switches so in some
sense like commander's intent in warfare
right so we've told the switches this is
what you should be doing when the brains
go away the switches will continue doing
what they were doing
back then so what we lose now is an
ability to handle a major change it's
not like we are losing all traffic in
the network the traffic is just routing
when when disconnections happen the
traffic is essentially being routed the
way it was five minutes ago or ten
minutes ago which is not the end of the
world if we can bring it bring things
back up but and and the other set of
techniques there are scores to your
application redundancy we are using and
I think if network partitions happen
somebody will take over and become the
new computer or the new brains of the
system if I'm a third of another issue i
mean you're talking on the scale on the
time scale of minutes here but is that
really fine grain enough for what you
want to do I mean there's new
applications flows thousands of them
coming per second that's true but when I
when I say flow I basically mean an
aggregated flow off a particular service
between two data centers i'm not talking
about individual connections there
there's some questions are back i think
other so gives us one from columbia
university so all of this is running on
top of an optical networks so are you
thinking about dynamically allocating
with wildlife path and wavelength in the
future um if if the optical folks would
give us an ability to program them on a
fast time scale i would absolutely love
to use that ability but today we don't
have that but if ya so i think the thing
is like we can dynamically change how
much capacity we have between 2 dcs or
how much link capacity and I I
personally love that I think that would
essentially my intuition is that let us
drive the efficiency even further
because different parts of the network
are under pressure at different times of
the day so we can steal some capacity by
yeah by the way some of this is tied to
the amount of money they paid for
getting a certain kind of man worth
right so you sort of pay that and we
have to meet that so anyway there was
another question Roger had a question I
think that'll be last question because
we are at their limit
so actually this is a two-part question
first ISM you have controllable traffic
and you have a non employable traffic
which was your iPad I think so depending
upon the randomness of the non
controllable traffic you really have to
allocate the bandwidth I mean that part
is not controllable right so the the
control and the background traffic has
to wait and you said in the passing that
it waits at the edge which means at the
host yeah so you have to modify aldi
host ah you do you have to modify all
the host yes and no I think our current
plan would be that hosts that are not
modified would get actually put down in
what's even a lower priority class like
scavenger class that's kind of one
answer that gives like how do you go
from where we are today to where we want
to be the other is that we can put rate
limiters in the operating system or the
hypervisor that's like one place to
change without modifying any of the
applications in and for some of our
services so there will be no code
changes and we will insert essentially a
shame that sits in the OS or hypervisor
to rate limit so that's I think that's a
that's a pretty tractable engineering
challenge ok thank you righto thanks um
I was thinking when Vijay Mia you know
was presenting he said there were three
pillars you talked about distribution
efficiency and performance so you talked
about distribution taped out to our
performance see dr. what I wish and see
so on that note let's applaud the
speakers and thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>