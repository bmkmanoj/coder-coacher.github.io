<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Discovering the Structure of Visual Categories from Weak Annotations | Coder Coacher - Coaching Coders</title><meta content="Discovering the Structure of Visual Categories from Weak Annotations - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Discovering the Structure of Visual Categories from Weak Annotations</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VPwGSroLLEg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
thanks for having me here and push me
further for the introduction so but I'm
going to talk about today is some of the
work I did during my grad school not so
long ago only two years ago and some of
the recent work I've been doing at TTI
which is not so much learning but about
discovering the structure of visual
categories from annotations from the
data that you can collect from the web
crowdsourcing in other ways so why am i
interested in this problem and so one of
the main motivations is I want to answer
detailed questions like these so suppose
I have a large data set of images of
birds and I want to pose a question like
finally birds with red beaks so how
might you solve this problem a popular
way of doing this is to train a
classifier for this label so you take an
image and you extract a bunch of
features we r sift and whatever your
favorite is and then you might learn a
classifier based on representations such
as histograms of these low-level
features and if you are lucky this might
just work and you get the right answer
so this is a red big flinch it has a red
big so it's great but often what happens
is it doesn't work and you might get
something that looks like that which is
red but its weak is not red and one of
the issues is the underlying low-level
features don't really know what what
does it mean for the beak to be red
versus the bird itself to be red vs. any
low level representation which is
correlated with red so in other way the
representation that is being used in
this case these low-level bag-of-words
lack an alignment with these high-level
representations which make it rather
difficult for you to parse this
high-level question into something that
can be decomposed into
a way that the model understands so how
might you try to solve this problem and
in particular I've been looking at sort
of two different ways in which you can
inject with richness into the model so
obviously this since eventually you're
interested in these human centric
applications it's natural for you to ask
how can humans how can we as annotators
as computer vision researchers help
inject richness into these models so one
possibility is that you have more
supervision available during learning
that can help you build richer models so
supervision can help align these
representations during learning during
inference to solve these problems and of
course if you have humans involved there
are a host a whole lot of other
questions that you have to worry about
in particular what's the best way of
collecting supervision what kind of
supervision is the most easiest to use
in terms of cost in terms of accuracy
what are the user interface that are
useful in collecting these issues and
the kind of things that are interesting
there are active learning and the weekly
supervised methods and of course you can
think of this whole setup as a human in
the loop a human could be a component in
this whole procedure where you have
learning algorithms you have humans that
can provide labels and together you want
to solve difficult problems that are
difficult both for the humans and the
computer so I'm interested in sort of
this sort of problems in in this setting
so I've structured the talk around these
two themes the first one is about rich
representations of visual categories in
particular I'll talk about what we have
developed part based models called
postlets which will sort of motivate why
more supervision is useful during
learning the second one is some of my
more recent stuff about thinking about
interacting with humans in a in a more
intelligent
in terms of discovering parts and
attributes of object categories from a
new kind of annotation framework all
relative annotations okay so there one
of the things I was interested in it's
one of the challenging problem in
computer vision is detecting people and
people are particularly interesting
because they often tend to be the
subject of a lot of photographs and
they're also quite challenging so if you
look at these images which are from the
Pascal data set common benchmark use for
testing detection algorithms you see
that there's a wide variety of poses
viewpoints there's a dot sticking out of
this person the other there's a horse
next to the person and the person is
wearing loose-fitting clothes that
occlude the parts and so on so if you
take a strong pedestrian detector which
is which works very well for detecting
people on the street in sort of
unaccredited settings and apply it for
detecting people like this it doesn't
work very well so you get something like
a 12-percent average precision for
detection so how might you try to detect
these people and it seems like because
there is so much of this compositional
structure that you would like a models
to be compositional in particular part
based so it's not surprising that you
would like to use these part based
models to detect these people now the
main question is what are these parts
that could be useful so in the vision
community there have been two different
ways of initializing parts a popular way
is this the defining parts based on the
human anatomy so humans have a
particular skeletal structure and so
it's natural to define parts that
correspond to limbs up ahead and and so
on and these are what are called as
pictorial structures because these so
the pictorially represent the kinematic
structure of these objects and the whole
idea is you're going to try to find
these parts in images the hands and
limbs and then stick them together based
on the skeletal structure the main
problem with this approach is these arms
and legs are rather difficult to find in
images so in particular these images if
you try to detect arms which are just
parallel lines you'll find them
everywhere so it turns out the natural
world is full of parallel lines and if
you look around you there are parallel
lines everywhere so these part detectors
are not very reliable so this procedure
doesn't work very well because the parts
that define this model are not
discriminative enough so for these parts
to be useful they need to be
discriminative that you should be able
to tag them quite reliably from images
and that has been the approach in
another set of work where one of the
leading approaches for object detection
is this deformable part based model from
thousands of and the idea is you try to
find parts that are discriminative and
the whole procedure is a discriminative
training procedure and the model is
structured to find parts that can be
reliably detected from images so here's
the model of a person on the left and on
the right edge where these parts belong
in images so this although this
procedure is great we don't really have
the kind of semantic alignment I was
talking about in the previous step that
you need so this looks like a face but
without supervision you don't know that
so if you want to go beyond detection to
answer things like what kind of
hairstyle the person has what kind of
dress is wearing it's hard for them to
go dig into the model to get these
answers if this alignment doesn't exist
or you don't really know this exists
without supervision so in particular we
need these parts to be somewhat
semantically aligned
for you to answer those detailed
questions so it looks like we have a
dilemma on one hand we need semantic
parts like hands and legs that can
support these higher level questions but
hands and legs don't really work very
well for detection so you need parts
that are also discriminative so how can
we get both so one way of getting that
is use more supervision so the whole
idea of postlets which in a series of
papers with lubomir and jitendra at
berkeley we have developed these parts
which are simultaneously semantically
aligned and also visually discriminative
so let me just show you some examples of
what postlets are so each row here is an
example of a pose let shown as the most
the positive training examples of the
similar these are the set of patches
that each pullet is trained to recognize
from background so just like faces when
you see a frontal face you know that
there's a person at a given location
looking towards the camera these are
also patterns that are easy to detect
like faces and tell you something about
the pose of the person so for example on
the top is a person with a high near is
forehead and it's a discriminative
pattern that can be detected it's not
constrained to be just from one
anatomical part like just a hand but it
represents a variety of different poses
that that can be found in images so in
the bottom row is the full lid hand
configuration here back facing people
here is a poster that tries to capture
the lower part of the body and if you
notice what's common across these images
is not that their appearance is very
similar it's the fact that the
underlying key pose or the configuration
of these joints are similar so once you
find them you know a lot about the
underlying configuration of these joint
positions so these patches are close
semantically
but if you look at the pixel values
they're not they're not similar so how
can you find these subsets of patterns
that are visually discriminative so the
basic idea is we're going to build an
engine that can on the fly generate
positive examples for any configuration
of key points and then we're going to
find a subset of these which are
visually discriminative so for example
here cept configuration on the image
shown on the left of this particular
pose and on the right there is another
person with the same pose and that's the
window and the right is what we want as
the similar window for the left one so
how can you find this patch and what we
do which which is somewhat different
from existing approaches is we assume
that our training data has extra
annotations so on the training data we
assume that somebody has labeled these
landmarks which correspond to the left
shoulder right shoulder eyes and nose
and once you have this solving for this
similarities is very easy so you start
from an image patch you forget about the
image information and just look at the
key points and then given a new image or
a new instance I can find an alignment a
similarity transform that aligns the
source to the destination and then I can
transfer the image patch using the same
transformation and that gives me the
most similar patch for another instance
now I can do this procedure and also
compute the residual error that's left
after their lineman procedure and that
tells me something about the quality of
the match so if the error is low I know
that the key points aligned very well
then i can do this procedure for every
instance on my training data so starting
from an image patch like this i can find
closest match to every instance and also
compute the residual error when i keep
doing that and gives me a number for
each instance I can sort these instances
bay
on this error and then threshold the
list at some point once that exceeds or
take the first K of these and this on
the fly gives me the positive training
examples for this post configuration now
what you want to do is then go ahead and
train a detector for these patches and
that's not something new you can take
your favorite image detection algorithm
what we do is modify the lalin tricks
detector to detect these patches the de
la lyrics detector is essentially a
histogram of oriented gradients based
image representation and linear SVM
which is very fast so we adopt the same
procedure but instead of detecting
pedestrians we tell it to detect these
these patterns now there are
exponentially large number of patterns
in an image because you can arbitrarily
define any window so and obviously not
all of them are going to be very good
and discriminative so what we instead do
is we're going to sample a large number
of such seeds for each seed I can
generate a set of positive examples
given the procedure I described and then
I'm going to select a subset of those
which are visually discriminative right
so I'm so I put down a random window on
all my training examples and these
define the seeds and some of these are
going to land on discriminative patches
like the faces or the poses i described
which have strong contours that let you
detect these patterns reliably in images
and of course some of the others are
going to land on textual s regions and
others where you won't be able to train
the discriminative classifier so the sub
use sample a large number of these and
select a subset using cross-validation
and if you do this procedure for the
people category on the pascal data set
these are the top 100 discriminative
patterns that emerge so what I'm showing
you is it is an array where each patch
is
utilizes the average image of the
positive training examples used by that
post lot so not surprisingly the most
discriminative pattern that's found is
the frontal face and we know that front
face detectors are reliable but then you
also get a wide variety of things that
correspond to the lower legs of the body
profile faces more pedestrian like
zoomed out portions of the image back
facing upper bodies and so on and what
this essentially does is it decomposes
the whole person category into a large
number of discriminative patterns that
are trained to detect a portion of the
pose of the person from a given
viewpoint and these are essentially what
we call pose let's and we'll show you
how you can use them for a variety of
different recognition tasks once you
have been on the library of these
supposed words so it's just now the
training set contains some hands late
night positions freeze image right
nothing else nothing else yeah so that
lets you on the fly generate positive
examples the alternate could have been
I'll label bounding boxes for parts i
think our discriminative but with pose
that's what you get is you get parts
that you wouldn't otherwise come up with
such as the half your shoulder and half
your head it's a very discriminative
pattern that doesn't really correspond
to any anatomical part on the whole but
get selected so here is an example of
some of these postulates like number 17
is hop the shoulder and half the head
when you select run windows
have three joins it you throw it away
yeah yeah we also throw away things that
don't match enough and things like that
but yeah so the alignment procedure is
very simple you randomly put down a
window look at what points are
underneath the window and compute the
similarity transform that scaling
translation and rotation in plain
rotation okay so one of the advantages
of having more supervision is each
pullet once detected tells you a lot
about the underlying semantics that that
exists so for example you can look at
the distance the scatter plot of the key
points the landmarks that are predicted
according to that post let you can look
at the orientation histograms and then
you can use them for a variety of
detection task such as detection so for
example for each pullet these are the
top examples you can compute the
estimated bounding box the relative
offset and then use that at test time so
a test time you we don't have the key
points and you're going to rely on the
detector that we trained but these
detectors were selected because they
perform well so these are indeed going
to be detected reasonably well in images
so for example if an image like these
these bullets might have fired on this
person and what's night was nice with
these postulates is there robusto
occlusions because some of these
portlets are trained to be detected only
on the part of the person and then you
can use these parts to then predict the
overall bounding box using simple Hough
transform and then scored the detection
based on a weighted combination of the
detection scores and this procedure is
actually quite competitive so in we
participated in the fiscal v 0 c 2010
challenge and this was the best
performing % detector compared to the
lalin tricks its
what times better and it's also better
than the deformable part based baseline
of course it's not fair because you're
using more supervision during training
which puts it at a disadvantage compared
to others but one of the nice things for
using more supervision is then you can
readily use these to do a host of other
tasks such as lamb localizing landmarks
you can predict the pose you can if
you're training data had segmentations
you can use them to predict
segmentations a test time with
relatively low effort just combining
these predictions in a reasonable manner
and you get state-of-the-art results on
segmentation accuracy so this is no
longer the state of the art but when at
the time of publication it was the best
performing segmentation algorithm for
person category one of the more fun
things we did is recognizing attributes
of people in the wild so we wanted to
predict see if you can predict the
gender of the person from images like
these and what's interesting is you can
predict the gender even though not the
entire person is visible so the top row
as you can guess are the females and on
the bottom are the males and you can
tell that because of the hairstyle the
kind of shoes they're wearing the pores
they are at and things like that and if
you try to train a classifier on this
kind of training data it doesn't work
very well in fact it works almost a
chance performance but somehow if you
could factor out the pose and give it
training examples of these kinds on the
left and now males on the right are the
females this is a much more easier
problem for a learning algorithm to
solve so in fact you can plug in
standard image features like hog and
this histogram features and is going to
learn a reasonable classified to
separate the left from the right and
this is exactly what pose let's give you
so given an image these polar detection
can factor out the pose from
the appearance and condition on that you
can train the discriminative classifier
to predict these attributes so so more
concretely we had 8,000 instances of
people on the Pascal dataset labeled
with 9 different binary attributes and
the task was as follows so given an
image and the rough bounding box of each
person and a given bounding box of the
person you are interested in the goal
was to predict the attribute of this
person we needed the bounding box
because there are multiple overlapping
people in the image and you needed a way
to tell which person are you interested
in extracting the attributes off but
other than that there was no information
given and our procedure for extracting
attributes is was very simple you start
with an image localize the post letters
that correspond to this person by
looking at the consistency of the
bounding boxes and then condition on the
post lets you can then build a hierarchy
of different features that tries to
predict the attribute given the image
patch for each puzzler so for example
the left one is a front face pose let
and its goal is to simply tell whether
it's a male or female given all the
front face activations right and the
kind of features we are using we're very
simple just the histograms of oriented
gradient some color features some skin
color masks and things like that and
these features were then used to train a
classifier which tries to predict each
attribute independently given each
pullet then you can combine these
attributes to pull informations from
different parts of the body so merge
these predictions then you can try to
use inter attribute correlations so for
example if it's a male he might likely
wear long pants or hat and so on and
this architecture looks a lot like a
hierarchical neural net
but the difference being each of these
layers are supervised because we have at
each level attribute annotations of the
instance which makes this rather trivial
to train but it's extremely effective so
for example here are some analysis on
the test set these are the top scoring
wearing a hat images and and you can see
that the hats are in different locations
of the image but it finds them fairly
reliably these are the top scoring
females on our test set these are the
instances with long hair this is a
rather difficult one wearing glasses
because it's a very few pixels in the
image that has that evidence so you
really need to localize that portion
using a part based detector and what
pose let's give you is it gives you that
flexibility even that late in structure
that comes from part detection wearing
shorts long sleeves here's a mistake
that fires probably because the hand is
occluded by the other part of the body
because hand is including the rest of
the body doesn't have long sleeves and
short sleeves here's a mistake so we did
also comprise a non gender recognition
because we our accuracies were pretty
high on average it does really well with
sixty-five percent whereas if you just
take the chance performance it's
thirty-six percent and if you take the
state of the art image classifier
bag-of-words multiple kernels and so on
and then the cropped image of the person
you get an accuracy which is forty five
point nine percent so that's quite a bit
lower almost near the chance performance
because the pose is not factored out if
you give it the cropped faces so if you
have an Oracle that can detect the faces
perfectly and then you train an image
classifier on that it accurately
actually improves but still not very
good and we tested our method with
respect to a state-of-the-art actually
face recognition system from cognitive
which one of our colleagues had access
to and that gets something in like 75
percent and one of the reasons is on the
Pascal data said about sixty percent of
images only have frontal faces forty
percent don't so only one of the eyes is
visible and these kind of gender
recognition algorithms don't work at all
for non-parental facing people so if you
had perfect accuracy on the frontal and
chance accuracy on the profile you would
still do worse than our algorithm which
gets eighty-two percent and here's some
sort of interesting confusions these are
man most confused with women and not
surprisingly it has learned to pick up
the hair style and these men have long
hair so they get mistaken for women
women most confused with men our
detector has learned to put a lot of
emphasis on the hats and classifies
these as men t-shirts most confused with
non t-shirts some of them are actually
hard to annotate correctly so annotators
thought this was long versus this this
wasn't so this is a mistake in our
annotation procedure short pants is
another one where annotators don't
really agree sometimes where do you
define the boundary within long versus
short maybe need a more finer grained
relative attribute here and some of the
mistakes tend to be because in images
like these it's hard to localize the
right part on the right person because
there's a lot of interpersonal oceans so
here the our detector probably thought
this was the person being referred to
where the actual person the question was
four of us the person in the middle and
of course their mistakes when the local
evidence is missing because of external
illusions one nice thing you can do with
these kind of models is because it's
fully supervised them has this layered
structure you can go back to the model
and ask where is the information
localized in the image on in the part so
for example these are the top five
postlets that had the most evidence for
recognizing gender compare this to the
posters that were used for recognizing
the length of the hair and glasses and
you can see that four glasses really
zoomed in to the to the at the level of
the face again so this is one of the
advantages of having these kind of
structured models that are aligned to
supervision lets you do this bottleneck
debugging and looking under the layer to
see is the model learning what's its
learning in is it failing because my
parts are not being localized correctly
or my features aren't correct and you
can press these kind of procedures very
easily because these intermediate tasks
are well defined and of course you can
have a system that can look at images
and generate high level descriptions
which is quite neat that so for example
it could look at an image like this and
come up with a sentence it's a woman
with the long hair with long hair
glasses short sleeves no hat too long
pants the sentences are not very
imaginative because we had a templated
sentence generation algorithm but one of
the nice things it does is it hedges for
uncertainty so for example if the
evidence is missing it defaults to
saying that it's a person because it
doesn't know if it's a male or a female
and doesn't know much else other than
the person is wearing long pants and
this kind of descriptions are actually
useful for communicating these
attributes to describing people to other
people so in summary this whole post at
architecture hopefully I made a case
that using more supervision you can
guide learning and solve this really
difficult problem of having parts that
are both semantically aligned and
discriminative by using more supervision
in the form of landmark locations I'm
having more supervision at training time
means that at test time you can predict
more things so you can use postlets as a
basis for a lot of high-level
recognition tasks such as pose
estimation person detection attribute
recognition action recognition and get
very good baseline algorithms for
recognizing their attributes and one of
the things I alluded to is that having
more supervision lets you once you're
designing these algorithms lets you
perform bottleneck debugging so you know
if my parts are not being detected
correctly you can evaluate your part
detector independently you can evaluate
your features used to recognize these
attributes independently and you can do
a much more informed search of our
parameters and structures with more
supervision and essentially guide your
learning using supervision okay so if
you have any questions about this post
let part let me know now because the
next bit is a little bit about some of
the more recent stuff been doing on
human-computer interaction stuff so when
did you show us those are semantically
alarmed when yeah you showed me that you
can
postlets to solve semantic labeling
problems right but I can also use pixels
to solve right so I mean they are
semantically aligned by design because
the way postlets are constructed are
okay what the semantic alignment mean in
this setting it means that they are
semantically allowing the space of key
points my landmarks so these parts are
designed to detect patterns that
correspond to a fixed layout of key
points exactly the lots of the key
points right recently do your position
of shoulder position of ways so they're
not sort of ambiguous things that could
be detected in images but sort of rather
discriminative configuration of key
points so once you detect them that
tells you a lot about these key points
that's the level of semantic alignment
of course you could have semantic
alignment of the label at the level of
segmentations or attributes but here
it's just for detection object detection
one of the biggest sources of variance
is the viewpoint and pose so pause let's
just try to factor that out explicitly
by modeling the conditional just
conditional appearance given the pose
and the viewpoint so each puzzle it
gives you a local appearance model for a
fixed set of key points see in a pose
let it varies so some of them are
actually zoomed out full bodies so they
are you know that could be all the key
points of the person which is roughly
like 15 or 16 where some of them are
fairly localized it could be just the
face it has four or five
right like the face and then I has just
eyes nose and the ears whereas something
like this has lot more key points so you
don't really commit to the zoom or the
scale or the location of these postlets
and the search procedure finds these
patterns automatically so maybe the best
way to Detective face is to zoom in even
further and it's part of your search
algorithm and they get picked
accordingly no so they were in marked so
we our annotation only required them to
label the visible ones and occlusion so
that's a good point I mean if they label
occlusions then you could do even better
factoring of pose but we didn't I mean
just makes the annotation too much more
cumbersome if they have to label another
binary label person if they know where
the knee is but it's behind an olive oil
not slave ya know what would happen is
some of these examples would have half
the motorbike visible in it and your
appearance model will have to deal with
this kind of procedure maybe it's a good
thing maybe it's not and so there's
still these kind of choices that oppose
alert has to figure out to see which
windows should be sampled know so this
were just iid sampling but you could be
more efficient in this procedure what
was the distribution
so just X Y scale uniformly at random
and then of course some of these are not
going to line on patches that have no
key points that get thrown out
immediately so you need at least three
if you're going to solve for rotation
and scale and even more to be robust so
I think once there are three key points
you can solve for similarity transforms
and then then is just the discriminative
procedure is going to select the good
ones how many should do I need so all
our show on the Pascal the 20 categories
we you typically picked hundred but
hundred was too many already for things
like bottles where the performance so if
you plot the detection accuracy as a
function of postlets some categories so
people in performance actually improves
all the way to 200 postlets there's
really a lot of variety in appearance
but things like bottles or TV monitors
you don't need that many even bicycles
you don't need that many in this sort of
you get viewpoints in some parts
basically the postlets were making
independent votes where and it might be
that certain both lives are inconsistent
they cannot happen yeah it together
right for example the front face it
blackface cannot happen at the same time
right but still he would order a
position and then accumulated which is
wrong that is true so what what I didn't
tell you about is in the ECC V paper
which is about there's an additional
step which Reese course is posle it
based on the context of other postlets
so it's not a joint model the detection
model is not the joint model but it's
this layered model where you rescore
these parts based on other part
detections and then you do the hub for
ting and that improves the performance
by three roughly three to four percent
and pascal so it's quite useful
going to the point of pragmatics we want
this to be working in real time right
and it is computationally expensive sort
of process right because we have all
these proposed talked about basically
cascading models or photos not a whole
lot but I mean there are some obvious
things you can do exactly yeah he said I
can just you now and basically I don't
need to evaluate a view the booklet well
yeah so we haven't done that but that's
sort of an obvious thing you can also
try to build these kind of convolution
architectures that so these parts are
highly intercorrelations exhibit wien
these parts and you can think of having
a separate basis function that you
convolve with your images a much smaller
so especially if you have lots of
categories you can have a fixed set of
basis and then reconstruct the responses
for these sort of like these terrible
models they were and others have been
pushing or sparse to let ideas from much
of other people or just hashing these
kind of detection based algorithms you
mentioned the company tech attribute
predictor in the precision you say
pinnacle birthday will only sixty one of
those faces we all say they had front
faces but the position recall curves
actually dropped really really
dramatically if you cook back oh yeah so
there is a red curve okay so the red
curve is popped it again actually the
precision even dropped before at like
you at point two at one point three
right it's ya mean works it doesn't work
very well it works reasonably well for
parental faces but it's not perfect and
when I say frontal the way I evaluate
frontal is both the key eyes are visible
so even within that there is quite a bit
of variance and also resolution is an
issue so if you don't have a high
resolution phase you really want to rely
on other features in the body
so maybe at that scale just default to
global features and with paws a--let's
you get that each each purple a
classifier can pick the features for
that resolution so in pascal there are a
lot of low-resolution images although we
try to get the high resolution e
clearance from flickr it's still that if
you don't have enough pixels on these
these algorithms don't work very well
and then can we just go back some
pictures a hundred person how many there
are eight thousand and how many images
so there were eight thousand instances
that so the number of images were
probably like five or six six thousand
yeah so I mean there's an obvious
division of this strain of this what
you're showing us here right there are
the blurry kind of large-scale images
and then there's one 557 mm-hmm five and
57 seem to be very small variance
clusters right they you know the same
kind of the same person yeah yesterday
there is a lot of redundancy in this and
so dip so the selection algorithm I
didn't describe is a sort of a greedy
procedure that finds the best one then
picks the one that gives you the best
increment in improvement in detection
and in that procedure you can sort of
trade off the diversity and accuracy and
it turns out that a little bit of
redundancy is good so to pose elated
slightly different scales improve
performance because the way your
detection works is just going to sample
a bunch of locations the sliding window
architecture and it's not a very dense
evaluation so having shifted copies and
slightly redundant copies are very
discriminative parts actually help
this is a very good point in fact if you
see 512 2345 57 and then 94 essentially
what the model is doing the bullets are
essentially looking at different alluded
clock so it may be handing the occlusion
case where it is saying really part of
the faces with a bird right it may be it
is a mechanism for a few yeah i was
missing here is a white male short hair
looks like logan number one head tilted
slightly to you know what is this one
tiny variance cluster is so massively
represented and i don't think i just
don't think you can average all the
faces that we saw down to give you face
fine you know TVD face fibers the top
left it's not not all the faces it's all
the faces that have the same key point
configuration and not not even all the
faces just the top let's say the top 200
faces so sort of the next image so this
is the average so these are the examples
that were used by this pose lid and what
I'm showing you is 1 6 11 16 7 showing
every fifth match that was found for
this key point configuration and it all
the way to do 46 there is a pretty good
alignment because the face it's a very
large data set so if you average all
this it's going to line up very well and
you get that blurry image so in the
Princeton GZA don't understand dear ones
which ones 512 4557 what it looks to
clear the average of any you know I
guess it just tiny clusters if I guess
is very good environment you always get
it very short no we know what you have a
facebook sorry it doesn't look like
slightly like it doesn't look quite as
white male is this it's also like Pascal
so you know it could be
yeah it's I mean a thousand faces isn't
very large by any standards so this is
just a small poster right and and this
could be it this could be even a smaller
number of images I don't remember how
these were generated was a while back
but it you get a lot of sharp parts for
faces at least because these are all
aligned according to the key points so
yeah so for example these are look very
white but maybe on average there more
white faces okay so anyway so if you go
to the post lets website they're a bunch
of visualizations like this that you can
interact with and look at the average
images in what are the contributing
images for each average so let me move
on to the next part which is slightly
different but deals with some of the
issues on collecting and interacting
with humans so one of the issues we had
when we started to go beyond people to
other categories how do you come up with
what to label so for people and animals
one obvious ways to look at what are the
underlying anatomical joints the joint
structure and then come up with these
labels so for people it works really
well because people are familiar with
people but for our categories like
horses when you tell them to label an
elbow it turns out elbows are not where
do you think they are four horses horses
front legs they're actually standing on
their toes so we get a lot of annotation
errors because people think they know
where the elbow is without actually
looking to the instructions and then
there are things like that base of the
tail the weather of the point the
inflection
which are well defined anatomically but
it's hard to localize them in images
because the there is a skin the animals
have four and they are often including
these parts so coming up with this sort
of key points are somewhat easy but
labeling them is rather difficult and
then there's a host of categories like
boats and architecture and shares that
there isn't a reasonable way of coming
up with landmarks that apply to all of
these instances so what are the
landmarks of chairs I can't even name
them ports or architectural objects and
so on so the question I was interested
in asking is how can you learn semantics
part based models in the style postlets
without actually having to spend a lot
of time and effort to define these parts
ahead of time so can we obtain part
annotations without naming them first
and the answer turns out to be very
simple you can you just show them pairs
of images and then set up this new task
where all they have to do is click on a
landmark which they deem are
semantically similar and what's nice
about this approach is a lot of their
time humans can mark these
correspondences without knowing the
names of the parts so the names of the
parts are great if you want to
communicate but for the purposes of
semantic part discovery the names are
not really important what's really
useful is what corresponds to what and
by not having this sort of global naming
structure and having things driven in a
pair wise manner you can try to elicit
such annotations by crowdsourcing
without having to spend the time and
effort to collect these name these parts
but how useful are these annotations so
for example we collected a bunch of
these correspondents annotations for
buildings
and these are the kind of things people
annotate so the corners of spires good
corners windows goes to Windows and
there's many to 112 many structures
which is kind of the attraction of doing
this correspondence labeling and compare
this to what you might get by matching
local descriptors like sift which tend
to be very noisy so what we did in a in
a previous work is sort of analyze these
kind of annotations in what I'm showing
you here is where do people click as
this image is shown with another image
and each color is a different annotator
and so there is a remarkable amount of
consistency in sort of the locations of
landmarks that are clicked but what is
also interesting is where the clicks
occur it tells you something about the
frequency of that part in a data set so
if the part is more frequent it will get
much more frequently and that kind of
gives you sort of a very weak frequency
measure of the apart and then once again
if you have pairs of annotations you can
string them together and propagate
correspondences so for example if you
start from a part like this you can use
the a pair to propagate it to another
image and keep doing this unlike
postlets you can't really solve for
translation scaling in closed form
because these ambitions tend to be very
sparse so to learn actual parts from
these kind of annotations you what we
did is treat the actual location and
scale as a latent variable and use these
kind of weak annotations to initialize
these models so you start with something
like this which is given by this
pairwise correspondences you roughly get
the structure but the alignment isn't
quite there you are then you can do this
iteratively by trying to learn an
appearance model and then refining the
alignment in this kind of p.m. procedure
that cleans up the annotations quite a
bit so if starting from this and after a
few iterations of the learning
you learn a good part appearance model
and also have refined the alignments
with each instance so in the interest of
time I'll skip over some of the other
examples but here are some of the
library of parts that were discovered
for a few thousand images from few
thousand images of churches and you can
use get parts that capture things like
the upper towers bldg sort of
discriminating patterns that are
frequent and present in a lot of these
images and these are roughly the
equivalent of postlets for people but we
were able to get them purely from this
sort of correspondence annotations
collected up to that of course we don't
know the names of these parts but you
can still use them for detection the
same way and it works relatively well
compared to DPM or else it outperforms
them by quite a large margin would
present to ground truths bounding box
well yes we had the bounding boxes also
annotated but the model here is the same
Hough footings and you learn the
relative displacement and then voting so
that obviously doesn't work very well
for things that have three towers of
four towers you need a multi mixture
model of some kind but it's still quite
good i mean the DP model has the same
problem in sort of models the structure
as a fixed configuration of parts so so
modeling these kind of objects is also
interesting even though if you know the
parts but like i said we don't know the
names of the parts but with some visual
inspection you can name these parts so
some of these at least and what's nice
is you can then visualize these parts in
images depending on where they were
found and create these overlays of
images with world
and these can be quite informative so
for example you get doors windows arches
and upper windows where you can then use
back to get fine-grained recognition so
if I want to find images with more than
one tower or five towers of a certain
color or shape you can just like post
which you can base your recognition
algorithm from the outputs of these
parte de Tech ters and learn better
attribute classifiers so all this was
about parts how much time do I have or
have almost run over to minute ok good
good ok so very quickly so taking this
idea about part discovery from pairwise
annotations the other thing that that
you would like to look at is parts tell
you what is similar but attributes tell
you what is different so for example
discriminative attributes within a
category let you distinguish one kind of
bird from another so that's sort of the
other dimension so parts is
correspondences differences are
attributes so one of the things we
wanted to do is discover attributes from
large data sets and these descriptions
are attributes are a proxy for what
should we recognize for a given category
so how can you collect these
descriptions a common ways to show an
image of an object and ask people to
describe this object but this turns out
to be not so interesting because if you
show an image like this somebody might
say it's a plane you might already know
that it's not very interesting so
instead what we propose is this new task
which is sort of a game where people are
supposed to describe differences between
these images in a very simple structured
manner they say a sentence and another
sentence separated by a separator
and what's what's attractive about this
is this annotation procedure is designed
to get discriminative attributes and it
forces the annotators to describe these
objects in more details so for example
they can they can say it's a plane
anymore they have to say something about
the kind of the plane it is so that it's
different and these kind of descriptions
also let you annotate images in a much
more reasonable manner so here are some
example annotations on birds if you take
an image like before and look at what
did people say about this image across
lots of other images and then just
cluster these sentences you get some
kind of a histogram which like before is
a prediction of how discriminative this
attribute is sort of instant specific
attribute so blue bird black bacon long
tail are very frequent and here's the
Wikipedia description of that and it
contains a lot of these attributes but
it is also sort of tailor to be
crowd-sourced so it doesn't have things
like super psyllium which nobody uses or
bibs but has things in very generic
terms which might be something you want
if your eventual goal is the people so
there's a little bit of analysis which
we did which lets you factor out that
parts and attributes from this text data
but maybe I'll save that for people who
want to stay
thank you and apologize for the time
running over but I think they're some
questions so took longer yeah but let me
just show you the sort of the fun topic
modeling we did which is so what's also
nice about this pairs of sentences it
puts constraints on the words and things
that appear so for example you know that
red and white are ejectives because it
appears it changes rudder is the noun
but this is something you would get with
a part-of-speech tiger not very exciting
but the additional constraint you get is
red and white or two kinds of the same
thing there is they have to be the same
thing about pointy and round have to be
two kinds of same thing I don't know
what but I know that there is a
underlying semantic category where these
two are sampled from so you can put all
of this together in this generative
model of topics and which it sort of
puts together something that looks like
a translation model between sentences
and a topic model that samples pairs and
just this is the whole generative
procedure but if you run this through in
the data set you get these kind of
images so on the top or clusters that
correspond to parts and the bottom are
clusters are words that correspond to
attributes in this case clusters of
semantically related words and an edge
between the top and the bottom tells you
an attribute so you know that the wheel
and cardinality is an attribute that is
present or is discriminative so the most
discriminative attribute is that rudder
color followed by the number of wheels
and these clusters are relatively
meaningful so pointy round flat pointed
sharp point square
shapes color cardinality kinds of play
in sizes sort of the rough location
something being closed and open near
on/off viewpoint for birds you get
things like like bunch of birds so there
were 200 bird species but nobody
recognizes the actual species these are
so the rough families that come up yeah
so you get this bipartite topic
structure that you can get from this
weekly label text but because there were
pairs of sentences and the task was
discriminative it is quite effective so
yeah so in summary basically there's
more supervision is good but supervision
is hard to get so you have to be a
little bit more careful in designing
these UI tasks to get the right data out
thanks for saying</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>