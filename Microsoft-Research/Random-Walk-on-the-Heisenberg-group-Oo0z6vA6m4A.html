<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Random Walk on the Heisenberg group | Coder Coacher - Coaching Coders</title><meta content="Random Walk on the Heisenberg group - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Random Walk on the Heisenberg group</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Oo0z6vA6m4A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hi everyone it's my pleasure to have
persi diaconis here Percy has been a
great influence on many people here in
particular a lot of my own research in
the last 15 years has been dictated by
the directions that Percy initiated 30
years ago and now he's you know and as I
said in the email you can find him now
in the list of the 20 most influential
scientists today anyway without further
ado today Percy will tell us about
random walks on the heisenberg group
thank you you've all hello thanks for
coming out let's see first there's a
there's a handout so this is a no free
lunch you have to make sure somebody
gets each person gets one of these page
pieces of paper okay that's the first
thing so this talk is about a homework
problem I'm in the middle of and I know
better than to do that to you you seem
like nice people so I'll try to explain
to you why there's something in it I
hope for you the talk is joint work with
Dan bump Angela Hicks laurent nick lowe
and harold Whitham it's about well the
heisenberg group so i'm going to call
that group h and it's very simple thing
it's the three by three matrices upper
triangular and with entries X Y Z and
I'll write such a matrix as X Y Z and so
in particular if I multiply two of them
times X prime Y prime Z Prime well
that's X plus X Prime they just add
above the diagonal y plus y Prime and
here if you figure it out it's Z plus Z
prime plus XY prime just multiplying
matrices and XY and Z well in physics
they're usually an R and C XY and Z
could be real numbers they could be in
number theory where this is a big deal
they could be in the integers for me
often they'll be in the integers mod n
but they could really be in any ring and
this this all makes sense the random
walk on this group there the group is
generated by this generating set I'll
call s which is 1 0 0 minus 1 0 0
symmetric random walk 0 1 0 0 minus 1 0
so random walk is you pick one of these
two coordinates at random and you put
plus or minus 1 there you write down
that matrix and then you multiply by it
on the left and so I'll try to motivate
it in a second but as math if I say Q of
G G is a group element is equal to 1/4
if G is contained in this set s and 0
otherwise ok that's one step of the walk
and then Q star Q of G is convolution Q
of H times Q
of GH inverse there's something over H
so what's the chance of being at GE
after two steps you had to have picked
something your first step and then pick
the thing that gets you to G your second
step and similarly we have Q star K of G
the chance of being at G after K steps
of the walk and under no conditions well
endure so example of the kinds of
theorems that people prove several
people in this room if the entries are
in the integers mod n then and if n is
odd because otherwise there's a parity
problem then Q star K of G converges to
the uniform distribution U of G which is
equal to 1 over n cubed so if you walk
around on this group you get to a random
group element random group element means
XY and Z can be anything and the rate of
convergence is less than or equal to
some constant e to the minus I think
it's 2 pi squared times K over N squared
and it's bigger than or equal to some
other constant times the same thing 2 pi
squared K over N squared and so this
says we say order N squared steps are
enough if you go 10 N squared steps if K
is 10 n squared the side is
exponentially small if K is 1/10 N
squared decide this big and the this
everything is explicit and so that's
that's a typical theorem in the subject
how long do you have to walk and if if
we're over Z
for example then random this is an
infinite group and it's not a billion
and it's I don't know it's so random
walk isn't recurrent you don't come back
to zero but you can ask how long do you
have how what does it look like and well
Q star K of of I'll write the identity
zero zero zero the chance you can you
can ask all kinds of questions but
what's the chance of being back at zero
after K steps
this is asymptotic to a constant over K
squared where C is equal to something a
1/4 squared divided by PI to the 5
halves times the square root of 2 if you
wanted to know okay so these are kinds
of theorems that that people prove now
this is a sort of general a audience and
I want to tell you why I care about
these things and maybe a little bit why
you might be interested in some of
what's coming so some motivation for for
this study because that's it you know
it's not hard to see that it's strange
it but it is transient it's just that's
a few little theorem but it's any group
very growth right there many many ways
to see it but I'm just no but that would
be a question you could ask is it
transgender or not and and you can ask
about off diagonal you know you can ask
a lot of questions and so my motivation
for studying this problem for giving
this talk is following the the first is
it's a funny question for you maybe but
not for me this is Fourier analysis good
for anything
actually useful Alex you understand that
by the time I get through it and we'll
just to say sentence about it this is a
random walk on a group and and you could
try to study that using Fourier analysis
and it's hard to do as you'll see and I
think that there are eight proofs of
this now and none of them using Fourier
analysis and so since this is the
natural way to try it well I hope
understand but that was my motivation I
just wanted to try to do one of these
problems by Fourier analysis the second
motivation is about the features of
random walk and I think this is a quite
important quite an important topic for a
lot of things so I probably don't have
to tell the people in this audience that
Markov chains are used for all kinds of
computational tasks and people like
Yuval and I and other people in this
room often study the question of how
long do you have to run a Markov chain
until it's close to its stationary
distribution well we have global kinds
of results like this one if I run the
Markov chain N squared steps then in
this very strong total variation
distance it's close that which means for
many many questions the answer you get
from the uniform distribution and the
answer you get from the random walk are
close together but you might not care
about all aspects you might only care
about the aspect you care about you're
running this random walk to do a certain
simulation problem you don't care about
all the other questions you might ask
how long do you have to run it to get
your feature right so this is an example
where that comes into focus so let me
explain that in this context
so the steps are I'm going to pick
elements of this set one at a time so
suppose the elements I pick or epsilon I
Delta I so that is I know just so this
is either you know 1 0 minus 1 0 0 1 0
minus 1 ok those are those are the steps
I pick I won't write the last 0 and then
my walk is a you know epsilon well I
guess the way I'm doing it epsilon K
Delta K times epsilon K minus 1 Delta K
minus 1 epsilon 1 Delta 1
that's my walk and I'm going to multiply
out and I'm gonna say that's equal to
say X K Y K Z K now it's very easy to
see from the rules what you know so here
X K is equal to epsilon 1 plus and so on
plus epsilon K and these epsilon are you
know 0 plus or minus 1 there's 0 with
probability 1/2 and there plus or minus
1 with probability 1/4 so this is just
simple random walk on the integers mod n
if I'm in this context and we know
everything about how this behaves right
that is this this behaves like the like
the central limit theorem says and and
if you if you think of the integers mod
n is endpoints wrapped around the circle
it's doing random walk and therefore it
takes N squared steps to get random and
and etc but in particular the well ok so
and the same for y y YK is equal to
Delta 1 plus and so on plus Delta K and
and the Joint Distribution of these two
things follows the bivariate central
limit theorem over Z and so when you
multiply these matrices what's here and
here are
are like Gaussian random variables and
so we know we know everything about them
so let's look at this third coordinate
yeah so each each time so I'm using
random variable notation each time I'm
picking one of these four things in each
time the fourth the third coordinate
zero so I'm forgetting about it so this
really could have been epsilon delta 0
epsilon delta 0 okay but I'm just
forgetting the 0 okay and they're random
okay so so epsilon Delta is equal to 1 0
or 0 1 or minus 1 0 or 0 minus 1 with
probability 1/4 each so those are just
the steps I'm picking each time and then
I'm multiplying them together so this
should really be a zero following each
thing and and then I'm multiplying in so
is that okay Sara ok thank you thank you
so what's the third coordinate so here's
EK i think it's this it's it's epsilon 2
times delta 1 plus epsilon 3 times delta
1 plus Delta 2 plus plus epsilon K times
Delta 1 plus and so on plus Delta K
minus 1 ok I think it's that it's very
easy to figure out what it is I think
it's that and so you know these
Epsilon's and deltas they're a little
bit dependent if if epsilon I is plus 1
Delta I is 0 ok they're a little bit
dependent but really ok so you can ask
now if you're a probabilistic at about
anything else or any kind of a thinker
about these kinds of things how does
this thing behave what do I have to
divide it by so that it has a
non-trivial limit distribution here here
we know that you know XK divided by
square root of K goes to normal with
mean 0 and variance 1/2 or whatever it
is and what do I have to divide ZK by
well what you can
show is that you have to divide this by
K so here Z K over K that has a limit
this goes to a limiting random variable
this converges as K gets Lorde's to a
non trivial limit and because you've all
wrote a book about it I'll just put it
down what what's the limit
this goes to I'll call it Z infinity
this has a limit this has a limit this
way weak limit and where the Z infinity
is distributed as the integral from 0 to
1 of B 1 of s DB 2 of s where these are
independent Brownian motions and that's
not very surprising if I divide this by
K you know this these are going to these
are go if I divide this by root K sorry
I divide it by K I put one of the root
case under these things they all go to 2
this goes to Brownian motion and then
this is a Riemann sum for this integral
it's not it's not very hard if you want
to make math out of it this is a
martingale and the martingale central
limit theorem tells you it has a limit
and the limit is identifiable as that
and so that shows that that this third
coordinate this Z coordinate these
coordinates it takes them in order to go
they want to get random on the integers
mod n they have to go n squared steps in
order to have a good chance of getting
down to the bottom this third coordinate
is getting random much faster so this
this this shows that you know taken so
so if if K is of order a constant times
N squared Z Z K is mod n is uniform if Z
is large so this third coordinate is
getting random much faster if I so this
is an example of
features and I think that it's an
important thing to try to study how long
does it take parts of a random walk or
Markov chain to get random cien right
and see should be large so forth anyway
those are those are those are so as a
conjecture that we don't know how to
prove if instead of doing this with 3x3
matrices if you do it with with D by D
matrices and then you say you pick a
coordinate just above the diagonal at
random and then you put plus or minus
one there and everything else is zero
just the analog of that you've all in a
landslide beautiful paper showing that
this takes I think it's if it's d
squared P squared steps on the whole
group but if you just look above the
diagonal it gets faster and if you look
on the second diagonal it's fast they're
still and in the corner it's very fast I
mean it is just above the diagonal one
of these entries that's the same as what
I just talked about gets random so just
just above the diagonal it takes it
takes P squared steps to get random 2
above the diagonal it takes P steps to
get random 3 above the diagonal it takes
P to the 2/3 steps to get random and J
above the diagonal it takes P to the 2
over J steps to get random and that's a
conjecture I can't prove that but I'm
pretty sure it's true ok so that's a
little bit of what I mean by features I
hope that's ok and now I want to talk to
you a little bit about what I mean by
this question so for a moment let me let
G be any group G is any group finite
group say
which group I like the Heisenberg group
and suppose that Q is the probability Q
of G is a probability on G then and I
defined convolution just by this recipe
etc so so I do the random walk generated
by Q and you can ask you know suppose
that it's you don't have parity problems
and that you're you know living living
on a generating set and stuff like that
so suppose that Q star K of G converges
to the uniform distribution and then you
can ask how fast it it occurs and one
way in which I've studied those problems
is to use Fourier analysis so so a
representation of G is a mapping that is
a mapping that assigns group group
elements to matrices glv with the
property that Rho of s T is equal to Rho
of s times Rho of T and so you assign
matrices to group elements in such a way
that products are preserved in that way
and the Fourier transform at a
representation q hat at rho is by
definition the sum over G of Q of G the
weights times the matrices so it's it is
it's a matrix the Fourier transforms the
matrix and as usual Fourier transform
takes convolution into product so Q star
Q hat at row is equal to Q had it Rho
squared and the uniform distribution has
Q no Q is uniform if you had a TRO the
Fourier transform the uniform
distribution is easily seen to be zero
if Rho is non-trivial irreducible
representation is called irreducible if
you can't break the vector space up into
two disjoint parts such that such that
the representation only takes you into
one part anyway and and is equal to one
if Rho is one of the trivial
representation and so the way you can
study this convergence of to uniformity
is by showing that high powers of this
matrix converge to 0 and the upper bound
lemma makes that precise and it goes
this way four times the distance of Q
star k2 uniform so the this is this
total variation distance is less than or
equal to the sum over irreducible
representations of the dimension of the
representation I'll try to explain this
x times the the size of the matrix Q hat
Rho to the K and this is squared and
this is the trace norm trace more so
okay that's that that's and and and this
V here this V is zero dimensional so
that's a bound that says if you if you
have your hands on how fast these things
go to zero then you then you can bound
this so this is a completely general
recipe and you know here's a this is
perhaps this almost the simp perhaps the
simplest non commutative group so if you
like that kind of stuff it's very
natural why can't we do it on this group
what's wrong with that what happens you
know what happens okay and it's a famous
hard problem and and I found out why and
you're gonna hear why okay that's but
what kinds of things happen so in order
to talk about that I have to tell you
what the irreducible representations of
a heisenberg group are and they're easy
so the representations of the heisenberg
group and i'm gonna tell you for the
unit Jers mod p just it's not hard to
say it for any group if you want to know
ask me after that for any it's hard to
say for any group if it's not hard to
say for any end but it's a little more
complicated and let me tell you what
what they are when p is a prime and so
there are P squared 1 dimensional
representations dimensional
representations characters and there are
P minus 1 P dimensional representations
and one of the facts of life is that if
you sum the squares of the irreducible
representations that sums to the size of
the group and P squared plus P minus 1
times P squared the sum of squares of
the irreducible repeat you write P
squared plus P minus 1 times P squared
is P cubed that's true so ok so what are
the representations the one-dimensional
1 representations they're their index by
pairs a B mod p and so XY Z this is so
remember representations a linear map a
one dimensional representation is a
linear map of a one dimensional space
into itself
that's just multiplying by a number and
this number here is e to the 2 pi I a X
plus B Y over P that's familiar those
those are the you can see that if you
you know multiply this and multiply
these product is the product it's ok the
P dimensional representations
there they go like this they're on a
vector space so the vector space I'll
take you can take all the same vector
space V is the set of all functions F or
column vectors F anyway F from the
integers mod p into C ok so it's just
that's a space and I just have to tell
you how x y&amp;amp;z act so here row and this
there's only one parameter that comes up
C of X 0 0 now that's a linear map of
this space into itself so it has to take
a function into another function so it
acts on the function f at the argument J
as f of J plus X so it just translates
ok but that's ok just shifts it's a
cyclic shift if you like it just shifts
the vector around the the next one row
sub C of 0 y 0
that's what underlies the Fourier
transform this axon f @ J as it's e to
the 2 pi I see over n times y J as a
multiplier times f of J so it acts
diagonally and that that's how it acts
and z ax Rho sub C of 0 0 z @ f of J
just acts even a simpler way it's e to
the 2 pi I see over N times e times f FJ
ok so so if you put it all together
Rho sub C sees a nonzero integer mod P
of X Y Z well it's something you know
it's it's it's e to the 2 pi I see over
N times they all combined right so Y
times
don't tell me times J and plus Z times f
of X plus J this is acting one f of G
okay so that those are the P dimensional
those are the P dimensional
representations and there's one for
every nonzero n is P you all knew that
right in this P is that I mean that's
you can just check that that they they
obey this this rule and that's not hard
to check and so now what is my Fourier
transform become so here remember my q q
is you know i pick one of the two
diagonal elements there plus or minus
one and i know so here Q hat Rho sub a
be the Fourier transform well let's just
you know it's the Fourier transform with
the one dimensional representations a
that's just well it's 1/2 cosine 2 pi a
over P plus 1/2 cosine 2 pi B over P 2
by P over P so it's just that that's the
that's the Fourier transform and and Q
hat at the P dimensional representation
it's a matrix and it's not a bad matrix
it's got a quarter in front it's got
ones just below the diagonal
it's got ones just above the diagonal
it's got a 1 here and a 1 here and then
on the diagonal is cosine 2 pi C j / p 0
less than or equal to j less than p
minus 1 okay so it's a it's that matrix
what's and one of the jobs that there is
going to be which was new to me is i had
to get my hands on the eigenvalues of
this matrix and so if anybody's seen
that matrix before or knows anything
interest
about it I'd be happy to hear about it
and so let's say what the job is in
order to bound the Fourier transform I
saw I remember my job was to try to use
this machine to bound the rate of
convergence of the walk so I have to
bound I have this calculus problem so
this is a matrix it's a symmetric matrix
so it has real eigenvalues the
eigenvalues are beta 1 of C beta 2 of C
beta I don't know P minus P of C the
real eigenvalues some some real
eigenvalues so you know if you write
that thing out I have to bound this this
should be Rho not bro not not the
trivial representation so I have to
bound this sum there's a sum of two
parts the sum over a and B not equal to
0 0 of 1 1/2 cosine 2 pi a over P plus
1/2 cosine 2 pi B over P to the 2k power
that's that's the sum over the one
dimensional representations plus the sum
of C equals 1 up to P minus 1 of well
this the dimension of the representation
is P and then the sum over J equals 1 to
P of beta sub Beta Beta of beta J of P
to the 2k power that's they should have
been yeah this this is a norm squared so
this is the sum of squares of the
eigenvalues ok so just I'm sorry but I'm
gonna make you look at this for a second
I know it's not fun to
get anybody else's calculus but I'm not
going to do very much of it and see see
if you can at least look and see what
the job is so one term in this sum is
when a is 0 and B is 1 that's a term so
when a is 0 this is 1 cosine of 0 is 1
this is 1 half plus 1 well this is 1/2
plus 1/2 cosine 2 pi over P to the 2k
power and that's one term in the sum
well cosine near 0 is 1 minus x squared
and so this is 1 minus 2 pi over P
squared over 2 squared something like
that
to the 2k power and so in order to make
this one term small K has to be of order
P squared I mean that's if K is 10 times
P squared this is like e to the minus 10
and so okay that and then in the usual
way that's all the trouble that is the
only other terms are smaller and they
all add up and you found this sum by
straightforward analysis as long as so K
has to be of order P squared so good I
need to know about these numbers and I
need to know about them with some kind
of reasonable exactitude and I will show
you coming a little bit beta J see is
you know certainly less than or equal to
1 minus an explicit number over P so
something like that is true and and so
this sum has P terms in it and so you
have to and and this sum has P terms in
it and so I need to choose
Oh Lord so that P terms P terms and then
there's a P so it's P cubed times one
minus constant over P to the 2k power
well you know if K is of order P squared
which I need to kill the linear terms
then this is tiny so that doesn't cause
any trouble so sorry for making you look
at it this calculus but that's what it
would be involved if you were able to do
that so I was left with the problem of
trying to bound the eigenvalues of these
matrices now by now I'm very good at
bounding eigenvalues of matrices and I
just thought you know it's a
tri-diagonal matrix how bad can it be
what what's that gonna do you know it's
not gonna cause any trouble but then I
realized that all the tools and tricks I
know or in probability language and
about stochastic matrices
oh well shouldn't be so bad you know
this cosine can be negative I'm sorry
cosine can be negative hi so and then
the rows don't aren't don't have a
constant sum and so I didn't know what
to do and and I tried some things and
they didn't work and so I tried some
other things and I want to tell you that
but before doing that because that I
want to give you some more motivation so
why would anybody care about these
matrices out there famous matrices so
more motivation so just to make it
simple let me try to take C to be one so
M M is equal to M one and that has
cosine 2pi over P or in I'll make PN now
doesn't matter times J down the diagonal
so it's this matrix just that diagonal
okay so those matrices come up here's
one place they come up probably most
people in this room know what the dis
Fourier transform is so the discrete
Fourier transform matrix FN has say JK's
entry e to the 2 pi i JK over in over
square root of n if you want it to be a
unitary matrix that's a--that's a n by n
matrix 0 less than or equal to J and K
less than n so and there are teams of
electrical engineers who want to know
about the eigenvalues and eigenvectors
of the discrete fourier transform matrix
is a fair-sized literature on that why
do I care about it well it turns out
that because of what the Heisenberg
group does for a living
this matrix commutes with them so so f M
F inverse I don't know if M is equal to
M F and I'll put it I'll put I'll put it
here and what that means is that they're
both symmetric matrices that means
they're simultaneously diagonalisable
and I thought ah I'm in luck these
engineers are going to know all about
the eigenvalues of the discrete Fourier
transform matrix and therefore since
they're simultaneously diagonalisable
I'll know a lot about the you know I'll
know a basis but I don't know how to
diagnose him and well ok that sounds
good it sounded good to me
but alas not a last but anyway what's
true the Gauss showed this but the fast
Fourier transform you know it's its own
inverse right so after the fourth is the
identity and what that means is that the
eigenvalues of this matrix are plus or
minus 1 plus or minus I notice it's then
that it's a unitary matrix so it has
eigen values which are roots of unity
and and the dimension of the eigen space
are around n over 4 within 1 so so
I do get a reduction of em you know
because em preserves the eigenspaces of
the Fourier transform matrix but it's
not helpful and there is a lot of work
as I said in the in the engineering
literature about various decompositions
you know eigenvalue decomposition for FB
but because the bases are so non unique
they weren't usefully related to em and
I I couldn't use it on the other hand I
now have very very good approximate
bases I can value decompositions of em
and they do decompose the Fourier
transform matrix and that seems to be
interesting so that's one motivation
okay for studying this this matrix a
second motivation comes from physics and
so the so this is Harper o hops that are
and martinis teenies martinis not
martini it's not a person it's the
alcohol so there's a very large
literature in solid-state physics about
about periodic the Schrodinger equation
with periodic potentials to just say a
simple version of it say on l2 of Z
square summable sequences of length C
the Schrodinger operator is just this
operator with a periodic potential takes
so I'll call it I don't know L of V is
an l2 function and J is equal to V of J
minus 1 plus V of J plus 1 plus cosine
theta J plus ada fee of J so that this
is the analog of a second second
derivative operator and they usually put
a constant in here of V and and if you
make if you want to compute anything you
discretize this operator and look at it
mod n and these are exactly my matrices
I mean that so this these are slight
shifts but that doesn't change anything
that is these these matrices if you
discretize it or take periodic boundary
conditions or my matrices and there is
enormous both applied numerical
theoretical work on what's the spectrum
of this operator the if you want to get
famous
Arthur Avila just won the Fields Medal
one of his accomplishments that's listed
is called a ten Martini's problem that
was a problem of Mark Katz who when a
talk of this sort said well I'll give 10
martinis to anyone who can solve this
problem and it was to to show things
about this is an infinite operator to
show things about
about the absolute continuity of the
spectral measure and well people like
Barry Simon and many other people have
written lots and lots of papers when
this is a general parameter when V is
two which is the case that we're doing
we're dealing with this is called
Harper's operator or the Hofstetter
worked on it in his thesis and if you
type in hops debtor's butterfly you'll
see lots and lots of references and
looks at what the spectrum of the
operator looks like I won't try to say
that more I can but there's a lot of
interest in the eigenvalues of this
matrix in the physics community and in
the solid state community so I won't I
won't say more about it unless unless
asked okay so I need to know about the
eigenvalues of this matrix so you know
this is in 2014 so take a look at the
handout here well this is this side with
the pictures so this is when M is 200
and the parameter a is C so this is
what's the biggest eigen value of that
matrix when when the parameter which I
was calling C and is here called a
varies in its in its range so what you
can see is that for example when when
when the parameter is you know 1 the
eigen value is very close to 1 and and I
said it's 1 minus a constant over know
the the parameter across the x axis is
let me write it down Thanks
the parameter the the the the matrix has
cosine 2 pi well I I called it C times J
over p 0 less than or equal to j less
than P minus 1 down the diagonal and the
parameter that's across the x-axis is C
okay so for each C this is a matrix and
it has a top eigen value and then
what's pictured here Opie is 200 which
is in prime but that doesn't matter
P is 200 sorry P is n so P P is is 200
so here it's called M that's the size of
the matrix is 200 so what you can see is
that the eigenvalues are pretty close to
1 and then they fall off they're not
monotone no unfortunately they are
symmetric and that's not hard to show
and okay that's good now in order to use
these bounds I also need to know the
smallest eigen value the one that's
closest to minus 1 and that's what the
smallest eigen values look like and
these look like mirror images of one
another but unfortunately that's just
what they look like they're
approximately mirror images those two
dots for example no but they're they're
not exactly mirror images and so I also
had to bound the smallest eigen values
and and so forth and III I'll come back
and maybe and talk about some of the
eigen functions this is the first
eigenfunction the eigen functions are
localized and they're there they're very
peaked around zero and and this this V
equals 2 which is the case where in is
the critical case if V is bigger than 2
then the eigenvectors are not localized
and when V is less than 2 there are
localized and here they also are
localized but but which you can see
they're mostly 0 but they're they're
kind of very peaked around around well
very peaked around 1 ok so you can look
and then you have to prove something
eventually you have to prove something
III just looked on the table and I won't
but it's a good time for a minute so
under showed me this I can't hand that
around because he'll kill me but this is
the Heisenberg group a portion of the
Heisenberg group with those generators
makayley graph at the Heisenberg group
and those of you who are interested can
come up and take a look at it
under inspection so okay it's I think
from a 3d printer and I never saw such a
thing so I'm thrilled thank you
but one can one can look okay so I want
to talk to you about about the how you
bound eigenvalues of of non stochastic
matrices and I'm afraid there's a joke
about this you know there's a joke about
a physicist and a mathematician and I
don't know somebody else and you know
the point is the mathematician goes back
to cases he knows and proves things by
induction right so I'm gonna go back to
what I know so I have this matrix my
matrix and I'm gonna I'm gonna
it's got cosine down the diagonal so
it's not positive so I'm gonna make it
positive and I'm gonna make it some
stochastic so I I'm gonna let em I'll
work with em one
I'll just call it in but then I'll I'll
just add the identity now it turns out
good to add a third the identity plus
two-thirds of em okay so once you add
the identity in two-thirds of em that
makes everything non-negative and the
row sums less between zero and one so
has non-negative non-negative entries
and the row sums
okay so you just easy easy to check that
and then I can make it into a stochastic
matrix by just making it an absorbing
Markov chain so I'm now going to make a
Markov chain K which is like this
here's infinity here's 0 1 up to well n
minus 1 and here's 1/3 the identity plus
2/3 m and then here are some numbers
which I'll call a wave 0 a 1 up to a n
minus 1 which just make the row sum up
to 1 so let me try to explain that well
let's say what they are a J is equal to
1/3 times 1 minus 2/3 cosine 2 pi J over
in so it's just what you need to make
the row sum up to what the identity if I
did it properly to make the row sum up
to 1 if I did it properly so what is
this this is a stochastic matrix all the
rows sum to 1 all the entries are
non-negative I added a site to the space
infinity and this is a Markov chain but
if it hits infinity it dies so this is
absorbing it infinity if you hit
infinity you stay there and the rest of
the time you could go to infinity this
is the chance of going from J to
infinity that's what this first row is
so ok I reduced I made the problem into
something that's friendlier to me it's a
stochastic matrix now of course this
matrix is a stochastic matrix so it has
1 eigen value which is 1 namely yes and
so if I can bound the second eigen value
with this matrix that'll be the top
eigen value of this matrix and then I'll
be in business so the way you do that
I'm gonna call I'm going to call these
this set of states s and
s bar is s Union infinity in case that
comes up and I'm gonna use I'm gonna
bound this by the minimax
type principle but I associate a
quadratic form with this matrix and it's
a little bit tricky it took quite a
while to get it right the quadratic form
is the darris life form EF F F is a is a
vector column vector and it's just equal
to this it's 1/2 the sum over X&amp;amp;Y
contained in in s bar of f of X minus f
of Y squared K of X Y K of X K is this
matrix and this is a symmetric matrix
and this part is let me write it down U
of U of X where these are Dara schlagen
functions so f of infinity equals U of
infinity is 0 but of course K k XY you
know KJ infinity is positive and so this
any way that that's the quadratic form
which is which is useful and needed and
what you can show the the usual
characterization of eigen values in
terms of quadratic forms shows that if
we can find a bigger than 0 such that
the l2 norm squared of any function f is
less than or equal to a times the FF
this usual way of you know the usual
thing is the eigen value is the
quadratic form divided by the length of
the vector and so and that ratio will be
bigger than 1 over a that is equivalent
to or implies anyway that that the top
eigen value of this matrix which I'll
call beta beta
is less than or equal to 1 minus 1 over
a so if I can if I can bound the
quadratic form I can get a bound on the
eigen value I want and now we use I use
this path method but here that it's a
little bit different because of this
infinity and that's a new thought to me
we need pants and for gamma X which take
any X and connect it to infinity so this
is X naught which is X then X 1 X 2 up
to X D which is infinity and they have
to be paths in the graph that is I need
K of X I minus 1 X I to be bigger than 0
so I I need paths connecting any X to
infinity ok and the way we use that is
we write f of X as well f of X minus f
of X 1 plus f of X 1 minus f of X 2 etc
plus f of X D minus 1 minus f of X D the
point being everything cancels out and
this is 0 so then that's just an
identity and and then use Koshi Schwarz
that's less than or equal to the length
of the path the number of terms in the
sum times the sum of squares f of X I
minus 1 I don't know minus f of X I
squared and then that's like this and
then you fool around in that way that is
a standard way of fooling around read
you've all spoke with David lovin and
Elizabeth Wilmer paths arguments and the
bottom line is
using the geometry of the graph you wind
up proving this kind of a bound you can
take a this a which works here to be the
maximum over edges X Y where X is in s
and y is in s could be infinity that has
to be allowed you know 1 over K of X Y
this chance of this path times the sum
over Z such that XY is contained in the
path of associated to Z and the length
of the path so that that you can prove
that kind of a bound and then so you you
want a to be small turns out in order
for this bound to be useful you want a
to be small and so a will be small if
you can choose paths in the graph which
take you from X to infinity with the
following property none of these things
should be too small so that that's
important and and you shouldn't have too
many paths that use a given edge because
you don't you don't want the sum to be
too big and so that's a kind of thing
that we do is come in a Toria lists and
and I'll just say sentence about about
that but then I'll just stop so so
here's so here's infinity lots of things
connect to infinity you can go from any
J to infinity so there's infinity and
here's you know here are these points 1
up to n minus 1 and if you start here at
the beginning this thing is very close
to zero
it's 1 minus cosine right even to the
side I don't I probably have these
numbers wrong but this thing is very
close to zero so you don't want to do
that so what you do is you you connect
points to infinity by going from here
over into where it's nonzero and then up
to and
and this point you go to here and then
up to infinity this point you go to here
and up to infinity you do the same in
the opposite direction you have to
choose a break point and you go here and
then here and then here and then up to
infinity points in the middle you
connect directly to infinity and what
you can see if you choose pans that way
and do the combinatorics you know this
isn't too small and therefore one went
over it isn't too big and this isn't not
you know at most four paths use an edge
and the lengths of the paths are not too
long and so you can control this if you
do this carefully you you get that a you
can take a to be of order a constant
times well for for this case a constant
times one over P to the 4/3 now that's
more than enough for the eigenvalue
bound this is a so that oh yes yes yes
C times P to the 4/3 and so the eigen
value band u gets beta is less than or
equal to 1 minus some constant over P to
the 4/3 and and that's not what I used
before but it's it's more than enough to
do to do the job you know I like that so
I'm gonna stop here except to say it
time flies and it's about it is about
time and over over time we actually know
for these matrices I actually know what
the top eigen values are at least in the
corners and so actually beta is equal to
1 minus PI over 2 P plus Big O 1 over P
squared and and actually if I put in a C
it's C over P as long as C is small C
fixed so I was able are we our crew was
able to relate
the eigenvalues of this matrix to the
eigenvalues of the harmonic oscillator
which are very well-known and and use
use that relationship in order to bound
these eigenvalues unfortunately I need
to know this not only for C fixed I need
to know what for all C and these cruder
geometric arguments work great
they don't give quite as precise results
but but they do give they do give good
answers so I want to finish by just
saying I I started on this talk for two
reasons I wanted to know is Fourier
analysis useful and if not why not and I
wanted to know about you know the
distribution of the center argument well
I can't answer you know this is pretty
hard work is all I'll say it really we
really have seven proofs of the fact
that N squared steps are necessary and
sufficient for random walk on the
heisenberg group so i you know did i
need to do it this way no is the answer
but still it it shows you know what what
the difficulty is having done this work
it is important to say there are lots of
other groups where the fourier transform
has exactly this form and so more or
less any class to nilpotent group the
the fourier transform the high
dimensional fourier transforms have this
form and so knowing about the
eigenvalues of these matrices there are
a bunch of other random walks some of
which our previous techniques didn't
apply to that that we can that we can do
well it is also we're saying that our
sharp these sharp eigenvalue bounds are
much better than what organic theorists
and Arthur Avila got on the problems
that they care about they didn't
actually care so much about the extreme
eigen values although they did care
about them and they got we we got much
sharper bounce and so so that's good so
I hope that that you know what I what I
was up to trying to do my work on the
Heisenberg group and I hope that was
instructive for you sometime thank you
trying to expand this a little bit other
the other matrix yeah this Sarah was the
editor for our super character paper
bless you
yeah I didn't try I just we just decided
I was gonna do this one and I didn't
know I mean I will I will eventually try
but one of the problems is for the for
the group of n-by-n upper triangular
matrices nobody knows what the
irreducible sore ends in some sense you
can prove that nobody will ever know
they're wild problems and for this case
you know I said well here are the
irreducible 4 by 4 ok 5 by 5 ok 6 by 6
and then it stops nobody knows what the
irreducible czar I think 4 7 by 7 and we
don't know the characters we don't know
the conjugacy classes so we did manage
to make new and better and easier to use
super character theories using these
ideas and that so I would like to I
would like to do more about that but
just decided I wanted to do this problem
how hard could it be it's a tri diagonal
matrix it shouldn't be so bad well okay
I've scars to prove it
wasn't so much fun can't find a
counterexample that's a good question so
see if I can explain that so the the
claim is that so the the these are the
scoops you and p FP or which are ones on
the diagonal star in FP and you know
what I what I claim is that I'm gonna
try to convince you that there is a
proof that you can't describe the
conjugacy
classes now of course for any fine I in
it's a finite problem and leave me alone
okay but I mean there's not you know
like the conjugacy classes of the
symmetric group I'll explain as a math
thing but at the conjugacy classes or
the symmetric upper index by partitions
and with you all that's okay right
the GL in you know these are nice groups
they're the ceelo P subgroups of GL n I
mean they're not bad groups so okay so
let's see I proved that if you had a
nice description of the conjugacy
classes by a by action you'd have a nice
description of what are called wild
Quivers so a quiver is just a collection
it's a directed graph okay and the
representation of a quiver is a vector
space at each at each place and at each
vertex and a linear map nothing has to
commute just okay two representations
are called equivalent if you can change
basis and make the linear maps actually
the same okay so if they're equivalent
up to change the basis and the problem
is classified quivers were they given
you know representations the Quivers so
these are familiar problems if you have
one dot in an arrow leading into it
that's classified
linear maps of a vector space into
itself up to change a basis okay that's
the rational canonical form that's good
okay we know what that is here two dots
that's you have two matrices in your
allowed you have a linear map and you're
allowed to change bases arbitrarily
there's only one invariant which is the
rank okay but still that's nice okay
quivers which in which the
representation type is indexed by an
integer like the rank are called finite
title Quivers in which the
representations are indexed by a finite
collection of real parameters or complex
parameters those are called what do they
call that finite type team that thing
and there's a trichotomy theorem which
says any quiver is a finite type tame or
wild and just so you see this some
content that sounds crazy but a
quiver is a finite type if and only if
it's an orientation of a Dinkin diagram
that's a nice theorem that Gabrielle's
theorem okay so I showed that if you had
a description of the conjugacy classes
you would have a nice description of the
a wild quiver this one two two two
arrows this is classify pairs of linear
maps from a vector space into another
vector space up to change a basis so
this here we have a pair of matrices and
the say say you have a pair of matrices
and you want to classify them up to up
to change of basis and there's no okay
now there's a theorem that says if you
had a nice description of wild quivers
any wild quiver then you'd have this a
unsolvable word problem that you could
solve so it's much worse than PNP it's
you know it's up there so the complexity
and now to try to get a finite
quantifiable version of that I'd love to
do that and I keep trying to find the
tame model theorists the theorem I just
told you about that there's a this
equivalence class
it's on page 300 of a book called
modules and March module modules and
model theory and unfortunately there are
299 pages before that zero but there is
a theorem that says that if you had a
nice description of wild quivers you
have a and and here it's very easy to
say what you do given a matrix you you
just you embed you embed to do two big
blocks in it and and conjugation
conjugation you know describing the
conjugacy classes of such a matrix you
can easily see is equivalent to
classifying pairs of maps from one space
to another so I hope that gives a flavor
for it and
but uh if you can ever make more sense
out of it I did spend a week with the
catcher intent who's a model theorist
trying to make a quantifiable version of
it I don't write but in some sense a
rather you can prove it
there we have a paper with Richard
Stanley and area areas Castro which is
called random walk on the upper
triangular matrices mod p and it has all
this literature and the proofs
everything everything's written down
it's about ten years ago we'll never get
out of here I'll answer any question
right so good use estimates from no
right euclidean space like every cell of
cost and then back right that's one of
the proofs we and in order to go from
infinite down to finite we needed Hornak
inequalities but they exist I mean and
actually this guy Alex op Allah says
this for any you know any nil potent the
screen nil potent group he has you know
the right her neck and equal so that's
one way of doing it now that's you know
that's bringing in a lot of hard work it
just seemed to me you know here's this
poor little group Fourier analysis it's
tridiagonal matrices how bad should it
be I mean that's what I was trying to do
it's an exercise I said it's an exercise
I'm just trying to do an exercise and it
got me show you
so I there you're proof with Alan sly
you know you did in by n I I forgotten
whether you did general P but probably
did you know that is your all's a
beautiful proof of the rate of
convergence on for a friend by n
matrices showing that you know P squared
is enough based on a very clever
foundational but but I mean there are
really a lot of different proofs this is
just it's a straightforward approach you
could try it on any group you know the
other cases they're special tricks were
using structure remain I wanted to try
it that is just it was really a homework
problem which
turn into three papers that's what can I
say each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>