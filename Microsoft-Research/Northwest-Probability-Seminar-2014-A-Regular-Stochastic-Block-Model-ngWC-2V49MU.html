<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Northwest Probability Seminar 2014 - A Regular Stochastic Block Model | Coder Coacher - Coaching Coders</title><meta content="Northwest Probability Seminar 2014 - A Regular Stochastic Block Model - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Northwest Probability Seminar 2014 - A Regular Stochastic Block Model</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ngWC-2V49MU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I'm pleased introduce our next
speaker Jana Demetrio from University of
Washington where she is associate
professor in the math department she got
a PhD at MIT in 2003 with Alan Adelman
whom we heard about in a proceeding talk
after that she was a miller fellow at UC
Berkeley until two thousand six and then
came to get up she's worked in various
fields probability perhaps most recently
but also numerical analysis common of
torques and the near algebra and she's
going to tell us about a regular
stochastic block model Thank You us for
the very kind introduction and I'm sorry
but I'm gonna have to correct you it's
not the same Alan Adelman I mean it's
actually not Alan Adelman it's somebody
else Adelman green is that what you were
yeah it's a different item seems to be a
popular name all right so it is my great
pleasure to be talking about an ongoing
problem that we're working on at u-dub
this is joint work with my students
Heron de Brito and xu shiiin do Ganguly
and with my colleague Chris Hoffman and
for part of this we were joined by lin
truong who was a postdoc at u-dub up
until this year so we're talking about a
regular stochastic block model oops I
guess this one or not
ah alright so I'm going to give you a
quick overview of what I'm gonna tell
them what I'm going to tell you then I'm
going to tell you then I'm going to tell
you what I told you so I'll give you a
quick introduction to the field actually
it's not so quick it's probably going to
be somewhere between a third and a half
really of the talk about independent
edge binary sbm which actually will be
partly described in the introduction as
well and then I will tell you how you
change that definition to make it
regular and I will talk about the issues
that arise why certain things becomes e
become easier why certain things are
slightly harder but overall the problem
is much more tractable it seems and then
i'll tell you about current and future
work all right so let me without further
ado talk about the clustering problem
this is a very well studied problem some
would say that is being studied to death
but it's still not solved completely so
it's still worth talking about in fact
it it's interesting to people the idea
is to input a network with clusters with
some sort of properties pro possibly
overlapping although I won't be talking
about this today and the idea is to be
able through some algorithm to detect or
recover these clusters accurately and
efficiently I'll talk about what it
means to detector to recover in a moment
but it has applications in in many
places machine learning community
detection synchronization channel
transmission and so on and there are
many questions that are still open and
quite subtle especially in the case when
overlap is possible which again is not
going to be the case that I'll be
talking about today there's a huge body
of work as I mentioned and it overlaps
many subject many fields optimization IE
theoretical computer science and math
there are two approaches that one can
take to studying this problem one of
them is to study actual networks and in
the try and see if your algorithm that
actually can classed can detect the
clusters this is Zachary's net
he's a the network that described the
interactions between members of a
martial arts club that suffered at some
point a division and some people went
with the old instructor and some people
moved to the new and essentially this
one this picture describes how the
cessation took place this is a group
that went with our other state with
instruction and this is the group that
seceded this is a network that's been
really studied to death it's used as a
benchmark for pretty much any algorithm
oh look my algorithm can actually
cluster has a cure is networked
correctly and so on the other approach
is just focus on studying idealized
models of networks like for example once
produced by order shiny and this is also
known as the problem of studying
spherical cows that's what we're going
to be doing so here's the stochastic
block model or sbm for short also known
as the planted partition model if you've
seen that in the literature essentially
it says the following thing is very easy
to describe you have to consider que
eres training graphs with sizes and I in
probabilities P I independent and non
overlapping and aside from that you can
put a on the join of their graphs a
multipartite and by multipartite I mean
basically that any two vertices that are
in non adjacent sorry nan not in the
same cluster get joined by an edge with
probability Q okay so you have some sort
of cluster some sort of K clusters and
then on the outside of them you put you
put a multipartite erdos-renyi with with
some given probability the question then
becomes under what sort of conditions on
all of these parameters when this should
have been actually a small case k sorry
can one recover detect the presence of
the partition recovery
here is understood as being generally
it's understood as being complete
recovery although sometimes weak
recovery is considered where you expect
to recover all but sub linearly many of
the nodes detection means that you're
able to say if you believe that there's
some structure like this at work or if
the if the network is just random okay
so the possibility of recovery has been
study generally used via the maximum
likelihood estimator and convex
relaxations thereof recently though
there's a new approach which is
described by using multiple structure
multiple structure matrix algorithms so
you think of the adjacency matrix as
being sparse plus low rank sparse being
the outside plus and the low rank
actually identifying the clusters so
being essentially a matrix that
corresponds to instead of clusters just
taking clicks the same notes so this is
one of the references there vinayak way
mark and Ibaka city there are more
general the most general analysis so via
impossibility with impossibility of
authority information theoretic bounds
plus a complex relaxation for the
maximum likelihood estimator is fairly
new what if it's actually very new it
appears in a paper by challenger from
2014 and it gives various order order
sharp bounds so that means that they
don't get any constants but the order of
the thresholds is found for various
regimes the only case that has so far
really been solved in terms of various
thresholds is the two equal cluster
binary case and one can say that most of
the work has been done by an aluminum
pneus of the Microsoft Research Group
and that's and his two students well 2x
student
should say so I'm referring to Ellen and
muscle and to journeyman and Alan's line
okay so here's the dictionary terms I
will talk about a strong recovery regime
when it is possible there are algorithms
that will give you the partition
completely weak recovery regime means
that you don't get it exactly but you
get essentially up to sub linear many
nodes labeled correctly and others may
be mislabeled there's an approximation
regime where you get a fraction of the
nodes correctly write the fraction is
bigger than fifty percent but the resume
be mislabeled then there's a detection
regime where you can guarantee to get
just above fifty percent of the nodes
correctly labeled but you can't quantify
that you can say how much better you can
do and then there's the impossibility
regime where it is impossible to do
better than guessing essentially and
generally it's because of
indistinguishability I'm not even sure
that that's a word indistinguishability
reasons where your model is essentially
indistinguishable from that of an elder
trainee with twice as many vertices and
the according justit probability so then
the nomenclature in the field is varied
and it seems that everybody uses their
own preferred words i'm using this which
is kind of a combination between muslim
and sly masoli and AB AB the abbey I'm
not sure how to pronounce his name
bandura and Hall I should be able to
pronounce his name because I know him
but it's embarrassing so this is a
nomenclature I hope you'll be able to
remember it so this is the exact
definition for the binary stochastic
block model you start with 2 n nodes and
you pick n of them to be labeled 1 and
the others minus 1 uniformly and
independently and then you add an edge
between vertices with the same a label
with probability P which sometimes can
be an in fact it's going to be a
function of N and then outside of the
art so you'll add labels so you add
edges between vertices with different
labels with probability Q
and you can call the resulting graph
model g2 NP q I guess I'm going to have
to use both of these somehow sure okay
so let me talk about briefly about the
strong recovery regime you will need to
have P and Q at least logarithmic in M
because otherwise you will get with
probably with essentially with well
depending on the constant with constant
probability will get isolated vertices
and those you cannot classify there's a
bunch of people who have worked on this
problem and have made seminal
contributions and it was solved almost
completely by AB and E Ryan Hall in 2014
and completely by a Missoni when his
life a few months later in other words
if you have this regime plus some tiny
conditions I guess which i'm going to
show you in a moment you can do complete
recovery so this is the state of the art
and when you cmn su it's going to be
short for most animals like you'll see
that a lot in this talk which is why I
just felt like I could short-handed so
the state of the art is a rather complex
characterization but certain cases can
be made more explicit so for example if
you think of P n nqn as being roughly
constant times logon / and although the
constant can fluctuate a little bit then
strong recovery is possible if and only
if this happens so in other words if
this number here or if the sequence of
numbers here is strictly positive at all
times then you can do then you have
strong recovery and it's necessary for
recut strong recovery to be at least
non-negative okay this matches the a
banty Ryan Hall results for but but they
weren't thinking in terms of letting the
constants fluctuate and therefore this
is this is what they got they didn't get
this yes
you say strong recovery is possible that
include a statement about computational
complexity so it's a little thing so AB
Madeira and Hall were showing that this
is possible for slightly different
conditions it seems that the algorithm
that msn has come up with is almost
linear in the number of edges so that
would suggest yes it's not there's no is
that what you're focusing on yeah or
just no no just a distance a possibility
of strong recovery yes yeah but it seems
like that's the case I think that they
will probably do a complete analysis
impact on was going to say something in
a little while but it seems like that
then I be the case so there's a weak
recovery regime in which you can recover
up to sub linearly many of the vertices
and this is again msn 2014 a menace
sorry 2014 and it's a very nice result
which essentially says that you have to
have the probabilities slightly better
than a 1 over N you want them to go to
infinity and pn + NQ n have to go to
infinity and you have to have this extra
condition attached so again not not not
sub logarithmic you cannot so you can
you can have them sub logarithm sub
logarithmic and you cannot because of
that you cannot hope for full for strong
recovery that's why you're and and it
seems that the reason why you do not get
or you in a sense what you cannot get is
caused by the fact that some vertices
may be mislabeled in a sense that they
might actually be labeled one but have
more connections to the wrong set to the
other set okay so this is the polymers
oh I'm sorry this should be a plus sign
yeah it's just it should this this would
be an identity yes
it says this is a plus apologies okay
then there is an approximation regime
which actually was first studied by
Kwajalein i hope i plan on pronouncing
that correctly who showed that if you
consider p and q to be a over N and B
over N and you have this extra condition
for some large constant then you can
detect so bear with me I'm going to talk
about detectability next you can detect
the presence of the partition but the
fraction of recovered vertices is
bounded you cannot do better than a
constant fraction of the vertices which
depends which fraction depends on C and
then some other m and s from 2003
algorithm there are others that I'm
going to talk about soon use belief
propagation to show that distraction is
actually achievable so yielding thus an
approximation regime in which you can
get a certain fraction of the vertices
correctly when see approach is to the
constant approaches a half which is why
nobody expected that any kind of
approximation could be done in the lower
in the lower range ok finally detection
impossibility again most only one in sly
have been all over this problem so it
turns out that they had a challenger
mashulya at the same time independently
obtained a different proof for this but
they showed that when you have here in
this regime of PNG being over 1 over
sometime especially specifically a over
N and B over and then there exist
polynomial time algorithms to solve a
tube to find a correlated partition if
and only if this identity is true this
is a threshold that has been conjectured
by the cell crows acala more and Jabbar
ova n has been reinforced several times
since then in particular in 2012 when
Muslim and a slide showed that if you're
under that threshold then the graph is
indistinguishable from another graph
with 2n vertices and average the average
of the two oops the
should be an end there I apologize no
this there shouldn't be an aunt no
because a sorry it's correct and hence
reconstruction is impossible because if
you cannot distinguish you cannot detect
okay so complexity as mentioned it's not
it's not written in stone but it seems
that there's polynomial sigh at the time
strong or weak reconstruction belief
propagation is also efficient
detectability was shown to be polynomial
time by both groups who show
detectability so the bottom line is that
there seems to be no regime
reconstruction is possible but not in
polynomial time that's a very
interesting thing which because it runs
contrary to the widespread belief that
there are hard regimes if you have more
than two clusters okay perhaps clusters
that are growing or not linear linen and
it has a connection to the minimum
bisection problem which again is known
to be hard so it's kind of an
interesting yes Chris I'm sorry but they
got lost um if there's this graph that
is somehow randomly generated and
somebody wants to answer all these
questions what is the information that
that person has accessed the adjacency
matrix yes thank you sorry so you have
access to the net to the graph you just
have to test whether it's a large graph
you need to test if the model that is be
generated from yeah is is the one
described here
okay so let me introduce this notion of
a regular stochastic globe model so
basically we do the same thing except
that now instead of taking address
training graphs we take uniformly
regular graphs so for submitted for
integers n t1 t2 you take to d1 regular
uniformly regular random graphs of size
n and you connect them by about
bipartite N and D to also uniformly
random everything is independent of the
other things that you that you're doing
and of course the question is why why
would you do something like that and of
course the answer can be manifold struck
the structure in such a model is a lot
more rigid can you say a lot more than
you could before can you do recovery in
other kinds of cases can do recovery in
the so called lower regimes when when
you have em p and q smaller there's also
edge dependence how does that affect
things because you know in before
independence actually was playing a
pretty heavy role in the calculations do
you use the same methods or you come up
with others and of course lastly because
it's their wynnum we're in a math
department okay so the first thing to
note if you remember I mentioned that
there is an impossibility regime for the
year restraining stochastic block model
and that is that if the relationship
between the two probabilities is of a
certain nature then you cannot
distinguish essentially between your
model and just a bigger address training
model with the average of the
probabilities here you can always
distinguish between this model and just
a you random uniformly regular d1 plus
d2 graph your model is also of that type
so if you if you look at the fact that
you have to D 1 regular joint
I ad to regular that means that that
graph is actually d1 plus d2 regular
however it's a very different
distribution than that so you can you
can tell basically and the reason is
that if you count the number of grafts
that you have in in both set this
exponentially smaller so so this is not
here's where things diverge for the
first time of course unfortunately
distinguish ability has no computational
value what we would like really is to
prove uniqueness of the partition so in
other words if you generate your graph
like this you would like to know that
there's basically no chance that it can
also be expressed in the same way with a
different partitioning of the two
classes of vertices okay then you can
call hope for recovery if it turns out
that you don't have uniqueness no choice
no no chance of recovery okay so that's
that's what one would like now of course
we've made progress toward this kind of
very interesting progress because we can
show uniqueness Wendy 2 is less than T 1
but for huge sizes for huge sizes of d2
the interesting stuff happens when d2 is
small so that's why this is very partial
progress and but we're not going to stop
here the idea is roughly to improve work
to improve our results on this lemma
which is the overlap Lima and which to
which I'm going to refer again which is
that if the partition exists if the
second partition exists if you want in
some case then the smaller swap set must
be large so if you have two such
partitions in your graph then the the
set of overlaps must be really large so
in other words you can just swap a few
vertices and hope that you get a second
partition we're working on improving
this from a ping the fraction from one
over to d2 to one half if
can show that obviously if we can show
that the you need to swap at least in
one half then that's or other one half
is a barrier because it's a smaller room
that you set it's very easy to explain
why this happens so suppose that this is
your left let's say d1 regular graph
this is your right d regular graph and
suppose that a second partition is
possible which essentially swaps the set
of vertices in B to the set of vertices
in C then it turns out that and it's a
trivial actually observation that if you
pick a vertex V in be the number of
connections that it has two vertices in
it has to be the same with a number of
connections of vert with two vertices in
D because you're about to swap things
and this has to be also the new swapped
thing has to be still d1 regular the
problem is that the one and the two are
different and we know that we cannot
have more than the two connection fact
he has to have precisely digital
connections to see Union d so it cannot
have more than D two connections to d
however if the set is small the chance
of getting a vertex who's all d-1
connections are to a rather than to be
insignificant and at that point you
would have to have the same degree to D
but d one is bigger than the two so you
can't do it okay so this is this is
essentially a good heuristic for the
argument of why why you need while you
have to have uniqueness of the partition
in such a case okay so I think I just
explain this then we have and this is I
think the only proof that I'll show I'm
going to zoom through the rest of it
there's an easy spectral regime the fact
that you are working with a d regular
graph allows you access to a whole host
of very interesting simple linear
algebra properties so as a consequence
of this properties you have the
theorem which says that if the
difference between the two degrees is
bigger than this quantity here then the
second largest eigenvalue of the
adjacency matrix is d1 minus d2 the
first eigen value of the adjacency
matrix is d1 plus d2 because this is a
dirty one plus two regular Gras but the
important thing here is that the second
largest eigenvalue is d1 minus d2 with
multiplicity 1 and with eigen vector
corresponding to the correct partition
so in other words if you have the
adjacency matrix if you are in this
regime then basically you just do you
find the second eigenvector and you're
good to go that's going to give you the
partition okay and also the consequence
is that the partition is recoverable
this is due to the fact that the
multiplicity here is one if you were to
have two distinct partitions both of
those would have to be eigen victoire it
would have corresponding eigenvectors
with eigenvalue d1 minus d2 however the
multiplicity of this eigenvalue is one
ok so the partition is unique and
recoverable and it solves them in by
section problem for this model like I
said the problem the proof is really
simple is just linear algebra these two
are facts you can check them then you
can split the Jason sorry I'm thinking
if people are falling asleep in the
audience they could spend some time
actually checking it's split the
adjacency matrix like this so now you
have to D 1 regular graphs and then this
is the bipartite d2 part these are jason
c matrices of random regular graphs and
this is the random adjacency matrix of
the bipartite regular graph and now you
just do spectral analysis on them
essentially so you calculate this and
bounded from above actually I should
have probably put an absolute value here
but it's a symmetric matrix I should
have put an absolute value
this is what happens so if you look at
an vector that's orthogonal to the first
eigenvector and the second eigenvector
then the value that this quadratic
intake is at most this and this is
strictly smaller than t1 minus t2 by the
condition how can you show that
essentially you split the matrix like I
said and then you make an gross
overestimate essentially you split it
into these two parts and you use the
fact that at least for the shoot this
should have been an IA one apologies
this should have been an a.1 so remember
that they want is just the part consec
is the part corresponding just to the 2d
one regular vectors do you want regular
graphs and on each one of them you have
due to Friedman a very nice result on
the second eigenvalue you bound that and
you get that the overall bound is to
square root of d1 the same thing holds
true for the bipartite graph it has been
shown that a similar bound on the second
eigenvalue is true by puter in 2013 and
therefore you put it all together and
you get you get this it follows
therefore that the second eigenvalue is
d1 minus d2 with multiplicity 1 etc etc
etc so this is an easier a shimmery
spectrum that's not the only case in
which you can get strong recovery in
this model this that's just an easy
regime and then actually we've we've
done that because it's easy to explain
but it turns out that essentially using
the methods of muscle year we can
adapting i should say the methods of
mass well yeah we can find this theorem
so if the difference between the degrees
squared is bigger than t1 plus t2 then
the partition is strongly recoverable in
polynomial time now mind you musta had
detectability so this is strong strong
recovery much much much stronger
essentially the methods are
adapted to work for the case where you
have a different kind or almost
independence of edges because you want
ND two are fixed and you can use the
configuration model for the uniform for
the uniformly random regular graph but
it's it's not exactly trivial work
however you get a lot more so he Mele I
used the matrix of self avoiding long
walks in contrast with the M&amp;amp;S strategy
of using the matrix of non backtracking
long walks the difference release that
in the second case the entries of the
matrix are bigger than the method is not
spectral where is the method method of
mass VA spectral so we used that because
it was more accessible to us and this is
all I'm going to tell you about how to
prove it I had several lemmas actually
um that I was hoping to show that I'm
obviously not going to have time for and
the idea is to do a local analysis you
have to show that no cycles are closed
and of course this is known for the de
regular graph it takes a little bit more
work to show it in this in the context
of this model where you put regular
graphs together then and this this holds
so so cycles are not as close as C log n
so cycles are far apart and then there's
of course the connection between path
structures and labels of the
neighborhood which in our case given
that we have a regular graph is very
simple to establish because we know what
the neighborhoods look like we know
exactly what the neighborhoods look like
whereas in the case of Irish rainy it's
still an element of randomness whereas
here it's an exact count and finally the
the true important ingredient is to do
the following thing you have to show
spectrum separation of the first two
eigen values of this matrix the matrix
of self avoiding walks of length L from
the rest of the spectrum so the first I
value and maybe I'm going to show that
lemma so if you show spectral separation
of the top two eigenvalues and
essentially you will get a partition
that is correlated with the correct the
original partition except in our case is
not just correlation in our case is
allows you to recover the original
partition up to n minus little oven and
then you can correct the blemishes by
majority rule okay all of them and then
you get complete recovery this is pretty
standard pretty standard technique okay
I'm going to just show you this so this
is the the core of the argument really
so you will show that the graph is
jungle free with high probability and if
that's that's true then the following
estimates are true for these two
quantities so e to em remember is the
vector of all once and Sigma is the
vector that gives you the signs if you
look at what happens here you see that
these two if you scale them down by
square root of em they will become unit
vectors both of them and so this n here
here and this n minus Big O of n to the
Delta where Delta is small over here
will essentially disappear and what
you're left with is telling you that
this is essentially going to go to the
first eigen value of SL which is going
to be d1 plus d2 to the l and this is
almost an eigenvector and similarly this
will go to what will turn out to be the
second eigenvalue of this i said which
is d1 minus d2 to the l and this is
close to an eigenvector you can actually
show that this is polynomially close to
an eigenvector provided that you can
show good separation from the rest of
the spectrum so if you can show that for
any other vector that's orthogonal to
these two guys then the same as teammate
each is much much smaller then you're
done and it turns out that that's true
it turns out that if you look at unit
vectors that are
to those two then the estimate is much
much smaller notice that here you have a
d1 plus d2 to L over 2 you essentially
want that to be smaller than d1 minus d2
to the l and that's what gives you the
condition ok finally then there's a
question of ok so we know that the
second eigenvector is going to be giving
you the partition up to sub linearly
many vertices which are going to be
incorrectly labeled but you could have
two partitions for which both for both
of both of which have the property that
the second eigenvector does that well no
because in that case the two partitions
overlap very much more than we've shown
is sorry overlap very the swap the swap
is very small and we've shown that the
swap has to be relatively big this was
the last ingredient ok so this is just
the recap we showed that strong recovery
is possible upon Romeo time and we
believe that recovery is actually always
possible because uniqueness that's what
uniqueness tells you that recovery is
always possible this is a rigid model so
if you have the graph you could in
principle just test all the vertices of
course that's not efficient but you
could you could test all possible
pairings and see if it works so that's
that's very different from from before
and the question is is there a threshold
for the complexity here so we've shown
that recovery is possible in polynomial
time in a certain regime we believe that
when we believe that will be actually
able to show that recovery is always
possible and because so this threshold
is given by the method we have no idea
if it can be pushed down just yet but
we're working on it but it's also
possible that this this is actually an
efficiency threshold
perhaps lower than this you cannot get
polynomial time algorithms and of course
then the idea is to generalize to
multiple clusters and I'm going to stop
here thank you yes they have a question
methods it's not mathematical um I'm I
understand that these results are
customs talking so on in some sense they
should call for large networks okay so
I'm thinking about something like
facebook but in facebook when i try to
think about communities i would guess
that there's actually a very large
number of community else not just too
yes we are a small finite number of like
this and there's overlap as well yeah so
basically what are um good practical are
multi baiting examples of a large
network which which on this page to
either to or a small number of community
that's why i talked about the vehicle
house what was that is why i talked
about spherical cows yes in general
you're completely right so the idea is
to get results that are asymptotically
true for many clusters for overlapping
clusters for clusters of different sizes
and so on and there's a whole body of
literature on that no thresholds so
thresholds that are perhaps at best
order thresholds so so we started off
with this example because this there
there's hope here that one can analyze
it completely higher than that probably
not however I am actually working with
Miriam fazil and with a couple of our
students on a problem just like that so
we're actually making some interesting
progress generally the algorithms that
will produce the clustering will be
so some some sort of complex relaxation
of Emily and it turns out that insert
regimes they perform well and the
question is what are these regimes and
generally the conditions that you get
involve all the parameters in the in the
problem so they're horrendous so then
you have to start saying okay let's see
if the clusters are equal what does that
mean in terms of the probabilities
associated to each cluster if the
clusters are very separated how can i
play the probabilities to get an
impossibility regime and things like
that just what there's one relevant
example of course the the RAF model is
you know is a very rough approximation
but clustering into two is something you
want to do all the time for instance
just live differentiate legitimate
websites from spam websites so that
would be two clusters and there are lots
of links between your legitimate
websites and each other and lots of
artificial links created between the
spam website and also some links that go
between these two groups but those may
be different nature and this can grab
structure is the basis for using you
know effort you know one of the tools i
should say for this kind of distinction
oh no it's very depict the real picture
is very different from these beautiful
models yeah so that's I so actually
there are some real examples there are
lots of real example we want to
distinguish between two so that part is
will these you know random graph models
are approximate I mean you mean in that
case you don't really have to cluster
right because the spam may consist of
many different spam plus ones yeah right
looking more not nut cluster point even
ya been waitin you want to make a binary
you want really hot iron practitioners
right and so the grass not necessarily
with equal weight or anything young yes
what are these counters almost with d 2
equals to look like just so I can get
some
My dear um you you can find
counter-examples essentially the idea is
that 44 d 2 is equal to 2 then you no
longer you don't have connectivity so
you have you can have long cycles of the
same length and you can essentially swap
those you can construct a soil yeah
you're saying it to think it's a serum
that for any deer I kill across right do
you I think we found oh yes yes yes oh
there's a theorem that says the
following thing with if d is greater
than or equal to 3 then with probability
1 aaas yes the graph is connected no but
I mean fit but what you'll what you
strongly suspects is true you strongly
suspect a mystic theorem but if you have
a d regular growth yes so like I said
you can actually find examples for the
24 d 2 equals to 2 where where the
probability of encountering a two-part
ish by a graph that has two possible
partitions is not zero it's not it is
not ask you about the problem I'm sorry
I may I guess I don't want to see is
that normally it does a partition
forgive you there Oh what's the question
is just a big hug are you saying you
suspect that it's determinist be true
that any d regular graph not
deterministically true i think that it
is true with high probability you can
always find it like for example if you
think of a square ok so you have n that
n is even so you just put in over 2n
over 2n over 2n over 2 and then you just
put their you put I guess G over 2 or
something like that regular examples and
then between them bipartite over to buy
part ID over to in this case d 1 is
equal to t2 but you can imagine other
cases how it works the idea is these
with copies but these were
with ya yeah so you know there's no
there's no reason to believe that this
is deterministic I think that it's with
high probability so but with vanishing
probability do you expect to encounter a
second partition a graph that has work
sorry I didn't understand you either
last questions good is that going on
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>