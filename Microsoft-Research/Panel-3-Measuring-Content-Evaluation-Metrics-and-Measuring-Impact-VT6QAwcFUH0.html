<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Panel 3: Measuring Content: Evaluation, Metrics and Measuring Impact | Coder Coacher - Coaching Coders</title><meta content="Panel 3: Measuring Content: Evaluation, Metrics and Measuring Impact - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Panel 3: Measuring Content: Evaluation, Metrics and Measuring Impact</b></h2><h5 class="post__date">2016-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VT6QAwcFUH0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">uh I'm great board from esos eran I know
many of you but not all of you were kind
of a repository that hangs out in social
sciences and humanities and strongly
support open access that's all I'll say
press this run thank you to Microsoft
digital science and all of you for being
here Amy asked me to put together a
panel on metrics and I said yeah maybe
not really sure I want to do that and I
said can I do it a little bit
differently and she kind of played along
kind of playing the straight man and I
said I don't want to do one where we
talk about the difference between a LMS
and altmetrics and gaming and all that
kind of stuff that bores us to tears
especially right after lunch so she said
my gosh the title of this workshop is
shake it up so do whatever the hell you
want so we're going to try to shake it
up you know Margie said that there's a
lot of different ways to think about
metrics and think about the way that
they're used and think about the way
that they actually entered into
conversations to maybe be a little bit
helpful instead of dogmatic or the Grail
that a lot of people start talking about
them and what we're gonna try to do
right now over the next session is give
three speakers three very knowledgeable
people from different perspectives a
chance to share how they see metrics
working in the world today so Mike
Taylor from elsevier will start off with
WTF and kind of challenge the way we set
and think about metrics and ashley will
come in from the funder perspective and
talk about you know what is the impact
of the money and give us some real world
examples and really kind of the stories
that play out for the funders and these
nuanced stories and ways in the ways
that metric can help us see something
that may be traditional numbers don't
and andrea is going to bring it all
together at the end with some real-world
case studies about how different people
not just ten year committees but
different people in the real world use
metrics to make some decisions and what
we're learning from
now I will first off say that we're as
Mark said that he has the easiest job by
basically just coming and showing up and
letting somebody else present for fig
share I have the second easiest job
which is to stand up here for a minute
and actually have really smart people
tell you things that may question the
way that you think about metrics so the
goal for our session is to leave you
with more questions than answers but we
will have some time for Q&amp;amp;A at the end
so maybe we will get them answered but
without any further ado like I could
afternoon my name is mike taylor i live
in a world which is constantly changing
up until two weeks ago I worked for
elsevier labs and then I moved to the
new and gigantic elsevier metrics team
it's so big that we could all sit at a
table there there are three of us and
we're charged with steering the metrics
and how elsevier computer metrics what
those metrics mean and how they
displayed this is going to be our task
for the next few years and as of this
morning I'm delighted to announce that I
started doing a PhD in the subject as
well this is just kind of rounding out
my life really really busy KY already
have three careers so i've been working
in metrics for three years and i've been
building up couple of really enormous
datasets most of which comes from
altmetric calm and one of the things I
really want to observe about this is
because I am altmetrics number one fan
um but any mistakes that are appear on
this any conclusions that you don't like
Greg asked me to say things people going
to disagree with so I'm going to say
that but I'm only human when it comes to
these things now as I said I work for
elsevier metrics and the tasks that
we're working with is to challenge the
way that we communicate metrics the
three of us golly Halevy Mike Taylor and
myself and uh sorry Mike Taylor Lee
callowhill EV Lisa collagen myself we've
all got experience of working in
collaboration I work with the Ultimatrix
people I've been hanging out at
conferences and writing papers for the
last three years lisa has been working
on a project called snowball metrics
which is an open academic LED scholarly
metrics for comparing institutional
behavior
so we have a real commitment to open
metrics methods it's really important
for us that we do this because we can't
go back into a place where there are
black boxes that pull out numbers that
general genio generate our views and
interpretations of what's happening in
the skull d well that can't be
challenged and it can't be understood
can't be manipulated and reproduced by
other people so our commitment to open
methodologies is a really really
important thing we believe in agnostic
data we even have a little manifesto
Lisa I wrote this pretty good document
the hef key statement on metrics which
is about 90 pages long there are more
elements to the manifesto than just the
ones that I've got up here but
nevertheless this is the kind of thing
we're talking about don't expect us to
be producing numbers that plane to
answer all those things because we live
in a world where we're changing to an
open environment where people are
talking about science and research in
ways that they have never had
conversations before and the resultant
of this is that there are different
people using different kinds of language
there are people asking different kinds
of questions that take us away from the
narrow very narrow wells that academia
has had for the last 40 years and we
have all known we've all witnessed what
has happened when you try and take one
number and expand it out of all shape
and use it for reasons and we're talking
about the wonderful journal impact
factor something that we can still be
used for a variety of things but we've
got all kinds of opinions about it
generally speaking use for its narrowest
sense it probably does the job one of
the great things about 40 years of
research is that we can clarify those
caveats we can say what we mean when we
talk about it but what we can't do is
then go and say well this means that an
institution or researchers work is
valuable door but let valueless it's
being distorted out of all kinds of use
so our commitment is to move away from
having one black box metrics to having a
series of open metrics I would rather
have our say for example three metrics
on a subject that go to eighty percent
of answering the question that are clear
and validated and crispy understood by
everybody who needs to use them
rather have those three and say to you
the people who consume our metrics and
use these things that this is how you
may use them it's up to you to come to
the conclusions about it I'm not going
to tell you whether what the answer is
to your question but I'm going to give
you the tools and the analysis and the
data and the validation to make sense of
the numbers so we have this little
background where the impact factor
another citation based metrics are being
used to answer questions that they were
never designed to answer and all of a
sudden a few years ago literally the
best thing the world happens altmetrics
rolls into town and it is literally the
most exciting thing that I have ever
read about in scholarly communication it
promises so many answers for us it gives
us a completely new perspective than
insight into the way that scholarly
communication is being committed to the
world it get answers all sorts of
incredibly interesting questions and I
have spoken at length on the art items
in the manifesto with apologies to
anybody who is involved in writing it I
had nothing to do with it are merely
read it I consumed it I translated it I
agree with most of the points in it I
don't agree with everything but I agree
with most of the point in it these are a
wonderful set of ambitions to have when
we are thinking about metrics what can
we do with the new data that's coming
into vault metrics but I think I need to
pause because I'm going to be really
mean about altmetrics and you really
have to remember that I am its number
one fan we have all these observations
all these hopes that are embedded in the
manifesto and yet when it comes down to
looking at the data from the last three
or four years and interpreting in an
understanding it there is a really big
gap between what we had hoped to see and
what we are actually seeing these are a
few of the conclusions that I have
reached from my very large data sets we
hoped that Ultimatrix would give a
broader picture of the universe at the
moment it doesn't we hope that it would
discover hidden publications and at the
moment it doesn't in fact if you compare
the performance of citation based and
metrics against none site none formal
traditionally alternative metrics we
that it's pretty narrow it's focused on
some very narrow subjects and it doesn't
give us the breath and the width and the
richness that we're hopeful but this is
a question of at the moment this is the
last three years so back in 1976 my
teacher said some pretty bitter things I
think my first report in high school was
pretty bad reading it's still pretty bad
reading but it's pretty much the same
for altmetrics it's got a lot of promise
it's not smart enough it doesn't answer
the right questions these are things
that we need to work on to expand to
really get to see some traction going on
in our alternative metrics we take it
really seriously in the elsevier but at
the moment it's it's a lot of promise
and here is if you like the problem that
I have I'm going to get back to that
slide from here's the problem that I
have because on the one hand we have
citation based metrics and we have 40
years of research and citation based
metrics we really understand what's
going on we understand disciplinary
differences we can go and buy books on
the subject we can read papers there are
people doing phd's in it there are
departments university departments
working in bibliometrics and we don't
have the same thing going on in our
metrics we have a few PhDs we have a
load of papers you know we're really at
the beginning of the understanding and
people being really mean to citation
based metrics are really really horrible
and it's because they've been distorted
out of shape but at the same time we're
getting away with awful things when it
comes to alternative metrics because we
just don't have the data points we don't
have the background we don't have the
research we don't have the books we
don't have the papers we don't know that
the funds to justify some of the things
that are being said about altmetrics at
the current day so this is the thing
that I have recently learned in the
course of one of my other careers my
theatrical career and is that we know
that there is difference between
measuring a thing and the thing itself
this is something that Heisenberg on the
left and bought in the middle and
discovered in the 1920s and anybody who
is an astrophysicist Rory energy physics
can meet me later and beat me up for
reading the script but what they said
was that there is this gap you need you
need to understand that when you measure
the part the spin of an electron that
what you're me take what you're
measuring isn't the actual spin of the
actual electron it's a it's a proxy for
the real thing what we need to do when
you say these things the responsibility
for the researcher is to make those
caveats to speak to their results but to
point out where the weaknesses are
measurement is always a proxy for the
real thing and this is one of the
criticisms that we read of the impact
factor that it is a proxy for impact
it's a proxy for quality people say
these things about citation based
metrics and they rarely say them about
alternative metrics knowing that
something is a property doesn't
understand ermine the the essentials of
scientific discourse but it places the
onus on the researcher to make the
conclusion to to say what it is they're
talking about to argue their position to
validate what they're saying about their
metrics so as people who are involved in
metric for we collected the data
responsibly we make sure that there
aren't people who are lying about data
or fabricating data we do really good
statistics with it we can make the
conclusions we say the numbers but when
we say those numbers it is up to us as
good scientists as good communicators to
say these are the caveats these are the
weaknesses this is the error of Prague
building and these are things which are
not being said about alternative metrics
at the moment and they are being said
about citation based metrics so when we
talk about bibliometric impact
previously it's only been the people who
are involved in bibliometrics that talk
about these things right so so if a
bibliometric on says I are the impact
factor they're talking when expert to
another they using language which is
qualified by the discipline it's in the
context of a particular kind of
discourse and as a consequence they
don't have to have the same kind of
explanatory discourse that we do when we
start talking about some of the issues
that arise as an alternative metrics and
a classic one for me is the case of
social impact which is something that is
used a lot right
so we can talk about all these citations
and we can talk about impact factor it's
got enough capital I and a capital F and
it's got a definition equation and it's
got references but we can't at the
moment use the same conversations to
talk about alternative metrics because
we have to explain everything and in any
case the data really isn't big enough
for example um in the year up to June
the first there were two and a half
million tweets that refer to two papers
which is a very very big number um
that's what just over one tweet paper
right to point to yeah there's 2.2
million papers published years that's
just over one tweet / paper and turns
out that almost ninety nine percent of
ninety percent of papers don't get any
kind of Twitter mentions so two and a
half million tweaks then go into a very
small number of things and you you end
up with a mean of about five and a half
tweets per paper that actually gets
cited but there's this really huge gulf
we don't see that big gulf and when we
come to talking about citations we don't
see ninety percent of papers not being
you know consumed or read or excited
it's the differences here are really
important when we start thinking about
making conclusions about what data means
so what can we say about two and a half
million tweets well we can certainly say
that they're really focused I'd wrote a
paper with Andrew plume also of elsevier
last year we were looking at the kinds
of paper that get the really big pieces
of impact we were expecting I'll be
honest we did the research expecting to
find that there were really trivial
papers there were papers about sex drugs
and rock and roll that we're getting all
the tweets turns out its climate science
and vaccines but you know okay nevermind
good paper find something else out right
but we talked about people talk about
there are papers that talk about tweets
Twitter being in some way analogous to
social impact and yet we're only seeing
any indications of this form of social
impact in about twelve percent of papers
and even when we do that it's a pretty
meaningless level of pretty meaningless
meaningless level of activity in my
opinion at least for most papers yes we
look at the top end of those papers
we're seeing great rich data
the most most publications don't get
anything and those that do get virtually
nothing so it's kind of ready
to talk about anything meaningful coming
out of this other than knowing the
topics that make people tweet and as
soon as we step away from our tightly
little world of altmetrics or
bibliometrics and we start talking about
social impact to real people they don't
have the same definitions though
bibliometrics do there isn't a narrow
there isn't a generally accepted
definition of what social impact is
certainly if you if you're a banker or
if you work in our not-for-profit then
there are definitions that people use on
social impact people try to do this I
mean there's there's a whole level of
our publications that are being made to
try and explain to define social impact
is in very narrow areas but there's
nothing at all in terms of in terms of
altmetrics and very little in terms of
bibliometrics so there's a real gap here
because people want that answer they're
asking is what is the social impact of
my research they're asking us all sorts
of questions so take take for example
how we might use different models to
think about in this case the value of
extending a life I made most of these
numbers up but I would defend them over
a pint and so this is a metric that I
created to thinking about why where you
spend the money to invest in extending
somebody's life by ten years and there's
the same numbers but taking a different
kind of perspective and I thought this
was quite an interesting thing to do
because and we've got two different
models both of which might be defined as
being social impact but actually giving
completely different answers using the
same data and I think this is the same
kind of thing we're driving at that when
people talk about social impact in what
scholarly what scholarly communication
is after we don't have a consistent
clear view of what it is to do that and
there are a number of problems getting
in the way of us being able to answer
these an answer these things he says
skipping through his notes really
quickly um another example would be on
internationalization so if we think
about our an international a metric to
say whether someone how some to produce
a number
on how internationally focused
researcher is we need lots of different
data and we can ask many many different
questions we could say well is this
person the question might be how good is
this person at getting money from
international funding agencies the other
question might be does this person spend
all the time flying two hot spots in the
winter like Cambridge what kind of
person is this what's the question you
want to know when you're thinking about
what internationalization means do we
want to know whether they're being read
in countries other than one the writing
and do we want to know whether they're
engaging with researchers in the
developing world these aren't questions
that are embedded in the data we need to
take those questions and do a gap
between how we are going to go about
answering what does a hypothetical
answer look like what else do we need
other than data to answer these
questions where I can get the data from
how good is that data and actually
that's the how good is the data how
complete is the data is a really
important issue for us because I'm know
we have this world where in our citation
data we have 40 years of research so we
have an idea of how complete something
is but when we started building when the
community started building on metric
tools what they weren't things
themselves well what date do I need to
answer this question rather they were
saying who has an API and how can we
query it and if you have an open API
that was free and the license terms were
unconditional and you could answer that
you could answer queries based on the
day away then you will be included in
that big box of numbers that we get so
Mendel a was in and Zotero who I didn't
have an API at the time or it's just too
hard to work was out and other papers
like other software solutions like
papers don't even met cut at all because
they don't have an API that's open so we
never actually really know how complete
how open the data is so if we try and
make it qualification we're saying yes
but I don't know what the answer is you
know we say that this is a highly
impactful paper on mendeley and we don't
know how it performs and other things
and we know that there is probably a
disappearing just difference between
mendeley in Zotero but we don't know
what that difference is yet so it's very
very hard for us to come to any kind of
conclusion to to qualify the data that
we have to answer the questions that
being asked
this is why I want to personally press a
big stop button and think to ourselves
about the problems that lie ahead of us
altmetrics alternative metrics is such
an exciting place at the moment you know
we have new platforms coming on stream
we have more people talking about things
we have data coming up we have young
people doing PhDs we have old people big
doing phd's it is a really exciting
place to be involved in but the problem
is right now is that somebody might take
us seriously um and this this would be a
really big problem if somebody woke up
tomorrow morning and said right I am
only going to find papers to get 5
tweets or I'm only going to fund
research for people who can get 5 tweets
I mean it's like crazies but this isn't
like citation where you can look back at
40 years with them history and say ah we
know something about this discipline
this is something which changes every
year there aren't the benchmark against
which we can make comparisons of
performance we have to create these
benchmarks we also have to expand the
amount of data we have a great deal of
work to be done we have to advocate for
people to open up their data we have to
ask places that don't have AP is to
publish auditable data about usages so
we can make comparisons between
different applications so we can draw up
a full view of what the behaviors going
on and we have another problem in terms
of working from our top down and saying
well what questions do you that does the
community you want because it's not up
to me as an individual or me as elsevier
or any other individual person or other
corporate entity to say these are the
questions that we think that you want to
answer you is our number to answer this
question rather it is the question of
what what academics have questions
Dean's have questions funding agencies
have questions and these questions need
to be defined and worked on understood
the data needs to be gathered we need to
advocate and argue for the data to be
open though there are so many questions
to answer i am going to shut up and
about a minute but there are so many
questions or one of the questions and
that I find so interesting is when you
look at an analysis of data we can see
that our issues in math right they make
the headlines everywhere in the social
media yet they don't appear on the TV or
the news or the radio at all I don't
understand
there is this massive gaping hole about
math in the mass media that's really
hard to say after lunch why is
aquaculture so popular in northwestern
USA I can't imagine but these are
questions that can be asked but at the
moment we don't have the tools to answer
them we don't even have the language to
ask by them so this is why I want to
press the stop button and ensure that at
least I have a conversation that I know
what I'm talking about for the next 15
years because frankly that's what my
career's going to be doing thank you
thank you um I just realized while
sitting there what my purpose here today
was and it's to be a buffer or
potentially an interlude between two
altmetric presentations yes and to
provide a little bit about the Thunder
perspective I'm Ashley with her research
we work with funders globally very small
funders that fund less than a million
dollars a year in very focused rare
diseases all the way up to the largest
usual names NSF NIH and we work with
them during the review process to find
reviewers and make other decisions and
then also from a portfolio perspective
to help answer questions like what
should we fund so it's our work that we
do with funders that is that is
represented here and it is by no means
any answers it's it's more of a problem
statement and a a call for help
generally speaking when it comes to two
metrics so what I would like to share is
a little bit about the questions that
are being asked today by funders when it
comes to metrics and an impact and then
share with you a little bit of the
perspective of the gap to use the mind
the gap that we've seen that exists in
that need so a few a few quotes that
that we heard recently but their
representative from what we've heard
from many funders which is for the type
of impact that they want to measure
publications for them don't cut it
however if you connect it with what we
just heard that's really the only data
that exists which is mostly true so how
do they deal how do you deal with this
this gap and also a statement of one the
realities which is not only is data
needed and tools needed in common
languages about what is impacting all
that but also just represent here a
shift in culture and awareness that of
the need and to get people thinking
beyond that the usual
the obvious and the tradition and what
may be slightly ambitious when again you
think about the type of data available
not only do they want to look beyond the
impact within research and how you can
measure that with outputs but also want
to dip into health impact and social
impacts and economic impacts so not only
our funders like everyone else short on
data when it comes to research impact
but then want to bring in all these
other areas and start answering
questions it's huge but that's also
where many funders are going and I use
the word health here and this was a
quote from a health research Thunder but
you could put in from any funder and the
type of research that they're coming so
an example of the type of questions that
are being asked by funders that are
unanswered again the problem statement
this is one example of a research to
impact framework and what's interesting
is there are many funders out there who
don't even have a framework like this
you'll see at the very largest funders
but in the small to medium sized funders
they don't have the time to even come up
with something like this what is what
this can help you see is the range of
questions and the range of metrics that
are needed that a funder needs they need
data about patient input but with
stakeholder engagement there's more and
more patient viewers but more data about
what's going on then in the green there
they're making decision about what the
fund which leads potentially to new
knowledge which is informing decisions
which hopefully leads to some
improvements in care reduce recidivism
all these things all of this area is
what they're looking at to measure their
impact every step of the way and some of
the questions that are within there and
this is probably for of thousands where
they're looking for for metrics for
indicators for answers for data what's
interesting is I thought I'd put it in
the
order of ambition where the easiest ones
easiest one came first and follow
through but which thing is most fun
almost all funders that we've ever
spoken to and worked with they can't
answer the question how many researchers
are in their in their state or in their
country or in their domain and have a
reliable number that they can use is the
denominator for certain metrics many are
focused on building capacity and
expanding field so they're asking
questions about who they funded and then
these more abstract questions are also
being asked which metrics and data to
inform those metrics and the tools in
the common language don't exist and in
that framework you saw huge ambition in
the type of questions they want to
answer and so many of them are still
stuck answering a basic question like
this which if you looked at that
framework kind of have a red box there
it's kind of small that's where many of
them are spending their time and don't
have a complete answer and that is just
one little piece of the impact question
when it comes to a funding perspective
so you talk about a group of
organizations that have huge need and
very little insight and well little
support when it comes to two metrics and
indicators another way to look at it
this is a list of actual indicators that
inform that that framework and that red
box there are the publication based
metrics and so if publications are the
only data set that is generally
available which is absolutely true then
how are they going to answer the rest of
the questions that they're trying to
answer and with so we talked about a
little bit those challenges to connect
it to the infrastructure question you
might say that that bridge is the
publication data and the rest of it is
completely wide-open some other
challenges some very realistic time
lines of what was your impact
they're asked well what was your impact
last year how if you can't ask if a
researcher has multi-year project you
can't answer them ask that question
every year how could you ask the funder
of those researchers but they are being
asked what was your impact in the last
12 months quite unrealistic and if you
look at this is a rough approximation
from hey I have an idea for a project
submitted to a an application to
eventually writing paper any citations
if you look at it from the perspective
above if you're looking at national
level funders of the political
administration's the political
administration's are wanting results
within a very short window of time which
is you wouldn't even be able to measure
the citations that came from the
projects that were funded at the start
of that so again very challenging
environment another challenge is also
just how to practically do this and so
sort of as a an awareness campaign their
school has started to help with the
challenge of research impact assessments
so for any of you when it comes to
metrics and impact in that general topic
it will be in its third year next year
it was held in Banff in the Canadian
Rockies wonderful location kind of like
what we have here with beautiful views
and they're also hosting it next year in
Doha Qatar which would also be a nice
place with my fuse but that gets to the
part of the challenge which is not only
is their difficulties in the data to
answer these metrics for all the
questions they answered but there's not
a common language about what the metrics
should be let alone the tools let alone
the capacity within the funding
organization to start measuring your
impact we we've worked with
organizations that are multi-billion
dollar funders and they have two people
who part of their job is to measure the
impact of the research funding and you
can imagine how difficult it would be
with all those challenges at clump so
again more of the the problem statement
of how what funders are facing when it
comes to measuring their their impact
much too much so my name is Andrea Haluk
and I am the co-founder and president of
plum analytics so what I'm going to talk
about are some real stories of how
people are using metrics to talk about
research impact so as Mike started off
talking about this you know
traditionally how is impact talked about
it's all about the citation count and so
if you're Thomson Reuters you have a set
of publications that they said this is
what we're going to measure with web of
science and then there's metrics a
citation based metrics that build off of
that data set if you're with elsevier in
scopus again they decided this is the
universe of what we're going to measure
that we deem appropriate to be inside
scopus and then there's additional
metrics that build on that and you're
getting just what was in that original
set from elsevier is deciding what what
is worthy to be measured Google Scholar
entered the scene and they have their
you know Google metrics and their
citation metrics and now they develop
metrics as well so when you take all of
those citation based metrics together so
this is a Venn diagram that looks at a
particular book and looking at the three
different providers of citation metrics
and saying you would need all three of
them in order to tell a complete picture
of just citation impact right so the you
know to get those 153 unique citations
you need all three tools to tell you
that because they're all sampling from
different places but it's a bigger
problem than that to tell this
comprehensive story of impact so if you
look at citation counts that was for the
longest time all we could measure that's
visible light right we could measure
visible light we can count it we can
talk about it but there's more that you
can measure here right the whole
electromagnetic spectrum as other things
that when you had basic tools like print
journals you can measure certain things
but the world is changing
and now I'm sure people have seen
diagrams like this before you know
what's happening online in 60 seconds or
this is data from 2013 and this is
talking about how there's 72 hours of
YouTube video created every minute right
the content is exploding the amount of
data that you can capture is exploding
and of course this isn't just consumer
based behavior this also happens around
scholarly output so here are some
sources of data exhaust of what
researchers produce or people
interacting with research produce so
it's more than just a journal article so
you put your dataset you share it a
dataset you can tell how many times that
datasets been downloaded you have a
video that you've produced it talks
about your new research approach or
educating patients or educating parents
of patients you can get views you can
see how much that's been interacted with
and this is just a sampling and we
harvest from all of these places and
anytime we build this slide it's
immediately out of date and we have to
go search new logos and you know add
more to it so this is an expanding
expanding field and it gets us to the
definition of something called
altmetrics so Mike talk about altmetrics
it's tweets and many people that is
synonymous so I they think of altmetrics
there's a company in the space a digital
science company called altmetric com
which also you know their definition of
what altmetrics is is is one what we
talked about with plum analytics is
really looking at all as in
complementing the traditional metrics
all that way and we honestly unless I'm
asked to speak about altmetrics we don't
even really use the term because we
really consider it all metrics being
trying to have this comprehensive view
that full spectrum of metrics is what we
endeavor to capture and we do that so
that we can answer the questions and
tell the stories behind research impact
and most notably that that hard question
of what's happened in the past year you
know the what have you done from
lately so citation counts take anywhere
from three to five years to get to the
point of where someone has done their
research published the paper someone
else's reddit done their research their
paper went through peer review and now
it's gotten your first citation count
that whole cycle takes years to get to
the other side of it so what's happening
in the meantime what can we use to tell
the story in a shorter point of view and
the way that we've looked at this when
we started gathering all that data
exhaust was to gather them and into five
categories first is usage so the first
and foremost I wrote a paper did anyone
download it I created a video did anyone
watch it this is if you ask researchers
what's the primary way you want to tell
if your work had any impact they'll tell
you this did anyone even look at it so
we're constantly trying to add usage not
just for journal articles but find other
kinds of proxies for usage like books
how many libraries hold a book is a good
proxy for book or monograph usage
captures our when someone is bookmarked
something favorited it put it as a
mendeley reader or somehow said I want
to come back to this and the studies
that are being done are showing that
captures are a good leading indicator of
what will be cited later and that
intuitively makes sense because if you
are using one of those platforms that
we're capturing metrics from and your
researcher and your bookmarking
something as you go back to build your
reference list where are you going to go
back to well wherever you captured it at
wherever you saved it so the data does
trend nicely with future citation
mentions are where the stories
themselves are hidden this is where
someone's written a Wikipedia page that
linked to that book or to that article
where they commented on it written a
review about it written a blog post
about it these are all minable in an
automated way to give the researcher who
produce that research a way to quickly
and easily find what people are saying
about it who are the key influencers
that it's interacting with and really
you start to learn what's happened with
their research social media love to hate
social media in many places right
because people think does this correlate
with anything else what we found is that
social media is a great measure of how
well promoted something is so I've
written an article I tweet about it my
followers retweeted often a millisecond
later without even having read it it's a
really great way especially for early
career scientists and researchers to be
able to tell are they doing a good job
of promoting their work it gives you
that lens into it and then of course
citations they don't go away their part
of this comprehensive picture and
especially as you look back five ten
years they are the best measure of is
was this quality research is this a
seminal article compared to other thing
I mentioned before about going beyond
just a journal article and this is the
sort of things that you can measure
these are all types of research output
being able to gather metrics about all
of these different things is another way
to tell that complete story or more
complete story and how that all kind of
boils down is into these building blocks
of article level metrics so for each and
every article video data set conference
presentation each one of these gathering
metrics around all five categories and
all you know the published version of an
article the open access version of an
article the you know the version in an
institutional repository gathering all
the metrics and being able to see what
you know being able to tell that
complete story and being able to
navigate through to see where those
numbers came from so making it very
transparent so in general before I jump
into the into the case studies I promise
there's three main steps that we go
through the first is to categorize so
you categorize the different types of
metrics right you categorized by the
type of output it is but you also
categorized by what you hope to later
get data about so being able to break it
down if you're an institution if you're
you know university you might care to
categorize things by Department by
researcher by lab if you're a publisher
it makes sense to categorize buy books
you're a funder you might want to
categorize by the the grants you fund in
the different areas in which your
funding them this if you put that human
intelligence in then when the platform
the technology goes and measures all
these data points you can then analyze
out at the other side so it's putting
that structure in that human
intelligence that lets you get get
insight at the end of the process the so
the first case study is University of
Pittsburgh what they do is a couple of
different things so this is an example
of a dashboard so you can see so I
haven't oh I do that's always exciting
when your presenter you don't know what
kind of kind of fun thing you're holding
in your hands so they have Reaper
researcher profile to give a dashboard
researcher by researcher what impact
have they had they've mapped in their
digital collections so this doesn't
necessarily work produced by their
researchers but the different subject
based collections that they hold being
able to look at metrics by subject and
across a whole the whole collection
their library publishes 26 journals so
we have article level metrics and
journal level metrics about the journals
and then of course on by Department
being able to get views by Department
and across an entire institution so the
first insight here is when we look at
this graph so this is looking at
articles published year by year by year
and saying what can we see usage isn't
on this chart but we do have social
media which is blue mentions yellow
captures and citations so if you look at
the red bars on this chart and I guess I
should point out this line across the
top is the total number number of
artifacts in a given year so that's
pretty constant year over year and you
can see the the citations for the last
two years and 2013 and 2014 the red bar
is barely visible I showed this slide at
a talk last week the week before and
someone raised their hand go you better
tell pit they have a problem there works
being cited in the past two years like
no no that's not it at all what it is
it's a citation counts lag they're just
not there yet but the other categories
of metrics the other things you can see
can help fill in that data to tell you
what's happening more recently so these
metrics provide a feedback loop like the
the Fitbit you wear on your arm or the
speed limit sign that adjusts and tells
you your speed on the as you're going
down the highway a little bit too fast
they're giving you data that you can
react to and much more real-time and
understand what's happening so here's an
example of an article in pitts
institutional repository so the article
level metrics for this include their
download counts here you know 75 times
that's been downloaded and this bottom
graph is what you would typically see
and then you know in an institutional
repository of what download metrics are
there but you can see that you can fill
in across the other categories of
metrics the other types of usage that's
happening all different ways to tell the
story about what's happening you click
on the the tweets about this particular
article what you can then see are the
conversations that are happening in this
case you know a potential funder is
tweeting about their work you can see
you know different parts of the world
interacting with it researchers
connecting the dots and this kind of
data when embedded in the in the IR is
much more of a carrot rather than a
stick this is you deposit here you get
value back out as a researcher and it
helps make that you know that feedback
loop and go full circle one of the other
things that you can see or I talked
about captures those bookmarks being a
good leading indicator so looking kind
of with a more publisher centric hat on
you can start to look at the different
journals head to head and say you know
look over a particular time period and
say how much are they captured and how
much is this leading indicators show one
journal versus another you can look at
you know social media / journal the same
kind of thin kind of things and drill
into the data from there then
next case that I want to talk about is
Autism Speaks and see if this auto goes
or not this is um typing autism into
google scholar right doing a quick
Google Scholar search what's being
pointed out here are the the publication
dates 2001 2000 2002 if you do any
general search like this on google
scholar you're going to be surprised if
you look at the publication date there's
in general nothing newer than a decade
ago that show up on your top results in
Google Scholar and the reason for that
is if you look at the decided by counts
that are excited by 552 things of course
if your relevancy ranking is based on
highly cited things you're going to
favor old things so if you are trying to
get the work you're doing exposed
there's a gap here so Autism Speaks used
plum analytics in order to tell their
community of what they fund there you
know the world largest funder of autism
research and they wanted a showplace of
saying you know they're out there
raising money twenty-five dollars and
fifty dollars at a time at you know it
walks and things like that they want to
tell their donors what they're doing
they want to be able to have their
researchers see what other researchers
in the field are doing so here this is
one of their one of their grantees and
you look at the publication year here
and all five categories of metrics or
something to tell there's data that they
can go in and explore and dig into they
fund different proposal areas and when I
first showed this graph to to my
colleagues at my partners at Autism
Speaks one of the interesting things was
they were very excited by this right
it's five bars showing their different
proposal areas biology treatment and
prevention screening risk factors and
dissemination and they immediately said
oh my god this is amazing it's like how
you know
like I see five bars I'm glad you like
our new reporting but what do you see
here and what they said was we always
thought this but we never could prove it
that this when we fund treatment and
prevention the gray bars is how many the
the ratio of the amount of social media
activity around treatment and prevention
is very high and if you compare that
with captures treatment and prevention
barely happening and for this data set
not a single citation and any treatment
and prevention article so they're saying
we know this has impact for our
community and now we have metrics that
that back up some of the funding
decisions were making similarly beyond
just the numbers being able to discover
those conversations that are happening
what are they saying and where are they
saying it is something else they find
important and to make this all public
that this site will be public and open
to the entire autism community and it's
something they're very excited and proud
to showcase what they're doing and talk
about ROI in a brand-new way right they
they've been talking about ROI of what
they fund the same way for the past 20
years they finally now have new data and
something new to tell and not just
Autism Speaks as funders in general last
case study really quickly is ohsu there
a medical institution and and one of the
things that they're doing is working
their library is working jointly with
the office of research to say we want to
compare like with like we want to
compare what we're doing in different
disciplines and there was no easy way to
do that so they took mesh keywords and
that ontology and created a custom
ontology based off of that that has
everything at the same level of
granularity went through and tagged
their researchers output which they then
can feed into a metric system like plum
X and be able to look in this case at
metabolic bone diseases compare all of
their researchers output to that and be
able to find well what are the what are
the publication outputs that have the
most impact and our own researchers
output around this very specific field
so it's taking again that intelligence
up front of how to struck
for the data of what kinds of questions
you want to ask being able to then have
the things beat all the metrics and data
points be automatically generated so
that then you can you can analyze it out
the other end other things that they're
doing are working with the expanded nih
biosketch to be able to include these
alternative measures of impact as a part
of their researchers proposals and
working collaboratively to do that as
well as really focusing on lay and
clinical outcomes and things again that
go beyond just traditional citation
counts to help tell that story and with
that I will I guess we're in panel mode
right okay so we've got a few minutes
left for questions we kind of took you
around the world there with how screwed
up everything is and the questions that
aren't answered to some actual case
studies that kind of show the ways in
which people are using the limited data
in creative ways to at least make some
comparisons and learn some things so I
will open it up for questions what so
that the early session everyone can I
had to answer the question about
platform lock-in and you know what are
you doing with this and so Mike Taylor
opened this session with a discussion of
importance of openness and transparency
for four metrics and I guess the
question before for both of you is how
are you there i guess you know
leveraging the power of openness to
promote you know community engagement
and trust in the data that you provide
sure i think the biggest thing is to
make it auditable right there's no
secret hidden formulas there's nothing
that we're not exposing the data and if
when possible giving a human readable
explanation to click through and drill
down so making it transparent as
possible is I think the best way to to
address them
so willing is asking me a question I
didn't mention that we provide data and
have data and I think that's what you're
asking about we collect data globally
about what funders are funding at a
project level that's the data that we
have and one way that we are making it
open and transparent and helping as an
example one of the many things we do the
day that we collect we feed it back into
orchid so if as a researcher you go to
orchid to sign up for an orchid ID you
can automatically pull in all of your
funding from our database so that you
don't have to mainly key in all your
grants which then gives everyone who has
access to all the orchid public data
files all of that records where it's a
person link to their funding as well as
their publications their employment work
history as an example great um Andrew
you're just mentioned something very
interesting in your presentation of
questions for you but if anybody else is
an answer to that be great you said that
the interest level that people express
by clicking and saving things is a
precursor potentially a preview of
citations I'm curious whether that's
testable and true in other words I want
to believe that that's true I'm not sure
I do believe it that's true because I
think that sometimes people are
interested in things in ways where it
informs them but they're not necessarily
going to cite it later and so I think
there's value in full but I'm curious if
it's really Carly yeah I mean there's
been some good studies done on mendeley
data right because men delays made their
data open and available and research
studies that are showing a strong
correlation with mendeley readers and
citations so it's it's new right it's
early so we don't have you know we have
ten years of data accumulating from all
these different sources not 40 years of
data but it's just really people are
doing that research to to bake it in and
some of it you can you can start to see
that right you look at these studies you
see the strong correlation and as we
instrument more
more proxies of how people are using
this hopefully the data trends continue
to continues to trend that way
so Mike you say we understand citation
based metrics who is we because given
that administrations and people making
very important people decisions about
people's lies don't seem to understand
citation metrics I'm quite concerned
that even when we do understand if we
understand the alternatives to these
that we're going to face the same
problem you say that there are a bunch
of questions that we need to ask and
from my mind what I've seen is that the
fundamental one is going to be how can i
make promotion tenure grant decisions
without having to read or understand the
work and with and in a way that enables
me to avoid getting sued how you know
this seems to be the elephant in the
room which is that is that this stuff is
just being misused period yeah when I
say the metric citation based metrics
are understood perhaps it'd be more
accurately to say that the material
exists for people to understand them
should they choose to read it there are
some really good papers available freely
available that any person can read that
surmise is 40 or 50 years worth of data
that talks about motivations and
discipline differences and so forth it
is possible to understand these things
being an idiot with numbers is not
restrained to people who have to make
decisions about these things the
question about making tenure is one of
the things I mean you've probably heard
me say this before Jeff that i live in
horror of somebody making funding
decisions on the basis of some of this
data like i say my big worry is what
happens if somebody takes it seriously
at the moment it's not in a position to
take seriously there was an example used
by someone from i think veteran kudos
about a usage data and in this
particular case she was using as a great
example of how you can get readership
through having a social communication
policy as a research if you use the
example of a team of researchers who are
basically constructed a marketing team
around the
what they were doing with the ambition
of making their paper the number one
down bread downloaded read article on
plus for a given time period and they
had a whole strategy and a set of
tactics and they they managed to do it
now there's nothing wrong with the paper
there's nothing bad about the paper the
paper is probably a good when I wouldn't
understand even the abstract frankly but
nevertheless there are such distortions
available as Andrea said taken cold just
looking at these numbers without the
context about the ability to look at
trends year-on-year trends disappearing
differences trends these just numbers
perhaps of how well these things that
performed what is interesting is when we
look at things like mentally mentally
counts are a private activity right so i
i'm using mendeley i upload document to
it I don't do it necessarily the
intention of telling the world that I
have just added a link to it so you're
measuring a kind of private behavior
whereas for many of these types you're
looking perhaps at the products of what
happens when you invest a press release
behind it what happens if you write a
blog about it that can be understood by
journalists what happens if you have a
large network of friends and we don't
have the data to be able to draw any
kind of conclusions other than site the
the numbers themselves as somewhat of a
counterpoint to that though when there's
people can game all sorts of metrics
right and and saying hey we're going to
put a campaign to get them to be the
most downloaded paper on a certain
platform is one case and what are they
going to use that you know that
researcher what are they going to put
their name on behind that data to say
yes this is something I did and I'm
proud of it I'm going to put it on my
tenure review packet they'll get they'll
get found out I'm an example that I'd
like to share is an early career
researcher who was trying to put
together her tenure and promotion
package and she had written a paper it
was in the humanities and she'd written
a paper that kind of took down the
standing wisdom at the time and went out
there and said yeah this is wrong and
she wrote her paper and time was going
on and it was not really highly cited
and she's getting nervous right like
this I
I've invested all of what i want to do
on this and i'm coming up for tenure and
it's not there it was right at that
point that she looked she was given
access to her plum x profile to look at
her papers impact and that happened to
include usage data in this case that she
had not gained she had not didn't even
know that it was available to her and
she was able to show that this paper in
a very niche field even though it wasn't
highly cited had been downloaded over a
thousand times from their institutional
repository and she was ecstatic right
because it gave her new data that said
look it's starting to get cited and it's
a way to show the impact I'm having in a
non-traditional way and you can be sure
she included that in the you know
defense of the work that she's doing so
you have to there's different there's
different ways to look at it and
different times when these metrics make
sense to use and other times when they
don't any other question
I think this puts us two minutes over
thank you again for the panelists and
thanks for you each year microsoft
research helps hundreds of influential
speakers from around the world including
leading scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>