<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Computational Limits in Statistical Inference: Hidden Cliques and Sum of Squares | Coder Coacher - Coaching Coders</title><meta content="Computational Limits in Statistical Inference: Hidden Cliques and Sum of Squares - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Computational Limits in Statistical Inference: Hidden Cliques and Sum of Squares</b></h2><h5 class="post__date">2016-06-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Bkm-HECa4qw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by microsoft
corporation may be used for internal
review analysis or research only any
editing reproduction publication
reblogged public showing internet or
public display is forbidden and may
violate copyright law
great so we're thrilled to have Yash
deshpande here um he was an intern with
us so some of you will know him from
there and he will be talking about
computational limits in statistical
inference hidden clicks and sum of
squares okay thanks everybody for coming
thanks for the introduction and it's
great to be back after a few years I
always liked visiting Boston so I'm
going to be talking about computational
limits in statistical inference and in a
talk like this it's almost a cliche to
start with something of the form that we
live in a world of big data so I'm not
going to start that way instead what I
would like to do is you know start I
just explained a few examples which
illustrate what I think are the
important trends so what you see here is
an MRI machine the first example is
about medical imaging and you know some
of you might have had the chance to be
inside one of these machines
unfortunately if you haven't the
likelihood is really high that at some
point in the future you will but the
point is the machine is basically to
take medical diagnostic images of the
kind that you see on the left now we
won't go into the physics of how the
machine really works but it suffices to
say that each measurement of the machine
gives you one point on the Fourier
transform that you see on the right so
now these machines if you have been in
them you know it can take a lot of time
to collect all of the points on the
Fourier transform so you have to stay
extremely still for about 10 minutes or
so this is hard enough for adults it's
completely impossible when you start
thinking about infants or rambunctious
little toddlers who will want to wander
about all the time there are or hold
that's right ah exactly exactly there is
other applications like like taking the
MRI of a beating heart and
your heart is beating then you it is
impossible to image this the the
breakthrough that you know that happened
about a decade ago was to realize that
you can get away with just taking a few
points in the Fourier transform so
instead of actually collecting all of
the points which will require your long
time to collect you just subsample some
of the Fourier coefficients and collect
them say you collect these red dots over
here this takes a much shorter period of
time because the machine has doesn't
have to run as much but then the
question is where you now only have a
few of the measurements can you still
get medical quality images and you know
really the breakthrough work by Donahoe
Candace and others was the realization
that you can use l1 based methods or
convex relaxations to obtain very good
quality reconstructions even from very
few measurements my second example is is
recommendation systems so a few years
ago a netflix run one of the early
machine learning challenges which
nowadays are very popular if you've seen
Carol etc they released a data set of
movie ratings that people had given so
for instance this is a cartoon of the
data set every column corresponds to a
movie and every row corresponds to a
user so maybe the first user is Jennifer
who I don't know maybe happens to be a
sci-fi buff so she really likes Martian
and and the new Star Wars movie and
perhaps the second user is me who
doesn't happen to watch a lot of movies
and maybe I didn't even enjoy the
Godfather very much so so the question
is you're obviously given just a small
set of the ratings because not everybody
is going to go and watch every movie so
given just a small set of the ratings
can you fill in the blanks can you use
can you infer the rest of the ratings
and use this for downstream applications
like recommendations and during the
price there was a lot because of the
netflix play there was a lot of work
that was done on on on problems of this
kind and more generalization
and the realization is that convex
relaxations of one or the other form are
extremely useful and informed in fact
formed part of the solution for even the
winning Netflix or price challenges I
believe now it is used also in in the ER
in the company's recommendation system
my third example is instead from
genomics so what you see here is a
representation of the results of a study
that was done about more than a decade
ago it was reported in pnas what they
did was they acid gene expression levels
from a number of patients and basically
what you have your is a jeans times
subjects matrix every row corresponds to
a specific gene and every column
corresponds to a subject and the
subjects are of two types you know the
blue ones are actually lupus patients if
you've watched house m.d. lupus is an
autoimmune disorder I don't watch house
m.d. so I had to look it up on Wikipedia
the green ones the green columns are
actually healthy control patients which
they also assayed what they found was
that there were a certain subset of
genes these were related to a certain
immune pathway called the interferon
immune pathway which had a abnormally
high expressions in in a small subset of
the lupus patients you know so within
this red splotch that you see over here
there is actually hidden an even darker
red splotch and these patients which had
these abnormally high expressions they
were actually the more critical patients
they were the ones with the more severe
versions of the disease the disease had
affected that liver kidneys etc this is
obviously findings of this kind are
important because you can use it for
targeted medication it tells you a
little more about the mechanism of the
disease because lupus is an immensely
complicated disorder we don't know a lot
about this so the question is given this
large messy data set can you figure out
you know patterns of this kind
so there is really an important
difference between between the first two
examples which is medical imaging and
recommendation systems and the third one
which is that you know in the first two
cases we have good estimators these are
often based on convex programming but
not necessarily and these are good in
two senses they have good statistical
guarantees they are statistically
efficient they take advantage of all of
all possible statistical information
that is present in the data set and
because for instance if they are based
on convex programming you can actually
compute them there are fast algorithms
to compute these things for the genomics
example you do not have this in this
scenario and in many other examples of
this kind what you actually have is that
there are either estimators that are
statistically very efficient but we do
not know how to compute these quickly or
the other way around in the sense that
you have something that you can actually
program on your computer but it is not
as it is not as good in statistical
performance so you know this is really
the main question that i would like to
think about in this talk which is that
how do we characterize computational
complexity in statistical estimation
settings this is actually an extremely
broad question and a reality because
people are extremely clever in coming up
with variety of algorithms and doing
computation in very very interesting
ways this is one of the probably the
most difficult way to earn a million
dollars that you can think about so
instead I won't won't i won't try and do
that but I will try and do something a
little bit simpler which is just talk
about one example which is the hidden
clique problem because it ain't in
reality is a prototypical example of
really this phenomenon where
computational constraints are more on
eros for a statistical estimation
problem so okay so this is a plan for
the talk I will set up the problem in
the beginning and then for a while we
will talk about what are the things that
you can actually do what are the good
algorithms
you know really there are spectral
algorithms and a little bit of twists on
that and i will mention a result there
after that will change gears and move to
instead trying to think about
computational hardness so this is kind
of the other side of the story and i
will talk about the sum of squares
relaxations just as a tool to
characterize the complexity and in the
end i'll just mention some other things
that i have been interested in the past
and what I think would be good to think
about if anybody has any bright ideas I
would be very happy to hear okay so
let's go back to the genomics example
remember that there is you know hidden
within these rows there are the Black
Rose which correspond to genes that have
these normal high abnormally high
expressions and let's just do something
very simple let's try and construct a
graph that summarizes this data okay
maybe we can see something in the graph
so the graph will contain your two kinds
of vertices the vertices on the Left
correspond to genes they correspond to
each row and the vertices on the right
corresponds to the patients I will
connect a gene with a patient if I see
that you know I see that there is a very
red dot in this matrix so if there is in
principle yes but for now let's just
think that there are no weights okay
this do a very simple thing okay so you
like you pick a threshold and if it is
below the threshold you you don't put an
edge if it's about the threshold you put
an edge okay so maybe the first gene is
not an interferon G and so it doesn't
have too many edges the second one has a
few more edges and so on and now you
build the graph and really what happens
is that the gene signature starts
becoming visible as a dense sub graph of
this graph okay and what you would want
to do is given the unlabeled graph the
graph without all of the colors you want
to find that dense sub graph maybe that
tells you something that tells you you
know which of the genes in which of the
patients are in so let us take this a
little bit seriously i will start with a
mathematical model so instead of having
two types of vertices
just scimitar eyes the problem and have
one type of vertex really the results
are will probably go through it just as
is but it is easier to talk on in this
setting so the null model or the the
null hypothesis is that the graph is
generally is a graph on n vertices and
it is a purely random order any graph in
the sense that between every pair of
vertices energy exists with probability
one-half independent of all of the rest
okay this is similar to the it is just a
simpler version of the Bible tight one
we had earlier and instead the
alternative hypothesis contains a k
vertices out of n which are colored here
in red and there they are all forced to
form a click so they're all of the edges
within those K vertices are connected
but the rest of the graph is generated
as random as before okay there's two
kinds of questions one can ask the first
is just detection can you distinguish
between these two hypotheses you are
given a graph can you tell me whether it
is generated according to this model or
the other k depends k might depend on n
in general yes yes indeed indeed think
of the K points chosen at random
beforehand before the gas so they are
not dependent on the graph structure or
think of generating the graph
conditional on a certain subset of K
vertices being a clique is two are the
same distributions the second problem is
the estimation problem which is
obviously harder than the detection
problem which is that given the graph
can you find me if it if the cliq exists
can you find it for me okay who can find
the clique in this graph yeah
okay the real problem is of course you
know things are never ordered and
they're never colored no but the edges
word out at the edges were fatter so
maybe I don't good even there so Adam
can still find the clip silly in this
one so maybe we should just stop here
but okay so this this is how you will
receive the data set and now you want to
do the distinction there is a useful
interpretation in terms of the adjacency
matrix what I will do is I will input
the presence of an edge by a plus 1 and
the absence of an edge by a minus 1 okay
so under the null hypothesis my
adjacency matrix or the adjacency matrix
of the graph is a purely random it is a
symmetric matrix with purely random
independent signs so it's filled with
rademacher independent random variables
under the alternative hypothesis instead
there is a principle sub matrix here the
sub matrix shown in blue which is all
plus once and then the rest of the stuff
is plus minus 10 4 okay and then the
detection problems is the same as what
we had before so as boas mentioned
earlier you what is what is what what is
the thing that you need to at least
solve this in principle is there any
algorithm that will solve this for
certain values of k NN and you know you
if you think about this for a couple of
moments you realize well if the clique
that you put inside the graph is bigger
than the largest natural you're
currently the largest leak that occur at
random and basically you can just find
it using enumeration you just enumerate
all the clicks of the right size and
then you're done okay this boils down to
a second moment calculation you can
compute the largest click is of size to
log in I think this is the earliest
reference is due to grim it but there
are all sorts of generalizations for
different models different values of
parameters that you can do but they all
boil down to use it using the second
moment method to estimate the size of
the largest leak again it's it's simple
to find the cliq just
using exhaustive enumeration let us move
to what is actually possible one thing
really that is probably the first
non-trivial idea that people had was to
use spectral methods and the spectral
method is based on the following simple
observation that if you take the
adjacency matrix and split it into two
parts first is its expectation
conditional on the click and then plus
something else then the expectation
conditional on the click is really a
rank one matrix it is the indicator
vector on the cliq times the indicator
transpose this indicator vector is very
simple it is once where the CLE on the
vertices which contain the cliq and 0
otherwise so it really looks like a
matrix of this form and now the noise is
whatever it is and when you look at this
it becomes very natural that you might
want to do principal component analysis
so you just use the principal
eigenvector of a normalized
appropriately to as an estimator for for
the cliq the way the efficiency of this
method really depends on the
signal-to-noise ratio in this regime in
in in this case the signal-to-noise
ratio is the ratio of spectral not it's
the spectral arm of the signal / the
spectral norm of the noise the spectral
norm of the signal the one line
calculation is proportional to the size
of the creek and the spectral norm of
the noise is of order square root of n
by very standard methods in random
matrix theory so you know intuitively if
K which is the size of the clique is
much bigger than square root of n then
your principal eigenvector gives you a
good estimator which you can clean up
and this was the work that was done by a
long crib elevation pseudo cough in 1998
there is a more refined version of this
phenomenon really this is a much more
recent work in random matrix theory and
what happens is a certain phase
transition of the following kind if K is
a little bit smaller than 1 times square
root of n then if you plot a histogram
of the eigenvalues of the adjacency
matrix it looks like this semicircle on
the left so it looks exactly like the
wigner semicircle and in fact what you
can prove is that the principal
eigenvector is as in particular
uncorrelated
thick in fact the you can do even more
the correlation is as though it is
random effectively okay on the other
hand if K is bigger than one times
square root of n a little bit bigger
then one eigenvalue pops out of the bulk
of the spectrum and the eigenvector that
corresponds to this eigen value which is
the principal eigenvector is indeed
non-trivial e correlated you can at
least hope to get an estimator that
might work okay there are many many
versions of this resulting they're
called spiked Wigner models in
probability theory the most advanced
result that i know of is remarkable
paper by knowles and gene but there are
many results by sonic of bike Benna Ruth
pÃªche a and others in the field ok so
here is the first result that I would
like to talk about which is something
that really breaks beyond the spectral
barrier which is that you know the
result is that there exists an algorithm
that will work if K is bigger than
square root of n over e so strictly
improves over the spectral barrier of
one times square root of n in moreover
you on this this really works in linear
time essentially linear time in the data
because the data is in an n-by-n matrix
so there are n square sum of bits of
information so you know what is what is
the key observation that makes this
possible which is that you know there is
a little bit more structure that you can
take advantage of and the structure is
well the signal was of course Rank 1 but
it is in fact also sparse so you can try
and take advantage of this extra
information and indeed PCA is completely
agnostic of this posture in PCO does not
care whether the rank one component is
partial not so this observation was also
used by philip and others in doing
reductions to the hidden clique problem
and there is various these are just a
small sample of the references that do
reductions from other machine learning
and estimation problems to the hidden
click settings so you can set up a
complexity theoretic reduction in the
will sense okay so how do we take
advantage of this let us just let us
consider the first you know the simple
way to compute the principal eigenvector
is this the power method okay so i will
start with an estimate v0 and i iterate
VT plus 1 is a times V T times a
normalization factor that makes my life
a little bit easier okay since VT is
supposed to be your estimate for the
cliq maybe you want it to be sparse
because you know a priori that this is
part so let us try and do the following
instead of using the power iteration as
before I will do a nonlinear version and
the nonlinear version is that instead of
applying it directly to VT I will apply
it to a function of V T and this
function for simplicity I will just take
it to be a coordinate Y separable
function so just think of the function
as I take some threshold in operator for
instance and applied to each entry of VT
and then hit it with the matrix again
and then I keep doing this this is a
simple idea and what obviously you would
want to do is to choose ft which is some
some some kind of thresholding function
that X that price and enforces the
sparsity in some way okay so for the
next few slides I will do a wrong
analysis but we will see why it's wrong
let us write out the is coordinate of VT
plus 1 the VT plus 1 is 1 over square
root of n that was my normalization
factor times the sum over j AI j ftv TJ
this is just a matrix multiplication
written out sorry likely I don't know
what was the last amount honestly I
wouldn't recommend it there is a reason
that is a wrong in the title
I prefer well good show it's been
corrected so yeah so I think this is
something so this this is something that
I think that physicists say that that
wrong calculation is better than no
calculation so let's go with this one
for a bit okay so you now when you look
at this sum and you remember that a IJ s
are you know these random signs when I
look at that some I feel like applying
the central limit theorem okay so maybe
let's just try and apply the central
limit theorem for VT plus 1 I this tells
me you know the normalization is
basically correct so VT plus one of I it
should be approximately Gaussian since
all of the edges are random signs if I
is not inside the creek this should be
Gaussian with mean about zero and
variance on basically the sum of squares
of the coefficient okay this
prescription also works for the previous
iteration so since vti is also a
Gaussian with mean 0 and some variance
you can write down what the variance
recursion is so you can write down this
variance at the next iteration based on
the variance at the first iterations you
get some recursion for Sigma T it is a
simple recursion that that is based on
the function that they function ft that
you use instead if I is inside the cliq
something almost similar happens except
that there is a small tweak since all of
the edges are not random signs some of
them will be all plus one the ones
because this neighbor this vertex is
going to be connected to all of the ones
in the inside the clique so those ones
do not have random signs they are all
plus ones so that accumulates into a
mean term but then the rest of the stuff
is the same as before okay so you can
similarly have a recursion for the mean
and this depends is proportional to the
ratio of K over square root of n times
some similar expectation that depends on
the mean variance parameters as before
this is okay does anybody know why this
analysis is wrong
okay are you worried about is exactly
the executor not apply the central limit
theorem right the the EA is the after at
least the first iteration the VT j's are
also complicated functions of the ages
and they are not independent of the aaj
so you cannot apply the central limit
theorem it's just wrong let's persist
for a bit we will see we will see a
little bit later how one eventually
fixes this so in pictures what you have
is you know this following history if I
plot the histogram of the vti is what
you get is that the ones been not
belonging to the cliq form this big
ocean of a certain variants and the mean
is centered at zero and instead the ones
that are inside the clicker a similar
Gaussian but much smaller in number and
those are situated at a mean that is
away from zero when you look at this
what you want to do is to push the mean
as far away as possible from zero and
make it as far in comparison with the
variance so that you can just maybe cut
in the middle and remove the clicker
okay so these are the equations or the
recursions that i will call state
evolution but it basically describes the
state of this algorithm and you know
what it is nice about this is that it
tells you what the optimal function is
this is a simple khushi Schwarz
inequality you can normalize the
variance to one and then optimize which
is the function to use and what you get
is a recursion for the mean parameter
okay the recursion for the mean
parameter is nice because now this tells
you what really happens depending upon
the ratio of K over square root of n
because in case over square root of n is
bigger than 1 over square root of V you
don't have any fixed points if you start
at the you always start at the origin
this is mutti plus 1 in terms of beauty
you start here you go up to the curve
you come back you go up to the curve you
come back etc if there is no fixed point
you will you will essentially go all the
way to infinity on the other hand if
case small K over square root of n is
smaller than 1 over square root of V you
have non-trivial fixed points and then
this you will just saturate at the fixed
point you will never be able to recover
ok so the analysis is wrong but the
theorem is right and what really makes
the theorem possible is that you have to
change the algorithm a little bit
instead of using using the
usual power iteration what you have to
do is or non-linear power iteration you
have to do it with what you would call a
cavity removal and this is inspired from
the belief propagation heuristics that
has been used in in machine learning and
in statistical physics do you need is
just to analyze it though the do you
need these things to actually make it so
the original without the cavity removal
the state evolution predictions are
false so you cannot so the the recursion
does not describe the original algorithm
without the cavity removal which the
cavity removal the dependence problem
that we had that you mentioned before
actually gets resolved at least as in
Tata Kelly okay so there is also a lot
of recently there was some follow-up
work by Bruce Hayek and others which
applied this and this stuff to some
other sparse graph estimation problems
okay so at this point I will just sort
of summarize the picture that we kind of
know which is that statistically you
just need k to be of order log in and
you can do exhaustive enumeration which
is quasi polynomial time here on the
other hand for any most of the
computation all of the computationally
efficient ones require k to be at least
a constant times square root of and
including our algorithm it did not
really improve over the spectral
algorithm by more than a constant so you
can actually reduce this constant as
much as you want if you sacrifice in the
exponent of n that is a there is a nice
trick that is actually even there in
Ilan's original paper but the fact is
that in a reality will never want to
apply this to any algorithm any any data
set because it what what it is asking
you to do is to look at pairs or try
pool suffered all pairs and tripods of
vertices and this is never a good idea
to do on it
the other direction is n square better
than n square there is an algorithm that
is order N squared which is due to give
all Paris I think yelled aquel and Corey
good algorithmic i don't know i think
they are constant is something like 1.6
I don't know whether it is sharp or
something to do is open i'm not sure i
think the n square usually just comes
from doing some sort of I compact like
some sort of power it usually all of
them are I turret of algorithms which
look very similar to the power iteration
with a few extra weeks of some kind of
the other I'm sorry what assumption
square they do instead of n Square log
in yeah so the log and we require for
four ok so the I did not talk about this
but in reality we can do the analysis
for the cavity modified power iteration
only for a constant number of iterations
what you need to do is to do a spectral
clean up after that so the log n comes
from doing the power it at the usual
power iteration for finding a but it's
just on a much smaller matrix heresy so
strong the constant is weaker and there
is a log in the constant is weaker but
the log n is a bit better yeah in
practice and reality i think this result
goes i mean if you actually run this
thing 20 iterations is fine yeah you can
run this thing for 20 iterations and it
first ferences the simulations in the
paper i did not do the spectral clean up
we just just did the usual algorithm run
it for a few iterations it it's done
so okay so this brings us to the
question it really is this regime hard
and for what algorithms can be hoped to
prove it so I I will spend a little bit
of time talking about the sum of squares
approach which is really a powerful idea
for general polynomial optimization this
is become extremely popular since the
work of para lo and lahser in in the
early 2000s but was also present in the
literature in in since the late eighties
what is the sum of squares there really
a principled way of constructing a
sequence of nested convex approximations
you have a difficult convex set and you
construct outer approximations in a
nested fashion and they seem to unify
many different other ad hoc versions of
doing convex Alex Asians or other
versions that were used in combinatorial
optimization zit is the lava shriver and
the Sher Ali Adams hierarchy and they
have very interesting consistency
properties which we won't go into and
there they are also very being convex
relaxations you know there are also some
more naturally robust you know this that
all statistical models are not always
correct there is always something that
is left unexplained and you don't want
your algorithms to be extremely
dependent on the model because that
means when you applied to some data it
will just give you some nonsense result
so convex relaxations are a nice way to
make these robustness properties happen
and the sum of squares relaxations have
some natural robust person monotonicity
properties that make them a very good
class of algorithms to consider and
really the third is that if you prove
something about the sum of squares even
a CS theorists it would be interested
some of them are obviously here but
really it has been extremely well
studied in over the last 10 or 15 years
in the cs theory community so if you
want to prove unconditional results if
you want to prove a hardness result
about a specific class of algorithms
I would wager this is a decent one to
think about okay so what is the
polynomial optimization problem for each
vertex in the cliq you attach a boolean
label which is 0 or 1 so I am sorry for
each vertex in the graph you attach a
boolean label that 0 or 1 1 indicates
membership inside the clique and 0 is
membership means it is not inside so you
want to maximize the size just want to
find the largest click subject to well
the boolean constraint and the second
constraint that the support of X must
determine a click and the way you can
enforce this is that for every pair of
vertices that is not connected you
enforce the product of the variables to
be 0 so it cannot be that both of them
are one this is natural the key idea of
the sum of squares is you know in store
one way to look at it is that you don't
want to optimize over points on the
feasible set of X instead you optimize
our probability distributions that are
supported on these points moreover and
this is key you will represent these
probability distributions using just a
small number of moments and you enforced
only a few of the consistency conditions
that these moments are supposed to have
ok so this will be a bit of notation
that I really need to set up but n in
braces is the first n integers n choose
D will be the set of subsets of size
exactly d n choose less than D is the
set of subsets of size at most D and the
disc the decision variable is something
that takes a subset of size at most D
and spits out a number between 0 and 1
ok and the interpretation that I want
you to keep in the back of your mind is
that this this number is is kind of a
probability or or a moment of some under
some appropriate distribution this is
not exactly right but it is a useful
mental model to have so what is the SOS
program you want to maximize the mass
that is assigned to the synchronous this
is analogous to the size subject to a
few cons
the first is just normalization
corresponds to probability distributions
integrating 21 the second is the
constraint that enforces the cliq part
which is that if X if a subset of
vertices in the graph does not induce a
creek then X must assign it zero
probability or zero value or zero mass
and the third is really the most
important constraint which is that a
certain moment matrix that is a linear
function of X this must be PSD and what
is this moment matrix it is a more it as
a matrix where every row and column
corresponds to a subset of size at most
D or ok a subset of size at most D over
2 and the entry corresponding to the row
s 1 and the column s2 is simply X
applied to the union of s1 and s2 ok so
this is a bit of a this is a little much
but let us just look at a picture so let
us look at the case when T equals two
convenient to think of dsr even integer
so if you look at d equals to you have
the moment matrix which where every row
and column corresponds to a subset of
size at most one so you have this null
set and then you have all of the
singletons right so you must assign
something to the null set you must
assign something to all of the
singletons and you must assign something
to all of the pairs
so let us talk about a specific test so
i will call SOS GD the optimum of the
SOS program when it is run on T on the
graph G and what i will do is i will
just declare that the alternative
hypothesis is true if this SS program is
bigger than k otherwise i will declare
the null hypothesis to be true the
rationale for this test is very is very
simple imagine that instead of the SOS
program i had the ahead an oracle that
computed the true optimum of the program
if I had that Oracle then this would be
the right test what I am doing here is
just replacing that Oracle with a sum of
squares relaxation of the arc which is
computable in n to the order d time ok
so this is the main result it is a
result for D equals 4 and this is the
first simÃ£o non-trivial level of the
relaxation which goes beyond the
spectral methods somehow which is that
this particular test fails when K is
smaller than n to the 1 over 3 and the
twiddle means that I am ignoring a few
log factors because I think boys might
agree that what are a few logs between
friends I'm not even sure how many log
factors and hearing here so
independently there was a work by rock
Omega R and protection and avi victor
ssin and they proved a slightly more
general result or a more general result
which said that the same test but for
generally failed if K is smaller than n
to the 1 over d if you compare it there
is obviously more general because it
goes beyond d equals 4 but for D equals
4 it is a bit weaker again I'm ignoring
log log factors here
okay so some comments that I would like
to make the a reality if you believe
that the problem is hard you would want
to prove that for any constant D the T
some of the summer squarest s really
fails at the threshold of square root of
n times maybe something that might
depend on d there was a very nice paper
by raghu maker and avi victors and in
2013 that claimed this result really
they set up a lot of machinery that all
of all of us used later on but
unfortunately the matrix concentration
methods that they used failed for very
subtle reasons it took a while also to
find compare examples to their proof or
it took of a while to find even counter
examples to the bow so is it was but
it's still a very beautiful paper and
what what got us interested in this
problem but you know just a couple of
months after we wrote our paper there
was a very nice observation by two
independent groups one is prasad
raghavendran and students a lil SRAM in
berkeley and hopkins kothari and
protection who are at MIT and ms are
respectively and they improved our
result to essentially the optimal
exponent of square root of n it requires
you know really one one or two important
observations beyond what we did it is a
very nice set of results indeed how the
hopkins paper has a little bit more in
sand they prove something also for the
generality
okay so if you have are there any
questions about this yeah particular
tests that all these things are
analyzing is provably optimal of
artistic all tests that use the degree D
SOS relaxation it's a good question
there is nothing okay there is really
nothing of this sort that's exactly in
this setting there is there is some work
by David's tower and the students on a
related problem on that is related to
tens for PCA and what what they show in
that paper is that you know if you have
if you have at one level as a second
order effect or a lower order effect of
that is present in the relaxation and
you do one more step of the relaxation
this becomes a leading order effect so I
am not sure whether one can prove that
this particular test is optimal maybe
what one can hope to prove is that if
the test is suboptimal perhaps
increasing the order by one or two or a
small constant number of by a small
constant actually removes this sub
optimality it's very subtle how to find
this question because in particular you
could say that the SOS solution also
gives you the graph itself you could
recover from that all the edges of the
graph so information theoretical it also
determines exactly whether there is a
click or not so you have to be a little
careful in how you define what it means
for a test the you know what kind of
information do you allow it is to use is
it just the actual value is it is it the
full moments
I mean even in the first part of you
talk just using the bulk of the
distribution groups of the actual I get
vectors non-linearity was already very
different right I think there is a
result that shows that no test based
only on the eigenvalues can work I
respect of whether it's polynomial time
on all right but that is not for this
model that's for a slightly different
model for the Gaussian version of this
point okay so what is the proof strategy
you know it's it's fairly intuitive it's
the following observation is that if you
think that the relaxation is good you
would think that well the SOS value on
under the null model would be pretty
small if it and under the alternative
model would be pretty large if the
relaxation were good in the sense that
it is faithful to the original
polynomial optimization if you think
that the relaxation is bad then all this
means that you have to do is that you
have to ensure that the value of the SOS
program is large under h0 or under the
null model ok so this is a maximization
problem one way to show a lower bound on
the maximization or the value of the
maximization problem is just to find us
guess a solution and compute its value
so that's what we are going to do we're
just going to guess a solution will
prove it is feasible and then we'll
compute its value minus 100 ok so before
going to the D equals 4 case I'll just
spend some time on the d equals 2 case
and we'll actually do a proof here so i
think i've heard there was a quote that
i heard only a part of the first part of
which is you know every talk must have a
proof and the joke there is a second
part to this quoted that they must not
be the same thing so so this is the
proof part so ok recall that we have
this moment matrix this is for D equals
to some something that has rows and
columns that are indexed by subsets of
size at most one so just an ulcer
and the singletons so recall what are
our constraints you want to set X fee
equals one so there is no there is no
nothing to guess there you if X of S is
not if X s is not a creek then you must
set XFS 20 so for instance if a pair of
vertices is not connected you must put 0
over there anybody have any guesses for
what you want to put on the singletons
and on the pair's ok I will take this
simplest possible guess right you want
to put something for the singletons and
something for the pairs if the pair is
not connected you must put 0 so you know
there is some multiplier alpha that
depends on the pair and that depends on
the singleton let us just take it
independent of which one it is so I'll
just put one value if it is a vertex and
one value if it is a pair if it is an
edge and for the non edges i just put 0
right so basically what i get is alpha 1
times 1 on on the outer product and
inside i have alpha 1 times identity and
then there is alpha 2 times the 01
adjacency of the matrix which i will
call script EG ok a question is now the
linear constraints are by the way
automatically satisfied so you do not
have to worry about them at all all you
now need to do is to show that is is to
show that the moment matrix is PSD so
when I look at this I wanted to do the
shore complement immediately so let us
do it the first part is the alpha 1
times identity plus alpha 2 times 0 1
adjacency matrix and then you have to
subtract the shore complement which is
the product of outer product or the
factors you get a lot of value alpha 1
squared times 1 1 transpose you want to
show that this matrix is PST let us look
at it in expectation in expectation the
the graph just becomes on alpha 2 times
1 1 transpose because each vertex is
connected with pro each edge is
connected with probability 1 half so you
just get alpha 2 over 2 times 1 1
transpose and since you want this to be
PST it makes sense to use the 1 1
transpose factors to cancel each other
out let's just so we
put alpha 2 over 2 minus alpha 1 squared
we bigger than 0 this is one condition
the second condition comes from the fact
that there is a deviation indeed so
we'll use the identity part that was
left over in the earlier equation to
cancel out the deviation the deviation
is just alpha 2 times G minus its
expectation okay so if you set also one
bigger than the spectral norm of G minus
its expect if you are done this again we
can prove is about of order square root
of n using very standard results in
random matrix theory so you have alpha
one is bigger than alpha 2 times square
root of n okay so this is the second
condition you can put them together
because you know you have an upper bound
on alpha one in terms of alpha 2 and the
corresponding lower bound these must be
consistent so if you solve it you get
with a one-line calculation that alpha 1
is supposed to be at most 1 over square
root of n under h0 or under the null
hypothesis the value is or the value of
the maximization problem is at least the
value of this solution the value of the
solution recall is just the sum of them
or the mass on the singletons so I have
n vertices and for all of them I put
alpha 1 so i get about square root of n
ok so what did we really prove we're
approved a result by phi grand crowd
kemer in the early 2000s which is that
this test fails to distinguish between
h0 and h1 when k is about of order
square root of ok before going to d
equals 4 i think it makes we would like
to just go over the key ingredients for
d equals 2 which is that you know
firstly we had a very simple choice for
x we analyzed its expectation you know
there was some some of orthogonal
projector there was something along 1 1
transpose something orthogonal etc and
then for the deviation we could use
standard results that were already known
you can use the matrix concentration or
results you can use the moment method
that goes back to Wigner you can use
epsilon net arguments which are I think
first
used by Freedman current samurai D in in
four random graphs you can use all of
these things the thing that changes when
T equals four is that really each of
these steps becomes a little bit more
complicated so the guess is kind of
similar in the sense that what you want
to do is to take X that is proportional
to the indicator that a particular
vertex is a click and then the
proportionality constant you know we
just make the simplest possible
assumption of choosing it to be just
dependent on the size so it's something
if it is a vertex something's it's a
edge something if it is a triangle and
something if it is a four click ok again
linear constraints are automatic all
that matters is the PSD constraint that
gives us conditions on these alphas okay
so this is the moment matrix for D
equals four it's in it has about 10 to
the four entries in it but instead of
looking at you know this this large
messy matrix i will just concentrate on
the sub-block that is indexed by the
edge the by the by the pairs and and you
know see what happens to each of our
steps for this sub block it illustrates
basically the main difficulty the
expectation you can instead of compute
instead of having just two orthogonal
projector ins you have actually three
orthogonal projections and they have
actually very widely differing
eigenvalues really the important the
point is that these the eigenspace is
corresponding to these eigenvalues
relates to irreducible sub
representations of the symmetric group
this is because this expectation is
invariant to changing the leg relabeling
the vertices so that is why this kind of
naturally concept but the eigenvalues
are very differing so you really need to
understand these symmetries and take
advantage of this fact especially when
you consider the deviation part okay
this way when you look at the deviation
you know it is an N squared times n
squared matrix so it has n to the four
entries but is dependent only on N
squared random bits ok so all of these
entries are highly dependent
okay and because of that the behavior is
very different from classical random
matrix ensembles in the sense that some
of this matrix actually has an unusually
high spectral norm base to the compared
to the prediction that you would make
without taking the dependency into
account so how do we make it work really
you just decompose the deviation into
pieces and you control each piece
separately and you try and align the
pieces which have large icon values or
large spectral norm with the eigen
spaces that are large and this requires
you to understand the symmetries of what
is happening will not go into this in
detail but I'll just end with a couple
of before I finish this section I just
want to mention a few open problems so
the question remains whether we can
generalize this to other models if K is
smaller than square root of n proving
that this is hard for d bigger than 4 is
still open the last time I said this I
had less bigger equals 4 and then like
two weeks later the bigger equals became
wrong maybe this time maybe this time
they will do it for d Biggar for can of
course it remains whether you can come
up with stronger hardness proofs okay
I'll just spend a couple of minutes
talking about a few other things that
I've been thinking about over the past
few years the the hidden Keeks problem
is also very interesting because it's
related to you know structured versions
of principal component analysis
particularly sparsity see so we had
there was work with andreea in which we
analyzed covariance thresholding which
is an algorithm and what you can do is
really go beyond this standard stp that
people usually use first parse pc which
emil and Dandrea we had another paper
which considered even more general
versions of this where the factors of
the principal components were
constrained to be in a cone in a convex
cone so for instance non-negative PCA is
is an example of this
more recently there has been a lot of
work on stochastic blocker models from a
variety of different communities so this
was a paper in which which we proved a
couple of things but I think the most
interesting part about this result is
firstly a universality portion where you
prove really that the asymptotic mutual
information is invariant is essentially
independent of the specific random model
that you use this is similar to
universality results in random matrix
theory where say the spectrum does not
depend on whether you have Gaussian
entries or plus minus one and trees etc
and the second part of the result is a
rigorous verification of what in physics
is called the replica symmetric on subs
I won't go into that in detail here
lastly i just want to mention some
things that are that i am thinking about
now or the reason could be very cool too
cool to are cool to tackle the first is
you know it is still related with with
pca and structured versions is that if
you if you see the use of these things
especially in nowadays it's popular in
medical literature to combine data from
many different past studies and then do
a joint analysis unsupervised methods
like pc a clustering all these kind of
things are used a lot in in such studies
for a variety of reason to do pre
processing of the data you want to
extract features you can use the
principal components as certain features
of the clusters as features I and really
they also formulate different hypotheses
based on this so I think there are a
couple of challenges that I find that
would be cool to think about the first
is obviously for clustering and for
structured versions of PCA we don't
necessarily have great algorithms so can
we do this in polynomial time or can we
come up with fast and reliable
algorithms the second you know is
important for particularly scientific
applications is that can you do
uncertainty assessment can you build
confidence intervals can you assign p
values to the estimates that you get
because it
important when you try and use these
things for for medical studies and the
second is related to the sum of squares
results when I learnt this stuff I by
the way I strongly recommend boys his
lecture notes they are fantastic when I
read those nodes and i learnt started
learning about this i found it really
cool but a reality if you try and
program these sum of squares algorithms
they are very difficult and this is
because it gives you a humongous stp
which is not easy to solve so but there
is there are some ways in which you can
make think about using these in practice
as well which is a lot less done in the
community then say for instance lower
bounds using SOS are the cs theory
community and one example i think that
would be nice to do is use this in
graphical models graphical models are
widely used for a variety of different
things like image segmentation error
correcting codes etc this is a book by
Daphne Koller who is a professor at
Stanford I think it is about 1,200 pages
of examples of graphical models so very
nice book so there's about three
important things that you want to do
with a graphical model but oddly you
want to sample from the model you want
to marginalize variables based on the
other so say you are given some
diagnostic information you want you're
given symptoms you want to know what
disease it is you can think about
marginalization and the third is
computing partition functions all these
are roughly equivalent and one can think
about computing partition functions as a
general version of all of these really
there are two popular methods that
people use in practice there's
variational methods where you come up
with versions of the partitions
approximations to the partition function
and prime computed belief propagation
and other versions are similar
unfortunately they yield only
approximation so for instance you do not
get rigorous upper and lower bounds
moreover these are notoriously difficult
to analyze especially for general
graphical models I think the sum of
squares methods can provide a way to
actually computing these or
make up with rigorous upper and lower
bounds for these things you give you a
knob it is a rough knob but at least
they give you a knob to trade off
between accuracy and computational
requirements and the consistency
properties of these sum of squares are I
think closely related to what are called
kikuchi approximations in physics so I
think there is a lot of work that is
that is that can be done in in in this
area so thanks for your attention and
I'm happy to take questions yes 1 over
square root the threshold in the first
part of your talk is optimal for that
class of algorithms excellent question
ah I suspect it is but I'm not sure yet
how to prove it one version of the
result that we have is that instead you
consider the same problem on sparse
graphs instead of dense graphs and for
that we can prove that it is optimal for
its optimal for local algorithms so if
you consider the problem on a sparse
graph there is a sense of locality in
the usual sense locality of so for all
local algorithms we're in the
neighborhood where the node makes
decisions just based on a bounded size
neighborhood around it this one over
square root of V is an optimal threshold
beyond that I'm not because we believe
probably Asian other iterative
algorithms and tense regimes is that the
reason why some of the limitations for
local algorithms not translate the dense
settings but there is no good sense of
locality on a dense crop right I mean a
reality though okay the graph in a
reality the graph that you must think
about in the dense case is is actually
the complete graph so every vertex has
all of the neighbors in its in its in
its neighborhood now the sum of squares
was actually one other reason that I'm
interested in summer squash is that it
gives you the right
a notion of locality which is really
based to the moments we're using only a
few moments so it may be that is one
notion that one can think about the
other thing is that this trick of a lawn
where you can reduce the constant by by
improving the by trading of the exponent
so clearly it can only be done say for
some linear time algorithms of some sort
this seems to be an even harder problem
that then anything else so i'm not sure
but yeah it's a it's a very interesting
question to think about yes it imagine a
variant of the planted quake problem in
which someone gives you maybe a hint as
to which nodes are correct or not maybe
you where biochemist and they flag
certainly knows is being more likely to
to occur are you aware of work in this
kind of hint model is it obviously
easier or is there is anything known ah
it's a good question this really this
this constant reduction trick is very
closely related to what you said we
really you just guess which pair of
vertices and then reduce the graph to
all of the common neighbors of this vert
of vertices that that improves the
signal-to-noise ratio that you have
there the thing is that in real
application it is a very neat trick but
I really do not like this trick very
much before for a number of reasons
first that you cannot run it secondly on
real data sets you know it is hard to
decide what is it what is the right
common neighborhood because that depends
on the threshold reality you would just
want to do something that takes
advantage of all of the weights so for
instance if it was not a completely
connected so if the cliq was not
completely connected but say was a dish
rainy with some probability P that is
strictly less than 1 it's not clear how
to do exactly like versions of this
stuff so I would suspect that I don't
know of any any work that really goes
beyond incorporate this into the sum
squares favor but just saying is the key
you just measure the objective function
differently for those vertices if you
had scores if you have if you are the
score for every vertex you try to
maximize the score evident maximize the
cliq size or something right right so
those alpha see ya so for ya it's that's
an ID as an excellent point for this is
one way to do it is that instead of
having the sum over all of the
singletons you had a weighted sum and
perhaps you put large weights on on the
ones that you think are more likely
maybe this would work I'm not sure if
anybody who's actually abused something
with this kind but it's a wonderful
thing to think of it you can get more
questions from them if you we're
supposed to be on a phone call that
started it sure sure sure today and
Henry is taking over and hosting so
there</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>