<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Brains, Minds and Machines - Surya Ganguli | Coder Coacher - Coaching Coders</title><meta content="Symposium: Brains, Minds and Machines - Surya Ganguli - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Brains, Minds and Machines - Surya Ganguli</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CSGONuMrj2s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
oh it's my pleasure to introduce Syria
Syria was I guess my one of my advisors
from my PhD as well as a professor at
Stanford University and Applied Physics
and he will be talking about what subtle
some random stuff yes
there's one dinner but there's one beer
okay we'll see ya
we'll see I should move okay thanks so
um I wanted to talk about glimpses
towards a new signs of brains minds
minds and machines basically weaving
together physics computation
neurobiology and I'd like to start by
noting one of the greatest ironies of
human existence easy start at least that
small part of human existence that
intersects with nips and that is that
neural circuits evolved to exploit the
laws of physics to compute but in ways
that are fundamentally different from
that if the computers that they
themselves designed right so they work
at very disparate timescales our neural
circuits are very slow they operate in
the millisecond time scale whereas
computers computers have nano second
clock speeds they're remarkably energy
efficient they operated 20 watts so
we're all dimmer than light bulbs
literally supercomputers operate at mega
watt ranges so they're faster they take
more energy yet their performance on the
types of tasks were interested in nips
at high-level AI is relatively abysmal
compared to neural circuits in humans
they have some other major differences
that are probably explanatory of this
difference in performance so the
computational path the intermediate
computational path that neural circuits
take a highly variable and analog
whereas computers are highly precise
they flip bits and very precise ways
from the beginning to the end of the
computation in a digital way information
encoding and neural circuits is high
dimensional and distributed information
and computers are placed and precisely
localized registers you know where every
piece of information goes in the
computer so these are very very
different methods of computation and so
in order to to bridge this gap where
we're really going to need is a new
science of minds brains and machines
that seamlessly weaves together physics
neurobiology and computation to extract
conceptual insight into how our neural
circuits compute and instantiate those
conceptual insights and artificial
circuits I will not offer that to you
today I can this is going to be a
collective endeavor that will occupy all
of us for many many years to come but
our lab does work at the intersection of
physics computation neurobiology and we
have several via nets that we'd like to
share with you
today so so one of the things that were
you know to two of the main ways that we
both model cognition and model structure
in the natural world are neural networks
and probabilistic generative models and
you know to understand how neural
networks compute we'd love to have a
unification of these two very disparate
methods of modeling and we've started
along those pathways in terms of
understanding the developmental
emergence of say probabilistic
hierarchal structure in both infants and
machines the other aspect of neural
systems is they operate using very very
high dimensional representations and
there's lots of interesting mathematics
operating in high dimensional spaces not
intuitive mathematics and it's likely
that the secrets of high dimensional
spaces are incredibly important for
understanding neural computation and so
we have some results in exploiting the
geometry of high-dimensional Aero
surfaces to speed up neural network
learning also equilibrium statistical
mechanics has had a huge impact on
machine learning you can just list the
names right Helmholtz machine Boltzmann
machine cavity method mean field theory
beta free energy there's a huge
interplay between machine learning is an
equilibrium stat mech but non
equilibrium stat mech which is the stuff
of dynamics and the stuff of life has
not had as much of an impact on machine
learning but I believe the time is right
for non equilibrium system mechanics to
start to play a role and we have some
results there that book that will
discuss and also we don't often have
broad theories that simultaneously
consider trade-offs between three very
important physical quantities and
computation energy speed and accuracy
and we have recently elucidated some
fundamental limits in the accuracy of
communication given constraints on
energy and time that serve as a
benchmark for what we can hope to
achieve both in biology and in machine
so this is a lot to cover and I'll just
give you a highlights here but
fortunately almost everything I'm going
to talk about is publish these are the
results and actually Andrew and I gave
back-to-back talks because we work
together on a lot of the same things and
so the first part of the work will be
with Andrew and Jay McClelland that the
second part will be a collaboration with
Joshua of NGOs lab the third part will
be was led by an excellent postdoc in my
lab now at Google brain research Joshua
soul Dickstein and the final part of the
was led by Shuba Nia Lahiri another
excellent postdoc in my lab and of
course I'd like to thank my funding
sources so so let's begin let's begin
with understanding the developmental
emergence of probabilistic article
structure and infants and machines so
this is again the work with Andrew Sachs
and Jamie Colin so here's a basic
observation about the development of
infant categorical knowledge okay so
even when you control for the perceptual
similarity of objects infants tend to
learn coarse-grained categorical
distinctions much earlier in
developmental time than they learn finer
scale category finer grained categorical
distinctions they can probe what
categories infants can discriminate
through looking time studies that say
aged six months before they can talk and
there's actually a whole host of other
phenomena here that we've also analyzed
but for this talk I'm going to focus on
the development of categorical structure
okay and there's a fantastic review of
all of this phenomena in this book by
Rogers and McClellan called semantic
cognition so in this book in in previous
papers Jamie Collins &amp;amp; Company tried to
simulate this process in neural networks
why do you get this progressive
differentiation of hierarchical
structure in human categorical knowledge
so what they did was they did a very
simple simulation with an artificial toy
data set where you had a bunch of
animals and plants and fish and so on
and that fed into some common
representation layer and a hidden layer
and you could ask the network various
queries and the system has various
properties that are properties of the
objects in the real world and they just
trained the network through back
propagation and they did they looked at
the dynamics of learning in the internal
representation right so each of these
objects have our oven already sent a
shins the input but they will develop
correlated representations in the in the
hidden layer and they've just by
visualizing these hidden representations
as they develop over developmental time
of the network they found that the
network behaved a lot like infants so
initially the weights are random so the
representations are random then suddenly
you can discriminate between animals and
plants the most coarse-grained
distinction then different types of
plants different types of animals and so
on and this is a multi-dimensional
scaling plot of the rep
tations in the hidden layer and again
you see this progressive differentiation
of hierarchical structure so when I
first thought saw this as i was at once
excited and Confused excited because
while these neural networks behave like
infants and confused as to why i mean
these are much simpler objects and
infants so there must be theoretical
principles at play that guide this
dynamics what are they so this is when
Andrew and I started thinking about
these linear networks and oh sorry and I
should note that these these kinds of
hierarchical representations are also
seen both in human and monkey and they
actually align this is famous work by
crees corte and company so the question
is how does it get there what's the
dynamics of getting there so you know we
made the bold assumption that may be a
linear network could also exhibit this
right because the learning dynamics of
linear networks are nonlinear these are
the learning dynamics of batch gradient
descent at slow learning rates you get a
set of horribly complicated nonlinear
differential equations we solve them and
as Andrew told you in the last talk what
what the system is doing is it slowly
building up the singular value
decomposition of the input-output
correlation matrix of your data a bit
mode by mode where statistical modes are
singular values that are large are
learned on a shorter timescale and the
time scale of learning is one over the
singular value so this is intuitively
appealing stronger statistical structure
is learned first okay okay but what does
all of what does any of this have to do
with hierarchy okay so the problem with
one of the issues with this work is they
were working with a toy data set what
we'd like is a more controlled data set
over which over which we have analytical
control and so the way that we
approached this was to describe the
world as some kind of hierarchical
generative model draw data from that
generative model statistical model feed
that data to a neural network and try to
understand how the statistical structure
of the generative model inputs of
imprints itself onto the learning
dynamics of the network I think this is
one route for research to go as we go
forward make the generative models more
complex and the network's more complex
and really try to understand what's
going on so this is sort of a first step
so what was our generative model well we
essentially mimics the process of
evolution
we had a a branching diffusion process
where individual properties or features
of objects for example can or cannot fly
would diffuse down the tree and each
time it diffused it has a probability of
changing and that that that feature gets
assigned to each of these objects and
your different features get assigned
independently through different
branching diffusions through this tree
so at the end of the day you get a
statistical structure where items that
are closer together in the tree or have
a more common recent ancestor and more
similar to each other and this is sort
of a classical description of heart we
structured data okay so now we know that
the learning dynamics of these linear
networks are sensitive only to the
second order statistics of this data ie
the correlation matrix or this or the
correlation yep so basically the
singular value decomposition of that
correlation matrix so to understand how
hierarchical structure is learned in
this simple network we just have to
understand the singular value
decomposition of datasets generated in
this way we can analytically compute the
singular value decomposition and we find
a remarkably simple structure the
singular vectors which what one set of
singular vectors can be thought of as
functions on the leaves of the tree
these singular vectors respect the
hierarchical structure of the tree where
the stronger singular vector is the
constant function the next strongest
singular vector with second larger
singular value is a low frequency
function that's constant on this branch
and the opposite value of but constant
on this branch so it makes it the most
broadest scale categorical distinction
and then the next strongest singular
vectors make distinctions within the
branch and then finally the weakest one
makes distinctions within individual
branches right so this is how the
singular value structure of hierarchical
structure data behaves and and this is a
match between our analytical expressions
in the simulation and so when you put it
all together you get a theorem
essentially any network must exhibit
progressive differentiation on any data
set generated by this class of
hierarchical diffusion processes that
mimic evolution essentially the idea is
network learns input output modes at a
time one over their singular value
singular values a broader heart
distinctions are larger than those of
finer distinctions and these input
output modes or singular vectors
corresponding to the higher distinctions
the tree so then you essentially get the
result and so when you put it all
together
can analytically compute the
multi-dimensional scaling plots that we
do expect from these linear networks and
this is what we get we just arbitrarily
labeled the leaves you know according to
just labels and so now when you compare
the very very complicated hard to
understand simulations of nonlinear
networks with the exact mathematical
analysis of the linear networks you see
a remarkable qualitative match and so
this is the difference really between
simulation here in theory here we have
much more conceptual insight into the
origin of this hierarchical progressive
different' of differentiation of
structure in these nonlinear networks
through rigorous analysis of a simple
theory in the spirit of what Andrew was
was arguing for a remarkable surprise of
this is that the second order statistics
of these semantic features are powerful
and sufficient enough to drive this
hierarchical differentiation that was an
interesting surprise so there's a whole
bunch of other psychological effects
that we can analytically analyze and and
incur and conceptually explain in terms
of previous simulations but we don't
have time to talk about that okay so
let's go to the next part you know we
need to understand the mathematics of
high dimensional spaces and so andrew
also alluded to this in his previous
talk so one of the one of the the
intuitions that we have about say Aero
surfaces over high dimensional spaces is
that they riddled with local minima
right in fact it's often thought that
local minima high error stand is the
major impediment to non convex
optimization however this is only true
sort of in low-dimensional landscapes
this is for example a caricature of a
protein folding landscape and it is
indeed riddled with local minima and
that's true because it's low dimensional
but we know that our intuition derived
from moving within a three-dimensional
world our intuition about geometry there
is woefully inadequate for generalizing
to our intuitions about geometry in in
high dimensions and in high dimensions
things are very very different in
basically typical ran random non convex
Aero surfaces local minima at high error
are exponentially rare in the
dimensionality instead saddle points
proliferate and we basically developed
an algorithm that rapidly escapes the
saddle points and
speeds up neural network training so
this is work that was done in
collaboration with some excellent
graduate students and with Yoshi Oh
Benji oh ok so the ideas again we
exploit some results that in statistical
physics where they look at the geometry
of high-dimensional landscapes and in
the basic idea is let's say you have an
error landscape over a million variables
right and let's say you you reach a
critical point the slope vanishes
somewhere what are the chances that it's
a local minimum we'll all million
directions of the function have to curve
up that's going to be exponentially
unlikely unless you're already near the
bottom all right so you can make this
much more precise for each critical
point you can plot each critical point
in a two dimensional feature space one
axis is the error level of the critical
point the other axis is the fraction of
negative eigenvalues of the hessian and
basically you might think that critical
points a priority live anywhere in this
two-dimensional space but it turns out
they don't they concentrate on a
monotonically increasing curve this is a
calculation done by statistical
physicists back in 2007 they concentrate
on a monotonically increasing curve the
global minimum is here the global
maximum is here that critical points as
you move up the energy ladder develop
more and more negative curvature
directions until you're at the total top
where in which all directions are
negative so basically at intermediate
levels of energy you have no local
minima you only have saddle points ok
now physicists would then say oh this is
kind of a universal result you know it
should be true for anything of course
computer sciences will fight back and
say no your analysis and random
landscapes is not relevant we're doing
special stuff on neural networks whose
error landscapes over synaptic weights
are very very different than your random
landscapes in physics were used to the
notion of universality where there
exists at least some questions whose
answers don't depend on the details but
in computer science were not as used to
that so I predicted that this would
happen in in neural network so I teamed
up with yahshua to test this and they
tested it in lo and behold that's
exactly what they found so in deep
neural networks trained on em nest and
see far 10 they found that the critical
points concentrated on a monotonically
increasing curve in exactly the
predicted two dimensional feature space
ok now what can we do
about this so it turns out that Newton's
method is attracted to critical points
of any index its attack attracted to
saddle points but you can cure this
problem by instead of dividing by the
Hessian Newton's method dividing by the
absolute value of the Hessian and by
that I mean take the Hessian compute all
its eigenvalues and replace the each one
of them with their absolute value it
turns out that that dynamics this
dynamics is rapidly repelled from saddle
points and you can justify it
theoretically using trust region methods
as well and it works remarkably well so
here's deep neural network training
where you just start off with stochastic
gradient descent and you see that you
get these plateaus in learning
performance as a function of time when
you see a plateau you might think oh my
gosh I'm stuck in a local minimum but
actually when you switch to this new
algorithm which we call saddle free
Newton you suddenly drop an error okay
so so this this algorithm can actually
do something by exploiting our knowledge
about the geometry of high dimensional
spaces I should say Yan lacuna also has
a different method for arriving at the
same conclusion that saddle points
proliferate in high dimensions okay all
right so now let's move from equilibrium
to nonequilibrium stat mech so that's
this part so there's been a lot of
advances in nanako broom sisal mechanics
recently that are really very exciting
essentially you can show that the second
law of thermodynamics that entropy
increases emerges through Jensen's
inequality as a corollary of some
equality known as the dresden ski
quality it's quite remarkable that the
jensen's inequality is what leads to the
second law of thermodynamics from a much
more fundamental law if you want to look
it up you just googled rozinsky equality
and you'll get to that literature but
what this means oh sorry so this is work
that was done with my postdoc jascha
who's now working at the google brain he
just left my lab recently so so
basically what this is saying is we're
used to the idea the things becoming
disordered over time that's the second
law but transiently for small systems
and for short times things can the
entropy can transiently decrease which
means you can spontaneously create order
from noise okay so motivated by these
ideas we
we're going to try to help this process
along by using neural networks okay so
the basic idea in use neural networks to
learn very very deep generative models
of complex structured probability
distributions so here's the physical
motivation let's say you have a
complicated data distribution that you
don't have analytical control over and
you would like to learn a probabilistic
model for that data distribution the
basic idea is destroy structure in data
through a diffusive process carefully
record that destruction of structure as
a movie frame by frame movie and then
use a deep neural network to reverse
time to create structure from noise
let's not let the laws of diffuse of
physics do it for us itself but let's
let's replace the laws of diffusive
physics with a neural network to help
this process along and again this was
inspired by recent results and non
equivalence mechanics which show that
entropy can transiently decrease for
short time scales okay so let you know
just to put it pictorially let's let's
say this physical die distribution is a
complicated data distribution we can't
write down a formula for it or
immediately train a neural network to
sample from it instead what we do is we
let the dye diffuse and it will then
become a simple distribution uniform if
you diffuse for long enough and then we
train a neural network to start from the
uniform distribution and go backwards in
time and rekey at the data distribution
and so then you get a generative model
okay so now if this is going to work so
let me now demonstrate that this sort of
audacious idea works so here's the basic
idea so this is a classical toy model in
machine learning the swiss roll and
what's the diffusive process look like
well we just let each data point diffuse
with a restoring force to the origin so
the stationary distribution is an
isotropic Gaussian and this is the
diffusion process okay and then what we
do is we carefully record this movie and
we train a neural network where each
layer in the neural network just has to
go one step back in time and reproduce
what happened in the past given the
present and we do this over many many
layers and train it to do that if it
works we should be able to sample from
the swiss roll in a very simple way we
sample from an isotropic Gaussian in two
dimensions that's the only stochastic
step and then we feel
get through the neural network which
could be a deterministic step or you
could also have noise in the neural
network so if it works I should be able
to take a new Gaussian point cloud feed
all the data points through the neural
network and reconstruct the Swiss roll
from noise and that's what happens so
it's sort of work it works pretty well
there's some straggler data points out
here what the system effectively learns
is it learns a transient vector field
that takes points everywhere and
concentrates it on the Swiss roll okay
so now we can so we can now move up the
level of complexity so the next level of
complexity is a toy model of natural
images say the dead leaves model where
you throw down circles of different
radii and the radii have a power-law
power-law radius distribution to mimic
the scale free nature of natural images
the the circles occlude each other to
mimic occlusion so you get all sorts of
interesting structure that mimics the
structure in natural images so this is
this is that this is a variation for I
make this it's okay I'm going to go back
to the movies so this is a this is a
sample of the dead leaf of the dead leaf
model today so this is a sample of the
dead leaf model
yes sorry this is this is a sample of
the dead leaf model this is a sample for
the next best probabilistic model of
dead leaves that's out there and then
i'll show you how our model works so the
basic idea is we train the model in a
bunch of dead leaves we have the pixels
undergo diffusion so we have a diffusive
process that turns dead leaves and white
noise we train your very very deep
neural networks with a thousand layers
to reverse this process and at the end
of the day if it works you should be
able to turn white noise feed it into
the neural network and generate dead
leaves okay so this is an example of
that working so as you can see it gets a
lot of the structure and in fact it gets
simultaneously things that are hard to
get in natural images it gets sharp
edges as well as long-range uniform
regions and also long-range angular
correlations in the edge right so
getting all three of those
simultaneously can be difficult okay the
other thing we can do is we can train it
on trainees we can sample from the
posterior distribution over images so
what we can do for example is trained it
on textures and then you know maybe
replace the interior of a texture with
white noise but clamp the pixels in the
boundary so the neural network for these
images operates in a convolutional
fashion so then information from the
boundaries should be able to propagate
into the interior and fill in the image
so this is an example of bark so what we
do is we just take this initial image
and feed it into the neural network and
the neural network fills in its best
guess as to what was in the interior and
as you can see it does it does a decent
job it gets these long-range
correlations and edges especially and
also the homogeneous regions so that's
the the basic idea so let's go back so I
should say that oh that's not what I
wanted sorry in case you missed any of
the talk use a quick review yes I should
say that for the dead leaves model we
achieved to our knowledge
state-of-the-art performance
as far as probabilistic models of dead
leaves go in terms of log likelihood on
the data that's the okay so there's a
couple of key ideas in here there's
actually two key ideas that I wanted to
to summarize is that we circumvent
simultaneously to problems that vex
machine learning one is the credit
assignment problem how do you assign
credit or blame to neurons that are far
removed from the final output okay and
we circumvent that by providing training
targets to every layer each layer only
needs to go one step back in time so
there's no back propagation needed
there's no credit assignment problem the
second problem in in stochastic in in
learning generative models as often
times when you try to model a
distribution you try to model it as the
stationary distribution of some
stochastic process this inevitably has
mixing time problems because if you're
stationary distribution has multiple
modes then it can take exponentially
long amounts of time to jump over free
energy barriers that separate the mode
so that's the mixing time problem we
circumvent that problem by demanding
that the neural network get from a
simple distribution to the complicated
distribution in finite time so we are
not modeling the data distribution as a
stationary distribution but as the
outcome of a transient nanako bromium
stochastic process I think these two
ideas will generalize in all sorts of
interesting ways both to understanding
potentially neural computation and
augmenting machine computation okay now
the final thing right there's this huge
order of magnitude gap between energy
and speed in machines and neurons so
this begs the question what are the
fundamental limits of energy speed and
accuracy in computation so computation
is very general but we worked on
communication and we came up with some
interesting limits there so this is
joint work with my postdoc Shubin a
Lahiri he's a man after my own heart he
was a string theorist at Harvard I was
also string theorist and we both switch
in neuroscience he's actually on the job
market this year so I you know I just
thought I'd let you know okay so it
hasn't this already been solved by
information theory and the answer is no
so information theory is very
be good on placing limits on the
accuracy of communication given energy
constraints but not time constraints so
if you revisit channel shannon's channel
coding theorem it gives you bounds on
bit rate in terms of energy constrained
channel capacity but oftentimes
achieving some bounds could require very
very large block lengths you take your
messages you put them into a big block
and you code that block and you transmit
it but time is of the essence in biology
biological systems do not have the
luxury of coding things into blocks
because by the time they build up the
block they might get eaten all right so
so it doesn't interim classical
information theory doesn't deal well
with it delays another interesting
subject is the thermodynamics of
computation where they asked what's the
minimal energy needed to achieve a
computation and they actually showed
remarkably that you could achieve some
computations with zero energy
dissipation but the catch is it requires
infinite time what was the basic idea is
that they elucidated the fact that n the
energy cost of computation occurs mostly
in erasing bits if you erase a bit you
reduce the entropy of your computational
device you must dump that entropy into
the outside world because the full
system must increase entropy if you dump
entropy the outside world you dump it to
the outside world you dissipate energy
that's the basic idea so you could avoid
this by making computation reversible
arranging computation so you never have
to erase bids but then if there's no
thermodynamic driving force pushing the
computation forward as opposed to
backwards the computation could take
infinite time so there's no really good
theory that simultaneously addresses
speed accuracy and energy okay so we
have a theory now that we've come up
with recently and the theory is
basically the following imagine that you
have some external signal lambda that
you'd like to transmit through and
through a circuit any circuit we model
the circuit we achieve generality by
modeling the circuit is any arbitrary
stochastic dynamical system and we need
some parameter of the dynamical system
and that parameters tau its fastest time
scale okay this could be an arbitrary
Markovian stochastic dynamical system
and the external signal couples to the
Markovian dynamical system by
arbitrarily modifying its rates and now
let's say you have a signal
receiver that can observe the states of
the system and from observing these
states it can reconstruct the input okay
the basic framework of communication
okay so we can we can quantify speed
energy and accuracy so we can quantify
speed as the speed with which the
external signal changes this is how much
the signal changes in one time constant
of the of the signaling system squared
okay we can quantify so by the way this
is a physical system so it's coupled to
a thermal heat bath we can quantify
energy as a patient by the amount of
power consumed times the time so this is
how much energy it dissipates within one
time constant of the signaling system in
units of thermal energy right so this is
energy and then accuracy is the inverse
variance of your estimator so we can
consider any arbitrary unbiased
estimator and accuracy we just defined
to be inverse variance if you define
these quantities this way you can prove
a very simple theorem the product of
speed and accuracy is less than or equal
to energy right so if for a given
accuracy you want to code a faster
changing world you better spend more
energy if for a given speed given speed
at which the world changes you want to
be more accurate you better spend more
energy what are the proof ideas behind
this well there is some work recent work
in non equipment stencil mechanics which
shows that energy dissipation can be
quantified through a thermo dynamic
friction tensor on a manifold of
Boltzmann distribution zeeeee associated
with equilibrium distributions of the
signaling system this is this is done by
Gavin crooks what we did was we lower
bounded this friction tensor by the
fissure information which is naturally a
metric on a manifold of distributions so
it has the same properties as this
thermo dynamic friction tensor and then
of course we all know that from for a
fissure information upper bounds
accuracy through the Kramer Rao bound
and by putting it all together you know
there's a lot of work involved you get
this very simple theorem if you define
things this way okay so now this opens
the path towards connecting two
experiments now we can ask well how
efficient our biological system subject
to these limits and also how far
artificial systems from these limits and
what do we need to get there
ok so we also I talked we also do a
whole lot of work in my lab where we
work very very closely with
neurophysiologist I haven't talked
anything about that word but we actually
make lots of experimental II testable
predictions we work closely with
neurophysiologists at Stanford to test
these predictions so we're doing a we're
having a lot of fun and actually I'm
looking for postdocs my postdocs are
looking for jobs and I'm looking for
postdocs so hopefully i'll get postdoc
before my postdoc gets a job but anyways
so so I I'd love to hear from you and
again this is mostly all published work
except for the speed energy accuracy
which is in preparation I'd like to
thank my funding sources my website is a
little out of date but hopefully you can
find most of these papers in the website
if not just googling thanks
I think thanks for the talk so yeah I
just before asking my question about to
comment that I have a paper with Terry
Sinofsky on basically similar idea of
using density estimation in learning a
stochastic process I think it's very
very exciting actually smile you may be
nervous your paper was my favorite paper
last year it was like effects yeah so
you mum your paper but yeah sorry the
paper is called the Wilson machine we
call it up with some machine Oh Terry
mentioned it yeah I remember now okay
yeah yeah so they're basically my
question is the things i've also been
dealing with in this way of thinking
about the problem yo it may be a very
powerful model for density estimation
but it is probably not useful for
learning representations have you
thought about that so basically if you
if you make this process that cause your
static let's assume take infinite time
yeah it's I mean the process going from
one frame to next frame is going to be
very easy right exactly yeah and so
basically at the end of the day you may
have a very powerful engine for you know
generative model but this is your
learning anything yeah I in my mother I
only have centers around filters
basically yeah so I sympathize with that
a regardless we looked at the internal
representations these thousand layer
networks and the first few layers were
good boys their heads everybody gets the
boards yeah we don't fully understand
the representations later on but i agree
with you if you have an upper bound on
depth then that forces a lot of
parsimony in the representation has to
create very intelligent representations
to get from where you are to where you
want to go in a finite number of steps
if where you want to go is extremely
complicated why not just take a very
very many steps but have an intelligent
a teaching procedure but I mean you I
agree that the representation become
more powerful if you have infinitely
more steps I mean the learning becomes
um don't you agree with that yeah so we
can prove that in the quasi-static linea
yeah except where the diffusion is slow
and the number of Deb's go to infinity
yeah this process will converge to the
true distribution of the data we have a
proof of that exactly i understand that
but in that process the lower
presentation become more and more boring
make no we make no claims about the
intermediate representation I accept
that the final representation is the
data distribution yeah yes you are dead
leaves generator was very interesting I
liked it a lot I'm wondering though how
you measured the training data
probability or testator probability
under this generative model at the end
it's oh yeah innovative model and it's
not an energy based model how do you
measure that probability yeah we use
basically similar ideas to the jars in
skinny quality where you can use
nonequilibrium paths to measure free
energy differences so this is one of the
interesting uses of the dresden ski
quality is that you can actually measure
free energy differences between two
states by looking at nonequilibrium
paths and averaging over them and that's
what we do and the free energy
difference is a log probability
difference so that's how we do it it's a
bit complicated it's hard to explain
immediately but there's a way to do it
what we do is we averaged over multiple
paths through the network that yield
dick they get to the same data point
that's what we do multiple paths through
the network to get the same output yeah
how could you get we diffuse we diffuse
back to the beginning and then go
through the network back essentially i
use the diffusion process yeah yeah
makes sense thank you ok
so let me ask the question I want to ask
Andrew before it's about the first part
of your talk so you're you're looking at
infant learning and trying to model that
with with big network somehow the brain
probably doesn't actually use gradient
descent right yeah yeah so how do you
think that would affect impact your
results so the brain must have some way
of solving the credit assignment problem
right it must back propagation is the
only way we know how to solve the credit
assignment problem and this is probably
the biggest open problem in I think in
in neuroscience is how does the brain
solve the credit assignment problem the
best I can say is that any method that
that might solve the credit assignment
problem might have learning dynamics
that's similar to back prom but that's a
wild speculation so it's actually quite
remarkable that back props learning
dynamics mimics that but but at the end
of the day it's not it's not that
remarkable right because it in this
simple data set the SVD controls the
learning controls the structure of the
data and so you might imagine that
almost any sensible learning algorithm
would pick up on the largest singular
modes first right it's just the second
order statistics right so so yeah I
don't know that's a kind of a muddled
answer but it's either speculative a
trivial alright just a quick question
about units so you can guess this is
yeah I Paul I was very intrigued I guess
that's the one I was just curious you
know about the speed accuracy yes sir jb
are the units of energy so your energy
there is again so it kind of log both
societies both sides are dimensionless
right if you look at it so energy is
dimensionless because I normalized power
times time by Katie ok so speed is
dimensionless in time but has units of
lambda squared accuracy has units of 1
over lambda squared so the product is
dimensionless so the units do work out
as a physicist I was also very worried
about units I'm totally sympathize with
you in fact dimensional analysis was a
clue for us to guess what the answer
would be before we could prove it
I don't know if I miss it but the
diffusion process so this was for a
single image or how does it generalize
oh yeah yeah so it's trained on an
ensemble of images right so it learns
the distribution and then you can do all
sorts of things right you can you can
actually do all sorts of things you can
sample from the distribution you can
infer by clamping parts of the image and
in ffort and infer the rest and you can
evaluate log probabilities that was the
answer to Titans question so so you can
do all of it but you train on an
ensemble so it doesn't overtrained one
image okay um one last question ok this
is a question related to the previous
discussion about the back propagation
being the only known way of doing credit
assignment that kind of raises analogous
like we know how we solve SVD or matrix
factorization or sequence of them while
expectation maximization or alternating
minimization so maybe it's not
necessarily has to be back propagation
and there is obviously a yeah it doesn't
have to be but that's um i mean ii m is
like an alternating minimization over
two sets of variables right yeah or so
multiple sets of variables there could
be multiple single but the fundamental i
mean unless you can exactly do the
minimization which justifies many DM
algorithms in either in the in the
m-step you're going to have to do
gradient descent at some point right so
gradient descent will be a computational
primitive in very very complicated
problems right where the m-step can't be
done analytically ok yeah I mean a dream
is that that may be objective function
minimization is not the right principle
for understanding brain dynamics at the
level of neural dynamics of learning
that that's a scary proposition because
the machine learning we always think
about objective function minimization
you have nothing else to hold on to
their but we know that oscillators don't
minimize any there's no Lyapunov
function for an oscillating system and
the brain oscillate so you know let's we
have to be very careful about the
assumptions we make all right let's
thank sorry again
you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>