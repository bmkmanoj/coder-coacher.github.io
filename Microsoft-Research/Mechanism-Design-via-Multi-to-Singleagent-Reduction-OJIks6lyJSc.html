<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Mechanism Design via Multi- to Single-agent Reduction | Coder Coacher - Coaching Coders</title><meta content="Mechanism Design via Multi- to Single-agent Reduction - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Mechanism Design via Multi- to Single-agent Reduction</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OJIks6lyJSc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
hello and welcome everyone I it is my
pleasure to introduce side alley from
University of Maryland side has been
here before as an intern yeah in 2009
and he is going to tell us today about
patient mechanism design by a multi to
single its introduction alright so my
talk is about page and mechanism design
wire reduction from multi-agent to
single agent mechanism so I will start
with a quick introduction of what the
problem is and then I will explain to a
separate approaches for dealing with
this problem the first one is called the
composition via Vai interim allocation
rule the other one is the composition
wire eggs aunty allocation rules so
here's a quick introduction so in
mechanism design we face an optimization
problem however the input to that
optimization problem comes from selfish
agents who may want to miss report and
then put in order to manipulate the
outcome in their favor so we assume that
the private information of each agent is
called the agents typed and for example
if you have an auction for single for
selling one unit of an item then the
private type of each agent would be
their value for the item and the
performance of mechanisms of course
measured with respect to the actual
types are not with respect to the true
types therefore in the mechanism design
we have to take incentives of the agents
into account roughly speaking mechanisms
can be this in the categorize two
mechanisms with money and mechanisms
without money and consumes with money
usually we don't want to maximize
welfare or some other objective which is
typically revenue but it could be other
objectives and ready money for example
if you have a job with scheduling you
may want to minimize make is fun for the
Burford maximization for the case of
kwazulu-natal thesis which means your
utility is your value for the outcome
minus your payment
the game furyk aspect of the problem has
been solved like you can get an optimal
solution by using BCG but
computationally it could be np-hard so
there are approximation for this and but
another way you can look at mechanism
design is based on prior whether its
prior free or Beijing so invasion
mechanism design we assume that we know
the distributions from which the agents
types are drawn there as in prior free
we don't make such an assumption so in
this talk I will focus on the Beijing
settings and assume that agents types
are drawn from independent distributions
and this is crucial for the rest of my
talk and I assume that further that the
objective is something which is linearly
separable over the agents it could be
welfare or revenue or any other
objective by linearly separable I mean
that the objective is some of the
objective values that you obtain from
each single agent which is obviously the
case for both revenue and welfare and
assume that the coupling constraints for
the agents are poly measured I will
explain this later and the main result
then is that in such environments you
can reduce the complexity of the problem
exponentially by doing a multi-agent to
single agent production and this is
without no loss in the performance
yeah I assume that I know the
distributions that there is some
distribution and I know the distribution
just you more everybody knows everybody
knows institution and everybody agrees
on the distribution so these are
critical assumptions and so what I'm
going to explain is a general framework
which can be applied to a variety of
settings for example can be applied to
non-causal in erie till tues when you
have budget or risk or studying even in
the settings where you don't have money
and or any arbitrary objective as long
as it is a linearly separable okay and
in most of my talk i will try to focus
on the like mathematical problems that
arise in the in this context in the
abstract form and i will try not to
focus on the game theory aspects that
much so here are some related work for
beijing mechanism design so these are
mostly economists work and simple cases
where there is like you have one unit of
an item and you have either risk
aversion or budget constraints and most
of them assume that agents types are
distributed identically and
independently so these are mostly
symmetric cases and these problems are
really hard like if you move from
welfare maximization to something like
revenue maximization the problem
suddenly becomes so hard like as opposed
to the welfare maximization where vcg is
optimal in revenue maximization there is
no like global solution and for
multi-dimensional types again this first
two lines are economists workday is only
handed these are only about various
special cases where the valuations
special have very special structure and
it can be reduced to single dimensional
case and then there are some computer
scientists works and the recent work
that deal with a specific variants of
like the revenue maximization problem
for example in some of my special and
settings like you have budget and they
have a limited number of items you want
to like you have some independent
distribution you want to maximize
revenue
thing like that and then there is this
very recent work which actually uses
techniques which are very similar to the
techniques that I am going to describe
in a snack but they don't present it as
a reduction they will use these
techniques to solve a specific problem
of revenue maximization where you have
independent like going to have
independent agents with additive
valuations for a multi-item auction okay
so i will start with the composition
wire interim allocation rule this is a
joint worked with who fone nemaha japan
a json Hartline and other action malakut
so actually the first part of my talk is
the recent one and the second part is
something which is just my just me and
it's from last year so and this this is
the first one is the more recent part so
I will start with a quick example which
should give you an idea of what it means
I'm by reduction from multi-agent to
single agent reduction consider any
auction for selling one unit of item a
tool and suppose pick any action that
sells to the highest bidder and consider
a truthful action now suppose these are
they have three agents here so T sub I
denotes the type of space of agents I
and like these are symmetric agents and
each agent has two types the high-tech
has two dollar the low type has one
dollar valuation for the item and they
haven't really called probability now if
you look at any auction which allocates
to the highest type you can actually see
that when the agent one for example
beats a it wins which probably seven
over twelve and otherwise he wins with
positive 1 over 12 in expectation over
what the other agent submit now consider
a single agent mechanism which does the
exact same thing like whenever the
agents beats a heat wins with this
policy and when he would be he wins with
this other probability and it's well
known that in any truthful mechanism
once you fixed an allocation you can
actually compute the payments
corresponding to that allocation which
leads to a truthful mechanism so from
the perspective of that a
that multi-agent mechanism is equivalent
to this single asian mechanism actually
this equivalence holds in general and
doesn't need to make any assumption
about causes near the risk-neutral T
budget or anything if you give me a
multi-agent mechanism and if you assume
that agents types are distributed
independently I can give you a single
agent mechanism which is equivalent for
each agent
I assume that the ties are broken at
Green for me at Rhonda but the
parameters are exactly the same here and
I'm saying that if this was a
multi-agent mechanism at ignore this
suppose you have an auction which
allocates to the highest agent then this
is the expected allocation that this
agent observed now suppose I give you a
single agent mechanism which allocates
with these exact probabilities suppose
there is no other agent I just give you
a mechanism that if you bid eight
allocates the item tree with this
probability otherwise with this other
probability and i'm still at what I'm
claiming is that from the perspective of
this agent there is nothing different
between this and the multi-agent
mechanism in fact more generally if you
have a multi-agent mechanism if you have
independent videos you can induce a
single agent mechanism agent as follows
fix some agent and for all the other n
minus 1 agent create dummy agents which
are drawn from the distributions of
those corresponding edges yes simulate
those the other agents to run them
multi-agent mechanism on those simulated
agent and this actual asian and this
actual agent will not be able to deliver
the difference between the simulated
mechanism and the actual mechanism and
here is the point where it is important
that the agents types are distributed
independently because if the agents
types are correlated then the simulated
agents pipes will not be correlated with
its actual agents so it is crucial that
the agents types are independent so do
the simulation it was a benefit you're
getting what was like this so I'm not
think I think I'm just easy reduction
you're taking a multi
coming up yeah I want what I want to do
is to go in the other direction I want
to instead of designing a multi optimal
multi-dimensional multi-agent mechanism
I want to somehow the design optimal
single agent mechanisms because single
agent mechanisms are easier to design
because instead of considering all
possible combinations of agents types I
have to consider only the types of one
agent so it's the space of possibilities
is exponentially smaller so that is the
idea that I want to get to yeah this is
the other direction I want to say that
anything can be decomposing this way in
general I claim that any multi-agent
mechanism regardless of the setting can
be decomposed as follows you can
decompose it to a central avocation
mechanism and a bunch of single-agent
mechanisms where it works as follows
each single agent mechanism communicates
with the corresponding agent and then
the single agent mechanisms
simultaneously submit some probabilistic
demands to the central application
mechanism the central allocation
mechanism then allocates resources to
these single asian mechanisms in such a
way that we in expectation would satisfy
these probably stick demands and then
and finally the single agent mechanisms
decide exactly the final outcome
corresponding to this age so let me
explain this to you by an example
suppose you are in a setting where you
want to sell one car to you have three
agents you want you have one car for
sale and each agent has a
multi-dimensional type and because each
car can be customized in different ways
before sale like you can for example
let's say you can change the color of
the car you can install like a sports
package so each agent will have
different valuation for the car based on
the how how you customize the car now
consider any optimal multi-agent
mechanism in this case and for each so
the only constraint is that you can
allocate the car to one person there is
no constraint on how you can customize
the car so this is clearly
multi-dimensional a setting and for each
type of each agent so this is the
this is a tight capital to do so type of
space a little F denotes the probability
of each type and X is the expected
allocation for each type take any
optimal mechanism and compute compute
this expected probability of allocation
for each type in that optimal mechanism
now I give you an equivalent decomposed
mechanism that does the same thing
suppose a foolish agent I do the
following i compute a single agent
mechanism for this agent subject to
these allocation probabilities what at
me what I mean is that following suppose
I have given you this probabilities like
poor point 45 and point 1 and ask you
what is the optimal mechanism for
selling to this agent suppose it's
subject to the probability of allocation
not being more than these red numbers
and suppose we have only this single
agent that there is no one else and I
claim that this optimal single agent
mechanism subject to this is exactly the
single agent mechanism which is induced
on this agent by the optimal multi-agent
mechanism so basically you can see it as
the following you have a bunch of
optimal single-agent mechanism it when
the agents submit their type this single
agent mechanisms submit their a
probabilistic demand like this tingling
mechanism stop submits at Point 45 to
the central allocation chasm and then
the central application mechanism
ensures that each single agent
mechanisms receives the resource with
this expected probably in expectation
this much so to define this formally so
so far I am considering what a single
unit of resource that mentality but this
can be generalized but I will stick to
this simple example define T sub n as
the union of the type all of the type of
special so this is the sort of the types
of all agents and without loss of
generality assume that the type of
spaces are disjoint now I define this
vector X which which is the expected
allocation for each type of each agent
and we call it the interim allocation so
in the decomposition that I'm claiming
the central allocation mechanism in
this interim allocation in other words
if each agent submits I mean reports a
type according with these probabilities
then they should receive the item like
the resource with those red expected
probabilities and then each single agent
mechanism is optimal subject to the
interim allocation rule so here are then
a number of questions that we need to
answer in order to be able to use this
technique to design optimal mcadams
first of all when is an interim
allocation rule feasible so for example
suppose here instead of point 45 I have
one that would be example it clearly an
infeasible case because what if agents
want submits a and agent to submit see
and if this is 1 then most of the agents
have to get the good with probability of
1 so that is clearly not feasible and so
what the first thing is we have to
ensure the feasibility of interim
allocation and suppose I give you an
interim allocation rule which is
feasible how can you actually implement
this suppose I have given you these
tables and I'm asking you to give me an
allocation policy that would implement
in expectation allocated to each type
with these red probabilities how would
you do that and the third question is
how can i compute the optimal interim
allocation rule and finally the last
question is suppose I give you the
optimal interim a locational how can you
compute the optimal singulation
mechanism subject to this interim
allocation so I will focus on the first
three questions and I will talk about
the last question later so so what do
you usually do you have an optimal
single agent Buddhism yes so that i can
compute the optimal single asian if you
give me these red probabilities i can
compute the optimal single asian
mechanism subject to not advocating more
than those probabilities superfood well
that is the first question so so this
using day you have a black box but yeah
I has another have a black box for that
and I want to
serve the first three questions just
suppose you actually inside just agree
on the Sun suppose you have a black box
but just optimal single-engine mechanism
graph of x is something external I'm
telling ya of cosine it does give it
like if it says this much you assign it
to no more than this probability and
yeah all right so is it without loss of
generality that you can actually
implement for nes or is something I mean
it's something that I assume but for
most of the like for all of the problems
that I know of you can compute this like
if you can compute a single agent
mechanism for some X you can compete it
for any X and it has some nice property
which as you'll explain later like the
expected objective is always concave in
X I would get to that later
optimism is optimal for so the optimal
single agent mechanism is the following
suppose I have this only this one agent
and there is no other engine in the
picture and I'm only asking you to
construct a mechanism for me let's say
to maximize the revenue from this single
agent and I am putting a constraint on
you that constraint is the following if
agent like take decision if agent bitsy
agent too then you're allowed to
allocate the item which probably at most
zero point 45 and if he beats d you're
allowed to allocate with probably at
most point 1 what is the maximum revenue
you can get in expectation from this
agent if that is your concentrate so
what I guess that sort of was pushing
more please or it could be anything I'm
assumed that any objective you want to
maximize let's say revenue okay so for
the rest of the talk let's say we want
to maximize revenue but it's quickly
apply to welfare revenue or any
objective which can be linearly
separated over the agents
it is an example of an optimal
simulation mechanism with the setting so
um okay so let's say I mean I can't
compete it off the top of my head but
let's say it depends on exactly what do
you mean but let's go back so let's say
yes let's say this is the yeah let's say
this is the setting you have and you are
constrained with these probabilities so
I can imagine that the optimal single
agent mechanism here would be something
like you either don't sell to this guy
and always self to the sky with at two
dollars like you set the price of two
dollars and if the guy beats two dollar
like it's not a price if the guy beats
two dollar you will allocate it to the
guy and you with this property and if it
beats one dollar you allocated authority
zero if you want to maximize revenue so
the studying I've explained here
originally was for maximizing welfare
but let's say you want to maximize
revenue and these are the Cure
constraints the optimal mechanism here
would be to sell a two dollar with this
probability which is the maximum you are
allowed to and if it beats one dollar
doesn't sell to him at all because if
you still at one dollar what is the
maximum price you can charge it maximum
person can charge is one dollar which
means that you also have to charge one
dollar for this higher because he would
lie to you if you don't if you want to
charge something higher
so so this X in the upper bunk escape
yeah it's an upper bound constraint on
what you can do what is the f1 it is the
desert distributions of these types like
f1 of a is like 40 2.5 s type a 20.5 it
as tightening and that is the
application the expected application
that it receives but I treat the
expected allocation as a constraint when
I want to compete optimal single agenda
case people suddenly mothers days are
intended yeah yeahs is for the signal
cage in McKesson that expectations /
just ran dozens of them again yeah the
random coins of the mechanism but when
you have multi-agent yeah but when you
have multi agents randomization could
come from other agents okay so so here
is formally the problem that yeah so so
from now on I will just ignore all the
mechanism design aspects and I will just
look at the following problem suppose I
have given these tables and like I know
for example the agent one has type every
dis probability and it has to be
allocated with it like this it has to
get the item with this probability and I
want to come up with some allocation
policy which in expectation satisfies
these red probabilities like whenever
agent like to beat see he has to receive
the item with probability point 5545 and
so and the questions that i wanted to
answer was whether these expectation
probabilities which I call the interim
allocation of whether there are feasible
and how I can actually come up with some
allocation that implements that and how
we cannot optimize over X and how can
compute the optimal single asian
mechanism so to give you an idea of how
we can solve this i will demonstrate a
computationally inefficient solution for
answering this question so it's based on
a network flow argument so for every
possible scenario of the agents I will
put one vertex so each one is a like one
possible profile of types for dajun and
for each type of each agent all will
also put one vertex on the right hand
side and i will add a source node and a
sink node
now i will add directed edges from the
source node to each scenario and the
capacity of each edge is the probability
of that scenario happening and on the
other side i will basically set a
directed edges from each type node to
the sink node and the capacity of each
such edge is the probability of that
note happening x expected allocation
that that type needs so this is like a
demand and this is like your supply that
you have and you set the Middle Ages as
follows for each scenario you add
directed edges through the types that
appear in that sooner like for a see you
add your head edges to a see any now if
you have a feasible flow if the maximum
float from source to sink saturates all
these edges then that means your interim
allocation rule is feasible right and if
you actually compute a maximum flow the
amount of flow in the Middle Ages
exactly tell you how to allocate the
item like for example if you're at this
scenario AC look at the flow that goes
to a and flow that goes to E so that
goes to sorry CMD and you what it means
that if this scenario happens you just
allocate the item to one of AC or e
proportional to that proportional to the
flow of those edges destroyed yeah yeah
I assume that you guys are disjoint if
there it doesn't really dance without
loss of generality because if they are
equal at the same I can like readable
them differently
yeah yeah I mean I assume that the type
of space of each agent are labeled in
different ways so if even if they're
like exactly they mean the same thing I
will just name them differently and so
okay but but written here you say this
is visible lastly if you if you can give
for to everybody in good ways yeah
exactly if these edges are all saturated
that would be condition of visibility so
as you can see this network flow
construction is exponential so you
because you have a note for every
possible scenario so it's not efficient
but I will get back this I just wanted
to give you an idea of how we can handle
this okay so let's capital X denote the
set of all possible feasible interim
allocation rules and now suppose for
each agent the R sub I of X is that like
X sub I is the part of the interim
allocation rule that is restricted to
the type of that two types of that Asia
and suppose this note this denotes the
expected objective of the optimal single
agent mechanism subject to this interim
allocation so first of all we show that
for any singulation problem with a
maximization objective this R is convex
is concave and this is regardless of
whatever objective you have or whatever
the constraint on a single agent
mechanism is it's always concave and
therefore you can complete the optimal
interim allocation rules by just solving
this convex program which maximizes the
sum of the single agent objectives
subject to physical beauty of interim
allocation however this convex program
is of course exponential size because we
have the network flow construction is
exponential so I we want to address this
problem so we want to check the
feasibility of internal occasional and
be able to optimize over them in
polynomial time so let's go back to this
network flow case and the condition for
so if you can imagine these these nodes
as demand notes and these other nodes on
the left as supply notes in order for
this network to have a feasible
assignment that satisfies all the
demands the condition that you need to
satisfy is that for any subset of
vertices on the right the total demand
should be less than or equal to the
total supply of their name
birds on the left hand side that is the
necessary and sufficient condition that
condition actually was discovered by it
was not discard but it was proved to be
sufficient by border although border
didn't use this network full argument he
had a very complicated argument for
proving this and it can be interpreted
as the following for any subset of types
and the the total supply of their
neighbors is exactly so what are the
neighbors of these types the neighbors
of those subset are exactly the
scenarios in which at least one of those
types up here and this is the total
demand so basically it says that the
total demand of that set should be less
than or equal to the probability that
there is at least one agent who's typing
that subset and again the consider
exponentially many equality's but this
is now easier to handle so now I want to
give you a poly material
characterization of this this space of
interim allocation note so define a
normalizing Terry medication rule as
follows just multiply the expected
allocation of each type by the
probability of that type a query and
define this function G of s where s is
the subset of types as to be equal to
the probability that at least there is
at least one agent whose type is in set
s now the set of all normalized interim
allocation rule is a polymer a defined
by these inequalities so because it's a
poly Metroid you can optimize over it in
polynomial time because you can find a
you can construct a separation Oracle
and to find a violated inequality by
minimizing the G of s minus X bar of s
for any given export this is a submerged
or minimization we actually show that
you can do it very efficiently in time
which is almost linear remember that T
sub n is the set of all types which is
actually the size of the vector X bar so
this is pretty efficient so again
see this is for checking you can you get
a separation or I kill and because you
have a separation or I kill this is a
convex space you can optimize over this
space using ellipsoid method in
polynomial time so you can compute the
interim allocation rules in polynomial
time now the question is how can we
implement the interim allocation rules
so if I give you an interim allocation
religious feasible how do you allocate
the resources to satisfy these expected
probabilities so I will start by
defining tight search which turn out to
be very important in our implementation
so let me let us look at the
interpretation of this explore office so
X bar of s when you have one item simply
it has the following interpretation it
means a the expected probability that
there is a type from data type from set
s shows up and also gets allocated to
that that is the meaning of X bar of it
it's basically the sum over ok these
notation means take the sum over all
elements of like look at the vector X
and sum over all elements of entries of
X that appear in s and what is G of s G
of s is the probability that there is at
least some type insert s and obviously
this first one has to be less than or
equal to the second one because if there
is an agent from set s who has appeared
and gets a look at that that M has been
allocated to there must be some agent
from set is that has appeared and but
the important thing is that when the two
are equal that means whenever there is
at least one agent from set s then at
least one agent from set s to it gets
allocated to like if there are more than
one agent in such as it doesn't say whom
to allocate to but there if there is at
least one you can allocate to that so
it's somehow it tells you determines
what should be your allocation in the
case where you have a tight subset and
it's easy to show that tight subsets are
closed under Union and intersection
because the tie subsets are minimizing
reserves of modular functions so they
are closed and therefore tight subsets
form a hierarchy
it means they are either content in each
other or they don't intersect like um I
mean I will show you in a second
actually you can show that when agents
are independent they form a fool a
hierarchy which means that there is some
ordering in which every tight subset is
contained in another type subset they
are not they can't you can't have tight
the types of sites that are disjoint so
here is an example suppose X bar is your
normalized interim allocation vector and
in this example these are the tide
subsets a by itself is a tight subset
and a C and E is also a tight subject so
because of that proposition regarding
the tide subsets what I know is the
following like if I am at this scenario
for example a C and E because a by
itself is a tight subset it has to
always be allocated so in this case
there is only one choice I have to
allocate to it and let's say look at
another scenario let's say the scenario
is be DNE now is the is the only one in
this type subset that has appeared so I
have to allocate to eat there is no
other choice but now let's say if you
have a scenario like PC in this case you
have two guys from one tight slap that
end and v is not in a tight circle so
definitely a boy that proposition I know
that I cannot allocate to be but it
doesn't tell you between C and E which
one I should allocate to but now observe
that if you had a scenario like this
where we have a full hierarchy of tights
upset this would completely pin down the
allocation because if you have something
like a full higher here of this type
then basically when the agents come in
the reporter type you just sort them in
the order defined by these hierarchies
and you allocate to the first agent
based on this ordering right so it would
be nice if our interim allocation rule
normally centrum allocation will
correspond to some hierarchy like some
fool hierarchy like this and it turns
out that the vertices of that Polly may
treat the poly metroid of this of all
interim allocation visible
instrumentation rules have that property
so let me define this formally
a partial ranking is a ranking of a
subset of the type space so basically if
suppose s is a subset of types partial
ranking assigns numbers between 1 and
the size of s 2 is number so it ranks
them in some order every partial ranking
corresponds to a vertex of the poly
Metroid and vice versa every vertex of a
polymeric corresponds to a partial rank
and here is how you get from a partial
ranking to a vertex of the poly major
you basically look at let's say this is
our partial ranking what I will do is
the funny i will stop we'll start with
the vector which is all the zeros and
then i will start increasing the first
element in the ranking which is a i
increase a until 8i set a by itself
becomes tight and then the next one is
then i will start increasing see until a
sea becomes type and then i start
increasing d until a C and E becomes
tight and at that point i will stop
because b d and g are not in the rank in
the higher in the partial ranking so
their corresponding values will be 0 and
this gives me a vertex of that Palmetto
and this you can go also in the opposite
direction if I give you a vertex of the
parliamentary can actually use a greedy
algorithm to find a corresponding
ranking and the nice thing about these
rankings is that you can actually
deterministically implement the
allocation rule corresponding to rank
because if you have a ranking like this
then it means that whenever the agents
arrive this emitter type you just sort
them based on their type according to
this hierarchy and you allocate the to
the agent whose type comes first and if
for these other types like VD ng that
the corresponding term allocation rule
is 0 so just ignore that even if there
is no type who comes before them in the
ranking
two of them are the same example maybe
go back to the same time no no if you
have a partial drank in it the
hierarchies will be always a strict yeah
I'm saying that if you have a vertex
this vertex all always defines a strict
ordering on some subset of the elements
and it gives you a deterministic
implementation for that vertex and now
then then important the nice thing is
that if you're normalizing term
allocation will happen to be a vertex of
that poly mesh right there is a
deterministic implementation for that
and if it is not a vertex what you can
do is that you can write it as a
communist convex combination of the
vertices of the polymer tray and you can
sample a vertex from that convex
combination and then implement the
disturb like user deterministic
allocation corresponding to that vertex
and it's actually it turns out that you
don't even need to write it as a convex
combination what you can do is that you
can round this X to a vertex in such a
way that every coordinate in expectation
remains the same so I will give a quick
this is pretty a standard I will give it
like a quick sketch of how you can do
this so let's say this is your
normalized interim allocation rule and
these are the tight sets that you have
so far let's say si on ER I both in the
same title what you do is that you start
increasing CN at the same time decreased
by the same value until for some Delta
this set a and C becomes type and you
can also do the opposite thing like
increase and decrease see until a he
becomes type and you can always show
that at least at one of the news that
those sets become tight before the other
entry reaches zero and this is always
the case and then you will randomize
between one of these two updates with
probability is proportional to delta and
delta prime and if you do this at random
like do the left thing with volatility
delta prime over the sum of the founders
of renowned do the right thing otherwise
in expectation the value of every
coordinate remains the same but you have
increased the number of tight sets by
one so if you keep repeating this thing
after a number of iterations you will
get to a vertex in which every
coordinate in expectation is the same as
your original point and you can actually
show that the number of iterations that
you need is at most and the length of
this vector
your fashion solution yeah yeah it's
exactly the same thing I've been tested
extreme points because this gives you
might be on Montalvo's more supportive
but the thing is that if you want to
write as a convex combination of the
vertices it could be like you don't want
to write as a convex combination of
exponentially many verses right it was
will do yeah this would do that but the
thing is it's fine because we don't we
just want to sample from that comics
Commission so we don't have to go
through the trouble of writing it a
common combination of a small number of
verses we can just use this randomized
rounding approach which just gives me
one vertex it could be yet you're right
the support of this randomized rounding
is exponential but we don't care about
if we just want to sample from this in
polynomial time so and this basically
gives me an implementation for any
normalizing through medication so let's
look at some generalization of this so
now instead of having a single unit of
an item suppose you have a Metroid which
has specifies your allocation constraint
which means for any subset of agents
they can be simultaneously allocated to
if their types form an independent set
of the matric it turns out that the
condition for feasibility and the this
necessary and sufficient for feasibility
is the following where the RM is the
rank function of the metric you
basically replace that whatever you had
on the right hand side of your poll
image which concerns with the following
this is the expected rank of the subset
of types that have appeared intersection
some set s that comes from your poly
Metroid constraint and like considered a
case where you had one unit of item and
in that case the rank function is just a
minimum of 0 and 1 sorry the minimum of
one and the intersection the size of the
intersection of T&amp;amp;S which means if there
is at least one agent whose type is in
set s this this whole thing would be one
if you have a rank one uniform a tree
and otherwise this would be 0 so this is
exactly what we had before but this is
more general this works for any Metroid
environment and all the other things
that I explained actually
works for these environments if you have
a vertex at vertex again corresponds to
a partial ranking and what you would do
to compute the allocation for a vertex
you would just go through that ranking
as specified for the vertex and you look
at the agents types you try to advocate
to to them one by one if if the set of
current if the current set of allocation
remains an independent set basically you
basically pick a maximal independent set
from every prefix of that ordering and
this can be actually generalized even
more actually the Metroid itself could
be randomized so this this is and the
same condition is still hold and this is
useful for example in environments where
we have a stochastic supply I will give
an example of this and this can be
extended even to environments with
polymers for your constraints so so you
can explain again what is gia I GM of s
if this is just this this quantity this
is the expected value of the rank of the
matrix where the expectation is taken
over all possible scenarios of the
intersection of some set s and the sort
of types that have appeared in that
scenario so this is basically telling
you the following support if suppose if
I fix a set s of types what is the
maximum that you can allocate to this
set this is the the rank of that set and
you can only allocate to a type if that
type has shown up so this should be the
intersection of TNS right so in a sense
this is this is telling you how much you
can allocate to this specific subset in
expectation over all possible scenarios
so let me give you an example of a
scenario a problem that key that can be
solved optimally by using this approach
suppose you have and you're in a display
advertising you want to sell contracts
for display advertising you have an
advertiser each advertiser has a budget
constraint and they have each advertiser
has evaluation for each type of
impression and they also have a cap on
how many impressions of each type they
want and the advertisers also are also
partitioned into certain categories and
the search engine wants to ensure that
from each category there is a cap of how
many advertisers from that category get
impressions of a certain type and and
also suppose that your exact supply is
not known because you're selling this
contraction at once you don't know how
many impressions you're going to get in
the future so this is something that can
be modeled exactly in the framework that
I can that I explain and we assume that
all distributions are not and you can
solve this problem optimally like you
can compute an auction for maximizing
revenue optimally for this setting by
using the framework that I explained in
polynomial time and like one thing that
I it's worth mentioning is that for if
for each advertiser I want to also
specify a cap for the total number of
impressions that the advertiser would
get then it would not the dis
constraints would not be a single point
measure it would be an intersection of
two pollination so that we still don't
know how to handle but for this for
setting which is as like generalized
what you have here you can still solve
it optimally in polynomial time okay so
the second part of my talk is a
decomposition via of an exhausting
allocation rule and so I will try to go
over this quickly and this is this
actually came before the first one and
Crona like this is a paper from last
year and okay so an example application
rule is defined as the following suppose
for each agent I will take the expected
probability of allocation over all
possible types of that agent like this
is for example the sum of the products
of these probabilities and this
allocation like point 5 times 1 plus 0
point five times this and you get a
single number for each agent and I call
this the eggs on T allocation so just
specifies that what is the expected
probability that you allocate the item
to the agent if you don't know the type
of that agent now I want to do some kind
of decomposition similar to what i did
with the interim a locational and
observe that the eggs on teh allocation
rules are much as small like if you
could use this it would be another
exponential reduction in the complexity
of the problem because the interim
allocation rule specifies the
probability for each type where is the
excellent educational justice specifies
one number for the agent pair each I
like item or resource so again we have
the face the following questions how can
be approximately decomposed mechanisms
why i exalt a allocation rule and when
an exam time allocation rule is feasible
and how can we compute the optimal or
approximately optimal one and how about
can we compute the optimal single asian
mechanism subject to any exotic
allocation so first of all observe that
if you have an interim allocation and we
compute the optimal single-agent
mechanism and if we cut then take the
corresponding exam to education wreck a
computing optimal singulation mechanism
corresponding to that this is going to
be larger in general than this one
because this this face is less
constraint because here you are
specifying an upper bound on probability
of each type here you're just specifying
an upper bound on overall probability so
this suggests that by using the ex-ante
allocation really will not be able to
get the optimal because this is like you
would expect to get something which is
like determine this could be in general
infeasible with the single agent problem
that you get from solving this okay but
there is a condition that it has to
satisfy for sure so you can see that for
example you have a one unit of item the
sum of the probabilities exotic
allocation probabilities shouldn't add
up to more than one this is always but
this is not of course a sufficient
condition like take this example suppose
there are n agents each agent has a high
type and a low type the high type
happens with probability 1 over N and
requires an interim allocation
probability of 1 and the low type
doesn't trip get any think any
allocation and this case you can see
that the ex-ante allocation of British
agent is 1 over N so it satisfies this
semi feasibility constraint but of
course it's not feasible because what if
two agents with high type appear at the
same time you cannot allocate to both of
them with probability 1 but what you can
do what you can show is that you can
actually get a 1-1 over the
approximation of any eggs ante
allocation like by scaling down these
probabilities you can actually implement
this in a feasible way and it this is
tight so I was talking about this more
in a minute
I claim that there is some kind of
multi-agent decomposition and I'd like
before which works as follows basically
they're there in this decomposition that
there are a bunch of single-agent
mechanism and each single agent
mechanism allocates to subject to some
semi visibly some semi feasible eglinton
allocation rule and then there is a
central allocation mechanism which tries
to resolve the conflicts between these
single agents because they could over
allocate and but it does it in a way
that would ensure that every allocation
is preserved with some probability gamma
which I will define later so I will
define this more precisely in the next
slide and I claim that gamma equal to 1
minus 1 over e can always be achieved so
this suggests that you can get a very
good approximation like you can get a
1-1 over the approximation by using
ex-ante allocation so I will make it
more formal in the next display but
another interesting thing that I will
show you actually I will focus on the
second one more is that you can actually
do it in an online fashion and you get a
one half approximation what's that the
workplace
oh yeah it's amazing say I said means i
shouldnt it's not our Chi the Tai Po and
so I will explain how this can be done
by a reduction to this magicians problem
which is something that I find that and
then I will explain that in the next
slide but suppose that we could do that
then I claim that an approximately
optimal mechanism can be obtained by
using eggs on T allocation rulers
flawless let's capital X denote the
space of eggs on T allocation feasible
semi feasible eggs onto allocation rules
so if you have a single item for sale
this would be simply that inequality
that the sum of the exalted allocation
rules shouldn't be more than one or if
you have K units of item shouldn't be
more than K now let's say this is the
expected revenue of the optimal
mechanism subject to some exotic
allocation again we show that this is
always concave for any maximization
problem in any setting and you can get
an upper bound on your optimal mechanism
by solving this maximization problem so
this is not something that you can
achieve this is a strictly an upper
bound on what the optimal mechanism can
do because this ex ante allocation will
serve semi physical they may not be able
you may not be able to implement that
visibility but I claim that you can
always get at least one half a proxy
factor of the upper bound in expectation
by a reduction to the magician problem
okay so this is the interesting part
which is called the magician problem
summation probably is the following I
suppose you have a magician and there is
a sequence of boxes that arrived online
and are presented to a magician there is
a prize hidden in one of the boxes and
the magician has cave ones that you can
use to open these boxes if the magician
uses a wand on box I the box will open
but the wand may break with a
probability X I which is written on the
box and we know that the sum of these
probabilities add up to no more than k
where k is the number of ones of the
magician now and we assume that
everything has been arranged by an
adversary so the order of the boxes the
probabilities written on the boxes on
where the prize is hidden and
we want to obtain a good solution for
this problem and observe that in order
for magician to be able to guarantee
that he gets the price with the
probability of let's say gamma he has to
ensure that every box in expectation is
opened with a probability at least gamma
otherwise the adversary we just put the
prize in the box that has the least
exhausted probability of being open and
I assume that the adversary once he has
arranged everything he cannot make any
changes like once the boxes ever started
arriving that Mercer cannot make any
changes anymore and further my assume
that I assume that the magician has no
prior information about anything not
even the number of boxes heat the only
thing that he knows this K and I present
a solution which is booked each box in
the roads yeah maybe this but yet the
bond of Master Ives you see the number
which is written on the box you have to
make a decision about the box whether
you want to open it or not if you escape
the box you cannot go back to the box
you have to make you seduced addition
will depend on how many once you've left
unbroken so far the decision you good
yeah your decision is based on the pure
past history and like the number that
you actually I show that the decision
only depends on your past history it's
not even it doesn't even depend on the
number which is written on the current
box you can make a decision like only
based on your past history the summation
of expired lesson yeah that is that is
what you know but he doesn't know how
many boxes are gonna be there like could
be like he doesn't know end like many if
anybody even he when he sees the last
box he doesn't know if there's gonna be
another one or not okay so I present an
algorithm which I call again a
conservative mention which guarantees
that each box is opened with a nigga
zante probability of at least gamma and
it tries to minimize the number of
broken bones and I prove the following
till I show that for gamma equal to this
number or anything less than this it
never needs more than K ones and observe
that when k is equal to 1 this is one
half and wait ven que gets larger this
actually goes towards one so which means
that if you have a large care you can
actually get something which is pretty
close to one
and here is how we can use it to like
solve the like this is just like one
example of how we can use the magician
problem to solve the the auction
decomposition using eggs on T
application suppose you have care units
of one item and you have unity among
buyers you want to sell them to maximize
in a way that would maximize your
revenue you solve this a convex
maximization problem which is subject to
these exalting allocation rules being no
more than K and this gives you an upper
bound on your revenue can be ready but
this is not of course obtainable now
what you do is you do the following we
create a gamma conservative magician
with K ones and you set gamma equal to
this thing and for each of the buyers
you do the following you write this X
aunty allocation rule on a box and
present the box to the magician if the
magician opens the box then you make the
item available to that the single agent
problem corresponding to that mechanism
is to correspond to that agent which
means that a mechanism may either
allocate the item or may not allocate
depending on what the type of the agent
is by this exhausting allocation rule I
know that the mechanism will allocate
the item with probability at most this X
hat sub i right and if it allocates then
I will go back and break the magician's
man so you can see that the probability
that the magician's one gets broken is
no more than this exhausting allocation
and they sum up to no more than K fine
is in that box
the magician still gets the thighs even
for one but yeah yeah the prize is
actually in the van it was just to make
the justify why the magician wants to
open every box with some probability
gamma I mean the important thing is that
dimension can guarantee that each box
and expectation is opened with exactly
probably s discount and because of this
guarantee you can actually get this
fraction of every one of these single
agent mechanisms in expectation so
overall you get this fraction of revenue
and this can be extended to Combinator
elections when you have multiple units I
mean under certain assumptions I don't
want to get to the details of that and
the important thing about this is that
this can be implemented actually as an
online mechanism and here observe that
the ordering in which the buyers are
served is irrelevant because I assume
that you can serve them in any ordinary
as long as you know what wires are going
to arrive in the future can solve this
comics program and then you can
basically whenever they arrive you just
do this thing like you right the
exultation on a box and personalization
and you decide whether to sell to the
corresponding x or not so here is a
quick note you always make an online
team doesn't matter so much right what
do you see no the original product there
was nothing online but I'm claiming that
you can actually use it to do online
actually you can I will I wish I had
time to talk about this but and you can
solve the online a weighted matching
problem I online a stochastic weighted
matching from weighted be matching
problem and with this technique and get
this same approximation factor like you
would replace k with beaver if you have
like single one matching you would get
one-half approximation that is tight
actually so here is a sketch of how the
medicine works so this is the easy part
the actual hard part is the analysis so
the match the magician Chile computes a
sequence of thresholds tetas one up to
10 and it attacked like it computes
these thresholds as he sees the boxes
and what the threshold here is how the
thresholds are used and let's say si is
the random variable denoting the number
of
once broken prior to seeing bugs I if si
is less than the threshold which means
if we haven't broken too many bands then
i will open box I if it is more than the
threshold then it does not open the
marks and if it is equals the thresholds
with randomized and with some
probability open the box where this
exact probability comes from a dynamic
breaker and here is how the thresholds
are computed for each agent for each box
I I will compute the tetteh I such that
the probability of having broken less
than or equal to theta I whines is a at
least gamma and I will choose the
minimum toy that has this property like
this is something that you can compute
by using a dynamic program but just by
having the knowledge of the
probabilities written on the boxes one
up to i minus 1 and by using the theta 1
up to that i minus 1 you can compute
these probabilities and then you can
choose tetas i such that this is exactly
this and then once you have that you
basically when you see the box I based
on the threshold and the number of ones
that have been broken so far you decide
what to do with box and it's actually
independent on the probability which is
written on back side that is the nice
thing so the dynamic program is easy to
like solve and it's also easy to see why
it works the hard part is to show that
when gamma is between 0 and this number
actually it could yeah when gamma is
that most like this number then the this
algorithm never uses more than K bonds
which means these thresholds are never
more than K because if the threshold is
never more than K it means that you
never mentioned never tries to open a
box if he has already broken all of this
came on and also I can show that a no
magician can guarantee a probability of
more than this for opening every box and
so you can see that this is pretty close
to
that's so it's almost close to optimal
so to conclude I presented the general
framework for constructing Beijing
mechanisms in a variety of setting as
long as you can solve the single asian
problem which is exponentially simpler
to solve compared to the multi-agent
problem you can get a optimal
multi-agent problem and the nice things
about this approach is that you can have
mixed classes of buyers in options like
most of the previous works like assume
that like all buyers in the auction or
this similar like they all have budget
constraint or they they all have
additive valuations or things like that
but suppose you have an option in which
you have swamp some asian has budget
constraint the other region has risked
parameter and so on and by using this
reduction approach you can actually mix
all of them in one auction and as long
as you can solve the single agent
problem for each one individually you
can have all of them at the same time it
works for a variety of settings with not
causing the authorities arbitrary
objectives and even with online arrival
and notice that in all of my all of the
things that I explained I never talked
about the payments or the incentive
constraints because all of those go into
the single agents problem so as long as
you can solve the single agent problem
you're fine and some of these techniques
as I mentioned for example for the
online is the cast equated matching you
can apply this magician problem to
obtain a very good bound so that was the
end of my talk I would be happy to
answer if you have any questions
clarify the relation to work you explain
to us so in the first you had no there
wasn't
oxidation ratio yeah the first one was
optimal the second one you cannot get
get optimality but it's okay to be
honest that for the second one is the
practical one because the first one it
relies on ellipsoid method so it's
purely theoretical but the second one is
actually pretty practical because they
sorry because the magician problem just
requires you to solve a dynamic program
and furthermore the single agent
parameters just as specify the exact
allocation rule is just one number for
each agent it doesn't depend on this
type of space of the agent like the
typist place of an agent itself could be
exponential in which case you cannot use
the first approach but still you can get
a very good approximation and it tells
you that actually one thing one mess
like one a conceptual message of this is
that when when you extend this to
combinatorial actions and this condition
of like what the condition that you get
is the following if you have an
environment in which the maximum demand
of each agent compared to the total
supply is less than 1 over K for some k
then you can get this approximation of
the optimal which means if that if the
demands of the agents are the small
compared to the total supply you can get
a very good approximation of the optimal
without going through all the problems
of like solving optimization problems
over polymesh ways I mean you just lose
a very small tiny fraction but you get
something which is practical so
the other the ladies exclusively to a
contention resolution problem
warlock and
context transmission I don't know about
that because I example a so one thing
you can do to believe it doesn't get you
half but should you mean there for a
first part or a second see this problem
cousin it like you said that it wasn't
online but there's some people I think
applied to study it all online sitting
because what they're building you can do
is just say I'm going to toss a point
for everybody independently with let's
say X I by 2 if I see excite I talked
with Excel bite you get a 1 for
approximately but here you get to one
half yeah okay yeah so let me tell you
one thing if you so what I explained was
something which is very closely Upton
was what you can do is that actually you
can do make a decision independently for
each agent and then like drop these
boxes at random and instead of this you
would get something which looks like 1
minus order of log K divided by square
root of K or a square root of block ay
divided by square root of K something
like that but that is I mean that would
be something which is a good in
asymptotically but for a small K and
it's not very optimal also quite for
everybody and then and then people who
show up on beverage you have to to scare
them yeah and choose something well you
can actually you can even do it online
like you can as they arrive we toss a
coin and with some probability you just
keep them or we dropped it but you if
you do it that way like this is an
elective approach which decides based on
the previous history but if what you're
saying is a non adaptive approach and
you cannot get better than the like I
said 1 minus square root of logic
divided by square root of K
okay so the good question so some of the
important questions are the phone so
first of all what I explained and it
settles the complexity of class of
problems where you have polymetal
concern but what if you have
intersection of two poly matrix status
is it pretty important class like if you
have for example a bunch of items and
you have a bunch of unit demand buyers
each one want at most one of those items
that's like intersection of two matrix
convened at the intersection of two pull
matrix so how about those cases can be
also compute the optimal mechanism for
those in polynomial time so that is one
of the open questions and I think that
is the most like that is the important
question I still have in mind
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>