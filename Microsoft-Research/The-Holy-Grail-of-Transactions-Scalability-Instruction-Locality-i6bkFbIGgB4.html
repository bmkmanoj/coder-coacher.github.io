<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Holy Grail of Transactions: Scalability &amp; Instruction Locality | Coder Coacher - Coaching Coders</title><meta content="The Holy Grail of Transactions: Scalability &amp; Instruction Locality - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Holy Grail of Transactions: Scalability &amp; Instruction Locality</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/i6bkFbIGgB4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so we are pleased to welcome Tina Tyson
here today you know is going to talk
about how to speed up the transaction
processing and improve scalability many
of you already know Pienaar from various
conferences in various discussions but
just for the record she works with
Natasha Allah make his group at epfl
where they've been pounding away on how
to speed up transactions for quite some
time so welcome thanks and thanks a lot
for joining my talk I'm really happy to
be here today so as we run one of the
most fundamental data management
applications on on today's commonly used
server hard way most of the underlying
infrastructure is actually wasted we can
hardly utilize all of the processing
units and more than half of the
execution time goes to waiting for some
item to come from a memory when we
execute transactions so today I will
initially talk about why this is the
case and then we will see some
techniques and insights to overcome
these problems so I wasn't sure about
the scope of people that might be
watching so initially what's transaction
processing well it was the primary
reason why relational data base model
was invented back in the day and today
it's still one of the most fundamental
applications in data management mainly
because it's crucial while handling
money in banks in finance sector and so
on and some of the characteristics for
these applications are that you see many
concurrent requests to the data and
these requests usually attach a small
portion of the whole data for example in
bags usual deal with a few accounts
in the scope of the transaction and they
also require high end predictable
performance now today we also have many
online applications that deal with
transactions and even though these have
different higher-level functionalities
and goals the core characteristics don't
really change and in fact these online
applications they amplify these
characteristics meaning that you see a
lot more concurrent requests to the data
and you have a lot bigger data meaning
that touching a small portion of it
requires really smart and index indexing
and caching mechanisms to be able to
maintain high and predictable
performance now moving from application
space to the hardware we run these
applications on before the last decade
Hardware initially innovated on implicit
parallel ISM through techniques like
pipelining instruction level parallelism
and simultaneous multi-threading at
around two thousand five due to power
concerns when they're stopped putting
more complexity in a single core but
they started to add more cores within a
processor so that we would be still able
to turn more slow into more performance
and today we have multiple increasingly
have multiple processors in one machine
so the cortical communication latencies
are no longer uniform and finally in the
future and we already start seeing some
examples of it of this today there will
be even more cores in one processor and
more processors in one machine and also
we expect them to be heterogeneous in
terms of their functionality so looking
at this picture basically what hardware
has been giving us over the years
was more and more opportunities for
parallels in even though it's leveled
and type changed over time and also it's
becoming more and more non uniform and
heterogeneous in terms of memory
management and functionality so what's
crucial for us for for the software
applications is that we need to be
exploiting this parallelism because it's
hardly unlikely to disappear with the
emerging hardware and also we need to
become aware of the nine formty and
heterogeneity to know how to optimally
do our memory management and and task
scheduling over these heterogeneous
course so let's now see how well
traditional transaction processing
systems can can exploit what's given
from the modern hardware here we
consider a very simple transaction that
just looks for one customer in the
database and reads the customers balance
we are running this transaction onshore
empty storage manager which is an open
source storage manager that has
traditional data management system
components and and also it's
specifically designed for the multi-core
era and we are using an Intel Sandy
Bridge server here so on the x-axis we
are increasing the number of worker
treads executing this transaction and
y-axis plots to speed up over the
throughput of a single worker thread so
ideally what we would like to see is as
we increase the number of worker treads
they would all be able to utilize a
corresponding number of course in the
same way so we can achieve a linear
speed-up and that's what the red line
here shows but what happens with with
the traditional system is that it's not
bad you still see a straight line but
the problem is that the gap
between the ideal line and the
conventional line it increases as you
increase the parallel ISM in the system
so this is not a product a problem just
for the current era where we have this
will become even worse with the next
generations of hardware yeah it is sure
empty we are using so it's similar to
like really a traditional commercial
systems in terms of its components and
and how it is design so on the other
hand also can actually the single thread
get the highest robot it can get on this
Hardware how well we can utilize a
single core regardless of the
parallelism that we have and for that we
are looking at the instructions retired
in a cycle when we execute a single
thread and the hardware we use has four
main issue processors meaning that in
each cycle each core can actually
execute up to four instructions but with
four transaction processing you can
barely retire one so there is quite a
bit of under utilization in this level
as well to sum up our the problems of
traditional transaction processing one
is that we cannot really take advantage
of the available multi-core parallelism
and there is quite significant under
utilization of the microarchitecture
resources within a core yeah current
Intel CPUs like how like how free of
structural complex is a four-way issue
can you really issue for instructions
and
and execute like top to bottom through
the entire pipeline or we mentor in a
running to a bunch of structural complex
to do for like issued for church I mean
it's it's hard to get 24 that's true but
like with applications that have smaller
instruction footprints let's say you can
usually go for more than two at least
like with the spec benchmarks and and so
on they can go way higher than this
so during the next half an hour I will
present several techniques that aim to
overcome this problem of
underutilization but I want to initially
give the higher level inside behind all
of them so in when we traditionally
think of transactions we tend to think
of them as a one big single task that we
need to assign somewhere and execute so
they are a black box to us and this
leads to suboptimal memory management
decisions and scheduling decisions for
the tasks attend now instead of doing
this we will think of transactions as
form of final grain tasks and or parts
and we will determine each of these
parts based on the data and instructions
accessed in each part and based on the
locality you want to improve you can
give more importance to data or
instructions then we will assign each of
these cores to one of these part and
basically schedule transactions
dynamically over this course and by
doing that if we go back to the graphs
we have seen before now I added the
green line and the bar we can get much
closer to the ideal lines we would like
to achieve both in terms of scalability
at the whole machine level and in terms
of utilization in a single core so for
the rest of the talk we will first focus
on scaling up transaction processing on
multi-core processors then we will see
how we can improve utilization in a
single core and in the end I will
conclude with some more higher level
takeaway messages that I think are
applicable for software systems in
general and show what is there to focus
on at this step so let's start with a
scale
ability now on this picture we see a
sketch of a shared everything
architecture the traditional shared
everything architecture for transaction
processing at the higher level we have
the management of the worker threads in
the system and at the lower levels we
have the management of database pages
and these pages can be either the data
pages that keep the actual database
records or the index pages that allow
fast accesses to the data pages now one
request come from different clients we
usually assign them in a random fashion
to to the worker threads of the system
and here we have the blue and green
workers so since the data is shared we
need to ensure safe accesses to this
shared data because threads might have
conflicts over the data so we have a
lock manager structure that basically
ensures isolation among different
transactions and protects database
records and at the lower levels of the
system we have short-term page latches
that ensures consistent read write
operations to to this data so basically
all these like acquisitions or you can
have different methods but in the end
you declutter the execution of a
transaction with many many critical
sections if you look at the critical
section breakdown in this architecture
using again a very simple transaction
that just reads one customer and updates
the balance for this customer we see
that even for this simple transaction
you need to go through around 70
critical sections and what's more
problematic is that most of these
critical sections belong to a type that
we call unscalable critical sections so
these are the ones that as you increase
the parallelism in your system the
contention
these critical sections has higher high
chances of increasing as well and
database liking lighting belongs to this
type via do you have often how many
critical second would you take to
transfer money from one account to
another where you could port you have to
be careful that there is money in the
first account will that be roughly
double it will roughly double this
because both are like reading one data
through the index structure and the
other two types of critical sections
I've shown them in lighter colors
because they create less contention so
the co-operative critical sections are
the ones that threads contended for a
resource can combine their requests and
do it all at once later so in in
databases group commit for for logging
can be an example for this and we also
have critical sections that create fixed
contention they usually satisfy some
producer-consumer like relationship and
they happen only among fixed number of
threads regardless of the parallelism
around so some critical sections within
the transaction manager is a good
example for its yes this data science
student is it for a particular data sets
as the number of tea the amount of data
increases will this go up so this is a
say in it's not exactly independent of
the data size because like as you if the
index size increases for example
probably you will need to latch more
pages so that might increase but not
exactly proportional like linear
so what is unscalable pretty flex the
second side so those ones are the ones
like where you let's say go from four
chords to eight course the contention on
these the treads contended for these
critical sections you might have a
similar increase for example like
previously on this page you only have
four threads that might get this latch
whereas if you move to eight or more
course you have higher number of threads
that might be wanting to get this ledge
so that kind of increase so these are
the ones that will become like even
worse when you have more cores in your
system that's what why we call them and
scalable so to sum up the
unpredictability that we have while
accessing the data leads us to be
pessimistic and therefore we execute
many unscalable critical sections during
a transaction and this eventually hinder
scalability in our systems now to
overcome this we will redesign both
layers of this architecture at the
higher level we will assign specific
parts of the data to specific worker
threads for example here customers from
whose names start from A to M will be
given to the blue worker and the rest of
the customers will be given to the green
one so this way we can basically
decentralize the lock manager and get
rid of the unscalable critical sections
on on the like manager and at this level
this is just a logical partitioning of
the data now going further down we will
replace the single rooted index
structure with a multi rooted one and we
will ensure that each sub tree in this
structure it will map to one of the
partition ranges at the higher level
and further down by by slightly changing
the database insert operation we will
ensure that each data page is pointed by
a single index leaf page so by doing
that we can ensure single-threaded
accesses to both database records and
end database pages and this architecture
we call it physiological partitioning so
like even though you partition the data
structures you still have a single
database instance so you you are still
in the in a shared everything system so
due to partitioning you might have some
overheads with multi partition
transactions and so on but it's less
compared to a pure shared nothing
architecture yeah I'm a little unclear
on your goal here on so when you show
that ideal chart that an idol chart in
assuming that there's no data retention
a segment because everyone is updating
the same hot item there's nothing much
you can do right the idea idea like laws
not to look like that um yes but still
that's like that's the ideal for you
that's what you would like to achieve so
you want to do that independent of how
much data contention there is in the
system yeah or you like it close to that
as much as possible because for that
charge what I was executing was actually
contention free transactions yes so for
that one at least you should be able to
achieve this and that's a character for
reach there is no such thing as not
scalable unscalable part um so there's
actually there still is because
especially because of the lock manager
because in order to get a lock their you
first need to latch the lock manager and
so on so you still have unfortunately
those critical sections
so with this design we if we look back
again to critical section break down we
now eliminated majority of the
unscalable critical sections and turn
some of them to fix type of critical
sections and this way what remains now
are mostly lighter critical sections in
the execution so if this if you're
looking up in a cone name on customer
name ok the BG will already be
partitioned by the train right the top
level of the beach ring will already
happen so what is neo bot physiological
partitioning of it it's only one thread
smarty root I see look what's that what
happens when you want to operate across
accounts here though yeah so that for
those type of things you can actually
like if the requests are independent of
each other those can happen like in in
parallel and or if they are dependent
and like you will need to pass control
from one thread to the other so of
course there is some corals associated
with that but as long as you can keep
such communications within one processor
like if you are not doing acts or socket
communication you still see the benefits
with this design that's our experience
at least so to show also show some
performance numbers here again we are
looking at the transaction from before
and increasing the number of work
addressed executing this transaction on
the x-axis and we plot the throughput on
the y axis which is transactions
executed in a second and it's in
thousands so the physiologically
partitioned design it performs better
and skills better compared to the
conventional design boat on the San
Niagara architecture that has support
for more Hardware context and on
AMD an architecture that has faster CPUs
and what's more important also for us is
that you see more benefits as you
increase the parallelism in the system
and that's what we want to see
especially for scalability and this is a
real names by matter we just explained
what are the critical section got
eliminated because of partition so we
eliminate basically latching on the lock
manager so you still need to lock for
the database records but like you have
no contention on the records and again
you have no latch for the lock manager
itself so that's what you eliminate and
also you eliminate latching on the index
pages
and latching on index pages like even if
you are read-only is sometimes like if
you see many requests rushing to the
index root it also creates a huge
contention even for read some read-only
transactions um do you have any more
questions related to this part yeah what
if the workbook becomes Hugh over time
and you're essentially running single
threaded through one of the partitions
denied your repartition yeah yeah we
have a mechanism like we have a
monitoring thread that that basically a
monitors the sub ranges depending you
can a user can determine the granularity
there but we monitor the sub ranges and
and also tread cues to see how contented
each partition R and and we can
repartition online based on that and
also like in this system for example if
you want to repartition let's say you
want to put this part of the range to
hear you just copy what the content seer
to hear and you don't have to worry
about what's underneath so you don't
have so much data movement yeah I don't
know so I start so what's about
individual using in your graph for BLB
it's clear that it's kind of it's also
underway on um so on this Hardware like
in the end for 64 and 16 number
creatures it's actually OS is the
problem because you start to hit some
como is in terms of scheduling like it's
the boundary for the machine so that's
what creating what creates the problem
for for these experiments and and also
you still have like some unscalable
critical sections here which either
comes from buffer pool or or some again
you still have some centralized
structures internal structures and we
have recent work actually that also
partitions them at socket boundaries so
you can eliminate
these as well or Iowa the memory bad
network it's we don't have a 0 in those
experiments so it's mostly a rabbit from
the cpu today your memories and i will
say for these machines it's mostly the
operating system because we don't
enforce any a core affinities for in
these experiments so it's like the way
it does scheduling becomes the problem
towards the end yeah you have to get
much more improvement on the am Dean
another Niagara what is the reason there
are simple reason um so that's actually
I think it's just related to a core
being faster there so it's just the
processors being faster it gives more
benefits I'm trying to understand so
between 16 and 32 let's say in one of
your graphs ok so the 16 will have 16
partitions in a b-tree and 32 left 32
additional you are these two different
beauties or is it the online i'll go to
come using it will automatically ok for
these experiments it's like you actually
have the whole data in place but like
430 to work or Tracy I you deal with 32
partitions and here so what happens if
you have a transaction there's complex
there you know I just multiple entries
is it partitioned across multiple
workers are is a single worker
responsible for the program um no it's a
partition that grows different workers
so for each table you have like a
different set of workers sure
architecture to support a yes so this is
just for this design we have that so
default in default sure you will have a
single thread executing a single
transaction for this design which I mean
it looks like a big change i mean how
transactions are executed
yeah we had to change some things at the
Y at the thread management layer quite a
bit of things at the thread management
layer as well that's true but you can
automate it to some extent using like
something x equal front end because we
were using some the sequel front end of
postgres to determine basically the
different independent parts of a
transaction like you can automate it to
some extent so we have seen how we can
improve utilization at the whole
processor level now let's see what are
the problems within a core for
transaction processing ok so initially
let's remind ourselves how the memory
hierarchy of a of a typical Intel
processor look looks like today so we
have closest to the core we have the l1
caches which are split between
instructions and data then there is
there are dealt with caches which are
also private per core and then there is
an shared l3 cache or last level cache
and in the end we have the domain memory
which today is large enough to fit the
working set sizes for for most
transaction processing applications so
as we go down in this hierarchy
basically the access latency is
drastically increase as as expected but
like in practice if you find some item
in l1 because of the out-of-order
execution you usually pay no cost for
this but if you cannot find the
instructions or data you need in l1
caches then you need to go further down
and the core might stall because of this
now let's see how much of this toll
we actually have 44 transaction
processing on this graph we are breaking
the execution cycles in too busy cycles
and end the stall cycles we are looking
at the standardized TPC C&amp;amp;T pce
benchmarks here we are running their mix
on onshore mt and we are using again
until Sandy Bridge server now busy
cycles for us they mean the cycles that
you can retire at least one instruction
and stall cycles are the ones where you
cannot retire any instructions so from
these toolbars we see that more than
half of the execution time goes to
various stalls now if we want to
investigate why do these tolls happen
the graph on the on the right hand side
shows the breakdown of the stall cycles
in a period of thousand instructions
into where they come from in the memory
hierarchy and also whether they happen
due to data or instructions for example
l3d means the stall time due to data
misses from the l3 cache as I said not
drawing these components on top of each
other it's a bit misleading because in
practice some of these tall x will be
overlapped with others again due to out
of order execution but it doesn't change
our higher level conclusions here so
looking at both of the virus we see
basically the l1 instruction misses
being a significant contributor to the
stall time for these workloads you you
might be able to overlap the supporting
efficiency ya know I just want
understand what they complete software
stack was here you are running short at
the bottom what did you have a talk we
have the application layer for sure
which has the benchmarks implemented in
you c plus plus yes yes yes so there is
no yeah communist does this include the
test driver part of it as well or just
ashore it includes the test driver part
as well but it's not I compared to shore
it's not so big that part so here you
might be able to overlap some of the
data mrs. from l1 caches to some of the
long latency data mrs. from l3 but we
cannot really overlap all of the
instruction related assault i'm here
with something else and also it's harder
to overlap instruction mrs. penalties
because when you miss an instruction you
don't know what to do next so it's hard
to compare to data seeing the problem
related to instructions now let's see
like what are the instructions
transactions actually execute where they
come from so when we look at that we
basically see no matter how different
transactions are from each other they
execute a subset of data based
operations from a predefined set and
these operations might be an index probe
index scan or record update delete
insert operations so they are not so
numerous now let's also say that we have
an example control flow for a possible
transaction and basically it has itself
a conditional branch and based on that
branch it might insert the record two
tables e or not so we also have two
trades in our system executing this
transaction with different input
parameters so t1 might executed without
taking the branch whereas t2 might
execute it by taking the branch and
inserting a record so overall even
turns out even treads executing the same
transactions they might not exactly
execute the same instructions but still
they share a lot of common instructions
because they share database operations
to quantify this commonality we we have
analyzed the memory access traces from
the standardized TPC benchmarks to see
the instruction and data overlaps across
different instances of transactions and
here I have the results for the TPC see
benchmark we have the TPC see mix than
the top two most frequent transactions
in the TPC see mix which are new order
and payment so each of the pies here
they represent the instruction and data
footprint for the indicated transaction
types and each of the slices indicate
the frequency of that portion of the
footprint appearing across different
instances so the darkest red part is for
example it represents the instructions
and data that are common in all the
instances for these transactions and the
light blue part is for instructions and
data that are only common in less than
thirty percent of the instances so from
these charts what we see that there is a
significant like a really non negligible
amount of overlap for instructions
whereas we don't see as much overlap for
data and and here we have a hundred
gigabytes of data meaning that at the
database records level considering
random accesses you don't see that much
overlap and and what what is common is
mostly the metadata information or index
routes also if we look at the
granularity of a single transaction
type we see more overlaps naturally
because same type of transactions they
have higher chances of executing the
same database operations yeah the
observation here we typed up the biggest
life because from here it looks like
most of his torture after because of
three time because there's very little
overlap okay oh crap suggests that most
of the stores that have instructions so
the problem is that even though you
share instructions the instruction
footprint is too big to fit in the l1
caches so like you like even though you
have common instruction parts you keep
lycra missing them because of the
footprint so it's like capacity related
and and for for data it's easier to
overlap the data mrs. so that's what and
also when you bring it's mostly related
to overlap at the hardware level
um so if we want to basically exploit
this instruction commonality across
transactions we decided to design an
alternative scheduling mechanism and for
that we again have to give an example we
again have to tread executing the same
transaction type but because of
different impulse day they don't take
execute the same thing exactly different
colorful parts your indicate the
different code parts and they are in
their pictogram larity that can fit in
an l1 instruction cache question sorry
instruction cache misses now why do they
go okay most on the be satisfied from
the l2 cache did you look at that yes
for sure yes you can so you don't have
to go from the processor you mainly
satisfy them either from l2 or some of
them from l3 so do a variation sorry
okay
so in the conventional case what we
would do is to assign these transactions
to one of the idle course and unless you
see an i/o operation or some context
switching they will finish their
execution on that core so let's say we
scheduled a t1 to the one of the idle
course and it will need to bring the
instructions it needs for the Red Code
part and so it will observe a penalty
for the instructions is it will see for
this part and only i'm going to count
such such misses on the side as cash
feels so next 1 t 2 comes to the system
again now both of these threads they
will observe instruction mrs. for the
code parts they need to execute and this
will continue till the rest of the
execution now rather than doing this we
will do the following google again a
schedule t one to one of the idle course
and it will again observe instruction
mrs. for the Red Code part but once it
fills up the l1 instruction cache we are
going to migrate it to another Idol core
and instead schedule the newcomer t2 to
the initial core yeah I would be 1c
instruction mrs. to all right
prefetching would please the
instructions affecting is a good one bit
either is it has a sequential so but you
still have some like either jumps or
branches in in the code so the next time
period feature doesn't handle all of it
I mean the stall time results I show
they are from run on the real hardware
so you it cannot get rid of all the
instruction misses
I mean you can try to have a better cold
layout but like it again changes
sometimes from workload to workload so
it's not that easy so yet this time t1
will still observe instruction mrs. but
t2 can reuse the instructions already
brought by t1 and again when these two
fill up their instruction cache is we
will migrate you want to another Idol
core until 2 to the core that already
has the instructions if you don't follow
the same judge it's 2000 have to be
exactly the same but similar at least so
we are assuming that you are trying to
batch as much as possible the same type
of transactions but we deployed it
without considering that as well it
still works just curious what is the
overhead of scheduling and paper another
core versus reading from an L 2 inch
change are they comfortable this is much
worse so the scheduling course of course
like if you consider just one hour to
read verses just one thread migration
the migration is cause there but in the
end you have this instruction locality
so you read reduce many reads from l2
like in the end it becomes I pays off
that you know the crucial part of it
when you migrate a thread that's what it
costs something that's overhead and then
it's going to execute for a little while
on that target core and it really falls
down through so well how long does it
need to execute on that target core for
that over into to get properly
advertised do you have a feeling for
that is thousand this one
ten thousand hundred thousand so we
didn't exactly measure that so we just
like left the transaction mixes running
over these cores and Andy Soule
improvement still so but like usually
since you see lots of instruction
commonality you'll benefit from from
this type of locality but I don't have
an exact number you don't know how big
the chunks where the executed one
segment so I mean it should be a most of
the time is close to the cache size like
but it doesn't have to be like exactly
all the instructions in an AK yeah it's
okay someone related to asleep what
exactly is a rich and yellow champ is it
based on the structure of the
transaction or is it based on number of
instructions of you so for this
technique um you basically like it all
happens at the hardware level so it's
just l1 cache size chunks as far as the
hardware is concerned like it's a
programmer transparent way of doing this
I will show another technique which will
try to align this with database
operations but the important thing is
that it just has to be l1 cache size
that's all the restriction berries are
you losing the one day Turkish already
completely these transactions move from
yes yes i will show numbers related to
that that that's a good point you know
one what's just how long does it handle
how long does a hand up 23 at stake in
general um so we so what we do there is
basically you you need to carry the
what's in the register files and the
last program counter so we measured it
based on the number of so we put them in
the last level cache migrate the thread
and read them back from there and we
calculated it like it's around 90 cycles
that's how the cost to do that
as long as you are within the same
processor and can users since there's
some kind of cache line and Bonnie back
and forth figure out
um so yeah but by doing this basically
we both exploit the aggregate at one
instruction cache capacity for a
transaction and and we localize the
instructions to cash's so you can
exploit instruction overlaps and this
technique we call it slick it comes from
self-assembly of one instruction caches
and as I mentioned is the programmer
transparent hardware technique so you
need to dynamically track the recent
misses for a transaction and also the
cache contents in all of the caches to
be able to know when and where to
migrate so this of course include some
has hardware cause it's not so big it's
less than a kilobyte space called still
but we are doing something specific for
for our applications so what we want to
investigate is basically if can we have
more software hints to eliminate some of
these Hardware calls and at runtime and
and for that we looked at the
instruction and instruction overlaps
again but this time at a much final
grand narrative it's the ground analogy
of database operations and these charts
again they show the instruction overlaps
for different instances of update probe
and insert operations called within the
new order and payment transactions of TP
CC and if we compare these overlaps with
the previous ones the dark red part
basically it becomes a lot more
appearance so if we can align our
migration decisions with the boundaries
of database operations or cache size
chunks in each database operation we
might eliminate some of the hardware
costs and also achieve more precise
locality for for transactions and we to
do that we designed a dick which
comes from advanced instruction chasing
so it's formed of two phases you need to
do initially a profiling to determine
the migration points in the transaction
and also the second things does the
actual migrations based on the decisions
of the first phase so for the profiling
phase we as an example we again have the
transactions from before and let's say
we determine the following migration
points so initially you need to mark the
start of think point starting
instruction for a transaction then
whenever you enter a database operation
you need to mark the starting point for
this operation as well and here we have
index probe and as you are executing the
index probe once you fill up your l1
instruction cache you need to mark that
point as another migration point and
then start the second phase of the probe
and we do this for all the operations
called within each transaction type and
and in practice you do this analysis for
for for several transaction instances
and you'll pick the most frequently
occurring migration points for for for
the transactions so then during the
second phase we will first assign each
of the migration points to a core and
now when a thread comes you will start
it from the start core once it enters
the probe operation we migrated to the
call for the probe operation and instead
schedule the next thread in the queue to
the start core and this continues with
when as well as long as you hit the
migration points so this time we again
exploit a great instruction cache
capacity we again localize instructions
to caches so we can we can minimize the
instruction misses for transactions
but compared to the previous techniques
leak we don't have to have as much
hardware changes all we need to check is
whether the current instruction is a
migration point or not and also we can
we can force more more precise
instructions locality by by doing this
now looking at how how these techniques
perform in practice the evaluation we do
for this part it sits on a simulator
because trade migrations if you try to
do it with the current operating systems
we have it has a lot of course the the
operating system has a lot of
bookkeeping for the context switching
and doing that dynamically with some of
the P thread affinity functions
sometimes it doesn't migrate the trades
exactly where you want them to go if the
destination core is already full so we
implemented our ideas on a simulator
which simulates an x86 architecture here
we have a 16 out of order course and 32
kilobyte l1 caches and we are looking at
a thousand transaction we are running
playing thousand transaction traces on
this simulator and they are taking as we
run the workload mixes 40pc CNT pce on
onshore mt so on this graph we have the
l1 instruction mrs. in a period of
thousand instructions and we are
normalizing the numbers on based on the
numbers for the conventional scheduling
so both the programmer transparent
techniques lick and and more so very
very technique addict they can
significantly reduce the instruction
alone instruction mrs. but as you also
point to that there's a catch here and
that is when you migrate the migrated
thread you leave its data behind so if
we also look at the l1 they
mrs. in a period of thousand
instructions we increase the data missus
in a non-negligible way but our claim is
that for for data mrs. at l1 level they
are not on the critical path for
transaction execution because they are
easier to overlap with out of order
execution and and the important thing
here is to basically not increase long
latency mrs. from the last level cache
so not migrating a thread from one
processor socket tool and other one so
actually physically sort of moving at a
thread from one point to another usain
takes 90 cycles and so on but that does
not include all the bookkeeping that POS
maintains for the threads right yes
there's a lot of bookkeeping so how in
practice how would you get rid of that
don't even so in practice what can you
do is you can maybe have toys to have
some sort of like more lightweight
context switching for specifically for
for this case where you just copy the
register values and the last program
counter and don't care about like rest
of the things and actually so there was
this work called steps which try to
context which transactions by a batch of
transactions on the same core to
basically take advantage of a similar
instruction overlap and for that work
they actually hack to also have a more
lightweight context switching operation
like that and they inlined all these
context switching points in the code so
we wanted to be like a bit more
programmer transparent while doing this
as well but of course both has like
trados like in one case you need to put
more burden of the program developed
or in the other case you need to add
some stuff in the hardware yeah trick
for the scheduling that you have is very
fine grain so it's also it's also very
specific right so what happens when know
in the first transaction when it's in
the middle of a red cloth it has it
tries to get a lock and it's not able to
get a lock and so it needs to be
scheduled out so your entire picture now
get some more modern drive so is it
Clovis to such things or um so you don't
really like you don't affect correctness
of the team but because of those cases
you might like by doing this you might
add some additional bad Luck's maybe to
the system but like at least in our
traces like considering that you at a
higher level design your system in a way
that you don't have such high contention
that shouldn't oak you shouldn't go cure
so much false question do model Amelia
mrs. to do battle Fred migration at all
in your in your own simulation yeah we
so we add a course that ninety cycle
costs like whenever we do that migration
you wouldn't actually use like os-level
handoff here like you you just like to
use her level threading I like such a
long jump or something hand stuff across
there is
caliber so the way we do it now it's
like just hardware dress or hardware
contacts you can again do it maybe we'd
use your level threads as well but you
need you need a specialized migration
cause at OS therefore to be able to do
this in an efficient way or you can like
completely change your software
architecture if you really want to take
on with this that like have treads fixed
on on a core and like give them make
them responsible for a specific like
cold portion that is small and like
handoff like not migrated treads but
hand off work in a way attach the touch
transactions on the way on different
dress so let me I have just a few slides
so mom so to also see the throughput
numbers with with the same set up
basically improving instruction locality
even though you increase you hinder data
locality a bit it pays off so we get
like around forty percent improvement
with with the programmer transparent
technique and we can get around eight
percent improvement with the more
transaction where one now to sum up what
we have seen so far so we initially
looked at scalability issues at at the
whole machine level and how we can
improve utilization therefore
transaction processing systems and for
that we initially analyzed the critical
sections in the system and focused on
eliminating the most problematic ones
and we did that in our case using
physiological partitioning to have much
more predictable data accesses and this
helped us in eliminating majority of the
unscalable critical sections and the
then we looked at
the major sources of underutilization
within occur and we solve that a one
instruction mrs. pay play a significant
role there but we also observed that
transactions have a lot of common
instructions to so to exploit that we
develop to alternative scheduling
mechanisms one was programmer
transparent and one was more transaction
aware to maximize instruction locality
and then minimize the instruction latest
all time and we also saw that being more
software aware it actually reduces some
of the drawbacks this type of techniques
might have at the hardware level now I i
will mention some of the more like
higher level messages from this line of
work that i think are valid in for
software systems in general and and what
is there to focus on in in the near
future so throughout my PhD I focused on
exploiting hardware for for transaction
processing applications but something's
both regarding scalability and
scheduling these writing are applicable
in in software systems in general so
regarding scalability when we show how
well we scale up we tend to show the
performance numbers we have which is
also valuable but they don't indicate
scalability on the next generations of
hardware you might have so I think we
need more robust metrics to measure our
scalability and at least I think that
looking at critical sections and
analyzing critical sections in a system
categorizing them based on which are the
ones that hinder scalability the most
this might help us in terms of better
measuring our scalability also with the
increased parallelism and heterogeneity
basically scheduling tasks in an optimal
way will become
you'll challenge for for our systems and
that the crucial thing here is to know
where you need the locality to be like
for instructions for transaction
processing systems this was the l1 level
and for data I didn't go in detail to
that line of work but for data it's
important to keep data as much as
possible in either in your local L tree
or in your local memory bank so
basically we need to adapt our
scheduling by putting emphasis on the
type of locality we want to have for
instructions and data and for the future
now we start seeing already start seeing
some proposals bought from academia and
some industrial places on specializing
hardware for for the various database
operations most of these work they now
focus on analytics workloads but there
there is some that that deal with
transactions as well and like even
companies like Google and Amazon who
used to don't didn't like this idea very
much they now realizing that when you
have an application that runs on
thousands of servers on many data
centers it's better to have a more
specialized hardware for it to be more
cost-effective and in performance
effective and and also from the software
side basically we need to be a lot more
aware of the underlying hardware and
hardware topology to be able to know how
to do the memory management and how to
schedule tasks in an optimal way and
also we need to know which type of
special hardware we can actually exploit
for which type of task and in order to
not to blow tower calls for all the
different hardware types we can also get
maybe help of the compiler community to
dynamic
generate some of the code for these
specialized cases so with that I also
like building systems it's not the work
of a single person so these are all the
people I had a chance to work with
during my PhD and and with that I'm
going to conclude if you want to know
more details about the various projects
or see papers or presentations here is
my web site and the code base we use
it's also online available and well
documented so you can check it out thank
you very much and I'll be happy to have
any further questions you might have all
right any more questions or did we said
everything is answered yes holy grail
has-beens he is handsome all right Thank
You speaker I get</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>