<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tutorial: Monte Carlo Inference Methods | Coder Coacher - Coaching Coders</title><meta content="Tutorial: Monte Carlo Inference Methods - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tutorial: Monte Carlo Inference Methods</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0l31GVOXflk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I'm incredibly happy to introduce the
speaker for the second introductory
tutorial today Ian Murray who is going
to talk about Monte Carlo inference
methods if you were the deep learning
tutorial this morning you probably heard
young Li kun say that he's allergic to
sampling so I'm pretty sure he's not in
the room right now Ian is a lecturer at
University of Edinburgh and I met him a
while back when he was a PhD student at
the Gatsby unit at UCL in in London and
my understanding is that Ian was
actually meant to be a physicist I think
until he met David Mackay and took a
machine learning course with him which
sort of changed his life and one
interesting thing about Ian that you
might not know is that when he's not
sampling he's actually juggling and he's
one of the few people actually is really
the only person I know who is able to
juggle with five clubs all right so
without any further delay I'm incredibly
happy to introduce Ian thank you very
much can you hear me at the back
so just as the last few people filtering
I want to set expectations this is an
introductory tutorial so my aim is to
get you to understand about Bayesian
inference and how we use Monte Carlo
methods to solve these problems and
methods some of you might know about
called Gibbs sampling slice sampling and
how to make these things work so if you
are already the sort of person who's
derived probabilistic models and
implemented something methods for them
you might be better slipping out and
going next door to a more advanced
tutorial I'm not going to assume
anything right now so Monte Carlo
methods are things that I think everyone
in the nips community should know about
even if you're not going to use them
yourselves or even if you just use them
to quickly sanity check something you're
doing but it doesn't make it into the
published paper so this term Monte Carlo
is a bit funny it refers to just using
random numbers to do something on a
computer some people give it a precise
meaning but people have been using
random numbers for a very long time the
term Monte Carlo was introduced when
people were developing nuclear weapons
as part of the Manhattan Project there
were a lot of physicists who were using
random numbers and this term Monte Carlo
sort of evolved at that time so Nick
metropolis wrote this very nice review
about the birth of the Monte Carlo
method and one of the anecdotes I really
like from this story referred to this
guy Enrico Fermi so Femi was a physicist
and he was doing neutron scattering
experiments and he would come into the
lab with predictions of how they were
going to come out and he'd made those
predictions by staying up late at night
and running simulations but unlike us
now when we have computers kicking
around all over the place they did have
some of the early computers there but he
didn't have one he had a hand adding
machine on his desk and he was doing
Monte Carlo simulations by hand and it
was an incredibly useful thing to do and
gave insight into the physical models he
was running so now we do have incredibly
powerful computers just kicking about
all over the place and we can use Monte
Carlo methods to get insight
to a machine learning systems so what I
want to do in the next couple of hours
is to give you some examples of how we
get insight into our models by just
drawing some random numbers and having a
look how we use those random samples to
do actual computations to do math and
we'll look at some of the methods so as
I said we're going to look at important
sampling rejection sampling and these
algorithms called metropolis Hastings
Gibson's life sampling one of the most
important things with any method is how
do I actually run it so that I get
correct answers so I'll talk a bit about
that at the end but for the moment I'm
going to have a running example and I
want to I want this seminar to be
entirely understandable to my running
example it's going to be linear
regression we've got a straight line
with a slope and an intercept so our
parameters here are two numbers theta
and if you're Bayesian you'd put a prior
on these things so you'd say there are
these two numbers I don't know what they
are you could imagine the blue line
corresponds to some slope and some
intercept that's the correct answer but
we're never going to know what that is
so instead we consider all the possible
lines that might be the true model so we
could have this distribution just for
simplicity a Gaussian these are the
slopes and intercepts I think might be
plausible and we could look at that
model by for every vector of two numbers
we sample drawing a line saying this is
the corresponding model and if you do
that you immediately notice stuff like
there's a big bunched up region here
near the origin and maybe you hadn't
thought about that when you wrote down
the model maybe you need to think a bit
about how the data will be centered or
maybe we need to repr amatory as a model
to try and make the position of that
bumper more flexible so it can be a good
idea just to do a forward simulation
like look at realizations of your model
in question whether they make sense of
course what we usually interested in is
having data this is just a model so the
way we'd get data for this model would
be we see these black lines their data
points yn they're evaluated at our line
plus some noise so if we have data we
can use this prior distribution to come
with a posterior what do we believe
about these two numbers given the data
which is proportional to how well those
two numbers explain the data and how
plausible a priori they are so we can't
really see what's going on in this graph
because that region is really tight so
normally if we get lots of data
hopefully that data is informative and
our beliefs will be much tighter and in
a smaller region than our prior beliefs
so I'll zoom in on that region this is
exactly the same graph I've just zoomed
in and you can see the the data points
and this posterior distribution what
I've done is I've drawn samples to
understand it so I've drawn twelve
samples from the posterior distribution
none of these purple lines are the
correct answer I will never be able to
discover that correct answer the blue
line from a finite amount of data all I
can do is say which models are plausible
and which ones aren't so the twelve
samples there are showing twelve
examples of the sorts of model that
could have generated our data and you
see they surround the correct answer
near where we've got lots of data we
have not really certain so we know
exactly what's going on there and as you
move away from the data you get the
possible explanations really spreading
out what predictions they'd make so
Bayesian inference just automatically
does this very clever thing that if you
go to a region where you don't have data
you can automatically be less certain
and that's just fallen out of the maths
we didn't have to put that in as a hack
we've got this nonlinear envelope over
where the lines are even though we've
got an underlying linear model so we get
an interesting predictive distribution
by looking at these samples I just push
this example slightly further there's
only so interesting linear regression
can be but what happens if a data aren't
linear so usually in the real world
we're going to get data which it's more
complicated than our model so here I've
got some data that if we look at it
that's clearly a nonlinear trend we
could ask ourselves what would happen if
we did Bayesian inference
with this dataset but using the same
linear regression prior that I've
already showed you and this is something
I asked my students back home so I give
them a quiz question I'd say we've got
this data set and I'm going to do the
same thing I did before I'm going to
draw 12 samples from the piste area and
say what possible lines are plausible
given this data set and various things
might happen so for example is the model
going to choose to explain just the
left-hand part of the data or the middle
of the data or the right-hand side of
the data or maybe all of those things
will happen so maybe if I draw 12
samples I'll get four of each or roughly
four of each and maybe something else
will happen what is not going to do is
come up with a nonlinear prediction
because it's a linear model and that
maybe there's something else we haven't
thought of so something I do with my
class is make people raise hands there's
a lot of you here and I'm not gonna be
able to count so I'll just let you pause
and think for a moment so you come up
with your answer and what happens is I
can draw my 12 samples from the
posterior and here they are
so there are 12 purple lines but they're
all almost sitting on top of each other
so from these samples I can see that the
posterior distribution is incredibly
certain that it knows exactly what the
explanation of this data set is it's
clearly wrong like this point here is a
12 sigma outlier like if you just look
at the data set and you look at the
model you can tell that it's massively
it'll fit but nothing in the Bayesian
computation for computing the posterior
notice is that it assumes that your
model is correct so if you have a model
which is too simple that's not able to
express express the flexibilities in
your data set then you can end up very
strongly peaked around the least bad
explanation you don't get this trendline
here because that would be a 20 sigma
outlier which is almost infinitely bad
so you're forced to find the least bad
situation and
so looking at the samples and doing some
sort of model checking can be a really
important part of a Bayesian procedure
okay so looking at samples is something
I do all the time I'm a bit of a nerd
and I like sampling so I just like
looking at these things but really it's
a computational audience we want to know
what math can we do after we've drawn
these samples so this equation is what a
lot of people mean by Monte Carlo some
people if they say the term Monte Carlo
they mean what's on this slide and it's
about integrals here I've got an
integral over some parameters weighted
by a distribution over those parameters
of some function and the meaning of this
integral is it's just the average value
of that function if I were to consider
the distribution PI as where the samples
come from and just saying that it's hard
for me not to say oh it's the average
value you'd get if you just drew a bunch
of samples from that distribution
pine-sol the almost obvious Monte Carlo
estimate of this integral is to draw a
bunch of samples from the distribution
evaluate the function for each of those
samples maybe twelve and then just take
the empirical average of the samples
instead of the true average which is an
integral that means you'd have to
consider the whole space every possible
setting of the parameters so Monte Carlo
summation is in some ways an obvious
idea it's it's something that people
running surveys do all the time so for
example imagine I wanted to know the
average IQ of attendees at MEPs
okay that's a big sum it's an iliyan
integral and the exact answer would be I
would round up all 4,000 of you
administer an IQ test every single one
and take the average value and I could
clearly get a pretty good idea if I did
the much cheaper operation of grabbing
12 of you at random giving 12 of you IQ
tests and taking an average of those 12
numbers and that would tell me sort of
what percentile roughly you were and I'd
have to be a bit careful about how I
gather those sample
they probably don't want to just take
the 12 people on the front row down here
that might not be a fair sample of the
distribution I'd have the Keener's so as
with the rest of this talk we're going
to have to be a bit careful about how we
gather those samples Monte Carlo has got
some nice properties it's naturally
unbiased so each of these function
values that I I take by definition their
average value is this integral so every
single term in this monte carlo sum is
an unbiased estimate of the integral i
want and if I average unbiased
quantities then my estimator is still
unbiased but as I averaged more and more
quantities the error bar of the the
variance of my estimator falls down so
the variance falls inversely
proportional to the number of samples I
draw if I want more accurate answer I
just survey more people will gather more
samples but it falls pretty slowly if I
wasn't happy with the variance of my
estimator and I wanted an error bar that
was ten times smaller then I need to
gather a hundred times more samples
because an error bar is a standard
deviation it's a square root of a
variance so Monte Carlo is not very good
for getting really accurate answers like
six significant figures but it's really
good for getting a quick idea of things
how does this apply to statistics so
these integrals come up a lot in machine
learning algorithms if you look at the
Boltzmann machine learning rule or the
e/m algorithm for interesting models you
need to compute integrals of this form
and integrals of this form also pop up
in bayesian statistics so in our linear
regression example here is why we need
to do integrals I've got a data set but
what's the good and having data unless I
can do something with it like make a
prediction so what I want to do is make
a prediction at this location X star and
say where would the next label appear
along this dotted line so what do I
believe about the test label Y star at
this location and the correct Bayesian
answer to that question is an integral
it doesn't involve fitting one line and
then assuming we know what's going on
it's
you know how to make a prediction if you
assume what's going on and if you assume
the slope and intercept we know how to
make predictions but we don't know that
slope and intercept you should consider
all possibilities weighted by how
plausible they are so that's an integral
and we can approximate it with a sum so
instead of summing over all possible
settings of the slope and intercept we
can get plausible examples which we can
sample so here are our twelve plausible
examples again I don't know if any of
these purple lines are correct none of
them will be exactly correct but they're
all reasonable and if I assume
temporarily that one of them is correct
I know how to make predictions so if I
assume this top line is correct there's
a gray bell curve which is the
prediction of where a label would appear
noisily around that line and I get a
different prediction for each of my
lines and the correct way to make
predictions is to average those
predictions so if I average each of the
predictions from each of my plausible
models I get the green curve and that
gives me a very reasonable description
of where that test location could appear
and it's much broader than the beliefs
I'd have if I just fitted one line I
could fit a line I could regular eyes I
could be very careful about how I fit my
line but fundamentally if I assume that
I know what the line is I'll end up
being too certain some of you who are
familiar with all this stuff will know
that this predictive distribution should
be a Gaussian distribution if I have a
linear model with Gaussian noise
everything is Gaussian and there's
clearly a weird bump here and that's
because I've only drawn 12 samples so if
you draw a small number of samples you
get a really good ballpark idea of
what's going to happen but the details
are a bit shaky and the joy with Monte
Carlo methods is that you don't have to
go away and derive anything new you just
show more computer time at it if you
want a more accurate answer so here's
exactly the same slide but with a
hundred samples and my predictive
distribution is now nearly on top of
the black line which is the predicted
distribution that I analytically derive
so with incredibly simple code that just
drill some samples and takes an average
I get like the correct Bayesian
prediction without having to derive
loads of stuff those curves are never
going to like exactly overlap if I
wanted to make this slide have the green
and black lines on top of each other so
that you couldn't tell the difference
I'd need something like a hundred
thousand samples and to make them agree
to several significant figures they'd
need more so if you want to know
something to six significant figures you
don't want a vanilla Monte Carlo method
but I don't really care about the
position of that curve to six
significant figures because it's a bit
sensitive to the precise modeling
assumptions I've made and I don't really
believe that it's significant figure of
any number anyway so in this figure I
admitted that I could actually make this
prediction exactly so I drew this black
line which is the true answer to this
integral so what's the point in doing
the sampling stuff if you can do things
analytically and the answer is there
isn't one you should do the math but
this model linear regression or
generalizations of it like Gaussian
processes are about the only Bayesian
models for which we can do the math
analytically as soon as you make the
model at all more complicated you
suddenly don't know how to do this
integral to make predictions so examples
of more interesting if even if we're
still drawing assuming that labels come
around some function with some noise if
our function is a nonlinear function of
our parameters for example a neural
network I hear that popular head these
days or if you had a more interesting
noise process or if you had hierarchical
beliefs like you thought there was some
structure to the weights and you wanted
to infer hyper parameters all of these
things would suddenly make the model
hard to deal with analytically as
another example which I'll come back to
you later
you might want to make the model robust
so rather than assuming your data comes
from some simple form you might want to
explain away some data points as
outliers so one way to do that is to
have a binary variable Zn is can be 0 or
1 if it's 1 then we assume the label as
an outlier it's drawn from some junk
distribution that has nothing to do with
the input location X and our model could
have some probability that an indicator
is 1 so epsilon could be 0.1 saying 10%
of your data points are going to be
outliers so if we wanted to infer that
quantity epsilon and deal with all of
these extra quantities we don't know z
we don't have an analytic way of doing
all of that and we'll need some sort of
computational method that's practical so
if you build one of these models you
have to make a series of choices you
have to say and what hyper parameter
would I have what weights will I have
given that hyper parameter what annoys
processes do I have and then given all
of those parameters how would I generate
all of the data given those parameters
so what you can do is imagine trying to
build a simulation of this model if I
wanted to draw some synthetic data and
ask have I accidentally made some silly
modeling assumption you'd need to write
down all of these probabilities and
you'd sample from them in turn and a lot
of people in this community would
represent that equation as a graphical
model in the tutorial next door they'd
represent it as a computer program where
you would have a series of steps saying
where each variable comes from in code
so for this tutorial what I'm going to
do is not have this splurge of maths on
every slide I'm going to try and keep
the notation simple so I'm going to talk
about things that work fairly generally
so they do work for models with a huge
bag of unknown things but I'm just going
to call all of those unknowns the
parameters and the latent variables
theta so theta is going to be everything
that we don't know everything that we
would have to generate before generating
our data
but that we don't observe in the real
world and everything that we do observe
that could be an input location and a
label but it could be a structured
object like a graph that's going to be
deep so what we're going to be doing is
looking at methods that will let us look
at what's a plausible explanation of our
data what do we believe about this whole
bag of unknowns in our model given the
data that we have observed that comes
from Bayes rule and all we need to know
usually is that it's proportional to
this probability of everything so the
probability of everything is this spray
of maths I wrote down on the previous
slide it's just the product of all of
the probabilities that you would compute
while sampling these objects if you were
doing a forward simulation and having a
look at what happens so if we can sample
from this distribution we can then make
predictions and there's another piece of
maths we can do which we might do if
we're interested in say just one of
these parameters so say I had scientific
interest in how corrupted my data set
was for some QA reports I had to say how
much of my data I thought was corrupted
I might want to estimate that number and
I might care about how certain my
beliefs in that number are so then I'm
not really interested in an explanation
of all my data I just want to know what
do I believe about this one parameter
given my data and that's another
integral so this integral is what people
call marginalization and for a lot of
people in the NIP community this
equation is called approximate inference
so a lot of people when they say I work
on approximate inference or I have an
approximate inference algorithm a lot of
the time what they mean is I've got a
method that will compute what to believe
about something given data in the
context of a larger model that talks
about lots of other stuff you don't care
about so algorithms like expectation
propagation and variational methods and
message passing are really good at this
approximate inference problem coming up
with marginal beliefs and to be honest I
Eve usually forget
the marginalization is even a
computation when I'm doing sampling
because when you're doing Monte Cali
there's no integral to do or no
complicated math to do what you do is
you sample everything you sample
complete explanations of where your data
came from and then you just throw away
the bits you don't want so if you sample
entire vectors theta and you throw away
all but the AI element then by
construction those things has
automatically come from the right
distribution so I could sample
explanations of my data set and then I
could just look at the values of epsilon
and those samples and plot a histogram
of them and that would be my beliefs
about that parameter so something in
some sense solves a harder problem than
a lot of these other algorithms you may
have heard about coming up with a joint
explanation of everything that's behind
your data might be more than you need
and that might be part of the reason
that it doesn't work as well as some of
the other approximate inference methods
on the other hand it's a much more
powerful tool it gives you whole
coherent explanations of what could have
gone on behind your data set and that
can be hard to pull out of an algorithm
that only gives you marginals
so those are the computations we want to
do we want to solve integrals
marginalization one is trivial the
prediction one is easy and we can do
that if we can draw samples from all of
these distributions and you might have
noticed that I didn't actually tell you
how I did that
so I had these plots and I had samples
but I just sort of said trust me I've
done this correctly so what we're going
to do is look at the algorithms to
actually generate these samples and I'm
going to start off with sampling
synthetic things from the model so
there's a forward simulation where we
just want to look at what our model
would do and see whether we believe it
how do we implement those well the
answer is we don't so if you want to
sample from any distribution that has a
name like a gamma distribution or a beta
distribution or a Gaussian then that's
something that I've never well know I
have implemented it because I'm in that
but there's no reason to implement those
things there are library routines in
MATLAB and R and the GSL that you can
just call so for every sequence in your
simulation you can just call standard
library routines and generate synthetic
parameters and then data and this book
which is free online explains how some
of those library routines work and I'm
going to have to explain how a couple of
them work because we're going to need to
understand those to do more interesting
and friends problems so in all of this
explanation I'm going to use the
following notation there's going to be a
distribution we're interested in called
pi all of our unknowns are going to be
theta and we're going to want to sample
from that and it might be a simple
distribution like a gamma distribution
or it might be a posterior distribution
a distribution over what models are
plausible and in those interesting cases
we normally can't evaluate pi for a
particular setting of the parameters
usually we can just evaluate some
unnormalized version of it that I'm
going to call PI star so the Bayesian
inference PI star would just be our
probability of everything which we can
usually evaluate and this normalization
constant
the probability of data or the
likelihood of our model for the purposes
of this afternoon we don't know how to
compute so we want sampling algorithms
that all that we're able to do is for
some settings of our model evaluate some
relative score that says how good they
are and then we want to be able to
sample given that okay so the first
standard distribution that's really easy
to sample from is an arbitrary discrete
distribution so this is something that I
do write code for because it's usually
one line if I have here a discrete
distribution over three values theta can
be a B or C then you create a stick of
length equal to the probability of that
value so the probability of getting a C
here is point two so I've got a stick of
length point here you lay those sticks
side-by-side and because probabilities
add up to one your stick is of length
one you draw a random number between 0
and 1 here point 4 and that tells you to
sample the value B so any discrete
distribution we know how to sample from
there's no big problem unless maybe
there's a huge number of values on this
stick and I'm going to tweet a link to
the slides after the talk so you can get
these references in the slides it turns
out this algorithm isn't the best way of
sampling a permit discrete distribution
it's what I'd use if I had two or three
values but if you have a large number of
values there are cleverer things you can
do and I'll just leave it at that
continuous distributions are harder so
there is a generalization of this laying
things out on a stick idea if I want to
sample from some continuous distribution
of high I can transform a uniform random
variate so if I draw a random number
a between 0 &amp;amp; 1 there is some maths I
can do that will transform that value
into a sample and the maths involves
doing the continuous generalization of
laying stuff out on the stick like this
you have lots of little elements that
you lay out on the stick and you have to
solve integrals and invert them to work
out where on the stick your sample is
landed so you can do that math for some
distributions but
an interesting model a posterior if even
one quantity you often can't do those
integrals there's a nice geometric
interpretation of how sampling works
they which is if I wanted to sample from
some arbitrary distribution that doesn't
have to be normalized what I can do is
throw darts at the area underneath that
curve so here I've drawn for samples
fairly uniformly at random from the area
underneath this curve and if you do that
and read off the value that they'll and
that gives you samples of theta and the
probability of landing in a small region
around here it's proportional to the
height of the curve so it's doing the
right thing things if sampled in
proportion to their probability if you
wanted to implement this the area to the
left of one of these samples is
uniformly distributed so we could call
that area a and that's precisely what
this maps up here is doing so this is a
nice picture but it doesn't necessarily
tell you how to implement the algorithm
how to draw these samples an algorithm
that does let you draw samples under the
curve is called rejection sampling so
rejection sampling assumes that you have
a set of library routines that can
sample from some convenient
distributions so for example a Gaussian
and you use that distribution to upper
bound the distribution you're interested
in so here i've multiplied the nice
distribution q by a constant k so that
the green curve is entirely above the
blue curve what I can then do is sample
points uniformly underneath the green
curve because I know how to do that I
know how to sample points from the green
curve and then select random Heights
underneath it some of those samples will
land above the blue curve and I'm not
interested in that region so I just
throw them away and as soon as I get a
point underneath the blue curve I can
treat that as a sample
I haven't unfairly biased myself towards
any location underneath the blue curve
and so I'm sampling uniformly underneath
it and then I can read the location
often that's an
example so rejection sampling is widely
used and may have been used by almost
every person in this room without
thinking about it so if you open a
matlab session and you type r and n to
get a gaussian random number somewhat
surprisingly the fastest way to generate
a sample from a gaussian distribution is
rejection sampling they have an
incredibly cleverly derived tight bound
which is designed to work well with
binary arithmetic that upper bounds the
Gaussian curve and it accepts samples
like 99% of the time and so is
incredibly fast to sample from if you've
got a complicated posterior distribution
it might be harder to come up with a
good upper bound and if you've got more
than one parameter and you want to do
the multivariate version of this figure
things get very difficult it can be hard
to provably upper bound the curve at all
and if you can you might want to be
clever and come up with the optimal
constant that will bound it as tightly
as possible and even then you might have
a large area where you reject a lot of
computation so rejection sampling is one
of the main methods used in library
routines to sample from standard
distributions but it's basically not
used very much in bayesian computation
because it's pretty important least not
by itself because it's very hard to get
working so we need some other ideas and
one of the ideas that's still very
simple to implement is to not throw away
all this computation we're doing so here
I've generated three samples and done a
load of computation I evaluated my
distribution pi-star to work out that I
need to reject them and I just threw all
that computation away and there are
methods that think well maybe you don't
need to do all this stuff what if you
don't need an exact sample you're
solving integrals maybe we can solve
those integrals more directly so that's
the trick of important sampling if what
we're mainly interested in doing is
making a prediction or doing some other
integral
we need for machine learning then we can
rewrite that integral by multiplying by
the distribution that we know how to
sample from and dividing by it so here
I've just multiplied by 1 I haven't
changed anything as long as I didn't
divide by zero here that would be bad so
I have to pick a convenient distribution
Q which isn't zero anywhere that my
distribution is nonzero and now as I've
colored in blue this is just an
expectation under Q so I've rewritten my
integral so that it naturally looks like
an average under say a Gaussian
distribution or a caoxi distribution or
something I know how to sample from so I
can now do simple Monte Carlo I can draw
samples from that distribution and
average what's left this is called
important something because this
quantity pi over Q is called an
importance weight it's saying some of
those samples that you're something a
lot because Q is high they're not as
important as you think they are but some
of them where the distribution has a
really high value you need to up weight
those that's an important region where
you need to pay attention to the
function values there so this is easy to
implement you just need a standard
library routine and to be able to
evaluate this thing oh except we don't
know how to evaluate that thing so if
our target distribution comes from Bayes
rule we often don't know how to evaluate
this quantity here we only know that up
to a normalization so if we can evaluate
that this thing is unbiased and it's
great but if we can't we need a
different version of the algorithm so
there's another version of important
sampling where you still draw some
parameters so this is like the slope and
intercept of our line or the weights of
a neural network from a distribution but
we compute unnormalized importance
weights so we evaluate the function we
do know how to compute the probability
of everything and compute these weights
and then we make them add up to 1 so we
create normalized importance weights add
up to 1 so now for every setting of the
parameters I've sampled I've got a
positive number
and those numbers add up to one so these
quantities are a like a discrete
probability distribution or they are a
discrete probability distribution as a
vector of numbers that add up to one and
what's important something does is it
replaces this integral which is an
average under pi with an average under
this distribution R and if we draw a lot
of samples this distribution R which is
a spiky distribution it's a discrete
distribution will eventually have the
same effect as pi for most reasonable
functions so as we draw many many
samples we'll get a consistent estimator
and we can approximate any integral by
something from a distribution that's
convenient to sample from so the
downside with any estimator is that
maybe it doesn't converge very quickly
or it's very noisy for a small number of
samples so if this was just some general
integral you could compute its variants
and do a lot of maths but as we're
interested in doing Bayesian inference
I'll just show you pictures to see what
what goes wrong with important something
so here's a linear regression example
and we need a convenient distribution to
sample from if we pretend we don't know
how to sample from the pastiera directly
we need some other distribution to
sample from and here I've drawn 60
samples from the prior because that's a
Gaussian distribution I have kicking
around and it's meant to be reasonable
values of the line and what important
sampling corresponds to here is
precisely assuming that the true line is
one of those 60 gray lines we don't know
which one but we're going to assume it's
one of them so what I'm going to do is
I'm going to compute the importance
weight for each of those 60 lines and
recolor them with intensity proportional
to their probability so then we're going
to see what the posterior distribution
looks like and it looks like that
so 59 of the lines are so faint you
probably can't see them and one of the
lines has probability 0.9999
so given this overly simple model that
our regression surface was one of this
restricted set we end up rammed into
believing that the least worst
explanation is the correct one and
that's something we've seen before and
we're going to then make wild
extrapolations like believing we know
exactly what's going on over here even
though that's not justified so important
sampling breaks down if we don't draw
enough samples to be able to get a
reasonable renormalized distribution so
it can just draw more samples then run
the same code but with more samples if I
draw 10,000 samples from the prior and I
recolor them I get this figure which is
much more reasonable so I have a spray
of lines the darker purple ones are the
more probable ones but the fainter ones
you can see might also happen and
they're spread around the true answer so
now I get reasonable beliefs that I
could use to make predictions and again
I see the time not very certain far away
from the data so it does the right thing
so incredibly short code I just drew
some samples from the prior which meant
sampling slopes and intercepts from a
Gaussian distribution computing how
probable the data was under each of
those possible explanations and then
coloring the lines we want more
interesting models than this one if we
have a nonlinear data set I may need a
more complicated model so here are 12
samples in gray from the prior of a more
interesting model so these are curves
with roughly the right sort of frequency
or length scale and amplitude to match
this data set it's a reasonable prior
that I've constructed by hand and in
purple I've drawn twelve samples from
the posterior so I did that exactly just
to show you that this is a sensible
model that will make precise predictions
near the data and be uncertain away from
the data so what happens if we do
important sampling here if I draw 10,000
samples from the prior so instead of
drawing 12 grey curves I draw 10,000
and I recolor them here is my posterior
distribution over what I believe
generated the data 9999 of the
explanations were terrible and the
10,000th who is also terrible but not as
terrible as all the others so I end up
really certain that this is precisely
what's going on even though it's
obviously wrong so I could draw more
samples that's what happens after a
hundred thousand that's what happens
after a million so if I draw a million
samples that's still pretty bad that one
on the other 999,999 were worse
important sampling simply leave this
vanilla version simply doesn't scale two
interesting problems if you have more
than a few parameters or if your data is
informative so that it will rule out a
lot of models that you sample from some
reference distribution then you simply
can't complete predictive distributions
using this method so what we need to do
is go back to the 1940s and 50s and see
what the physicists did when they wanted
to sample from complicated distributions
so for the next sort of 15 minutes or so
I'm going to explain some of these
methods which are based on Markov chains
and it dates back to this paper from
1953 I just had to put up this paper
because the opening of it is so awesome
and I really wish I could write a paper
like that's one day the purpose of this
paper is to describe the general method
suitable for fast computing machines of
calculating the properties of any
substance that's pretty cool especially
it's our fast computing machines are a
lot faster than their ones so we should
be able to do a lot with this method
including computing the properties of
almost any model so a lot of what we do
is still very much based on this paper
you've probably heard of the metropolis
algorithm and you will do shortly so
Nicolas metropolis is the first author
of this paper you'll notice the authors
are in alphabetical order or as is
common in something
some of the other authors you might have
heard of so edward teller is known to
some as the father of the h-bomb and he
apparently was the one who had this idea
of using Markov chains to explore models
and they had an idea of using some
convenient system we could simulate on a
computer to explore models so there's a
paper a link to at the bottom by
Marshall Rosenbluth that explained what
these authors had to do with this paper
and you can read the paper for yourself
if you're interested in the history but
something I thought was interesting was
that Nick metropolis apparently provided
the computer systems for this work and
you know that shouldn't be knocked
running a computer system in the 1940s
and 50s was a serious task but
apparently he had nothing at all
specifically to do with this paper he
didn't come up with the algorithm he
didn't run any of the experiments he
didn't write the paper nothing a bunch
of other people did have something to do
with the paper for example Aaron Anna
Rosenbluth apparently wrote all of the
code and she ran all of the experiments
okay so what was the idea in that paper
the idea was to use Markov chains and
the Markov chain is just a model where
if you have a sequence of parameter
settings then the next parameter setting
in that sequence is drawn from some
distribution and that distribution only
depends on the previous setting of the
chain so an example that you're probably
familiar with is our transition
probabilities are a Gaussian centered
around the current state then we'll get
a figure like the one on the right we
have this slow diffusion which doesn't
depend on where we've come from but will
slowly drift away from where we started
over time of this Markov chain would be
divergent it would wander off to
infinity but it does so quite slowly so
if we take s steps then the distance
that we travel scales like the square
root of s so that's one thing a Markov
chain can do another thing a Markov
chain can do is that
it can fall into some hole and never
come back so you can get the absorbing
Saints another thing a Markov chain can
do is form some sort of cycle so there
might be a set of states that it just
keeps hopping around or there could be a
set of regions that it hops around but
never goes to other regions and after a
certain number of steps you always know
roughly where it's going to be so none
of these types of markov chains are very
good for exploring models what we're
wanting a Markov chains that will
explore parameters that might have
generated our data and if a Markov chain
doesn't do one of these three things so
there isn't much else it can do except
fall into an equilibrium distribution so
these chains that form equilibrium if we
start them somewhere they'll fall into
some region but rather than disappearing
down a hole or forming into some
deterministic cycle they'll hang around
that region in a random way and after a
very large number of steps the
distribution over where they end up will
tend to some fixed equilibrium
distribution and I've called the
equilibrium distribution here PI because
what we're going to do is set PI to be
the distribution we want it to be so the
sort of math exercise I'd have had at
University of Markov chains would be
here the definition of a Markov chain
derive its equilibrium distribution but
what we're going to do is say we know
what equilibria distribution we want we
want to explore the plausible parameters
for this model please give me a Markov
chains that will have that equilibrium
distribution so I can simulate it so
that's what we're going to do I need to
cover a little bit of Theory not very
much theory one thing is this technical
term agogic which is really annoying
because no two papers seem to mean
exactly the same thing whenever they use
it but one definition of ergodic is that
this equilibria distribution that it
forms it's the same no matter where you
start so if I'd initialize the Markov
chain somewhere else and followed it it
wouldn't have like wandered off and
reach some other equilibrium
distribution it would have fallen into
the same
place and it's pretty easy to make
Markov chains ergodic so at least for
discrete spaces if I discretize this
finally the maths is very easy and it's
very short to show that a Markov chain
is ergodic if the state space is
connected if in a finite number of steps
like K equals a hundred steps it's
possible to get from any place any
parameter setting to any other parameter
setting then it's not possible for
either of these bad things on the bottom
to happen you don't get stuck in an
island or a cycle or fall down a hole
because you can always get everywhere in
the distribution so we're going to be
interested in a Ghatak Markov chains and
then there's only really one equation
that we need to satisfy for using Markov
chains to do inference for this idea
called Markov chain Monte Carlo and
that's to understand this one equation
on the slide which is called the
invariant condition or the stationary
condition and it's a basic
self-consistency property that Markov
chains have to satisfy so we're wanting
Markov chains that reach equi Librium
over time so that the distribution over
where you end up is some distribution pi
so if you ran the chain for a long time
and you sample the state from pi if you
then took just one more step to a new
place theta prime you could do this
integral to work out the distribution
over where you end up where do I end up
if I draw a sample from my chain and
then take one more step and what we want
is that distribution to be paid we want
it so that if we've reached equi Librium
we stay in equilibrium so our task is to
construct Markov chains that have this
self-consistency condition and the turo
ghatak and then we can simulate these
markov chains and they will draw samples
from our distribution for us and
satisfying these conditions turns out to
be remarkably easy so the solution by
metropolis and others was this very
simple algorithm
what you do is you use an arbitrary
convenient distribution like a Gaussian
as the basis of the Markov chain so the
basis of the Markov chain could be one
of these Gaussian diffusion so by itself
would wander off to infinity and you
start simulating that but you just use
that as a proposal so if we start here
we might propose going to some other
place under the Gaussian diffusion but
the algorithm rejects some of the moves
and says no don't go there
stay where you are it tries proposing
somewhere else
the algorithm says no don't go there
stay where you are but sometimes the
algorithm lets you just follow the
diffusion so here we happen to a
followed a diffusion that was heading in
the right direction after a couple of
false starts we explore the distribution
we're interested in and whenever we make
proposals that would wander off outside
the support of our distribution the
algorithm rejects some of those steps
and kicks us back into the region we're
interested in so it makes these
decisions it says if you're going to
accept then the next stage of the chain
is your proposed place if you reject
then the next state is just where you
what we're already so you record a
duplicate in your sequence of states and
these decisions are made randomly so
there's a probability of accepting
remove and it's some expression we know
how to compute it only needs to involve
the unnormalized quantities here that we
know how to compute so within the
algorithm we think about going to some
new model explanation maybe this slope
and intercept would explain our data
better or maybe this slope and intercept
set of binary variables saying which
data points are outliers and a bunch of
other parameters would explain our data
better we look at whether there's a high
probability of the data for those
parameters compared to where we are now
and then we decide whether to go there
or not
the Hastings version of this algorithm
Hastings is a statistician who wrote a
paper in about 1970 generalizes the
methods so we can have interesting
proposals here so instead of a Gaussian
proposal we might have some cleverer
distribution queue
and what this ratio does is stops us
from something from the wrong
distribution in various ways one thing
we might do if we were too clever is
make you behave like an optimizer so we
could optimize and find the best
parameters we could propose going to
those and if we do that a lot will
propose going to those parameters with
high probability and this term in the
acceptance ratio stops us behaving like
an optimizer and says now you're
proposing those parameters a lot more
often than you should do fairly and so
I'm going to reject that move so the
metropolis Hastings algorithm is this
and it gives you a Markov chain that
satisfies this stationary condition and
you get to choose the proposal operator
here so you can be creative about how to
set that so what I'm going to do for
about three minutes is check your
understanding see whether you've been
awake so I have a question for you this
is an example where I'm going to make
the state space theta very simple so
theta is going to be a real number
between Naughton 10 and in this problem
I'm going to assume that the region
between naught and 1 is interesting for
some reason and the region between 1 and
10 is interesting but different so there
are these distinct regions of my state
space that I'm interested in and I want
to be sure that I explore both of them
so I'm going to try and be clever which
is always bad idea and I'm going to
construct a special proposal
distribution Q and there's math but I'll
talk you through it so if you were in
the first region if you're over here
what the math does is say it's how far
through this region are you like 90% of
the way and it proposes going to the
other region so that you're the same
fraction of the way through so if you're
90 percent of the way through the first
region you propose going to be 90
percent of the way through the second
region if you're in the second region
this bit of math says if you're halfway
through the second region then bounds
back to be halfway through the first
region so this is a proposal that used
by itself would just bounce you back and
forth between two particular play
is on this line so by itself it wouldn't
be a valid MCMC method but we can use it
anyway within the metropolis method and
we should be able to get a Markov chain
that leaves our stationary distribution
invariant so for example I should be
able to run a Markov chain method and
get a sample that sample might end up
here and then I could use this proposal
to propose taking one more step so that
the next place the Markov chain visits
might be in the other region so that I
might get two samples that could tell me
about different regions so if I wanted
to run the algorithm I need to compute
this acceptance probability these
proposals are deterministic so the
probability of proposing this particular
move is 1 so I've got 1 divided by 1
which cancels and I could accept with
that probability and what I've just told
you is totally wrong so if you run this
algorithm you don't leave the stationary
distribution invariant and I've done
something mathematically evil so what I
want you to do don't go anywhere because
it's too many of you here talk to your
neighbor for like 2 3 minutes and try
and understand why does this update not
leave the distribution invariant you
could consider the uniform distribution
between norton 10 you can make pi
uniform can you see why this algorithm
is wrong and then can you understand
what I've done wrong in the math so chat
with your neighbor introduce yourself to
two or three minutes and then I'll give
you the answer ok you're not all there
yet but I'm going to pass on so the main
purpose of this exercise is to get you
to think about what it means to leave
the distribution invariant so for
example if I sampled uniformly between
naught and ten 90% of the time I'd end
up in the second region right so if I
applied this update and I accepted with
probability one then 90% of the time I
would sample here and then teleport to
the first region and ten percent of the
time my initial sample would be in this
first region and I'd move it over here
so we can think about what would the
distribution look like after one step of
transition I'd have a big peak that says
90% of my masters in this first region
and then a longtail thing in 10% of my
math is over this big region in other
words the distribution over where I end
up would be totally non-uniform even
though my target distribution is uniform
and I started out with an exact sample
from a uniform distribution so this
algorithm clearly doesn't do the thing
the Markov chain Monte Carlo algorithm
is meant to do it doesn't leave even a
uniform distribution invariant and so
then the second part of the question is
like well why not this is general
algorithm the metropolis Hastings
algorithm and I just implemented that so
I thought I was free to pick you however
I want and now you're telling me I have
to be more careful than that so the
problem here is that these numbers Q are
not 1 I've got a real line and the
proposal here is actually a delta
function saying you go precisely here
and the density of a delta function is
either 0 or infinity so really I did
infinity divided by infinity and they
weren't the same infinity so bad things
happened so if you ever are doing
anything with probabilistic modeling and
you have something deterministic like a
delta function or a transformation of
variables you often have to be a bit
careful most of the math for Markov
chains is really simple if you assume
you're in a discrete state space and
usually even if I have real numbers I
just imagined would this be ok if I
finally really finally discretize my
state space and then ran this algorithm
because that's what I'm going to do on a
computer anyway I'm going to use
discrete binary numbers to represent
things and this is an example of where
if you discretize things it doesn't
actually work out so those of you who
know a load of real analysis will hate
me for this but this is the low-tech way
of understanding what's going on I've
zoomed into the picture and I've really
finely discretized things and if I have
a parameter on the right-hand side there
are nine different bins
discretized numbers that would all
propose going to the same bin over here
because when I divided the number by
nine I scrunch down the range and so the
proposal to go from one of those bins to
this one has probability one has now
discrete I can talk about probability
one but what about going back I've
forgotten about where I came from so you
could think of making this choice of
which bank bin to land in arbitrarily
maybe if you sort of run out of
floating-point precision you could just
make up the final few significant
figures at random so something you could
do is just pick one of these bins at
random which could would give you a
probability of 1/9 here so you get a
factor of 9 in this acceptance ratio
it's not 1 and the reason I'm telling
you this is partly to be careful with
deterministic transformations but also
because there are algorithms that are
really popular in this community where
you have to be careful about this issue
so Hamiltonian Monte Carlo or hybrid
Monte Carlo was made popular by Radford
Neel and the main T's for Bayesian
neural networks and there's a lot of
activity around these algorithms that
nips till now and these papers contain
cryptic sentences that say things like
you need to maintain phase space volume
or your transformation has to have
Jacobian one and what these papers are
saying is don't do this sort of thing
don't stretch out your space or if you
do you better correct for it very
carefully if you hate the level of
technical explanation here you can go
and read Peter greens reversible jump
paper from 95 and he gives the correct
generalization of metropolis Hastings
for deterministic transformations and
more complicated settings so greens
algorithm tells you to include a
Jacobian term in the acceptance ratio
and that derivative is exactly the
factor of 9 that I've got here by hand
waving so that's something that you
should be aware of but you don't
normally run into very often so where we
left off before our break was the
metropolis Hastings algorithm
and this algorithm is very easy to
implement we have to decide where we're
going to initialize we have to decide
this distribution queue we need to be
able to evaluate pi and we need to
decide how long to run for and that's it
we just need to real to evaluate this
function we don't need to know all of
the details the algorithm doesn't care
what your data are or what the meaning
of your parameters are you just need to
be able to evaluate this function so
here's the complete code for the
metropolis method you give it your
initial condition you give it a function
handle saying this is my target
distribution or actually the log of my
target distribution for numerical
reasons how long do I want to run for
and I'm going to assume a Gaussian
proposal mechanism and so I just give it
the step size the width of that Gaussian
so the algorithm has a main loop to the
iterations the proposals are a Gaussian
perturbation of your chosen step size
you evaluate how good that proposal is
using the function handle that you gave
it so that's the only place that
knowledge of your model enters the
method you decide to accept or reject
with this probability ratio and that's
it so it's very short piece of code that
lets you explore plausible explanations
of a vast array of models
it might not scale that well to enormous
datasets and enormous models that's why
it's still an active research area at
Namsan there are several workshops on
this topic but for a lot of problems
it's a good starting point that with
very little mathematics gets you answers
so here's the the simplest case of
running this code so here I've created a
new little function Sigma which will
create these plots and it just calls the
metropolis code from the previous slide
for given step size and it's going to
run for a thousand iterations and the
function handle I'm giving it is just a
half x squared or a half theta squared
minus 1/2 theta squared which is the log
probability of a unit Gaussian or a
standard normal distribution up to
constant so my target distribution here
for testing purposes is just a Gaussian
distribution and in this middle plot
I've drawn the graph for a step size of
one so I initialized at zero I instead
of drawing a sample from a Gaussian
independently I sample from a Gaussian
centered on where I currently am and
then I decide whether to take that move
or not if I didn't reject any moves I'd
wander off to infinity
but by rejecting about a third of the
moves this sequence over a thousand
iterations explores my space between
plus or minus one or two and explores
this distribution so if I crush this
plot horizontally and drew a histogram
of where the Markov chain and gone I'd
get a bell curve I'd get my target
Gaussian distribution there was a free
choice here a step size and I could set
the step size to a hundred so now I
start at zero and I make ridiculous
proposals I say maybe try going to plus
78 or minus 48 and those places have
basically zero probability under this
Gaussian so for hundreds of iterations
at a time this chain just stays where it
is it's valid algorithm if I ran this
for billions of iterations it would
explore the correct distribution it just
does so very very slowly so this step
size is important if rejecting a lot is
bad I could decrease the step size to
say 0.1 and then I could accept more so
in this trace I only rejected twice in
the entire run and that's terrible that
means that the method basically doesn't
work and that's possibly surprising and
rejection sampling we don't like
rejections that's just wasted
computation we want exception sense but
in the metropolis method rejections are
a key part of how it works the whole
point is the rejections tell you where
your target distribution is they tell
you how to explore your distribution
rather than wandering off to infinity or
staying where you are and this trace
that we're looking at is almost exactly
a Gaussian daffy you
with step size 0.1 because it roughly
experience no rejections it hasn't
actually seen anything about our target
distribution we need to run for many
more iterations before we get some sense
of the support of our distribution and
its relative probabilities so we need
the rejections and you can theoretically
derive what the right acceptance rate
should be so for one-dimensional
proposals like these the optimal
acceptance rate turns out to be about
forty four point one percent and any
value is valid so you can aim for an
acceptance rate of roughly a half maybe
a bit less and the method will work well
and it will still work to some extent if
you get it wrong so you tune the step
size on a preliminary run until you get
the acceptance rate you want if you're
doing proposals in a large number of
dimensions then the optimal acceptance
rate turns out to be 0.23 for and so
about 1/4 and again you would tune the
step size and tell your acceptance rate
was about 1/4 so going to shift slightly
a little bit more into understanding how
we run these things and what it means so
it's a reminder we're doing Markov chain
Monte Carlo you the user tell me what
model you're interested in you write
down the probability of everything which
defines a stationary distribution that
will tell me what plausible parameters
are you initialize the parameters
somehow to some setting that vaguely
makes sense and you run a Markov chain
that then ends up exploring the space of
parameters so it will explore the slopes
and intercepts of lines that intercept
your data or the weights of your neural
network that give reasonable predictions
on your data set and if you run this
train for a long time then the
distribution over where you end up is
the target distribution and if you ran
for one more step the distribution over
where you end up would still be your
target distribution and so what that
means is we can now form estimates of
integrals so the average value of our
function evaluated on one of these some
one of these states for my mark
Chane is going to be integral if we run
the Markov chain for long enough if s is
a large number and we could use an
adjacent sample and the average value of
this function will also be this integral
because this parameter here has come
from pi so here I've got two unbiased
estimates of my integral I could run my
chain for s steps or I could run for s
plus-1 steps and I get two different
Testaments because they're both unbiased
I can average them I could add them up
and divide by two and that would give me
a new unbiased estimate so I don't need
to throw one of these away to adjacent
steps on this Markov chain are going to
be really close to each other these
aren't independent estimates but they're
both unbiased and they don't need to be
independent I'm still allowed to average
them and it will still be unbiased so in
general what we can do is we can write
down the simple Monte Carlo estimator
that we would write down if we had exact
samples and use it anyway even though
these samples came from a Markov chain
and they came from adjacent steps of a
Markov chain and for large time steps we
will get unbiased estimates of this
integral for small times is this bad
thing that happened here we had a
transient phase where we're not really
something from our distribution so that
contaminates this some a bit and we
might not have an unbiased estimate of
this integral but in the limit of taking
a large number of steps it doesn't
really matter whether we include this
transient phase or not so this
expression is still a consistent way of
estimating the integral whether we
discard this burnin period which some
people do or not and I would tend to
throw away like five percent of my chain
just because that it doesn't really
matter whether you do or not all right
so then how long do we have to run this
painful and figures like this one a
really misleading because they're in two
dimensions and we're not really
interested in doing MCMC on
two-dimensional distributions there are
better numerical methods than Markov
chain Monte Carlo for two parameters
we're interested in high dimensions but
they're hard to draw on a slide so
here's my attempt high dimensional
spaces are really spiky and weird so
there are these corners of parameter
space which kind of far away from other
corners of parameter space but they
might also be good explanations of our
data so this is a cartoon and it's
really a two-dimensional distribution
but it's got isolated spikes which is
more what you're like in high dimensions
and what I've done here is I'm exploring
the uniform distribution over the gray
region and I initialized here at the
bottom so I ran the metropolis method
for 2,000 steps it took some random walk
around the support of this distribution
it rejected whenever it stepped into the
white void and I ended up here on this
run I'd run up I'd end up somewhere
different if I ran it again so I'm going
to claim that that point there is very
very nearly a sample from my target
distribution from the uniform
distribution over that grey sir if I
hadn't run for 2,000 steps if I'd only
run for a hundred then I'd be less happy
so it took me a long time to escape from
this arm down here and what you see in
the top right is the distribution over
where you end up if you only run the
Markov chain 400 steps so it's really
probable you're stuck in the bottom arm
of the distribution and the some
probability you escaped elsewhere but
not enough on the other hand if you run
the Markov chain for 2,000 steps the
bottom right at the slide shows the
distribution over where you end up after
2,000 steps and to a few significant
figures it's correct it is the
distribution we aimed for so here
2,000 steps is long enough I've done the
maths and I've shown it so a brute force
there's numerical computation this slide
took the longer stuff the whole
presentation to create and so I hope you
appreciate it it's it's possibly a bit
surprising so if we look at this Markov
chain it didn't wander into this arm at
all or this one or this one or this one
most of the state space how can I
possibly claim that that there is a fair
sample when it has no idea what's going
on up here because I didn't even go
there so I think there's it this is a
fair confusion it takes a while to get
in your head how these algorithms work
for this particular distribution 2,000
steps is long enough the distribution
over where you end up is this we didn't
go up this arm this time but if I ran it
again I could do if I wanted twelve fair
samples from my model so that I could
make a reasonable prediction I could
maybe run this thing for twenty four
thousand steps and I could get twelve
independent samples or nearly
independent samples and I could use all
twenty four thousand steps in my average
and I'd have a lower variance estimator
so I could get really good predictions
here even though on a short run the
Markov chain doesn't explore some of the
maids of the distribution and on
interesting distributions you could
easily have thousands or an
exponentially huge number of modes and
your Markov chain is never going to
visit the vast majority of them and it's
a mistake to think it has to so if you
had to enumerate your whole state space
why are you doing MCMC why don't you
just do your son by hand like so the
whole point of these methods is that you
can ignore most of your state space you
gather samples that are representative
and you can use those to make
predictions alright so now we've got
this technology we've got the metropolis
Hastings algorithm and we've got some
understanding of how to use it we
doesn't run a long Markov chain and use
wherever it goes as plausible settings
from our model even though they're not
independent even though it went to
explore the whole model and there's
still a lot of choices about what we
would do so if you were to create an
MCMC scheme you need to make choices is
this Q distribution I'm going to be a
local diffusion like I've shown in
almost light so far or are you going to
be clever and do something like
approximate your model and try
propose from an approximation to your
model are you going to update just all
of your parameters like perturb all of
them or are you going to pick a
parameter at random and we've just one
of them at a time it might be easier to
control the step sizes for that or you
might be able to share a lot of
computation if you only move one
parameter so for all these different
choices you get a different transition
operator and any of these transition
operators are valid Markov chain methods
that you could use in an algorithm and
something I really like about MCMC
Markov chain Monte Carlo is that it
composes very nicely you don't have to
come up with the best method what you
can do is have several transition
operators and use all of them so if you
had three transition operators a B and C
you could use them each in turn so if
you started out with a sample theta1
which came from your stationary
distribution you could apply a
transition and that would give you
another sample which also marginally
comes from pi because this is a valid
transition operator so you can feed that
into the next transition operator and by
induction every time you take a step
with any of these transition operators
you will have a valid sample from your
distribution so the concatenation of all
these operators a then B then C will can
take a sample from your distribution and
spit out another sample from your
distribution and so it leaves the
distribution invariant and together
they're a valid MCMC method if there are
Ghatak if all of these operators
together can get you from anywhere in
your state space to anywhere else then
your proof is done you have a valid MCMC
method so these things individually
don't actually have to be a Ghatak
transition operator a might only update
variable number one and trusted
transition operator B might own the
update variable number two so transition
operator B is not a Ghatak it only moves
variable two and never explodes the
others but that's okay as long as
together it's a valid method so the
concatenation of different meth
can be better than any of the individual
transitions by themselves as long as
they contribute something towards
exploring the state space the most
famous example of having a series of
operators like this is Gibbs sampling
get Gibbs sampling says take one of the
variables from your model and resample
it from its conditional distribution so
if you had a bunch of binary variables
like in image model or binary variables
like our indicator variables saying
whether we had outliers or not you take
one of those variables out to get what
your current setting of it was you look
at all the other variables and then you
decide do I want to make this variable
black or white and then you select
another variable either at random or in
turn it doesn't matter and you make the
same decision again you resample it so
because we can always sample from
discrete distributions Gibbs sampling is
very easy to implement for discrete
variables here I just need to sample
from a one dimensional binary random
variable and I know how to do that
continuous variables a bit more tricky
here I only move horizontally or
vertically I update one of my two
continuous parameters at a time or in
general one of my several at a time and
given a current location you need to
work out what the conditional
distribution of the variable you're
updating is so if that conditional
distribution happens to be something
with a name a gamma distribution a
Gaussian you can use library routines to
sample from it or you might have to
prove something about it like it's Log
concave and then you can use clever
adaptive projection samplers that know
how to sample from those conditionals so
Gibbs sampling can involve doing some
maps and being clever although there is
software that does that encode some of
that cleverness for you so one of the
most successful pieces of statistical
software is bugs or wind bugs and it's
successors like Jags and now in this
community we have Stan and other
software and what bugs and Jags
does is
derives these conditionals for you for
your model and works out how to do all
of the updates and then you don't have
to set step sizes and it explodes your
whole model if there's a variable it
doesn't know how to update it can fall
back to doing a metropolis method and
update that variable that way so when
you implement Gibbs sampling it might
not work very well this red distribution
is correlated if we know z21 is high
then theta2 is probably high as well and
that means that these horizontal and
vertical moves can't be very long
compared to the distance that the Markov
chain has to traverse to explore the
whole distribution and that will get
worse if the variables are more
correlated so there are a bunch of
things we could try and do about that
one is we could try and transform the
space to make the variables more
independent I have to be slightly
careful about how we do that there's a
method called adaptive Direction
sampling where there are Delta functions
and you have to be careful about what
they are and include a Jacobian turn
another clever thing we can do is
blocking which is updating more than one
variable at once so to give you an
example of that I'm going to have to
talk about a particular model and I've
made this as simple as possible so here
we've got two unknowns regression
weights and binary variables saying are
we an outlier or not if a Gaussian prior
and our regression weights we think 10%
of our data points are going to be
outliers and labels are either noisy
versions of a straight line or they're
junk if the binary indicator is one so
this is an example of a model where lots
of it is quite simple tractable stuff we
know how to deal with we're really good
at doing computations with Gaussian
distributions and linear combinations of
Gaussian distributions so for this model
we can derive exactly a Gaussian
distribution on the weights if we
temporarily pretend we know which data
points are outliers so if we pretend
that we know which data points are
outliers we can ignore them and then we
just have a linear regression problem
and then we know exactly what this
posterior distribution is so it turns
out you can have one line of vectorized
code and MATLAB or numpy or whatever
that will sample all of these weights at
once they might be strongly correlated
but we can just replace them all and
then we can have another line of
possibly nice vectorized code that will
update all of the indicator variables
temporarily pretending and we know what
the weights are so this is nice block
Gibbs sampling scheme which is a for
loop containing two lines of code that
just updates W and then update said
alternately very easy to implement and
might work quite well so that's a clever
trick we can do
the unfortunate thing here is that
there's lots of clever things we could
do and we don't know which ones to do
there's another clever thing we can do
when we know the whole distribution over
the weights that's a sign that we
probably know how to analytically
integrate them out of the model as well
if we're able to form and normalize this
distribution we know how to do integrals
involving W and indeed here we can
analytically work out up to a constant
the posterior distribution over the
indicator variables given the data by
integrating out the weights so you can
evaluate this probability up to a
constant for any setting of what's an
outlier and what's not because if we say
these are the outliers you ignore them
and then you have a tractable linear
regression problem so you can around
MCMC on a collapsed problem where you
don't have to talk about the weights at
all you just update what you believe is
an outlier and you could do that by
gibbs sampling the Zed's would no longer
be independent so we won't be able to
vectorize this we'd have to sequentially
go through the Zed's but we don't have
any real numbers and we just update
these things and it's easy we can also
collapse out the Z and update only the
weights maybe using a fancy method like
Hamiltonian Monte Carlo and we then
don't have to deal with the discrete
variables at all and this is kind of
annoying because it's not at all obvious
which of these three ideas is the best
one and it will depend on stuff so it
depends on I
you going to implement this on a GPU and
you really care about vectorizing and it
might depend on the structure of the
model and the data you have so maybe
it's really obvious what all those Ed's
are for your data and then exploring
those exits just go to the setting which
is obviously correct and then you're
done you have an analytic answer so this
method might work really well but it
depends and in most of the probabilistic
models that appear in nips papers the
models are a lot more complicated than
the one on this slide and there's a
bunch of choices about what you can
collapse out of the model so you'll see
papers on things like Layton to reach
the allocation or topic modeling where
there are different parts of the model
that you can collapse out and there'll
be different trade-offs on which one you
should do and this isn't just an issue
with multicolor methods if you're doing
variational inference for these models
you can also collapse out different
parts of the model so one of the great
challenges for people doing
probabilistic programming is that
they're claiming you can define your
model and a clever compiler will work
out how to do inference for you which
means including deciding how what
choices to make here and these are
choices I don't know how to make half
the time so you know it's a really
ambitious reach a research agenda that
these people are attacking and there's a
lot there's a lot of theoretical
problems that this light has this idea
that sometimes we can do analytic math
so we can integrate out analytically
some of the unknowns in our model and
then sometimes that makes things better
but sometimes it's good not to integrate
things out because we might get
convenient updates that work well and we
can take that idea to an extreme and
actually introduce extra variables just
for our own computational convenience so
there's a whole family of Markov chain
Monte Carlo methods called auxilary
variable methods that do the reverse of
being clever and integrating things out
and put in extra variables that we
didn't need to so we form a new target
distribution over the unknowns that were
interested in
that actually appear in our we're sorry
appear in our model and some extra
variables I'm calling H which might have
nothing to do with our problem at all we
just put them in for the fun of it
and we need this target distribution to
be consistent with the distribution
we're interested in and then we can
sample a Markov chain that explores both
theta unknowns and the auxiliary
variables H we then marginalize by
throwing all the H's away and then we're
left with samples from our own names and
most work in MCMC can be interpreted as
coming up with a clever representation
of your problem which might involve some
axillary variables and then running
metropolis Hastings on it there are a
few methods that aren't metropolis
Hastings but most of them are basically
metropolis Hastings so you know most of
what you need to know is then just down
to the details of how you're going to be
clever
so the first exhilerated was called the
Swenson Wong algorithm and I've already
mentioned Hamiltonian Monte Carlo is
very popular in this community and the
moment there's a large amount of work on
so-called pseudo marginal methods these
are methods where you can't actually
evaluate pi star even you can't evaluate
your target distribution up to a
constant but you can estimate it and
pseudo marginal methods are just clever
auxiliary variable methods where the
auxilary variables you introduced allow
you to use estimates of your
distribution rather than exact
computations so the auxilary variable
method I'm going to quickly tell you
about
it's called slice sampling because it's
sort of a beautiful simple method and
it's one I think is a really good entry
into running MCMC if this is something
you want to try out yourself so this is
a method by Radford Neal and the idea
goes back to rejection sampling so I'm
going to show you a one-dimensional
figure because I'm going to assume we're
just going to update one parameter at a
time and in rejection sampling we need
to sample uniformly underneath the curve
right and that can be hard to do but
we're doing Markov chain Monte Carlo we
don't actually need exact samples we
just need to take a Markov chain a
random walk so what slice sampling does
is so that instead of sampling exactly
under this curve I'm going to take a
random walk underneath this curve and
see where I end up so to talk about
being somewhere underneath this curve I
interested to introduce a new variable H
which is the height where am i between
zero and the height of the curve which
is pasta so given this distribution over
two variables theta and H my target
distribution is just a uniform in this
area under the curve and I can do Gibbs
sampling I can update one variable at a
time so I could update the height given
where I currently am so I'm going to
move vertically and I just sample from a
uniform distribution on that line so
that's easy and then I can turn my
attention to the parameters and I need
to update myself along this horizontal
gray set of segments and that segments
that set of segments is called the slice
so we slice sampling boils down to how
do I move around the slice for unit
modal distributions this is really easy
to implement you can do rejection
sampling but the rejection sampler is
really simple
I want to sample from this segment
underneath the curve I just write down a
bigger segment than I need so I just
push out a bracket until it's clearly
outside the distribution which is easy
to check then I tried sampling on that
interval and in this case the sample is
not on the slice so I'll need to try
again but I can be clever
I now know this whole region over here
isn't acceptable so I can shrink in my
bracket so the algorithm very rapidly
exponentially quickly makes the red
interval tight and at some point I will
definitely get a sample somewhere on the
slice so these rejections are just part
of the internal working of how I'm going
to move horizontally they're not part of
my Markov chain so this is a method that
will keep
looking until it finds an acceptable
point and then it just goes there
unlike metropolis Hastings I don't need
to record rejections they don't have
horizontal lines on these trace plots
and what's good about this procedure is
that I don't need to know in advance how
broad this distribution is it
automatically shrinks in to find a
sensible step size so I don't need to
tune things as much as for metropolis
Hastings if the distributions not you
know modal you have to be a bit more
careful
so Radford Neil's paper had a series of
updates that tells you how to move
around on the slice and we give up on
Gibbs sampling so given that we're
currently here it might be that we never
transition over to this part of the
slice we just move somewhere within the
local region and that's okay
because all we need to do is leave this
conditional distribution invariant we
don't actually need to sample from it
exactly we're running a Markov chain and
the Markov chain will be a gothic if
it's possible to go down along and up
and eventually explore the whole area or
under the curve
so really you'd want to look at the
paper to get the details right because
it's very easy to get the details
slightly wrong on this algorithm but
it's very easy to implement you slap
down some initial interval you extend it
until it sticks out at the curve and
then you just sample from it as I showed
you on the previous slide so these
algorithms are really easy to use
because unlike Gibbs sampling you don't
need to derive a load of mathematics you
just need to be able to evaluate your
target distribution which is the
probability of everything it doesn't
have these rejections so it doesn't have
these long horizontal regions and a
trace plot and it adapts the step sizes
so there's a first method to run it's
often the one to try if you're wanting
to parallelize things or really get the
fastest performance you might want to
tune and clever metropolis Hastings
method but if you've got a simple
problem and you want to try out MCMC
methods for the first time this is what
to do Bradford Neil's paper which I've
linked to here and the slides will be
available is a great read it has a lot
more ideas and I fix
here with co-authors I have a paper
elliptical slice sampling which is a
multivariate version of slice something
that's good for Gaussian processes and
we have a paper on archive called pseudo
marginal slice something this tells you
how to use this black box simple updates
where you don't even know how to
evaluate pi you can just estimate it
randomly so the these algorithms are
really broadly applicable ok so what I'm
going to do for this sort of final and
10 15 minutes or so before questions is
tell you a bit about the sort of
practical issues of like actually
running these things and getting them to
work and I think I know what a lot of
you will be thinking because I've given
a few of these tutorials and I the main
questions I get are things like um how
long do I need to run this thing for and
how do I diagnose if it's correct and if
you look at papers that use MCMC methods
you'll see that they're full of these
sorts of Diagnostics so here are some
figures I've stolen from an eps paper
and one of the things you might do is
plot a trace plot this was run for 5000
iterations and some quantity some
unknown theta is plotted over time
there's a burnin period so maybe when
you're estimating things you want to
throw away the first thousand steps
hopefully this thing has reached some
sort of equilibrium but eyeballing that
I'd probably run it for at least another
ten times longer and see what happened
it's now 15 years later so that would be
easy to do another thing that a lot of
papers do is plot Auto correlations or
Auto covariances so how correlated is my
state theta at time 0 compared to 400
two time steps later and if any of your
variables are sort of quite predictable
or correlated if you know where they
were 400 steps ago then you know that
400 steps isn't long enough till it like
get an effectively independent sample so
if you have high ORAC appearances it's
bad news if the auto covariance is all
appear small it doesn't mean you're okay
but you know that you're in trouble if
you get large a politico variances so
the standard software like okay
that will create these plots and run a
whole bunch of other Diagnostics for you
and I've linked here a really nice
review on some of the practical issues
of running Monte Carlo but actually none
of this stuff is I think the thing you
should worry about first the thing that
I worry about first is that when I've
implemented a method is probably wrong I
probably just screwed up my code right
and there's this really nice paper by
John Gorky where the title of the paper
is getting it right we all want to get
things right and the thesis of this
paper is that your MT MC code becomes
big and complicated
you've probably messed up somewhere and
then there's no point running a load of
Diagnostics if you're busily exploring
the wrong distribution so what we'd like
is some way of unit testing our code and
these are randomized algorithms so they
can be a bit hard to test so he had a
neat idea it's related to other checks
that other authors have done which was
to implement code that can draw samples
of data from your model so we spent ages
writing MCMC code that will explore
plausible parameters it would be really
easy to write code that would generate
synthetic data I recommend you do that
anyway just to psych eyeball what your
model thinks so if you have these two
pieces of code you can check that
they're consistent with each other and
as this second piece of code is fairly
easy to write if there's a mistake it's
probably in your MCMC method so here's
how you do it here's how you get it
right you generate some synthetic data
so make up some parameters generate some
data and you know what parameters
generated those data you can use your
Markov chain code to move to some other
parameters that could just as easily
have generated that data if you didn't
know otherwise so you move the
parameters you then throw your data set
away and generate a new synthetic data
set from scratch a new data set that
could also come from those parameters so
you move the data set you then clamp the
data set and move the parameters and you
go back and forth
so what we're doing is we're running a
Markov chain on the joint space of
unknowns and data exploring every
possible setting that we think a model
thinks is reasonable before we see any
real data and if that code is consistent
we'll do that correctly and the
parameters will explore their prior
distribution so if you had some
parameter like a regression coefficient
which had a Gaussian prior you could
plot a histogram of that parameter and
it should look like a Gaussian and that
doesn't happen when you've messed up
your code Wow if you introduce
artificially a small mistake into MCMC
code what normally happens is some whole
region of the state space becomes
disfavored and you get a trunk cut out
of your histogram so you plot this
histogram it should have been a bell
curve
it's obviously not you know something's
wrong and this isn't like a hypothetical
story John Gorky admits in this paper
that he went back and checked his
previous papers and found mistakes and
his published results embarrassing
fortunately I read this paper after that
and I submitted a paper to nips 2010
where I know that in the day of the
submission deadline I made some change
to my code I ran this check and when few
realized I've made a mistake and fix it
up so you know this check meant that
either my paper wasn't rejected or I
didn't have to embarrassingly correct it
later there's plenty of other papers
I've had to do embarrassing Corrections
later but this wasn't one of them
because there was this useful tool for a
bunch of other consistency checks that
are useful so I started out by saying
there's not much point being very
carefully Bayesian if our model is
overly simple or wrong if you read Ann
Gelman at all's Bayesian data analysis
book there's a section on posterior
model checking that says you can use the
samples from your posterior not just to
check your code is correct but also to
sanity check whether your model makes
any sense and that's sort of a really
valuable tool kit that comes easily with
MCMC methods
you will be creative and for your
problem there's all sorts of things you
could do like if I draw synthetic data
what sort of predictions do I make
compared to if I knew the truth
underlying of the data if I can't make
good predictions in a synthetic world
then I know my system isn't going to
work well in the real world and I should
go back to the drawing board so that's
something in the way of practical
techniques what should you actually do
which method should you use so if you're
running forward simulations you're
generating from particular distributions
to simulate something then you need
exact samples the Markov chain doesn't
really cut it so you're going to use
something like projections something to
draw from the distribution you want to
but if you have a complicated
distribution like from a Bayesian
posterior rejection sampling won't work
so you're going to use important
sampling if you can get away with it
which is almost never but on small nor
easy problems and for more interesting
problems I suggest that you start with
MCMC methods and if your values are real
value if your variables are real valued
I'd suggest starting with slice sampling
if you're careful and you know what
you're doing you might get metropolis
Hastings to work better and if you're
clever about deriving your updates you
could do give something or one of these
other methods so I'm going to point you
to some reading these two reviews are
what I learned Markov chain Monte Carlo
methods from Sir David McKay's textbook
is excellent radford Neil wrote this
literature review you as his transfer
document to becoming a PhD student it's
about 130 pages and it's amazing I
learned something new every time I read
that document there's a new review of
Monte Carlo methods in this very
expensive book by CRC but there are
several free chapters online that cover
things like the the math behind the
green method so if you're doing clever
deterministic updates getting that right
describes Hamiltonian Monte Carlo
and also a bunch of stuff to do with
diagnostics and checking if you want to
go beyond this introductory seminar at
nips
there are a lot of relevant workshops
and I'm probably missing some out but
the ones here all have a very large
Monte Carlo method content so these
methods are interest to real problems at
nips the first two is sort of how do you
run these things when your models are a
lot bigger when getting a Markov chain
to mix is going to be hard or computing
with a large data set is hard blackbox
learning is about well how do I make all
these free choices like how would I
represent my model when I don't want to
have an expert at hand and Bayesian on
parametric's is one of the biggest uses
of MCMC methods there are also a couple
of workshops that I put at the bottom
where they think Monte Carlo methods are
a terrible idea and they've got much
better solutions so go to those
workshops too and they have ways of
solving inference problems that don't
use these nor easy random numbers and
get more accurate answers but even if
you were going to use the methods from
these workshops what would you check
them with I check them with MCMC and
there's still a lot of problem in
Bayesian statistics is dominated by MCMC
methods not yet the methods from these
communities so that's the challenge to
them and I hope they succeed to replace
MCMC methods so what I'm going to do
just to finish with is show you an
example of MC MC running before I have
some questions so three years ago there
was a cattle competition to do with
predicting locations of dark matter in
the sky and unlike some cattle
competitions this didn't have an
enormous data set and it didn't have a
really complicated model but it required
doing the statistics kind of correctly
so the top three entries to this
competition were all basically
statisticians who knew how to do
statistics correctly and the top two
winning entries both used MCMC
approaches so the one of those
approaches use flight sly something and
here's an example of an easy data point
from that challenge so this is a
synthetic image showing you the
locations of galaxies and a patch of sky
and lines showing the orientation of
those galaxies so most galaxies appear
to be elliptical disks and they point in
some direction and naturally they would
have a uniform distribution over how
they're oriented and you might be able
to spot and it's not very hard to see in
this figure there's a suspicious region
where the orientations don't look random
and that's caused by a lensing effect
there's dark matter between us and the
galaxies which are bending the light and
causing this distortion so what you can
do is solve the inference problem where
is that dark matter and what its water
its properties so we have theta arrow
names are the XY positions of this Dark
Matter halo its mass its size and its
shape so you might have five or six
parameters describing the dark matter
and then you can run a Markov chain in
that six dimensional space so what I'm
showing here is just the walk the Markov
chain did on two of those parameters
showing the position so I initialized
Dark Matter halo at a random location
here
it used slice sampling so it adapted Li
saw that it could take a massive step
and then some bigger steps and then it
took a load of really tiny steps I
didn't have to tune these step sizes
because it did that for itself and saw
that it knew the dark matter location
was around this true location with high
precision so this was really easy to
implement like literally a few minutes
once you know what the model should be
but this is an example of a problem
where you probably shouldn't do MCMC so
here if I ran an optimizer I could just
say that that is the optimal location I
could measure a curvature or something
else to come up with a Gaussian
approximation and I could put Aero bars
on that answer so this is an easy
inference problem
it's not that high dimensional and MCMC
is just not worth the hassle a lot of
the data sets were a bit more
complicatedly so here's a more
representative patch of sky from this
challenge and the we were told there are
three Dark Matter halos and this patch
of sky somewhere and you can try and
spot where they are but it's not
particularly obvious so what we can do
is we can initialize a Markov chain
where we explore this larger state space
of maybe fifteen twenty numbers saying
what are the parameters of these three
Dark Matter halos the black crosses show
the right answer and then a Markov chain
can explore the positions of three Dark
Matter halos and these three Dark Matter
halos move around a lot because it's
incredibly uncertain where they are
after a long time you can look at all
the places they visited the colors here
aren't particularly meaningful because
the halos can swap over so you know
remove the color and the intensity here
is showing the mass of the Dark Matter
halo so is exploring the position and
the mass and a whole bunch of other
stuff I'm not visualizing and here you
can tell that yeah we're really sure
there's a massive Dark Matter
concentration around here and around
here which is correct we're not sure
there's one here because or if there is
one there is not very massive and there
are all sorts of other places where
there could be Dark Matter halos so the
posterior distribution here is really
complicated and I'm just showing some of
it here that I can visualize and if you
fit this and then try and make
conclusions about physical theories how
does Dark Matter interact with gas you
just come up with spurious results which
are due to errors in your fitting what
you need to do is propagate all of this
uncertainty through and use it to
compute what does this patch of x sky
tell me about physics so that I can
combine it correctly with the inferences
I'm running on several million other
patches of sky which the MCMC is running
on separate machines so this was a
challenge where the MCMC was very easy
to implement and really the only way I
know of getting
sensible predictions about the physics
so I will stop there and take questions
thank you very much
please come up to the microphones to ask
questions there's Microsoft as there's
microphones on the aisles they used to
be that working yeah first of all great
talk thank you and thanks for the red
mitten that really made it for me oh
thank you so the the high dimensional
space you talked about you're doing this
walk and you say the corners are all
heavily separated you have that
visualization you say you jump to
another corner if you like and you can
say that that's enough steps to get
there and I guess my question is how how
do you assess whether that was a lucky
early number or an abnormally long
number how many of these long iterations
do you have to do to be sure that that's
kind of the right number right
okay so there's there's a question here
that I have brute-force this problem so
I did a mass of numerical computation
here which means that I know that 2,000
is long enough and that's the short
answer to the question is I brute force
this and then I know but for another
problem the real question is well how
would I know for a real model rate so if
this arm had some massive blob attached
to it and that's where the distribution
really is then 2,000 steps wouldn't be
long enough because I need enough steps
to make sure that I would explore down
that arm find where the real action is
and spend time there so for any real
model well not any real model but for
most real models it's really hard to
know that that isn't going to happen
maybe if you run it for longer you'll
discover that you just had pseudo
convergence and you're actually going to
spend the rest of time somewhere else
and I don't think it's unfair to say
that the majority of nips papers over
the years have used MCMC are probably
not running for long enough they are
almost certainly trapped in some small
part of the distribution and if you
could run them until the age of the
universe you'd discover that they did
something slightly different so no no
easy fix basically you just I don't
think there is an easy fix so there was
a lot of excitement in the 90s about a
method called perfect simulation so
there's some very high dimensional
distributions
eating models with a million spins
amazingly it's possible to prove that
you how many steps you need to run for
and people like Jeff Rosenthal in
Toronto you have some great theory for
some realistic statistical models saying
how long you need to run for but most of
the Bayesian nonparametric models used
here no I don't think we're there yet
I guess have a quick question but you
might imagine that you might not have to
mix thoroughly in different applications
so in some applications you might just
do sort of partial mixing but if you're
computing some sort of expectation that
might be reliably estimated well before
the mixing is complete so many comments
on that yep so great I entirely agree
sometimes you might not care too much
about mixing sometimes MCMC methods in
this community are used within the inner
loop of an optimizer and the sort of
stochastic approximation theory that
says that maybe your optimization will
run out correctly even if you don't
equilibrate at each step sometimes
you're doing this thing basically as a
really good heuristic for approaching
the right thing but you care about
engineering performance so there's a lot
of recent nips papers on large-scale MT
MC that haven't tried to look for
convergence but have compared themselves
to non-bayesian things by looking at
test error so in terms of engineering
performance the question might be if I'm
running for a certain amount of compute
time what's the method that will give me
the lowest test error and if that method
is some MCMC that might not quite be
reaching equilibrium then great I'll do
that if on the other hand I'm doing MCMC
for scientific data analysis and I'm
trying to say what we believe about
physics
after a 300 million dollar experiment
then I might care a bit more about
whether this is the correct computation
or whether it's some hack that might
kind of sort of roughly be right so I
think context is everything here and I
think there's you know the space for
people playing parts and lease and
trying massive systems and there's also
space for people really trying to get
the right answer when you've got seven
parameters and I think both areas are
important
okay if there's no more questions I'll
ask I'll actually ask one man
I remember reading Radford Neil's thesis
awhile back and and of course being very
in love with the idea of you know using
MCMC to do inference with neural
networks and I wanted to ask you like I
know that Radford and his students had
won some competitions a couple of of
years 2004 right yeah but but I don't
see that much activity with like you
know Bayesian treatment of neural
networks and any particular Beck MCMC do
you think that this is just something
that will sort of come back or do you
have any perspectives and of course any
inevitable connection to deep learning
really sort of pretty interesting okay
the question is why when Radford Neal
wrote this amazing code fbm that he
released in about 1994 and you can still
download it compiles it works very well
and he ran he won competitions and nips
workshops in 2004 by just dusting out
that code and running it why is it that
there hasn't been so much activity since
and I think part of that is that that
code doesn't really scale to enormous
datasets so they're batch methods where
for every MCMC update you need to chew
through your entire dataset before you
make a single move and to have these
really valid Markov chain methods the
statisticians have traditionally tried
to construct you kind of have to do that
and so for the next community that
wasn't so interesting it's not true to
say that there isn't lots of interest in
bayesian neural nets though in the last
two or three years there's been a lot of
work so starting with Welling and tazed
work and a series of papers following
from there there have been mini batch
versions of hamiltonian monte carlo and
to be honest the theory is a little
shaky but empirically these things work
very well and these methods are getting
on firmer ground over time so that's an
interesting area there's also been an
explosion in interest in stochastic
variational methods so some people are
using neural nets using variational
approximations rather than Monte color
approximation
so that work is interesting and then the
Cambridge engineering group is even gara
money in history and gal and others have
interpretations of drop out as doing
variational approximations so I think
it's still very much interest in
regularizing large systems sometimes
using random sampling it's just that it
might be we need to move away from some
of these traditional statistics
algorithms to slightly updated versions
all right well thank you again for a
really really great introductory
each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>