<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Compositional Models | Coder Coacher - Coaching Coders</title><meta content="Compositional Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Compositional Models</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/11wXwmLwDHo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it is my very great pleasure to welcome
Alan you'll who is not just a collector
of incredible Lee large numbers of
professorships in different departments
but also has been a collector of degrees
at institutions like Cambridge MIT
Harvard and collection of sort of the
world's great universities in this
career has been at the smith cattle well
I Research Institute and is now at UCLA
sporting these affiliations i would say
Alan has worked in almost every area of
computer vision and much of machine
learning I he's received numerous awards
but I think he's one of the few people
to have received both the Mara prize and
the test of time award following not
that work put some other work electric
fire I'm getting old I'm afraid to my
mind one of his major contributions is
mean he's been such a consistent
advocate over of a bayesian view of
computer vision or vision as Bayesian
inference and through let's say analysis
by synthesis the sort of pragmatic
combination of discriminative and
generative machine learning which as we
know as often is something that we use
here quite a lot to build practical
real-world systems so I welcome the
short title of which I shan't read and
please welcome Alan you ok well thank
you very much so I I think given the
audience I've over prepared material i
found out when i was talking about some
of this at UCL so i'm going to skip over
things and I'd like ever you know like
people to tell me what you're interested
in what you find boring and you know
they want to hear anything more about so
I guess possibly the main thing i'm
proud of is I've never been professor in
any department that I've ever taken a
course in so seems a bit too easy just
so unimaginative to stay in one area so
um what is vision do I think this is
very in a very staffing slides which
probably in this audience people don't
need to know about but it's just to make
a few points that vision is if you think
about it from a human point of view
whom's can do incredible number of
visual tasks in computer vision there's
been a tendency sometimes suggest sort
of sort of fine caps in boxes and things
like that and and yet the ability of
humans to get in meadowville amounts of
information from images and this sort of
ties in with an idea that's being thrown
around in the state's a bit of sort of
chewing test for for for vision and
people that idea come across here yeah I
mean live so you'd like to have a vision
system that can output a representation
that's rich enough so that you can ask
any question about the image and
expected to answer completely futile of
course in the short term but a good
thing to aim at local ambiguities I
guess perhaps some stevy I think it's
talking here shortly there's some work
here by was Berg matagi and so local
regions of ambiguous here is my attempt
to do psychophysics or collaborate with
people who do psychophysics so what do
you think that is if you haven't seen it
before portuguese villa not bad not bad
yes ok well now we give you a clue one
of those five could be yeah well there
was obviously mistaken whoever labeled
this image turns out it's a boat for
part of the boat and the point here is
getting for people who know computer
vision of course is that locally images
are incredibly ambiguous globally
they're not and part of the skill is to
try and work out how to put the context
in to make them impressive anyway so
that's work that was done with with Devi
and will probably be discussed later on
why vision hard well they're incredible
numbers of images and those incredible
number of objects in the world from my
generation I think we were crazy to
start doing vision in 1980s and people
who
1970s were even crazier because we
simply did not have the computer power
you know when I started out people were
building special-purpose hardware to do
convolution so that you could involve an
image in half an hour and you know i
think you know we have nowhere near the
the firepower that we need to deal with
a complexity of a vision here yes
getting back to the sort of analysis by
synthesis vision is our conference
inference you know these two examples
here this woman is not really flying a
carpet and that man is not really
levitating but you know the cubes here
for the for this looks like its shadow
thrown by the blanket the woman standing
on but in fact to show my flag outside
the image and that's just a sort of a
wet spot on the pavement in a sort of
position that it looks like it's a
shadow thrown by i cast by the human and
yet I pretty sure everybody sees a
flying carpet everybody sees a man
levitating it was that right okay for
it's worth a general claim and this
would bring into other you know areas is
it perhaps a key problem of visionless
complexity you know that gave back to my
comment about trying to do vision in the
1980s there's complexity of images
enormous numbers of images again when i
started out you'd writer you know if you
publish the paper with five images
results on five images you were doing
well you know if they were real images
you'd be doing even better and obviously
now you're going up to hundreds millions
and so of images so complexity and local
ambiguity of images also i saying also
perhaps a little bit neglected more the
complexity of visual tasks you know
instead of just protecting objects you
ought to be able to describe them find
the boundaries you know and if it's a
cat you want to find what the cat's ears
are what the cat is doing and so on and
a fundamental issue i think perhaps more
for people like me who interested in the
brain is is how on earth can the visual
system deal with this complexity how can
a human opener by in about 150
milliseconds which is a time you take to
blink you know pretty you know
the course-level serving the image it's
an incredible complexity problem I think
so my first program is trying to address
those issues which involve doing certain
types you know having a research program
which me will be aimed at having
something as complex as a brain and
perhaps relating to it at the same time
to keep myself honest we will publish
bits of this in the computer vision
community on certain aspects of it but
in my mind at least this is part of a
large larger bigger program which
includes interactions of psycho
physicists foi electrophysiology and so
on and even some theoretical
mathematical results thready gross
outside putting up here because for me
some projects are sort of a bit like
white whale projects you know Captain
Ahab I want to do this I don't know if
anyone else will be interested in but
what I did here with this paper which is
clouds say got into the accepted by
weather machine learning things is to
work out how you could actually
represent and rapidly access roughly an
exponential number of objects and try
roughly to sort of see if you can mimic
the structure you know if you could
almost try and deduce the structure of
the human brain almost from first
principles thinking of it as a solution
to a complexity problem you know almost
infinite number of images you can see
20,000 200,000 possible objects
depending on which psychologists you
talk to and yet in about 150
milliseconds you get all of them how can
your brain even store that number of
things and how can you access them so
quickly I don't know if people know
about less valiance work or you probably
know less valiant but he wrote a book
called circuits of the mind which I'm
not sure anyone apart for myself as read
various people have seen it but what I
really liked about it was he tried to
address the complexity of you know of
you know the human brain how could we
store certain numbers of memories and
had a neural network system which was
rather like a programmable computer for
which you could you know in which he
would try and deduce balance reverse
algorithms so I really like these
of chutzpah that attempt to apply
complexity to the whole brain and this
paper here and this is the end of my
short advertisement for it is to a plot
try and apply that sort of argument to
computer vision so here then is issues a
big need big data sets I don't think
anyone he would deny that of course
going down from whatever it was a
hundred was a big data set 15 years ago
now you go up into that level so there's
the whole argument of levels of numbers
of images and maybe the numbers of
objects but for me I think as well the
you know one thing one wants to bring in
this number of tasks bring in different
types of tasks because if you do that
that will help drive algorithms that
will solve things that are not
necessarily tested on Pascal o image
that you know so in computer vision is
an awful lot of you know a lot of drive
towards competing on certain data sets
and everyone sort of mush it like a
bunch of lemmings to compete on things
and I've done that in the past myself
but you know start you know start
thinking unless there's a rich enough
set of tests we may be specializing in a
very small number of things and missing
out a lot of the bigger picture so
that's getting into the cheering test
ideas visual tasks I think no need to
say what all these tasks are here and
here is a apply of my attempt to sort of
get rich assets of tasks so here what we
took is we somewhat unimaginative Lee
took the Pascal 20,000 images and we did
labelings of the 20 objects but not just
giving the boundaries we broke them down
into parts you know so here examples cat
sheeps etc so we have work done on doing
that and we have released the data set
and we'd be very happy if people would
use it or give comments back it back on
it even unkind ones because we spent a
certain amount of time getting this done
so but you know so here as well as
feeling I I didn't you know that this
would allow an to get my group to work
on things which we used to find that
reviewers would like because you know
hey how would you evaluate something
like passing
you know and so here with this sort of
data set we can do that out here's
another thing we did on Pascal again
which was taking it so that we could
label one out of you know when to 3566
hundreds of course a very important data
set was II know they came which
Microsoft data set for four scenes which
you know led to a lot of very good work
here yeah the idea is right you you
would like to scale it up you might make
it bigger enough so that results
hopefully will generalize to more
situations okay so 35 or 66 hundreds
have been optimistic I wouldn't really
guarantee you some of those labels so
now I'm going to get him to some
examples so I guess we'll for them and I
think I may skip over one or two of them
and I'll sort of be trying to monitor
you to see what is interesting or see
which ones people ask questions about
and so for me these in my way of
thinking about visions sort of thinking
of mimicking human vision system you've
sort of got low level vision you've got
mid-level vision you've got high level
vision and so the first project the
image patches would start out very much
at low level vision segmentation I think
I'll skip that passing objects and
estimating 3d structures is more
high-level vision and possibly for this
audience that something people may be
more interested in the image features I
think of course there's a whole lot of
interest in in features and somewhat
milk or features shift patches and so on
for my mind I started getting into this
because when we were working on things
like image you know like Pascal
challenges we would use the standard
types of features you'd throw them into
the formal part models you know you'd
have clusters and things and then I've
sort of made the mistake of looking at
what images correspondent to what word
in the bag of word description of safe
features and he couldn't really see any
sort of lime or weasand to what was in
there many patches which actually looked
sort of completely different with
somehow assigned to the same visual word
and you know while the method worked it
worked at a rather black box type level
I think
that may have been fine for the
cold-sleep performance it was doing but
it seemed if one wanted to do better
maybe the thing was to try and restrict
you know to try and go with a more you
know more basic representation so people
working on image patches here or or not
not particularly okay well maybe let me
sort of move through this so you know
why would you like these features you'd
like to have features like sift which
are invariant to certain photometric and
geometric transformations so when David
Lough invented sift features that's what
he did he would mark interest points
he'd look at invariant you know we need
to take photographs of things from
different viewpoints different angles
different commissions and find
empirically that SEFs were quite good
because they were invariant and so that
was sort of a very good justification
for using them however then they should
have developed a type of life of their
own and people started using them for
everything else you know so if you think
about using sifts for detecting cars as
we and other people were doing in Pascal
challenges you know well sure you want
features that are invariant to things
but you know what are they invariant to
they're not environments of the things
that they would lo was considering you
know you want something if you can get
it this invariant to whether it's a
Porsche or BMW or something so there's
invariance you want but it's not this
there are other papers I think out of
MIT which I should have got a slide from
Antonia tabla which was sort of saying
what does the image look like if you
filter it through a hog and you see that
you're working with a very blurred type
of image so here one project we had was
patches and again for me and this may be
more my generation and yours was
wondering how many patches happen in
images you know if you work in speech
you believe there were phonemes and
everything is made out of roughly 50
phonemes I mean if you talk to real
speech person they say that's a bit of a
fiction and phonemes are complicated but
nevertheless a sort of some ground that
you're building off to you know to make
you do to construct larger systems
so what could you do can you look at
patches can you try and study what what
patches happen in images there's been
quite a lot of work on this but our take
on this was to try and put in a few
extra ingredients into it so if you take
a patch and you just sort of look at the
patch and see what other patches happen
you find that in fact you're being
rather inefficient because that same
pattern at the top you know it's just
the same thing it's just an edge if
today were slightly and if you
representing them as individual patches
you're representing them all as a single
patch okay we're so you're presenting
several different patches where really
it's just one aspect of another one
similarly with photometric changes you
need to you know you're representing
things which are actually visually very
similar as being somewhat different I
guess people here perhaps haven't
followed the literature of receptive
field models of neurons anyway a large
number of people do statistical studies
of images about what filters you ought
to have and you always end up with Gabor
filters and the reason for that
basically in mind viewers images a shift
invariant so they can ball filter is
just encoding a type of shift thing you
know so that's why the Boeing you don't
even need to look at images to get Gabor
filters in fact some papers that people
like me wrote in the 1980s you could get
good ball filters from sort of basic
heavy and learning rules was just
throwing in white noise as long as it
was shipped invariant you get the same
thing out so a certain amount of
receptive fields like a baws are sort of
encoding not what the image is actually
like the encoding the fact that images
can move around this way which is not
possibly than that best way to do it so
here we have reeking an idea from from
Brendan fry had epitome paper some time
ago was to sort of have intelligent type
of patches which we are calling mini
epitomize after fries work where you
would represent you know you start out
with having a large patch here and any
particular image patch is month is only
a sub bit of this large patch so if you
wanted to have an edge in this different
position
or an edge was different you know
photometric values you could just take a
sub bit of the patch and extract that
out so here's perhaps a better example
here's a mini pity me up there with
white and black on the back and that
green part of the mini epitome an 8 x
it's a bit of it corresponds to that
part of the cat whereas this red mini
pitted me corresponds to the edge
further down so you would have a set a
dictionary of say of K which could be 64
to 256 or something like that many pit
amis of which which any particular image
patch is is a particular eight by eight
sub region of it and you can do this
nicely it's sort of like an extension of
k-means you have a generative model for
patches are generated by one of K
possible things but with these latent
variables which allow you to change the
photometric value and which allow things
to be moved around so you can train this
nicely and you can get a dictionary out
here where the dictionary over here is
compared to what you do if you just take
the basic patches themselves and I don't
know this is possibly a little
subjective but I think you sort of see
more interesting patterns in here than
over there partly because you're not
really encoding different shifts are the
same thing that will impose inside here
you start getting bigger texture type
properties coming out here and over
there you're getting things which are
far more like the standard the boers and
extra the balls that people tend to find
you train those on Pascal images this
was out in cvpr for us again and this is
in my line of analysis by synthesis I we
wanted to show that you could
reconstruct images with this I mean I'm
not interested in image reconstruction
per se it's just to say that this
representation is rich enough to capture
that property whereas if you took hogs
or something like that and features and
throw away everything else then from
Antonia's work and others you don't have
the ability to reconstruct you can have
got a blood thing so it's one of the
points I would make about sort of
of analysis by synthesis or generative
models is the idea of a generative model
is really to say that you know
everything about the object okay you
know what appearance II could have you
know all the geometry and so on a state
of computer vision this is I don't know
if a debate about general traverses
discriminative but if you are doing a
limited number of tasks you probably
don't need a generative model if you
want to detect a car in Nevada for
people who have no Nevada you don't have
to build a model that has ADD eat a lot
of details about the car because in
Nevada there were cars this road this
mountains as rocks UFOs not much else so
it's a fairly easy discriminative thing
on the other hand if you want to detect
a car in Los Angeles as all sorts of
crazy things around you need to have a
far more detailed model of the car you
need to have the ability to I think to
represent the appearance on all our
geometry so this is sort of a high-level
plug for generative models and in this
case just to show if you take these
pitta means you can reconstruct the
thing to a reasonable degree of accuracy
and if you were in the into a
construction you could do a lot better
another point here which relates to my
to sort of my sort of general interest
was the issue of universal dictionary
you know how many 8 x patches can be in
images you know it seems you know
amazing you know enormous number and
certainly in the 1980s we had no idea
what what they could you know what they
could be now you know a certain amount
because you know Jacob compression works
reasonably well so you can get some idea
from that but still Jacob compression is
built on representing images by linear
combinations of things and that's
possibly not the best way to do it so
what we can study here was by taking
these mini epitomize and then trying to
find for all patches and all patches
means I think taken from ten thousand
images from Pascal could we find a mini
epitome which was arbitrarily close to
it or close enough to it so that
perceptually it was visually similar
okay so this is going back to my feeling
of when I was you know work trying to
look in the guts of our methods for
doing
the paschal object detection you know
the the features being extracted
corresponding a whole range of different
image patches and they seem to be almost
nothing in common with them perceptually
similar meant for us normalized cross
correlation of 0.8 or higher you know
there were more complex psychological
tests but you know they don't really
seem to work anything better than that
and if you do this the figures you get
out of here so k equals 32 was using a
dictionary only of 32 many epitomize and
then this is a cumulative distribution
function for the percentage of patches
that were covered with such-and-such
normalized cross correlation and if you
go up from 32 to 512 you know just
bigger you're increasing the number
you're not increasing it a huge amount
and still what you're getting is it you
know about seventy percent of image
patches can be covered by a small
dictionary of I think the number here is
for 256 with 0.8 accuracy yeah models
are more like a compression argument
because this is all samples from the
bottle conditioned on the observed image
right it is not the giant of some oh
yeah no I wouldn't say this is a
generative model this is sort of like an
opponent of a generative model I it's
well it's it's more buy more yeah it's
more like you know by concern of our you
know of trying to say that the number of
images so huge is necessarily you know
is nevertheless finite okay what can
happen in images if you read David Mars
book he sort of well anyway so get into
that but questions of what can happen in
images can you characterize everything
it can happen in there an image this is
a small example of saying that maybe the
total number of things that happen in
images it's not you know though big is
not absolutely enormous so for
generative models yeah you'd have to do
extra work on this and this is not a you
know that's not proven here
now I guess just being a tooth at would
not get into a computer vision
conference or results to say if you took
these uncertain task you could do things
which were roughly as good as hogs or
sifts not admit if you put in all the
extra machinery of Fisher vectors and so
on but if you ran them on sort of fairly
equal grounds and arguing that well
people have had five or ten years to
find any sort of trick to make hugs and
sifts work well and with these types of
patches you you know they're fairly you
know fairly new and weakened the fact
work can get up to close up to that
level already it's not it's not bad it
leaves out for me some of the more
interesting things which is how to deal
with the invariances so I criticize the
hogs and safe by putting in variances
which not necessarily the ones you want
these are richer but it leaves open the
question for next year of how to model
what the variations that again happen
here is work that's perhaps going to be
an easy CV so if some of you may have
seen it this is slightly different this
was active patches which are slightly
you know perhaps slightly less
theoretical e nice but you have sets of
patches you allow them to rotate you
allow them to expand and so on and I
think what we did here is we ran these
on various different data sets and
partly the pointers on the data set on
the left as a bunch of people who study
this material data set and they have
various approaches using certain types
of filters they've tuned they get good
results on those these ones here were
ones that people would you say hogs or
sift different types of things and so I
a do as well if you just take image
patches and put them together can you
get good results on all these different
things and since the paper got accepted
it meant that we could get roughly state
of the state of the art on these these
datasets are things that we've sort of
released so we could only run hogs or
sift type things ourselves but
nevertheless the patches themselves are
working well on this
active patches these types of
transformations you need to have the
invariance to in this case the certain
amounts of rotations at the max of scalp
okay so this is my you know low level
you know low level topic and saying that
maybe he knows arguing that's at MIT
well I think as a good you know an
audience that was very much on the side
to say that once were to have a campaign
for real patches rather than four
features and also from representations
that come out from the data rather than
being imposed you know by more
mathematical models okay I think I'll
skip over the segmentation stuff that
relates to attention and stuff like that
which I think probably people are less I
would guess people here or less
interested in and then move on to high
level high level stuff so this is
getting into conversational models and
objects in the first work cvpr was this
is done by shin Shh n was sort of to
start exploiting our data set seek and
you find the not only find the object
can you find the head of the object and
the torso and what we were doing here
was related to these you know ideas of
convolutional models the second piece of
work is in review but is interesting
enough I think people would probably
like to hear it so I'm going to talk
about it anyway we have put it on
archive and it's currently being
reviewed somewhere so here you know with
objects and doing it on animals are
particularly interesting because they
have all these sorts of deformations
they have these occlusions and so on and
also in the data set sometimes a high
resolution in which case you can see the
feet the parts and other times are low
resolution so you cannot so what you do
is you build models of the object you
can take you could hear you're sort of
building off things before so this this
time when we started the project we're
building off deformable part models if
we were doing this again we might build
them off deep convolutional networks but
the you know it's anyway you need a sort
of a basic detector
for individual pieces and the idea is
more how you combine them together and
put them put them together so here
models for the holistic object which
would be like the things people used on
image you know on Pascal passing our
challenges for objects and then similar
models for individual objects horses
torsos etc now here you get into a sort
of a combinatorial issue because if you
taking you know if you're looking at an
object like a car you know it cause you
know the only is limited number of
viewpoints of a car and everything is a
slightly deformed version of that if
you're dealing with an object like a
human or cow that's got the formable
parts there's a valve a large number of
configurations that the object can have
in addition to the viewpoint so you can
have a mixture model for every possible
configuration of the object but that's
going to lead you to a very large number
of configurations so instead what we
would do is well we build that model but
by having it built out of compositions
of elements you can put together so you
could build the cow by having one type
of torso in another type of head and
another type of leg and you can build
another tower by having other types of
torsos and other compositions so you can
to extend share parts between different
objects between different examples of
the soil different it can positions of
the same object and when you do
inference you can aim at detecting the
orig original parts first and then
looking for the compositions of them so
that saves time for representation that
takes time for inference the
mathematical work I mentioned a little
bit earlier about trying to show that
you could do exponential number of
objects in a particular architecture
fairly fast was based on some of
analyzing those types of models okay so
here objects spatial relations between
them here are just a few examples right
i mean this is just a show hey you get
the object where you can get the head
you can get the neck you can get you
know get these other
part as well coming out so perhaps now
I'll move on to the next piece of work
there so that was cvpr this other one is
unpublished and this ties into my take
at least on deep convolutional neural
networks and people using them here or I
mean sure everyone knows what they are
but I don't know is this a other deep
convolutional neural network fanatics in
the audience or not have to ask the
questions how yeah ok so I so I think
you know so now getting now again to
address humans by themselves and
obviously yeah these two numbers of pose
variations and so a decently in the US
there's a tendency of people to throw
deep convolutional networks and
everything and of course they've got
some you know really impressive
successes on certain types of problems
but it seems that they're also you know
certain limitations of them that they do
not explicitly represents facial
structures or things like this you know
you could have a deep convolutional
network for detecting a single person
that might work if they were standing
upright but for detecting a group of
people there's too many possible
positions where the object could be
relative to each other even for a single
person if the person is you know
standing upright on the street corner
then typically in one configuration and
you can model that and you don't need to
model more but if you use these data
sets we're getting I think from you know
leads data sets on other ones as a
enormous number of configurations so
even though people at Google have done
deep networks for doing deep networks
for doing it it's not necessarily i
think the right representation to use so
the idea here was to build the object
out of parts and so this is going back
to you know the well fish leavener
schlag in 1973 one of our reviewers said
oh is a spit like zoo and ramen on and i
almost said well it goes back that he is
before that in the sense you know you
have models you have nodes for positions
they could be visible
mostly visible you can allow them to be
invisible slightly to the extent that
you need to on this data set and then
there would be parts here that would say
be the elbow the wrist you could try and
build a detector for the wrist a
detector for the elbow and why the
reviewer said sue in Ramadan is of
course a did they've got very nice work
on that that we and other people have
used in the past but here now taking
advantage of the deconvolution or
networks no statistical power it seems
reasonable that you could look at
different configurations of an elbow and
D convolutional networks which whatever
they're doing is sort of seems like a
very intelligent way of doing large
numbers of templates at the same time
would be able to capture the sets of
variations you got here which if you try
to capture this plus this plus
everything else to just be far too many
of them for that to represent so use the
so here's a figure this is you know the
example a cave here is the elbow there
is a wrist and then you know so you
could try and build an elder detector
but also locally you've got more
information from this just by looking in
this box around the elbow you not only
you know can detect the elbow but you
can also see which direction it is to
the shoulder and which direction it is
down to the rest so a local detector was
not just an elbow detective it's an
elbow plus the local configuration and
the local configuration you know you
have mixture models for these the
student did a certain amount of
clustering to see how many examples you
know you needed I think it's sort of
roughly a eight one direction eight for
the other direction and you put that out
as when you're training your deep
network you put that out as one of the
classifications so instead of having 200
objects out at the top level of your
deep network you have 12 joints and sort
of 8 16 or 24 configurations of them
depending on whether it's a wrist which
only has one direction here so it's got
eight an elbow that's got eight here
eight here or shoulder that's got this
one this one this one and has got 34
okay so you can
train this with the DCM model standard
thing I mean the student had to modify
it slightly but it's sort of basically
the code that you get from Berkeley
people he had to sort of artificially
make a few more images which for this
case is reasonable you take the images
you just sort of rotate them around a
bit you know that's not giving you much
more information but obviously if you
want to train the techno boy like this
you can just take the elbow and rotate
around that and it gives you equally
good data so so the features are here
they're not like you know the Berkeley
people were not training features on
image dance or something and
transferring them over here we're just
sort of doing this training directly on
that and then the results are it works
really pretty pretty well so this was
the you know leads data set when you do
infants on so on the model you know I've
described mainly the issue of detecting
the elbows blow you know the elbow
joints plus local configurations but
there's a graphical model of the spatial
relations which you learn as well and
throw the whole thing together and I
test it on those data sets and the
results really pretty pretty good so on
the leads data set the improvement over
at least what was published when we sent
this in with 7ap which is a reasonably
big improvement and on one of the data
sets it was large the buffy data said
Shin Jae said he couldn't train on it
was too small so I said well why didn't
you just run you algorithm anyway take
whatever you've got on the leads thing
or the other data said run it and there
he was getting sort of five AP
improvement without training again
bringing out the idea that seems it
seems to have come out cups from the
Berkeley people that if we train on a
big data set it probably transfers over
to smaller ones anyway so I think for me
this was this was you know this is
interesting because you know the form or
the deep convolutional networks you know
as a statistician I had my regression
and you know regression does wonderful
things if you've got enough data but if
you want to but at the same time I'd say
that
deep networks need to be better if they
could explicitly represent things
explicitly represent spatial structure
and this was sort of like a first
attempt to take the advantages of
pictorial structure models graphical
models but combine them with a sort of a
power of the deep networks to you know
to extract features and so on okay so
that was one level we did something on
cars that I had a poster at in
Nottingham a day or so ago this is you
know it's also attempt of sort of
parsing not just to detect the car but
to sort of find the bits of the car we
all sand so on there's a several other
ideas in there which I may not go into
the but that's extra thing oh and horses
that's pretty sure not to get into nips
this year but I like it anyway where you
can take the horse you can start
breaking it down into these different
pieces as a as a challenge okay so now
I'll maybe move on to the final project
i'll talk about so you know so having
being interested in passing objects and
you know perhaps people and so on you
you start thinking about why do
everything in 2d you know the structures
about humans which you know are
basically three dimensional you know my
arm is you know this length of my arm is
fixed in three dimensions in 2d it's you
know contracted due to the projection
etc and yet so the prior knowledge of
prior models are perhaps better done out
out there the other aspect which isn't
shown here but actually is in follow-up
work is that the student chung you Wang
was interested in working on action
recognition and you know it started
thinking about her persuading him that
probably to really do action recognition
properly you needed to pass the object
you know pass the person and then maybe
also to do action recognition in general
you'd like to do it with a
three-dimensional model because
otherwise in
I can be walking I can be running I
could doing a bunch of different things
and if you just got a 2d view from it
and represent it and if you just for a
lot of filters at it you you know you
may have your work cut out for you
dealing with all that so do his
follow-up work that he's done which is
showing should appallingly results using
what I'm going to describe for doing
action recognition as well so here we do
build off the as who and laminin work
that I mentioned earlier where you have
an input image you have a to depose
which would be the output of the zoo in
ramen on model detection of certain
joint parts and then you would like to
estimate the 3d pose from that as well
as the camera view point so to go from
the 2d pays for 3d pose you need to have
in my language a prior model of what the
object is like and there were certain
structures that was you know done I
think CJ Taylor had anthropomorphic
constraints ratio of limb lengths are
roughly the same so we're using those
but those didn't seem to be adequate we
were building off work at CMU where they
took a whole lot of 3d data and they
threw it into a PCA thing and you get
the principal components and do things
with this and that was sort of
interesting but obviously these the
space of configurations of human is not
a linear space so our technical
involvement there was to apply sparsity
to it so to say that from that you know
from that three-dimensional date of the
joint positions you could learn sparsity
basis so that each you know certain
configurations could be expressed as a
small combination of certain other ones
so it's a week assumption saying the
hole in their space is in a whole space
is linear which is obviously wrong but
it's more like a local assumption there
were sort of linear sub regions in this
space it could be dealt out with by
sparsity you also have to deal with the
camera position as well because you know
you unknown it's not just the 3d pose
but it's also the camera view point so
there's a reasonable amount of technical
work that needed to
be done I think I've already said this
representing the object by joints
constructing a prior model said using
the zoo and ramen and thing at that
stage and so the prior yakin trains the
CJ taro anthropomorphic constraints and
then this Cisco learning using sparsity
which we now have a sort of somewhat
nicer story than we had you know when we
published that work to understand what
that is doing and how it relates to
ideas and manifolds and things of that
form you know estimating the camera
thing this is who you know a chicken and
egg problem you've got two variables
that cause the thing you have to search
over camera directions you have to
search over 3d configurations you know
you have various problems of course that
the you know the estimation despite how
good soon move Rama lands model is that
you not get to find the post positions
exactly in the image again have errors
in there though so they can be several
3d poses and have similar to D
projections and additionally this
ambiguity from the camera so
nevertheless a certain amount of
mathematics in it this alternative
direction method is you know particular
one we use because of a collaborator in
a Beijing University another point it
was not put in here was actually the
technically was the use of the l1 norm
so in many cases in vision you'd have L
to norms as sort of a default type norm
l2 norms are somewhat sensitive to
outliers because you're penalizing the
by the square and so here you throw in
l1 and it actually improves things it's
something I keep Tina I keep telling
students you know l1 is better than l2
and no one believes me and then I got
lucky hit one week you note to people
not to of my student thought they only
believe me put them in and found it
actually did work better so you get you
know occasionally people listen to you
the positive results here which would
quite reasonable this was the CMU thing
that we're building on the ramakrishnan
thing is there we get better results and
I guess again if we didn't get better
results this wouldn't have been
published
the results of you know the results are
still making errors you know I think the
average air will be something like the
half the distance between you know half
the length of your foot in follow-up
work which is sort of submitted to a
journal but not publish you can show
that if you have a sequence of images
you can do a lot better you can sort of
use temporal coherence of the estimates
over time to improve things you can have
much well actually the way it really
works is you can have multiple solutions
so for here you're just getting your
best solution out by from searching over
the viewpoint and the pose that's not
necessarily going to be good because of
ambiguities there but you could have
several different you know several
alternative solutions using a method
that came out avi advices lab in Israel
and then over time you can sort of
impose temple consistency to pick the
ones are the most consistent and if you
do that the elevat-- gets lower possibly
if we tie together with shin JS work on
detecting things using the deep
convolution and networks we get a lower
results as well but anyway for me it was
an interesting thing to see how well one
kid that you know in this type of work
that you can get reasonable estimates
from single image in 3d I try asking
psychologists whether that's possible
for humans and they generally say no
it's impossible but they haven't really
done it on this type of this type of
data so so I realize I'm coming a little
bit early to the end partly because I
drop one section but knowing the nature
of things i'm sure people up set of torx
go over time but I've never heard people
being upset of talks were under time so
and we were on to the end that give more
time for questions or for hearing all
the other talks you've got set up today
it was out the thing about estimating 3d
but halim it out so just a summary of
this is that well a lot of vision is
currently down in 2d we have to the
images nevertheless and again for myself
and the spirit of the questions you'd
like to ask
that really images you know are
inherently 3d when we were doing all
that labeling I started looking at lots
of images and you know you start
persuading them sell yourself you can
see the 3d structure in all of them you
know obviously I look at this room here
just from a single I this incredible
amount of regular structure in it and I
could you know a total distance of
everybody in the room for reasonable
degree of accuracy just from single
image without needing anything extra and
I think that's probably true of you know
of most types of objects and again I see
a certain amount of other people you
know Stanford was releasing a data set
with the Pascal images where they have
the three-dimensional cars of moved into
the image and so on so it seems a more
natural way of representing objects
certainly from the prior position or so
from the point of view of analysis by
sense of this person it makes a lot more
sense something we haven't done here but
is sort of we did in a sort of a sudden
te a project is if you're dealing with
tracking people over time if you have a
3d model it allows you to tell what's
likely to be clued in what is not likely
to be occluded where should we do it all
in 2d you sort of get complicated
situations of where occlusions can
happen and again having spent a lot of
our time labeling the Pascal objects you
can see that there's a hell of a lot of
occlusion in those images and there's a
hell of a lot of clusion in almost every
image you look at and in general
computer vision is not you know it's a
task that the computer vision hasn't
really come to terms with and so you can
either deal with occlusion bicyle making
a model a bus so it doesn't really care
but if it makes it my bastards not jente
you know it doesn't care enough it sort
of made give you a rough idea but if you
really want to get the thing solved it
seems to me you have to start using
three-dimensional models which deal with
those types of situations ok anyway
vision here vision is extremely rich
interesting research field probably no
need to say here but you know I keep
trying to tell people no you don't just
have to
do a task that somebody has established
the data set forth you know we never you
know computer vision would never have
started if we know what we have to wait
for data sets to be around you know so
think of the enormous range of tasks
think of things like the sort of chewing
type tests what would you like it could
be the vision system to do be demanding
for it don't just get stuck in doing
caps in boxes as people like myself have
spent too much of our time doing a
second issue is really the complexity
and so here is the sort of these
theoretical studies which I'm keen on
trying to see if you you know think of
what's fundamental envision dealing with
the incredible variety a number of
patterns and at least in a human vision
system and of course in a computer
vision system being able to do what
those incredibly efficiently so it's got
to be something about the structure of
the world that enables us to do it and
the structure of the world they're
analyzed it's partially hierarchical
similar to the deep convolutional
networks more explicit in terms of
representations part sharing and sort of
abstraction of levels of representation
otherwise I will perhaps close up now
and tour the project here with the
papers cited they're available from
websites and so on okay well thank you
very much for coming in this early and
listening to me said that some you're
interested in how the human vision
system work and that's why you do
computer vision how much do you think a
successful can use vision system that
could solve the Turing test would
actually need to resemble the way we do
it well done all the way we do it I
don't yeah I don't know I mean I think
this is partly a sort of a value
judgment you know I'd rather understand
the human vision system I will have a
yeah wonderful set of practical computer
vision algorithms I just think for
myself a better way of doing it is to
best way for me to trauma like myself to
try and and some human visual systems to
build something with those abilities
I think at the moment the answer is that
I learn more about what humans can do by
having a human myself and opening my
eyes and I do from a lot of psychology
studies and so with a different audience
I can't be saying that to provoke them
you know be arguing with the kandake
armors and the banker sins of the world
to say look why don't you get involved
with doing real images and doing
experiments on them and see what humans
can perceive in those situations so with
those data sets one thing I'm trying to
do is to trying to get people to go out
there and do experiments on them and so
Debbie's work we the thing I mentioned
the beginning that you may talk about is
sort of you know has that sort of
relation you know you know for me right
at the moment I think you know the the
issue is right how much you can get out
from the brain if they were if you do so
I can physical experiments on realistic
images that would help us a lot might
not help them necessarily if you do
neuroscience fmri or imaging you know
understanding about electrodes that
would help too it's pretty clear at the
moment that all the knowledge you get
out from those is extremely limited but
that may only be a limited amount of
time there's some very interesting new
techniques to photo imaging things that
you can do high-resolution fMRI things
which you know are still far too
premature to say they would help people
in computer vision but the optimistic
ideas maybe if they develop it within
five or ten years you might be able to
start reading out activities of large
numbers of neurons and be able to pin
down where things are happening in the
brain Jim DeCarlo among others has done
some interesting work on that whether
that helps computer vision probably not
at the moment and i'm not you know and
i'm not sure but anyway it's a
generational thing people my generation
like it i think people younger than me
down as far as i can tell is that
accurate i won't mention it well yes but
anyway yeah i'm talking about how you
know so much business going to pretty it
upon the actual
mom you know in structure of the world
so it's kind of two questions mostly
related so first off how much of the
human vision system and the power of the
Commission system counselor having two
cameras as opposed to one and you know
do you think that we in computer vision
could gain more from you know building
data sets that actually have paired
images you know that that provide depth
information and thereby 3d information
and second have there been any studies
done of people who lose sight in one eye
early in life and their ability to
detect depth and 3d structure and images
there's been a number of studies i'm not
sure quite what the take-home messages
are those this man Mike mays who lost
sight at the age of four or something
and then he had an operation when he's
43 and we gained it to some extent and
what it seemed was that he could
recognize patterns but where he couldn't
really work visually see what the
pattern was he could sort of work out
maybe this pattern was of face of his
wife or something but it was sort of a
conscious act to decide it there's
probably person who asked that is pavan
sinha because he did this project for
cash in india and a whole lot of studies
of you know getting these villages where
people have cataracts and doing
operations and i think a lot of them
seemed to recover so i think you're
human vision is incredibly plastic which
is interesting and even if certain areas
are destroyed that seems to be some way
around them in some cases in terms of
how much stereo is useful for this this
is where you know first I'd be
interested in having stereo datasets
anyway to see what extra stuff you can
do for them if you'd ask the computer
I'm not sure one could get much
information from psychologists from that
ah either I don't know if it's really
enough on it well I mean there's a lot
of things I've been told about stereo
well it doesn't work about six meters it
doesn't you know it takes too long you
can't do it in real time and you just
have to go to an IMAX film to see you
know that's not really accurate you know
it's partly a lot of the studies done
under these controlled situations so if
you're looking at screens and you're
merging thing
in the image and you're taking getting
rid of all your accommodation cues and
so on yes it takes time but that's not
you know vision in the natural situation
the world number of studies that my
friend Hainish bull tov had done which
was arguing that you didn't need 3d
information that 2d was enough but you
really have to evaluate how hard those
tasks were based on the amount of
information available and like me no
friend and Kirsten did studies like that
and he with ideal observer models and
argue that in fact you were using you
know you're doing better than the 2d
ideal observer model could and so you
have some three-dimensional information
you know so a nobleman would say you had
to have three in the information anyway
so I'm afraid it sir yeah i mean i think
all i can say for my own thing I think
if I the one thing I found myself
needing needing two eyes for was trying
to run down the mountain late at night
and realizing if I ran with one eye I
wasn't seeing the ground nearly as well
so I had to but basically otherwise I
think you know one I for recognition is
probably enough you know and here I'm
saying mugging for having 3d models are
not saying that you need to have 3d cues
directly from the image you can get that
or fan out from the TV models so I think
it's probably high-precision things have
been sports players who lose an eye and
then never the same again now why would
put out if anyone remembers cricket
players from the 1960s who was a
brilliant player lost lost and I could
still play what was known as brilliant
as he was before yeah well yeah yeah you
mentioned this idea of a Turing test
based on a static visual scene then I'm
wondering if if this is really a
relevant task I mean humans have
because they need to accomplish survival
relevant tasks in this world and when
vision develops they're exploring the
rug hauling around if something is
occluded they can crawl around and look
at what what had been occluded
previously and so on it is it may be may
be looking at static pictures is really
our ability to analyze static pictures
is really an artifact of that and really
what we should be looking at his video
streams and the streams generated by
robots that walk around and so on yes I
have lots of interesting lunches with
Stefano saguaro where we have
discussions on that I mean I argument
with there would be I think when humans
are developing it's probably pretty you
know the evidence suggests that humans
you know infants really make use of
three-dimensional structures to get
images you know to get start
understanding and certain abilities
which depend on it depend on motion cues
develop a lot earlier than static the
ability to pause static images on the
other hand once you become an adult you
know or whatever level you get up to the
fact that we can do things with images
static images a sign that it's possible
and from my perspective it's a sign of
the computer vision systems should be
able to to do it so it's not that you
know I mean if it sort of was really an
artifact and you could argue that right
you know I just show you one image
you've got no idea what's in it another
general point to make there is I think
that a lot of work in vision you know I
would argue a case as this is be
provocative to Stefano is not here but
you can use motion sequences as a crutch
to avoid doing the really hard problems
you know so I'd argue that the fact you
can get a lot of depth information out
from a single image is that maybe you
should be getting you know using
sequence of images to update the
information you get out from a static
run rather than just using the image
sequence matching interest points
together and so on
you know much is a at the time time is
coming up so yeah I'm happy to make more
for vocativ comments later yes okay
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>