<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>ML Day 2014 - Learning to Act in Multiagent Sequential Environments | Coder Coacher - Coaching Coders</title><meta content="ML Day 2014 - Learning to Act in Multiagent Sequential Environments - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>ML Day 2014 - Learning to Act in Multiagent Sequential Environments</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Rvnfvsd-Mis" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
the the afternoon session we have some
excellent talks lined up but first we
have something extremely special for you
Michael Lippmann is going to give us the
premiere live performance of his machine
learning adaptation of Michael Jackson's
Thriller so if you if you google machine
learning acapella then you will you will
see Michael Lippmann singing Michael
Jackson's Thriller over-fitting
yeah but first he's going to give us a
research talk instead I didn't see that
coming but it is true I just released
the video yesterday so you feel free to
go and check it out but I am NOT going
to perform it live all right
anyway thanks a lot for coming and
forgettin for the organizers giving me a
chance to talk we decided it would be
maybe ok to give kind of a reinforcement
learning talk at a kind of a tutorial
level because probably that's not where
most people are coming from so so
depending on how you think about these
things this is going to be a narrow
narrow narrow talk or a talk about
absolutely everything because it's about
acting in in multi-agent environments
which is what we do all the time this is
what we do we interact with people I'm
interacting with you right now
alright so or you could think of it as
well with in machine learning there's
reinforcement learning which you know
maybe not a lot of people do and within
that this is multi-agent reinforcement
learning so maybe that seems narrow but
um but I think it's actually really cool
and it's worth it's worth thinking about
and thinking about how this could
interact with other aspects of machine
learning as well and I had a little
clicker thing but I don't know if I do
anymore I'm gonna use the spacebar
alright so so let me start off by kind
of setting up the classical
reinforcement learning setting and then
we'll move over to the multi-agent
setting as we go so so here's for
whatever reason that ended up being a
Mario theme so there's Mario like from
the video game and for whatever reason
he's trapped in a grid world and just
like many reinforcement learning
researchers and what what Mario is
trying to do is trying to get to one of
these
red-gold states it's red to match his
let's say hat and he can move left up
down left right in the grid
he can't move through these sort of
heavier lines those are those block his
his motion and these ones that are kind
of dot dashed a little bit like half
heavy half light it's like 50%
probability that that Mario will be able
to get through those so if he if he
tries to go through it it might fail and
he'll end up where he started
or it might succeed and he'll end up on
the other side so can you look at this
and figure out what his optimal strategy
would be to get to a red goal all right
he should go to the right
oh sure that would be the first action
that's not a bad choice which goal is he
gonna go for or is it all contingent
could be contingent this one this one's
nice and close so what is the expected
number of steps that it takes to get to
the close goal this goal here so well it
depends on how you do it but one way to
do it is to go here here let's see 1 2 3
4 5 6 7 8 9 10 11 12 13 14 15 that was
expected values that's what I was doing
with the double hit so 15 steps to get
to that goal did anybody find anything
better than that top right the farthest
one 1 2 3 4 5 6 7 8 9 10 11 12 13 yeah
so what's my point
my point is I can make a grid to fool
you it's not clear that that's an
important point to make but that is the
point that I was making with this but
but I wanted to get it across a couple
of things one is that it has all the
elements that we need to define a
reinforcement learning problem one is
the states in this case its positions in
the grid
but more generally it's going to be
whatever state whatever aspects of the
environment are important for decision
making so it could be how tired he is
and whether there's you know I don't
know the right words Goomba maybe what
the like baddies that are that are
prowling around in the grid maybe even
when his goal is it might actually
change and therefore needs to be part of
the state
there's
this so there states there's actions
which is what moment-to-moment he is
capable of doing which in this case is
the up-down left-right
there are state transitions so what's
the probability that if Mario chooses to
go to the left that'll end up in this
grid cell on the next step and there's
the reward function which in this case
there's a payoff for getting to one of
the the red goals and otherwise maybe a
small cost for just you know wandering
around in the grid consuming energy so
so that's that's the essence of an idea
this is what most of reinforcement
learning is about one way or another
usually it's not quite so large grid see
that's just not nice to my feel I'm
sorry um but but one thing that you can
notice about that is it's kind of a
lonely place to be Markov decision
processes right cuz there's a little
Mario in the corner and then there's all
this empty space you know just wandering
around Cambridge and there's no humans
so the idea of multi-agent reinforcement
learning is learning what to do given
that there's other critters also
prowling around let's say in the grid
also trying to decide what to do alright
so so that's what we're gonna get to but
first I want to point out if you haven't
seen this sort of thing before how do we
actually go about solving these things
how do we solve these kinds of
sequential decision problems in this
case it was formulated as a Markov
decision problem or Markov decision
process which as I said state's actions
rewards transitions usually we'll throw
in a discount factor to help make things
mathematically helpful and it's all
about the interaction between the agent
and the environment to solve these
things one of the ways that we can do it
if you know the entire environmental
model in advance you know what the
states and actions and rewards and
everything are you can solve the system
of equations called the bellman
equations what they tell us is
associated with each state and an action
you could take in that state
what's the immediate reward going to be
plus the discounted expected value of
the state that you end up in for taking
that action of the value of the state
you end up in which is going to be the
maximum over all actions you could take
there of again this Q value in the
resulting state so you know there's Q's
on both sides so it's not completely
trivial to solve this but we know lots
of ways to do it it's not a big deal and
if we solve it if we actually have a
solution to this we know how to behave
optimally what we do is
time we're in some state s T State s T
at time T we're gonna choose the action
that has the highest Q value it has the
highest expected future and present
reward for taking that action so that's
that that's the solution if you don't
know the reward and transitions in
advance
you could learn them right so you can
wander around in the world and try to
learn a mapping from state and action to
reward and state an action to next state
or you can do tricks like like Q
learning which tells us that what we're
trying to do is each time we take a step
in the world we have estimates of these
Q values and we just change them a
little bit so that they're a little bit
more like solutions to this so that's
the essential idea for single agent
reinforcement learning there's as I say
there's a couple different ways of
solving these policy searches another
where you're actually trying out
different ways of behaving and see what
they actually see how well they actually
work
value based methods are trying to
estimate this Q value and iteratively
improving it using experience and model
based approaches say okay we're just
gonna we're gonna do something like more
akin to supervised learning which is to
learn directly a mapping from state
action to next state and reward this is
a nice learning problem in the sense
that each time the agent takes a step in
the environment it gets a sample it gets
an example of what what the reward was
the immediate reward was and what the
state transition was this which is more
commonly done in the community is a much
harder learning problem in the sense
that what you're trying to do is predict
what future Q values are going to be but
those are removing target as you're
moving around in the world and making
improvements to your q function you're
trying to train towards that but you're
also trying to train that so this can be
very very messy and and there's it's
hard to get formal results that say that
you're gonna do the right thing here
it's not so bad at least in principle
all right so what I'd like to do is move
us to the multi agent setting so so the
multi agent setting is really hard for a
bunch of reasons one is it's kind of not
entirely necessarily clear what it is
you're trying to do so let's say that
we're still trying to maximize reward
that seems reasonable right we still
have some notion of a reward function
we're still trying to find a way of
behaving that makes us as happy
possible okay that's as fine as far as
it goes but it's still not completely
specified what we're gonna do is imagine
that there's okay so here's Mario again
here's he's us and this is Luigi that's
right
Luigi he's another guy in the
environment but let's imagine that he's
also like us right so he also has same
set of actions transitions now are
actually dependent on the actions that
we both choose so both of us are gonna
get to choose an action and what happens
next depends on what we both decided and
we both have our own reward functions so
Mario's getting paid off for the actions
that happen and so is Luigi's and my
actions Mario's actions can depend on
Luigi's choices as well right so I don't
have complete control over my own
rewards anymore right I'm kind of
dependent on what what the the decisions
are my partners in the environment so so
here's a little good world example just
to think through how how this goes so
what we're trying to do is again we can
go north south east west the walls are
the same as they were before Mario is
still trying to get to red because of
his hat
Luigi still trying to get to green well
Luigi's trying to get the green because
of his hat what should we do all right
was a little bit subtle before because
it was complicated this now the grid is
really easy right the shortest path for
Mario is and well any of a number of
paths would take take a shortest number
of steps to get to the goal what's the
problem the problem is we don't know
what Luigi's gonna do so so what do we
do how do we how do you do this in
general so Mario could choose to go to
this square on the first step but Luigi
could also have chosen to go to that
square on the first step so that doesn't
seem so safe right so maybe Mario should
go up one because that's on route to the
goal but is it definitely isn't gonna
collide with Luigi so that's good now
the weeds you could reason the same
thing and now in the very next step will
be here which is kind of the same
problem that we're in in the beginning
except one more step closer to death
so it's not exactly clear okay well what
do we do then we could do the same kind
of reasoning and mario ends up in the
green and luigi ends up in the red and
now we're really in trouble because now
there's no way they can both follow the
shortest path it'll be nice if they had
kind of worked it out earlier so I don't
know
not so maybe the punchline of this talk
is I have no idea so moving on so
alright so there's a little collision
from this date it's not quite clear what
you should do certainly mark well let's
see what happens all right well Mari
ended up trying to go that way and Luigi
the other way and they missed each other
and that's great because now they can
just follow path to the goal oh wait I
forgot to tell you as soon as anyone
gets to the goal the game is over so in
this case Luigi got is a hundred points
for getting to the goal and Mario is
done nothing nothing sadness so um they
can both get the hundred if they both
get there at the same time but they
didn't so that was kind of a bummer for
Mario so uh so so now there's sort of an
interesting thing going on right so they
can be a little bit cooperative in that
they can both get high reward or they
can be a little bit competitive they can
get each other's way and prevent the
other one from getting reward it's not a
zero-sum game in the in the classical
sense right where it's not that Mario
lost because Luigi won it's that Mario
lost because while Mario lost right and
Mario could have won even though Luigi
won all right so when we try to actually
define something like the bellman
equations for these kind of games what
we realized pretty quickly is it's going
to depend a lot on what we think the
other player is going to do and so if
you if we're thinking now kind of in
terms of the literature on this on this
problem you get lots of different
algorithms popping up depending on what
you think the other player is like which
is kind of unsatisfying but let's just
go with that for a minute let's imagine
that there's a lots of different
personalities that that the opponent
could take Mario doesn't know which one
it is but mari is just going to commit
it's gonna pick one and depending on on
the assumption about how the opponent is
going to choose actions we actually get
something like a bellman equation with a
different operator here that this used
to be a max in the other equation now
it's you know kind of depends on what
the other player is going to do so the Q
value to to Mario q1 in some state given
a pair of actions for the two players is
the immediate reward to Mario for that
pair of actions that part's easy plus
the discounted expected value of the
next state that they end up in together
and what
in the next 8 well I don't know but it's
gonna have a set of Q values associated
with it and we have to somehow summarize
them so this operator is just a generic
operator where I'm saying let's just
summarize and we're gonna look at what
different possibilities are for this
slot they they come from different
assumptions and they lead to different
algorithms and different results so so
if we assume for example that the other
that the partner is this some kind of
fixed sort of Markov chain type agent
and we know what it is then then it's
really it's really easy because what we
can do is we can take an expected value
over the actions that that other player
is going to take because it's just
following whatever it's following all
right and so then this this operator
this summarization over the over the
value in a state ends up being I'm going
to choose whatever actions best given
that my opponent is choosing randomly
according to this distribution so you
can do that and solve it it just turns
it back into an MDP all as well if
everyone could just be fixed and tell me
their strategy the world would be easier
for me all right if the other if their
partner is like that but actually has
its own internal state it's not basing
its actions just on the current state of
the environment but also you know its
mood or things that have happened in the
past now it gets bad it's it's a
partially observable Markov decision
process if the behavior is known it's a
pom DP learning problem if it's unknown
we don't really know how to solve those
so yeah so there's that alright so but
there's other things we can assume as
well so we don't have to assume that
they're sort of you know mindless robots
that are doing whatever they're doing we
could assume for example that they are
really really helpful
like Princess Peach would probably be so
Princess Peach is our assumption our
Princess Peach type assumption says my
opponent is going to choose whatever
action is going to make me most
successful all right and so that
substitutes this summary operator with a
max max right so of all possible joint
actions we will choose the thing that's
best for me all right super helpful very
easy to compute it actually it turns it
again back into an MDP we're essentially
I'm choosing actions jointly I'm
choosing actions for all of us which is
great but kind of unrealistic maybe we
should be you know
computer scientists about it and think
worst case so worst case this is like
thinking like AI people best case
analysis and this is like thinking like
like theoreticians worst case analysis
the worst case analysis says I assume
that whatever the opponent is trying to
do
he has no concern for his own welfare as
by the way neither did Princess Peach
his only concern is to make my score low
all right
he's out for blood my blood that turns
the summary operator into a minimax
right is saying I'm going to choose
whatever action is best for me given
that the opponent is trying to low is
make my score as low as possible this is
a well defined operator you can solve it
with linear programming things work out
sort of nice but again it doesn't feel
entirely realistic right so let's go
back to that grid example and see what
happens actually a slightly different
grid this is a grid well anyway this is
a good that turns out to be surprisingly
interesting given given how small it is
so here Mario is trying to get to the
red Princess Peach is trying to get to
the pink there's you it might not be so
visible but there's walls around these
colors so the only way into this red
thing is to come from where Princess
Peach is standing the only way into the
pink thing is to come from where Mario
is standing all right so there's kind of
in a in a standoff right from the get-go
so what does the Princess Peach
assumption get us for this the so-called
friend queue algorithm it says that well
Mario's trying to maximize reward and
and Princess Peach's try to help Mario
maximize reward so the strategy ends up
looking like this right Princess Peach
gets out of the way after you right or
not even after you when you're done it
will all be over and then Mario scores
and you know and all as well all right
that's that's this is an imagined
situation right because we don't know
that it really will play out that way
but Mario computes his values computes
his policy under the assumption that
something like this is going to happen
in this situation though it's quite
different what's going to happen so a si
so we reset that one and now this is
what the strategy looks like for the
second game right so mario thinks okay
Bowser anybody do we know dragon
creature bad dragon creature is is
trying to hurt Mario Mario is trying
get to Mario's goal bad dragon creature
doesn't want that so bad dragon creature
just parks in front of the goal
effectively blocking Mario permanently
Mario at this point thinks okay well if
you're just gonna block me I'm not gonna
waste any energy moving around I'm just
going to die here
I'm just gonna stand here forever so
this is the optimal strategy under the
assumption that you're up against this
sort of Bowser II character again these
you know we can compute this we have
reinforcement learning algorithms that
will converge to this solution and then
when you actually try them with other
agents they do terrible things like
right or or like I'm just gonna walk to
hey why are you still in my way for I'm
expecting you to get out of my way they
just they play terribly alright so so
what do we do well so there's an answer
to this kind of question what do you do
when there's two different players and
they have their own they're their own
feelings this is as far as I know the
only the only result that has won both
the Nobel Prize and the Academy Award
for Best Motion Picture and that is Nash
equilibrium so the idea of a Nash
equilibrium says we're gonna try to find
jointly a way for us to behave so that
we're both as happy as we can be in the
sense that if you stay fixed I'm doing
what's best for me if I stay fixed
you're doing what's best for you it's a
natural enough kind of concept the
equilibrium aspect of it is is the idea
that once we're there there's no
incentive to change might be hard to get
there in fact we think these things are
very hard to compute in general
especially in these sequential grid
games but but nonetheless maybe this is
a concept that we can go for and in
particular it gives us a way of thinking
about once we're in a new state how
should we summarize the value let's use
the value of a Nash equilibrium so
that's an okay idea let's see each so am
I gonna animate this apparently all
right hang on a second I wanted I want
to lead up to it so apparently not
apparently I'm just gonna animate this
alright so so let's look at this game so
what's a Nash equilibrium for this game
well the Bowser thing is that is a Nash
equilibrium if the if if Mario thinks
Luigi's not going to move then Mario's
best strategy is to not move if Luigi
thinks Mario's not going to move then
Luigi's best strategy is to not move it
is a that is a Nash equilibrium it's not
a great one but it's but it is what it
is the Princess Peach thing is not a
Nash
equilibrium right so the Princess Peach
thing says tamari is just gonna start
going for the goal of course Louise you
might start doing that as well they're
gonna collide in the middle given that
Mario's trying to get to the goal
Luigi's best strategy is not to get out
of the way it's to stay put all right so
so this is not Nash field what is a Nash
equilibrium so I was very surprised the
algorithm I'm about to tell you found
this solution which I purposely tried to
make not a solution but it actually
turns out to be a solution so what's
gonna happen
well let's take you through step by step
so in the first step Mario starts
heading to the goal Luigi stands in
Mario's goal okay it doesn't seem like
it's gonna go particularly well Mario
continues towards his goal and then
misses it and then takes one additional
step into the corner while Luigi comes
out of the goal what's the situation
we're in now Mario is one two three
steps from Mario's goal Luigi is one two
three one two three steps from Luigi's
goal they can now proceed happily
towards their respective goals if each
player is if if Luigi adopts that
strategy that was Mario's best response
if Mario adopts that strategy that's
Luigi's best response this is also a
Nash equilibrium but it's a much better
one than they're just sitting on your
butt one right so this is actually
really cool I was I thought this was a
bug for a long time but in fact this is
a very reasonable strategy like it seems
like they're way too smart the thing
that I think is really cool is it's this
step here when Mario's here right why
should Mario continue going away from
the goal it has to go back to the goal
at one point why does it just wait there
Louie why why won't Luigi move right so
if Luigi comes out to here Luigi is one
to three steps from the goal at Mario's
1/2 steps for the goal we just starts
you know booking it towards the goal and
Mario's like ding game over right so
Mario has to has to do one of these
right like it's okay it's okay
look I'm not a threat to you and now we
can proceed right it's like I you see
these things on TV sometimes or they're
like pointing guns at Easter you have to
put the gun down and back away from the
gun right you can't just like no yo you
put the gun down right because no one's
going for that so it actually converge
on that so
completely on its own I mean whatever it
followed its algorithm it did right
because it's not safe for Luigi so the
weeds you won't move so Mario won't get
his score so it's better for Mario to go
all the way out and be clear yeah a bet
it is a better strategy in the sense of
maximizing the total expected reward but
it's not a Nash equilibrium because it's
not safe for both players all right
anyway particularly like that example
all right so so the Nash equilibrium
algorithm says that what we're going to
do is summarize the values in a state by
computing the Nash equilibrium backing
that up so this turns out to be
problematic so for one thing you need
access to your partners values to
compute that maybe that's not so bad I
can learn about what I'm doing I can
learn about what you've learned that
should be alright but unfortunately it's
not a non-expansion it's not a summary
operator that that actually is
guaranteed to converge to go to a fixed
point and in fact there's multiple
answers right so during the algorithm it
can be faced with it with a set of Q
values that have multiple ways of
computing Nash equilibria with different
values which one should it pick the
highest one the lowest one the middle
one a safe one it's not really clear so
the result of this is that it doesn't
necessarily converge sometimes it just
doesn't converge and in fact this isn't
just a little thing that we're going to
be able to get away from this whole idea
of using Q values to decide what to do
in multi agent environments can't work
it's a fundamental limitation of this
class of approaches which I won't get to
give you the whole proof of but marty's
and cabbage came up with a really simple
game he calls it the owl game I don't
know so he added the owl I put in with
Luigi characters it's now complete mixed
metaphor I have no idea what how to
think about this but I did find a Mario
owl so it you know makes it a little
work so so but this is a little MVP a
little two-state problem we're in this
state mario gets to make the decision
and in this state Luigi gets to make the
decision that has no deterministic Nash
equilibrium the only Nash equilibrium in
this tiny little game have the property
that both players have to randomize now
usually we think about randomization is
something that you have to do
because of simultaneous moves in a game
like matching pennies game right or
rock-paper-scissors if I always throw a
rock of any deterministic strategy and
rock-paper-scissors is going to get
trounced right but if you know but if
but if you're not trying to fake out the
other player what's the point of being
using a mixed strategy being
probabilistic it turns out that it's
necessary to get everybody's values to
kind of equilibrate and so that's what
ends up happening in this little game
here that that if Mario does something
deterministically the best response to
that is for Luigi to do something else
for which the best response is for Mario
to do something different it can't
stabilize the point of this example is
that means that because it's there's
only one action in each of these states
no matter what qubit values you give me
I'm never going to determine I'm never
gonna derive from that a stochastic
policy the values are not enough
information to decide what to do you
need something about the whole picture
and so this whole idea of using Q values
to decide what to do just like it can't
work in this game it can't work
therefore in general so that was I guess
a punchline I've got 15 seconds is that
true at 15 seconds all right so in Ko Ko
values all right moving on
so cocoa values are a result due to
kalaiy and kalaiy this class do we have
any collides with us all right so they
came up with this idea of mixing cocoa
in addition to being delicious is also a
mixture of cooperative and competitive
and so what you do is to compute the
value of a game you say well what will
we get if we work together versus what
will we get if we worked separately and
let's use that separate value to
actually determine side payments like
we're gonna work together but since you
had the power position in the beginning
I'm going to give you some money to make
it worth your while to do this thing
that's best for both of us we're gonna
maximize our pot and then split it in a
way that's kind of fair so they
determine this idea we applied it to two
sequential games and actually were able
to prove that a learning out a
reinforcement learning algorithm in this
setting converges so that's pretty
satisfying it's not going to be the Nash
which is a bummer and it's not going to
be it depends on this notion of side
payments which we don't always have
access to but still it seemed like a
cute answer to thee there's just no way
to solve this problem problem all right
so just you know really quickly this is
what it does in this case they end up
paying each other to take risky moves
and then they make go to the goal
together split the winnings and all
right so last slide right so standard
reinforcement learning is kind of kind
of lonely and if we want agents that are
actually interacting in the real world
they have to deal with the fact that
there's other agents in the real world
if nothing else the other reinforcement
learning agents that we finally field it
so given that that's the case we have to
think about what it means to learn in
that setting there's often no right
answer some of these games are
unsolvable in a sense really the right
way to think about this is to keep in
mind that multi decision decision-making
always takes place in a culture it's not
just a limitation that we have to guess
who our opponents are that's just
reality and we have to learn about what
kind of opponents we tend to play with
are we in a society where everyone helps
each other or splits the winnings are we
in a society where everyone's trying to
throttle each other you're at your
optimal action depends very much on the
actions of others okay that was my talk
Thanks we have time for a quick question
seems like a lot of these you know a lot
of spoils down to symmetry breaking of
various kinds and that there must be I
mean on one hand you mentioned that the
sort of random moves aspect of it is
that enough in general to deal with
symmetry breaking between both the
different Nash equilibria but also in
you know in your first example or
somebody's kind of taking a top root in
the bottom root right right does it
would that sort of solve that without
thinking about these so yeah I hadn't
thought about it that way it's
interesting so symmetry breaking
certainly in the case where the two
agents had to choose paths that didn't
bump into each other but they crossed it
was all about well they both could do
exactly the same thing but we can even
muck with that question have it be
asymmetric and still have to worry about
symmetry breaking in a sense right if
one agent was actually getting a whole
lot more reward for getting to its goal
and had a slightly shorter path say and
the other one had longer paths but could
make some amount of money from it it
still might be the right thing for one
to wait to let the other one go through
maybe you need side payments for me or
something some kind of some kind of
agreement gonna play this game again
against each other tomorrow and I might
be in the other role in which case I
might want you to do well so that you'll
be happy with me the next day it's not
just symmetry breaking there's a lot of
weirdness
the goal and I'm gonna wait until the
other guys goes to the farthest corner
before I do the code to make sure like
I'm optimal and right so there's some
really interesting work in the in the
context of repeated prisoner's dilemma
where if you have it if you're playing
against somebody who learns right we
care about learning we're playing
against somebody who learns and it's
just going to assume that I'm a fixed
strategy it's gonna adapt to me then the
best thing that I can do is go right to
the edge of cooperation the littlest
amount of cooperation that I can you'll
go along with that I'll get a high score
so there are some really nice results
saying that in many games you can
actually do that you can extract out of
a learning agent the maximum amount that
can be extracted before but eventually
it's like don't forget you I'm just
gonna go for myself I don't get an
additional reward by by doing the thing
that you're asking me to do the problem
with that is of course both agents could
be in that mindset in which case they're
both extracting the maximum reward in
which case they both do horribly it's
like um everything ultimately boils down
to chicken this is my feeling about
multi-agent game theory is it tastes
just like chicken all right so all the
games end up having the property that
you want to be the last one to learn
right is if you show that you're a
learning agent then the other player can
take advantage of you by adopting a
strategy that when you adapt to it is
best for the opponent so you just want
to show that you can't learn right so
they'll adapt to you and all will be
well but you don't want but you're both
trying to do that so you want to be the
last one to look like you can learn
right don't blink let the other one play
first and everything's better so that
it's a horrifying all right let's think
Michael again all right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>