<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Time series inference with nonlinear dynamics and filtering for control. | Coder Coacher - Coaching Coders</title><meta content="Time series inference with nonlinear dynamics and filtering for control. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Time series inference with nonlinear dynamics and filtering for control.</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rGZAis6Z3GQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright well hello my name is Roland
McAllister and I'm a third year PhD
student at the machine at the machine
learning group I'm supervised by Karl
Rasmussen and I'm going to talk a little
bit about our research in learning to
control so I'm going to break this talk
up into five main points first I want to
talk about control a general framework
for control a very simple one and then
second how we can learn within that
framework thirdly how we can incorporate
filtering into our control to help us
out with observational noise and then
fourth how that affects the learning to
control by using filtering and then at
the end we'll have some fun videos of
the experiments so so first up i'll talk
about control so as way of introduction
consider you've got some non dynamical
nonlinear dynamical system such as the
car pole it could be anything but for
concreteness we'll we'll go with this so
the car pole r is constructed such that
a cart can move along in an X direction
and attach to the cart is a frictionless
is is a pendulum with a frictionless
pivot and that pendulum is free to swing
around and we can also apply some forces
in the in the X direction as well in
which to influence the system and the
goal of this our goal here is to be able
to invert the pendulum which starts off
in a downwards position and be able to
balance it up right at an origin and the
control task here is how do we apply
forces in which situations that the cart
pole is in such that we can achieve our
goal and and this system is not is not
so trivial in the way that we can't just
use some linear controller to soar but
we can't just use some PID controller it
has
a little bit more complex than that due
to the non-linearity of the dynamics
okay so what's a what's a more solid
framework for that well we can start by
saying by writing down the latent state
of the system for the cart pole this to
fully describe the labor the dynamical
state of the system all you need to know
is X the position you need to know theta
which is the angle of the pendulum and
the time derivative of those two with
that information you can predict how the
system will evolve so we start off with
this and then we've got some noisy
senses which are giving us starter on
this system so we've got some wire which
is related to X except it's got some
noise added on and from why we're going
to make a decision given by the
rectangle box you about what force we're
going to apply to the cart in order to
control it so this force as well as the
current state of the dynamical system
will define how the system evolves in
the next time step and that's according
to some unknown layton potentially
nonlinear function f and being a time
series model the whole process repeats
okay great so this is how we can control
the system but what if we start off in
the position where no one's given us a
control yet no one's know it no one's
told us what that mapping pires and and
additionally we have no idea of f we
don't know physics at all what can we do
well so our approach is to first of all
to start looking at F and and to say and
to say that if we can if we can train a
probabilistic model on the potentially
nonlinear dynamics function f what we
can do is we can make predictions about
future dynamics and ultimately what we
care about is long term dynamics which
we might not be able to learn directly
but what we can learn is short term
dynamics from one time step to the next
and so the ability to make sequential
predictions from one time step to the
next will allow
us to simulate how the system might
evolve from now up until some time
horizon far away so this brings us into
learning control and the simulator for
us plays a critical part so the only the
only difference between our simulator
and what's happening in the present
reality is that in the present reality
we have these specific truths about the
state of the system X which we can't
observe directly but we do get a
specific observation read out by our
senses and from that we have to make a
specific decision about how we're going
to control the system so the only
difference in simulation is that there's
many ways that this system could evolve
so we use an analytic framework of
Gaussian distributions to capture this
and this is opposed to something like
particle filtering one of the reasons we
do this is that associated gradients are
nice and easy to compute and very smooth
and one of the philosophies that we
stick with is that a good simulator must
reflect reality so that means in the
distribution of simulated variables X Y
and you we would be acting precisely as
we would be acting in reality so for
instance in reality this is closed loop
control so in simulation it better be
closed loop control as well and this is
opposed to other methods such as model
predictive control which do a popper
eight closed loop in reality bets open
loop in simulation by pre computing
future actions up and took some time
horizon far away okay so why is this
useful what why are we talking about a
simulator so a simulator is valuable to
us because it means you can dream up any
particular controller that you might
like to try next or any particular
parameterization of a controller and
what we can do is we can run that
through the simulator and see how well
does that control your system does it
control it well law or not so well and
we can pick the best one and then try
that on the actual system okay so
what's a more concrete way of saying
exactly that so what we first do is we
define a controller we define a control
this functional form which is fixed and
there's a particular parameterization
theta that we're trying to find and the
controller takes in that observation two
together with the parameterization and
outputs a control signal okay so with
our simulator what we can do is we can
simulate from our current position X
naught up until some future time horizon
X horizon and so with what the
controller what the what we can now do
is we can evaluate the controller by
taking an expected summation of
discounted costs from now up until the
horizon where the expectation is over
possible future evolutions that the
system can take from now on up until
some future time horizon and we can also
get gradient information from that which
helps us to optimize the controller and
so with respect to all the data that
we've seen so far we can come up with a
particular locally optimized
parameterization for our controller
which we can then try on the system okay
so so that's a method to that's a method
to learn how to control and what I'll
talk about next is control with
filtering so if we go back one sec one
of the problems that we have up at the
top there is say in the cart poll
example say that the cart poll is
managed to swing up this pendulum and
it's currently balancing it at the top a
good controller in that wreck in that
region of the state space will
effectively learn high gain parameters
where where it knows that if the
pendulum starts to fall it's it's very
quickly got to get the cart under the
center of gravity of the pendulum before
it starts to fall past some threshold
and then the cart can't catch the
pendulum anymore and the pendulums swung
down and the system's totally
destabilized so one of the problems with
that is if is is if we
got any any kind of reasonable amount of
observational noise that's getting
injected directly into our controller
through these high game parameters and
produces wild variations in you and
that's how we're affecting the system so
with wild variations in the output of
the controller we get wild variations in
the in the state of the system alright
so we can we can fix that with some
filtering alright so what is filtering
so filtering same same kind of graphical
procedure as before we start off with a
latent system state X but we also start
off with some some filter state a prior
belief about the state of the system as
before we get a noisy observation and
what we can do is we can combine both
sources of information our prior belief
together with an incoming observation
and together that can form a updated
belief about the state of the system
which we can input our into the
controller to make a decision about what
we're going to do now the good thing
about this is that the controller can
now make a better informed decision and
it's also it's also it's also a much
smoother signal o into the combination
of both these sources of information
before input into the controller and
then as before control with the latent
state of the system defines how the
system will evolve and the last step is
that because we know that this is going
to happen we can update our belief about
the system together the action that we
took to define a predicted belief
posterior about the state of the system
and the time series continues so so
we're almost there we've we've filtered
that we've filtered our signal that the
the last the last thing left were the
last thing left is that our simulator if
we leave it as it is is now a bit out of
match with what's happening in
reality this see me we haven't yet told
the simulator that what we're doing is
we're going to be filtering the system
so to make it a realistic simulator what
we'll do is we'll go back and we'll say
okay so in present reality what we had
is this belief distribution about the
state of the system and that's going to
affect you and in the next time step
that will thus affect x x and y so it
really is going to affect our system so
we better tell the simulator that so the
difference here is that X Y and you are
the same as before but with our belief
posterior instead of say having a delta
function around the mean of our belief
we now generalize that to say okay
there's potentially different means that
define our belief posterior that that
happen in the future and just to be just
to make things a little bit easier
rather than having a distribution over
the matric of over this some matrix that
defines our or our variance of our
belief we just we just we just take that
as static across all potential features
just to make things a bit mathematically
simple simpler okay great so I think yes
so now so now I'll get into some of the
experiments and I first of all start
showing you a video of the system
training so first of all it starts with
zero prior knowledge about physics and
it and also the controller is completely
not trained so we'll see in about four
or five trials the system will
eventually learn what to do so the goal
is that cross at the top that's the
origin and this green bar here is to
give us an indication of the applied
force that we are applying to the cart
and what we'll see as a gold bar come
out where that immediate reward is to
give us an idea of how close the end of
the pendulum is to our goal layer which
is the cross right at the top so I'll
play that
which is not open all right so initially
it doesn't it doesn't know what to do
it's just trying random actions the
second trial it's kind of it kind of got
a hint of what was there you've got a
reward up at the top there so third
trial now it's starting to replicate
what it would it would have saw there
and by the fourth trial it's almost got
it and by the fifth trial it does it
does pretty well so now if we take this
controller and then run that many times
once it's trained what we can see is
independent of the start positions of
our variable say X or theta over time
they converge to X the region's ero and
also inverted which is 2pi at the top
and this is completely unsurprising to
our the confidence bounds that our
simulator is giving giving us described
in blue so now I'll show you a and a no
filter controller and with significant
observation noise so X we've got four
centimeters of uncertainty given by and
we can see that uncertainty in the end
of the pendulum given by that bound that
blue kind of ellipse on the end so it's
it's kind of just on the edge of being
able to control it so if we ramp up the
the observational annoys standard
deviation to about six centimeters and
six degrees this is now kind of just
getting off the cuff in terms of what
aid no filtered controller can handle
and you can see it's not it's not quite
being able to control this thing
alright so now we can now we can talk
about that third or fourth step where we
say okay let's fix that policy but let's
just help it out by filtering filtering
the observations that it's getting so it
does do it does do a little bit better
we haven't yet told the simulator that
we're doing this but we're just helping
me out by filtering those observations
so it does a bit better and but it's not
until we really go back and tell the
simulator that this is what we're doing
without collecting any more real-world
data but just re optimizing that and now
it does much better it's sticking to
that it's sticking to that at least
within the the width of that cross there
each time so yeah so and then that's
basically because our our simulator is
now much more realistic with respect to
what's what's happening in reality and
that's about it
we have time for questions and scaling
this up to bigger problems so i think
this currently in terms of higher
dimensional problems i think is
currently scales quadratically so one of
the things that we've been looking at is
how to we compute a lot of covariance
matrices and so the off diagonal terms
who have been looking at ways to come up
with approximation techniques to
effectively make it linear with respect
to how many state variables we had in
this case we had four but to really
scale it up that's one way of making it
at least approximately linear yeah so
we've we've started to test that out now
and it we've found what you might expect
it at least with an approximated a
filtered simulator it does a lot better
than without using a filter at all but
not quite as good as if you compute
everything exactly but it's not far off
the mark so I think those approximations
are potentially the way to go you have a
few examples of somebody demonstrating
it for example or good instances just
visit a prior observation everything
goes through as this or it does it
require money engaged author I Jemaine
is that of policy that it can that
doesn't matter what articles you view so
I we gobbled a small car like you could
make a video game out of it and probably
get 100 really good human human control
since this where they control the cart
very well you some what you said is it
just safe I media it doesn't matter it
doesn't matter how you generate the data
it's forgot the reinforcement learning
terminology if it's on off policy or on
policy but it's independent of what
generated the data yeah so in the last
example we actually use data from the
know filtered controller but they're
filtered controller once it once it once
we told the simulator that that's that's
what we're about to do it didn't care
about where the data came from just as
long as it had data system rather than
the assumptions that you have for me for
this invasion a yeah a good point so so
that's something that we're going to try
next to kind of to kind of weed out
these criminal pertinent issues that we
might be able to looking in simulation
so I think one thing that happens in
reality is you often get a bit of delay
between what the signal is when the
signal reads and you know when when you
actually get the signal it's it might be
a few you know a few milliseconds after
the fact that has happened so one way
that we've been able to at least deal
with that now is by incorporating some
of the old actions into the
representation of the state so so even
though there's this mismatch between an
observation and applying a control at
least had some idea of the control that
it was applying a previous to the
previous to the new observation coming
into to kind of help to kind of help the
Markov assumption a bit more</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>