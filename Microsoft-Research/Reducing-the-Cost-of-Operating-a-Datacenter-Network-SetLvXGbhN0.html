<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Reducing the Cost of Operating a Datacenter Network | Coder Coacher - Coaching Coders</title><meta content="Reducing the Cost of Operating a Datacenter Network - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Reducing the Cost of Operating a Datacenter Network</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SetLvXGbhN0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
hi good morning everybody thanks for
coming it's great pleasure today to have
antique Otis who's visiting us from
University of Waterloo and he's going to
tell us about a bunch of exciting stuff
is done at Waterloo and with some HP
collaborators around managing and
operating their center networks Thanks
good morning everyone today I'm going to
show you how to reduce the cost of
operating a data center network by up to
an order magnitude now because the data
center network is ten to fifteen percent
of the total cost of operating a data
center this can result in pretty
significant cost savings and the data
center has been described as the new
computer and the network plays a crucial
role in this new computer it
interconnects the compute nodes so we
need a high performing network and in
particular network performance is
critical for doing things like
dynamically allocating servers to
services so this allows you to
dynamically grow and shrink the pool
servers assigned 0 service so you can
maximize your server utilization if you
don't have enough bandwidth in the
network then you need to statically
assign enough service to your service to
handle its peak load and this results in
very underutilized servers which then
costs more because you have to buy more
servers another thing that a
high-performing network is useful for is
doing things like quickly migrating
virtual machines and this is also useful
for service level load balancing and if
the network doesn't have enough
bandwidth it can be a serious bottleneck
in the performance of big data analytic
frameworks for example in the shuffle
phase of a MapReduce job up two
terabytes of data transferred across the
network now when designing any sort of
network we need to take into account the
constraints end and goals of the target
environment so the data center has a few
new things to it the first of all is its
huge scale the network needs to be able
to interconnect hundreds of thousands of
servers and with very high bandwidth as
I just mentioned an additional lesser
lesser consider requirement of the data
center is that the network
needs to handle the addition of servers
to the data center so this is an aerial
view of Microsoft's data center in
Dublin and these these white units on
the roof are modular data center units
and they probably each contain about a
thousand to two thousand servers you can
see that the roof is about a third of
the way built out and so as we build it
out there's going to be significantly
more servers added to this data center
so they're in there they're in modular
oh that's good coach so there's also a
traditional raised floor data center
within but so this is sort of a you know
if they're trying out this different
architecture right so if we're designing
our network to handle this sort of
growth we need to the network needs to
be able to have incremental
expandability and if we don't account
for the fact that our data center will
grow and change over time then our
network could end up being a mess after
several years so what we need is a
flexible data center network no I'm not
sure this well I don't think this is
much right so if we consider the
traditional data center network topology
such as a cloth flattened butterfly
hyperx hbu cave and so on these
typologies are all highly regular
structures so they're incompatible with
each other and they're incompatible with
legacy data centers let me just
illustrate this with a simple example so
this is the the standard fat tree so
here each switch in the network has four
ports and then these yellow rectangles
represent racks of servers so let's
suppose things are going well so over
time we need to add a couple more acts
of service to this data center the
question here becomes well how do we
rewire this network to support these
additional servers if we want to
maintain the factory topology we have to
replace every single switch in this
network that's not cost-effective and it
could result in a significant down time
which is unacceptable that
environment so we need flexible data
center networks additionally data center
networks are hard to manage ya mean
publicus is visible hey if you if you
have threat Corday's possible to build
out a network that has experienced
airports in its we can still add more
tours and aggregation layers that
actually replacing the core right so
that that's certainly true that the I
mean I've chose this example to be very
bad right this is the worst case however
even if we did sort of plan ahead for
the growth we're spending a lot of money
up front that we don't necessarily need
to and Ethernet speeds are increasing
faster than Moore's law so we can sort
of ride the cost curve down if we can
delay deploying additional capacity
until we need it okay so besides being
inflexible data center networks are hard
to manage is primarily because of their
huge scale they can consist of up to
tens of thousands of network elements
and additionally they have a
multiplicity of end-to-end paths so this
is useful for providing high bisection
bandwidth and high availability however
it makes traffic management quite
challenging because traditional
networking protocols were not built to
handle this multiplicity of n dead paths
ok so just let me summarize the
challenges briefly I've identified these
two challenges first is designing a new
upgraded or expanded data center network
is a hard problem and the second is then
managing this network is also
challenging and that's mostly because
these networks are very different than
enterprise networks so to resolve these
challenges I have made the following
contributions in my dissertation first I
developed theory to understand
heterogeneous high performance networks
by heterogeneous I mean that the network
can support switches with different
numbers of ports and different link
rates second I've developed to
optimization frameworks to design these
heterogeneous type of data center
networks the first is I the framework
designs heterogeneous cloths networks
and I'll get into exactly what those are
in a minute and the second designs
completely unstructured day
the center network so these are
arbitrary mesh networks and I'll get
into why we want those and then the
third contribution I I proposed right I
made it is scalable flow-based
networking in the data center so this
allows you to manage the individual
flows in your network using software
running on a commodity PC in the
application we're going to use with this
is to do traffic management in the data
center alright so to describe my first
two contributions to you I'm going to
describe these two optimization
frameworks that I developed the first I
call leg up and as I mentioned this
developed this designs heterogeneous
cloths networks the second I call rewire
and this designs unstructured data
center networks so both of these are
optimization frameworks for data center
network design and as input they take in
a budget which is the max amount of
money you want to spend on the network
if you have an existing topology they
can take that in so they can perform an
upgrade and expansion of that
additionally they take in the list of
switches so this needs to have the
specifications and prices the switch is
available on the market if you include
modular switches you need include also
the details of line cards and then
optionally they can take in a data
center model and this is a physical
model of your data center so you can
describe the rack by rat configuration
of your data center and they these
frameworks will take this into account
to do things like estimate the cost of
links so like a link that attaches to
two adjacent racks should be cheaper
than a link that crosses the length of
the data center so my frameworks take
this input and they perform an
optimization algorithm to find some
output topology now when we started
thinking about this problem we started
with this hypothesis and that is that by
allowing switch heterogeneity we be able
to reduce costs and the reason we made
this hypothesis because of the
regularity and the the rigidity of
existing constructions don't allow any
heterogeneity in your switches so by
allowing this we believe we can come up
with more flexible networks that we can
then expand
upgrade more cost-effectively so when
deciding the output topology for an
optimization framework for the first
pass we decided to constrain this output
to a sort of a cloth like network and I
call this the heterogeneous cloths
network and I began by heterogeneity I
mean that we can use switches with
different numbers of ports so that is
their rate of seeds can be different and
we can have different link rates these
switches with reduced costs are you
taking into account in any way that
additional or the higher cost of
managing that in a new set of switches
right so I am NOT taking that into
account I'm taking into account the
additional costs to build it but not to
manage it yeah DCM is has Virginia's
device how you consider like if you buy
a lot of device of the same type given
by much lower cost them by many
different kind of devices relatively
where you need to customize each device
based
needs is it's very difficult to ask
mobile vendors to customize a something
right so I have considered that I don't
explicitly include it in my in my model
right now however you could pretty
easily extend the framework to include
that where you have sort of buy it bulk
discounts right but I don't consider for
now okay so when I'm describing this I'm
going to go into describe the theory of
heterogeneous cloths networks now and
then I'll show you how to actually build
these things with an algorithm so while
I'm describing this I want you to assume
that we can route on these networks and
that we can do load balancing perfectly
across them and then later on I'll show
you how to get rid of these two
assumptions all right so to review the
cloths network it looks like this and
this is what i call a physical
realization of the class network because
it represents the physical
interconnection between switches but it
turns out there's a more compact way we
can represent this and that's by
collapsing each of these bipartite these
complete bipartite subgraphs into a
single edge so we have something that
looks like this I call this the logical
topology so here each logical edge
represents this complete bipartite sub
graph and the number on it indicates the
capacity of the underlying physical
network so it turns out for a class
network the logical topology is always a
tree but I started thinking about this
and I was thought well why can't we
separate this and deploy the capacity
across a forest of trees and so the
problem becomes now if we can split the
capacity like this the problem of
designing a heterogenous Claus network
is we're given a set of top-of-rack
switches and each rack has a demand
which this is the uplink rate that it
would like so here you can think of this
rack wants for gigabits of uplink and
over there they want 64 gigabytes of
uplink and this rack of service should
be able to get this uplink rate
regardless of the traffic matron
so this is also called the hose model
now it turns out that for this this set
of top of racks which is there's three
optimal logical topologies and by
optimal I mean that these typologies use
the minimum amount of link capacity
necessary sufficient and necessary to
serve these demands here all right so
optimality at least in the theory is
only on link capacity now there's so for
any given set of top Iraq switches there
can be a bunch of different out logical
topologies so our first result is how to
construct all optimal logical topologies
given a set of top-of-rack switches then
once we have these logical topologies we
need to know how to actually translate
them back to a physical Network and so
that's our second result is that given a
logical topology we find all physical
realizations of it so for this logical
topology here's one physical realization
of it question these two slides back so
I'm trying to understand that there's
clicking on the right is very
this irregular he might say but you're
sending the guy 164 sending eight one
direction it fixes the other is it is
showing that x1 next to a different axis
which is what optimization is so x1 and
x2 are just these logical nodes that
represent a physical set of switches so
there can be different ways to represent
these with physical switches the I the
intuition here is that these nodes need
to send sixty four units of traffic and
be able to send that anywhere in the
network so but because these nodes only
need four units of traffic then we don't
necessarily have to send all 64 of that
connecting to these guys if we if we get
for connecting to those guys and that's
enough to to serve all the different
traffic matrices possible so the sum of
uplink bandwidth is the same in all
these logical constructions it's just we
sort of distributed it differently no
I'm not assuming the traffic demand so
I'm assuming this this hose model which
is actually a polyhedron of traffic
matrices so it's an infinite set of
traffic matrices and it's all the
traffic matrices that are allowed under
this these rates here so as long as this
rack never sends nor receives more than
its rate and that's a valid traffic
matrix yeah so we're sort of optimizing
for the worst case traffic matrix
possible
okay so again this is the physical
realization of this and here's physical
realizations of the other topologies so
you can see we just essentially spread
the the capacity out across a certain
number of physical switches and then the
link rates are determined by the the
logical edges okay so to summarize the
first result is how to construct all
logical topologies the second is then
how to translate a logical topology
until it's different physical
realizations so together this give us a
theorem that characterizes these
heterogeneous class networks and as far
as I'm aware this is the first optimal
topology construction that allows
heterogeneous switches now this theory
is nice it's very elegant however it
doesn't tell us how to actually build
these networks and practice because the
metric for a good topology under the
theory is that it uses the minimal
amount of link capacity but in practice
we need to take and do other accounts
other things into account such as the
actual cost of the devices so in
practice a data center network should
maximize performance while minimizing
costs should also be realizable in the
target data center so this means that
for instance if we have in order to
realize that apology if we have so many
switches that it's going to draw too
much power that doesn't do us any good
if we can't actually build that and then
finally if we're talking about up during
you're expanding a data center network
you know our algorithm should be able to
incorporate the existing network
equipment in into the network if it
makes sense to do so never talk inside
to do the proof of the theorem money
like what why are you not running into a
computational hardness associate
being packing or such ballers okay yeah
that's good question whyatt why don't we
have to worry about bin packing here and
that's again because I'm assuming the
load balancing is perfect so we can
split flows so if you have splittable
flows you don't run into the this bin
packing problem you can just solve using
linear programming actually we can solve
it analytically so we've solved it
you're okay are we free to speak those
things I any wishes so by yeah by
arbitrarily that's why I'm assuming this
for the theories because it makes it
very much easier I don't know any switch
that can do that which is why we
wouldn't do that in practice and that's
why at the end of my talk I'm going to
talk about traffic engineering like
doing flow scheduling so we can maximize
throughput even with this type of
topology yeah I'm also wondering if
you're taking failures into account so a
lot of actual data sending our works are
built so that the failure of anyone's
which has a minimal impact on the
network
ol but she seems like if you have x1 and
x2 and x2 is handling ninety percent of
traffic and x1 is only handling 10 and
failure next to with positive
proportionate destruction right so this
far as failures the way we handle that
is that in the optimization algorithm so
there's different ways you can formulate
it but I've formulated as a constraint
in the optimization problem that says
each rack must have this money this much
capacity if there's a certain number of
link cuts yeah Laurie don't you also
have flexibility as one of the tools
maybe the vet rack in the week will be a
treatment right so in um when I
originally did leg up I did in fact have
I had the optimization criteria was to
maximize bisection bandwidth plus
flexibility where we had some notion of
flexibility but it turns out I think
it's really hard to capture flexibility
in a nice simple metric so that's why
for my second step and rewire I I
abandon that because I think it I
couldn't find a good formulation of that
and I'd be happy to know if you have a
good one yeah that's apology looks like
well I got a symmetric
if you make any chance to your pathology
can you still maintain afternoon
or you have to greyrock optimization
right so the changes I mean so I want to
emphasize that this is just the theory
and you know so when we actually design
these with the optimization algorithm it
does try if you need to make changes it
tries to minimize the cost of making
those changes like it tries to we take
an account the cost of rewiring things
and so on so as a human you know ideally
my goal is to not have to think about
that and let the algorithm think about
that for you then it's the key to
approach is flexibility and making
changes from behind you can accommodate
just come up with
can you to show something that's
actually make changes to it yeah so you
can flexibly I mean right so these are
more flexible in the sense that you have
with the cloths you have one
configuration right so for these I've
actually just shown one way of
physically I mean so for this just these
these set up top arrives we had three
different logical topologies and
actually there's just I only showed you
these three physical realizations but
there's a bunch of different ways so
because of this additional flexibility
like say you need to make one change
here you have a lot of topology options
that are still like optimal under my
link capacity constraint where's what
the class you just have one arrangement
so that's why it's so much more flexible
because there's this multiplicity of
topologies that are also optimal and I
want to emphasize that the algorithm
doesn't require the optimality of the
topology this is just what it aims to do
you have any example to show okay given
this traffic matrix you can Klaus has
this technology to and those cross and
Regina's cross can come up desist
apology to accommodate this traffic
Annette
and in the future if this crappy you
meant change is it cheaper to
accommodate the new target
primo and he
so I guess I'll have a specific example
for you right off the top of my head I
don't think it'd be hard to find one
though and I'll show you our experiments
of using this to upgrade the University
of Waterloo's data center and we do
actually find significantly lower cost
solutions okay so like I said that's a
nice theory but we now need an
optimization algorithm to actually
design these sort of networks so I'm
just going to briefly go over the leg
off algorithm so what it does is it
performs a branch-and-bound search of
the solution space normally with branch
and bound you can guarantee the
optimality however we can't quite
guarantee that because we do have to use
some heuristics to maps which is two
racks and we do this to minimize the
length of cabling used and then you know
this algorithm does scale reasonably
well in the worst case it is exponential
in the number of top of racks and the
number of switch types however you know
in my experiments I didn't find this
this behavior for a 760 server data
center found it but took about five is
ten minutes to run the algorithm and
this is for the hardest input I could
find if you give it an easy input for
instance if the top Arak switches are
homogeneous it only takes a couple
seconds to run for a data center 10
times that large it takes a couple days
but my implementation only runs on a
single core and it would be easy to
parallel lies or distribute this yeah
she's got a contractor written strange
behavior
the optimization
yeah so I did so you're saying in the
formulation the problem the cost right
so I didn't do that just to avoid the
additional complexity of that however I
think like given these pretty good run
times I think it would probably be
possible to do that but I haven't
explored it okay so to summarize leg up
I developed this theory of heterogeneous
cloths networks implemented the leg up
design algorithm and then I evaluated it
by applying it to our data center and
I'll show you more results later after i
describe rewire but for now i'll spoil
some results and say that for our data
center cuts the cost of an upgrade in
half versus a fat tree so let me move on
to rewire and now rewire we're going to
do away with the the structure of the
network entirely and design entirely
unstructured networks and I'm really
motivated by this question of well all
right so it seemed like the structure
was hurting us in a class network and by
allowing some amount of flexibility we
could do a lot better so if we just use
an arbitrary mesh how much better could
we actually do the problem here is that
now we have a really hard network design
problem the heterogeneous cloths
networks were still somewhat constrained
so we could sort of iterate through all
the different possibilities and evaluate
them but now we have a completely
arbitrary mesh so there's many many many
different networks for any given set of
top Iraq switches so to solve this i use
the simulated annealing algorithm and
the goal of this algorithm is to
maximize performance and by performance
I mean bisection bandwidth minus the
diameter the network so if you don't
know what bisection bandwidth is right
now i'll explain exactly what that is in
a minute and the diameter is the worst
case shortest path between any type of
top of racks and i'm using diameter
here's sort of a proxy for latency
because latency is actually very hard to
us to meet you need to know queuing
delays and so on so that's why I
diameter is just a proxy for that
these are these are in different units
so what I do is a scale them to be
between zero and one so diameter you can
think of the best diameter in a network
is one is one hot behind between all the
notes the worst is a path so you can
scale out to be between zero one and
then you in then bisection bandwidth I
normalize as well so then you can wait
each of these by however much you want
and it does take some playing with the
way it's to get what you want but
because you can you can tweak it okk we
do to reduce an arbitrary scale factor
yes sir ok so then rewire maximizes the
performance subject to the same
constraints as leg-up subject to the
budget your data center model if you
give it one but now we have no topology
restrictions and here the cost we take
into account are the cost of any new
cables that you may buy the cost to
install or move cables and then the cost
of any new switches you may add to the
data center so rewire performs a
standard simulated annealing so at each
iteration it computes the performance of
a candidate solution and then if that
solutions accepted it computes the next
neighbor to consider and so on and
repeats this until it's converged now we
do have some heuristics for deciding the
next neighbor to consider but I don't
have time to cover that because I want
to talk about how to compute the
performance of a network and it turns
out there's actually no known polynomial
time algorithm to find the bisection
bandwidth of an arbitrary network so the
bisection bandwidth is the minimum
bandwidth across any cut in the network
and we can find the bandwidth of a
single cut pretty easily so let me need
to note the servers on one half of this
cut by us the others by s prime then the
band width of this cut is equal to the
sum of link capacity crossing that cut /
the minimum of the sum of server rates
and s and the sum of server rates in s
prime so for this specific example we
have four links crossing the cut let's
just assume their unit capacity and then
we divide by the min of s has two racks
of servers
say there's 40 servers per rack and s
prime has six servers of 40 or sorry six
racks of 40 servers so here the the
bandwidth of this single cut is for / 80
then the bisection bandwidth is the
minimum bandwidth overall cups so on a
tree like network it's easy to compute
this because we can simply enumerate
over all the cuts there's only 0 of n of
them and we can compute that equation I
showed on the previous slide and we have
a polynomial time algorithm yeah your
initiative bisection bandwidth normally
you would just computer is the ice cream
to bandwidth traversing of hot but
you're dividing by the number of servers
so it seems like it's actually more like
fair share bandwidth per server or
something be right so the reason I'm
dividing by the number of servers is
because we can have these heterogeneous
rates and so we need to take you know if
one if a one half if here all these
servers were super high capacity all had
10 gig links and these had one gig then
it would it's not just fair to divide
they're not / anything all right so we
need to normalize it by the amount of
capacity we actually expect to cross
that cut and so the reason why we divide
by the min is the art so that lets just
assume homogeneous rates for now and
here there's two racks of servers here
they're six well these six racks can't
push so there's six units of traffic
they can't push six units of traffic
across here because these guys can only
receive two units so that's why we
divide by the min these guys can
likewise only push to out because in my
section that's ready yeah I think I
educated yeah probably should call it
cut bandwidth or something so people
have a notion of what that is right okay
so this is easy to compute on a tree
however on an arbitrary graph there can
be exponentially many cuts therefore it
would take exponential time to compute
this in the worst case so if we stop to
think about it the traditional min or
max flow min cut theorem allows us to
find the min cut in a network by solving
this flow problem
so we'd like to be able to do the same
sort of thing here but the problem is we
don't have a single flow problem we
actually have a multi commodity flow
problem because each server can be the
source and the sink of flows and in
general the main cut max flow theorem
does not hold for multi commodity flow
problems however we've shown that in our
special case there is a min cut max flow
theorem and what we've shown here is
that our hose traffic model so
throughput in the host traffic model is
equivalent to bisection bandwidth so
therefore if we can compute the max
throughput in this traffic model then we
found the bisection bandwidth and turns
out some guys at Bell Labs had done this
a few years ago so combining these two
results we get a polynomial time
algorithm to compute the bisection
bandwidth and then we've just run the
simulated annealing procedure as normal
computing this at each iteration to find
the performance of the candidate
solution so I'm going to move on to my
evaluation and here the question we
really want to answer is well how much
performance do we gain because this
additional heterogeneity in the network
so to evaluate this I tested several
different scenarios first I tested
upgrading the water lose data center
network then I tried iteratively
expanding our network so this is I had a
certain number of servers at each
iteration and use the output from one is
input into the next iteration then I use
these algorithms to design brand new or
green field data center networks and
then also ask them to design a new data
center network and then iteratively
expand it so this is the cost model I
used these are the cost of switches the
cost of links for switches is very hard
to you know get good estimates on street
prices switches so these are the best i
could find just by googling around i
wouldn't actually stand by these exact
specific values however i think their
relative differences are meaningful
these are the prices we use for links to
simplify things like categories to
linksys short medium or long and then
charge accordingly according to the
length and according to the rate and
then I charge a different cost to
actually install the car the link and
because we charge this amount if you're
going to move a link we charge that same
amount so we don't recharge you for the
link itself okay so to compare my
algorithms against the state of the art
these are the approaches I used so I
compared against the generalized fat
tree is using the most general
definition of a factory possible and
here I don't explicitly construct the
fat tree instead I just bound the best
case performance so you're given a
budget and I bound the best case
performing fat true you can you can
build using that budget second a test
against a greedy algorithm this
algorithm just finds the link that
improves the performance the most ads it
and then repeats until it's used up its
entire budget or all ports in the
network or full and then the third thing
I test against is a random graph is
because a group at UIUC has proposed
using random graphs as data center
networks and this is due to the fact
that random graphs tend to have really
nice properties okay so this is what our
data center network looks like we have
19 edge switches each of these edge
switches connects to 40 servers so we
have told 760 servers our edge switches
are heterogeneous already and all our
aggregation switches are the same model
there are all these HP 5406 switches
however this is a modular switch and
they do have different line cards so
they're heterogeneous as well and this
is our actual topology so you can see
between their top of rocks and
aggregation we only have a single link
there's no redundancy there additionally
our data center handles error quite
poorly there's no isolation between the
hot and cold aisles so to model the fact
the fact that my algorithms can take
thermo constraints into account I simply
allow you to add more equipment to into
the racks closest to the chiller so here
this rack you can take I can't remember
the exact it's 20 kilowatts of equipment
whereas the reaction the other end can't
take nearly as much because they don't
get as much cold air from the chiller I
want to emphasize this very much is the
first pass approach and if your data
center was severely thermally
constrained you'd want to do something
more sophisticated than this okay so let
me show the results of expand in the
Waterloo data center now so this is our
original network and here I'm showing
the normalized bisection bandwidth and
bright below it i'm showing the diameter
and then here I'll show the number of
servers we've added so you can see our
data center network right now has a
normalized bisection bandwidth of just
over point 01 so this means that it's
over subscribed by a factor of almost
100 so in the first iteration I added
160 servers and then asked each of these
algorithms to find and upgrade given a
fixed budget all right so all these
items have the same budget and across
iterations the budget is kept the same
so you can see that the fat tree was not
able to increase the bisection bandwidth
of the network while the other
approaches were however the factory is
able to attach the new service to the
network without decreasing the bandwidth
of the network you see here that the
greedy approach and rewire perform the
same they both significantly increase
the bisection bandwidth and actually
decrease the diameter by one and leg up
just increases the lesser spam yeah
mm-hmm for your work hi they were going
through an upgrade complicate senators
we just sort of looking into it not
great
um I'm wondering hearing is that so I
did ask them about an upgrade plan yeah
I mean it's hard to compare against that
I didn't explicitly like measure what
they would have done and they told me
you know essentially our network doesn't
need this high have high performance so
I mean I you know I admit it's sort of
i'm applying it to the network that
doesn't need this kind of bandwidth like
for instance we only have one link
between our top of rock and irrigation
switches so what they told me is that
these the things found by leg-up are
probably not what they would have
thought of but that they seemed like
okay this is interesting solution we
could probably implement this they had
not considered like the network's found
by rewire or the greedy approach because
these are arbitrary meshes and they
don't want to do that because it would
make it harder for them to manage their
network but you know I'm trying to to
push the frontier what's possible here
so that's why I think these are still
interesting okay so as we keep
iteratively expanding the network adding
more and more servers you can see that
the performance gap between rewire and
the other approaches gross so here after
we've added 480 servers rewires network
has four times more bisection bandwidth
than the fat tree it does slightly go
down in the next iteration and that's
because we're adding more service
routing more demand to the network
however is able to decrease the diameter
down to two and because we have this
multi objective function it due to my
waiting it preferred decrease in the
diameter is compared to ink the increase
in bisection bandwidth and you see that
the greedy algorithm underperforms over
time so initially it did extremely well
however it wasn't able to increase the
performance of the network past this
point and that's likely because it made
a poor decision here and over time is in
able to then it used you know all the
good ports and over time it doesn't
change where things are rewired excuse
me where things are wired so it locked
itself into this sort of
narrow solution okay so then the next
scenario is just asking these algorithms
to build a brand new or green field data
center so wholly runs were looking at
here thank you said the greedy I've
really made a bad decision did you do
that repeatedly or did you just from
onyx all right so I did do it repeatedly
I'm not showing error bars here but in
the inn across all the experiments that
seemed to do the same thing and that's
it is actually a deterministic algorithm
so yeah so there's no you didn't curb
the system to know the error bars didn't
represent the results of having perturb
the system to give reading opportunity
to to behave differently
right I mean right so that's because
we're using this static and but we're
using our existing data center right so
we run a deterministic algorithm on out
we get the same result right sorry so
there's no right there's no error bars
on this uh so the error bars would be on
rewire because it's simulated annealing
algorithm however so for that we ran it
enough times that they're like I didn't
actually put them on this chart but
they're you know very small it seems
like you should do something it really
is subject to fall into two pits then
you may have shown us a case where
greedy is trapped in a pit let me
it's may be interesting to ask what
would happen if I perturb you know the
number of servers added a little bit in
a way or just advice provided some
source of intricate but they would let
you explore more of the space with
greedy as well as well because we wired
as you point out already s were endless
in it it causes it to explore more of
the space just to understand whether
whether the greedy because otherwise the
greedy they can comparison to previous
or robbery but it okay I mean that's a
fair criticism that I did not yeah I
didn't necessarily test enough scenarios
with greedy in the paper there's more
cases where we tested and we've we sent
we seem to find the same things that
greedy initially does well and then over
time doesn't do very well okay so for
the next the nessus interio is designing
a brand new data center so for this we
asked these algorithms to connect a
network with 1920 servers so for this
I'm assuming that each top of racks
which has 48 gigabit ports and 24 of
these ports connect down to servers and
24 of them are open and are free to
build the switching fabric on top of
using these open ports so here again I'm
showing the same type of thing we have
bisection bandwidth on the vertical axis
the diameter here and then the different
approaches so this is the for a budget
of using a quite small budget of 125
dollars per rack rewire is the only
algorithm that's able to build a
connected network and so I want to
emphasize this this budget does not
include the cost of the top of rocks
which this is only for cabling
aggregation and core switches so the
reason rewire is able to build a
connected network here but no one else
is is that the fat tree in leg up have
to spend money to buy irrigation and
core switches so their networks and
currently at this lower budget costs
more the random Network has some
randomness in it so it's not able to
complete to build a connected network so
that's why rewires the only thing that
has connected to apolog-- here as we
increase the budget you can see that
rewires starts to significantly
outperform the other
approaches for all budgets except for a
thousand dollars per rack where the
random Network actually has more
bisection bandwidth and this is again
this is the expected bisection bandwidth
for the random Network so I'm not
actually explicitly building these
random networks i'm using a bound that's
proven by some theoreticians on the
amount of bisection mammals that have so
you see that even leg up significantly
outperforms the fat tree designing brand
new network so here it's network has
twice as much bisection bandwidth with
thousand dollars per rack budget rewire
really outperforms the fat tree so with
a five hundred dollar per rack budget
has sixty eight times more bisection
bandwidth will increase the the budget
to a thousand dollars per oggi it has
six times more bisection bandwidth and
in this case where the random Network
actually beats three wire in terms of
bisection bandwidth again this is
because I use this multi objective
function so rewire preferred decreasing
the diameter by one rather than
increasing the bisection bandwidth could
have found the random solution yes yeah
it's just didn't want because you said
you objective function differently right
and I mean also you could seed rewire
with a random Network and then ask it to
improve on that and it does it can
improve on that I've ran some
experiments the performance gap though
does seem to grow or shrink as you use a
higher switch Rae disease is the higher
the radix of the switch the better the
random Network does try to optimize the
thing that you're showing the y-axis
here than my sexual violence it's it's
trainer it's trying to optimize
bisection bandwidth minus dynamism okay
yeah so you're you're being a little
unfair you were an algorithm here
because you're showing its performance
on a metric that you didn't tell top of
my score exactly i mean it so trying to
show both of those but it's hard to
visualize them so you know that's why so
here yes it does have a lower diameter
yeah okay now people have brought this
up already so the problem we have with
moving towards these heterogeneous
networks is management and in particular
there's a few things that are hard on on
arbitrary network so routing is
difficult on an unstructured network if
you're we're talking about a
heterogeneous cloths network then we
could make minor modifications to two
architectures such as Portland and vl2
and be able to route on a heterogeneous
cloth network this is because
fundamentally these networks are still
tree-like so you just go up to the least
common ancestor and back down however on
an unstructured network it's quite a bit
harder there is 11 architecture that
allows you to to route on unstructured
networks this is called Spain it's by a
group at HP Labs in the way it
essentially works as a partitions the
network into a bunch of vlans and then
just source routing across these vlans
for load balancing one solution is we
could do we could schedule flows and I
have two solutions for that that I'll
talk about next and then the other
solution again as Spain does have load
balancing built in where we do this
source routing and the source also do
load balancing and then another option
is to use multi path
gcp which has been shown that it's able
to extract the full bisection bandwidth
from random networks so we'd expect it
to be able to get extracted full dosage
man with from our unstructured networks
as well now it is unclear how much it
would actually cost to build and over
the long term manage these arbitrary
networks however I do believe that the
performance per dollar in building the
network compensates for this all right
so I'm going to move on to my third
contribution now which is a framework to
perform scalable flow-based networking
in the data center and I'm going to
apply this to managing flows and the
reason we want to do this sort of flow
scheduling is for one maximizing
throughput on networks like like I just
show these unstructured networks but
additionally even on highly regular
topologies like a fat tree we can have
this situation where flows collide on a
bottleneck link and if we just moved one
of these flows over a bit then we could
actually double the throughput of both
of these flows and it's been shown by a
group at UC San Diego that if you
perform flow scheduling in the data
center for at least some workloads you
can get up to one hundred and thirteen
percent more aggregate throughput
however their approach it depends on
open flow an open flow is not scalable
which we'll get into for a few in a
minute and the reason it's not and so
therefore their approach is not scalable
as well so i have two traffic management
frameworks to solve this problem the
first we call mahout and mahout uses end
hosts to classify elephant flows which
elephant flows to us our long-lived
high-throughput flows so once the end
hose classifieds the elephant flows it's
set up at a controller and the
controller dynamically schedules just
the elephant flows to increase the
throughput and then our second solution
is called devo flow I worked on this
joint with a bunch of people at HP Labs
and our goal here is to actually provide
scalable software-defined networking in
the data center so software-defined
networking allows us to write code that
runs on a commodity
server and manages the individual flows
in our network so this enables a type of
programmable network because we can then
just write the software to manage the
flows and this is currently implemented
by the OpenFlow framework which has been
deployed many institutions across the
world you can buy open flow switches
from several vendors like NEC and HP and
I think that open flow is great I think
it's a great concept but its original
design imposes excessive overheads now
to see why this is let me explain to you
how open flow works so this is what a
traditional switch looks like where we
have the data plane and the control
plane in the same box so the data plane
just forwards packets and the control
plane exchanges reach ability
information and then builds routing
tables based on that however open flow
separates these two so it looks
something like this where we have a
logically centralized control plane add
a central controller and then open flow
switches are very dumb switches the just
forward packets so anytime a packet
arrives at this open flow switch that it
doesn't have a forwarding table entry
for it has to forward it to the central
controller the controller decides how to
route that flow and then it inserts
forwarding table entries and all the
switches along the path flow so the
reason we want open flow in the data
center is because it enables some pretty
innovative network management solutions
here's a partial list so a few of the
things we can do with open flow are
things like consistently enforce
security policy across our network it
can be used to implement data center
network architectures such as vl2 in
Portland it can be used to build
commodity excuse me load balancers from
commodity switches and relevant to me it
can be used to do flow scheduling to
maximize throughput or can also schedule
flows to build energy proportional
networks so that this works by
scheduling the flows on the minimum
number of links needed and turning off
all the unnecessary equipment so this is
a great that open flow can do all these
things but
unfortunately it's not perfect and the
reason why it's on purpose because it
has the scaling problems so implementing
any of these solutions in a you know
mid-sized data center will be quite
challenging so our contributions with
this work our first we characterize the
overheads of implementing open flow in
hardware and in particular we found that
the overheads open flow there's
obviously a bottleneck at the central
controller since all flow setups need to
go through the central controller that's
you know creates an obvious bottleneck
but we found that that's not the real
bottleneck the real problem is out the
switches themselves we found that open
flow is very hard to implement in a
high-performance way in the switching
hardware so to alleviate this we propose
devo flow which is our framework for
cost-effective scalable flow management
and then we evaluate diva flow by up by
using it to perform data center flow
scheduling so I don't have enough time
today to to go over the overheads of
open flow so I'm just going to skip to
this and then go into our evaluation
yeah I wasn't writers dude II so you
said I would have expected the
problem to me that your every single
clothes that it gives their why I work
why has to go through a single
centralized point but he said it wasn't
that it was that there it's hard to put
open throwing to anything else wishes
but I'm not sure which part you're such
a mission statement consumes like
Oakland so is running in decentralized
minds not individuals which is so open
fo also has to run it switches let me go
switch to my backup slides here so you
know I didn't really show the full
picture of how this this architecture
looks like so we have the asic here
which does this is you know hardware for
specialized forwarding packets but also
switch has a cpu and that's used for
management functions and so anytime we
do this flow set up through the ASIC it
has to go through this CPU and the
reason it goes through the CPU is
because it needs to perform ssl aunty
and then also it needs to perform TCP
between the switch and the controller so
this cpu in switches today is pretty
wimpy it can't handle this low of
setting up all the flows and you know so
we did simulations that looked up ok
well obviously let's just put a bigger
CP in there and maybe that will work we
found this cpu would need two orders it
would need to be tours or magnitude
faster than what's currently implemented
in open in HP's hardware at least we're
not sure about other manufacturers but
for HP it needs to be two minute to or
is it magnitude faster yeah so
essentially thing is it's a server you
can be whatever you want it
you're saying these eight piece is
switch control plane CPUs are so slow
that even though there's 200 times too
many of them they have bottle back yes
unless you grow you know I mean for
hundreds of thousands servers then the
the centralized controller could be a
bottleneck but you can still distribute
it to alleviate that pressure overview
how the cpu load is on the presenter
right so its CPU loads one hundred
percent and there's so we measure yet
you can measure the CPU loads one
hundred percent so we perform this
experiment where we tried to just see
really set up flows and we found that
the the 5406 which could only set up two
hundred seventy five floors per second
and then the cpu is a hundred percent
yeah say the US Association it's not
known so it's it yeah it sets that up
just once I mean it's not that dumb to
do it to do it every time but it's still
it's uh yeah very high overhead at the
control point in the switch I mean so to
show you like we can expect bursts of up
to ten thousand flows on an edge switch
in a data center so you know there's 40
times more than the the switch can
currently handle and and it does create
a lot of latency in this flow set up so
we measured the amount of time it took
to just set up this flow and it could
take two milliseconds
I mean so a part of the problem is these
are commodity switches like if you were
to go out and buy a high-end router you
know they could probably get rid of a
lot ease then the second problem is that
these switches right now aren't designed
for open flow so they're designed to do
you know normal switching stuff and then
they're adding open pull on top of it
and for the major vendors now they're
not going to let OpenFlow drive their
switch development for at least five
years or so I mean I do think it's a
great opportunity for some startup to
come in and build specialized open flow
switches just just an edge switch so
yeah in the Wisconsin measurements they
show that you got bursts of up to 10,000
volts per second okay so the way that
diva flow works is we want to find this
sweet spot between the fully distributed
control and fully centralized control so
diva flow stands for devolved open flow
and the idea is that we're going to
devolve control over most flows back to
the switches so our design goals were
this we want to keep most flows in the
data plane and that's to avoid the
latency in the overheads of setting them
up at the central controller then we
want to maintain just enough visibility
for effective flow management so we only
want to meet you know maintain
visibility over the flows that matter to
you not all the flows and then our third
goal is to actually simplify the design
and implementation of high-performance
witches because I said you know it's
difficult to do this in open flow right
now you need a fast which cpu if you
really increase the speed of CPU may
have to really check the switch itself
so we want to do away with all that by
keeping most flows in the data plane so
what we do this is through a few
different mechanisms we propose some
control mechanisms and some statistics
gathering mechanisms now one thing I
didn't talk about is that collecting the
statistics from closes also has quite
high overhead this is because openflow
offers one way to gain visibility over
your flows and that is you can ask it
for all the forwarding table counters
and say how many bytes did each flow
transfer
in the last you know since I last pull
geo and by doing this this is very high
overhead because you have to get the
statistics for every single flow out the
switch so the control mechanisms we
propose the first we call rule cloning
and the idea here is that the ASIC
itself in the hardware should clone be
able to clone a wild card rule so open
flow has two different types of
forwarding rules it has wild card rules
which can have wild cards and exact
match rules so the real cloning the way
it works is if a flow arrives and it
matches say this rule then it can
duplicate that and add a specific entry
for that flow into the exact match table
and the reason we want to do this is so
that we can gain visibility over these
flows we can pull these statistics and
just getting the visibility over the
flows that we we cloned then the other
things we propose there's some local
actions so rapid rerouting so you can
specify fallback ports and so on if if
the portfolio happens and then also we
propose the multipath extensions so this
allows you to select the output port for
a very flow according to an arbitrary
probability distribution so if a flow
arrives you can select its output port
according to some distribution such as
this and this allows you to do static
load balancing on mesh topologies then
this
so implementing this is a little bit
harder all these things so I one episode
we haven't implemented Devo flow and
hardware so all these things we talked
to HP's a sick designers and they
assured us though here this should be
relatively easy to do in the hardware so
right so this this i'm not saying perfil
a per packet seeing / flow you're not
doing for package getting these un
numbers or getting the desired
statistics becomes reading fun
because then you know like if you do
poor pocket it's easier to get the
distribution you want you start doing
poor flow then you have to estimate flow
sizes things like that right so not yeah
we're not taking that into account but I
mean the theory says if there's lots of
mice then this will give you sort of
optimal load balancing there's lots of
elephant flows then yeah you can run
into these problems that you mentioned
and I'll get into that so we're going to
actually do some flow scheduling to
schedule the elephant blows explicitly
ok so the statistics gathering
mechanisms that we propose our first of
all you can just turn on sampling most
commodities which is already have
sampling and in our experiments we found
that this does give enough visibility
over the flows another is triggers and
reports so you can set rules that if a
forwarding table rule has 42 certain
number of bytes then it will set that
flow about the central controller so
this allows you to gain visibility over
just elephant flows and then there's
additional li we can add approximate
counters which allow you to track all
the flows matching a wild Karrde rule
this one is much harder to implement in
hardware so I won't use it for my
evaluation but the idea here is that
unlike open flow we want to provide a
visibility over a subset of flows
instead of all the flows all right so i
mentioned yeah we we haven't implemented
it but we can reuse existing functional
blocks in 86 for most mechanisms so
deepa flow provides you the tools to
scale your software-defined networking
application however it still might be
quite challenging to scale it and each
application will be will be different
the idea is essentially that you need to
define some sort of notion of a
significant flow to your application and
in flow scheduling which will i'm going
to show it's easy to find the
significant flows they're just elephant
flows for other things like security it
may be more challenging to in and you
know so that's why for now i'm only
going to show how to do flow scheduling
with you go flow so the idea here is
that new flows that arrive are handled
entirely within the data plane by using
these multipath forwarding rules for new
and then the central controller uses
sampling or triggers to detect elephant
flows and the elephant flows are
dynamically scheduled by the central
controller and then this the scheduling
is done using a bin packing algorithm
okay so in our evaluation we want to
answer this question so how much
performance excuse me how much can we
lower the overheads of open flow while
still it's achieving the same
performance as a fine grain flow
scheduler so we found that if you
perform this flow scheduling you can
increase the throughput thirty-seven
percent for shuffle workload on a closed
Network fifty-five percent on a 2d
hypercube and these numbers really
depend on the workload so I also reverse
engineered a workload published by
Microsoft Research and I didn't see any
sort of performance improvement using
this workload and a big reason for this
is that I don't have the there's no sort
of bursts of traffic in this it's sort
of as flat as I reverse engineer this
from the flow inter arrival times and
the flow the distribution of flow sizes
I understand the evaluation you're
you're saying you want to reduce the
overhead on the control
while achieving the same performance
then your results are increasing
right so yeah I'm going to get into the
I just want to show you these are if you
do flow scheduling these are kind of the
increased performance you can get with
it vs vs ecmp using just static load
balancer brushes random easy and be
social brute so this is fine grained or
diva flow versus randomized with ecmp
and then here's the here's the results
on the overheads so this this the
vertical axis is showing us the number
of packets per second to the central
controller this is simulations where I
we reverse excuse me where we simulated
open flow based on our measurements of
our real switch so you see if we use
open flow stats pulling based mechanism
to collect statistics then we have about
7,700 packets per second going to the
controller if we used evo flow-based
mechanisms such as sampling or these
thresholds to gain visibility over the
flows then we can reduce the the number
of flows to the excuse me the number of
packets to the controller by one to two
orders of magnitude and again this is
just because we're only gaining
visibility over the elephant flows here
at least with the thresholds this is
only the elephant flows and the sampling
we're collecting samples for every
single packet but it's still lower than
collecting the statistics on every
single flow so there's no no cost and
performance doesn't say for the same
performance this is the decrease in
overheads all right so this is showing
the number of flow table entries at the
average edge switch so here with open
flow we have over 900 flow table entries
with diva flow we can reduce that 75 to
150 times and this is because most flows
are routed using a one single multipath
forwarding rule and then we only need to
add specific exact match entries for
elephant flows
this wasn't right right so simulated it
so so I proof I basically simulated a
lot of different scenarios and found so
sort of did like a binary search in the
performance to get on the same that is
the before it says aggregate throughput
so here on this workload we're getting
the same way or get throughput using the
fine grain scheduler as devo flow and
these are the overcoats okay so you did
you're assuming a traffic pattern is
some time right and you're simulating
what what decisions were made as to
which link gets which one it was
figuring out how much accurate this look
was there okay so that's been you're not
seeing any like TCP effects on packable
know we're doing fluid la bolsa
relations okay because any TCP sometimes
has good performance that just dropping
to my sweet packet slinky 110
right right so yeah we're not yeah they
sort of flow level simulations Nestle is
this telephone similar to hire are in
the same sense we always schedule for
the big flows except a heroine in that
that's the ash which was there for this
so panera they this is essentially
hadera here because they're using open
flow statistics blowing mechanisms to
gain visibility over the elephant closed
ok so this is what hadera does and then
diva flow is using are more efficient
statistics gathering mechanisms to only
look at the flows that matter the
elephant flows ok so to summarize diva
flow we first characterize the overheads
of open flow then we propose diva flow
to give you the tools you need to reduce
the reliance on the control plane of
your software-defined networking
application and then we showed that at
least for one application which is flow
scheduling it can reduce overheads by
one to two orders of magnitude all right
so I want to just briefly summarize the
cost savings that are possible with my
you know because of my results here and
that you know the network is five to
fifteen percent of and this is just
network equipment is five to fifteen
percent of the total cost of ownership
of the data center so leg-up can
basically cut the cost your network in
half rewire can cut it by as much as an
order magnitude so then you can
significantly save money on your total
cost of ownership your data center with
these two approaches and then server
utilization is often low because of the
network limitations so if you can
extract more bisection bandwidth from
your network then you might need to be
able to deploy fewer servers since the
servers are the majority of the cost of
your data center you may be able to
achieve some cost savings there as well
so to go over my future work I want to
do quite a few things so first of all
I'd like to work on a few a bit more
theory type results and I think it's
really interesting idea to use expander
graphs is a data center network so
expander graphs if you're not familiar
their graphs that are essentially
rapidly mixing and in the the rewire
work we compared we Swift switched out
our objective function instead of saying
ok maximize bisection bandwidth we asked
them to maximize the spectral gap of a
graph which is the notion of a good
expander and we found that these
to actually perform extremely well and
so I'm really interested I think there
is a connection between the expansion
properties of a graph and the bisection
bandwidth something interested in floor
explain that further there's quite a bit
assistance work I want to do I think
that would be really cool to actually go
out and build an architecture
specifically designed for unstructured
data center networks like I said HP has
one but I don't think it's the last word
on that I think there's a lot of
problems still open on managing you know
multiple data centers so I'm interested
in inter data center networks some other
thing that I have an ongoing project on
is deadline aware big data analytics
adding deadlines to these real excuse me
to big data type queries another thing
I'm interested in is green networking
systems we have a submitted paper that's
on reducing carbon emissions of Internet
scale services and you know so if you
look if you think about my work so far
I've worked on the data center
infrastructure part of things and I sort
of want to move up the stack so I want
to move up to work on cloud computing
big data analytics but more on top of
that I want to work on applications on
top of big data analytics the top of all
these things so on a work on things like
how can we upgrade our cities how can we
apply these same sort of analytic and
theoretical techniques to design
transportation systems to jointly design
smart grids transportation systems
design and it also city services like
police and fire services and then also
won one thing I'd like to apply these
things to is building zero energy legacy
buildings so people have ways right now
of building you know buildings that use
no energy whatsoever they're completely
carbon neutral I'd like to develop
low-cost ways to retrofit existing
buildings to be the same way and I think
this is really a grand challenge because
in the next 40 years we can expect to 24
billion people to move into cities so
our cities are going to grow
tremendously the number of cars on the
road will double if we don't have smart
people thinking of way
to solve these problems then we're going
to have over congested roads and too
many people moving into crummy
neighborhoods with bad infrastructure
one project I've already worked on this
is looking at the return on investment
for taxi companies transitioning to
electric vehicles and we do find that
with today's gas and electricity prices
it's actually profitable for taxi
companies right now to move to electric
vehicles okay so to conclude I developed
the theory of high-performance
heterogeneous interconnection networks I
built to data center network design
frameworks leg up and rewire my
evaluation of these shows they can
significantly reduce the cost of data
center networks and then I proposed Eva
flow for cost-effective scalable flow
management and that's it are there any
questions yeah with the evaluation to
rewire said something really interesting
you said
I was asking you compare it against a
menu InDesign network and this part of
the answer you said well the guys looked
at rewire solution I said oh we hadn't
thought of doing it that way but we
never actually do it because we both men
so I'm wondering are these physically
realizable in a practical so I think
they are and I saw I mean all right r I
mean IT guys you know IT guys are
resistant to change and I mean so the
cabling would be a problem at smaller
scale like in a container it would it
would be you know doable and then you
would probably have to have a different
solution or you could run rewire it on
the Entertainer scale and then I mean I
think really the hard problem right now
is well how do you route on this big
oven you know on this big of an
arbitrary mesh and that is going to take
some work yes but you know the purpose
of my work of doing this is to see well
what's possible if we get away from what
we're doing right now we think ahead you
know how much better networks could we
build and that's really the question
that motivated that work yeah
things are you talked about you know
through security things that open floor
when they first talked to what is he
playing they said in a personal network
at Mission Control this great thing but
when they talked about this it was
mostly in there
to assume that the data are now in maybe
you know that peripheral never limited
control it's not that
so maybe that kind of security features
not that necessary so you know maybe you
know just fine to do some kind of yeah
playing pushing cell blacklist switches
and so on so that
right so yeah I want to say we weren't
the first people to say let's use open
flow in the data center other people
have done that before us and so we're
sort of we looked at all that work and
we we said well look this really cool
work but will it work and that's that's
like the goal of the diva flow work was
to give people ways to actually use open
flow in the data center I mean I don't
necessarily think that open flow is the
right solution for security in the data
center and because as you mentioned you
know / flow security in the data center
might not be doable and so we may need
other things but one way you could use
diva flow is on a per night on a per
flow but on a categorical basis apply
some sort of route route your traffic
through a set of metal boxes that apply
these features and that could be doable
with using these wild card rules yeah
the flow evaluations so you see if I mr.
correctly that the main goal neckla
implode is the CPU on each switch all
flow set up and that limits the
but then the evaluation was looking at
metrics like packets per second for
statistics and forwarding table sizes
and things like that we try to use these
as a proxy for flow set up yeah it was a
missing link there but I right we were
trying to use that as a proxy for flow
set up like if we were for studying only
you know an order magnitude fewer flows
up through the controller that means
only a order of magnitude are being set
up through the switch cpa as well</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>