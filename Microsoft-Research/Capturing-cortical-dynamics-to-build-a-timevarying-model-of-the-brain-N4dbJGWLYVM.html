<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Capturing cortical dynamics to build a time-varying model of the brain | Coder Coacher - Coaching Coders</title><meta content="Capturing cortical dynamics to build a time-varying model of the brain - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Capturing cortical dynamics to build a time-varying model of the brain</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N4dbJGWLYVM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I self it's my great pleasure to work
come and ruin Lee from interval to
Washington to talk about brain dynamics
and this is one of the the topics that
probably will capture a little interest
in a lot of research here doing a neural
network type of research so our
professor Lee graduated from MIT hover
speech and hearing ph.d program um he
recently moved to Utah just a few months
ago a year ago oh wow time flies okay um
I got the opportunity to visi slap a
couple months ago there was a lot of
exciting things going on love there so
today he will come here to tell us
something about arm speech and hearing
research as related to brain imaging and
it relates to bring dynamics so give
flow to Asia okay thank you thank you
thank you lee for the invitation sir
it's great to be here my goal today is
to try and convey what type of what's
difference here in neuroscience and
really in terms of their interface
between neuroscience in engineering i
think that's actually where the most
exciting part is so where we're at and
where we want to head to my my home
departments vision hearing and I'm an
adjunct professor in electro engineering
but our lab is actually based in the
Institute for learning and brain
sciences and we'll show you the type of
equipments that we have for capturing
brain dynamics over there but I want to
start with something that sort of maybe
capture our you know thinking every day
the combination of neuroscience in
engineering and what are the things that
that the challenges that we face but one
of the things that that we're interested
in is how do people selectively a tent
or sound of interest this is something
that a human can do quite easily but
machine yet can do properly another
thing is recognizing speech just a brand
new speaker coming in a human can
potentially understand even my weird
accent of mixed with colonial British
Australian and some Bostonian accent
hopefully you can still understand my
speech properly you don't need to train
yourself or 20 hours of my
of tokens of me speaking so how do we do
that it's something that we want to
capture in terms of how our brains do it
and learn from it to one of the things
that we're also interested in is whether
we can design machines and now it
depends on different levels whether we
can mimic interact or even complement
human abilities so understanding where
we're at in the brain what our deficits
can we capture brain signals and
interject and of course you know this is
the ultimate goal right have a seamless
human-computer interaction and I think
that's something that you're or
interested in so truly believe there's
successfully interface between neuro
science and engineering would although
it actually starts needing from the
brain state classification problem can
we identify which brain States urien and
therefore make brain computer interface
and maybe in having artificial
intelligence that's actually enhance our
capabilities so let's start with our
basic problem right one of the things
that we study is the cocktail party
different now in this environment there
are lots of different conversations
coming up right and for us it's really
easy to pick a certain conversation so
for people that you dub they might say
hey you know I've got a Huskies ticket
can you come over and you know to want
to join me for the weekend to watch it
for grad students or postdocs they'd
probably say hey there's free drinks in
room 204 be right you've sharply tuned
to that um and that's defined back in
1953 identifies the cocktail party
problem yet we can't solve it
computationally right now I'll tell you
why this is also important from a
learning environment point of view here
is a Chinese lesson for the kids right
Szczesny how and repeat after me and
Jenny says how many help is great
unfortunately if you've ever taught
that's not the environment but this is
right so being able to selectively
attend effects education affects a lot
of things this picture also reminds me
off that you know there's a lot of
machine learning going on he
right the interesting part is that these
are the little machines that learn every
single day well in terms of I labs they
actually we we do a lot of brain imaging
on how they learn we want to understand
how they do it so that we can better
learn so in some sense this is a generic
problem statement of of how we can learn
from the best learners okay so if the
signal is not good even though it's
Niihau probability might have got now
which is very different from the signal
so if you don't attend it properly
nihaal means how are you mal means cat
right so if you don't selectively attend
and fish out the acoustic signals
properly there will be information loss
so even though it's a very perceptual
question it has a very high upstream
consequences so it's really hard to
describe what an auditory object is so
I'm just going to play a game with you
here we're going to play the game called
password you tell me what the password
is okay listen to the male voice and
tell me what the password is there might
be a price at the end so male voice give
me the password the password drink is
hosted apps now okay so what is the
password Huskies the password is Huskies
okay now I said that I'm there might be
a prize at the end right so what did I
buy you a drink now listen to what you
could have done to get that price from
me the password drink is awesome it is a
very expensive game to play if I if I
miss out right but I can direct your
attention to the same physical stimulus
yet you can pick up the signal depending
on what you listen to this is the
dynamics that we talked about attention
is rewiring your brain at 100
milliseconds level or giving you the
respective choose to bias the physical
stimulus so that you get one message out
or the other that's cocktail party
effect and that's what an ultra
objectives we listen to the male voice
vs the female voice now there's slightly
different from the visual scene analysis
right so envision here is there a screen
in front of the whiteboard and in that
case something's in the foreground
including the background hence there's
an edge and edge detection envision
helps a lot to segregate objects in
front background in all tree is very
different we call it transparent scenes
spectral temporal elements add on top of
each other and it's like transparent
it's really hard to segregate them now
there are different cues that we can use
here i'm just i'm using visual analogy
here and have different things in
different colors so you could imagine
that i can attend a certain feature here
if i'm attending to yellow in terms of
neurophysiology we know that if you're
attending to a certain color those type
of neurons actually are active more and
therefore have a bias in terms of
tension now you can selectively switch
from one cuter another and also I just
played a game with you because I was
fiddling around with with audio told you
a lot about huskies and you'd up i
primed you that the word is Huskies so
priming also works so in our lab we're
interested in ultra choose how they
group sounds and how to group acoustic
sound and how selective attention works
so what are the auditory cues that we
normally use in an orchestra you can
listen out for the violins versus the
flute right so that's tambor intensity I
have a louder voice then the fan that's
actually humming in the lac now that
I've directed you to that fan you're
probably just listening to that fan
instead of me going to concentrate more
on spatial cues in pitch spatial cues
are nice especially when you have been
farmers in terms of you know I know that
you have a speaker conference phones you
can actually beam form to a certain
speaker it's the same things that we use
for spatial cues in human pitch weather
the high pitch low pitch male voice vs
female voice so just briefly I know that
Audrey spatial cues may just going to
talk about it briefly so he's her he's a
subjects head he's an acoustic event
coming on and the sound arrives to the
HC lateral year the year that's closer
slightly ever so slightly faster than
the one that's opposite by tens or
hundreds of microseconds that's good
enough for us to have something called
interaural time difference q we human
can actually go as far low as 10
microseconds as that discrimination in
to our level difference of the head at
higher frequency access the caster
acoustic shadow such that the closer
year actually received a signal is
slightly higher by few DB then the
Arsenal that's further away and that
gives you the acoustic use there are
some spectral cues as well given all
this now I can direct you to listen to
one Q versus another what we want to do
is to find the brain dynamics that's
associated with you rewiring your brain
really briefly to listen out to her
stimulus and here's the paradigm that we
use we ask the subject to week you the
subject which side to listen to so
here's in left arrow and then they
maintain fixation and then what happens
is a stimulus comes on two simultaneous
digit ones coming from a left one coming
from the right one have a higher pitch
and one has a low pitch and so we
because the arrow was in the left
pointing to the left the right answer
when we asked you to respond would be
three right now you could imagine that I
can just rewire your brain slightly
differently using the same stimulus that
point the arrow to up and down so now
pay attention to the pitch the pitch of
this I've denoted high pitch are in red
so it's if I have the arrow going up you
listen to that simultaneous speech again
and to answer correctly you have to
listen to the high pitch and the correct
answer sheet
and what we're going to do is to once
you've been primed what to listen to
look at the brain dynamics to how you
actually rewire to listen to that Q and
so that's the dynamical process that
your brain goes through the technology
that we use is as follows emmy gee
measurement this is newly installed in
and you'd up two years ago it's a
multi-million dollar machine i think
it's the price tag is in inventories 3.4
million i think sits in a sits in a room
that is magnetically shielded that's 306
channels measuring the magnetic fields
coming out of your bring we
simultaneously use EEG
electroencephalography so not only
picking up the magnetic fields we're
taking picking up the electric potential
on your on your head on a separate
session we do an MRI scan we want to
know the anatomy inside your brain so
that we can relate the brain signals
that you measure and spatial eco
register it such that we know where
anatomically is come from why don't you
do that when you co register your
anatomy with your brain activities that
you measure what you can now do is to
capture the brain dynamics and we make
bring movies so I told you that they get
a visual cue in the beginning right so
here's the brain movie showing the first
300 milliseconds I have to explain this
brain a tiny bit it's the funny brain
view everything gets sort of inflated
I've marked the visual area the auditory
area intentional area an executive
region the red the red spot it's the
flow of activities brain activities
precisely it this is emmy gee so this is
measuring about the boat postsynaptic
potential and if you look at a time
scale we can get down to milli second
position right so here at 1000
milliseconds prior to the onset resound
you receive official Q&amp;amp;As about 100
millisecond delay
to reach into the visual cortex and as
you see there's a wave of activities now
in your occipital cortex and your visual
cortex marching further down now you're
holding fixation but you're actually now
rewiring your brain to attend to added
space reviews at a pitch cues right and
you see there's a sustained activities
in the attentional areas now this is
actually called the frontal eye field
it's also involved in vision directing
your eye gaze from one position to
another if you think about moving your
eye gaze is casting a spatial attention
from one position to another and the
question that we ask is is this space is
this region even though it's only four
eyes previously defined is it also
useful for auditory special here region
cultivation ah yeah so so this is
actually this is actually the pre
central sulcus yeah ok so at zero
milliseconds that's when the sound comes
so just watch the auditory cortex when
it received the old the ultrasound now
this is fundamentally slightly different
from fMRI right traditionally in
neuroscience fMRI or even in terms of
before that lesion studies people talk
about where in the brain matters right
here we're trading off a tiny bit of a
spatial resolution but we want the brain
dynamics right you know that in fMRI
every two seconds you can you take a
brain picture and you infer where things
are in the brain two seconds is an
eternity in the brain I hope right I can
do a lot in two seconds 1001 1002 you
just switched off because you know that
1002 is redundant I'm counting once in
two seconds your brain shuts on and off
depending on the context and all this
stuff so we want to capture at
millisecond level we're about what the
brain is doing now using this technology
what we can then do is very precisely
work out the difference between your
wiring of the brain even before the
onset of a sound as your rewiring to
listen to
so right before the onset of a sound
when you're paying attention to space
the spatial cues one part of the brain
frontal eye field is involved more
active when you're paying attention to
pitch and now the region's it turns out
that this region the superior temporal
sulcus is previously been recognized as
that the people using a musical pitch
this discrimination and absolute pitch
people with absolute pitch as stronger
activities here so naturally your brain
is recruiting different brain regions at
different time to help you to do the
task because of the fine resolution and
this is where we differ from fMRI we can
distinguish what happens right after the
sound on set also FM area is really
noisy right if you have ever been to an
MRI scans is it is constant like that it
interferes with the very signals that
we're measuring in speech and hearing em
EG is quiet so we can definitively work
out during the stimulus how we're
reacting to the sound and here we can
actually see that while the spatial
areas the frontal eye fields is still
involved while the Simmons is on it's
less active or it's no longer
differentially recruited when the sound
is already on now there are other ways
of using this technology here we're
using sending an attentional probe into
the sound so what is what do we mean by
that well we're also going to give you
two sound one coming from left one
coming from the right and your task is
to count how many E's so let's say
listen to the stream coming from the
left ee maae lk great there are three
e's response three however we're going
to tag this with a certain frequency a
low its high frequency for the brain
dynamics people it's low in terms of the
speech people right thirty-five hurts 45
hurts it's nothing that that you think
of is something that's high frequency in
the brain we think of four to six Hertz
428 hers it's low frequency or even down
to one two three
so in any case we're tagging 35 to 45
hurts we can talk more why we chose
those frequencies so basically he's a
speech signal and we're going to
amplitude modulated we're going to use
that as a probe to find out where in the
brain is actually responding to the 35
hurts or 45 hurts remember that when i
cue to listen to the left this is now in
the foreground right and if it's true
that you're modulating what you're
listening to the things that in the
background shouldn't matter right you're
concentrating too to the left stream and
we can look at the taxi so what happens
now listen to the left 35 vs the tag
let's tune the brain at 35 hurts we're
just doing a phase-locked value here
right you can see that again the left
frontal eye field it's locked to the
stimulus and also the auditory cortex
remember 45 hertz is this site that you
don't listen to right we told you not to
listen to that when we tune in to 45
hurts that signals no longer there in
the frontal eye fields you can
counterbalance in you know which side
that you're listening to what frequency
only the left frontal eye fields is
locked to the stimulus that you're
listening to and the probe turns out
cycle physically is transparent as in
the subjects doesn't even know that the
even if we change the probe in midstream
sufficient even know so it's a neat
neuroscience cue that we can use and
also that can be utilized for a lot of
eventual deployment for brain computer
interfaces specialist in seem to
clinical here yeah that's because we
combine the anatomical scans with Amy
Jeanne hmm so after you Alicia
um let's say fmri the canonical
resolution is roughly one millimeter
although after a lot of spatial
smoothing it's around five to six
millimeter effectively we would say one
centimeter so from us for us we care
about the network approach we don't care
about whether this region sup region of
the Audrey cortexes responds more to to
sound a picture of space we want the
global picture of the brain Erik Larsen
actually here if you want to learn more
about this experiment I'm just going to
briefly describe it one of the things
that we want in a lab is to eventually
work out how to tune and hearing a
hearing aid dynamically right so hearing
aids right now don't work in a very
noisy environment if you ever had older
relatives listening you know in a in a
restaurant they would rather take it off
right that's because everything's being
amplified it doesn't selectively amplify
to resound right now if we want to
selectively amplify the sound we need to
know where you're attending to and one
of the questions that we also need to
address is have you just switch
attention from one person to another so
here we're designing experiment just to
see whether you've switched your
attention from one speaker to another
use the same queue whether you listen to
your left or right visual cue to third
of times that great you know just
respond to the two digits one coming
from the left one coming from right from
the side that with cute you are but one
third of a time we're going to say I
don't know just kidding i'm going to
switch the attention to the other side
the right at that 600 milliseconds when
we told you to switch your attention
where in the brain helps you switch that
tension turns out that the right
temporoparietal Junction here's the
manipulation point right just kidding
switch attention Green is switch
attention white is hold attention you
see a massive difference in terms of
that particular region there are the
region says involved the right frontal
eye fields or the middle frontal gyrus
great there are different regions but
for us is whether we captured that
signals and do classification to work
out whether you've just switch attention
or not normally look religion yeah my
attention is called generic attribute
that's right that cross that goes across
a visual that's right so so one of the
questions that we have is its attention
super modal right if you're paying
attention it doesn't matter where you're
paying attention sound or space it
should be just an attentional Network
auditory attentional Network has not
been studied thoroughly and that's what
we want to contrast that division our cv
j has been implicated in visual cortex
and i'll show you the coolest thing
though is this not only that we find a
difference in terms of the brain
dynamics we then correlate back to the
behavioral performance right and you see
a massive correlation depending on
whether you can switch attention versus
not and correlate that into the
differential activities of that region
it's almost like I actually when Eric
show me this graphic that's too good to
be true it's really highly significant
clinically we can use this to perhaps
diagnose central auditory processing
disorder there are people that have
normal audiograms but just can't do
things in a cocktail party environment
so from clinical perspective this is
useful for machine our brain computer
interface usage is it turns out that one
part of the region is really good
predictor of your brain state and this
is what Lyris pointer is that here's a
visual attentional Network that's been
previously mapped out I've shown your
left frontal eye fields right frontal
eye fields right temporoparietal
Junction it i think it's I'm bit biased
I'm an auditory person auditory always
cares about time and frequency it is not
the native axes that the visual people
think about so the questions that we
want to now address is yes we have a
network how do these signals pass from
one note to
another also do they have this when they
pass the signal can you tell that they
have some correlation how do they pass
the signals in the first place and what
I'll get back to that so as we said
we've started my lap over here at u-dub
and looking specifically into all three
brain sciences but we have a newer
engineering go in the lab as well so I'm
going to walk through different
experiments different people doing
different things and hopefully if you're
interested in it please also talk to
them during the cost of the day so Ross
Maddox also here he is currently
studying audio-visual binding problems
now why do we care about audio-visual
binding there are many objects that
comes on right if you have a a computer
passing a video scene right how do you
know whether the sound and the vision is
the same person right it turns out that
we use a lot of temporal coherence right
at the syllabic rate my mouth moves at
this four to seven Hertz signal and my
syllable is also coming up at four to
seven Hertz that gives you a cue that my
mouth moving and my sound coming out
it's coming from the same object right
and so we're designing experiments where
r us is looking into the different ways
of binding right how temporal coherence
can change your perceptions of binding
and the implication is that if we can
harness this you can also be turned into
you know how machine can use that type
of information for binding information I
think I think one of the main problem is
you know ultrason analysis visual
analysis is still a very hard
computational problem right now can we
learn from human to do that task another
project that we have is with the
graduate student with Elliott Sabah and
his Coast advice by professor les atlas
and what we look at is the signal
processing in the brain now what does
that mean
speech speech people would tend to think
about you know carriers at pitch you
know hundred and few hundred Hertz and
then you know you have four months at
the thousand Hertz as i alluded before
in the brain we talked about four to
eight Hertz as some brain signals
carrier and there's some high frequency
at about 30 to 150 hurts here is showing
a mouse moving in a maze and where it
goes into that particular part of the
maze effects where the face procession
went that particular when there at that
particular region that knew Ron is fired
slightly earlier in phase of that carry
frequency so it becomes that the phase
of another frequency can be used to coat
here in you know we're in space in the
memory in the hippocampus people have
done experiments on memory retrieval
items retrieval if I give you a sequence
of you know 1 2 3 4 items you can be
locked into the different face of a
lower carrier to give you that
sequential results back so we start
starting to analyze the signal
processing in the brain the coupling of
different frequencies and try to map
that out you can also imagine that this
is can be used eventually in EG and emmy
gee right i can dynamically look at how
different parts of the brain are
communicating using these lower
frequencies and therefore infer what
brain state or what items you retreat
I've mentioned this problem statement
that that our laps go one of our lab
goals is to get a dynamic a hearing aids
I can dynamically tuned to the signals
of interest and in this case we want to
know what you're attending but in a
generic problem statement is what is
your brain state if we can measure that
you are you just switch attention to one
from another I can also measure whether
oops I've just made an error here's an
error signal coming from the interior
cingulate cortex for example
so a broader statement is can we
classify brain States so Eric Larson
who's sitting in the back post up in our
lab he is working on this particular
problem we start with brains lots of
brains let me take a step back to in
terms of brain computer interface you
can classify all your brain signals all
you want for a particular subject you
will do very well however when you have
a brand new subject it's really hard to
predict the n plus 1 subject what that
brain state classification is going to
be we want to make a difference in that
room is can we predict a brand new
subject what sort of brain
classification taken into account of the
bank of brain signals that we have and
this is how we approaching this problem
first we calculate the game matrix what
this means is that we have an MRI scan
we decimate the space in brain this is
pure bios of Outlaws just to work out
you know how the sense is seeing each of
the dipoles in your brain and then
however unfortunately this is we're
talking to Lee before us it's we're
using the cheat data right now we're
doing modeling because we can simulate
bring signals ultimately we never going
to measure individual neurons in the
brain not yet anyway right inset what we
overall slips into that's right that's
right but not the whole brain if you
poke the whole brain and not show that
organism we survive right amps to go to
the publishing on your own oh that's
true oh yes um from from a I think we
really want to do a non-invasive imaging
approach I think that's where the comer
commercialization aspect will come in
I'm not sure everyone put up their hands
for for sticking needles into the end of
the brain so what we're measuring is
something coming out of the head is the
eg signal to infer back to where those
signals coming from the brain you need
inverse approach right this is high
dimension not as high dimension rank
deficient need an inverse approach so we
do the inverse imaging l2 norm
constraint we talk about the the the
actual methods later on if you're
interested one other problems turns out
that luckily I guess each of our brain
it's different he's look at each brain
here right each brain it's just slightly
ever so slightly different in a jump
tree they all have some resemblance to a
generic brain but they are all different
the different in the folding changes the
physical properties of your EG and mu g
signals by the right hand rule right so
a lot of brain from your interface right
now uses the motor strip the motor
central sulcus and half isn't they all
realign quite likely like this but it's
nice once in the prefrontal cortex where
the cool things are happening right
attention executive control those are
the things that the bracings that we
really want to capture the forwarding is
so different that can massively change
just imagine you're folding right just
do your right hand rule here their field
will change very differently so one
thing that we have to do is to align all
these little brains sorry big brings
whatever ah see racal morphing is the
approach that we we use look at one
brain we're going to map it to a generic
brain and come back to it just to see so
you can see that it there's some sort of
decimation right low-pass
characteristics but it captures mainly
the soco gyro patent which is important
for us to get the field magnets by doing
so what we can then do is to start
approaching solving this n plus 1
problem we if we have a new brain we're
just going to cheat one more time when
we do the brain computer interface for
the brand new subject all we need to do
is just as this person to get an MRI
scan so we're almost getting all the
information we don't need so much
training of the classification the goal
is by understanding just capturing the
anatomy of the brand new subject can we
do much better in terms of brain to
clear interface and so in a preliminary
modeling experiment suggests that the
answer is yes where is the regime that
we do in general in terms of brain
computing face when we want a massive
deployment right it's low trial counts
we're not going to get the subject to do
a hundred trials and classify right
that's the commercialization aspect is
sort of low we want low SN our low trial
counts those are the things that we want
and it turns out that in this simulation
having the anatomy of the untrained
subject right so we're training all the
brain datas that we have with the
anatomy now with a brand new subject
with that new anatomy can then actually
help and in the low trial count and low
snr it actually provides improvement in
fact if you only just use the sensor
space so only classifying brain signals
using the eegees channels you can't even
start projecting to the new a subject
normally doesn't work unless it's a
well-defined motor imagery their view
the few experiments that work and that's
actually where the mainstay of PCI right
now ok I'm going to conclude with a cool
new experiments that's done by nima good
friend of Vera sees a former student of
shehab I know that some of you know she
have as well here is a direct recording
on the brain so this person's skulls me
removed electro is placed on the brain
this is for epilepsy detection a priest
you're planning and the subject is being
played two sounds again it's the
cocktail party problem this is the CRM
corpus coordinate response measure and
the two centers is going on and speaker
one says ready Tiger go to green five
now speaker to simultaneously just like
the example that I please you there's
ready Ringo go to read through now the
person is to you no respond to where
Ringo that went to write so when the
subjects listed Ringo great okay I need
to know it's red to and ignore green
five this is the computational problems
that the brain is facing right you're
all familiar with computational otras
analysis try to fish out noise it turns
out you know as an engineer there's a
signal to noise ratio right there's
always noise in human there's no noise
it's what I'm attending to and what I'm
not attending to is noise it's really
hard to define noise in human you can
switch your attention at any point in
time so this is the signals that you get
is the summation of the two and if I'm
only recording one signal at a time this
is the brain response the spectral
temporal response in the auditory cortex
to speak or two alone to speak a one
alone this is the result of the unmixing
of the auditory cortex due to the
attentional bias to when you're
listening to speaker two alone speaker 1
alone and see the correspondence here
signal here you can the bus but you can
aim different electrode yeah the signal
please do something yeah so so the
spatial dimension here it's not the most
important part you can even tune down
into like oh no this this should be
multiple electrodes
oh yeah that actually gives you another
piece of information the weighting of
the electrodes may give you a clue of
where the actual area of the brain
that's most responsive to this so that
actually gives you the where question
right here at least gives you the
spectral temporal dimension that the
auditory cortex we get because I police
are actually to the only read that paper
yeah but he's this sort of you know a a
prove it off something that we already
know right we can do this it turns out
that the auditory cortex when you have
attentional bias can much like the
spectral temporal signals like this so i
guess the to conclude i want to sort of
you know open up of where we think that
it's a head and the road ahead of us and
also how that integrates to perhaps
things that are maybe in five to ten
years coming up commercialize about or a
decade later I think speech perception
it's fundamentally something that is
time critical right speech and hearing
science always time frequency axes are
the most important dimensions to us so
one thing that we should start looking
into is how different brain regions
coordinate at different time that should
provide us with a new way of thinking in
terms of the computational modeling it's
not just the where it's not just the
anatomical connection people have been
doing that a lot right how information
is being passed dynamically that that's
the time dimension even though we've
from the engineering side we done that
using you know the hidden Markov or
whatever hidden time state changes right
you try to learn that well why don't we
learn from the brain as well right
another thing is the interaction between
top-down and bottom-up attention it is
really hard to ignore when a foam comes
on right now right that's like an
auditory siren you automatically
switches to the film and in fact that's
why a phone rings is to grab your
attention right that's the competition
between top-down attention oh wait i'm
supposed to be listening to the speaker
or not and and to ignore that phone ring
that's the competition now a question
that would be of interest is it solitary
siren and or or nothing right it's a
siren only just wrap your attention like
this or can it be sort of like a
attention-grabbing but depending on your
brain state whether that will grab your
attention or not so I understand that
there's a great interest in Microsoft
looking into triage alert system like
they're busy biosystem right in that
sense you want to know whether I want to
alert the user whether it's a good time
to stay away from my work right now the
decision point of course is based on a
lot of data learning and whether how
likely it is from the decision model to
alert to the user but you can flip it to
the other side right I want to know how
likely the brain wants to be alerted or
hey brain i'm going to give you about
thirty percent chance of actually
capturing this signal just like if you
have a green light on your phone right
and you actually if momentarily I'm just
bought with my work I see a green light
flashing I can make that decision myself
to whether i want to grab the phone and
look at that message really quickly
right so here it's giving an option of
not just an or nothing alarm but let the
user give the user back the chance to
say hey here's the signal in the back oh
yeah i'm actually interested in it
because I just got bored a tiny bit
right so it's an interaction between the
computer and the user and using that
from from a psychic use extend how to
integrate that in
now of course the few other things that
we can do right one imagine in 10 or 20
years time when EEG technology it's
available such that we don't need all
these gels right in fact dry caps being
produced now slowly becoming more of a
mature technology you could have EG caps
that literally just have these micro
sensors in your hair picking up the
signals wirelessly transfer to your
phone device the key is what signals are
you picking up and how what are the ski
signals from the brain that is usable to
interact with the device that's the key
right and that's why we're interested in
brain state classification we want a
bank of vocabulary of your brain such
that we know how to look it up moreover
to look it up it's one thing we need to
know the dynamics the other is how
generic it is right we don't want to
train every user all the time maybe
someday by using Ngata me we can just
say hey he's out of the box a new phone
device here's some eg caps just put it
on this way and we'll do the learning
myself so that's where go to end and
just like to acknowledge in terms of
funding Lee was asking what type of
funding agencies now are interested in
in brain state classification from a
hearing aid point of view the National
Institute for deaf and communication
disorder so sort of the hearing research
branch in NIH is obviously interested in
the hearing aid application but the
defense is also starting to to get into
the brain state classifications for
reasons that that might be obvious and
like to thank my collaborators both in
UW in Boston University and also Mass
General and if you want further
information here's our website thank you
back to this attention of
sort of inverse reverse engineering
probably just making fun do you know any
condition bubble that somehow can give
you similar in that they are measured so
that the same kind of President Lee
afterwards can recover one of these
dream the two questions here right one
is what dimension should we use right so
what feature space what hyperspace
should we do that classification for for
computation or dress analysis
segregation or extreme segregation I
think what we haven't been using and
this is the type of research that we
want to at least give clues of what
auditory cues we use right so grouping
cues are the most important things
continua t of sound so it's that there's
a whole branch of audrey grouping
literature the result one you don't care
all the Jews are as long as you can
produce something that gives you the
same kind of measurements maybe see me
this one favor no no there's not the
brain that you measured really cares
about the cute butt though what the
person that is doing it it's using all
the cues that the person can use to
segregate this or else this sound
wouldn't work right yes and the
interesting point was we were also
talking about you know a metric of how
good you are in terms of after you
segregate right it's good to have an LG
norm right it means square error
engineering standard go to that may not
be the most relevant thing for the user
rite aid and a mean square error might
have spikes in there which really
perceptually unpleasant right what up
and and just like mp3 coding why is it
that transparent coding works you need
to work out from the mp3s d from the
perceptual side from the cochlear side
you can work out the masking here is the
higher level right what drooping cues
can make the streaming
more seamless and you can still you know
have some residual error that is
irrelevant in terms of getting a stream
map of yours last year okay ah yes so so
the input now we know it's a base of the
signal yes so Theresa actually missing
diagram then I really want to know in
terms of the broad signal that you
measure oh I see from from if every
actress yeah those yeah and then
whatever the process they do they will
recover this assuming their patients are
implying the brain absolutely but don't
forget the the following all right we're
measuring from the auditory cortex right
vision to focus on one of them right but
I've showed you in previous slides that
when I pay attention to space or pitch
it uses different areas so here it's a
it there is a spectral temporal
modulation of the stick the physical
simile which is great but that doesn't
necessarily tell you where the attention
of signals coming from somewhere else in
the brain is telling the auditory cortex
modulate this way right so it is the the
trade-off between you want to know
locally very well in terms of this the
spiking of the neurons versus I want a
global view of the brain and how things
are being connected and I think you need
both there's every single level in
neuroscience is important right down to
new single unit neurophysiology you have
a multi-unit recording and for Masse's
assists neuroscience approach obvious I
think the signal is piece of work is
that the ad shows you that whatever
recover you have in on recording this
should tell you to forget the original
so what it means is that if you in terms
of research one if you have this
competition model maybe they can be
verified some kind of government and
that give you reasonably good output
that you might measure over here and yes
the Yankees shows that you can actually
recover the releasing of that it allows
you to investigate what teams are
otherwise they do bodily the VR right
right and and you could imagine that you
know if I drop some spectral temporal
element would with that matter so there
there's some metric that you can use
getting what the brain cares about
versus not it's a fun though lucky about
how rotation is we're working on that
one same testing attention with subjects
that have severe attention deficit
I work with this too
yeah yeah there are a whole host of
psychiatric disorders that may influence
attention ADHD and also autism we're
talking about groupings here right how
how different frequencies in the
cochlear even though you hear pitch
coming out they all have to fire at the
same time and group all these things
right so back in the ter hearts day talk
about you know pitch templates right
frequency templates but ultimately is a
grouping binding problems how do i bind
these things together from different
frequencies to you know or different
dimensions it turns out that autism it's
are characterized as a lack or a hypo
continuity they the long-range
connectivity is not as good now now
extrapolate it's probably a problem with
grouping or at least some aspect in in
there asd spectrum is the lack of
grouping that could also be in HD ADHD
we don't know and there's a whole host a
question semin from the perceptual
grouping on to serve a higher learning
or disorder between gender wage is their
ability to
focus attention
haha my wife Allison time I I really
want to do that study but healthy far
but at we don't know we don't know we
haven't systematically study it that's
the official yeah so to close up early
time the processing or understanding
games right working with the floor that
cute clues that influences subject given
without him knowing yes so I let me
rephrase this this is a high dimensional
feature space that you're looking at
there is space there's pitch there's all
these things you can tune into whether
you can attention we modulate one
special Q which might be stemming from
brain stem if it's IED or iod or other
other cues we're actually doing
experiment right now just trying to see
whether there is a direct coupling from
your eyes with very rudimentary binaural
cues coupling I think traditionally
speech and hearing sciences stem from
the engineering side ever since the 50s
60s is always bottom-up as good
engineers anything that you know top
down it's just noise let's cut it out
and understand department very well I
think in the last five to ten years
we're starting to try to see what the
top down signal so there's not much
literature out there so we have to
systematically go back all the way down
to brain stem all the way down to
cochlear to see where is top-down effect
and that's actually how to measure
something about that we've also done
experiments where in addition to play
stimuli we put in these short quiet
noise first the users don't even realize
it there and if you look at their
performance on the test don't affect
their ability to selectively attend
together but then we look at the brain
activity we see their areas of the brain
there it specifically involved in
suppressing the response to these sort
of distractions to go so we're trying to
work out what the network is that allows
you to do that so even if you don't know
that they acted you you know that's not
to be current well the you don't know
they're there but yeah they're doing
some task and just distracting sound and
we asked them afterwards they even
noticed is there every subject said no
and yet there's this brand is involved
in allowing them to not notice it
raised the question so it sounds like
this attention process is kind of
generic but is it true that really all
happens kind of at the top or is it more
like if you measure what's going on the
auditory pathway as flexion changes is
there a change in that pathway as you
would hope so and I think I think that
is the reason why hearing aids don't
work right hearing aids it's a bottom-up
amplification we want to have the top
down signal we want the control signal
back know if it really is a generic
selection mechanism then the mechanism
for going down to the future level
that's jarett oh that's uh it could be
so this there will be a lot of even
pathways that are not mapped out
systematically i think that the hearing
literature has always been the afferent
pathway at the bottom of pathways how
the top down pathways going all the way
to the outer hair cells to tune
specifically the cochlear amplifier will
be the interesting product right it's
hard to access to that data price you
know some news you say using spatial
cues we are tightening
music excuse armonica it's another thing
to is rupak amazing onset and offset do
you know specialties are not harmonic
for the structure
yeah that's that's the huge feature
space right the onset tends to be more
important than offset but there are
people that talks about you know how the
change in the scenes between the odds
and offset through their studies that
Murray trade over in UCL specifically
studies a transient on set off set to me
I think onset it's just again another
huge queues what is the what exactly are
we doing in terms of as an organism to
try and solve audris analysis right is
to make sense of the world in terms of
things that that you care about whether
the lion is coming over from the right
as opposed to some sound coming in the
roaring sound and all these frequencies
it's it's important for you to solve the
problem so you use all the cues on set
is great because as a sound emulating in
space if they all come from the same
source they will have the same answer
they all have the same facial
relationship and I bet that our brain
capitalizes that information because if
officer this musket fire to the
liberation which is how many more
basically first term the onset that
that's true that's true so on said it's
much more reliable that's a great
Bayesian bring that we we have we would
wait on set as the greater evidence than
offset yeah same yourself socialization
big piece that's the present there's
cues for runs at an offset that aren't
affected by reverberation at all which
is watching someone's mouth or something
like to and I mean we are just beginning
to look at it but it's very likely that
there's crossover of these signals that
can help you do this
okay they don't even come from from
acoustic I thought you were just talking
care of you here for this excuse the
leadership of optimism allowing our
books in the hall come again talk about
my having this means is that no
attention adjusted in this fellow oh no
no I do not want to convey that at all
no especially I just checked the box
that this is going out oh this is this
dog is available to everyone no no this
is not that there are many there are
many studies in the past that talks
about attention there are great debates
on whether sin analysis is pre-attentive
or not this attention matter when you
say great street there's a great long
debate since early 2000 I there a lot of
studies before I think would be fair to
say is from EEG event-related potential
studies they answer different questions
I think that from a network approach is
quite new just just because the Emmy geg
you know stringing all these things
together took a while to develop that
deck technology so in terms of how the
auditory cortex communicates with the
frontal eye field I could imagine that
that's I could say that the maybe that
as far as I know there are no other
literature that talks about that they
have google people the worst it I I know
that like Martin cook you know that
there are a lot of they I mean of course
they care about Audrey grouping right
but one thing that it turns out that
from my doctoral thesis a lot of casa
people think of you know it you have to
phrase it in a probabilistic sense right
each time frequency pixel it's either
belong to speak of one or speaker to or
you know whichever speakers you always
so this is the interesting part you can
you can measure the same thing in a
human it turns out that they don't have
to add up there's no reason for a
subject to think that you know each time
frequency pixel has to conserve
probabilistic Lee we phrase that model
because competition alee we need that
constraint turns out when you do order
that they there are asymmetries in the
auditory cues that actually we pass a
scene differently as the problem
statement that the Casa people phrase
the mechanism was ill and the child was
born every child learns it later we
don't know I know that colleagues in our
and in RT pollen is starting to look
into it I can tell you that in visual
scene analysis pawan singh her it was
actually reader of my air of my
dissertation back at MIT he goes to
india and looks at a particular group of
children where they have this disease i
actually don't know what the tango term
is basically there it's really hard for
them to see but a one quick procedure
they can fix the vision so they can look
at you know the when they can't do see
analysis to when they can do and how
quickly it isn't is it in eight so from
the visual literature there's this
literature that can point us to the
innate pneus for us to do this in all
trace analysis unfortunately we don't
have that information yet as far as i
know some people mentally the others
using this because of
this is the spouse notch filter is
alluding to yeah i i'm pretty sure i
don't need to test that that's got to be
universal okay so we do have thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>