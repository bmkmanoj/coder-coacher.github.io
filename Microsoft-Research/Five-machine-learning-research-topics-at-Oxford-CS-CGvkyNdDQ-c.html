<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Five machine learning research topics at Oxford CS | Coder Coacher - Coaching Coders</title><meta content="Five machine learning research topics at Oxford CS - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Five machine learning research topics at Oxford CS</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CGvkyNdDQ-c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so what I'm gonna do at this talk is
just basically go over five directions
that interest me for my recent research
and and hopefully try to by doing this
establish connections with folks hearing
in Cambridge and you know and embark on
cool joint projects together so the five
directions that I'll talk about today
are the ones that we have over here the
first one is of a theoretical nation
nature's about and it's about random
forests I think Microsoft Research is
doing the coolest work that there is out
there in random forests and I've been a
huge fan of the work of Jamie shorten
and Antonio and you know that produces
great work and great books that even my
undergrads love to read and understand
and implement and so at the practical
frontier they've really made great leaps
and bounds and what we've been trying to
do is work on the theory because to this
day the original algorithm of Bremen
remains not proven to be correct I'll
say more about that I'll talk about the
randomization of algorithms there is how
you take a stochastic algorithm and make
it at the myristic and why would you
want to ever do that I'll talk about
probabilistic graphical models which is
also an area that I think there's a lot
of being a lot of work good work here
especially with infertile net and so on
and I'm going to talk about the problem
learning there and in something that
sort of hot on you it hot on you in the
sense that I I submitted this paper at
midnight last night to ICML and how to
for some models be able to make learning
linear in the size of the graph as
explanation the size of the graph
then I'll measure some briefly some deep
learning and then I'll talk a bit about
Bayesian bandits and optimization a
topic that I'm super interested in so
here's a motivation for the work so
assume that and you've seen plenty of
motivation for random forests but I'm
gonna give you an alternative motivating
take suppose you wanted to build a
digital magazine that is Europe in your
iPad and every day you have your
magazine or on any topic of your
interest like you could have the
Bayesian magazine in this case which is
that's the title of the magazine and
every morning you get interesting
articles you know it's like Andrew
gell-mann's blog Christian Roberts blog
and so on so it's sort of cool easy read
articles about machine learning not
necessarily you know serve a jml our
paper and you might want to have a
magazine for many other topics of
interest
Bayesian or neural networks or rocks for
statistics or these are some of the
things that interests me as topics to
read but in order to have such a
magazine you need to scout the whole web
for the interesting articles to be able
to deliver to each user you need to
personalize to each user and you if you
want to succeed you probably will have
you know you would hope to get at least
over a million users you need to know
what article is about what topics so in
a sense you need to classify all the any
new article that is of interest into a
particular topic
now imagine that so you could use a
resource like Twitter for example to
tell you what's exciting out there in
terms of articles what's what's trending
is something that captures the zeitgeist
but then you would have to grab that
article and say what are the topics that
that article is about so here's one way
you could do that and by the way this
digital magazines actually
real thing we I built it with my
students
it's called site and CNN acquired us and
it now powers CNN and this is how we go
going about tagging so the problem with
tagging is this that most of the web
doesn't have labels we want to be able
to classify webpages but if you just
rely on this you know what folks have
done in terms of building ontology and
so on there's very little you can do so
what we did was over several years we
crawled delicious this is being recorded
right and we did get delicious it then
no one stopped us from getting it and
delicious I believe now is it belongs to
Google it used to belong to Yahoo was
one of those products acquired by Yahoo
that did not evolve and what we did was
we crawled about 100,000
I'm sorry 100 million pages there's a
one factor higher than a shoe almost two
factors higher than a Wikipedia English
and we used the labels that the users
provide us to essentially build
classifiers and the way we build us
classifiers is we do extract the text
out of an article we also extract the
text from the articles that point to it
so we get a context we produce thousands
of features for the document and then
from those thousands of features we use
a classifier what could you use here to
classify the web random forests and that
brings us to the this issue that despite
the fact that running force is so useful
in the medical imaging and computer
vision in and that is like one of the
best classifiers out there it's
obviously scalable we were using it to
tag a billion documents
it has this problem that for an academic
that that is so far not satisfying that
is it's never been shown that it's
actually gives the right answers so if
you have some measure of cost or
performance it's never been shown that
as the number of data goes to infinity
that that cost will go to zero
never mind getting convergence rates
it's just that very basic does this
actually work as I keep getting more
data so I'm gonna take you over an
example in this case I'll focus on
regression and and I know that this is
sort of a powerhouse on random forests
if you haven't seen running forest
before I apologize for this part of the
talk I will assume that you do know
random forests if not going bug Antonia
for a copy of his book but so very
quickly for those of you who already
know random forests what you do there is
you build trees that split the space and
then on each leaf of the tree you fit a
just in this case we're doing random
forests for regression so we just say
fit a line so you do a piecewise
approximation to your data if you're
doing regression and typically the way
you do that is you you construct
predictors for each leaf and you
construct many of these trees capital M
of them and then you average these trees
and this gives you the strong classifier
so each of each of these guys
corresponds to the classify for the tree
and then you combine them to produce the
forest so it's actually extremely easy
to implement as to construct the trees
we often use heuristics like in this
case the heuristic that's being used is
that if we're trying to minimize the
squared error between the to fit any
point in the mean then we just need to
make sure that we we try to minimize the
maximize information gain or in this
case error reduction
another thing that we do and some folks
don't actually go over this exercise of
doing some optimization to do the splits
and folks just do do this randomly they
just pick some point at random and then
they split and then just construct
histories at random and I'm going to
show you a few examples of what you get
and miss by doing that another thing
that I'm gonna do to show you the sort
of results we can get is I'm going to
assume that if I have say these features
in a million dimensions or so and I'm
going to take a subset of them I'm going
to pick a few of the dimensions and then
I will project my data a subset of my
data points into those dimensions and
you can think of this as the projection
into one of the dimensions and for a
subset of the points then I'm gonna do a
search along the solid line by while
computing this objective and then pick
the optimal split point I'm going to do
the following thing that is essentially
in order to get a theory and this is the
sort of the key trick that has allowed
my students and I to get what I believe
are the best bounce right now for or at
least a theoretical random for is the
closest match or what exists in practice
we split a date into two streams so when
a point comes each tree separately flips
a coin and decides whether that point is
going to be used to decide how to it
builds its structure of the tree or to
do the estimation of the leaves you know
to sort of fit the line okay so with
separating estimation from structure
building in a point that is used for
structure in one tree can be used for
estimation in the other tree so that's
also sort of important so we're trying
to maximize this then what does it mean
for something to be consistent it means
that you minimize some error so if this
is the optimal function then what we're
saying is for any X so we're going to do
this for any X if you have
a classifier that depends on your data
set it has some sort of randomization
due to all the different randomization
you could inject in the algorithm and
also for any input you want this error
to go down to zero we need the
regression so that you can approximate
the XMB
the conditional mean how do we prove
that this risk goes to zero I'm just
going to quickly in three slides go over
them the main ingredients in a proof do
you offer and enforced the first thing
is to appeal to a theorem that actually
turns out to be only required about two
lines to prove and what that theorem
says is that if your basic weak learn is
consistent if your three is consistent
then the forest is consistent when I
first saw this theorem I thought oh
that's going to be so easy to prove
results for all sorts of random forests
and then I realized no one had ever
proved consistency for a tree so the
decision tree is that that's even more
shocking decision trees have existed for
such a long time and no one's ever
bothered to actually show that they're
consistent now when we do the other
thing that we need to prove is that if
we separate structure from estimation if
we introduce if we're creating two
streams and we have this conditional
risk we want to show that but if the
conditional risk goes to zero then our
original risk also goes to zero this is
very easy to prove it's again about
three lines and I can go over these
proofs with you later and so and that's
just as an exercise of you build the
joint you take a double expectation you
use the fact we put assumptions up to
make sure that these guys are bounded we
use Dominator convergence the classical
limit to swap limits with integrals and
that's that's it that's the proof
what it gets interesting is the last
step of proving one of these results so
for the single tree now we know we
separate the structure from estimation
and for the single tree in order to
prove consistency we can resort to a
huge literature that is out there on
consistence consistency of nearest
neighbor classifiers in particular if
you think of classifiers and regression
functions as well the intuition here is
that each leaf is the seneschal
neighborhood as you as you're splitting
the space you're just creating these
neighborhoods now what you would want if
you wanted to get a perfect fit to the
function is that you would want these
neighborhoods to shrink to 0 ok so if we
have any leaf so I'm gonna make up this
condition that the diameter which is a
measure of the size of the leaf must go
to 0 I also wanted as the size goes to 0
that the number of points still is large
enough that I can get a very good fit so
I need a number of estimation points to
go to infinity now in order to establish
these conditions all we have to do is
say that a leaf will not split until a
particular parameter of the algorithm kN
and until there are that many points
until there are K and oops sorry until
there are K n points in that leaf we
want split this is sort of already
proven because just by having this
condition and letting this parameter go
grow with the data then we achieve this
result so in effect we only have to
prove this that the diameter shrinks to
prove that they are diameter shrinks the
whole space shrinks you only need to
prove that one of the dimensions shrink
that's two pages of analysis
which I'm not gonna bore you with but
it's sort of it's becoming kind of
standard because there is a sort of a
general formula for doing this and then
whenever we get a new algorithm we just
need to rework the proof a bit to be
able to get the sort of thing so the
theoretical front thanks to tools
developed by George II and Roy and the
Gossie we were able to actually now do
this but the key trick is that all these
theorems from this literature rely on
the fact that the structure has to be
independent of the estimation and by
doing the splitting of the streams we
can reuse some of the ideas there now
what are they how does this manifest in
practice so here there are three in red
green and yellow there's three
theoretical random forest algorithm so
by theoretical random force I mean our
random force algorithms for which we can
prove consistency and the one at the
bottom here is primont now Bremen still
does better than any of the methods that
we have the goal has been to narrow this
gap and so there are P ow and looked a
variety of or Lucas was part of this too
they prove this first result which they
published in general are more recently
VI was able to come closer we're
currently sitting here but I think we're
getting very close to prove to have
theory for something that's very close
to what's used in practice these are
simulated simulation results so that's
with some you see I take one UCI data
set here is another we've done this for
many years I did this as always observed
the same thing where the sort of
Brahmins are down here hours are here
and then the B ahoo which are the other
best results out there are up here and
so we observed that for another data set
difference is the dual stream yes and so
we've been studying how how much
difference is the to streaming causing
how far is it separating us
it turns out that that we then have been
trying different permutations what if we
took primont with the exact same split
because all of these have also slightly
different splitting strategies and so
we've been sort of playing a bit and in
fact there's some versions here and this
might change because my students are
working on this as we speak they're
trying to get this particular version to
say well there's some particular
versions where if we start introducing
our ideas from splitting that we serve
came up with following the theory we
start doing better than the ten to
Brahman but for that one we have no
theory it's there so there's a whole
story that comes in with that kind of
randomness which I've obviate we do a
form of bootstrapping as well because in
our splitting mechanism we only use a
subset of the points so we're doing some
form of sub bagging but we haven't
actually so we do have a form of sub
bagging but Brian does actually do the
bootstrap and he doesn't sub bootstrap
he actually uses the whole thing Antonia
has this beautiful picture in his book
where he shows that the advantage of
that is that if you have outliers you
can be a bit more robust you don't get a
max margin classifier but you become
robust to outliers so bootstrapping can
help you in those situations in for our
particular way of doing things it will
not necessarily helps us with if there
are outliers so we if we introduce a
data set with outliers I think the
difference is going to be even bigger
without a tip Misha Ryan Barrett I
believe so
or the new merge trees you're using and
also what you're having Theory right so
we've tried to use the same we kind of
actually show here that they session
more trees it's what you should be doing
this is actually increase the number of
trees many treaties you can afford any
stop when the results don't
he doesn't make lots of suggestions
which in our experiments we've been
trying to follow as much as possible and
of course here is the sort of con come
on come here without this application
we've also been running this very
recently on trying to predict joint
joint locations given the class labels
and they're essentially we see that for
this type for this application be au and
2012 and ours are pretty much the same
we still sort of age it a little bit and
again the original employment version of
run force is so bad
again the go has not been to outperform
priming the the goal here is how can we
narrow the gap between the theory in the
practice that is this algorithm out
there that is extremely hot there's
three simple two that everyone uses for
which we have no theory can we at least
buy it can you atleast come closer to
that algorithm and eventually get
something that performs just as well and
for which we can develop the theory
we're so not yet there but I think we've
taken steps to do that and then one
thing we've been able to do where I
think we start departing from the state
of what exists is for online random
forests has never been proof Ida and so
we've actually been able to use the same
sort of ideas with the few complications
and that have to do with how online
trees have to keep statistics in order
to decide where to grow or to stop
growing but if we have streaming data we
can still get the same type of
convergence consistency now and they're
the type of things we had to do in the
theory and fortunately I was on the
train and I still don't have wireless so
I couldn't put a picture here which was
going to show that we beat the state of
the art in terms of algorithms so I'll
just tell you we do beat the state of
the art and so it was one of those cases
where by actually considering the the
guidelines of the theory we were able to
create an online random florist that is
better than any other online reinforce
we know off so that's essentially what
where we are with random force and
obviously there's lots of things to do
still I think we would love to get
finite time concentration bounce and I
think some of the things we've done
hopefully laid some foundation toward
getting that we would like to then
attack all these more complex models
that are and Jamie and Antonia and of
your others have been developing and so
I think there's a lot of cool work
happening now at this interface between
theory and practice in random force and
I think we're coming closer
so very quickly another topic that I've
been working on with that are planning
to continue working on and this I do not
what extent folks you will be interested
in for that I did want to explore it
because I think it's a fascinating thing
so max walling came up with this
algorithm called heard it what hurting
is is basically a two-step greedy
algorithm where if you want to suppose
you have some statistic mu and you want
to approximate it by taking samples so
you have some sample statistic you do
like an ergodic average and you want to
approximate it so mu could be a
histogram and these could be just
indicators of which being you fall in
and then you're using this empirical
histogram to approximate this
multinomial distribution the algorithm
is greedy you don't need to know the
horizon and it's just basically involves
a dot product and then something that
looks very much like an online gradient
descent with a step sizes one so you're
trying to match the statistics and the
algorithm has some parameters W it's
completely greedy and the reason why we
like that algorithm is because if you're
approximating this the convergence rate
of this algorithm is 1 over T as opposed
to a Monte Carlo algorithm that would
have a convergence rate of 1 over square
root of T so it's quite an improvement
and you can see it in practice so here
for this for a model with two variables
I've run the stochastic algorithm which
is in blue which is a gift sampler and
then in red is an extension of hurting
to MCMC algorithms which we call hurted
Gibbs and it converges much faster and
for some examples like image the noising
it again outperformed escapes it does as
well as another deterministic friend a
mean field accepted min phil has a fear
free parameters and if you don't know
how to tune those you can't match the
performance of gibbs hurting and the
application we were really interested
with with that we think it's sort of the
sir would be the sort of the killer
application for an algorithm like this
as named entity recognition with or
coreference resolution with the CRFs
so in CRFs for named entity recognition
sometimes it's important to have these
edges that create is that model long
dependencies if you don't call reference
resolution that is you want to know
obama does not like the republican party
he said that so you need to know that
that he is
referring to Obama and so you need these
long dependencies and that causes
trouble for algorithms like Viterbi and
so on and the stanford named entity
recognized implements something called
annealed Gibbs to be able to to solve
this problem so we just took their
recognize and we replaced annual gifts
by hurting and then what we find is that
in but the error fold again
deterministic version is or in this case
actually precision is higher than the
precision of the anneal Gibbs which is
the state of the art in half the time so
it's a much more efficient algorithm so
this is exciting because we can go from
a stochastic algorithm to determine
stick algorithm get an algorithm that
converges much faster and you turn Chen
proved that this was able to actually
prove the theoretical rate of
convergence Newton is now working with
Zubin one challenge like how can we keep
extending this result and there were
some limitations with this theoretical
result and it would be interesting to
this would allow us to move to sort of
push the frontier for our graphical
models we can do in first efficiently
eventually there are limits so there's
this theoretical bounds on what you can
do with it and risk algorithms versus
randomized if you're in a computer
volume of convex parting high dimensions
then provably there is no deterministic
algorithm capable of approximating that
whereas there is a stochastic one that
can do it in polynomial time but so this
is sort of very interesting interplay
between randomizing algorithms and D
randomizing when should you randomize
when should you randomize and that's
sort of a I think a fascinating area of
research another problem that arises
with graphical models is
learning the parameters not just doing
inference but in order to create those
CRFs I also have to learn the parameters
of the potential so that I can do all
these sort of natural language
processing applications and vision
applications and so on now there is many
CRFs may type of Markov random field out
there that people use the physicists
love the sizing models near scientists
use them to Gaussian graphical models
are used a lot in epidemiology log
linear models are classical models and
statistics and now in in machine
learning this sort of conditional log
linear models or conditional random
fields have become very popular in a
multitude of applications ranging from
biology to to language division and so
on typically what we would like to do
and to learn the parameters is to
maximize the log likelihood now the
problem with these models is that when
you write down the log likelihood the
log of the normalizing constant which is
intractable requires an exponential sum
in order to get it appears in the
likelihood so we can't evaluate the
likelihood and moreover if you want to
do gradient descent to optimize this the
gradient also depends on the on the log
of this intractable normalizing constant
so we need to be able to approximate as
expectation now there are some cases
where you can do this expectation so if
you have a chain an hmm or a common
filter in the Gaussian case you can do
this if you have a tree you can do this
and in fact if you have a generalization
of trees which it graphs with what this
comes of the concept called log tree
with you're able to actually do
approximate this with something called
the junction tree algorithm we decided
to attack models of this structure
grids now for a grid if you look at the
tree with of a grid with that is
K by K note the tree with us K so the
fastest exact algorithms that we have
our exponential entry with and so their
cost would be 2 to the K so exact I mean
I'm after exacting inference here I no
longer want to do approximations so 2 to
the so with the best that we have now
inferences 2 to the K and I'm going to
show you a trick that allows you to do
the interest in K so sorry and learning
go ahead in planar graphs is basically
tractable in certain cases it's in
certain cases that's true it's not
always so basically you do reductions to
sat and to scientists tractable max at
is intractable for the general potential
cases and then anything above 3sat is
intractable people assume that you can
compute marginals in planar graphs in
very restricted cases so for example if
you have the potential all being
positive you can do that for the general
case for example if you under my if you
want to find a map estimate of this and
then you don't have and you have
positive and negative I think there's
also there's one sub case that way you
can do something good with me and feel
but in general in that case that maps to
two sat - sorry max 2sat and max 2sat is
np-hard so in the worst case inference
in these models is np-hard and I'm gonna
show you soon this I'm gonna give you
the lattice as an example but soon I'm
gonna move to more interesting models
for which I will still make the claim
that it's linear let's assume that this
is not one of the trivial models that
you could use an exact inference so that
these are arbitrary potential
for example and that they're not
necessarily Gaussian and the positive
and negative so you have a frustrated
system so even with a simple cycle you
already would get yourself in trouble
because it will be frustrated now what
we're going to do is century um so why
does the junction three work the
junction tree which is the sort of the
most popular method for inference it
attacks an np-hard problem but it
introduces a new concept called three
with and then inference becomes
exponential in the tree with as opposed
to exponential in the size of the model
so if we could come up with a different
concept
other than tree with where the algorithm
would still be exponential in that but
that other concept happens to have a
size that is lower than three width then
we will be able to you know have a new
class of problems that we can attack
efficiently and so the concept that I'm
going to introduce is the one
neighborhood so what's the one
neighborhood a one neighborhood is the
union of all the variables are involved
in a clique that I'm trying to estimate
and the neighbors of the clique or
imagine that the clique of interest is
the clique 7:8 that I'm trying to whose
whose parameters I'm trying to estimate
so it's shown here in green the one
neighborhood then will be essentially
the variables that intersect that the
cliques whose clicks intersect with this
so four seven five eight and nine eight
so in other words these variables here
I'm also going to define the cliques
involved in this neighborhood and call
them F so their cliques four five and so
on there are some other cliques that
arise due to marginalization now assume
that you're trying to compute the
marginal and on the one neighborhood you
do that by integrating over all the
variables other than the variables
involved in the one neighborhood now
this marginal has a particular form it
has as factors the potential of interest
the other solid
just in the in the picture that I just
show these solid edges and a term that
does not depend on the potential of
interest and that's gonna be surf the
critical thing do the guy that interests
me appears by itself moreover when you
try to prove that the exponential
representation of a graphical model and
a Markov representation of a graphical
model sort of global and local views of
the same model when you do the mapping
by is something called the Hammersley
clifford theorem you you as part of the
proof you need to construct this mobius
transforms and so on and then one of the
things that comes up is this question of
uniqueness of uniqueness and existing
existence of potentials so it turns out
that there is one particular type of
parameterization of the potential called
the normalized or canonical potential
and it's essentially you could normalize
it to any value but you could just
normalize it by saying the potential
will be zero whenever one of the
variables in the potential is zero and
it's very easy to construct this an
Ising model so we'll already be in this
form if you write the potentials for
your model in that form then this guy's
unique and because this guy is unique
and it appears as a factor here and it
appears as a factor in the original
model we can essentially just consider
the variables in the in this one
neighborhood a q so if you have a big
table of data instead of considering all
the million variables that you might be
trying to analyze you just need to pick
the subset of the variables that you
care about that are in this one
neighborhood for the clique and you do
maximum likelihood there and that's the
algorithm so all you have to prove is
that X of a Q is a sufficient statistic
for the parameter of that potential and
then we do that
so basically oh and this is the sort of
an example of this so basically the
algorithm
you construct the white neighborhood for
each potential you do it separately so
the algorithm is parallel you construct
an excellent MRF by that I mean just
that collapse that marginal we need to
know something about the structure of
the marginal and then you basically just
do maximum likelihood on the reduced set
and then you just read off the potential
and then this theorem here for those of
you who have the patience to read it
quickly is basically saying that that
neighborhood one neighborhood is a
sufficient statistic and so now if you
have models like these so it then
becomes interested to know when is this
sensible idea if you have a restricted
Boltzmann machine this or bipartite
graph then this would be a terrible idea
because the one neighborhood in that
case is still the full graph so the full
set so the algorithm would be
exponential and there's no hope there if
it's a chain the algorithm the one
neighborhood is small but again why
would you bother to use this algorithm
when you can just use hmms and so on
perhaps one advantage that this
algorithm is natively parallel if you
have a grid then this is linear in the
size of the grid whereas the except for
the restrictions the closest that we had
before was was an algorithm that was
exponential if you have a chimera
lattice how many of you know what a
chimera I have seen chimera lattices
before okay there is a company in
Vancouver called d-wave systems they
build quantum chips and what they chips
do is they draw samples from these
models but one of their challenges is
how to estimate the parameters of these
models because as their chips been
growing over the last few years and
they've been publishing these in nature
so they have some credibility as the
chip grows estimating the parameters of
that the Machine doesn't do the machine
just as the inference with the quantum
machine but it doesn't know how to do
the learning but fortunately for them we
can use this algorithm and convince them
well we actually took one of my students
is working there now and we're talking
about being able to do this to solve
that industrial location and of course
then you get these extremely hard
problems in physics like cubes with of
frustrated potentials for which we know
that inference is really really hard for
which we can still do the learning and
for this particular model here is the
comparison of the computational time for
a particular precision the this is just
showing what the theory says this model
is linear the junction 3 would be
exponential in computation I wish it did
but now we do need to pull off these
potentials separately if we do it it
would be we could maybe use this
intuition to build an approximate
algorithm and that's one of the things
that I actually want to be doing over
the next year or so is to try to see
okay so maybe we this algorithm will not
always give us the maximum likelihood
estimates but when it gives us different
estimates how good are they and for
Gaussian MRFs we see that it actually in
terms of relative error it actually does
better than Junction than using the
junction tree now a lot of people are
surprised by this that you can do better
than the exact inference with the
junction tree the promise of junction
tree is not exact always it requires
that triangulation is np-hard and if so
Junction tree is and even if you have
the optimal triangulation then there's
still a question of whether the cliques
are in this capture the model class of
the cliques of the original model so
there's some subtleties that actually
often people cite Junction tree is
giving you the exact inference but it
doesn't really always
all right so very close another topic
that I'm interested in is deep learning
so this was the state of the art of
speech a few years ago and now it's a
lot of cool you know you can actually
order figure out what a restaurant is
provided you don't use Siri and the
pipeline that's been used to do this at
least for the features up to now is
something that I mean Jobs Dow who
worked at you with you guys in Redmond
was working on with like leading and
other folks to be able to build the
speech recognizers of Microsoft and it
uses a deep net now the kind of work
that I've been doing recently with deep
nets and this also worked with Misha
denno who is the guy that's been doing
the random forces who I'm hoping will be
here next early next year doing an
internship we've essentially done the
following if you take a receptive field
in a deep net you have these receptive
fields for your data either your speech
or images and if you throw away most of
the parameters and you use the just a
few of the parameters in a predictor say
a Gaussian process predictor you can
actually reconstruct the parameters so
in deep learning the the parameters
match the statistics of the world so a
receptive field which essentially in
Europe in Europe and v1 must capture
something about images in the world the
whole idea is that if you have a
thousand by a thousand images the set of
possible images is 256 ^ million and we
would never be able to do computer
vision if you had to consider the
worst-case set of the worst-case set so
instead we assume that we're going to
match the natural statistics of the
world
now since the parameters have structured
the basic idea is I don't need all the
parameters we only need a subset and
given that subset we can predict the
rest so we've implemented this and we'll
be presenting it this year at nips where
we actually just use
some cases 10% of the parameters in a
deep model and we only do gradient
descent on that 10% and we do that for a
range of models convolutional
architectures you know basically
whatever you do using deep learning this
applies and then we're able to actually
predict the remaining parameters so you
actually don't need to keep copies of
the full set of parameters and this for
parallel arkad if you have to distribute
the parameters of a service as Google
does this actually is a useful trick
what interests me next in doing next
with this is that if you have much fewer
parameters that means you instead of
just two ingredients you can start doing
other things like variational base and
even HMC and so I think we finally are
coming closer to having Bayesian
versions of this which hopefully in
practice will will work better so I'm
just not going to go into this in detail
I'm just gonna mention what the problem
is so a lot of problems that this is
sort of a big area of my research some
of you where it is worship a couple
weeks are going already sort the next
part of my talk so a lot of problems in
in the world out of this nature you want
to maximize a function but you don't
have access to the function the
functions are black box so if I want to
interact with the user number program I
don't have a mathematical model of
Antonio so all I can do is ask and turn
you a question probe and get a reply and
then a lot of parameter tuning we get
grad students to tune the meta
parameters of the algorithm until it
works well essentially our grad students
are the black boxes that are doing the
optimization for us if you want to
deploy webpages massive scale and do
analytics that is you want to
automatically configure a game or a
website to best match what
users want or to maximize the revenue
for the content providers and then again
you're doing this is in a black box
setting so there's a huge range of
problems that are of this nature I'm
just gonna I'm not going to go into what
we do but I'm just gonna leave you with
one image and this image is this one and
it's based on something that actually
Subin said in a meeting earlier this
year and I said we should be thinking
like in the world of big data and so on
in trying to get automated statisticians
you know sort of something that the
automatics that decision I think this
what do you call it I think I'm calling
it the automatic machine learner
inside of that and so the idea is if you
have a tool box like in this case that
there's a regression using a bunch of
methods and you give me a data set then
I would like to automate it so in this
case there's a psychic learn
I would like to automate scikit-learn so
that when you give it a data set based
on the features of the data set it can
automatically decide which method is
should try and what the parameters of
that method should be and that to me is
sort of that one of the things for which
the sort of black box optimization where
black box optimization can make progress
if you use frequentist techniques here
you get into trouble because sometimes
the number of arms is too large and
frequenters techniques require that you
pull each arm and they're just not
applicable but if you use Bayesian
methods and you can model your problem
well you can actually use your model
correlations and so on in form of a
prior to actually do this to actually
get applications that work in practice
that allow you to efficiently automate
any type of system whether it's a
compiler whether it's hardware whether
it's your machine learning toolbox and
so on so with that I'll end my talk
thank you
thank serving so I think I know because
I've seen this bit but baby you wanna
tell us split
oh yeah Matt Hoffman it's in Cambridge
you know oh ok so you have several
several regression functions here in our
PF it's much here no he's probably in
the lab working on a page so you have
several methods and for each on this
axis over here so so these are in four
different colors mean different types of
random forests different types of linear
SVM's and I think there's kernel kernel
oh nice SVM's and then nearest neighbor
regression functions and la su methods
different types of random forests with
different parameters so each of these
guys is one method and so what we're
seeing here is that so we take so often
in practice what you do is you take your
data and you do say break it somehow
save your doing five for
cross-validation and so on you train you
in one set of the date and you test on
another set of the date and this all
makes sense when your data set is not a
million points because cross-validation
takes a very long time when you're doing
if you're gonna consider all possible
models and all possible parameter
settings of those models doing
cross-validation is really prohibitively
expensive so so what this does is it
tries to minimize the number of steps
you have to do cross-validation so you
would run all the methods initially but
what you hope is that with time it's
picking the models are the most
successful so
even before you have to consider all
models in all possible faults you'll be
able to have the winning model that's
essentially what we're after working on
bandits with structure down spaces right
where the arms are basically the arm
space is huge and basically there is we
assume certain structures and this sort
of reminds of the work that was being
done at on Gaussian processes where
people were looking at what were the
sort of correlations between what was
the sort of pavement functions that we
should be using for washing processing
right so isn't the sort of so the first
question is isn't the problem then just
in defining priors over these spaces
which would lead to better sort of
probing and secondly the second question
is in terms of really interacting with
users the thing what happens is users
change over time the model that you're
interacting with that is actually
dynamic itself right so you might be
sort of we might be sort of trying to
infer what is that really the latent
sort of curve we are sampling from but
then it is changing over time so do you
know of any work which sort of looks at
it from the point of modeling dynamics
as well
yeah this probably being several works
on this I know look born at Harvard has
been working either if he I think his
paper was rejected but he's he's working
on this yeah he has a I think a smart
idea for doing dynamic Thompson sampling
in terms of something you can do with
you the GP model a lot of people use GPS
I've used a lot of GPS I also did some
work I see on dynamic GPS for animation
for for this type of problem exactly
what you're asking but but jeebies are
very nice a very simple models I mean
there might be hard to explain to people
in the beginning but once you get them
they're actually super easy to implement
very nice we're working with decision
problems so a number of data is not an
issue it's the opposite just having too
few data destination but GPS on the
other hand that's just one of the
possible models you could use if I mean
so at one extreme you have your modeling
using nonparametric methods with GPS to
to model the arm structure but then at
the other expert then you could have
logistic arms you could have then at the
other extreme burner bethe Bernoulli
arms in which case we're back to the
bandits terrorism but but I think
there's a continuous ranging here and I
don't actually see any difference
between the bandit problems and what we
call typically based in optimization
because it's just a question of changing
different models and different utilities
at the end of the day this is all
maximun expected utility and there's
different utilities like Thomson UCB
expected improvement and so on and then
there's different models and it's just a
question of mixing and matching them but
the two communities have done different
things and there's been a lot of
nowadays I think they're converging
together
if you're going to nips go to the base
in optimization workshop and the second
day work but essentially the other thing
that's coming in is all the work on
experimental design because people are
developing things like the knowledge
gradient and so on but that's really
just optimal designs that people had
done what's his name there's a beautiful
base in Buckhorn and the foundations of
Bayesian statistics that actually covers
this problem as well and the in the
theory we've had a lot of very nice
regret bounds for cumulative regret and
for some of these work we also get
bounds that tell us that not only that
the cumulative regret vanishes isn't
totally but do you actually get the best
arm that you do the optimization in
other words the theory still does not
reflect the fact that we do better with
the Bayesian models there's a huge gap
so when the one of the best techniques
out there right now is something called
Thompson sampling and it's
mystic that goes is almost 100 years old
and there's some theoretical results but
there's this huge gap still between the
theory and what we observe in practice
so that's another area that's kind of
like random forests with this we do much
better than those racing algorithms yeah
with the hosting races and so on the
newer techniques have definitely
superseded we have twenty five yeah
here's essentially bunch of these
methods with the you have different
utilities this is the GPU CB with this
is Thompson then again there's all sorts
of things that's coming to the picture
like are you interested in just finding
the best arm or minimizing cumulative
regret or do you have a finite number of
arm poles that you're allowed or you
know there's all sorts of constraints
for under which different models are
preferable in the end you want to have a
good commanding understanding of all of
these but through the the the hoof leg
racers have been superseded by a lot of
techniques they no longer</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>