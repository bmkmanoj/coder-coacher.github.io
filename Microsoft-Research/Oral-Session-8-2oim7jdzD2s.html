<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Oral Session 8 | Coder Coacher - Coaching Coders</title><meta content="Oral Session 8 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Oral Session 8</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2oim7jdzD2s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hey okay hi everybody welcome to session
number eight we have an outstanding
paper award in this session so stay
tuned for that and also I'd like to
request that if any of the speakers in
the session are not sitting in the front
row that they come up and join us right
up here so it'll be easy to transition
from talk to talk and without further
ado I'd like to introduce the first talk
so it's entitled from bandits to experts
a tale of domination and independence
and this is by naga alone Shimon sewer
Nikolas a bianchi and Claudia Gentilly
and Nicola will be giving the talk thank
you um my voice used to be better than
this but yesterday was even worse anyway
I hoped I'd make it Italian to the end
of the talk bear with me I'm sorry okay
so um this is about a model of learning
which is called non stochastic
sequential decision-making it's you can
view it as an abstract model for online
learning and there's a player that
repeatedly chooses action from a set of
K available actions and so this the
learning proceeds in steps at each step
T nature assigns a loss to every action
in the set of K actions available and
this assignment a hidden from the player
and then the player picks some action XD
maybe the green one over there and
possibly using randomization and it will
the player will incur the loss that
nature had to sign it to that specific
action afterwards the play will get some
feedback information now there are two
diff there are different models for
getting feedback the first of all is a
bandit observation they learnt the
player only sees the loss of the action
that he actually chose whose loss is a
pain and in the expert model or absurd
expert typist observation model the
player observes the loss also of the
actions that haven't been selected so
all the losses are revealed to the
player in the next round the next play
nature assigns a fresh
set of losses to actions which again are
hidden and the player keeps on choosing
and incurring losses so if every series
of play is done either in the Bandit
model or in the expert model so either
too and in either game the Bandit game
or the expert game the goal of the
player is that his total loss must be
close to that of the single best action
and here is very important that we have
no stochastic assumptions on the way on
the process on nature that is assigning
losses to actions so this is very
important there is no probability is the
only probability we'll we'll see here is
the randomization of the player okay how
does the player how is the player
evaluated in these kind of games is
evaluated according to regret okay you
regret is a function of number of tea
place is just a difference between the
total loss of player so this is the
action it shows the time T and this is
the loss associated with that action
with some other over time steps and then
we take the expectation over the
possible randomization of the player and
we subtract to it the total lost of the
single best best action so you see here
we have the minimum of action of the
total loss of action a and since we
assume that losses are following a
bounded interval this quantity and these
quantities are both linear in T so their
difference is also at most linear in T
so the regret can grow at most linearly
in t even if the player plays at random
and there are some known results for
this model so for the Express model the
hedge algarin achieves regret which is
sublinear in t as square root of T and
grows as the log of the number of elva
vailable actions in the harder model for
bandits where you just observe the loss
of the action that you played there is
another algorithmics tree that whose
regret is bounded by square root of time
time scale and K so you see there is an
extra factor K which accounts for the
reduced feedback information that the
player gets in the Bandit model this
algirdas model is learning models this
sequential decision
is were pioneered in the 90s by Manfred
bar motor sitting over here Nick little
stone Peter our of Sharon you are friend
and myself and these bounts are not to
be tight only you can shave shave only
the log factor here in the bandit.bound
by using a more sophisticated version of
x3 now here in this talk we are going to
look a very nice model that interpolates
between what did I do okay let
interpolate between the experts and the
Bandit models so suppose for instance
that you want to apply this abstract
model to a recommendation system don't
do it but suppose that you want to do it
and so suppose that you are selling
items like cars to incoming customers so
now selecting an action means
recommending the associated item to the
to the customer so for instance if you
select this item here you will recommend
some discs are to the customer and if
the customer likes it it is likely that
it will also be interested in this other
car which is very similar to this one so
you can now assume that every time you
get some feedback for this action he
also we get some information about the
feedback that you would have gotten if
you had recommended this one okay so you
can establish a relationship but
represented by this age the relationship
can be also directed so for instance if
you are recommended game consoles
electronics let's say and if you
recommend the game console and the user
is interested in game consoles then it
might be interested in high resolution
cables to connect it to the TV set the
other way around is less likely so you
might have it some directed the
relationship between actions here okay
so um so now you can take your actions
that we saw before and build a graph on
them where the edges corresponds to
similarities between actions so this
isn't the case of undirected
similarities alright so now we can play
the same game we played before but with
the graph under underneath so now what
happens suppose the player chooses this
action the green one and now he will let
be
even this the neighborhood of the action
he has chosen okay so you see now that
this model interpolates between experts
and bandits in the experts that graph is
a click no matter which action you
choose you are revealing the losses of
all the other actions in the bandits is
just a naturalist graph no matter what
which action you choose your only you
only observe the loss that you are
actually occurring alright so now we can
sort of a study these intermediate
models and it turns out the undirected
case the crucial quantity is the
independence number of the graph which
is the size of the largest independent
set so this is the largest set of
vertices such such that no two of them
are connected by an edge all right and
mine orange Amir showed using the ELP
algorithm that basically you cannot
obtain a regret bound of this form where
this is the independence number of the
graph and you see that this bound
interpolates between the experts bound
where you have a clique and independent
number of the clique is one and the
Bandit case where you have an edge list
graph and the independent number
independence number of that is K so you
recovered both bounds however the
algorithm is kind of complicated it must
solve a linear program at each step and
other interesting features of this
argument is that the result holds also
when the graph changes over time so the
set of the associations between the
similarities between actions can evolve
over time in which case you have the
Justice some in the regret bound you
just have the sum of the independence
numbers for the single graphs ok so our
results build on that and ok and we
first of all for the end Raticate and
unrelated case we obtain an algorithm
which is as the same regret bound as the
menorah and shamir arguing but it
doesn't need to solve any linear program
and kind of surprisingly it doesn't even
need to know the current graph the
current similarity graft of among
between actions before making
predictions any prediction
so it doesn't it only needs not the
graph with among actions after a
petition as with made they know the to
update the to refine the probability for
the probability distribution of
elections so we'll come back to this be
afterwards the second set of results is
for directed servation graphs and we
have a second are green and the elected
observation graphs are harder because
then the undirected case because in this
case the player receives less
information then directed case because
the orientation of edges reduces the
feedback information we see that in a
moment here yet we can prove that the
regret get only worse by logarithmic
factors so we have reduced it
information but we don't pay too much in
the regretful that however in this case
the observation observational graph and
must be known before its prediction
because it is needed in order to compute
probability distribution with which
which the player chooses the action okay
so let's see Freddie undirected the
observation graphs the player strategy
is very similar to the x3 Algren so the
player puts an overwhelming probability
of picking and the action who's a
cumulative estimated loss in the past is
minimal okay so this is minus minus here
and the estimated the loss is computed
according to important importance
sampling just like in the x3 algarin and
just here in order to complete this
probability which is the probability
that the lawsuit observe it you need to
know that the structure of the graph so
before in order to choose in the action
you don't know to you don't need to know
destruction of the current graph but in
order to update your estimates you need
to observe it okay and in the analysis
okay devices is kind of similar to the
standard bandit analysis and you end up
with the forming which you have
disassembled conditional probabilities
which is the key element and a very nice
result which was already proven by Samir
a menorah and we reprove on a purely
combinatorial basis is that you can
upper bound the dis quantity by the
independence number of the graph you
know it's very easy to see that by
choosing data properly you can obtain
decided the regret bound okay so you can
check special cases that is this
quantity in the Bandit case amounts to
one so that when you sum it up you get K
which is exactly the dependence number
for the agile s graph and in the case of
experts you get adjust the probability
of picking the action so that when you
sum it up over the action you get one
which is the independence number of the
clique so this is all nice and you have
a complete characterization because this
upper bound is tight you can prove a
matching lower bound up to logarithmic
factors so in the directed observation
graph things becomes more difficult so
here you see whenever you pick and I
whenever the player picks in action he
gets the loss of that action and then
observes the losses of all the actions
that are reachable in the neighborhood
by the action he chose so free so this
one is not observant because it is in
the neighborhood of the undirected graph
but not reachable by the chosen action
sick this one ok so the feedback is less
with respect to the same graph without
orientation so and this shows up in the
analysis indeed that you see that you
cannot control this quantity which was
key in the analysis of the underrated
case by the independence number of the
graph okay and there is an example so if
there is a click and you can make by
orienting the edges you make you can
make a total order on these sequence of
actions in this case this is a click the
independence number if you ignore edge
orientation is one but there exists the
distributions over actions such that the
the crucial quantity that we want to
control is linear in the number of
actions even though the independence
number is constant so now you need
something here in order to to do this in
order to control this quantity and the
key trick we use is a dominating set so
we go from independence to nomination
rather than from domination to
independence I'm sorry so it's kind of a
and so what is a dominating the
diminutive set of an oriented graph
that's the smallest set of vertices
is such that any other virtus vertex not
in that set is Richard by at least an
edge of the vertex in the dominating set
okay so this is the smallest dominating
set for this graph now the if you
introduce exploration in your Al Green
over at the dominating set of the graph
this exploration is will help to
increase the probability of observing
the losses of actions because
essentially you you give a fair chance
of observing the loss of any action in
respect to to the structure of the graph
and this is the essentially the chick
that is that is done by performing
exploration over the dominating set so
essentially you read you increase that
the conditioning event over here and so
this conditional probability goes down
and you can control it better and the
dominating set you don't need to compute
it exactly you can just take a great
approximation and you are then you are
off by a lot factor which is essentially
okay in the regret bound so now there is
a very nice lemon you remember in the
undirected case this quantity here was
controlled by the independence number of
the graph in the directed case if we
constrain the probability by introducing
this exploration over the dominating set
we are able to control this quantity
essentially in the same way though we
introduce this additional log katie
factor here alright and the proof here
uses the torrents theorem which relates
the independence number of a graph to
its density and you can see no gallon
spirit in this pro fear okay again if we
plug this in the previous analysis and
there is a bit more work to do and this
gives us a regret which is very similar
to what we had before it just works by
logarithmic factors good so conclusions
so we saw that okay there are two issues
over here that are interesting first of
all the lack of feedback is caused by
edge orientation that costs you only
love factors in the regret because of
this trick of using a diminutive set for
performing exploration and there is
another
issue here in the undirected case you
can get away making prediction without
knowing the structure of the graph in
advance we just even the graph even when
the graph evolves over time you can make
a prediction without knowing the graph
you just give it you're given the graph
after the prediction in order to update
your probabilities and if you try to do
this in the directed case then you get a
much worse regret so it's not clear
whether this observe observing the graph
is necessary in the directed case in
order to get a tight regret bound at
least within logarithmic factors okay so
this is an interesting open problem
thank you I'm done ok we have time for a
few questions all right thanks ok so if
you with a graph that has a very large
independence number but there is a
single action that's very well connected
and suppose that one is the best does
your algorithm kind of benefit from that
you mean in the directed case in either
of the two so in principle oh yes I mean
it okay then the independence alright so
the it's a scan over if you have a very
well connected action independence
number cannot be too alka high most
interesting in the directed face down in
the dirty case then that action will be
the dominating set most likely I see
hi yes it's just a quick clarification
question do you assume that when if
there is an edge when you receive full
feedback for all the neighbors your you
receive correlates I'm so some notion of
correlated feedback okay in there you
can in this in this setup you see the
losses that were assigned to the
neighbors exactly you can assume that
you just see it's a stochastic signal
which is drawn from a distribution
formula jf results for that case um no
but I don't think it's a problem we have
results from random graphs but not for
random observations but I don't think
it's a it's a big deal to get that any
more questions ok so let's thank the
speaker once more Daniel yeah I don't
tell me more hands
you
okay so moving right along the second
talk of the session is entitled a little
dimension and the sample complexity of
optimistic exploration and this is by
Dan Russo and Benjamin van rooy and dan
will give the talk okay thank you so I'm
going to start with a motivating example
which is the online shortest path
problem with banded feedback that's not
good thank you okay so this kind of
problem arises for example in wireless
network routing so we want to repeatedly
route packets through this network from
a starting node V 1 to an ending node V
12 and the time for a packet to travel
between two neighboring nodes is
inherently random but there are fixed
underlying parameters to the network
that specify the expected travel time
between two neighboring nodes these
parameters are unknown to us however
reflecting for example the fact that we
may be uncertain about the amount of
traffic currently headed through
different parts of the network but when
we're out a packet through the network
we observe the total travel time of the
path that was taken and from these
observations we can learn our goal is to
route a large number of packets through
this network in a way that minimizes the
expected sorry the expected cumulative
travel time and here we see attention
between exploration and exploitation so
by trying new paths in this network we
may be able to learn how to route future
packets more efficiently but this
experimentation is costly and we want to
manage it carefully this is an example
of a linear bandit problem notice that
the travel time along some given path is
the sum of the travel times along each
of the edges in that path we could
formulate a more general linear banded
problem as follows so it will be some
action set here script a and will
associate with each action a feature
vector 5a will model the mean reward of
an action
as the inner product between this known
future vector and some unknown parameter
theta the goal is to learn to take near
optimal actions we're the largest
expected reward is given by the
maximization problem in final bullet an
interesting feature of the linear Bandit
model is that because of the known
structure to the rewards at different
actions were able to learn from our
samples of certain actions about the
reward value at other actions and this
can allow us to learn to attain your
optimal performance while never sampling
some of the possible actions this
insight is formalized in several recent
papers that show that variance of a UCB
algorithm yields tight regret bounds of
order d square root T here D is the
dimension linear model and T is the time
horizon of the problem I'll define
regret more precisely in a moment but
for now the key takeaway is that this
bound exhibits no dependence on the
number of actions and instead depends
only on the dimension of the linear
model somehow the complexity of our
model class what we're interested in
here is understanding what happens when
we consider more general types of model
classes and in understanding how our
performance depends on the complexity of
these model classes and how we measure
that complexity so a formulate a more
general multi-armed bandit problem as
follows we want to take actions with
near optimal expected reward the
expected reward of an action is
specified by a fixed reward function
here we denote that by F sub theta and
we don't know this reward function but
we know that it lies in some class of
possible functions here script f which
is parametrized by theta we may also
have some prior distribution over the
set of reward functions and the agent is
going to sequentially choose actions a 1
a 2 and so on and upon choosing an
action will observe a random reward
that's drawn with fixed mean as
specified by the reward function now
from these observations
agent can learn over time to make
increasingly effective decisions so this
process is depicted in the diagram at
the bottom of the slide we specify some
model of the system for example the
class of reward functions and then we
choose an action we observe some reward
we learn from the observed reward we
choose another action and the cycle
continues we'll evaluate the performance
of an algorithm in terms of its regret
where we say that the regret up to time
T measures the cumulative gap in
performance between an algorithm that
knows the optimal action and always
chooses that and the actual algorithm
we've selected and the actions a one up
to a capital T that are indeed chosen
the main contribution of this work is an
upper bound on expected regret of the
form shown here in blue and this depends
on the complexity of the class of
functions through two measures the first
is the log covering number of the
function class this measure is closely
related to many notions from statistical
learning theory they from the study of
supervised learning and it's roughly
capturing the sensitivity of the class
of reward functions to statistical
overfitting but we also need a new
measure of complexity that we call the
looter dimension and this is roughly
capturing house sampling one action
allows us to reduce our uncertainty
about the reward value at other actions
we do this for two types of algorithms
one is Thompson sampling and one is used
a general UCB algorithm these are very
popular types of algorithms that are
actually widely used in practice and now
if you take this found you specialize it
to the case of linear reward functions
or generalized linear reward functions
you recover the tightest known regret
bounds for UCB algorithms that are
specifically designed to address those
problems classes and those bounds are
themselves nearly type so it's natural
to wonder
whether familiar notions from the study
of supervised learning might suffice
when considering these general
multi-armed bandit problems i'm going to
show you here through an example is that
a significantly new notion is needed so
consider the following problem it's
fiction exit sorry let's fix an action
set containing actions a one up through
a n and a set of possible reward
functions f1 up through FN now the i
function is going to take on the value
zero at all actions except the action AI
which it takes on the value one so it
looks like this picture depicted here on
the right where it takes on the value
zero for all these actions except
there's a single one in which the reward
value spikes up to one now we want to
consider a noiseless prediction problem
in this setting so suppose that we draw
actions uniformly at random from the
action set and all we want to do is
label the reward value at the selective
action well here we note that the reward
function takes on the value zero it all
except for a single action and so
already just predicting the value zero
yields a low error rate of 1 over n
theoretical results would also suggest
that this is an easy statistical
prediction problem for example the VC
dimension of this class of functions is
independent event it's just one and
other notions of complexity would give a
similar result but if we consider the
closely related multi-armed bandit
problem then we get a different story
here in order to attain in your optimal
performance we actually have to identify
which action is optimal so in this
picture here we need to identify where
along the horizontal access the spike in
the function value occurs and this turns
out to be very hard notice that whenever
a suboptimal action is selected we
observe the reward value of zero but all
except one function in this function
class agree on that prediction and so we
rule out only a single prediction sorry
only a single function based on the sobs
over
vision in the worst case we can only
identify which action is optimal by
exhaustively sample in every possible
action and therefore regret is going to
scale linearly with n in the Bandit
setting so here we seem to need a new
notion of complexity that can capture
problems of this form and we're going to
introduce that now before giving a
precise definition I want to give a
motivating story let's consider a
politician who's speaking to a group of
reporters but who would like to keep his
true position hidden from them so the
politician is going to sequentially
present pieces of information but each
piece of information has to be new in
the sense that it can't be some clear
consequence of what he's already told
them so the question is how klong can he
continue to elude the reporters so how
long can he continue before his true
position is pinned down this is a
measure of the information structure of
the problem it captures the essence of
what will define to be the ludar
dimension so to give a more precise
definition and start with an ocean at
independence roughly 42 for a given
class of reward functions well say that
in action a is independent of other
actions a1 up to a n if we can find two
functions in that reward class to make
similar predictions at these actions a1
through a n but could nevertheless
differ significantly at action a so in
this picture at the bottom of the slide
the action a 10 seems to be
approximately independent of the first
nine actions because these two functions
depicted by blue and red dots are close
together in their predictions in the
first nine actions and then are very
different in their predictions at the
10th more precisely I'll say that in
action a is epsilon independent of other
actions a1 through a n with respect to
the class of reward functions if we can
find two functions in that function
class they make similar predictions
on a one through a n overall as measured
by bullet point 1 and then differ
significantly in their predictions at a
as measured by bullet point 2 we then
take the ludar dimension of the of this
problem to be the length of the longest
sequence of actions such that each
action in that sequence is appropriately
independent of its predecessors and this
then leads to a new notion of model
complexity that we use to bound the
expected regret of two types of
algorithms I haven't really commented on
these algorithms yet but I'll do that
just briefly now so one type of
algorithm is a general UCB algorithm or
an optimistic algorithm and the rough
idea is as follows in the first step we
construct a confidence set so a subset
of the set of reward functions
containing all of those that are
statistically plausible given the data
that's been observed then we build an
optimistic model of the environment
under which the value of each action is
taken to be the best value that's
consistent with one of the plausible
models in the final step and this is
bullet point two we take an action
that's optimal under the optimistic
model of the environment and here
optimism encourages exploration because
optimistic values tend to be higher at
poorly understood actions but as we
sample poorly understood actions we
learn about them and these optimistic
estimates are adjusted this process
causes the algorithm to overtime
converge toward optimality what we do is
construct a specific optimistic
algorithm and then study its rate of
convergence I should note that there's a
very large literature on this approach
spanning multiple problem classes the
second type of algorithm we study is a
posterior sampling strategy it's known
as Thompson sampling or as probability
matching and the rough idea is to start
with a prior distribution over the
family of reward functions and then
each time step two sample in action
according to the posterior probability
that it is indeed the optimal action now
in earlier paper of ours title learning
to optimize via posterior sampling
establishes a close connection between
thompson sampling and optimistic
algorithms and it's going to allow us to
analyze these algorithms in tandem in
this paper in particular what we get for
Thompson sampling is a bound on its
Bayesian regret or it's expected regret
under the prior distribution I'll just
briefly sketch the proof of our main
result there are two main technical
steps that are needed to complete the
analysis in the first step we build
general confidence sets so subsets of
the set of reward functions containing
all of those that are statistically
plausible given the observed data and
the size of these confidence sets
depends on the log covering number of
the function class so that leads to the
first notion of model complexity in the
second step we measure the rate at which
confidence intervals shrink at different
actions as we explore and that depends
on the ludar dimension of the function
class and leads to the second notion of
model complexity just conclude by
wrapping up what we've discussed is how
multi-armed bandit problems seem to
require fundamentally different notions
of model complexity than what we're
familiar with from say supervised
learning but i think that there's a huge
value in having a unified and precise
conceptual understanding of these
problems and that a lot more work is
needed in order to reach this goal here
we've taken one step forward and I hope
it inspires a lot of further
investigation okay thank you
since any questions for the speaker yeah
a quick question how do you estimate the
ludar dimension how do you estimate it I
guess I'm not sure from data so here we
have a known class of functions so we
bound it analytically in special cases
out of curiosity you have a lower bound
in terms of the looter dimension because
there seems to be a natural strategy
that the environment can play which is
to play the function that is Lee the one
that is eluding the actions that you're
taking so it is so we don't have a lower
bound I don't think that in general and
optimistic algorithm will be optimal for
the full family of algorithms here so
there is a lower bound in the sense that
there are cases like the class of linear
models under which the bound is tight
but it's not tight for all possible
function classes so you don't think so
does that mean you have a lower bound
just one that is not tight no we don't
have a general or so for certain
function classes you can get regret
which is not in scrotie yes can you get
this with your analysis so not with our
current analysis you can with a ludar
dimension I believe but our analysis and
our confidence sets aren't tight enough
say for every example that's been
considered in the multi-armed bandit
framework
yeah sure and do you think this is
useful for problem dependent lower bound
problem dependent bounds as well I do
believe it for at least a ludar
dimension I believe ya handy yes okay
let's thank the speaker was war okay so
the next talk is entitled adaptive
mark-making via online learning and this
is by Jacob Abernathy and satiate Callan
Satine will give it okay thanks for the
introduction yeah this is an
market-making using online learning and
this joint work with Jake Abernathy who
is that mr. of Michigan okay so I'm
going to start by describing the setting
for market-making in in stock markets so
so when you go to a stock market and you
want to transact you want to buy or sell
a share it won't be really transact okay
so basically there is something known as
an order book for every financial
commodity which essentially lists for
every share how many shares of a you
know are people willing to sell for a
certain price and how many shares of
people willing to buy for certain place
okay so for example here's an order book
for the Bitcoin market Bitcoin USD
market from yesterday so you can see
that at various price levels there are
various amounts of bitcoins greater
being sold at that price and traders can
interact with such an order book by
placing market or limit orders okay so
so where do market makers come in so
market makers provide liquidity to any
financial market they provide they do so
by providing buy and sell both both buy
and sell orders to the order book so
they take positions on both sides of the
order book and the reason that I mean it
might seem like a pointless exercise but
the reason they do it is because they
always quota sell price it's higher than
the buy price okay and their hope is by
transacting with people who want to
actually buy and sell that financial
commodity they can actually make a make
a profit by looking at the spread which
is the difference between the buyer and
sell price ok so
so in this particular scenario in the
Bitcoin market for example the spread
was like about nine dollars on a price
of about nine hundred dollars it was
about 1% okay so that's so by making
many many such transactions and looking
at the spread that they have between the
buy and sell price market makers make
their profit alright so in this talk we
we consider the question of how do we
how do market makers actually work okay
so how do they set their spread sizes
and how do they make money and we
consider the question of how you know
how do we set you know what is the
principled way of setting different
spreaders attractively and we use the
techniques of expert learning or online
learning to handle back of this question
and it there are several difficulties
and actually applying standard
techniques from online learning for this
problem namely the fact that these
problems come with a lot of state
information and typical online learning
algorithms can't actually handle state
but we actually give some theoretical
results which show how to be how to
handle a state that is implicit in in a
spread based strategy and we actually
implemented our algorithms on real data
that Jake downloaded from some website
and it somehow ended up performing
better than expected sometimes even
beating the best strategy in in hand
side okay so that's the that's the plan
for the talk so to set the setting a
little bit more formally so we consider
in an online market making scenario
where transactions are always always
done at various discrete time steps the
time steps are indicated by little T
which goes from 1 to capital T before
the beginning of every time step a
market maker places their buy and sell
orders to the book then then they
observe the the current price of the
commodity which might be adverse really
set so we don't make any stochastic
assumptions on the on the price of the
commodity and then once the price is
observed the market maker observe
execute any applicable orders okay so if
the price is very low then they might
actually end up buying some shares with
the prices high they might end up
selling some chairs yeah that's how the
market maker interact
with the the market so so let's let's go
come back to the notion of spread so so
market makers want to make a profit by
quoting buy and sell prices which are
which are away from each other and a
simple way of doing it is something
known as a spread by strategy and this
is a natural class of strategies it is
parametrized by a spread size parameter
B and what a market maker does with this
purchase parameter is to specify a
window okay of price levels starting
from price level 80 or the wittle 80
plus B okay but that's the window right
there and then once the window specified
the market breaker maker places in order
to buy one share of the financial
commodity for every price level that's
below 80 and then the places are ordered
to play to buy sell to sell one share of
the financial commodity for every price
level that's higher than the right
endpoint of the window ok so the gap the
gap between the left and right endpoint
is the spread now let's see what happens
when when you actually get the to see
the price of the commodity if the price
falls inside your window then none of
these buy and sell orders are applicable
the price is too high to buy any stocks
the price is too low to sell in any
stocks so nothing happens there's no
transactions the window does not move
them were muck and make a stays where he
is another hand if the price actually is
lower than the left endpoint then all of
these 80 minus PT by orders come into
effect because the price is low enough
so at this point the market maker can go
ahead and purchase these 80 minus PT
shares ok and then he moves the window
so that the the new window exactly
captures the current price ok so
graphically what happens is essentially
you move all the shares that you just
bought to the sell side of the book and
then you move the window so that
basically your new window exactly
captures the current price ok so you see
what is happening here basically
whatever stocks were bought by the
market maker that immediately offered
for sale
price that's about B units higher then
then what they bought for okay so if if
at some point the price ends up becoming
higher than the right endpoint then all
of these shares will be sold and the
market maker can book a profit of B for
every such every such share that's
basically how they make money something
similar happens if the price is larger
than the right endpoint in which case
the market maker will sell a bunch of
shares and move the window to the right
so that it again captures the current
price okay all right so now hopefully
I've from these pictures I have
convinced you that the reason you might
want to have a large spread size is
because the buy and sell orders for for
the shares are matched essentially so
for every such matched pair you end up
booking a profit of V okay that's the
upside that's how market makers make the
profit but the downside is that what
happens if the if the price never
actually leaves your window if the the
fluctuations in your prices are very
small and the vid and the price stays
inside the window then there will be no
transaction so then there's no profit to
be made anyway so if you end up if you
have a large window then you run the
risk of having no transactions because
the price stays inside the window
another hand if you take a small window
then you might actually make a lot of
small amounts of profits that might be
good as well so there's both upsides and
downsides of choosing large and small
windows so so one way of characterizing
the the effect of having a specific
spared size is is given by this theorem
here which you prove in our paper so the
the theorem shows that I mean it's not
important to understand the expression
but it shows that basically you make a
profit of about be over to every time
the window shifts okay the window does
not sure if it will make any profit
another hand you might also end up
making a large loss if the final window
is very different from the initial one
okay so if you somewhat unbalanced your
books then you might end up making a
large loss so so that's one of the
statements from our paper okay now given
these upsides and downsides of of
different spread sizes you might ask
okay how do we select a good spread
sighs okay
so since we take the view that prices
are generated online without any
specific stochastic assumptions we have
no knowledge of how the prices are going
to evolve in the future so we don't
really know exactly how to choose a
specific spread sighs okay so you might
take the online learning approach tackle
this problem by by considering a class
of different spread sizes okay so so
there's some set of some set B of
candidates spread sizes that you might
want to use for your prize sequence you
just don't know which one is the best
one okay and a goal of an adaptive
spread spread based market making
strategy is to is to basically come up
with a way of changing spreads online as
you go along so that your payoff is
about as high as the best best payoff
that you could get from your candidate
set of sprites that's the standard
notion of that we use in online learning
and the performance metric is something
known as a regret which has been talked
about in the last couple talks the
regret is simply the payoff of the of
the best possible spread based at
achieve among gear set minus the regret
of the payoff of the algorithm okay so
now since we have such a well-developed
theory of online learning you might be
tempted to apply a simple experts
learning algorithm or something like
that but the problem is that online
learning algorithms typically assume
that we have no state in the especially
expert learning algorithms and here our
strategies have very significant state
and that state is represented by the
amount of stock they are currently
holding okay so different strategies
might hold different amounts of stock
and it might not be easy to to just
compare the amount of holding that one
strategy has versus another tradition
okay and this might motivate you to try
a different model maybe like
reinforcement learning or something
maybe online learning is not appropriate
but it turns out that somehow we can
actually handle this this problem of
state using some combinatorial
statements that we prove and we can
actually give an algorithm which has
square root T regret after after T steps
that's our main theorem is an adaptive
algorithm which get square root T regret
after T steps
okay so i'll i'll go into some details
of how we actually design this algorithm
so the question so the main question is
how do we handle the state of the
problem okay so the state by state I
simply mean the amount of stock that a
current strategy is holding that I mean
and that that amount could be both
positive or negative based on whether
they have been buying or selling stocks
so the first statement that we actually
prove is something which we call a
nesting lemma it's a very simple
statement which says that if you have
two different spread sizes b and b prime
let's say be smaller than B Prime and if
initially the window for for B is nested
inside the window for B prime then this
always happens I mean this this stays so
okay the windows never cross each other
somehow okay and the proof is actually
quite simple I won't go into the details
it's I'm going to give a proof of
picture there's a simple case analysis
you you look at where then the the price
falls and by looking at where the pies
falls you'll notice that the windows
shift in such a way that they always
stay nested okay that's nice the second
state statement we prove is something we
call the invariance lemma again it's a
simple statement which says that the
amount of stock holdings held by any
strategy is exactly characterized by the
left endpoint of your window in fact if
you add up the amount of stock holdings
which may be negative or positive with
the with the left endpoint then that is
a constant over all time okay again this
is a statement that you can prove by
picture here you simply take the three
different cases you look at how much the
window moves with different different
possibilities for weather with the stock
price falls and you realize that the
amount of change in your holdings is
exactly offset by the amount of change
in the left endpoint okay so so this is
this is again another convenient
combinator statement that you can prove
the upshot of these two lemmas is that
the state is action not such a big deal
so the difference so any to spread based
strategies in a given class will
actually have very similar state so the
total amount of stock that they hold
will be will be basically
same up to a constant amount okay now
this is very convenient because that
that means that we can actually change
state from one strategy to a different
strategy with the bounded cost it's in
fact a constant cost and now we can
directly plug this observation into a
regret minimizing algorithm which is
based on a standard experts learning
algorithms such as multiplicative
updates or or follow the perturb leader
what the algorithm does is it runs this
export learning algorithm on the set of
strategies whenever the algorithm
prescribes a strategy that has not been
used in the previous round then you
simply execute some market orders to
ensure that the status is the same as
the new strategy and you can do that
again with with the constant cost
because the the difference between the
state the state is bounded and once your
state is matched you execute the same
buy or sell orders that the new strategy
prescribes okay and you continue in this
manner okay so that's the that's the
algorithm it's fairly simple to show
that the regret of this Metallica rhythm
is can be expressed as the as the regret
of the base expert learning algorithm
like multiplicative 8 so for the point
of leader plus the number of times your
state changes essentially the number of
times you change an expert in your ex
product learning algorithm okay and for
for either multiplet updates or follow
the perturb leader you can bound the
total number of expert changes by x
square routine and also the grid is also
bounded by square root II and adding the
two together we get a bound of square
root in the on the regret of thurber
rhythm okay so that's that's the generic
algorithm so so with this algorithm we
ran some simple a simple experiments by
using data that were downloaded from
some websites we implemented the
algorithm with multi grade updates we
also did it with follow the perturbed
leader and we tested it with some simple
bass lines like uniform averaging or
follow the leader in fact and also we
compared with the best artists in
hindsight we obtained some somewhat of
surprising results so so while the
algorithm that was based on multiplet
updates
was almost as good as the best strategy
in hindsight which is not surprising
what was interesting was that sometimes
we actually obtained a payoff that was
better than the best strategy in hand
side which is a which is somewhat
surprising and it's um seems to indicate
that what we had been considering as as
cost in terms of switching from one
strategy to another actually turn out to
be a payoff sometimes actually
beneficial to change state and we don't
really have a sorry good understanding
of how that happens but but yeah but the
algorithm seems to have pretty good
performance in on the real data yeah so
that I think that's all I have to say
about this work and thank you very much
please stop by our poster hi it seems
are you don't take into account
transaction costs no I don't yeah so how
does that complicate I would think a lot
of your proofs won't might not hold um
that's I mean if you if you think of
transaction costs as being a
multiplicative factor I think that can
be handled but I mean I haven't thought
about it very well I mean this is all
these are additive guarantees so okay so
i can actually answer to you the right
now is thinking about it will get the
same line of regret bounds because
basically we obtain the same performance
as the best strategy and which will
include a transaction costs and the
extra state switching cost is only 10 x
square root tea and that's the so that
the extra transaction costs that we
might increase also square root tea so
we'll get the same kind of a great
bounds ok questions
I could hear you as long as you may need
to but in real life t is has a finite
time you know t is less than 400 minutes
yes so so you know i mean we actually
never use the fact or even assume that p
is unbounded even in our experiments you
basically only did this process for like
a day's worth of data and actually I
maybe I should have mentioned in the
beginning is that so we have seen the t
is actually a finite number and at the
end of all the trading period ste you
have to liquidate area inventory so
that's that's the assumption in this so
you you know whatever position you have
yourself a liquidated at the current
market price okay Gregor sorry here
so what ensures that you have square
root these switches only I'm sorry so
what ensures that you have square root
eh it's only because in general you
could have more okay I mean so in
general you could have more but okay so
maybe what I what I explained here was
not completely accurate we actually mix
strategies using the weights that are
given with experts algorithm and then
all we need to bound is the l1
difference between that between the
weights from one to another and that is
actually bonded by square root II so
even though you know you might not get
actually square base which is but we can
actually mix them actually by mixing
them in this enzyme you mix the holdings
you mix the orders as well so that's
what we do technically
okay okay so uh thank you alright so the
next paper is the recipient of the
outstanding paper award the title of the
of the talk is some modular optimization
with sub modular cover and sub modular
knapsack constraints and this is by
rishabh ire and Jeff bilmes from the
university of washington and i'd like to
invite both of the others please on the
stage so to receive their certificates
please join me in congratulating them
and rashad will now give us the
outstanding talk yeah thank you for
coming for the talk and thanks for the
award okay
but it doesn't simply anything coming in
here
it's going again
ok
yeah okay
does anyone know where the lights are
off
so can just use this one yet again seems
to have go on I'll just use this
probably this one okay sorry so today
we'll be talking on a sub-module
optimization with sub modular cover and
submerge the knapsack constraints and
this is joint work with Jeff booms so
the outline of the talk is I will give a
brief introduction of sub modular
functions then give the problem
formulation and actually motivate our
formulation as well and then give a kind
of unifying framework of algorithms and
hardness results and finally end with
some empirical results so starting from
the very basics set functions are
functions defined on a finite ground set
of items and essentially for every for
every finite ground set you have a
subset and a set function provides
evaluation for every subset sub modular
functions are a special class of set
functions that have what is known as a
diminishing returns property and
essentially the diminishing returns
property basically says that the gain of
adding elements ranks as the context of
ESET grows so kind of canonical example
of this is the urns and balls example if
we define a set function as the number
of distinct colors of the balls in an
urn then it is easy to see that this
function is submerged the since you have
a diminishing returns a set function or
a sub-module function is said to be
monotone if if the valuation of the
function grows as the context of it
grows and finally I would like to
introduce the notion of a modular
function which in some sense is the kind
of continuous or discrete equivalent to
the linear function in the continuous
domain so sub modular functions have in
some sense have two sides one of them is
the aspect of sub modular minimization
where in some sense we want to minimize
a sub-module function over all possible
subsets this problem is polynomial time
and in some sense reflects
the relation to convexity from
applications perspective this kind of
models cooperation between items and
this is because of the diminishing
returns property that in some sense you
have you have many items they cooperate
to reduce the cost if you rather than if
you had the sum of the individual items
the other side is of submerged the
maximization where we maximize a
sub-module function over all possible
subsets while this problem is np-hard it
turns out that you can get a number of
constant factor approximation guarantees
and and in some sense this kind of
relates is a relation to concavity of
sub modular functions from I
applications perspective this is very
useful in modeling diversity and
coverage in a lot of subset selection
problems often however what we want in
some sense is to simultaneously maximize
coverage or diversity while kind of
minimizing cooperative costs between
items and and also naturally these
icarus as constraints often for example
we might want to we want my we might
want to maximize diversity or coverage
but we have a budget constraint in the
form of a kind of cooperative cost so
this kind of now motivates our problem
which we shall actually introduce so
historically turns out that this problem
has been handled we are difference
between submerge the function
optimization where in some sense we have
the co-operative costs and we have the
the the function capturing coverage and
diversity and we have this parameter
lambda which kind of which kind of looks
at the V combination of the two in some
sense though this is a very nice model
it turns out that these problems are
really hard and in fact an orderly NP
hard but they are even NP hard to
approximate so in so in this in this
talk and in this paper we actually
introduced the following formulation
which which also I would like to point
out is often more natural in some sense
so this formulation basically is is what
we call SCS
C&amp;amp;S CSK which is sub-module of costs of
modular cover and sub modular pasta
modular knapsack the main idea here is
that we minimize a certain submerged the
function which kind of models
cooperation having a coverage constraint
on the other sub modular function or we
might want to maximize a submerged a
function cap capturing coverage or
diversity while kind of having a budget
constraint on the cooperation
interestingly though d s optimization is
actually NP hard to approximate this
problem kind of retains approximation
guarantees I would also have to point
out that we shall actually assume throw
this talk that the functions F and G are
monotone sub modular so our main
problems are our main contributions are
in some sense we be sure how SCS cns CSK
are two formulations subsume a lot of
important and commonly occurring
optimization problems we are related or
sub modular functions we provide a a
kind of unifying algorithmic framework
for these problems we also provide a
kind of complete characterization on the
hardness of these problems and and we
shall actually emphasize the scalability
and practically practicality of some of
some of our algorithms so a motivation
and a kind of special case of our
formulation is the sub modular set cover
in the sub modular knapsack where we
have where the costs are additive but we
have coverage and diversity as sub
modular functions and these occur in a
number of applications like sensor
placement data subset selection document
summarization in each case we kind of
want to find subsets which which cover
cover information of the entire the
entire ground set or the entire set of
items and at the same time we have
constraints in the form of in the form
of budget constraint which could be for
example the number of sentences or the
number of sensors a second special case
of this is where we have cooperative
costs as submerged
functions but we have the other function
here as a narrative function and again
this also occurs in a number of
applications for example where we want
where we want speech corpus subset
selection with limited vocabulary
naturally we can kind of formulate the
problem of choosing choosing a largest
corpus of utterances with a budget or on
the vocabulary as as an instance of this
problem since the since the function
capturing captioning vocabulary can be
modeled via a bipartite neighborhood
function which is submodular the general
case is is SC SC and a CSK where we have
both the function f ng s sub modular and
and these also occur in a number of
applications for example a sensor
placement problem but instead of bunch
of additive costs we could have
submerged the costs and in some sense as
cooperation cooperative cost between
sensors which which actually is very
naturally in many applications another
application is the same limited
vocabulary speech corpus selection but
instead of instead of just having
instead of just trying to maximize the
number of utterances we might want to
have an acoustically diverse speech
corpus and a third application which
also can be modeled in this way is a
kind of privacy preserving communication
application so it turns out that the
problems SC SC SC NSCs k are actually
very closely related and they are
related in some sense we are these by
criteria approximation factors so by
criteria approximation factors are
actually kind of approximation factors
which have been used a lot in the theory
community and essentially what they mean
is that we have we have an approximation
factor on on on the set X here but at
the same time we be in some sense relax
the constraints a little bit by allowing
two to allowing the set to satisfy a
kind of relaxed constraint which is
given by Rho so your note that the
parameter Sigma is greater than 1 and
rho is less than 1 similarly we can we
can define a by criteria approximation
factor
4s for the other problem as CSK where
again we have an approximation guarantee
on this set but instead of instead of
having a kind of instead of satisfying
the budget exactly we can relax the
budget by this factor Sigma so in some
sense we cheat a little bit here but but
but often these guarantees have been
used a lot in the theory community and
and in practice these numbers are really
are not that large even so our main
result which kind of connects the
problems SC SC and SCS k are that given
a particular by criteria approximation
algorithm for one of the problems we can
actually run that algorithm multiple
times potentially to get almost matching
my criteria approximation for the other
problem so essentially what this means
is that both these problems are closely
related and and kind of have all have
matching approximation guarantees and
hardness results so now getting to our
algorithm acclaim work one of a word of
a key key elements in provided in the
analysis and providing providing our
results is the notion of a curvature of
a submodular function which kind of
captures the the distance between the
sub modular function to a linear
function so it measures how close to
linearity is a sub modular function and
in some sense curvature is one of a
fundamental complexity parameters which
kind of modulate the hardness of
different sub modular optimization
problems we shall see will in this case
that the curvature provides a very
interesting modulation in the hardness
so before giving the before providing
our framework of algorithms we actually
kind of look at the hardness and lower
bounds and provide a kind of complete
characterization of the hardness so when
you have the functions F and G as
modular functions we can actually we
have their our algorithm which achieve
FP toss or fully polynomial time
approximation scheme kind of guarantees
and and this is this is a standard
knapsack problem
when the function f is modular and the
function G sub modular this is the sub
modular set cover and the submerge the
knapsack problem and and again we have
constant factor approximation guarantees
and and the guarantees in some sense are
kind of modulated by the curvature of
the submerge the function G when the
function f is submerged by the function
G is modular the guarantees are much
worse and the guarantees are kind of
square root n in the worst case but
again the curvature of the sub-module
function f provides a kind of modulation
of the hardness of these and finally we
have the functions F and G being both
sub modular and the hardness of this
problem is actually the same as the
hardness when the function G is modular
so an immediate observation here is that
the hardness depends mainly on the
curvature of the submerged the function
f and and kind of very loosely on the
curvature of G so our algorithm which
framework is as follows we repeat the
following we have an iterative procedure
the main idea is that we choose
surrogate functions F hat and G hat for
the sub-module functions F and G the
main intuition is that these surrogate
functions would be easier than the
original sub modular functions and and
in some sense we obtain we obtain the
optimizers by solving the problems SC SC
and SCS CSK with these surrogate
functions instead of the functions F and
G so it is a very simple algorithmic
framework and an amp and for this talk
we actually and in this paper we
actually look at surrogate functions of
the form of modular upper bounds and
lower bounds of sub modular functions
and and approximations of sub modular
functions so getting to the surrogate
functions we have modular upper bounds
which in some sense are akin to sub
gradients of a convex function and and
in some sense these are tight modular
upper bound which are induced via all
rings of the elements of the ground set
we also have modular upper bounds and we
have two specific modular upper bounds
which again are tight and these are akin
to super gradient
of a concave function and we have two
specific instances of modular upper
bounds so in some sense again we have
sub gradients of a sub-module function
and we have super gradients of a
sub-module function and the third kind
of surrogate function which we which we
look at our our approximations and and
and we should we actually use we
actually use ellipsoidal approximations
of submerged of functions which in some
sense give tightest approximations
theoretically so we look at now a
special case of SC SC and SCS k which is
a sub-module set cover and submerge the
knapsack it turns out that the classical
greedy algorithm which was actually
shown in the 80s for both these problems
can be seen as a special instance of our
framework when the function when the
function G is kind of replaced by its
modular lower bound and in some sense
the approximation guarantees our
constant factor I would like to point
out that the approximation guarantees
for a sub modular knapsack is is
constant factor directly the
approximation guarantees for the
submerges set cover are actually log
factors but but but through our analysis
we can actually show that they have
constant factor by criteria
approximation guarantees now getting
back to our original problem scsc NSCs k
we provide very simple i iterative we
provide algorithm which is very simple
and I trade you technique essentially we
choose a surrogate function as a modular
upper bound and I totally we solve the
submerge the set cover and is submerge
the knapsack our main result here is
that we get an approximation guarantee
that depends on the dimension of the
problem and the curvature sorry
you
look
you
lose connection
okay maybe that is why in this way right
our second framework of algorithms are
where we use the ellipsoidal
approximation so essentially we use the
ellipsoidal approximation as a surrogate
function for the for the sub-module
function f and our main result here is
that we can we get a guarantee again
dependent on on the dimension of the
problem and the curvature and this can
improves the guarantee from a factor n
to a factor square root n right so I
would like to point out that this and
this algorithm actually gets a guarantee
which matches the hardness of the
problem of two log factors so so now
finally we would like to look at some
applications we look at the limited
vocab you data subset selection problem
we kind of model the acoustic diversity
we r sub modular functions like the
facility location and we saturated
coverage function which kind of are
naturally coverage functions and we
capture the ocean of limited vocabulary
VI bipartite neighborhood function so we
actually compare our results on the
timid speech corpus and as a baseline we
just we just choose random subsets so so
these are the results where we actually
compare the valuations our main
observations are that first of all all
all our algorithms perform much better
than just using random subsets which
means that they seem to work in some
sense and and secondly we see and
secondly the most importantly we see
that the height rative and much faster
algorithms perform in some sense
comparable to the much slower and tight
ellipsoidal approximation based
algorithms empirically so to conclude we
propose some very efficient scalable and
tight approximation algorithms and kind
of a complete characterization of the
hardness in what we have in the paper
but not in this talk is we actually have
some extensions to kind of handle
multiple constraints and non monotone
sub modular functions and as future work
we are we are really interested and we
are
eager to kind of in kind of look at our
algorithms for a lot of real-world
applications so thank you and let you
take some questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>