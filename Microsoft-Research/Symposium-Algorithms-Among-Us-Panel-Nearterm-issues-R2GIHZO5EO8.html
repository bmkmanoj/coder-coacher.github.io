<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Algorithms Among Us - Panel &quot;Near-term issues&quot; | Coder Coacher - Coaching Coders</title><meta content="Symposium: Algorithms Among Us - Panel &quot;Near-term issues&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Algorithms Among Us - Panel &quot;Near-term issues&quot;</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/R2GIHZO5EO8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so just to explain a little bit about
purpose of the panel so I'm a goal here
is to try and develop a discussion
around these new term issues that the
first talks have begun to address all
run out of leads just one for me good
and I've also again that your questions
if submitted into that form through some
magical process that I don't completely
understand will end up in my hands on
stage at some points I'll be able to put
those to the panel is so please do
continue to do that and other than that
please let me Oh what's going on here
that's all good um please let me
introduce you to the panelists to whom
you have not already been introduced so
of course you know Ian suited far left
you've met Eric and holy yeah in the
middle we have Cynthia to walk who is
the distinguished scientist at MSR and a
real world leader in the development of
differential privacy and a recipient of
the edgar w Dykstra prize so thanks very
much for joining your Cynthia that's
fantastic in the middle of course we
have Neil Lawrence a professor of
machine learning at the University of
Sheffield general chair of nips this
year of course so you may well have seen
nips many times previously and also a
regular contributor to The Guardian on
issues of surrounding the societal
impacts of machine learning next we have
finale doshi balance it was an assistant
professor at Harvard University in
machine learning and real expert in the
application and machine learning to
healthcare applications and finally of
course on the far right we have Tom
again and who you've already met so well
the way I will structure this is to read
out some questions that I've
pre-prepared hopefully that will
generate some kind of interesting
discussion amongst our panelists and
then at the end they'll turn to any
questions that we've been up to harvest
from the crowd and I wonder if it would
be possible for me to ask Tom to move
over to the Baker chair so i can get in
thanks to do great so um thanks very
much
alright so what I think I might start
with is a question too don't don't dumb
I'll start with Cynthia hi Cynthia
thanks very much so my question to you
is that it might be argued that fairness
can be compromised by permitting an
algorithm access to all the data so here
I'm thinking about for example a bank
and using an algorithm to make decisions
about who might be awarded alone and
then if that algorithm is not
appropriately instructed otherwise it
might draw upon things like the
ethnicity of an applicant in making that
decision so do you agree that that is
something we should try to prevent and
if so how should we go about doing so
well um I don't actually agree that it's
something we would necessarily try to
prevent right we've got disagreement
already that's a fantastic start panel
so basically you're talking about a
classifier that takes information about
somebody applying for a loan and tries
to make a decision so let's take an
example that we we like a lot suppose
we're not talking about loans suppose
we're looking for students who are who
are talented because we have some kind
of summer program that we'd like to run
and suppose that we have a minority
group in which the talented students are
steered toward math and science and in
the majority group the talented students
are steered toward let's say finance so
a quick and dirty classifier that will
find most of the talented students would
be a look for finance because after all
that's the right interest for the
talented students in the majority but
not only is it unfair the minority it
also misses out on the sort of gems of
the minority and therefore has reduced
utility and so what's important is to
have a culturally aware classifier that
understands that being in the minority
and having an interest in math and
science should be viewed as similar for
the purpose
of finding smart students as being in
the majority and studying finance well
thanks very much I'm but do you argue
then that there's never any need for
regulation on these issues of vendors
there's no need other to insist upon
some sort of guidelines governing the
actions of these albums well that's a
complete print question sure um you
could you could have blatantly unfair
systems that say whenever there's a
member of the minority fire them this is
clearly insane so of course you want to
have some sort of regulation but what
you need is some way of coming up with
task-specific notions of who should be
viewed as similar to him and I should
also mention that implicit in your
question is the assumption that it's
possible to hide the sensitive attribute
while revealing while still revealing
other attributes that are relevant to
the decision and that's not always clear
there has been some work along those
lines but it's not always clear that
that's possible did anyone watch we
spent them yeah i just want put in a
plug for this paper by google auto show
and colleagues at ICML this year that
gives a very nice sort of adversarial
way of training a machine learning
system precisely to make its decision
invariant to some set of constraints so
you could make it invariant to race or
sex or whatever or multiple properties
oats it's an example of the kind of
thing cynthia was just mentioning there
are technical approaches to this and of
course for making loans there are laws
already and so I know that at GE finance
for several years they actually had a
they try to approach the problem from
the point of view of carefully
constructing the training set which is
not a successful I don't think a
provably correct strategy so it's an
area where I think we need a lot more
work
yeah i i'm not sure if Cynthia and I
disagree but I want to make a sort of
point I guess in a sense that I see
coming in sort of from a legal
perspective so if if if an algorithm is
being used to make predictions about
certain kinds of people and we can
imagine it also outside of the loan
context we can imagine as there actually
are algorithms which are being used for
example to decide who is a more
significant risk when it comes to
granting parole or which areas of a city
are there hot spots for criminal
activity in in these kinds of cases what
the algorithm is doing in making its
prediction in conjunction with human
beings and I think that's the part where
where I where I see where Cynthia is
coming from on this is is that it's it's
potentially creating a kind of
preemption which which often we think
from a legal perspective that we ought
not to preempt people's life chances and
opportunities in particular kinds of
ways and that's why some of our legal
mechanisms are reactive rather than
proactive and so I think there's an
important point to keep in mind here
that in some circumstances where
algorithms have a pre-emptive outcome we
have to be pretty careful about how
they're being used but I don't think
that necessarily means to say that we
been using algorithms but rather we ban
particular uses and the way they're
being used in conjunction with human
beings and the question is around the
effects of that presumption if that
presumption is pre-emptive in a way that
interferes with due process or
interferes with people's life chances or
opportunities then maybe we want to
limit or regulate its use but it's not
the algorithm per se necessarily that
does that as much as the system and what
it seemed it seeks to produce any other
thoughts on that issue you know and so
another topic I'd like to move on to is
so if we agree that there might at some
point be the need for regulation policy
responses to some of these issues and in
erica's told us a lot about
equality issues that might develop as a
result of sure sorry is that any better
apologies um okay so to repeat what I
was saying with heard that there are
likely to be fairly negative
consequences resulting from the
introductions of some of these
algorithms they might in some places be
the need for regulation I think many of
us might agree with that and so question
that I put to Neil is what policy
responses might actually be realistic
how might we actually get some of the
policy responses we want through the
political process yeah I think that's a
very good question and I think I'd like
to start by picking up on Ian's analogy
so that the tunnel analogy what one
could say is that we're all passengers
as a society in this car steaming into
this tunnel and policy is how we choose
to steer the car to damage our privacy
or save more lives or whatever else now
the problem with a lot of these
philosophical constructions of these
questions is they eliminate something
which is extremely important that's
uncertainty in the real situation with
that car it would just kill one of those
before it had time to make a decision
there would be no decision made any
decision that would be made would be to
start mitigating put the brakes on
inflate the air bag and so these
decisions are much less clean than these
artificial philosophical examples give
us cause to believe now in the presence
of uncertainty the right thing to do is
to try and obtain more data and to tread
carefully before disrupting everything
so I think the first thing you do is
make sure that you're anyone who sort of
says they has all the answers they
clearly don't so that's the first rule
and then I think what you want to try
and do is coevolution of the law legal
process with the technology and that
really requires very close interaction
between people like Ian and people in
the machine learning community so that
we evolved tools which will answer the
sort of challenges and we inform each
other so my analogy full way that's
happened in the past of a good allergy
for the UK is the highway code so the
highway code when cars came on the road
in the first place these problem
were present already and the initial
rules were things like make the car go
at three miles an hour with a man waving
a red flag in front of it now there's a
nice example very close to where i live
in sheffield that was going on with a
steam traction engine in eighteen sixty
eight I think and a horse and trap
overtook the steam traction engine and
the steam traction engine turned into
the horse and trap and killed the
occupant that's because that type of
legislation was incorrect making it go
too slow actually cause the horse and
trap to overtake there's no mirror
signal maneuver so the law actually had
the effect of killing people and that's
the sort of thing we're going to
experience and so the what we moved to
with a highway code where we introduce
laws bicep of saying should not mast
starting with guidance I think a lot of
data protection legislation is already
doing this so I think that that's where
we should go with policy but that then
the evolution of those laws needs to
evolve with the technology thanks Neil
and Eric I wondered if you wanted to
come in on that I mean you must have
thought about what policy responses
there might be to the questions of
employment and that emerged from the use
of machine learning technologies would
you like to comment sure um you know on
employer in particular it we now see
that many jobs are being automated but
it's also worth kicking the longer
perspective that the technology has
always been destroying jobs it's always
been creating jobs and for much of
history that's been in a balance some
people take comfort from that and they
say well so let's not worry about it
it's automatically going to be fine
going forward I don't think that's the
right way to read history there are many
new technologies now and if you are a
little more cold-blooded about it you
can look at at other very important
inputs to production from from whale oil
to horses and others that were once very
important just like human labor is today
and technologies evolve that made them
less essential and it's possible that
can happen going forward that's it I
think it's as as Noah's saying it's very
hard to make predictions about exactly
how the technology can involve and which
ones are going to kick in at different
times and so we have to bear in mind
that you can do as much damage by
preemptively banning or regulating
certain things as you as you would if
you let things
fold often the market does work very
well to balance these things so I see
often as you know there's both kinds of
errors of over-regulating and under
regulating I think we have to be
conscious of both of those possibilities
okay and I think another area where we
as a community might make a contribution
is to look at the ways in which AI
technology can enhance human performance
so it's been intriguing to see some
cases like freestyle chests which is
chess players combined with computers
that are better than any human and
better than any computer and we've seen
this also in protein folding and I think
you can point to a lot of other tasks
where the human being is providing some
essential component even defining the
task in many cases so you know there's
been a tendency of course as engineers
to think about how do we replicate this
human capability or how do we build this
car that drives autonomously and maybe
we should be thinking instead how do we
build the most effective human machine
combination how do we make the most
effective human machine transportation
device and that that viewpoint I think
might lead us to to a very different
outcome than if we just focus on
automating away as a single human
capacity bility finelli I agree so human
machine interaction and also what we
expect from our machines I think it we
can change that right because we're the
community who built them and so if in
your example you said well the the the
diagnostic tool doesn't give you any
idea of why it told you that
recommendation but that's because we
didn't give it right or we didn't expect
it from there and I think there's a lot
of opportunities for rich machine
learning research along those lines of
and what we need maybe as a field is to
come up with just like philosophers need
their trollee car examples you know our
are totally cars or log likelihoods
right because that's something that we
can think about we can optimize so how
do we quantify things like
interpretability or technical debt or
these things that can make our system
spin out of control and if we can create
our little examples of how to quantify
those maybe we can optimize systems that
will be able to work better in
conjunction a few mins in some areas
we've done that like with privacy we're
able to we have quantified notions of
privacy but in general if we can build
those tools than we can perhaps to
better at optimizing and that'll give us
some other moral dilemmas as we try to
figure out what if there is a
theoretical trade-off between
comprehensibility and performance so at
what price also I just want to point out
with the new trolley and the tunnel
examples that we're also having some
kind of tunnel vision there I spoke to
Daniela ruse about this and she says
well we're going to have a whole lot
more technology than just self-driving
cars I mean the whole world will be
instrumented and we'll know that the
child is getting near the street and so
on and so forth will have a lot of
predictive ability so it's important to
keep that in context to yeah and if I
could just that I mean I grew but what
you and Daniela are saying about that
but but also another positive way to
think about this is that I mean people
have to make these split-second
decisions right now dry them to a tunnel
and they're going by gut instinct or who
knows and often we know they're probably
make the wrong decision the fact that we
can now take the time and codify we
could have a poll we can have the
legislators we can have a whole bunch of
people and think we have a group of
philosophers and legal experts weigh in
and decide in this situation would we as
a society want to delegate the decision
do we want to have it be the the child
in the baby carriage or the fat man on
the bridge or whoever I mean we can
debate these we could spend you know a
year or two trying to figure out what we
really want to do and then codify that
into the software so that when that
millisecond occurs then it happens
instantaneously the way that we decide
out there may be some minority that
disagrees or some other group that
disagrees but ultimately the ability for
us to codify it seems like a unambiguous
improvement over having it just being
done on the spur of the moment by people
without having to thank it through here
here um yes I I'd like to totally
disagree with that
I think is a great danger in this era in
believing that there's a right way to
respond to these circumstances and I
think that one thing that's quite hard
coded into humans is a variation in
response and that's a necessary part of
our survival that people will react in
different ways then come by in a
variation there is randomness yeah that
and that's sort of yeah I think that's
true but the idea that there's a right
way I mean the fat man on bridge example
is just absurd I mean who would ever be
able to justify pushing a man off a
bridge I mean the guy is not even going
to deserve the trolley Annie and then
and then afterwards you explain to the
family of the guy you did it i mean that
no one could ever manage that and it's
the distillation into something that's
pure and that there is a right answer
for many of the things we face there is
no right answer seeking a right answer
is dangerous it's the sort of thing that
lake in the right answer is dangerous so
you can the right answer is dangerous
because we don't know the objective
function we're working against you need
a diversity of approaches so that we can
explore how society should evolve all
the problems in previous societies are
from people claiming they knew how
society work and if we're to take that
responsibility on I think we're in
danger of creating a very dangerous
society it's an interesting attitude
because when we build systems we are
willy-nilly expressing values it's a
absolutely rigorous formalization
codification of these values are you
saying don't pay any attention don't
look at it I'm not saying don't pay any
attention I think what's the consequence
of paying attention that a general this
general approach that we can codify how
humans should behave in that answer that
we should decide how a car should behave
in that situation I think there is no
right answer that's why humans behave in
different ways as wit riz species that
has evolved to do different things
because under these circumstances we
don't know I mean there are different
values people which have to respect
those different values and encoding a
single system of values in that way I
think is extremely dangerous maybe just
one last response and okay but but but
we do have in many fields of engineering
principles of robust optimization which
say okay I don't know exactly what the
objective function is that I'm
optimizing and I want to be robust to
the possibility that is misspecified and
so I think we can and and I think
actually this is a very important thing
for us as a community is to and and
there are already several posters at
this meeting about robust loss functions
of various forms so so I that those
lessons should Perry carry over here and
we can try to think about optimizing
some notion of robustness that says no
we don't think we know the right answer
there is no point right answer but we
could still optimize over some ensemble
thanks to me um I've got a million
really fantastic questions here that I'd
love to get through if we have time so
let me dive into one that's actually
been targeted to you finale so this is a
question about the use of machine
learning and medicine and that us will
this actually prove the most tricky of
the potential applications for machine
learning because of the potential dire
consequences of errors so will this
continue to limit the entry of machine
learning into medicine and in particular
can you foresee any backlash against the
development of such methods if something
does eventually go wrong saner clinical
to decision support system making the
wrong decision so I actually don't think
it's the most dire of dire fields I mean
we don't we don't accept a plane crashes
right we don't and you know tcast has to
work at and we are much more forgiving
of mistakes in medicine which happen all
the time and kill lots of people i would
say that in some senses machine learning
is already becoming part of medicine if
we count good search as part of machine
learning because what do doctors do they
look for answers so i think in in ways
this is going to end insurance companies
are using all sorts of sophisticated
models to design healthcare policy so i
see that thing that this is already
happening right um and I I don't think
that anytime soon we will be fully
automating the process and that's a good
thing again going back to the
comprehensibility and medicine is a
field where we can't create nice black
boxes because we don't know about a lot
of confounds but i but i actually think
that this is going to be a very subtle
and gradual transition that is happening
already
and I can move on to the next question
and which is actually one for Eric out
first speaker sure so this is a question
for Eric our first speaker who described
the opening skill income gap in the US
due to the new technologies being
introduced and how does this translate
globally so what how might this story be
different if we look at in particular
developing nations rather than just the
developed nations yeah that's a great
question a very important one one thing
that if I had a few more minutes I would
have wanted to talk about is that
globally incomes are rising and much of
the developed world has benefited
tremendously China in particular has
moved from a low-income tree toward
becoming more of a middle-income country
and much of that is by doing some of
that same kind of relatively low-skill
low-wage labor that isn't done in
western europe and america and japan so
much anymore is that's been a great
success over the past 20 30 years that
said the past isn't always prologue if
you go some of those pictures I showed
we're actually of Chinese factories and
if you go there today you'll see a
tremendous push towards automating those
factories and in many ways the kinds of
work that China and the Philippines and
Vietnam have been doing most
successfully today are very much in the
bullseye of what's most automatable it's
easier to automate many of those tasks
if you visit them like I have you see
people doing a repetitive task fairly
simple task over and over and Daniela
and others have been working on some
much more sophisticated robots Rodney
Brooks just came back from China and I
actually could see a much bigger shock
in some of those countries and we have
even in the United States Canada and
Western Europe in terms of the impact
going for the next 10 or 20 years and it
may be harder for them to jump up to the
creative interpersonal tasks that robots
aren't very good at than it is for
countries where most of those routine
physical jobs have already disappeared
thank you and if no one wants to comment
I'll move on to a question for Ian so
this question asks Howard evidence or a
decision from
team learning algorithm hold up in court
and I think this is something that many
might want to additionally contribute to
that question goes on to ask how can we
gain and create confidence and
algorithms to making them more
transparent more in terrible so that
such thing might be possible okay I
think the two aspects of those questions
are related in a kind of way and one of
the things that I was sort of hinting at
in one of the examples that I gave was
that in fact it would be very difficult
in some cases to to bring an algorithm
to the fore in the context of a legal
proceeding in in for various reasons one
is that if the particular algorithm is
for example governed by trade secret or
other other such mechanisms then getting
it even on the table might be difficult
maybe it would be in the case where the
entity being sued decides that it wants
to put that forward in order to defend
itself but it's hard to imagine that
quite frankly but the other the other
issue really is to the extent that the
algorithm was somehow being held out as
the proxy for the expert often when we
put an expert on the stand to the extent
that that's the case what we have in
some of these situations I think and my
technical colleagues can can speak to
whether I'm wrongheaded about this or
not but but in some of these situations
it would be very difficult for the human
beings whether it's the programmers of
these algorithms or the people who put
them into use to explain in a meaningful
way given the complexity of all the
different inputs that might have gone
into the algorithm deciding what it
decided that it would be very very
difficult in the way in a way that's
quite different for a human expert if
you were to put them on the stand so I
think it would be a tremendous challenge
and just very quickly I think that that
those sets of challenges tie to the to
the opacity or potential transparent
pneus of the algorithm we could imagine
other scenarios in a more open source
kind of setting where those things are
more made more available I think some of
the problems would still be a challenge
but some of those would be addressed by
trends
see I don't think transparency is a
cure-all though anyone want to comment
on the difficulty or value in making
machine learning opens more transparent
more in terrible well I think that your
point is excellent it would be in
general completely incomprehensible and
to look at the code or whatever but but
there is the fact that the algorithm is
sort of infinitely interrogative
interrogative all you could just keep
reading examples to see what is it doing
and it would be interesting to try to
develop a theory of how do you choose
the examples in order to quote-unquote
reach an understanding of what's going
on I'm not sure if that's what happened
you maybe you can tell me but I I
remember after for example the Jeopardy
where where Watson made that response
where it said Toronto and it was about
which u.s. airport and I think people at
IBM spent a lot of time afterwards
trying to retro actively refit how it
came to make that decision maybe by
doing exactly what you just suggested
probably a lot of people here know much
better than I about that so I'm I think
there's another aspect to this question
as well which is actually as we try and
understand better how to get algorithms
to explain themselves I think we're
going to start understanding how badly
humans explain themselves and it will be
a reverse effect that in terms will
start understanding that the when the
doctors put on the stand saying why did
you make that instantaneous decision to
to cut out that org he'll will realize
that that's to the made-up explanation
that it was just given the information
he had in the time that was his instinct
and that's all he can say and that's how
our algorithms are reacting at the
moment given the information they had
that was the best decision that they
didn't have time to think about
additional context about his mom or any
of these other things the point that you
just made is one of the reasons why a
colleague and I recently argued that the
prospect of machine experts isn't so
far-fetched because the sociology of
expertise has built into its very nature
nature this idea of the under
determination and the it's there's
something there's an intangible pneus to
what an expert does and I think you're
right to say that the Machine replicates
that
I think I'm gonna go behind it doesn't
mean in many ways I think the machine is
going to be more interoperable be easier
to integrate easy for you to say it
because because of the fact that it is
in most cases relatively deterministic
and you can you can go in a way that
humans can't so so I human machine is
going to make very different kinds of
mistakes then humans do but I think
they'll be easier to reverse-engineer
what happened in many cases that and be
less likely to rationalize ex post with
a different explanation and I would just
say just say again this is a topic
that's under active research in the
machine learning community how do we
provide explanations for decisions
individual decisions on individual cases
and I think our big challenge there is
really just what Neal was saying we
don't know how to evaluate our
explanations either do we compare them
to human explanations do we ask how
persuasive they are to a human and so
that's a big methodological problem for
us is is to figure out what what is the
objective function we're trying to
optimize when we're trying to produce
good explanations now I do you want to
comment on how much limitation this is
in healthcare particular that algorithms
are transparent and how that might
compare to how human experts currently
make decisions so I mean I think it's a
similar issue in in Health Care's and
other fields and particularly important
because oftentimes this the system
doesn't have access to a lot of
information that the doctor does and
vice versa so you really do need a team
effort there I think that we we can come
up to your last point of methods to
evaluate explanations may be the same
way like we expect our algorithms to
make good predictions if a human at
least understands the algorithm can they
can they hallucinate what the algorithm
is doing and I think there's there's
notions that we can start developing and
formalizing I think explanations are
tricky but but there's ways that we can
start cracking at this soon today yeah
and so I'm afraid that's actually all
we've got time for which is real shame
because there are so many topics we
could get into and discuss further but
hopefully we'll have a chance to do that
open copy break so if everyone could
just thank our panelists once again
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>