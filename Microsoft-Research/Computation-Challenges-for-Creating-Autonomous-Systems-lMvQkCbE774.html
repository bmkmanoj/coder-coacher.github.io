<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Computation Challenges for Creating Autonomous Systems | Coder Coacher - Coaching Coders</title><meta content="Computation Challenges for Creating Autonomous Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Computation Challenges for Creating Autonomous Systems</b></h2><h5 class="post__date">2013-02-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lMvQkCbE774" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you all for being here thank you
to microsoft research for organizing
this very inspiring meeting for and for
challenging us to think about the future
of computing computing in the 21st
century now today i would like to talk
to you about how computation is
progressively becoming much more
interactive with a physical world and
also with people the next generation
computers will be able to sense the
world they will be able to act
autonomously in the world and they will
be able to interact with people in other
words they will be robots and i know
this because the previous speaker told
me so in fact John hopcroft was my
teacher at my advisor when I was a
graduate student John thank you for
giving me the opportunity to work on
robots with you at Cornell and for
opening this great world of excitement
to me so we have come a long way since
the Czech playwright karel capek
introduced the word robot to denote hard
and repetitive labor now some of us fear
the idea of machines doing our work and
others are widely excited by it but our
dream to create machines in our own
image that are smart and obedient goes
back to antiquity the quest for smart
machines has progressed from the
technological marvels of antiquity to
the 17th century automata to science
fiction robots and to today's reality
and they are becoming increasingly more
lifelike robots today are impacting all
aspects of life from small and near for
example in medicine too large and far
for example in exploring the universe
from manufacturing to homes
robots contribute to science and
knowledge and make the world a better
place now in order to build robots we
really need a broad spectrum of
technologies we need sensing we need the
ability to interact with the world we
need decision-making we need human
machine interaction and we need the
supporting software and hardware to do
so but robots are becoming ever more
autonomous and capable and imagine in
this reality of robots I would like to
give you I would like to start with one
example so imagine all the tasks in your
home that you would like to automate
maybe you would like to have a robot to
fold your laundry maybe you want to have
a robot organize your rooms maybe you
would like to have a robot cook your
meals in the kitchen well it turns out
in a kitchen we already have some small
robots so we have a robot that washes
our dishes we have a robot that cooks
things with science but there's still a
gap between these existing robots and
and something we would like to call the
Iron Chef robot so what is that what is
the problem well if you think about all
the things we've got in our kitchen we
have cabinets that are challenging to
open inside the cabinets or a huge
variety of packages and things we have
dials in on our stove for for cooking
and we have objects in the kitchen that
are really in general very hard to model
and unpredictable so what is so how can
we how can we bridge the gap between
this complexity in the kitchen and
actually putting a robot chef that will
make food for you well one thing we can
we can do is we can bring all the
ingredients we can simplify by putting
them on the counters and this ms on plus
is in fact what the Iron Chef is faced
with on our TV shows we can use standard
tools and but and which
a model a human centric workspace so my
students in fact my undergraduate
students who are were just like you have
been taking the challenge of making this
robot chef and have been developing a
system called big wat i would like to i
would like to introduce to you now this
robot execute simple baking recipes from
museum amazon plus setups until the
robot actually generates finished
products like cookies cookies tend to be
very popular in our laboratory the robot
uses a human centric works place but it
uses standard tools in other words the
use of the robot uses spatula to mix
objects in starting from a generic
recipe given to the robot in natural
language the robot is able to execute a
sequence of instructions that enables it
to go from basic raw ingredients to
finish products to cookies so let's
watch this movie for for a while we have
created a database of recipes supply the
natural language and on the video screen
you see what these recipes look like and
we have developed a natural language
parser that takes these recipes and
breaks them down into actions that can
be executed by the robot we have then
implemented there are new control
strategies that enable the robots to do
things like mixing scraping finding
objects pouring and in this video you
see big BOTS execute some of these
instructions that were automatically
generated from a recipe to make Afghan
cookies so I introduce to you our robot
man's and Manos which means mind and
machines it turns out mixing is actually
much harder than then you'd think
especially for machines but this can be
done using state-of-the-art perception
algorithms and state-of-the-art
manipulation and plan
algorithms that enable the robot to find
the next ingredient needed in the recipe
pick up the ball add ingredient to the
ball and then mix to blend everything in
so Afghan recipes are often cookies or
chocolate cookies that have crunchy
cereal in them and so here's our robot
successfully having read the
instructions and mixed the batter now
the robot is not the most graceful
executor in the kitchen but nevertheless
it's successfully it successfully
executed the mixing and it created the
batter the robot is also very efficient
because now it's noticed that there is a
little bit left on in the bowl and it
will go ahead to scrape it so our robot
chef is also able to find the oven in
the kitchen and open the door and the
robot uses wheels for moving back and
forth between different workstations and
here's the robot going back to the
mixing workstation and bringing the
cookies to the oven so let's see the 20
minutes of baking time is the required
baking for a integral for this recipe
and after 20 minutes here is the result
and this is my student Mario who
developed this recipe so I encourage all
of you to bring robots in your
laboratories because the then you will
be you will have the pleasure of having
fresh cookies baked by the robot
everyday so it turns out that there are
other tasks inside the home and in in
the world that you might want to
automate them some tasks require more
than one robot so for instance say you
are moving to a new room or new
apartment and you want to buy new
furniture and
is an example where a group of robots
are able to work together and cooperate
to assemble your next IKEA set a
furniture for your rooms so in this case
we have two robots equipped with robot
arms these are laboratory replicas of
industrial robots and these robots have
specialized tooling that is added to
their grippers to enable operations that
are actually very complicated to execute
by robots such as screwing here you can
see a close-up of how the robot finds
the parts and and made them and the
mechanism used to to spin the part in
place and by the way this work was done
by undergraduate students in my
laboratory and it's been a very exciting
project so who these examples I would
like to show you that in fact in order
to create autonomous systems that will
do things for you it's important to
think about multiple robots working
together especially when you have a
large scale environment or a complex
environment like in the case of the
furniture assembly robot so in this
environment the robots move around to
sense the environment they compute based
on the values they sense from the
environment and when they take some
action in the process of sensing each
robot will aggregate information that is
only local to the robot and this
information may be valuable to the other
members of the team so communication is
an important aspect of these kinds of
machines and because of that we have a
distributed system in which each robot
has access only to local information but
what we would like is to guarantee some
sort of global behavior for the system
in other words make sure that your team
of robot furniture assemblers will
assemble the correct bed or table for
you so with these examples I
would like to tell you three things so I
would like to tell you that in order to
make these are amazingly capable
autonomous systems we really need to we
really need to know mathematics we
really need to start by modeling these
systems and by building a theory on how
they work the next step you need is you
need to know how to program you need to
go you need to be able to implement
these systems in hardware and in
software and the third thing you need is
you need the courage to follow your
dreams so you really need to be able to
think about what kind of new
capabilities or activities you would
like to enable the systems you are
creating to do and just go for that go
for your dreams because it's possible to
implement them so let me say a few words
about each of these three important
things so with Syrian algorithms I just
want to give you an example of the
flavor of the kind of thing you need to
develop that will that allows you to
model robots that think locally yet have
a global a guaranteed global behavior so
the problem i would like to use for this
is a very simple example i have an
environment i have a bunch of yellow
robots that can sense in the environment
and i want to place these robots in a
way that matches how interesting the
environment is so in other words where
it's interesting i want to collect a lot
of samples and where is less interesting
i want to have sparser sensing and in
order to solve this problem i actually
have to think about a number of
different aspects of computer science so
i have to model the multi-agent dynamics
and this comes from a discipline called
control theory then i have to model how
the robots communicate with each other
and this can be done by using graph
theory and finally i need to think about
how the robots ends and this comes from
learning and estimation theory by
bringing these mathematical formalisms
together we can actually come up with
algorithms to
of this problem so it turns out there is
some very specific mathematics that
enables us to formulate this problem in
terms of a strategy that involves
gradient descent in terms of a strategy
that involves computing a cost function
doing some local sensing and analysis
and moving according to how according to
what this cost function tells us to do
and maybe the the mathematics is not so
important for this talk but but the
general process is if you do not know
where the environment is interesting
then you would like to actually learn
that so you would like to learn the
weighting function that goes with the
cost function in the system you'd like
to then use it to estimate the next
place where the robots have to move and
then actually act by moving so using
mathematics we can come up with a
formalism that shows us how to drive the
robots in such a way that their motion
will optimize a cost function
representing the collective cost of this
network so an interesting technical
aspect of this is how do you learn where
it's interesting in the environment and
the way you can do that is by
approximating a function that tells us
how the environment is interesting using
something called a basis function so
basis functions can be in anything they
think can be gaussians they can be
wavelets they can be polynomials and in
general they have these unknown weights
in front of them and these unknown
weights can be learned by doing
measurements in the environment so the
idea is like this the robots have some
estimate of what the sensory function
looks like they take a measurement they
get a sense of how far off their local
estimate is from what they have just
measured and they correct the parameters
in other words what is needed for a good
solution is an algorithm that enables
the robots to moving the physical space
to cover the environment while learning
the parameters of this learning of the
sensory function
in a parameter space and it turns out
that if every robot does this locally
the whole system will converge in a
guaranteed way to robot configurations
that are guaranteed to match where it's
interesting in the environment so this
is pretty amazing if you think about
launching a group of robots in a place
you don't know anything about doing very
small local operations and then ending
up with something that is absolutely
guaranteed to be the solution of this
hard problem it's very very amazing and
this is what we get from from being able
to apply mathematics to model the
problem so here's the practical control
algorithm that results from this each
robot measure some coordinates computer
geometrical object measures the sensory
function updates parameters and then
applies a control input and that's all
there is to it so here's a ques a little
simulation that shows a group of robots
and if you look carefully there are two
red crosses in this environment and
these red crosses are where it's
interesting in the environment and you
can see in the three graphs you can see
the point of view of each robot as it
runs in the world trying to estimate and
approximate the location of these
important events in the world so I
wanted to give you a sense of how
mathematics can be used to encode a very
hard problem in a practical way but what
can we do this in a really practical
situation so consider instantiated this
solution to a very simple problem now my
robots can fly and I they have
downward-pointing cameras and I would
like for my robots to spread out in the
world in such a way that their camera
covers the entire environment so that
they can perhaps do continuous
monitoring of the environment while
using the same methodology of gradient
descent using a cost function we can
instantiate this problem by by
introducing a cost function that has a
lateral
components to drive robots to move
inside the region of interest and away
from neighbors in other words to spread
out and a vertical component that allows
the robot to move down to see better
while moving up to see more so the
critical thing is coming up with a
trade-off between all these four
components and it turns out that this is
sufficient to implement a very
sophisticated algorithm where we we do
not have to know ahead of time how many
robots are in the system and the robots
will adaptively collect samples in the
world figure out how to move to optimize
this cost function and place themselves
at optimal locations that will give us
the coverage of the environment
according to to the highest possible
resolution and to show you that the
system is independent on the number of
robots here is Brian the father of the
system killing one of the robots and
here the robots readjusting to make up
for the failure of of the third robot
and in fact the second robot dies next
and you will see the the remaining robot
should way up because that will enable
the camera field of view to cover more
of the of the space so okay this is a
this is a laboratory experiment that
shows you how you can take a practical
theory and implement it on real hardware
what can you do with it so one possible
application is in biology where
biologists sometimes like to find
invasive tree species so in this picture
you see some trees that are much darker
green than the others and those are
invasive trees brought on an island in
the Pacific and these trees are removing
all the nutrients for the native trees
they should be eliminated but it's
impossible to tell where these trees are
from the ground however if you bring a
fleet of robots and you allow them to
hover above the tree canopy you can
create you can aggregate mosaics of
images
this one they will tell the biologists
exactly where the batteries are and then
the trees can be eliminated the other
thing you can do is you can use these
robots to get a glimpse of the secret
wife lives of animals so in this video
we were interested in the secret lives
of Wales we worked with some animal
biology to some marine biologists on on
a census and behavior classification
problem for whales and the problem is
that at the moment there is no other
solution for for finding out what whales
do all day and and when they show up at
set locations the reason is that the
tools biologists currently have our
paper and pencil they usually sit
perched on them on a hill and with
binoculars they look at the sea to see
who goes by if they send a helicopter
the whales get scared and go go under if
they if we try to do this with a plane
with photography taken from a plane the
plane has to fly too high to give us
good enough resolution but with our
robots we can do some amazing close-up
unprecedented imagery of animals and in
the system you have seen a segment
involving whales and other other animals
we have looked at our sea lions and
flamingos and and it's really fantastic
you can think of these robots as being
remote eyes that will self organized in
the right configuration to keep track of
where it's interesting in the
environment and to provide the the user
in this case the scientists with very
very detailed information about what
these animals do all day when people are
not watching so very very exciting to be
able to take these robot devices and
make them work in real life as
settings here's the robot again and the
launch using again an undergraduate
student who worked on this project so it
turns out when we did the same
experiment with sea lions the sea lions
actually noticed the robot and they got
really excited by it and in fact in this
video you can see they almost try to
catch the attention of the robot and
when the robot went away the sea lions
chased it and wanted to continue to play
so i have i have shown you a few
examples of how we can look at theory to
formulate complex problems involving
multiple robots involving low local
decision-making and global behavior and
then i have shown you some examples
where we can take these ideas and we can
map them into actual system so what was
necessary was to to bring in the
hardware and to think about the problem
instantiation to think about what is the
cost function associated with these new
physical problems and how can we then
map these cost functions into a
practical simple algorithm with all
these solutions our resulting programs
are very easy although the mathematics
for how to show that they are correct
get quite tricky but you really need to
do both you really need both in order to
to realize a valuable and reliable
system now for the last part of my talk
I want to address the third item I I
told you I will tell you about how do
you how do you go about chasing your
dreams and I would like to give you a
personal example so when I was a when I
was a kid my most favorite cartoon was
the barber papa family how many of you
are familiar with this cartoon so the
barbapapa family
the family of blobs and these blobs can
take on any shape that is needed to
implement a task so for instance when
their community is invaded by a giant
beam the barbapapa family turns into
tools that can chop down the vine and
then they can turn into containers and
tools that will gather all the seeds and
and cook them into into soup so this
barbapapa family it's really quite an
amazing family so how is this related to
what I just what I just told you about
with respect to multi-robot systems so
now consider that consider how we might
take the robots you have already seen
and we might miniaturize them and we
might make them increasingly softer and
flexible so that the robot bodies
themselves begin to look more and more
like materials now at the same time if
you think about what is happening with
materials and many materials are being
developed right here in China today
materials are getting increasingly
smarter themselves and what we are
noticing is a convergence between
machines which are getting more and more
and more soft softer increasingly more
like materials and materials which are
getting to be more intelligent and the
convergence between these two is what I
call programmable matter it's the idea
that one day we will be able to program
not just bits and bytes inside our
computers but all the objects around us
much like this barbapapa family is
capable of providing of a programming
its own physical shape to deliver
functionality to whatever whatever the
task is so this is very exciting to me
I'm so excited to be able to work on my
childhood dream because I I spent up but
I spent my early childhood dreaming
about the barber papa family and and so
for the next few minutes let me show you
a little bit about what is possible in
order to implement robot systems that
look increasingly more like this
barbapapa family but let me give you a
definition of programmable matter in
slightly more technical terms so program
you can think of programmable matter as
being achieved when a collection of
modules that are physically connected
have the ability to respond to the
request of changing shape as a goal for
example the goal shape maybe a
locomotion task in other words the
ability of the collection of robots to
navigate through a narrow corridor and
you can imagine the robot squeezing
themselves sort of like toothpastes to
travel the corridor but the robot may
also not know ahead of time what what
the task at hand is and at the last
minute the robot may discover that it
needs a third hand or a second head or
it needs to turn into an alligator and
the idea that someday we can build
machines where small modules collections
of modules that are physically connected
can realize this goal of programming
shape is what I call self
reconfiguration or programmable matter
with respect to to shape now I have at
the moment two different approaches to
implementing programmable matter that
attempt to bring materials and machines
closer together the first one attempts
to to implement the idea of building
smart paper but can you imagine your
computer's being just like a paper you
fold you put in your pocket next time
you need a machine you unfold it and you
carry on computing so this is the idea
of making programmable matter
I building smart sheets the other idea
is trying to aim at programmable matter
by building smarts and and here the the
intuition is like this if I give you a
bag with smart particles in it and then
I programmed this bag so I give this
back the particles in this bag an
algorithm that enable the particles to
aggregate in whatever shape is needed
and then I say okay bag make me a rabbit
so the particles inside the bag talk to
each other figure out who's in the final
shape who is not in the final shape
eventually they aggregate a nice rabbit
that we can pull out of the hat that
would be that would be a tremendous
advance in in the space of programmable
matter because then instead of having to
take to use 12 of any kind that you
might need all you need is this bag with
smart material because the back could
become anything you want when you're
done with your tool maybe the hammer you
you needed to synthesize you can return
it to the bag for future use so how much
of a science fiction is this well it
turns out that in my laboratory working
with students just like you we are we
have embarked on the idea of building
smart paper that has the intelligence of
origami algorithms and so here in this
picture you see some early results on
making computers that look like origami
sheets these computers have embedded in
them computation actuators sensing
elements and connection elements and you
can combine these you can command these
smart sheets to aggregate different
shapes using algorithms that implement
origami folding so let's watch so here's
the sheet you have seen in the previous
picture and you can see the sheet
folding itself into the first shape
in this case a small boat now the same
sheet can then disassemble can flatten
out and it can be used to to make
anything in our case the next shape we
have programmed on the sheet is a plane
so here's the sheet again self-folding
into into a plane so this idea behind
making things by but by using smart
paper and origami folding can actually
be used to make any type of any type of
robots and in this picture in this slide
you can see some sketches and some
pictures of an origami robot my group of
undergraduate students again has
developed in response to an interesting
challenge put forth by the African
robotics network and this challenge is
to create an autonomous robot that costs
only ten dollars to make because if we
can create these inexpensive robots then
we can give every student one robot to
learn about physics to learn about
robotics to learn about spatial
interactions so the the top let's see
the top left figure shows you the sheet
that was folded into the wheel of the
robot in the bottom set the bottom of
sketch shows you the sheet that was
folded into the body of the robot and
here is the ten dollar origami seg robot
at work this robot is a is a little bit
like an ant at the moment so the robot
can drive around and when it finds an
obstacle it changes direction so this is
very exciting because we can begin to
think how by embedding smarts into
regular materials in our everyday life
we can achieve some form of intelligence
now the next concept I we are interested
in developing is this idea of smarts and
so here's the concept here is how it
might work I have a bag of these smart
particles and suppose I want to make
another edge so I can pour the smart
material around the the existing wrench
I could tell it make me make me another
copy I could shake the material up and
out come the two copies now how do we
how do we achieve this for real how to
achieve this concept for you well it
turns out that we cannot build
millimeter particles smarts and but we
can build centimeter particles we call
smart pebbles and here you can see the
hardware that is involved in the sport
pebble system and the magical component
is labeled in the diagram by C and this
is something we call an electro
permanent magnet this magical component
has the ability of giving our cubes
program connections so that the
components can be switched on and off
according to what the decision-making of
the system is it also allows the robots
to communicate with each other to make
collective decisions and it allows the
robots to pass power from one unit to
the other because that limits the amount
of stuff I have to put inside the unit
and that also enables us to make smaller
devices so here is a video that that
shows you how this process you have seen
in this video shows how you can select
robot pebble system to duplicate complex
2d shapes we start by surrounding the
original shape the humanoid form shown
in black with robot couple models as the
robot pebbles communicate the connecting
bonds flash orange the basic idea is to
identify and duplicate the modules on
the border of the original shape the
models turn blue when
realize that they may border on the
shape to be duplicated using a geometric
algorithm the system differentiates
between the blue modules that border on
the shape to be duplicated and the blue
models on the exterior of the composite
block of material as each blue module
determines that it borders on the shape
to be duplicated it turns yellow and
sends a message to its conjugate border
module that lies some distance to the
right this offset distance is
automatically chosen by the system in
such a way to ensure that the original
and duplicate shapes never overlap once
the conjugate boromir's the border of
the original shape the system performs a
flood fill process to inform all models
inside the new duplicate border that
they are part of the duplicate shape as
a result they turn orange finally once
all the modules that form the duplicate
shape have been notified of the unique
status the system breaks all the
unnecessary mechanical bonds between the
other models leaving only the original
and the duplicate shaped behind we've
extended this process to
three-dimensional shapes we have run
hundreds of experiments demonstrate that
it works reliably and robustly with any
original shape so the idea to innovate
on how we package a module together that
is capable of program connections of
local communication and a power transfer
together with algorithms that rely on
local communication to implement complex
global goals like programming a new
shape it's what brought us to the point
of where we were with the previous video
and I just want to show you that in fact
the resulting algorithms are quite real
we are able to take our physical
experimental platform that we have
developed in the lab and here you can
see the smart pebbles what they look
like and we are able to implement
everything you have seen in simulation
on these really tiny modules that you
can think of as computers because
essentially that's what they are they're
tiny computers that have the ability to
be more physically interact
of with the world and with each other
and these physical computers can use
local communication to implement
operation such as localizing each other
figuring out where they are in this big
matrix of devices identify what is the
object that they are tasked to deplete
duplicate so this is not given to them
in advance they have to sense what this
yellow square looks like and then once
once they've learned that shape they
communicate with each other and you've
seen flashing lights those those
flashing lights indicate communication
and you can see that the system was was
able to to duplicate the initial object
so we have a ways to go to implement
these ideas at the millimeter scale that
you have seen in the smart sand video
but the point that I would like to leave
you with is that computing is changing
and we have we have been moving from big
mainframe computers to desktop computers
to computers that now have sensing
communication and computation
pervasively embedded in them and this is
a progression towards an increasingly
more physical and smaller instantiation
of computation the next steps are robots
and then even moving beyond robots
programmable matter where we will begin
to program not just the computation
performed by the device but also its
physical properties its shape its
elasticity perhaps even its optical
properties so someday we might even be
able to create systems capable of
displaying invisible cloaks so with this
I I thank you for being here and I hope
you get inspired by all the talks you
here today
to learn about computers and begin to
chase your own dreams thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>