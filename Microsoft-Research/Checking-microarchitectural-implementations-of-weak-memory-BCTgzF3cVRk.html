<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Checking microarchitectural implementations of weak memory | Coder Coacher - Coaching Coders</title><meta content="Checking microarchitectural implementations of weak memory - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Checking microarchitectural implementations of weak memory</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BCTgzF3cVRk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I'm gonna do this briefly this is Dan
he works at Princeton he does a PhD and
he's going to talk to us about how do I
verification so enjoy hello everyone
hear me okay great so thanks for having
me it's great to be here as dad said I'm
a PhD student at Princeton final year
PhD student my area is broadly in
computer architecture and today I'm
going to talk about my thesis work in
specifying and verifying memory
consistency models I want to start with
some very high level overview and then
I'll work my way towards why consistency
why consistency is important why we need
to be more careful about specifying it
and then what my approach is to
verifying it so this slide is nothing
new what we know everything is
multi-core that's not going to surprise
anybody here the only thing I really
want to say is that this is really true
across the spectrum from supercomputers
and data centers all the way down to
mobile phones nowadays even have four or
eight cores in them so really this is
memory consistency is becoming a thing
that really applies across the entire
spectrum of computing we have to make
sure that the consistency models that we
use and build actually reflect that fact
and in particular i mentioned mobile
devices not only because they are
multi-core but in fact they're much more
than that they actually are
heterogeneous nowadays they're very
heterogeneous and even more and more so
with every generation that comes along
so if you look at a typical mobile chip
nowadays it looks something like this
where you might have CPUs the way and
that's what you would normally think of
for memory consistency models but really
that's just a small piece of the chip up
in the corner there chip nowadays would
have GPUs and accelerators and digital
signal processors and all kinds of
things that haven't in the past been
considered in the same way relating to
memory consistency models but now they
have to as they become more and more
programmable we have to make sure that
they also get included into that space
as well because they're all
communicating via shared memory so from
an architect's point of view the picture
might look something like this complete
with architecture style cartoon diagrams
and everything but really the point is
that these
they don't just all live on the chip but
they all communicate via shared memory
and in particular nowadays everything is
converging towards shared virtual memory
so it becomes easier to share things
like pointers and data structures with
pointers back and forth without having
to do all kinds of translation back and
forth the way it used to be for CPU GPU
a few years ago when it first got
started now everything is converging
towards the shared virtual memory
paradigm and that's true even sometimes
when the physical memory space is more
complicated so even if you think of say
nvidia discrete GPUs for example they
might still have a physically separate
piece of memory for the GPU in the CPU
but even then they tend to share the
virtual memory space for exactly those
same reasons so that's all important
that's good we need we know we need to
build consistency models to address
things like this but then last piece of
it is that we don't it's not just about
building and designing these systems we
also have to verify that they work
correctly and don't have any bugs in it
and unfortunately that's a very
difficult thing to do and as you can see
behind me there's some there's been some
bugs that have appeared in the memory
systems with somewhat regularity so
there's been a few years ago there was a
bug in the AMD tobs that affected the
way tran memory translation was taking
place and then just a couple of months
ago now until released the fact that
they had a bug in their implementation
of transactional memory and so their
solution for that at least for the chips
that had been sent out so far was to
just simply disable it and say you can't
rely on it and might have a bug and at
least in some corner cases and so the
only safe thing to do is just turn it
off and so that's pretty unfortunate
that's not really what you want you want
your features that you're advertising to
work and so what this means to me is
that we need to have some more
intelligent way to go about verifying
that these features memory consistency
transactional memory and so on are
implemented correctly at a micro
architecture level so that typical
approaches that are used in terms of
testing and just brute force testing to
make sure that you don't see any illegal
outcomes appearing that is great but
it's very expensive and it's not very
it's not as intelligent as it may be
could be so that's what what my main
topic is going to be today about will be
about how to do this microarchitecture
level verification at a more
intelligent way okay so this is the
overview of my talk today I'm going to
focus mostly on the first project here
it's called pipe check and that is the
project to specify and then verify
memory consistency behavior at a micro
architectural level as opposed to an
architecture level and then compare
architecture and micro architecture to
each other and make sure that the ark
the micro architecture is correct and
then at the end I'm going to talk very
briefly about follow-on project called
armor which would be more about back up
to the architecture level how to specify
things and then beyond that how to
translate between memory models if I
have say the example of arm process or
mobile phones nowadays if you have a
piece of code compiled forearm and you
want to run it on x86 processor because
Intel wants to implement Android or
something like that then now we have a
way to describe situations like that as
well so i'll talk about that briefly at
the end okay so I want to start by
talking about why we need memory
consistency models in the first place
just to make sure everybody's caught up
and then from there I'll move on to my
focus on hardware and then into the
microarchitecture space so first of all
why consistency models at all why do we
even need to worry about this and so I
tend to think of it as being along a
spectrum something like this where on
one side you have this notion that
things tend to be strongly ordered and
it's more and everything is more
intuitive that way if I'm a programmer
and I say that I have a store 2x
followed by a store to why it makes
sense to me that everything would stay
in that order all the way through it
just it starts to get very hard to
reason about things especially in your
head if things are getting reordered
compared to what you the order that you
write them down in originally on the
other hand if I'm an architect which I
am and I looked at what i can do to
improve things like performance what I
would say is I would be a good architect
I would do my profiling I would see that
most accesses tend to be private to a
thread they might be accesses to a stack
or to private variables things like that
and if that is the common case meaning
that they don't need to be that we don't
need to worry about strict ordering for
private accesses then i can say i might
as well reorder them and let them not
worry so much about whether they appear
in different orders to different threads
things like that and in particular from
a micro architectural point of view that
allows me to add things like store
buffers and re-write coalescing buffers
and things like that so if I have a
cache miss on the store 2x I don't have
to have the store to Y and the load to Z
and everything else waiting in line
behind it they can actually go around it
they don't have to wait in line so on
one hand I get more intuition on the
other hand I get better performance and
at least from a hardware point of view
as you can see in the middle on the
spectrum here there's really no single
answer to this question of where exactly
do we fall in line between these two
extremes there's been some strong memory
models in practice there have been weak
memory models in practice and it's
really so like I said there's no single
answer and it's even a challenge to
define where exactly these things are
it's not necessarily even a simple
spectrum like I've drawn it here but
really it's a challenge to even figure
out how to write down some of these
models in the first place from a
software point of view what it looks
like is this where we would say that
because we want to write portable
software and we don't know how any
particular piece of hardware is going to
behave in terms of memory be ordering
what we would say is that certain things
are we have to write our software in a
portable way that doesn't rely on any
particular ordering constraints so a
situation like this where we have a
typical memory consistency model litmus
test or test written to test a
particular feature of a consistency
model this one is called message passing
and what's going on here is that we have
a producer wants to send a message X
from the producer to the consumer and
the way we do it is to say we have a
store 2x followed by a store to a flag
why that says the message is ready and
then on the consumer side we do it in
the other order we read the flag first
and then we read the message to see if
it's the value that we know we should be
expecting in this particular case and so
if we go back to the hardware picture
where we don't know what reordering zar
possible it's possible that either the
compiler or the hardware might take
these two stores 2x and 2y on the
producer and reorder them and then we
get behavior that doesn't make sense so
from a software point of view this is
considered to be a data race which is
ruled out as illegal because we don't
know what's going to happen in the
situation of a data race so what the
software people have said over the years
is that
rather than having undefined behavior
like this become possible we just rule
this out and say the only way we can
have communication between threads is to
say we have to have explicit
synchronization between them of some
form so maybe it looks like this here's
an example using c11 Atomics although
the same might be true whether it's
mutex unlock in mutex lock or maybe it's
even GPU colonel end of the kernel and
then wait for the end of the current on
the other side whatever the case might
be sometimes these things are wrapped up
in higher-level constructs but the idea
is the same that we need to have some
kind of explicit synchronization between
the two different threads to make things
happen correctly and then so what it
looks like is we have in this particular
case we have the c11 notions of sequence
before and synchronizes with and so on
but really what we want to do at some
higher level is to say we have this
happens before relationship between the
right of X and the read of X to say that
the read the right had to happen before
the read and therefore the reed is going
to return the correct value and then so
the question is so that's from a
software point of view the hardware
point of view is to say we actually do
want to specify what reordering zar
happening because at some at some level
these synchronization constructs shown
here are going to have to turn into
whatever synchronization is necessary at
a hardware level whether that's
preserved program order meaning things
just naturally stay in order or
sometimes you have to insert fences to
restore the orderings things like that
the first question is how do you even
specify what needs to happen in the
first place and then the second question
like I hinted at in the beginning is to
say even once you specified it that's
great but then you have to still make
sure that what you specified is what's
actually happening on a particular
microarchitecture and that additional
challenge there is that there can be
lots of different microarchitectures for
any one given architecture right if I
have say an x86 specification I can
implement that as a processor with a
store buffer and see to a total so
ordering TSO as required I can implement
that as a sequentially consistent core
and that's strictly stronger it allows
fewer V orderings than TSO would but
it's still a correct processor and even
within TSO I might have the small core
Adams that's maybe an inorder core my
have a big out-of-order processor
superscalar all kinds of things that
might happen whatever happens at the
microarchitecture still has to be
implemented correctly according to what
the architecture says so that's what my
talk is going to be my pipe check talk
is going to be about and so the first
thing you might do to specify what a
hardware memory model looks like would
be something like this this is the kind
of detail that you tend to see in
architecture level specifications where
on one side I have x86 total store
ordering on the other side I have some
approximation of what the power memory
model is and so the general idea at a
high level is to say that if I have a
pair of accesses in program order from
the same thread maybe a store followed
by a load or a store followed by a store
if that's the case then I need to know
whether that pair of accesses is reorder
abul or not so on x86 the only pair of
reacts esas that re orderable or a store
followed by a load whereas on power it's
pretty flexible almost anything can be
reordered so it looks like it's simple
enough to write down what's going on but
unfortunately it's actually more
complicated so if I ask God what the
memory model for power would be it's not
going to be that it's going to be
something much much more complicated
where it has dynamic behavior is being
added in it has you have to find the
fixed point of some recursive
definitions of instructions in the chain
and the pipeline here it's much much
more complicated than just saying that
it's this nice little table here so so
if i say store 3 into X and then store
fall into it surely even on the power
I'm not my going to end up with three
yes exactly so even for x86 which looks
much simpler in comparison even this is
not sufficient enough to describe all of
the behaviors of the memory model for
the reasons you just described that
there's more to it there's a dress you
call me ona presumably exactly there's
things like that there's things like
multiple copy atomicity there's things
like dependencies all kinds of stuff
that has to get added into the model to
really get a fully complete description
of what's going on so even this piece
that I'm showing here at the bottom is
just the preserved program order part of
the power memory model it's not even
talking about all the address and data
dependencies in the behavior of
different fences and things like that
it's really there's it's much
more complicated than just saying it's
this little table here and that's
exactly the point is that it's a
challenge to write these things down and
to know that it really captures all the
corner cases of the model you can write
down tests that are six or seven
instructions in size and still not know
nowadays even still how those particular
test behave so it's really challenging
but yeah that's a good point ok so
anyway once we have a model assuming we
do have some model and we made good
progress as a community and figuring out
how to write these down and improve them
to make sure we have complete coverage
what we did what we then do is we take
this particular litmus test and here's a
more abstract representation of the same
software example I showed earlier the
message passing test and what we do is
we write it down in such a way that we
can draw something like it happens
before graph or at least this is one
approach the axiomatic approach to draw
a happens before graph based on what we
see either in terms of static properties
such as preserved program order which I
just showed or based on dynamic
properties such as one instruction
reading from the value written by
another instruction so here's an example
showing like I said MP message passing
and in this case what we have is a
preserved program order relationship
because the store has to be ordered with
the store we have a reads from
relationship based on the fact that if
we say register r1 is returning the
value one based on our hypothesis and
the only place it could have gotten that
value is from instruction I to the only
thing that's storing a value of y of 1
to y and then we have another preserved
program order relationship on the right
side and then we have what's known as
reads from earlier or from reads in
which case I for execute and returns its
value before and I one in this case
writes 12 X and so what we have here is
a cycle in the graph and what that would
mean intuitively is to say that an
instruction happens before itself
somehow which is it doesn't make sense
for that to happen and so what we say is
that a cycle in this graph means that a
execution is forbidden so that sure lied
on something you said the only place you
could have gotten that one form would
have been this store that sounds like
sort of fuzzy so concept so take my
place you could have got this value form
so we do two things along those lines
first of all we generally assume that
everything is zeroed out at the
beginning of time just to make it easier
to think about but then in general and
more importantly I think that if you
have an execution that might have lots
of different stores of a particular
value than what you would do is you
would check all the different
possibilities so if I had lots of
different stores to one I would first
check the scenario where this load reads
from the first store and then I would
check a different scenario where the
load reads from the second store and so
on i would have to check all the
different possibilities and so for a
small litmus test like this we're
writing it for a particular purpose it's
generally written with the intention of
having to force it to read from a single
location by design but in general from a
broader program it could be much more
complicated there'd be lots more dynamic
scenarios okay so this would be the go
ahead how did you know to have a
different relation for reads from and
reads from earlier I mean is it clear
from the from the code on the left that
you need to use these two differences or
could they both be reached from so in
this case it's because it depends on the
value that gets returned by each load so
if we say that our one has the value 1
at the end of time that means that I
three must have read the value from I to
because that's the place that it got one
from on the other hand if I for returns
the value 0 then it couldn't have
happened after I won at least from the
point of view of I for because it
returns that belly otherwise I would
have returned the value 1 okay so that
would be the nice cases if we could just
say cycle equals forbidden no cycle
equals permitted but unfortunately
because of all the more permissive
consistency model behaviors what turns
out to happen is more something like
this where we have a slightly different
litmus test against small but what's
going on here is we have a store 2x
followed by a load of X and then on the
other side we have a store to y followed
by a load of y right away and what
happens here is we can do the same kind
of analysis we can say that there's
reads from edges there's preserved
program order edges there's
reads edges and again looks like we have
a cycle but what happens in this case is
because this reads from edge these two
breeze from edges actually represent
potentially at least forwarding from a
processors local store buffer it turns
out that this particular case a cycle
this outcome is observable in practice
and hence the cycle is not enough to say
that this outcome is actually forbidden
and so what we need to do to actually
capture all the details of consistency
models is to say that we either need
more complexity in the way we describe
our edges which is one approach that
works and you can say that maybe reads
from internally reading from the same
thread is not something that
participates in your cycle finding
process or it doesn't compose
transitively with other edges the same
way that PPO or from reads or different
edges might depending on your
consistency model a different approach
would be to say that rather than adding
complexity to the edges you add
complexity to the nodes and so in this
case what you would say is take a
particular node that was there before as
one single thing and split it up into
different pieces may be performing once
with respect to yourself and performing
at a second step with respect to all the
other processors and all the other cores
on the system or in general there might
be multiple nodes that a particular
single node splits into as you go and so
in this case if i draw the separate
nodes you can see that again there's no
cycle in the graph and so what you want
to do is either you add the complexity
to the edges and figure out which cycles
actually do mean something or you split
up the nodes and then you can get back
to the point where a cycle does
correspond one-to-one hopefully with a
forbidden execution and so for pipe
check when I mean I'm going to what I'm
going to do is I'm going to take this
approach to say split up the nodes and
really map that deeply into what the
microarchitecture is doing I think this
is a more intuitive notion for what
happens at the microarchitecture so what
that looks like here is if I go back to
the message passing example for just a
second if I use this analysis where I
split up the nodes in two different
pieces and then I want to say I want
sorry I want to figure out how this
litmus test gets mapped onto this
particular store buffer or this
particular five-stage pipeline with a
store buffer and so like I said what I'm
going to do is
going to split up these I 1 and I 3 i'm
sorry i 1 and i 2 stores in two
different nodes and it's going to look
like this and I'm going to make a
correlation between these different
nodes and different pieces of the
microarchitecture so I have a these two
stores these two nodes in the expanded
graph represent accesses entering the
store buffer and these two other nodes
and purple represent accesses reaching
main memory no just to note is pretty
arbitrary because for example you know
but they basically the way of you it is
there exists multiple internal states
that are somehow observable to others
would say so so it could be the case
that you don't have splits like the note
into or just just one note but you know
multiple ones in the middle and then
have different guys being able to
observe different intermediate things
right absolutely so that completely
depends on what the architecture says so
for something like x86 TSO then maybe
two nodes is sufficient for something
like power arm things like that then you
might have to expand it in a lot more
nodes so I completely agree with that it
so from there's also the distinction I
want to make especially from a pipe
track point of view that there's a
difference between what the set of
things that might be permitted on any
given architecture by spec and what any
particular single implementation allows
by spec and those two even those two
might be different and so that's the
that's exactly the comparison i'm going
to draw here you just messing up the
notes is sort of a way to capture the
different visibilities of stores
basically is that sort of the starting
point of how you split the news or is
that a wrong assumption no that's
exactly correct that the store might
become first visible to the issuing
processor because it's in the store
buffer and then so any subsequent load
would come along and be able to read it
but while it's sitting in the store
buffer the other threads may not be able
to read it yet exactly when you said
write these events out I you come up
with some micro architectural picture
like the one on the left how do you know
that it corresponds to you know the
actual micro type architecture of the
process so that's the rest of note
that's the right exactly the upcoming
slides so actually if I click Next if I
go back to this picture and then
what I'm going to do is translate this
picture on the left into a much more
expanded graph so it's not just about
the points where it reaches memory but
where it actually reaches each
individual store of each individual
stage in the pipeline stage by stage and
I'm going to create what I call a
microarchitecture what happens before
graph rather than just the architecture
level happens before graph so if I work
it out I still have this same
correspondence between nodes in the
previous graph that I showed in orange
and purple and micro architectural
locations but now it's not just those
two it's actually those are just two
among the many possible stages in any
particular microarchitecture and again
like I said this may be different in
some implementations may be there is no
separate orange and purple here maybe it
is joined together and that's legal it's
illegal might be a legal implementation
but it's really what we want to do is
compare but the spec said on the
previous couple of slides to what a
particular microarchitecture is doing on
here ok so let me zoom in on different
pieces of this graph to show you how
it's created step by step so I'll start
in the corner here so the first thing we
do is to say that program order is
something that happens at the fetch
stage that's our starting point but what
we don't do is we no longer assume
anything about preserved program order
it's not something that we say that
we're going to assume somehow that order
that excesses remain ordered when they
get to the point of view of the cash or
the memory hierarchy for example instead
what we're going to do is only assume
program order at the very beginning of
the pipeline and then for everything
else working down the pipeline we have
different arrows representing just
behaviors that is stage by stage level
so for example what I'm showing here is
an inorder pipeline and so i can say
that maybe the decode stage has two
instructions that come in in order and i
know something about the decode stage
itself as a self-contained unit that is
also an inorder stage so if things come
in in a certain order they also go out
in a certain order and so that
translates in the graph to say that if I
have this blue edge on top or any edge
on top coming into the decode stage then
I also have the corresponding edge going
out of the decode stage and in this case
that can propagate all the way down i
also have
the path of each instruction as it
executes going from fetch to decode to
execute and so on down the pipeline and
again this is another example where we
might have multiple different scenarios
depending on the fact that there might
be different paths that an instruction
can take if there's maybe different
parallel pipelines that a single
pipeline might split into different
paths somewhere or loads and stores
might take different paths through the
pipeline if there's multiple
possibilities like I said before we
enumerate all the different
possibilities and create a graph like
this for each of the possibilities see
if the execute stage was not in order in
this particular example for instance so
in this case it would be in order all
the way down until at some point if I
have a say an out of order issue stage
which will have in a couple of slides
then I would be missing one of these
arrows going across this is a cruel
servers know exactly and then also not
just that one arrow but then everything
else that would depend on that arrow
would also be missing in that case so as
I work my way down I also have the reads
from and from reads edges that I had in
the architecture level descriptions but
then what happens as i zoom in is i
actually have a particular point in a
pipeline where these micro architecture
or these reads from and from reads edges
are also taking place so for example if
i translate a reads from edge that we
saw in a like we saw on the message
passing litmus test before one
possibility would be to say that a load
when it reaches the memory stage it's
reading from a particular instruction
that must have passed through the main
memory hierarchy and so we have a reads
from edge from this what i call a
performing location in the pipelines
from this performing location to for the
store to this performing location for
the load up in the corner but that's not
the only possibility and this is one of
the reasons why i like that more complex
nodes instead of the more complex edges
scenario is that maybe this is one
possibility but if i draw it a little
bit differently if i want to reflect the
fact that excesses might enter the store
buffer a little bit early before they
reach the main memory then i can have
different possible outcomes as well
where reads from edge goes from a
different performing location to the
loads performing location so i have two
different possibilities that i can check
and they could potentially lead to
different outcomes in the end and in
the words they could maybe one could
produce a cycle and the other may not
produce a cycle and so not only does
this tell us why the multiple node graph
is maybe more intuitive from a micro
architectural point of view but it also
gives us some idea of why exactly
different scenarios give us different
executions so it might point out
explicitly that it's something about
store buffering and it something about
an access not having reached main memory
yet or something like that so if I go
back to that sixth instruction litmus
test that I showed a little bit earlier
there was the one that motivated us to
have to add this different this added
complexity to the graphs and this is a
great example of why having the
microarchitecture level detail helps in
that case so if I treat this first of
all as a scenario where every
instruction has to access main memory
and all all memory accesses are
performed through main memory only then
I do get a cycle in the graph and
there's no way that that can happen that
forbidden outcome can happen but on the
other hand if I add the store buffering
behavior in and I have the reads from
edges performing at different locations
within the pipeline then I have a
different graph that looks like this in
which case there's no cycle in the graph
and so not only is the outcome allowed
but like I said before we actually have
some more intuitive notion of why that
outcome would be observable on this
particular microarchitecture and then
following that like I mentioned for
preserved program order what we would
want to say if we want to go back to
those tables or the more complicated
form of those preserved program order
tables that's a maybe a store to store
in this case has to be ordered with each
other and so what we would want to do is
to check that the first store reaches
the main memory of the memory hierarchy
before the second store reaches the
memory hierarchy that would be our
condition for a preserved program order
and so what we do from a micro
architecture happens before graph point
of view is to say that we have a graph
that looks something like this maybe we
have some extra edges in there to
reflect some special behavior of the
store buffer so in this case we would
say that maybe the store buffer only
allows one outstanding access at a time
and so we get this diagonal green edge
and if we put all these edges back
together whatever they may be and we can
say that that arrow that we were looking
for for preserved for
graham order is actually a consequence
of some sequence of microarchitecture
alleges it's not just something that we
have to assume anymore something we can
actually check for directly so that's
also great we now have way to check this
kind of thing directly and it's a little
bit it's a great way to verify that
whatever we expect to happen at from
architecture point of view actually is
happening in all the different scenarios
of a microarchitecture point of view one
of the other nice benefits of having a
level of micro architectural detail is
that it also allows us to model things
at a micro architectural optimizations
that simply can't be addressed at all by
architecture models because by
definition they're meant to be
independent of any particular
microarchitecture but then what that
means is you have all kinds of crazy
things like speculative load reordering
and out of order processors and fence
speculation all kinds of stuff that's
happening at a micro architecture level
that just can't be described in those
models and now we can have a way to
describe them at a micro architecture
point of view and still verify that what
they're doing is legal according to the
spec so in this example I have a case of
speculative load reordering where what
would happen is to say that if I have
two loads that are supposed to be in
program order and in preserve program
order with each other but suppose the
first load has a cache miss well the
second one has a cache hit from a
migratory microarchitecture point of
view what tends to happen in some
processors is that the second load will
execute speculatively it'll say just
read the value that you have for now and
move along assuming that it's correct
and when the first load comes along and
actually does get a chance to execute
that load that first load execute then
the second load can now safely check
whether the speculation it made earlier
was correct if it was correct then you
just keep going and you've gotten your
performance benefit from speculation if
it wasn't correct then you go back and
flush that second instruction and
everything that came after it and retry
it with the correct value and so from a
consistency point of view what that
means is that strictly from comparing
one that two loads reach the execute
stage they might not actually execute in
order they might actually execute out of
order and still have it somehow be
correct and so from a microarchitecture
happened
before point of view what we do is add
features like saying this particular
edge again this is a message passing
litmus test where we have the two loads
that are supposed to be in order if they
are not actually executing an order what
we say instead is that we can model some
features of the microarchitecture in
this case checking for cache line and
validation if that's the way that the
speculation is implemented we can say
that preserved program order actually is
implemented this way we have we're tying
into the fact that speculation is
actually part of the pipeline and so we
still have the cycle assuming that our
speculation is correct and we have a way
more importantly we have a way to model
this speculation without just saying
that it's somehow magically still by as
still meets the specification without
knowing exactly how it does it this pipe
check approach tells us a more
fine-grained way to analyze exactly why
speculation is correct in what it's
doing and the other nice thing is that
this pipe check approach is it's not
limited to homogeneous pipelines so if
you think back to the motivation that
everything is becoming more and more
heterogeneous pipe check itself can
easily extend to heterogeneous pipelines
as well there's nothing fundamentally
different about it we just have
different sets of stages on one side
than we do on the other side the same
holds true even the same analysis still
works even if the pipelines are
heterogeneous so what we did to test
pipe check is we actually implemented it
as a tool that does all of the
enumeration of different graphs and all
of the checking for cycles and things
like that automatically all you do is
you specify the behavior of a particular
pipeline again at a stage by stage
granularity to say that maybe the decode
stage is in order the issue stage is out
of order whatever the case might be all
the way down stage by stage the stages
are the definitions of the
microarchitectures themselves they're
not all that big you can see they're
just maybe up to 100 lines or so of code
even in the most complicated case and we
have anything from the very simple
pipeline that I showed earlier the
five-stage pipeline to the gem 5
simulator pipeline the most detailed
model to the open spark pipeline to get
more of an industry strength pipeline
also modelled in there
and so we write down the set of stages
in the pipeline the set of performing
stages as I talked about earlier and so
on some of the other little details that
we need to add into the model and we
take that and we and pipe check then
performs all the analysis to say that
what I want to do is I know from
architecture point of view from some of
the other work in the other tools that
are out there I can say that I know
whether a particular litmus test is
forbidden or permitted by the
architecture specification and then I
can also take the pipe check model to
say whether it's observable or not
observable on a particular pipeline a
specific instance of that architecture
and then I can compare the two to each
other so sometimes there might be a
match between them if it's permitted and
it's observable or if it's forbidden and
not observable then that's great I might
have a scenario where an outcome is
permitted but not observable on a
particular pipeline which is also
correct there's nothing wrong with that
it just is a quantitative way to say
that a particular pipeline is stronger
than it needs to be in terms of memory
consistency on the other hand if I have
a scenario where an outcome is forbidden
but it turns out to be observable on a
particular pipeline then that's actually
a bug in the pipeline and not only have
we found it then but pipe track gives us
a way to actually zoom into the
microarchitecture and figure out exactly
where that missing arrow is and what we
think we might need to do to fix it what
might be the missing edge in there no
possibilities it makes me shiver a
little because I think it's quite large
number is there is it is that intuition
wrong or you clever about pruning the
hosters social space so that's a great
question so fortunately we don't need to
be particularly clever in this case
because it turns out that most of the
litmus tests tend to be very small
they're no more than four to eight
instructions in size usually and so what
that means is you have a graph that's
maybe a dozen or so pipeline stages in
length and maybe eight instructions
across and it's on the order of 100
nodes in a graph that's very quick so if
we if we were to extend this naive
approach to testing broader programs as
a whole it would run into the state
space explosion quickly but for litmus
tests that's not the case it tends to be
very scalable
and I'll show some results in a second
it was also be some kind of small small
model thing where you know that doesn't
you know if you can just test all kind
of small programs then just be
completely shorted done everything right
just taste small examples sorry I missed
that so so you can you can you can test
small examples and that would give you
some kind of completeness for kind of
all posts pork sandwich or something
like that so so that is a great question
and I think if I understood the question
correctly that it's you can test some
set of test cases but you don't know if
that's complete although i'm saying is
probably it's probably doable to find a
set of small test cases that is complete
that's my conjecture so what we do have
I don't have any particular proof that
anything is complete what we do do is
check directly for preserve program
order first of all and then we have the
fact that researchers in the past some
people in the room even have come up
with large Suites of litmus tests over
the years to test all different cases
that they can think of and even use
computers to generate lots of different
cases and really try and make it as
exhaustive as possible so I think that's
really what we're going for is as much
coverage as we can get from litmus tests
and hopefully that tends to be
ninety-nine point something percent and
whatever that mean I mean there's no way
to at least at this point what we've
done so far we don't have any formal
proof of completeness given a set of
litmus test but word we just try and be
try to be as exhaustive as possible and
pull from others who have done similar
approaches yes if you find a bug you'll
know that you're asked review of the
pipeline is has a bug right that's a
good thing but the lack of such evidence
in particular if you only can test the
small litmus tests it's not a very
strong indication that your pipe owners
actually but free right because you
might be missing a lot of practical
corner cases that are not part of the
litmus testing framework right
intentionally so because it only looks
at the memory instruction so you might
be missing the ugly cases where you have
many branches for example or or other
instructions that are not part of
usually in part of litmus tests so I'd
be curious to see what future work on
shows you might be looking into
to to take another corner it's a great
question i think even another concern
that tends to to come up especially when
i talk to companies about this kind of
thing is that maybe a particular load
what it looks like a load from an
architectural point of view gets may be
split into multiple loads in the
microarchitecture because it's a
unaligned access or something like that
there's all kinds of stuff and yet
branches so we don't have any hundred
percent guarantee that if this works
that everything about your pipeline is
correct we're just trying to get closer
and closer to it and provide a general
approach that says if you need to split
up a particular I don't have a picture
anymore but if you have a particular
load that you want to split into
different loads than that works the same
approach is still holds true and you
just check whatever you might have and
then ideally we would be able to say
something like it's not just preserved
program order but we would check that
any fence constraints are guaranteed
directly not just through a litmus test
but through that replacing an
architectural edge with the sequence of
microarchitecture alleges that kind of
approach that would be the more general
case that would make me at least more
comfortable that everything is complete
but limits tests are a great sanity
check if nothing else and they tend to
have pretty good coverage directions of
existing my parks pictures right yes are
they tight the descriptions did you try
weakening them and see whether they are
still in the memory consistent smooth so
in some ways actually we may not have
intended to do that but sometimes that
is just part of the natural process of
coming up with the model that we would
so for example the first time we would
come up with a model for a micro
architecture it might come back as
having failed a few tests and what that
might mean is not that the micro
architecture is broken right away it
might just mean that we missed something
about the model that we need to go in
and say oh this little piece is actually
stronger than we need to it's stronger
than we thought it was in the first
place and we have to go figure out why
that's stronger and then check the
updated model again things like that we
didn't necessarily go back and check
whether things are minimal because in
general they may not be minimal the
pipeline might be over constrained and
that's still a legal way to build a
pipeline so what we're the goal of the
approach any way is to say that well I
first of all we would want to be true
too
the microarchitecture is doing but then
beyond that what we are hoping to do is
to say that we're reducing the burden
from having to check something about the
chip as a whole with billions of
transistors on it and instead just zoom
into particular stages and say that
maybe I have the RTL for a decode stage
and it's much easier to say something
about the decode stage being in order as
a self-contained unit rather than having
to verify the whole chip all at once and
so we're really just trying to zoom into
individual stages and make the burden
for the verification of RTL to say that
you're verifying local properties rather
than any big global properties all at
once such descriptions say 45 state six
stages the trip then you can just make a
graph conclusion problem the whole
notification yeah so maybe a different
approach or a different interpretation
of that question would be to say that if
I'm arm and power or people like that I
want to have the weakest possible
implementation of my already weak spec
and I want to get squeezed as most
performance as I can possibly get out of
it then I could do exactly what you said
try and take the minimal graph that is
correct and use that as my micro
architectural goal and so we haven't
tried that yet but it's an interesting
idea that yeah we could try in the
future okay so we have this tool does
all the enumeration for you and just to
quickly reassure people that were asking
about the performance and the state
space explosion this is just a numerical
proof that it doesn't blow up in
practice so this is testing oh I don't
know if I mentioned this before this is
testing all x86 pipelines we were at
least at this point we're focused on
preserved program order as our primary
verification step and we're hoping to
extend that to other features in the
future but we're testing preserved
program order and we were testing the
litmus tests for x86 so those are shown
as the different cases along the x axis
along the y axis we have the runtime and
across long the different bars in
different colors we have the different
the four different pipelines that I
showed and as you can see even in the
worst case it's no more than a few
minutes to run all different cases of
the litmus tests check all the graphs
and make sure that everything is correct
and this is even too
actually given even though we actually
implemented this tool in Coke it's a
theorem prover which is we're trying to
start from a more formal basis to begin
with so we can also connect other more
formal verification tools at some point
in the future we want it ultimately we
want to have this be part of a rigorous
stack maybe all the way from software
down to Hardware down to
microarchitecture are from software
architecture to micro architecture and
so our goal was more on correctness our
focuses on correctness rather than
performance and so this auto this is all
the performance numbers of a tool that's
auto extracted from coke 20 camel and
then executed in that case if we were
really optimizing performance we could
have really squeeze this even further
this isn't really an upper limit of
performance this is just doing the most
naive thing and we still get really
satisfactory performance even in that
case so that's one of the great
takeaways of this is it's a smarter way
to verify things that a
microarchitecture level without even
having to worry about state space
explosions or unscalable verification or
anything like that and the other thing
we can do is look at particular litmus
tests that come up and say if I want to
know exactly what worked what didn't
work on a particular pipeline that gives
us a lot of information as well i can
say that across the four pipelines two
of them matched everything exactly what
the architecture said in one case in the
simplest pipeline which didn't even have
a store buffer that was just the
five-stage pipeline then there are some
cases where an outcome is allowed by the
architecture but not observable on that
particular pipeline because it doesn't
have a store buffer intuitively and so
pipe check finds that for you and points
it out to you and again like I've
mentioned it's not a bug it's just
interesting to know that you have some
quantitative way to say such a thing on
the other hand in the gem 5 pipeline and
have others also observed in parallel to
us there's that they're used to actually
be a bug in the way that it implemented
load to load reordering it implemented
it correctly for accesses to the same
address but not for accesses to
different addresses and so there was a
case where the illegal outcomes
according to the architecture actually
became observable on that particular
pipeline and so pipe check highlighted
those as well and then that let's assume
in
exactly what went wrong and figure out
where in the graph to draw that missing
edge that we think needs to be there to
forbid these outcomes again yes and in
these cases that was the bug if you fix
that one bugged and all these go away
yes okay so that is the conclusion for a
pipe check that we have this nice
quantitative and more rigorous way to
compare what a micro architecture does
against what the architecture says
should or shouldn't happen and it lets
us do all this verification in a much
more rigorous way let's just do it in a
scalable way and we have this tool that
you just give it the model and you tell
it local properties that are much easier
to check individually and it puts them
all back together and does all of the
analysis for you to verify that the
microarchitecture behaves as it should
any questions I'm curious what's the
advantage of using this tool over
running the actual litmus tests on the
actual implementation directly what what
is the difference that you learn I think
you have that extra in direction of
someone having manually to to care about
that abstract description from at least
your life it's really messy code and
I've been in there so what do you learn
about understanding while your abstract
pipeline has a bug how hard is it to
translate that back to well actually the
real thing had a bug and what's the
comparison to running dash litmus test
on the thing so I would say that this
isn't necessarily meant to replace
running it actually on the processor
because that's really the greatest way
to check whether it's observable or not
observable in practice what I would say
is that in the past there have been
papers that have done there have been
plenty of papers and research about how
to just run the test and then if you see
a failed test you have to do some
detective work to say where exactly did
this go wrong is it somewhere early in
the pipeline that it went wrong is it
somewhere maybe just the store buffer
was only supposed to let out one thing
at a time and it actually let out lots
of things at a time and things like that
without having some more zoomed in view
of what's happening you don't
necessarily know where to look and you
have to just look all over the place
whereas if you have the
microarchitecture graph
can say that i'm looking for an edge
between these two nodes and so i know
that there must be some hedge somewhere
in this vicinity maybe it's because of
the store buffer diagonal edge that i
showed earlier maybe it's something else
nearby but it's probably somewhere here
because i know that maybe everything
else is in order already and so it
probably isn't in that domain so you do
have a little bit more some little bit
more knowledge to work with and doing
that detective work and then the other
nice thing is that we do have it does
lower that rtl verification burden
assuming you get your micro architecture
description correct to only having to
verify things at a stage by stage
granularity right yes yeah I think that
last point is the critical one where you
have to be lucky well lucky enough in a
way to replicate the bug in the micro
architecture in your abstract
description and I'm wondering how likely
that is so how how did you replicate the
bug from the gym five pepper in your
description how to dab sort of happened
isn't easy to recons you construct that
sort of process so at this point it was
pretty much a manual process we did have
to go into that messy code and try and
come up with that model ourselves in it
that was a little bit of an iterative
process you basically you captured the
buggy into leaving as well in your model
as well because i think the things are
not really obvious right they kind of
are working corner cases i presume yeah
so actually in this case for the gem 5
example it wasn't actually our intention
to create the microarchitecture to find
the bug per se we were going in assuming
that it was correct and we came up with
the model it didn't it showed a bug and
then we kept pushing on it and
eventually we said we can't figure out
anything else in this code or you could
say the same thing about RTL we can't
figure out anything else to say about
the RTO maybe it actually is a bug and
then once we did that we actually did go
back and run the litmus test on the
simulator hardware and then he said it
is actually observable and here's the
proof this would be compositional on the
level of instructions say if I want to
then what you run your tool I'm
something that has more than three or
four instructions going to just go
testability I get to grab and run
through a little thing or can it can use
somehow split up into smaller groups so
that is the goal to make it
compositional and that's I think one of
the benefits of the more
complex nodes case as opposed to the
more complex edges case is that if you
expand the nodes then you don't have to
worry so much about edges not being
transitive with each other if we just
have in this case we have a micro
architecture happens before edge is a
micro architecture happens before edge
and they're all transitive with each
other and so you could compose them with
each other you could compose either
different stages with each other you can
compose different instructions side to
side with each other and it should all
work correctly so we haven't we haven't
pushed as far as hard on this for things
like the power memory model yet but
that's still the goal is to make
everything be composable in exactly that
way because we think that really helps
it be scalable for exactly those reasons
some sort of abstraction layer just
above because you could explicit
incompetent as you get the proof from
the underneath there yeah exactly so
that we started in coke like I mentioned
as a way to ultimately make it formal we
haven't gotten to that point yet but
that would be something we could
definitely try and push on yeah how do
you relate your pipeline models to the
models of the actual microarchitectures
you're studying especially if their
proprietary ones so that was actually
the challenge I mean so this was a
collaboration with Intel and even so
they wouldn't they weren't willing to
release any Intel microarchitectures to
us and have it be publicly published and
so that is part of the challenge that
we're just trying to get our hands on as
many pipelines as we can and just keep
implementing them and see what comes out
of it and so we did manage to get the
open spark which we were happy about
because that is a commercial strength
processor that we were still able to
model but that's it is a in order
processor it's very parallel but it is
still in order and so the more if we can
get more microarchitectures that are
available then we would love to keep
adding them we're just working with what
we can get as academics instead of
industry researchers really understood
just how microarchitecture all your
micro architectural models are hair so
the diagrams that you show look if
you're more or less just splitting
events
now splitting right events Andrea vents
is that actually all that you're doing
so there's more contact so you can
imagine a more concrete operational
model let's say in which you have all
all the details of how instructions at
access registers and how that gives rise
to dependencies and all that kind of
thing is that part of the modeling here
so that particular piece of it their
registers and non memory accesses at
this point are not part of the model
although we could potentially add them
in and they would float through I and
theoretically they would flow through
the pipeline in the same way and just
not float through the store buffers and
things like that so in order to model
dependencies register dependencies and
things like that properly that would be
probably one thing that we would need to
add at least in some way to capture all
the details of that but from a higher
level point of view although I did
describe things as being a stage by
stage level an instruction by
instruction level even that does have
some flexibility in exactly how detailed
you make stages so if you think of
execute stage might actually if you get
deep enough into the microarchitecture
it might actually look like three or
four different stages and so you could
expand that just as easily or at the
same time if you wanted if you were
worried about verification time for
things like that you could even compress
fetch decode rename and so on into some
single stage and say this whole thing is
in order I don't need to worry about the
fact that that's three different stages
because they're simple enough that I
don't need to draw separate edges in a
graph for them so you do have some
flexibility to figure out exactly what
level of detail you want to zoom into
and then make your graph accordingly I
would imagine that most of those extra
nodes are relevant the only ones that
matter are the ones that you can read
from all the Creator dependency yeah so
in that case it becomes a balance of do
you want to have more nodes in the graph
and therefore have a little bit longer
verification time potentially for pipe
track itself but then the trade-off is
that the microarchitecture level
specifications you have are much more
fine-grained and then presumably much
easier to verify as local properties
whereas if you go the other extreme
income
press everything together maybe you save
pipe check some time but then you have
more of an individual verification
burden to say that the whole front end
of my pipeline is all in order as a
single unit so we picked one spot in
here that we thought was a good way to
correspond things to the way
architectures might be described but
then you do have some flexibility to
tweak it one way or the other in
particular for the TSO description do
you bound the size of the store buffer
we do not bound the size of the store
buffer in this case so from architecture
point of view they generally tend not to
either if they if they're if it were
something that was fixed in the
architecture then it could be that way
but usually tends to be just something
this is a particular microarchitecture
has a 16 something entries but that's
not something that they want to
guarantee for the rest of the life of
the TSO consistency model or something
and so they don't generally put it in
refinement on these graphs that could be
useful for black so what bitch was
saying about the optimization you could
actually prove that the compression of
some events into one was not observably
dip or didn't introduce any new
behaviors so yeah we've thought about it
conceptually in that way but we haven't
actually gone in and proven things per
se although the idea would look exactly
as like it does for preserve program
order the way we verify that that what
we expect to have or in PPO we have some
edge that we're expecting to replace
from some sequence of microarchitecture
edges in this case it might be the other
way around that we have some sequence of
microarchitecture edges that if you
squeeze them together they result in
just this coarse grained edge but the
similar idea would be there because of
this transitivity property that we're
going for yes the song you talked you
mentioned a couple of actual bugs that
have been found on real-world processes
this is this approach do you think could
be extended to find those particular
birds nests so how far away do you think
you are from be able to do that so yeah
the bugs we did find in practice were
just in that simulator gem 5 we didn't
find any bugs in open spark for example
so that is reassuring but
in the future it would be nice if we
could get to the point where if Intel is
trying to model this transactional
memory implementation that they just
came up with then they could use pipe
check to say that this is working
correctly according to the
microarchitecture model that we have and
therefore have some more reassurance
that it's correct before releasing it
out into the wild and only then
realizing that something went wrong with
it and so yeah we haven't gotten it to
the point of being used with an industry
and that kind of scenario yet but we're
hoping that by spreading the word we can
convince people to start thinking along
those lines some interesting sort of not
real evidence but I happen to work for
the company at the time that had the
first boxer for AMD and without
disclosing any of the details but I'm
the specifics of that but were
sufficiently obscure that I don't think
a manual translation of the paper into
that would have found it that's why i
was going to innate the questionnaire
there that you really have to be lucky
for some of these guys to to really find
this so which is a bit of a don't I
really but I'm yes I think what would be
more ideal maybe as if we had some more
automatic or semi-automated way to get
from RTL up to specifications like this
and that's yeah it's a little bit
farther out at this point we're hoping
that we're at least narrowing the gap
somewhat and then that can be a starting
point for future free shirts to say this
is what we need to target rather than
just targeting the specification as a
whole but yeah it's not there yet that
would be a little bit farther down the
line I think this is the one half of the
of the cornered you need and the other
ones that there's a question from the
back there the problem is not just let
this test themselves in their pure form
right if you oftentimes have to have
these other instructions and when you
run the actual litmus tests they I mean
jabbed and Luke make sure that there is
a lot of irritation around the actual
pristine litmus test right and then you
in many cases actually need that extra
irritation around the core of the test
to bring the the microarchitecture might
say for example at the store buffer has
only specific size just me to it have
their single entry more that rolls off
you find the corner case to hit this so
and then the question is we when you
augment the litmus tests with these
extra instructions how easy is it then
to run the pipe check with enumerate in
this increased lip miss test and this is
also i think the challenge that you'll
probably be facing in a real-world
application yeah so that is true so I
think we would have to happen in that
case is that whatever guarantees that
you would be expecting to make for a
particular stage that would somehow
depend on the fact that you're not
overflowing your store buffer reaching
this corner case or something somehow
they would have to translate into the
fact that you can't actually prove that
the microarchitecture level
specification that you wrote down for
that particular stage is actually
correct in the general sense because
there might be this corner case and so
that again gets back to this previous
question that if we had some better way
to get from say RTL up to the
microarchitecture then that would
hopefully catch cases like that but yeah
at this point it is just a manual
process that where we keep iterating
till we get closer and closer what you
composing would be an interesting idea
tend to be hardened where there's some
like many leave in Marty land there
already or does it make more sense to
look at some artifact that was you know
created in the process of designing I
mean suppose you're on the inside of the
design house are the models that you
write anything similar to to any of the
documents or models that are used in the
production of the chip itself yeah so
again that's something we're hoping that
the more we talk to industry that the
more they may be willing to release at
least some details about how that
process works but in a general sense I
think it would be useful even so like
you said before having art yelled and
just figuring out what kind of
microarchitecture you want to build in
the first place you could first write
down a model like this maybe you have
some internal design I've talked to some
people at TI that have mentioned they do
have something like this and I'm sure
other companies do as well that you have
some model of what you want to happen at
the microarchitecture and then once you
figure out how that works and verify
that that's correct then you can go
about verifying or building each of the
individual stages and then back to
verifying that they do what they say
they should be doing
what was the reaction of you'll posted
Intel sorry what was the reaction of
your hosts at Intel so the team I was
working with was more of a research team
so they were excited by this kind of
research there you were not so they were
not so embarrassed I guess about having
bugs be found because they weren't the
people that were making the bugs that
produced the bugs in the first place so
from everyone that I interacted with so
far it's been this is a positive step I
think to making this helping us avoid
these kinds of bugs in the future it's
not really we haven't talked to the
people that built transactional memory
and had the bugs yet so I don't know
what their reaction would be but
hopefully it would be the same thing
that this is hopefully they would be
appreciating the effort the other those
you do I know they have some tools
already that image to check simulation
pro season next year so yeah so there's
there's been plenty of efforts for
testing based approaches and so we
haven't talked to them directly but we
see this as yeah augmenting this like I
said before that we don't want to
replace testing you still have to do
testing as much as you can to verify
that it works properly but this is
hopefully something that augments that
testing with a little bit more of a
zoomed in picture of what's going on
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>