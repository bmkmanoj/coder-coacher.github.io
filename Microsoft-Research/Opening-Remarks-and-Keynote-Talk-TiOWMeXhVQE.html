<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Opening Remarks and Keynote Talk | Coder Coacher - Coaching Coders</title><meta content="Opening Remarks and Keynote Talk - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Opening Remarks and Keynote Talk</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TiOWMeXhVQE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I am going to get this thing started
I have some announcements to make and
first of all welcome thank you all for
coming i know people probably since we
have a lot of locals folks people will
be streaming in as they fight the
traffic but i hope we stand with good
crowd already this morning that's great
first of all critical announcements
we've got the breakfast as it is is back
there those of you that are unfamiliar
with his room the facilities that are
out the door around the corner so just
on the other side of this wall we have
that will have a demo fest this
afternoon is about five demos I think
we've got scheduled and that's going to
be right out in this Lobby out here
that'll be at four-thirty and then
tonight there's a reception after the
demo fest is done we will go over to the
reception area which is called the
spitfire grill at our commons it's a
short walk from here so if you're park
you don't need to move your car or
anything like that and and I have a map
to show you later how to get there first
of all if there's any questions about
anything going on here you can ask one
of our team dan is here there's Dan Vani
is Manny here there's Manny in the back
wen Ming is here there's wind Ming and
let's see you got Harold I don't know
Harold is in here yet nope uh he's
clocking in okay Kenzie is Kinsey's
there there's Kenzie in the back okay
and kristin is right there and me I'm
Dennis by the way so those this is being
live-streamed and so I wanted to let
anybody know that those of you that are
compulsive tweeters and especially
people that are viewing this live if you
want to tweet comments feel free to do
so we have people here that will be
monitoring the tweet stream and and so
it's it's you know pound sign East
science 14 is that is the hashtag
there's also we have this linkedin group
Microsoft Azure for research and so all
you have to do is if you want to join
that please go on now it's my distinct
pleasure to introduce our keynote
speaker Raghu radhakrishnan is he's an
important pioneer yeah some of you
noticed being is wonderful for getting
pictures and so we've got the pictures
of Raghu through his career from being a
young assistant professor to being a
vice president at Yahoo to being a
thought leader at Microsoft so these are
the stages that we we see it as career
so far but he's a pioneer in the area of
deductive databases data mining
exploring data analysis data privacy
web-scale the data stuff lots of data
stuff he PhD from Texas he's an ACM and
Packard fellow it was a professor at
university wisconsin madison vice
president and research fellow at yahoo
and now he's a technical fellow here at
microsoft and so raju let you take it
away and someone's going to
automatically change this there it is
great Thank You Dennis so as I told
Dennis I'm actually grateful he chose
not to use the photo with a jacket ah
alrighty so today i'll tell you about
big data in general but this won't be so
much to talk about as you it will be
more or talk about the field as i see it
and how its evolving I here Microsoft I
actually now we're two distinct hats one
I had an applied research group called
sizzle cloud information services lab
and I also now run the engineering for
the big data teams right so one leg on
both sides I guess let's get going here
so when thinking about this buzzword big
data what is it really I think the best
way to wrap your head around it is by
thinking of what it lets you do that you
couldn't previously so I'll begin by
giving you
a quick overview of a few applications
and I'm just going to choose some
examples i worked with that yahoo time
permitting you'll see a little bit about
other things but i'll then segue from
that into the tools that enable these
kinds of applications and that will take
me to where I believe the frontier
shifting so if you take the tools that
have taken us thus far MapReduce and
things that build on MapReduce what next
right now try and give you a glimpse for
some of these the last the very last
part of the talk time permitting will be
about a project called reef that we had
been working on here that we just
contributed to open source it's designed
to work with things like the yarn
resource manager in the Hadoop ecosystem
so Microsoft is now pretty heavily
involved in the Hadoop open source world
okay great by the way if you have
questions just put up your hand we'll
talk don't hold your questions till the
very end I'd rather talk with you than
at you so feel free let's look at the
applications so one of the first things
I worked on at Yahoo was this project
called a web of concepts and over time
at Yahoo it formed the basis for a
product effort called web of objects
Ovoo and similar things are there at
pretty much all the web companies here
we call it web of things at google it is
the knowledge graph same thing at the
end of the day the web used to be this
collection of pages and search was
defined as finding those pages that were
closest to your search term by some
measures such as tf-idf modified in
various ways today the game has shifted
today the expectation is that when you
ask for Mumbai you want information
about the city right so users think in
terms of concepts or tasks and the
gamers to understand their intent at
roughly the level that they think about
it and to present web content that is
to be packaged so conceptually you want
to organize information about the web
information found on the web around the
concepts that you think people care
about right so everything about Mumbai
is together not organized by the page
where it was found but pivoted rather on
the concept that's basically the idea
and an index here would be not so much a
token based index but something that's
concept base okay as example to what
this would give you if you go search for
Julia Roberts or pretty much any
celebrity all the search engines will
give you some variation of this page the
most important thing to notice the vast
majority of the real estate is based on
content that is somehow in a database
that's been put there by retrieving
information from all over the web
scraping it massaging it getting feeds
and then spicing all this together the
organic 10 blue links that you are used
to this is the first of the blue links
the rest of it is down there below the
fold okay so whenever possible you will
try to present this kind of result why
the click-through rates are much higher
okay the information here is pretty much
on the web what's different about this
search result is you have synthesized
you have aggregated information about
the concept in question from many
different places presented them once of
course if they're talking about Julia
Roberts their sister-in-law there's a
bit of a problem here right so figuring
out what they mean when they type in the
keyword the true concept this is tricky
so when you're not sure you present the
10 blue links right you only present
something like this when you are really
really sure you know what they're
looking for and part of the way you get
to be sure you use your understanding of
the content to present all of these
refinements or what they just typed in
okay and if they click then it is a
reinforcement that you indeed guest at
what they were looking for okay so
the work on organizing the information
on the web pays off in two distinct ways
helping the user navigate or helping you
understand their intent and then
delivering the payload the actual
aggregated search result all right as
another example if you're looking for
Italy restaurants near ann arbor it
leaves a kind of south indian whatever
puffed rice cake the way this is put
together you crawl the web you classify
pages as restaurant pages was in there
you classify pages as menu pages within
their you extract terms that refer to
you know various kinds of food and you
build an index with each restaurant
recognize that you already know that
this is a restaurant or concept and then
within that you're looking for entities
that are menu items and now you're
building an index or a combination of
restaurant and menu item right so now
when a user clicks and says Italy near
ann arbor they're actually going to get
restaurants that have italy's in their
actual menu the menu contains the step
okay all right especially as you shift
to form factors like mobile where space
is at a premium using the real estate to
deliver highly dense information is
critical because people don't want to
school let me give you another example
content optimization if you take the
front page of yahoo traditionally it's
been curated by editors every link you
see there is manually placed by an
editor I should say was today virtually
every link you see is placed there by
some algorithm at the point that you
click on it on demand this shift there's
something I this another of the projects
I worked on while I was there let's talk
about this set of four links in
particular this called today module on
the Yahoo front page this is the same
the choice of which is the first link
and which are the four links you show is
the game underneath there is a pool of
about a hundred links that you could
show editors curate that pool but the
algorithm selects what to actually show
the difference in doing this is enormous
the click-through lift by doing this is
over three hundred percent okay and
financially that translates into many
many zeros with a significant leading
digit okay so today / arrow in the high
90s ninety percent of all yahoo traffic
goes through pages that are fully
optimized in the sense what does this
really take right on this page itself
most of the links are algorithmically
selected what does it take to do things
like this let me go into a little bit
more detail you take all the articles
the first time you see them you have no
clue who's likely to click on them so
you go by doing content content analysis
you analyze the articles offline you
look at the historical performance of
similar articles you fit machine
learning models and so you have an a
priori estimate of click through for a
brand new article then you use this to
have prior probabilities in estimating
the click-through rate when you show
this to a certain kind of visitor you
refine these estimates using online
statistical explore exploit algorithms
okay these are so-called bandit
algorithms based on slot machines but
the idea is when a new article is when
you have an article you show it to
someone and they are the click or they
don't take if you show it enough times
you have a pretty good estimate of the
probability that the next person is
likely to click however there is a
delicate trade-off the articles DK they
are time sensitive you have a few of us
right in those few hours if you spend
all your time estimating the probability
by the time you have a really good sense
role of it so you want to spend the vast
majority
a time exploiting the most popular
articles while at the same time
parsimoniously exploring to figure out
which articles are growing in popularity
which are shrinking which are indeed the
most popular at any given instant in
time and the mathematics of how to blend
explore and exploit is what the
statistics behind this is all about the
engineering behind this is a different
kettle of fish you have to show articles
to people around the globe on tens of
thousands of service take the results of
something that happened in Shanghai feed
it into your modeling servers in
sunnyvale and the next person in
Singapore right is influenced by what
the person in Shanghai did now the
butterfly in brazil israel okay so if
you take what it requires to do all of
this i'm just going to put up names or
some systems that are underneath this
hadoop we use hurdle for everything from
the actual queries to extract and
analyze the summaries through the data
pipelines getting enormous amounts of
data gathered in one location on the
globe and then move quickly to other
locations on the globe end-to-end from
the person in Shanghai to the person in
Singapore it's about 10 minutes okay
data streams pipelines no sequel stores
all of these things that you think about
as current code Big Data technology
boils down to one thing you have
databases operating doing the things the
databases traditionally do large-scale
queries large large scale crud workloads
right but now at web scale right and
some of the criteria how important is it
to have fully serializable transactions
what kind of availability demands do you
have these mobs have been changed in
significant ways to wear a brand new
generation of architectures has come
into work so let's look at that for a
moment what do I mean by web scale by
the way the first example on the web of
concepts
hopefully the point there was clear
although I didn't put up a slide showing
the underlying infrastructure you simply
cannot build that kind of information
extraction at web scale without using
technology like Hadoop peanuts at
extensively okay what do I mean by web
scale here are some numbers I'm not
going to talk to them in any detail but
basically a hundred billion emails a
month okay geo replicated across a dozen
zones I'm just going to say very very
very conservatively 10,000 plus service
right millions of reads per second of a
given page visits in the order of tens
of billions right add serves ad
impressions same scale this is orders of
magnitude larger than the largest
Enterprise databases have historically
be well it's going to be draft within a
few years but what we are going to see
from yet another class of applications
the Internet of Things and this n
compasses not just things like websites
it's going to seep into everything you
do right biology environmental science
it's all potentially going to be
transformed by the ability to embed
sensors of every stripe and everything
you do your ability to observe is going
to be unparalleled the fundamental
difference between a website and a
traditional application like say word
this observability if someone sneezes on
a webpage I know how long and how loud
right and that's what I can react to
that observability is lacking in a
package application running on your
desktop where no one can view it of
course some of you are thinking office
365 no doubt yes we can watch you sneeze
and that means many of these techniques
are now going to find their way to
traditional enterprise applications
different story Internet of Things means
you know this
story of your refrigerator talking to
your shopping list app on your phone
saying they're out of eggs now that you
are in the grocery store get some this
is not science fiction your thermometer
can sense these are commercially
available today oh there's no one in the
house drop the temperature and mostly
people come back around five o'clock so
let's crank it up again around four
o'clock so it's warm and toasty by the
time they're back in these are all very
doable right it's only a matter of
standardization right so that it can be
done at scale and cost-effectively
enough for all of us to use as opposed
to just the geeks in Silicon Valley
right it's simply a matter of the price
curves coming down and that's a matter
of standardization the numbers here are
mind-boggling in case you do not notice
by 2020 they are predicting there will
be 50 billion devices right the amount
of traffic on the internet is going to
be 275 exabytes per day these are I
popping Lee large numbers and all this
data what does it mean we are able to
observe data automatically capture this
data in ways we never could before prior
to the internet generation data entry
was largely through keystrokes the
volume of useful data you can enter that
way is minuscule right now data capture
has become trivial ubiquitous everything
is observable for better or worse the
cost of hardware has plummeted and our
technical ability to build scale out
analytic systems which is what I'll talk
about for the rest of the talk means we
can actually do useful things with all
this data in domain after domain after
domain and that I think it's a
confluence of things that that's made
this whole field explosive lastly don't
underestimate the power of the cloud
right you can have all the technology in
the world that lets you operate a form
of 10,000 servers but if you need to buy
those 10,000 servers and install them
yourselves you're going to be a long
while doing it the cloud takes away that
last barrier you can rent on demand
right someone also want that form for
you
so with that setting the stage let's
more I'll skip this here's an example
from Microsoft actually what does it
mean to have an operating system for all
the sensors that go into your house how
do we standardize this as I said
standardization is the next frontier
already lots of people are thinking
about this it's going to happen okay so
what kind of an architecture do I
envision all this will require first I
think in contrast to traditional
database systems you cannot have things
siloed your analytic stores your
transactional stores for analytics many
different kinds of backends all this
increases the barrier to use by a user
you going to have a digital shoebox or
some people call it a pool or a lake
whatever the metaphor you use one place
where you can put a diverse range of
data documents beat multimedia bait
traditional relations be it streams made
graphs right and you can put any amount
of data in the same place it will just
expand and hold it and not only hold it
allow you to retrieve efficiently okay
second that efficient part is going to
push the frontier in terms of the
technology a lot of the scale out to
date codes to work in powerful databases
and work on parallel distributed file
systems like gfs and the intuition
essentially is spray your data uniformly
across a bazillion machines and then
charred your computation accordingly
break up your query for example into
little tiny pieces on the shards of data
that that larger query touches so if you
scan a file and the file is on a
thousand machines then you query becomes
a thousand little baby queries each
running on a shard of your data okay
that simple principle carried us a long
way but now in addition to this spread
there is a death if you look at data
that's on local disk all the files
Systems we talk about essentially stop
there there are three copies on the
local disks of three different machines
that's as far as the metadata in HDFS
gets you but now whether something is in
the local disk or main memory or SSD or
nvram makes a huge difference not only
performance wise these different forms
of memory are becoming cheap enough you
can have scads of main memory for
example loaded onto a machine and if you
don't make use of it wisely you will not
meet the performance criteria that you
need on many of these applications
furthermore for inexpensive storage you
don't stop with a single machine of
course you have remote storage you have
tape you have all kinds of storage
that's farther away slower but much
cheaper right this means if you really
want to store everything at all without
worrying about the cost your buffer
management becomes crucial you're not
going to see buffer management on
steroids you know very far very near and
across tens of thousands of machines
this is the challenge of tiered storage
right and oh one last little detail
historically database buffer managers
have been tied crucially to one class of
workloads sequel queries now as I will
describe in a moment you're going to
have a plethora of analytics from graph
processing to stream into machine
learning to sequel queries and you need
above a manager that can take you a long
distance towards all of these workloads
so as a database person you know my cup
flow with over this is lifetime job
security okay of course how beautiful
your store people will always have good
reasons for keeping their data and other
places as well maybe on pram maybe in
some specialized stir that you are
locked into for whatever legacy reasons
it's up to us to make access at least
maybe not every bit as officiate but
access functional access available to
all data no matter where it is
now on top of it from the end users
point of view they care about many
different programming paradigms sequel
is ubiquitous if you take things like
hive peg they are just variants of
sequel in my opinion MapReduce has
gained a lot of mindshare but let's stop
for a moment and think about MapReduce
how many of you know about MapReduce
already okay great so I can be really
brief MapReduce is just sequels grew by
at a very surface level that is indeed
true there's a bit more to it first when
you group the underlying rose in the
group if you preserve them as opposed to
distilling them through an aggregate
that is the map step and subsequently
within each group rather than applying
one of a predefined set of aggregates if
you could write arbitrary user code to
run against a partition that reduced so
MapReduce and sequel group is indeed a
very similar the language construct O's
of course to lambda calculus but if you
take all of this you're still missing
the essential contribution in MapReduce
which I think are twofold first an
enormous range of user code can be
effectively paralyzed in this manner
that's a deep insight right for the
class of things people did at the web
search companies altavista being inc to
me right this was well known the genius
of the MapReduce abstraction was to make
it broadly available the second step
when you paralyzed at the scale of
thousands of servers prediction all
database systems didn't go above tens
right the scheme of restarting when any
one part field is not good enough you
have to plan for failure so failure
centric architectures for the other big
contribution of MapReduce systems but
language wise today MapReduce is folded
into sequel extensions like hive ok go
here in Microsoft something called scope
ok and if you look at the statistics not
if I have ninety-eight percent of all
MapReduce queries at Facebook at Yahoo
they're really hive queries or pic
queries that have been translated into
MapReduce so induces using MapReduce is
a vanishingly small case right the
revolution has been somewhat different
but those two or three percent but you
still have to default to MapReduce do
turn out to be significant which is why
all of these languages support
essentially MapReduce in slightly
different syntax okay coming back stream
processing real-time analytics is now
again becoming mainstream things like
roll-up drill down bi it's a two billion
dollar business for Microsoft alone
right as a market it's much bigger
machine learning is growing explosively
right when you have so much data
everyone cannot understands a human
analysis manual analysis can't cover all
of it what can we teach machines to look
for on their own if you take this
diversity of analytics over all the
variety of data that you expect to store
here the underlying computational
substrate how do you build it are you
going to build a different stack for
sequel for stream processing for machine
learning or can you share some steps in
between right this computer brick is an
area where there's a lot of research
going on today okay and I'll say a
little bit more about and this is when
you look at virtually any big data
player today you will see some variant
of the slide in how they think about the
space it does not unique to us so here's
a link actually is a link actually where
you can go look at what many many people
are doing in this space all right in the
second half of this talk I'll talk a
little bit more about that compute
fabric and essentially here's a question
I will try and address given that this
enormous amounts of very diverse data
being analyzed in a single application
know my mind you write at different
stages in the pipeline I use sequel I
use machine learning I used streaming
systems I use graph analytics
what's the system underneath am I going
to see a single sequel box that all of a
sudden does streaming that's machine
learning that's this that's not
everything or am I going to see a whole
Federation of systems built completely
standalone which I mix and match on my
own dime I think either of these is
unrealistic the first will simply not
allow for the agility that we see here
in the space the and the systems will
simply not be usable right sequel by
itself was being criticized for having
everything but the kitchen sink then you
take all of these other things and slap
them all together it's unmanageable the
second alternative go ahead and build
your stats by yourselves in each domain
I mean in each type of analytics and let
the end-users figure out how to mix and
match end users will not they cannot
write you need to make interoperability
across say sequel and machine learning a
first-class design consideration and
make the edges seamless that means the
underlying computation fabric needs to
satisfy some crazy some criteria the
intermediate computation and a sequel
query must be something you can pipeline
to a MapReduce step for example the
iterations in machine learning you must
be able to seamlessly pass along so
these kinds of considerations where do
they lead us what I think is going to
happen is an evolution of what we are
already seeing in Hadoop with yarn how
many of you have heard of yarn or mrs.
or okay fewer people so let me speak
briefly about this if you take MapReduce
the original implementation of MapReduce
in Hadoop which was the open-source
implementation of the GFS MapReduce
stack from google hadoop was largely
done at yahoo the original
implementation of MapReduce was
monolithic right the programming
paradigm of MapReduce the implementation
all the way
down to the bare metal it was one
composite thing then Along Came pig
which was Yahoo's variant of sequel in
power low there was hive which was
Facebook's version of the same thing
these higher-level languages they had a
choice do we implement them from scratch
oh boy it's going to take a lot of
effort right what we just translate big
queries into MapReduce programs that was
the original implementation approach
they simply translate and then executed
the translated program now there was a
price here if you take a big program and
translate it into MapReduce and stare
the same Pig program and as a human
being rewrite that in MapReduce from
scratch the difference between these
words and the original days easily over
a factor of 10 in performance today it's
still a non-trivial tax you pay not as
big it didn't matter right as I told you
from the very earliest days people
started using these higher level
languages more than MapReduce to the
point where today the usage is in the
high 90 percent of MapReduce programs
being translations which led to the
obvious question maybe we should think
about implementing languages like pig
and hive natively then again do we need
to start from scratch are there things
that are common yes there are so think
about how these programs work I user
submits a job you know that the job is
going to be broken up into baby jobs
running against parts of the data to do
so you're going to have to go to the
underlying cluster and say hey give me
compute containers with some constraints
so let's take a very simple example I
want to scan a very large file and count
the average of some column in that file
right so my data is broken up across a
thousand machines and what I'm going to
go and say is hey give me a thousand
swats on each of these thousand machines
so each slot I can use to compute the
local aggregate and then I
sum them all up so first you go
negotiate you get the resources then you
install the particular executable for
your query in each of these thousand
slots and then off you go what you put
into those thousand slots is specific to
this particular query move more
generally to sequel the sequel engine is
something that does this but the actual
negotiation with some owner of the
underlying resources this is common this
realization led to resource managers
like yarn which was the yahoo version
mrs. the Berkeley version corona the
Facebook version Omega the Google
version there's a pattern here right
everyone's are the same writing on the
wall and the net of it is now the
resource managers form a common
substrate no one has to do resource
management separately they sit on top
get their resources from their own they
do their own thing okay this also
provides some other advantages if you
have a multi-talented cluster priority
can be set by whose needs are most
important regardless of whether they
showing the sequel query or a
machine-learning query you can
interleave very different types of
computation you can have heterogeneous
job pipelines all executing on the same
cluster there is no notion of these
machines do only sequel those machines
to only machine learning right lots of
things followed okay so that's resource
management let's talk about yarn very
very briefly set some of this already
right multiple kinds of engines can
share the same pool and I'll use the
word container or slot interchangeably
right if you take this the underlying
algorithms for scheduling deciding
whether Tony's request Trump's Raghu's
request it should of course this is a
lot of research okay now I need to have
a technical slide here come on I am not
wearing a coat so here's an example of
some work
we actually did on preemption and I'm
just going to give you a brief feel for
this it also helped me illustrate some
of what goes on in MapReduce so in
MapReduce it works in waves first you
take your data you partition it in a way
that's appropriate to your problem using
the map step the mapper is all execute
as independent tasks when they are done
the reducers come along and take each
partition and do something else and what
you are left with is a collection of
partial results which you some together
and you get your final result let's look
at this graphically here are the mappers
each line this there's a mapper that
starts here and ends only here this
mapper begins ends begins ends and so on
these are all mappers each row here is a
particular machine and what's going on
on that machine ok this of course is
time these are stragglers it's well
known that the tale of map jobs in a map
phase have a huge impact on overall
performance let's see why in standard
MapReduce in all of these systems by the
way to begin with preemption was not
part of the story so once you allocate a
SWAT to a task you simply wait till the
bitter end until the task completes and
returns the SWAT team so in that regime
let's see what happens you allocate all
these slots to the map steps of a job
they're all going to go on some of these
finish before others so you can reuse
some of those slots right so some of
these rows are overlaid on some of these
rows ok effect on some of these machines
the reduced step starts but now
somewhere here let's say those reduces
get blocked because they are waiting for
the straggler to compute remember in a
groom by if you are consuming the result
you need to be sure your group is
complete when you get there you hit a
hard wall in sorting the exact same
story you guys know what I'm talking
about so here these machines lie fallow
why the choice is to nuke all these jobs
and start over
which they do sometimes it has its own
consequences or you just wait to be
unblocked when finally the straggler
completes you can resume and then the
remaining producers are scheduled as
well and the job eventually completes
now this red area starkly illustrates
the impact of a straggler in the absence
of preemption okay so you cannot begin
to see the subtleties under the covers
okay now let us look at this option
instead if you could preempt if you
could say at this point you know save
the state efficiently and use these
machines to do something else because
you have blocked on these particular
tasks what could you do right if you
were to do this you could continue you
could use you could get some of the
other reducers going on the same
physical machines here right those are
you know I wish I ever Nordic in that
case I wouldn't have to use this but
here if you look at these producers they
are running on these machines which have
now been paused on these reduce tasks
you get what I'm saying and therefore
instead of sitting on your fanny you
could actually be doing some useful
stuff which means the whole thing
complete sooner ok this is under the
assumption that you can efficiently
checkpoint the work done by these guys
so you need a checkpoint mechanism but
if you had that you get a significant
improvement in performance right this
are kind of stuff that going on as we
speak these systems for all the
impressive you know characteristics
there is a lot of traditional stuff that
they need to incorporate let alone break
some new ground in many ways so this
particular scenario we showed it had a
one-third improvement all I want to say
in the next few slides is you know what
all of this work they actually
contributed to
nope open source Apache open source here
are the GRS for those of you who are
interested this is just to kind of make
a point Microsoft is serious about
giving back to open source so we are
very much involved in Hadoop both as a
consumer we offer a product called HD
insight which is similar to EMR right so
we consume from open source and as you
saw in this example we also give back in
fact several the people on the Hadoop
Council committers they work at
Microsoft all right switching gears now
I have taken you all the way through
yarn what next well let's follow the
thread I know want to use yarn or a
resource manager like yarn to build
sequel the bell machine learning have I
gotten everything in common that I could
from here on do I have to build custom
versions of each of each stack hopefully
not so the reef project was an attempt
to push the common substrate one level
higher there are a lot of things for
example if I have a thousand tasks which
of them died just monitoring them can be
a chore restarting automatically can be
a chore checkpointing can be a true how
many of these things can you package
into a collection of libraries that are
common what are the further deeper
benefits of doing this that's what the
reef story is about and I will try and
walk you through that okay again this is
a microsoft project done answer so now
going on in the big data team as well
and again we have open source rate
through apache so let's take a concrete
example of machine learning user
activity model you want to find out what
a particular user likes okay and you
infer this by looking at the pages they
browsed the queries they asked and the
ads they ignored orkut mostly people
ignore ads if you look at the number of
pages per user
a few tens the number of possible pages
there are in a typical website millions
write the number of queries that users
collectively asked hundreds of millions
so they actually asked any given user
relatively few the point being these are
highly sparse but these are the
observations you have to work with how
do you learn from this imagine this time
window in which the user's activity is
overlaid and you gather this from
various kinds of logs web logs or search
logs and in any given window you look at
the things that you can observe oh the
user just issued this kind of query you
visited this kind of website based on
that what are they going to do next that
is the crucial question this is what
user activity modeling is going to
provide for you features that given this
will allow you to predict this okay so
someone visits finance and issues a
query about the stock market chances are
good they will sign up for a trade ok i
am oversimplifying but you get the idea
how do you build such models you take
your logs and slide this window step by
step and each slide of the window right
gives you a case to learn from so the
very first step is to gather all these
logs clean them up deal with things like
robot clicks and winnow them out of the
way and then slide slide slide each time
extract a row in your training database
this is a ginormous task ok and it is
preparatory to any kind of modeling ok
and if you look at the times involved
acquiring the data takes several of us
these are somewhat old yahoo numbers but
you will see very similar numbers from
being go from google feature and target
generation right roughly in each window
feature window has about a terabyte this
work extracting the cases takes about
four to six hours the actual model
training what we write papers about
this takes one to two of us and not just
for a single model while you are at it
you might as well build hundreds of
models okay because you're going to
evaluate them all through bucket tests
just think about this for a moment when
you think about the nexus between big
data and analytics or machine learning
this slide is something that should be
burned into your soul okay machine
learning academically is all about the
algorithms machine learning for real is
all about the plumbing right and this is
why having your scalable data management
systems be seamlessly integrated with
your machine learning frameworks is
crucial okay let's look at this in a bit
more detail one of the things i looked
at it yeah who was fishing mail spam and
the like so here is an example about the
spam filter you have your inbox and the
algorithms the spam filters try to take
a given users mail and folk them into
real mail at spam okay the user actually
sees this through the mail front end
okay occasionally you screw up you take
spam and put it in the inbox even more
occasionally someone will complain they
will mark that as spam and that is your
signal to learn from okay
now let's fast forward what does it take
to learn from that you need example
formation modeling evaluation the same
cycle you saw earlier right and if you
look at this I will start moving a
little bit faster here the example
formation can really be done reasonably
efficiently using systems like Hadoop it
is really a gigantic join between the
mailbox and the spam signals you can do
this Hadoop helps you do this world okay
if you take the modeling step different
story modeling is not so easy modeling
is massively iterative and MapReduce
does not really support iteration in
between iterations you need to write to
disk so nowadays you have seen things
like spark and shark that try to keep
things in memory to avoid writing to
disk in between iterations okay the good
news is MapReduce supports one of the
basic models for machine learning the
statistical query model of michael
currents pretty well other models there
are maps oh well to MapReduce okay neta
fit this iterative cycle even a single
modulus iterative but there is an outer
loop a prior model observe errors
updated try again do different kinds of
features try again this whole iterative
cycle is not well supported in MapReduce
so going a little bit faster net of it
is what I just told you it can be used
but not easily not efficiently what is
this led to people cheat people are
infinitely creative they use map only
jobs no reduced step what the heck is
going on here they essentially getting
threads from a ginormous thread pool and
having a party okay you can do anything
you want just give me the resources and
get out of my way but if you do this
I hate animations okay you know these
are examples for those of you familiar
with machine learning there are things
like all reduce and darshan trees which
old superimpose a complex communication
structure across many tasks you code
this on your own each of these is a map
only task you establish your own
communication channels if one of those
boxes it dies you're in doodle you deal
with it all of these things that
MapReduce supposedly game you as part of
the framework you do all over again
because you are abusing the system to
make it jump through hoops it wasn't
designed to okay so all said let me cut
to the chase here yeah I can go on to
more and more of these examples all this
leads to unhappiness on both sides the
people who do this kind of abuse not
because they want to be abusers because
they don't have options they have a
fault tolerance mismatch so they have to
redo fault tolerance or live without it
the resource model you know they're just
building their own there is no notion of
a tree for example the their integration
with MapReduce essentially used to treat
them as map only jobs they have to deal
with networking cluster memberships bulk
data transfers pretty much all of the
work for the cluster life is not so
great for example the network usage
patterns in these kinds of applications
are very different this leads to
problems another consider what happens
when you need say gang scheduling if you
need a collection of nodes to be given
to you in order to proceed this happens
for example in giraffe the graph
processing system what does giraffe do
it gets resources one by one from the
underlying resource manager until it has
enough meantime it's what's on the
resources it has this means the
utilization of the system thanks
everyone else is affected because you
have given someone resources piecemeal
when what their requirement is an
all-or-nothing give me at least a
hundred right
so how do you deal with all of this what
you really need is this intermediate
stage that complements yon right to
facilitate development of applications
on top of yarn and lets you build
pipelines of these different kinds of
computations so let me illustrate this
with a concrete example and then I will
conclude in this example the job driver
it's really the control in the example
program activity it's the user code this
is what you would execute in a MapReduce
task you actual code running their
evaluator is simply the container it is
a reef time just saying it is a reef
controlled evaluator okay with this let
us consider the simplest of tasks let us
say you want to do distributed shell
from the same command on two different
machines here's what would happen in a
scenario with reef you want to run this
command you would first come to a
cluster that knows about Hadoop yarn
brief all of these things and when you
come in the client is your client
executing trying to do distributed shell
you submit your job the first thing
that's going to happen is you will
launch a reef container on one of the
machines the very first thing that reef
container will do is to launch the
special program called the job driver
which includes your code to orchestrate
the logic of your program the job driver
now negotiates on behalf of the job
right with the resource manager the
negotiation results in tokens it can use
to create tasks it will get containers
through the resource manager and on
these it will install some libraries it
will install the actual user code right
your task and it's able to run this task
on this machine imagine the same thing
happening on all the other machines in
parallel
okay now your command is running on
these two machines and instead of
running dire you could be running
anything at all you wanted across these
two machines right Oh 20,000 machines
for that matter these are going to
communicate with the job driver through
a regular heart beating mechanism which
primarily allows the job driver to take
over the mundane task of babysitting
these bazillion tasks restarting them
should one of them fail and the restart
may happen on a different machine
because of it we if a task fails the job
driver will detect this go back to the
resource manager get another container
restructure activity if you want the
state that was lost to be available you
will explicitly use one of the
checkpoint commands to save the state
durably across machines and that will
then be available when the job is
restarted so not in a nutshell these are
all capabilities that make it possible
to write say a sequel engine or a new
implementation of giraffe or a new
implementation of a machine learning
algorithm at scale one last thing and
this is a really really important thing
let me get past the deed okay so let us
say this particular task completes you
still have the evaluator so reef
essentially acts as the middleman
between you and the cluster manager and
when you complete one of the tasks you
have been so installed in a container
you have the opportunity to install a
follow-up task so think iterations in
machine learning after one iteration
completes your containers available with
metadata about what state you have left
over from that iteration you can install
the code for the next iteration the data
never goes to disk and comes back you
iterating in main memory this by itself
either doing this or running in
something like spark gives you a 30x
improvement and performance okay now
when you pull all this together what you
effectively have is the rate system in
the interest of time speaking of
checkpoints I am going to check point
right here okay for those of you
familiar with RX there is a lot of
similarity with RX in the underlying
api's net net if you are interested all
of this is available through github send
me or Dennis mail and we'll send you
link so let me pause here today the main
message I want you to take away over the
next five years everything is going up
revolve around data the kind of data is
drivers right it could be data that's
very specific to what you are doing but
what is universal is our world is
becoming data-driven and this is going
to require us to develop systems to
manage data of a diversity and a scale
that's unprecedented and to provide
analytic tools here's one other point if
everything is data-driven a corollary is
domain scientists who care about the
domain science not learning
ever-increasing complicated versions of
sequel and this and that are going to be
using these systems that means you need
to build domain-specific languages
you're going to see a plethora of
analytics is my guess so having the
kitchen sink is not an option having
many tailored systems if that's where
you're going these tailored systems
inevitably will have to do some part of
they work in concert with other systems
right everyone for example will have to
live with sequel right how do you
support these kinds of diverse analytic
engines on top of a common compute
fabric a common caching capability a
common distribution checkpointing fault
tolerance right
all the major players are thinking about
this in the cloud Google Amazon
Microsoft they are all in the game it's
a fun time to be working on this kind of
stuff so I'll pasta</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>