<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Exploring Richer Sequence Models in Speech and Language Processing | Coder Coacher - Coaching Coders</title><meta content="Exploring Richer Sequence Models in Speech and Language Processing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Exploring Richer Sequence Models in Speech and Language Processing</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5vZSGLEf5Cs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so we're pleased to have electri
and Eric got his PhD degree from UC
Berkeley and he's a associate professor
at Ohio University ohio state university
right now he has been working on
conditional random field for many years
and today he will talk about his recent
work in this area six thank you so I
when I when I was trying figure out what
to talk about in this talk I thought
well you know my students are doing all
this wacky stuff and how do I sort of
summarize all this stuff so let me give
you a quick overview of the lab
basically my my viewpoint is is that we
want to be looking at linguistics to
help us do speech recognition language
processing we want to be looking at
language processing and speech
recognition to help us do linguistics
I'm going to talk more about the former
point than the latter point today but
that is sort of the overall thing that
we're doing and traditionally we've been
working a lot with you know in machine
learning on the CSE side and you know a
lot of people in linguistics to sort of
get some good insights recently i've
also been dabbling in stuff with
biomedical informatics and and a partner
over there the Wexner Medical Center so
i'll talk a little bit about that today
to just to give you a broad overview of
I was kind of surprised when I actually
sat down and said how many projects are
we actually doing and there's quite a
few both on the speech side in the
language side so we up until recently
had Chris brew who left for ets so I
inherited a number of language project
so it's but a lot of the stuff that
we've been doing traditionally that
people kind of know me for is that
working on speech recognition with crfs
and feature based stuff there's been
some stuff with Karen Levesque you doing
discriminative FST s and also some of
the feature based stuff we just recently
got involved in a government project on
multi
we'll keyword spotting and you can read
all the rest of the stuff we do a little
bit of linguistics so computational
models of child language acquisition so
that's some work with Mary Beck man and
some stuff on the medical side of
looking at timeline extractions from
electronic medical records and virtual
patients and there's also a little bit
of stuff on kids literature on how to
predict the reading level so there's a
number of partners in all of these
things I won't have time to go into
details so today I said okay we'll try
and give you a slice across two so I'm
going to try and balance my speech time
in my language time although I will tend
to talk more about the speech I think
because I'm very blue and about it but
okay so here's the overview so today I'm
going to give you many of the people you
know there's a lot of old familiar faces
in the field in the room so I'm going to
give you a for those who don't know what
I do give you sort of a how did we get
here background on conditional random
fields and stuff like that and then I'm
going to talk a little bit about some of
the stuff that we've been doing with
segmental conditional random fields
articulatory feature modeling and then
I'll talk a little bit about the
language stuff that we've been doing
where we've been taking CRFs and which
kind of grew up in the natural language
community and using them as event
extractors to then do semi-supervised
learning okay so background or how we
got into this mess okay so a lot of the
things that I've been interested in
thinking about how do we break apart one
of these problems into some sort of
feature description and use combiners to
do feature based modeling and that's
sort of the overall theme of the talk
and the idea is that in general we're
going to be looking at ways to extract
features from either speech or language
and then combined using some sort of
weighted model usually a log-linear
weighted model and just to give you a
flavor of how I got into this whole mess
the little bit of the old history and I
did my thesis way back when on
pronunciation modeling so I stole this
slide from
rohit who's slow that's still the slide
from karen on pronunciation variation
within switchboard okay and that that
comes out of work from Steve Greenberg
and where they did transcriptions of
find transcriptions of how people
pronounce words in conversational speech
and you'll see that a lot of there are a
lot of different ways that people say
like for example probably and in fact
it's very unlikely in fact it was not
observed that you actually see the full
pronunciation of probably but usually
you say probably or probably you're
probably you know so there's a lot of
variation and this is a real problem
from the viewpoint of trying to build a
good model of this is because we don't
have complete linguistic evidence in the
signal according to linguists right so
these are linguists are transcribing
these utterances and saying here's here
are the things that I've observed okay
so I did a thesis which nobody should
ever look at on doing pronunciation
variation and people you know back in
the 90s we're getting you know mana
success by basically saying well I'm
going to take my pronunciation
dictionary i'm going to add these
pronunciations well i don't want to add
too many of them because things are
confusing and stuff like that but we
never really got too far from with that
approach because the more pronunciations
you add to the system the more likely it
is that you're going to start confusing
things okay and so let's take a little
bit of a little closer look at some of
the data and the the thing that I'm
going to show you is actually inspired
by some work that was done by Murat
sorry Clara and sanjeev cooper at
hopkins which basically said you know
what are the acoustic the real acoustics
that go along with the variations that
we're observing so i have this following
little example that if we just look at
the buckeye corpus of speech now this is
again conversational speech but we get
long-term speakers switchboard has like
five minutes of each conversation side
these are like hour-long transcription
so we get a lot of instances of
variation within the speaker
and we can take a look at things like
let's just take a look at ad versus at
and the buckhead corpus ok and I'm going
to look at just looking at the the
formant structure of just saying I'm
looking at where that where are their
peaks in energy ok for a and then when
the dictionary said there should have
been an a but it would be the
transcriber said that it was a net ok so
that would be like if I had bad and
somebody transcribed it is bed right
which would be a confusable thing but
people do say this ok so here are the
data that it just correspond to as
versus that they are transcribed is as
and these are as transcribes as s so
this is the first format frequency
versus the second formant frequency and
if you've ever done any linguistics then
you'll know that this sort of is one
section of the valve al triangle okay so
um distinct dozen more example duration
thank you terms of not in that not in
this corporate the duration is actually
roughly equivalent for this speaker yeah
yeah so so not for this but I I agree
that that we're only look I'm only
looking at one dimension of a very
multi-dimensional thing right okay so
just to give you I'm and the circles
really mean nothing as statistically but
just to give you a sense of like here's
an here's a region where we see as and
here's a region when we see s okay so
now here's your test what happens when
as going to be transcribed as s so the
observer saw this particular thing and
said oh okay this ah this should have
been a nap but it really wasn't a so the
transcriber decided to change the
canonical pronunciation to F right where
do you think that data should be should
be you would hope in that overlapping
region right of course it's not because
otherwise why would it be showing this
slide right so so the actual data which
is marked with the blue stars
is all over the place okay and so some
of it is in that region that you would
expect some of those are in the ad
territory some of them are actually
higher than a ok so the moreover this is
almost into it okay in terms of where
the formats are and some of it actually
sits in fact lower than ad and there's
actually no phone that i was actually
lower than at right you know and again
this is a very hand wavy argument but
you can but this kind of data was door
born out more statistically with with
the study that marantan and son chief
did okay but here's the key where is the
data not the data is not over here okay
these are the back valves in here okay
so what does the transcriber really
trying to tell us it's not that it
really was pronounced as an ad it was
that it was a front vowel it should have
been a nap I don't know what it really
should be I don't know what the back I I
don't know what the eval height should
be so I have some uncertainty as the
height I mean that's sort of my loose
interpretation of this that we might
have uncertainty in some dimensions in
terms of the variation but certainty in
other dimensions so if we start thinking
about the linguistics in terms of the of
a multi dimensional variation of some
sort of phonetic space then we can start
modeling some of these phenomena okay so
right so we can't take those
transcriptions at face value okay so
where where I've been looking at
different kinds of representations is to
think about sub phonetic representations
such as phonological features or
articulatory features to represent these
kinds of transcriptional differences and
they the interesting question then
becomes how do you involve that into
some sort of statistical model so
backing up a little bit to the
statistical model side I you know
grew up in a Dixie where neural nets
were a big thing so you know one of our
favorite things to do is to basically
take some acoustics here put a
multi-layer perceptron to try and
predict you know which phone class is
this given this little local chunk of
speech okay so i might have a local
window speech and i'm going to get a
posterior estimate that for every frame
effectively out and if you are a XE
person you might put this in to do a
diagonal ization and d correlation and
put it into an hmm but one of the things
that we did is basically say okay we
we're going to build a log linear model
on top of that which is a conditional
random field ok and I'll talk a little
bit more about the conditional random
fields in a second but essentially what
you're going to get out is instead of
having these frame level plus series you
end up getting these sequence level
posteriors out ok so why do I like this
framework so much well it's because we
can start playing around with saying hey
look i have little local detectors of is
this a manner is this a place is as a
height i can talk about these as
features that I'm extracting from the
data and I can talk about combining
features of different kinds of things so
so this can be used in in place of or in
parallel to other types of things so for
example zilla did a lot of work in to
begin with on trying to extract things
like you know using the sufficient
statistics as the functions right and
using a higher hierarchical or hidden
conditional random field right to try
and do this kind of estimation yes so
we've done we've done that you have to
put xn and it's square straight so you
gave the sufficient statistics it helps
us a little bit but it's it complicates
the thing the the that it just it
doesn't help enough to add that in all
the time but it's a good thing
but please do oh how you sort of train
this bus by you need a training signal
and you just showed us that the training
signal was unreliable right so ah you're
asking no you're you're asking something
is not on the slides but it's a very
good question um so what we're doing so
there's different let me there's two
answers that question and in fact they
do come into the slides later on okay so
yes you're right you're anticipating so
at the first guess what we're going to
do is basically take take a phone level
alignment in fact usually from an hmm
because we need we need that that's one
of the downsides to this is that you
actually need to have if you're not
doing a hidden conditioner infield you
need to have that that label sequence
right and so what we do is we take the
phone we take a phone sequence and then
we can just back propagate it to what
the appropriate features were trained
independent nets and what we're
capturing essentially is in some ways
the errors that each one of these makes
in its estimation and we might get a
little bit of feature asynchrony or
stuff like that but but if you want to
go to model more like the the kind of
articulatory feature model you're going
to need some other mechanism to try and
handle that misalignment and I'll talk a
little bit about Rohit's model which you
might have seen before about trying to
do articulatory featuring limits so hang
on to that thought okay yeah oh my
descendants yeah what we've hidden from
you knows yeah you don't need to have
precise alignment for the same level to
transient train this to train a a
straight-up CRF you need to have the
frame of alignment to train the hidden
one you don't right because because
because the alignment is actually hidden
is is actually one of the one of the
things that you need to do but you can
do you you can do an embedded Viterbi
type style training which doesn't I mean
with one realignment basically you get
back to where you need to be so it's
it's not a big deal to
when you put this year yeah you know in
the forum there's model the whole
sequence in the ferry phone that by
default you kept hidden there all right
on you don't really explicitly rather
they know so so the way the word so the
way that we're training it actually you
do have well we've used from both one
state and three state type models but
but yeah you do you do actually have an
explicit the way that we train it we
have this explicit labeling then we go
and we return be real able and that that
gets us you know to get away from the
mismatch okay um and this actually gets
that actually gets to an interesting
point about the the criteria so let me
let me hold off on that idea for a
minute okay so one of the very few
equations that I have on this in this
talk is basically what we're interested
in a label posterior given the acoustics
and what we're going to do is talk about
this in terms of a bunch of what we'll
call state functions which associate
with each label essentially some feature
function here some generic feature
function I'm going to label those as my
state functions and also the transition
functions which talk about pairs of
states in this case because this is what
we call linear chain CRF and it may also
have associations with the future
function this actually turns out for us
to be an important thing it's an open
question is how important is it to have
observational dependence on this
transition we find that that having the
ability to talk about whether you're
going from one state to another based on
the transitions which you can't do in an
hmm straight up okay this actually is
important for us that we get a quite a
bit a little bit of gain out of doing
that okay okay so just to sum up this
little this little sort of like this is
like 2008 and before kind of thing you
know basically you could play around
with different versions of the functions
so for example if your yes or hip knee
you might say here my
20 you know I've got 10,000 gaussians
which are my 20 closest ones right and
that would be a very sparse feature
space right so you there's there's a
nice thing about this is you can talk
about different kinds of features plug
them in together and and and just train
okay just to summarize the results that
we got on previous studies with time at
phone recognition basically we found
that we were beating the 10m hmm systems
using the posterior xinput with many
fewer parameters okay even just a mana
phone base CRF was actually beating our
try phone-based hmm now admittedly this
is not a discriminative lee trained hmm
right so caveat there right oh and for
timet phone recognition training
training things discriminately is it
it's a mess it's just not an updated
really to get it done you until the IBM
right you know Brian's going to show me
wrong right exactly so the transition
features actually helped us quite a bit
that was one of the other things we saw
and we played around with things where
we combined a large number of
phonological features and phone
posteriors and then we found that that
was a good effective combination
technique okay so any questions on sort
of the background before I sort of segue
into like what's near ok all right ok so
I'm going to talk a little bit about the
upcoming an inner speech paper that we
have on boundary fractured crfs so I'm
glad Jeff's in the audience because I'm
going to sort of sing to his choir you
know this whole this whole talk in some
ways it's like bringing coals Newcastle
because you guys do a lot of this kind
of stuff within the speech group so I'm
you know we're adding little bits of
knowledge I think to the kind of things
that you guys are looking at so the
frame level approach wild nice and easy
to train has a real problem is that the
what we're trying to do is maximize the
conditional maximum likelihood of the
frame labels as opposed to upstream what
we really want is words in the end not
even just phones we
onwards right so there's this big
criteria mismatch and and it gets down
to exactly about this issue about
segmentations and the fact that you know
if I get and and the the way to think
about it is if I if I had cat k and I
and I got a whole bunch of k's um let's
see cat is not good because azor should
be long so so let's imagine I a
recognized cat is like k should have
been one frame long and then there's a
bunch of ads and a bunch of toes right
if I just label that one frame wrong
that's big it's a big problem in terms
of the recognition right but the
criteria basically says oh well you just
got one wrong out of out of this long
sequence right so this is so the frame
level criteria is kind of a really bad
mismatch okay as a side note to that so
one of the things that you want is sort
of you know fun atactic grammars and
stuff like that say this sound is likely
to correspond in the next time on a
frame level those those probabilities or
actually potentials in the CRF get
really spread far apart because you've
got all of the sequence of frames right
so so this is kind of bad from that
point of view and we want to be able to
incorporate long span features like the
duration exactly to your point we you
know duration is very distinctive
especially in a lot of languages that
are non English format trajectories
syllable phoneme counts all these kinds
of things so we want something else to
to work on so segmental CRFs to the
rescue dadada so the idea is I'm going
to change my my labels from q2 why where
I'm talking why is now a segment and as
opposed Q is my frame okay and we're
going to talk about labels being on the
segment label but every label is going
to correspond to some sort of chunk of
frames and probably jeff has talked your
ear off about it but the idea is that
were what you're going to do is you've
got the simplicity sort of segmentation
that's going on that basically says hey
you know this y3 actually corresponds to
two inputs and I now need that
segmentation is somewhat hidden from me
okay so but we're going to the nice
thing is now you know phonotactic
grammars now actually fit right so we're
now talking about really this this phone
is changed from that phone okay so when
you plug all this thing in I'm going to
take all my state and function in and
transition functions and put them into
sort of one on form here what I'm going
to do is look at the segmentations
possible segmentations and some out over
all the possible segmentations and then
now I can get a posterior on the
sequence rather than a posterior on just
the frames okay so so that's so that's
that's essentially the model that's
built into a scarf okay and one of the
questions so one of the mantras in my
lab is you know what is the stupidest
thing you can think of to do first and
one of the questions we said well what
if you relax that assumption to
basically say hey I'm going to actually
just try and jointly pretty able to
predict the labels and the segmentations
at the same time which saves me and
training time because I don't have to do
all of this hypothesizing and summing
over all is they all the possible
segmentations but on the other hand it
is exactly the same type of Viterbi
assumption that i'm making in terms of
the segmentation and my segmentation was
wrong in the first place then I'm going
to have to correct it with a retraining
right okay so so this is the model that
we're working with but in I think a lot
of the lessons that we learn are really
sort of transferable between the two I
don't think there's a big deal about
this this is mostly because it makes it
computationally tractable within the
university setting i think is yeah for
us okay um so yes when you consider when
you're doing decoding say mm-hmm don't
you still
to consider all the possible
segmentations you you do have to
consider all the possible segmentations
during decode time right so this is
mostly to save on the on the train time
that's right we also do put a little bit
on our segment length so that was
another that's another thing that we
have done to sort of optimize the Dakota
thing so and one of the things that
we're trying to do and I was very
annoyed to see Jeff coming out with a
paper round this before right before we
did but is to do one pass decoding
directly from the acoustics so that's
why we were kind of interested in
getting into this model so yes in
addition you'll be square of this
invitation will be squirreled away it's
no no and so you have it so there's some
computational tricks yeah okay it's so
you limit so you limit I'll show you in
a second uh training doesn't fix don't
you still have to do the work for Z um
do you have to do this you do have to do
the work for Z but oh I okay right so
this is the problem with presenting your
students work great it's like the reason
why you went into this is not the reason
that ended up happening the so so you do
need to do it for this you're right and
the so we did these computational tricks
on the segment lengths to basically make
it more manageable so basically we
experimented with going off slide a
little bit we experimented with the idea
actually let me come back to that but
but because I'll have a slide that I can
point to in a second so I actually just
go up here so on the efficient segment
until CRS slide that the upshot is is
that we found that if you met we played
around with actually in a classification
task what would be the optimal size of
segments like if I if I had a long
segment longer than whatever my optimal
my maximum size was d I'm going to call
that D right then I would go and a
segment a phone in 222 boundaries right
is it so i might have the first half of
a phone in the second half hour prone or
maybe you know multiple chunks of d
length and the question we had was how
bad does that hurt and we found out that
after doing some experimentation on for
the timet task at least that having a
optimal size about 10 was a good
trade-off between decoding speed and
accuracy we didn't really lose so much
accuracy by saying my maximum length of
a duration was 10 and if I had something
as longer than 10 that i'm just going to
postulate two segments okay in the end
all right so I the want to know that one
of the ways that I've been thinking
about this problem is that what you're
really talking about is a different kind
of CRF when you're talking about a
segment till CRF in some ways what
you're really talking about is a model
where the time now become sort of a
first
variable right so the segmentation
variables are also going to be our the
segmentation times of my segments right
are actually also variables that I want
to hypothesize okay so when I have a
graph like this a CRF basically defines
its probability structure over the
clicks in the graph okay so I have some
natural clicks here because I can talk
about this phone is between this time
point and this time point okay I can
also talk about so I can talk about
extracting evidence from that from the
acoustics I can talk about a prior to
say this phone is likely to be this long
right on this on the thing but I also
have these inverted triangles which
basically say hey here is a phone and
it's an its successor and i believe the
boundaries at this point right and i can
talk about acoustic observations that
correspond to the transition between
phones so there's a lot of in there were
a lot of models that that at some points
tried to like that there was a model
called the spam model out of ixy that
tried to focus on transitional areas in
speech rather than than segmental areas
and this is a this is a natural way to
think about incorporating that type of
evidence of a different nature okay you
to model incision
you mentioned earlier yeah well the form
of transition that that's an interesting
question so so the steady state within
so the natural like if you let's imagine
if why we're a try phone right then you
would expect that a formant transition
type structure might be important here
but if you are looking at like local
area of forming transitions from but ah
for example right this might be an area
where you could actually incorporate
features like that right so it gives you
that flexibility to start thinking about
the linguistics of the situation right
and plug in I got this great transition
detector that will do you know X right
and I can plug it in as oh this is
something that focuses on boundaries
versus steady state okay now the problem
and and I said before we were very
interested in all these transitional
like the that having dependents acoustic
dependence on the transitions between
phones if you try and do this directly
within the segmental CRF you incur
essentially an N squared D and times the
length of the that's the this is the
code complexity and the question is
because you need to keep track of
essentially the durations of either one
of these and also the pair of these
things right so this all of the pot you
have to look at all possible segments
and I so so what Ryan discovered is hey
let's do the following we're going to
we're going to model a boundary itself
as a intermediate node this is
deterministic right this essentially is
going to carry something about the about
its segmentation but this doesn't carry
any information about the segmentation
so you give up the ability to talk about
the value the duration of both segments
in joint okay so you do give up
something with this model but if you
don't have features to talk about that
then you can basically say look I'm
interested in knowing
the time point that this ends right it's
transitioning to this this point and
maybe I might look over a local window
of these features ok so we're changing
the problem so that you're not able
you're no longer able to look over the
entire span of the segment but you're
allowed to look at a local window around
a boundary and that's really what we
want to focus on ok so Ryan called this
a boundary factored segmental condition
random field ok look in the book the
value so this is a deterministic value
this basically carries like this might
be an ad that goes for five frames well
and this one is an ad that goes for
three frames right this just says you're
going into a nap right so that makes
this well it's not deterministic because
essentially it carries a prior on you
know the duration of segments right that
but but it just but and the time point
is actually carried by the the previous
time right so this this this one way to
think about is this has a it carries a
label and a time point as opposed to a
label in adoration ok right so um so it
turns out that with that clever trick
you actually can get quite a bit of
speed up in doing this ok so we did some
yeah well the second the one why do you
see you a call it a segment goes here
and it's not segmental it is it is
segmental because we still have we still
have these segments right the labels are
still on the segmental level right but
you said
many of the features about the segment
in the transition in the transition
about it's the segment's still now
respect this transition if you turn your
own a broken leg like a long cold that's
right so so but and these are
transitions between segments right so
this segment actually gets to observe
everything about its segment information
sure sure this gets to this gets to
observe all of the data that is within
its box that is a segmental thing is not
a single frame the little boundaries
before you can track features all this
that's right so you have to hypothesize
the segmentation right I don't know what
you cynthia computation up over you
because because because we don't have
that we no longer have to model the
joint pair of all at all of phone one
with duration d1 ties all of phone to
with duration D to we compile it down
into a single time point so that so
you're only allowed to talk about the
transition feature is allowed to only
look at local boundaries but the seg the
segmental feature the the state feature
is allowed to look at all the segmental
information okay with in its segment
okay okay so and i want to point out
that that most of the models that are
out there don't even use any information
on the observational side for the for
the segments right there just mostly
priors on this right so it's it's
particularly this question of how do you
get the the observational dependence
into the transitions in the right way
right computational capacity in terms of
decoding fun yes about it yeah yeah but
because it you still otherwise you don't
you don't get a feature on if i would be
able to feature expressions based on the
second
so the second one oh so we do it so so
here's how we okay you're anticipating
me good good good good okay so let's
let's do a head-to-head comparison
between a frame level CRF in a segmental
CRF okay thatthat's that was the key
thing because we realize that nobody's
really kind of done that and so let's
let's see what happens we're going to
use the same exact feature input so I'm
going to use my posteriors coming off of
all of my either my phones or my phones
and phonological features okay and and
I'll train up my usuals Jeremy more
style CRF frame level CRF ok now the
question is how do we go to extracting
segmental level features so remember you
have to hypothesize a segmentation and
then you do the future extraction so
what we're doing is basically saying
okay the state features we're going to
sample uniformly for my posterior space
so if i have something so i might take
five snapshots and if it's a ten long
i'll make them evenly spaced that
they're three long i might oversample
okay but in what i end up with is a
fixed feature vector okay that basically
describes you know different points
within that within that segment okay we
played around with different things like
you know what's the maximum of this
value which the you know the you know
maximum within the window you know all
the all these kinds of things we played
with a number of things this seems to
work well you know but you could imagine
plugging in something else there's
nothing that says that you can't you
know what you need is an online feature
extractor that basically says I'm
hypothesizing this segment at this time
give me the features that correspond to
this right so ours basically our version
of that basically just says I'm going to
uniformly sample within my posterior
space of in time I'm i should say okay
and we throw in the duration feature as
well okay and for the experiments i'm
going to show the maximum duration was
ten okay for the transition features we
played around with two different kinds
of things one is that basically this
frame level transition feature basically
just says give me the posteriors that's
around the boundary frames within some
window and i also might
incorporate a direct MLP prediction of
is this a boundary or not okay so you
can you can train an MLP that does that
as well now the segment level basically
basically says I have to look at I have
to look at the hypothesized segmentation
on either side and and now extract the
features corresponding to both sides so
if i want the segments I now have to
both hypothesize a segmentation on the
left and a hug mentation on the right
okay so this is more expensive okay and
this is what we're trying to get away
from okay but but we want to know how
much do we lose by actually going to
this local boundary idea okay so this is
core test accuracy we also tend to
report on enhanced accuracy which tends
to be about which is like the full the
full dev site and that tends to be about
two points higher so if you're ever
wondering why these numbers look a
little lower than Jeremy's because he
tends to report on the hand stuff so the
core test basically if you plug the
stuff into a tandem hmm you get about
roughly the same thing as frame CRF on
the enhanced that it tends to be bigger
and and statistically significant so
when we just use segmental state
features with no bound no transitional
information we get essentially a boost
and that so this is really compared to
that okay so you get about two points by
going directly from frame to two
segments okay when you incorporate now
if you do the full scr f training where
you have to look at all pairs of
possible things you get a training time
about 62 minutes when you now collapse
it down by having this fan into a single
state you actually improve training time
quite a bit this is per epoch of
training so when we put in the segmental
transit transition feature we get
another point here and then so that's
nice right so it turns out that if you
if you instead do some sort of windowed
computation you can actually get a
little bit more and you'll notice that
this becomes really expensive to train
in the CRF
world right uh it's not so bad it's you
know about eight times faster or so
right that to do this sort of just focus
on the boundary rather than looking at
all the all pairs okay so you can you
can do this trade off of like is it
worth the extra point to to do you know
essentially three times as much training
you can you can play with this idea
right there's some happy medium we're
just showing you some choice point now
this was using just the phone posteriors
arm yep are the numbers in the last two
rows there there's two different
training times
yes right yes that's right for the
boundary as CRF and regular scra right
exactly are the accuracy is the same the
accuracies turn out to be exactly the
same oh because the models are exactly
the same because we're not using any
features that talk about the scra is not
using it's not using anything that's
right it's not it doesn't have a feature
that talks about that so it's just
wasted power exactly right yes so
they're there they're equivalent because
of the types of features you want to use
so what will advise you just say mental
pictures that you can flip something
take average oh that's right you'll miss
no no we can do that we do that if we
did that is one of our experiments as
you just make instead of taking the
average MCCA take an average of the
posteriors yeah right and then we said
we've done that and that works about the
same as taking the uniform sampling or
all the stuff you know so I mean each
one works a little better a little worse
in various situations right okay uh so
that's if you add in the phonological
features it turns out annoyingly that we
actually ended up with the same amount
by adding the phonological features so
it helps down here so we're still sort
of puzzling over that a little bit I
don't know what to say about that yet
okay that so we're eventually moving to
a word recognition on on this and the
idea is that we can one option it that
we're interested in doing is basically
saying look you know we can use jeffs
scarf system as a higher level
processing to basically take this this
becomes a first pass that the scarf can
now incorporate larger segmental
features on so we basically put out
lattices and then and then have scarf
sort of work on that so that's one
option that we're looking at for doing
word level of the coding the other
option that we're looking at is Jeremy
Morris did some sort of hybrid
recognition style thing for basically
turning this into like a hybrid neural
network except for that works on
sequences rather than individual of
bumps so so that's all I had to say on
this topic
and I see that I'm running really long
so I i guess i'm going to give you guys
a choice because because I will have to
sort of shrink so I have some I I know
Rohit actually gave a talk here on the
articulatory feature stuff last year
when he was interning so I could turn to
the the more text based processing stuff
or I could talk a little bit a little
dabble in each so maybe I'll have a show
of hands so the rule month you'll know
how do we have we have time I don't we
have 1140 we have damn showing up oh
right we have yeah I was 1130 pick them
up okay if there's a group of us saw
this going to lunch we should probably
eat the wrong lunch crowd yes absolutely
correct so just so you can help
calibrate there's probably the five or
six people whose through who might have
seen row in stock okay so that's that's
a good calibration so maybe I should I
know so would people would people prefer
to hear more about like our ticket so
articulatory feature modeling that sort
of answers Lee's question a little bit
about the stuff or text based processing
and I'll touch on each one just to give
you the flavor what's going on so but ok
I'll I'll zip through a little bit okay
our tech kotorye feature modeling okay
so um what are we what we've been doing
up to this point is basically taking
articulatory features and then sort of
using them as estimates and then
combining them by thinking about a
linear sequence of phones okay and one
of the things that we're interested in
I've been working with Karen Levesque
ooh and with two of my students Rohit
who you guys know because the intern
here last year and pretty eot and we've
been working on some articulatory future
modeling and we've been sort of
complexity RFS in a different dimension
which is in terms of factors state
spaces rather than factoring time so um
just to give you a view of this what you
know
one way of thinking about the world is
of of language that so if you have the
word sense it can often be pronounces
sense because there's if you get the
articulator zout of synchrony right you
end up having more of a closure and then
a release which I ended up sounding like
a tea so it's so you get scents instead
of said sense it's hard to do on the fly
okay so they're models of this like
articulatory phonology and Karen spent a
lot of time thinking about models and
this bill done some stuff that Lee done
as well and thinking about can we build
models how articulate errs combine to
produce phonological effects okay so the
idea here is that if you get an A
synchrony in the tongue body the tongue
tip in the lips moving at a different
time than the vom and the glottis does
you end up with I should say it's over
here this the asynchrony well I guess
both are asynchronous but you end up
with a nasalized Val and you also end up
with this extra phone that didn't appear
in the original transcription and the
claim is that this it can account for a
lot of that pronunciation variation we
saw in the beginning it Canada can't is
it counts for some of the effects it
doesn't count for all of them okay and
so Rho it built this model that he
talked about last year where we're
trying to do articulatory feature
alignment and this gets back to the
question I was asked about how do I get
the targets for the all of the mlps that
I'm training right and the problem is is
that if you're going from a phone if
you're going from this right if I just
project from this phone to each one of
these dimensions so that I've changed my
change my categorization because i'm
using articulatory features rather than
phonological features but point is the
same I don't going to be able to really
match what's really going on which is
this right so a mapping hear from you
know n in and this nay
n right isn't going to be able to get me
this particular asynchrony very well
okay unless I have a really fine
transcription in fact so we can use the
switchboard transcription project as our
fine transcription and then project
backwards so that's that's the way that
we're going to see our models to answer
some questions but if I'm just going off
of a dictionary where don't have
detailed hand transcriptions I'm not
gonna be able to get this so the
question is how do I bootstrap from
models where I've got this really fine
transcription and be able to project
back to this kind of space and that's
where the alignment comes in so I
basically want to say hey I've got the
word sense coming in and I want to be
able to say I expect sort of some
asynchronous thing going on with it
there's some synchrony going on between
the streams but they're not there's some
asynchrony going on between the stream
so i need to have a model of that okay
so we put up this monstrosity right oh
right but let me let me make that a
little simpler okay so we so let's think
of each one of these colors is some sort
of asynchronous state where basically
you're now thinking about what is the
value of so I have essentially a phone
that mat so between this word in the sub
word state it will map to a particular
articulatory feature which is
corresponds to a phone okay now there's
a model that basically says are these
features allowed to be asynchronous what
is the probability distribution over the
being them asynchronous and I can talk
about different streams going on in
parallel one two three as many as I want
right where they each have these these
linear structures going over time but
also have synchrony constraints that go
on between the streams okay so this is
kind of a really ugly factor graph the
red by the way the red things are
trainable the blue things are actually
deterministic so to not it's not as
badly as having to learn everything
about about these things right so but we
can take advantage of these
deterministic constraints because the
number of redken
is actually pretty small right and we
can boil down this model into
essentially a very simple model that
talks about a vector space of sub word
configurations okay and we can talk
about the fact that so let's imagine I
put a limit so if i head cat and I had
three streams I could have one one one
two two two three three three or I could
have won 11 and then one to one and then
two two two three three three so I can
start getting things that are
asynchronous out of out of line right if
I put a limit on that and how far my
articular is allowed to get out of
synchrony and and the the the ones in
two to three basically talk about the
canonical positions for the cup the a
and the ta's right or for the sense
right for sup at and right so I can talk
about oh well this one moved ahead but
this one didn't write and all that
really is is a change in index space
okay so if you do that basically net we
now have basically a bunch of trainable
parameters and then 11 basic
configuration parameter that basically
says hey this is a these are the
possible transitions between these two
states so we actually keep I do we keep
that fixed and I doesn't matter okay but
the idea is what we're doing is
eliminating deterministic variables and
that basically so we originally try to
employ the this with a general purpose
CRF toolkit and it got hairy by doing
some neat tricks which actually are
related to the tricks for the boundary
related if factored thing about about
restricting your state spaces you can
actually get a computationally tractable
model and we found that if you take the
alignment error rate we actually end up
getting articulator alignment errors
reduced by about five to fifteen percent
relative okay so that's sort of where
where that's been
to initialize the model but using
switchboard transcription project okay
so you have hand transcribed things and
then we're going to propagate well
actually in that case we're training and
testing on that set but the ideas
eventually we're we're going we are we
then took that and aligned all of
switchboard for another experiment but
I'm not going to talk about that but yes
so you use that it's a bootstrap uh we
don't have too many results for this
model no that's right we probably should
do a 10 minute result for this model
right that's a good boy okay and if
you're interested in more details that
that was Ras are you 2011 paper okay um
so Rohit's got to graduate at some point
so that he can go off and be a great
research or somewhere so you know and
that he was playing around with trying
to get this to be full recognition and i
said well you know what if i tried to
this is keyword spotting we we enlisted
the help of yossi keshet who was who had
done some discriminative keyword
spotting and built a model that
basically says hey I'm going to develop
this feature based keyword spotter that
basically says alright I've got two
segments of speech and one has the word
one does not have the word and the
objective function that I want to
optimize basically says my score for the
thing that has the word had better be
better than the squirter that doesn't
have the work okay that's the very
simple version of that so that's we want
we want this function and I'm not going
to talk too much about that but the idea
that's behind this is that this now
becomes this we can parameterize with
some lambda weighted sets of features
that we extract from something okay and
in this case we're going to use the
alignment model to basically say what's
my best alignment okay of the word and
then we're going to extract features
from those segments found in the
alignment okay and currently he's done
this with phone-based alignments but
they but what he's working on now is to
try and get articular based alignments
okay and at the moment and so we just
finished a paper rougher ms MLS LP which
is the machine learning and signaling
speak machine and machine learning and
speech and language processing new thing
that that's the the symposium is coming
out it turned that this actually worked
pretty well in low-resource settings
where you don't have a lot of data which
is important to us if we let's say have
a language from a new data data from a
new language where we don't have a lot
of data so we haven't conducted any
experiments on that kind of multilingual
setting but so I realized that was an
extremely hand-wavy sort of version of
this but I do want to talk a little bit
about the stuff we've been doing with
electronic medical records so if anybody
had any questions about that but that's
just to give you the flavor of we got
articulator alignments and then we're
going to use those to extract features
for doing one of these keywords sputters
okay that's the that's a real rub huh
well yeah yeah so the this so this turns
out to be the the model turns out not to
be a CRF in that case it turns out to be
a perceptron it's kind of like a
perceptron yosi wouldn't call it a
perception I call it a perceptron but it
you know right its train it's actually
trained using a max margin style probe
so I guess you could think of it as an
SVM on linear SVM okay last bit I've
been working on some electronic medical
record stuff which is kind of been a
kind of eye-opening and it's kind of
taking taking me back to the CRF stuff
you know where did CRF stuff come from
in terms of language processing so as a
zillow is saying I've been dragged back
into language it's like whoa okay so let
me give you a record this is sanitized
medical record out of out of a su
Medical Center and you'll notice some
interesting things about this so you've
got some information of corresponding to
the the patient and the doctor and all
the this is all structured data right
and then we've got a bunch of
unstructured data that's that's in
particular region so what's a what's the
history you know what did we find in
terms of tests what are we planning to
do okay and the idea is that we're going
to get multiple of these notes and
eventually over the what we want to be
building over lifetime is a model where
we say here's an hour first narrative we
have some things where you know here's
our admission date there are some things
that happen before admission there's
some things that happen after after
mission but before discharge and there's
some things that happen after discharge
or plan to happen after discharge right
and here's out second admission which is
hopefully after the first discharge date
hopefully way after but not often in
some of these things and so if there's a
readmission you know we now know that
the medical history is going to
correspond to some events but maybe some
other events that weren't in this
original thing and how we find the old
these correspondences okay so this
becomes this interesting problem of how
do I know when a medical event in one
document or within one document it
corresponds another medical event within
that document or across documents so
there's sort of two problems we then
document and cross document coreference
resolution and medical events so it's
kind of like a nerf war resolution in
language processing but we're here we're
talking about events event resolution
okay so the obvious thing to do oh and i
just want to point out you know so here
we've got cocaine use you know is these
are labeled twice in the same data and
this is the same but you might have
chest pain here but this chest pain
actually is not the same as that chest
pain okay we'll see an example what that
really means so and the thing is is that
labeling these things is hard and the
the you can get reasonable in turner
entertainment if you spend a lot of time
at it but it takes forever we've got at
this point at the point of this study we
had three patients and about 35 clinical
notes I mean it's really taken and that
was with four annotators going
through and doing all the stuff so it's
pretty pretty ugly so we want to use
some sort of some semi-supervised
techniques right and the question is can
we use unlabeled data to do that right
so what we want to do is build the
temporal classifier to do this because
when people think about coreference
roses I don't often think about semantic
concepts which is chest pain acute chest
pain oh they're the same thing right so
there's a semantic overlap right but it
turns out there's a lot of cues that
correspond to temporal information
according to events that that really can
tell you is this the same event or not
I'm going to skip the semantics so I'm
gonna we take it for granted we extract
a bunch of semantic features that are
medically relevant okay but I want to
talk about the temporal stuff because
that's that's actually where our newest
work people done that kind of thing for
for this man except so the idea is is
that what we're going to do is try and
build course time bins and build a
sequence tagger conditioning them field
to basically try and assign medical
events to relat times that points in
time relative to admission okay so we
may have things that are way before
admission just before admission like way
before mission like a year okay before
admission after admission or after
discharge okay those are big our added
mission is the other one right and the
idea is okay so here's our medical note
right so you've got cocaine use and the
hypertension it's with a history of
right so that's our way before right
chest pain which started two days ago is
also before admission he does not have
chest pain now that's after admission
but ever since the episode 2 days ago
before admission so here we have chest
pain these two are not Co referent but
this episode is co referent with with
the chest pain okay so we're going to
need some semantic information and know
that chest pain could be an episode
right so that's what the semantic stuff
is doing and the temporal stuff is
basically giving us a different view so
we so we can extract state function
and then we're going to have some
transition functions in the usuals hear
of it okay so what is it what are the
kinds of functions that you would
actually extract one is what what
section are you currently in okay it's
not a guaranteed thing so past medical
history doesn't necessarily always even
it's going to be a bias definitely two
things that are before admission but
things doctors don't always pay
attention to what they're typing we're
right so things that can happen after
admission still occur in that frankly so
we look at the section but it's a good
clue right we're also interested in
lexical features so history are
presented with right these are going to
be the preceding diagrams right that
that will tell us something we also
extract things like not only just the
preceding by grams but we also you know
parts of speech and stuff like that so
we so we know that oh this is probably a
temporal model modifier right so that's
a good thing to know okay the other
thing is is that there's actually a lot
of time relative time references in here
so two days ago two days ago well these
two things are going to map to so we
look for the closest relevant you know
bits of temporal expressions so we do a
temporal expression parser for example
and essentially what we end up doing is
building this model using all of these
expressions and we can now label every
event with we know the admission date
because that's in the structured data
now we basically say we can give a
relative time essentially based on where
we're signing things in the temporal man
okay why is this important to us well
essentially we're going to then take our
semantic features as one view of the
world temporal features are another view
of the world and we're going to do
semi-supervised learning in a multi view
type of environment and we play with two
different multi-vue strategies one is to
basically do code training where you
basically say I'm going to train
classifier on one type of feature
predict label instances in my
unlabeled data that I'm pretty sure
about and then use that to train the
other and then go back and forth and go
back and forth thing back before then
he's just angry it's it's just a huge
sparse vector okay in a yes right and
the other option is to basically talk
about posterior regularization I'm not
an expert in history regularization so
I'll wave my hands by basically saying
look the you can develop a probability
structure and then have a prior from
some other model well the other model
the temporal model now serves as a prior
for this the dismantle and the semantic
model is a prior for the temporal model
and then you can again do that kind of
labeling so that you try and constrain
the amount that you that you disagree on
those on the outputs those two things so
you go back and forth back and forth so
just to give you a sense of this if you
do supervised learning on a 60-40 split
of the data this is a data set we
probably have all the data you get about
seventy-seven percent on clinical notes
with recall of precision of
seventy-seven percent sub is this Co
referent recall of ninety percent coach
training basically works a little worse
than posterior regularization for this
but essentially we're getting pretty
close to the supervised learning which
is surprising to us and we have not
actually taken advantage of all of the
best numbers of clinical notes that we
could do so that's it to be the future
experiment okay so summing sorry to
blast through the last bit but I just
wanted to give you a flavor the kinds of
things that we're working on in the
speed in speech-language technology lab
we've got a number of other projects as
i said but a lot of the stuff that we're
doing is basically on you know how do i
think about intelligence feature
extraction and then plugging it into
relatively standard segmental tools so
we've been doing this when speech
processing and you know how do you do
intelligent factorization of that space
and the text processing we've been using
those as features for something else
so and one of the things that we've
noticed is that you basically if you
start using things like you know people
talk about CRFs is a replacement for
HMMs the kind of discriminative hms
there they're a little more powerful
than I mean even a linear chain CRF is
more powerful than a hmmm when you start
thinking about transition observations I
i think the the practical import of it
is that I mean the high gold paper kind
of shows that you can factorize the
space back into that in practical terms
this is an easy way to have this
observational dependence right and so
we've been using the sequence models
also as features for other learners and
finding out that you know incorporating
sequence information at the lower levels
can often help when you're doing the
prediction for other tasks as well so
this is the the two main messages so
that's all I have to say I'm sorry sick
vision full inches uShip yeah we all do
you have any felt on Fox you can copy
them no but I would love it if you know
I think I mean out I've been I've been
I've been wrapping my brain around that
for a while and I have some ideas on
like curve modeling and stuff like that
but mm I don't you know it's not I don't
have anything solid so you coming
covering many models in stock cities
I was sweating on Jeff to do that no I
mean um that's an interesting question
because it cuz the state space uh i mean
i think i think the way that Jeff does
it is not is not a bad start because it
because he thinks about it's hard to
talk about this with him in the room ah
but but you know thinking about the the
language model state space you know
allows you to think about any any depth
of language model that you want because
you're just thinking about this estates
basin but i think if you i think you
know my take on it is you're probably
going to if you're not going to be able
to get an all one system I think you're
gonna and and we don't we get this with
the the deep nets and we get all this
stuff is that I see this as the acoustic
smoothing over over a number of lower
level estimates that can give you some
sort of phonological substrate and then
you use that to go upwards from there so
I think the language model stuff you
know you would do it you could do some
interesting things either with Mack
Center CRF style models on top of a
lattice a kind of the way that Jeff Jeff
thinks about this model i think is it
and so as long as you can so I that
would be my answer but I don't have a
good answer for you i think is the
upshot yeah
alright</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>