<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Gaussian Processes for Inference with Implicit Likelihoods | Coder Coacher - Coaching Coders</title><meta content="Gaussian Processes for Inference with Implicit Likelihoods - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Gaussian Processes for Inference with Implicit Likelihoods</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6CZFi7q1wKM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
eyes
sure to introduce merly Ron who is a
faculty member at the department of
statistics in Penn State University he's
visiting the university of washington
washington on sabbatical and today he's
going to be talking to us about gaussian
processes and implicit likelihoods
thanks very much Chris thanks for the
invitation so thank you all for showing
up I'm not sure how off-the-wall this
topic is here but there's quite there's
quite a variety of interesting work
going on in this place so hopefully this
will be interesting to several of you
all right so I'm just going to jump
right in rather than start off by
rereading the title time the the topic
is dealing with with complicated
scientific models and just brief
motivation for this work so the
scientists this all started because a
few different scientists came to talk to
me at Penn State because they needed
help with their monte carlo methods and
i've worked quite a bit on monte carlo
method so they came to talk to me what
monte carlo methods but then it turns
out that solving their problems monte
carlo was a small part of it so i had to
learn all this new stuff so this is what
this talk is about the new stuff that I
had to learn because the scientists came
to me with their difficult problems so
that's the the problems that they come
with their often interested in sort of
the mechanisms underlying physical
phenomena so these are this is to draw a
distinction between their models and
purely statistical models where you're
trying to just fit data so but it turns
out that they're also useful for
predictions and projections and the
reason why I make this a separate bullet
of course you think models are useful
for predictions and projections but it's
it's worth pointing this out because
these models have mechanisms and physics
built in and so you could think about
doing things that you would ordinarily
not do it statistical models which is
you don't want to extrapolate
statistical models very much but with
these models maybe you're okay with
extrapolating and so it's it's really
critical to work with a model provided
by the scientists and i have this up
here more for stat
stations than anybody else because they
often complain saying this is
unrealistic why don't you just do blah
no do this cool statistical model and
for these two reasons because you want
you want the actual physics to help you
do extrapolation and you're actually
interested in the underlying physical
phenomena you need to work with the
models they provide and then maybe you
build statistical layers on top of that
ok so these scientific models are
numerical solutions of mathematical or
deterministic models or stochastic
models that reflect scientific processes
and often they're these models are
translated into computer code where you
study the simulations of the of the
physical process at many different
parameters or initial conditions okay so
what are some of the challenges often
these these models and simulators are
very computationally expensive so the
climate scientists I work with it it
might take two weeks maybe a month to
run some of these the more expensive
models and it's also not automatic every
new run there's a PhD student in
geosciences who's tinkering with things
to make sure that things work right and
it's it's not possible to write
closed-form expressions for from any of
these models so I cannot write down any
mathematics that describes it might be
system of a few hundred or a few
thousand differential equations and you
cannot translate it into math the
likelihood function may be expensive to
cap it evaluate so if I'm talking about
a stochastic model maybe I can write
down the model but and it looks nice but
it may be expensive to evaluate so if
you want to do maximum likelihood or
Bayesian inference it's this is this is
this is problematic and then this is
sort of an obvious statement but it's
worth reminding people about this is
that the model is not for any initial
setting or calibrated values of initial
of the parameters it may not it's not
going to pre produce reality and you
need to make an adjustment for that so
that's modeling discrepancies and so
this is why why the word in
plis it is being used here so the likely
is often implicit which means you don't
act literally you don't have a
likelihood function because it's
deterministic model and there's no
probability model so there's no
likelihood or there is a probability
model and there's a likelihood but it's
expensive or there's other problems with
it and so you treat it as implicit it's
there but you're not going to actually
deal with the likelihood directly so I'm
going to talk about two examples
depending on how things go mostly talked
about this and then briefly talk about
the second project so the first one is
was brought to me by climate scientists
who have these models that they use for
projecting the behavior of global ocean
circulation systems and then the other
one was brought to me by these disease
dynamics researchers at Penn State who
are interested in seeing how infectious
diseases spread this is deterministic
and this is stochastic okay so just a
little whiff of thought the science I'm
underlying it if you if you've seen Al
Gore's movie maybe you're already
familiar with this the meridional
overturning circulation all you really
need to know about this is it's a way so
that the red arrow is here I tell you
that sort of the warm waters from the
equator are moving out towards the poles
and in the colder water from the poles
are moving back down to the equator and
so there's this sort of this very very
very slow mixing that's occurring and
what what you need to take away from
this is that if that mixing doesn't
continue the way it has its it somehow
will disturb the equilibrium that the
climate system has right now so
essentially if this mixing slows down or
collapses is the word they like to say
that they like to use then this is going
to start causing dramatic changes to the
climate say of northern Europe among
other places so this is an no this is
called the Atlantic meridional
overturning circulation and this is key
to maintaining the climate system so the
collapse of this may result in dramatic
climate change and so this is a very
complicated model that they have to
to try to model this this MOC and one of
the things that ski there's lots of
stuff that needs to be tuned you know
think of them as inputs or i'm just
going to refer to them as parameters
because it turns out that both the
scientists referred them as parameters
as well as statisticians refer to the
most parameters i'm going to refer to
them as parameters and henceforth but
think of them as sort of things that you
calibrate in your model kv is one of
them and k VK v is is is a particularly
important parameter that influences how
these models work so kv quantifies the
intensity of vertical mixing in the
ocean so you can see how that might be
related to how the circulation system
behaves and it turns out that this thing
cannot be measured directly so you have
to rely on indirect information and this
is this is these are observations so
this is what one would call data in the
usual sense observations of two ocean
tracers so these are spaced spatial
fields information gathered literally
ocean liners going off and dropping
things into the ocean and measuring
things so so these are the two traces
c14 and CFC 11 this provides some
information about the mixing and then
this is is sort of this is climate model
output not typical are typical notion of
data because every time I feed an input
I'm getting the same value out it's
deterministic right so for different
values of K V as input I'm going to get
different values of the spatial field as
output from the from the model and so
these guys will look there's there they
are trying to mimic these guys okay and
of course they're going to be different
so now this is no longer just the two
this is a spatial field z1 there's a
spatial field z2 now I have spatial
fields y1 and y2 that correspond to this
but they're not functions of the input
kv so far so good out ok and just to
give you an idea of this is a very very
crude image plot but just to give you an
idea what we're dealing with this is
this is a spatial field is a slice of it
for CSC
seven one of the two traces at different
settings of kb point point oh five point
two and point five and these are the
actual data ok no processing just some
smoothing it's a very crude picture just
to give you an idea that well you know
these things vaguely look like the real
data nothing quite matches it but what
what the scientists would do before they
were talking to statisticians or doing
something more elaborate would be
essentially finding something some
output that most most closely behaving
like this data and then declaring that
to be the winner in some fashion right
put it to the eight is your motivation
of their physical phenomena that's
that's right that's right ok in all
honestly they do stare at some aspects
of the data when they're building these
things but but but your answer sort of
the the I mean what you're saying is how
they want it to be but at some level
there they peek at some level they're
aware of what's of what the data look
like when you say they see how close to
the mesh you mean eyeball it or no so
they you want to be very crude about it
might do something like a root mean
squared error and and and then whichever
one has a lowest one is declared the
winner or if you if they wanted they
recognize that they want a distribution
over the cavies so you know that they're
smart people so they will do something
that's very ad hoc like treat those
distances as some way of getting a
probability distribution even though it
has nothing to do with probabilities and
then they get something some kind of
distribution over the values of cavies
but they're not happy with it which is
why they came to talk to me okay and so
this is a cartoon and machine learning
guys I know you guys seem much cooler
cartoons in this but this is just to
remind you of what what what we're doing
so these are the different inputs the
green guys are the this is different
values of K V the green ones are what
we've we've actually got run our model
at and an f of X 1 and f of X 2 this is
now spatial field
the spatial field actually bivariate
spatial field because I'm looking at two
traces and bivariate spatial field here
so you can see what I'm dealing with I
have an input and the output is to
spatial fuels okay so sort of a
complicated output and and X star is the
one where I don't have where I haven't
run the model at and f of X star is then
this this if you want to just think of
it simply this is sort of a
nonparametric regression problem but the
output is functional so sort of a
functional data problem but that's sort
of underlying it if you're just
interested in interpolating but now
we're interested in doing statistical
inference based on this right so I'm
working towards building a tool to do
statistical inference so I'm going to
fit the emulator to a training set from
the complex model so I've run the model
at different settings and I'm going to
fit this thing called an emulator and
the emulator is actually going to be
stochastic remember that the model is
deterministic so it's a bit peculiar
that I'm doing this but I'll explain to
you why for starters for starters by
doing having the stochastic interpolator
I have in essence a fast approximate
simulator so I put a new new input I can
actually simulate the output and because
it's stochastic I can actually get
uncertainties associated with the
prediction so there's going to be
greater uncertainty where there's less
training data okay and and this is this
actually my quiet climate scientists
friends love this because their
physicist by training and physicists
think about uncertainty in this way they
think of uncertainty is they you know if
they got all their physics right they
don't need probability and statistics
that's sort of how they think I'm
exaggerating a little bit but but that
that's sort of how they think and so
they like this expression of uncertainty
they it's it's going to be more
uncertain when you're far away from from
the values where you have training data
okay and of course Tony o'hagan who
would start a lot about this thing as a
researcher and in the UK said without
any quantification of uncertainty it is
easy to dismiss computer models so this
is this is a another nice reason why you
want to actually quantify uncertainty
properly and then finally this provides
a probability model
which is important because I'm now going
to go away and do statistical inference
based on this right if I don't have a
probability model I cannot do
statistical inference ok so now I'm
going to use the Gaussian process as a
way to to deal to do this interpolation
and to build this approximate
probability model so for those of you
who work I know some people in machine
learning live with Gaussian processes
how many of you here were used Gaussian
processes much anybody oh okay so okay
so so you can you can zone out for a
little if you like so the Gaussian
process are very useful models for
dependent process and this is actually
how I got I firsts my exposure to
Gaussian process first in spatial
spatial models and time series and and
there are turns out that they're also
very useful for modeling complicated
functions and the key idea there this is
this is peculiar if you're not used to
thinking about Gaussian processes in
this way because I you used to think
about it as modeling dependence but it
turns out that it's very nice for
modeling complicated functions and the
idea is that the dependence sort of
adjust for nonlinear relationships
between input and output and this is
still peculiar but maybe that my toy
examples will help you in a little bit
but before I get to the little examples
just some quick review of what Gaussian
process models are or overview for those
of you who are new to this so supposing
I look at oops supposing I look at a
process at location s in some domain D
so just think of D as being why not just
think of it as two dimensional Euclidean
space so that process Z living at s now
has some mean function that describes
where it is at a sort of the mean of the
process at location s plus w of s which
is the spatial dependence process so now
this looks an awful lot like a linear
regression the only difference is that
everything has has a an index associated
with it that's the only difference so
far right there's an S associated with
it
this thing is you're mean function and
this is your error if you're doing
linear regression this would just be
independent and identically distributed
you're doing simple linear regression
but an interesting thing to notice this
location s may be physical so it might
actually be geographical index or it may
be from input space so if you're in 2d
space it might be parameter theta 1 here
and theta 2 over there and what we do is
we model dependence among the spatial
random variables by modeling this w vs
as a gaussian process and a gaussian
process is just it's an infinite
dimensional process such that if you
take any finite collection of points s 1
through SN in this say two dimensional
Euclidean space then this the collection
of these random varies is a multivariate
normal okay and I'm going to assume that
i'm using a parametric covariance
function so everything is positive
definite and so on and here's just an
example just to give you an idea of how
it works the covariance between the
process at location x SI and the process
location SJ is maybe kappa times this
function that decays as i increase the
distance between these two things right
and so this just gives you a quick idea
of how this works so as i increase the
distance the dependence gets gets weaker
and there are two parameters fee and
Kappa that are positive that I can
fiddle with to control how the
dependence decays so two things close to
each other a lot of dependence two
things far away from each other weak
dependence and how it decays is
determined by these parameters okay and
so now this vector Z Z of s 1 through Z
of s n which is going to be my spatial
field is now given the parameters theta
that determined the covariance and beta
that determine the mean function is
going to be multivariate normal for any
any collection s 1 through SN so finite
collection of all these random variables
oops okay so once I've specified this
model right now I'm just talking about
Gaussian processes and Gaussian process
modeling okay so once i specify this
model inference and prediction can be
done by a maximum
like liver bays and maximum likelihood
just means i'm going to maximize that
likelihood with respect to theta and
beta and bass means i'm going to put a
prior and theta and beta and then do
markov chain monte carlo something else
to to learn about this posterior
distribution pi theta beta given Z this
basic my Gaussian process stuff but this
is key how I do prediction so once I fit
the model so i have my data and I fit a
model so I've learned about theta and
beta I can now predict at new locations
let's call it s 1 star through sm star
and so this z star is going to be the
collection of predictions at s1 start to
sm star and under the gaussian process
assumption these two guys have a joint
multivariate normal distribution
specified in this fashion that's the
right hand side is just notation
multivariate normal and once i have
these parameters i can tell you
everything there is to know about that
multivariate normal distribution and if
you go back and just do very basic you
know first-year graduate low you know
first few weeks graduate level
multivariate normal theory you know that
the Z star given these guys is
multivariate normal and you know the
mean and covariance and if you're doing
base it you it's just one step beyond
that you're going to average over the
uncertainty due to those parameters so
essentially what's the take home message
here once you fit the model you now have
a model for all other for the random
variate at any other finite set of
locations in the same space okay and
this is just to give you an idea of how
it works for dependent processes so this
black dots are is just a simulation from
an AR one an autoregressive one time
series so the black dots was this
simulation from that and if i ignore the
dependence and i just fit a regression
without dependence in the air i would
get the the my interpolator would be
these green guys so the green the stick
green one would be sort of my best
action and then the the dashed ones are
the interval the prediction intervals so
you see it's missing a lot of the
structure of my data but if I fit the
Gaussian process to this so the blue and
the red their Gaussian processes with
different kinds of covariance functions
and I'm just putting it in there to tell
you that you have a lot of flexibility
with how smooth you can make these
processes so the red one is in some ways
the the better fit right the blue one
over smooth depending on what you what
your presumption zar about the process
I'd say that the red one is a nicer fit
and it sort of picks up all the Wiggles
in the data ok and so ignoring
dependence you do poorly you put in the
dependence you do better this is kind of
obvious because the original process I
used to simulate this has dependence in
it so this is a little less obvious
except for people who do who do this in
machine learning you can actually use
this stochastic model to to do something
nicely for for complicated functions so
take these two toy examples so the sine
curve and sort of a damped sine curve
and my black dots are essentially a you
know correspond to input-output pairs
for this function that I've kept hidden
from what from my model so I pretend
that I don't know the function and I see
XY pairs using that those the sine curve
in the damp sine curve now I fit a
Gaussian process to both of these so
here's a model I'm going to use the same
gaussian process model to both of these
this is a constant mean which seems like
a ridiculous thing right I'm assuming
that that there's a constant mean a plus
W of X and this is a Gaussian process
over this region 0 to 20 so that's my
one be spatial domain I just use this
very very simple model and the beauty of
this is that I didn't need to know
anything about the underlying a function
that was the non linear linear functions
that were used to produce these two
things and it just does the right thing
it
apps and it produces a nice
interpolation and it gives you
prediction intervals all right and and
you can sort of get an idea of how the
dependence helps essentially if you if
you have a point here and if and you
have data point here in a data point
here you're trying to figure out you're
trying to make a prediction somewhere in
between the dependence tells you that
well it's going to look an awful lot
like this in an awful lot like this and
it's not going to look as much like the
guys that are further and further away
so the prediction gets pulled towards
the points that are nearby and that
automatically deals with the
nonlinearities and this is this is the
trick that that one uses to do this
emulations of complex computer models
and it works very nicely you can about
you know this is a toy example so if we
were just sitting this look as we go
let's do something that's a sign or a
damp sign but now imagine a spatial
field and to 2d spatial filled with very
weird nonlinear relationships possibly
between them and you don't really have
ideas for how to fit this but this this
just seems to work remarkably well ok so
the summary of the intra in torrential
problem is we're interested in learning
about this parameter theta so to take
you back to where we started theta is kV
remember the input that I said had a lot
to do with the prediction of the MOC
this circulation system you're
interested in learning about kv because
if i tell you something about kv i can
tell you something about what the mlc is
going to be and i can tell you something
about the uncertainties associated with
that so the statistical problem that
ends up being the following so you are
going to take the model output which is
a bivariate spatial process at each
theta so I'm going to say that thus I
one side to side k are the K different
spots at which I ran this model and I
have this is a spatial process this is a
spatial process and so on right that's
that's that's sort of my computer model
output
and then I have the actual data which is
a spatial process one spatial process
too ok and so what can we learn about
theta given both these pieces of
information and so i use a bayesian
approach to do this and it's in it turns
out that that it's really the right way
to proceed here because there's usually
real prior information about theta the
scientists know something but the
physics and the parameters there and the
likelihood surface for theta is often
multimodal if it's poorly informed or
can be multimodal and then there's
issues with identifiability sometimes if
you have high dimensional input space
and in any case it's just nice to have
access to the full posterior
distribution if you do is sort of an
optimizer and try to learn about the
shape of this thing via gradients and so
on it's just awkward it's just you don't
learn as much as you would like to but
doing doing having access to the full
posterior distribution is very helpful
and the status multivariate it's
important to look at by varied and
marginal distributions and it's just a
lot more convenient to use a base
approach so I'm you advocating a base
approach as a very practical way to
proceed rather than then then then just
you know giving you philosophical
reasons why why you should use it to
their of course nice philosophical
reasons to use it and then it's a this
thing is amenable to a hierarchical
specification and this is useful and
when i'm specifying the multivariate
spatial process ok so here's here's the
two-stage approach a I might skip over
some of the details of how this is done
and even my slide skip of skim over the
details but I'm hoping that this outline
is completely clear to to everyone so
here's here's how we split up the the
inference here problem we first think of
the problem is as a problem of figuring
out a probability model for the data
using the computer model right because
right now if I there's no direct
connection between the input the kV that
that parameters were interested in
and the data that we observed I don't
have a probability model connecting them
my only hope is to use the computer
model runs as sort of a surrogate right
so I'm going to find a probability model
for Z using the simulations and i model
the relationship between Z and Y sorry Z
and theta via this flexible emulator for
the model output ok so this ADA of y
theta this is now my Gaussian process
emulator okay so this is telling me if i
plug in a value of theta what kind of a
computer model output I will get and
then if I add in a discrepancy term so
this is you spend a fair bit of time
fussing over this as well I'm
acknowledging the fact that even if I
put in the theta value that's sort of
the perfect k v input the output is not
going to match the observations and it's
not just going to be iid noise the IT
noise would be this term there may be
systematic discrepancy for example
towards the polar regions these models
may generally have a positive bias and
things like that so you want to actually
allow for that those kinds of
discrepancy so you need you need to do
something fairly flexible for this
discrepancy term as well but once you've
done this once we so this is the hard
work this is really hard work you figure
out how to build this approximate
probability model once you've built the
probability model then it's easy right
easy modular computing it's easy because
once you have the probability model you
have a likelihood and when you have a
likelihood I've stick in a prior
distribution and I have a posterior
distribution now yes
very complicated and I would like to
leave that are complicated with a reason
very sophisticated yes this is
you're building
ready
proc summation crap for this one so so
is the conclusion that the models are
can be simplified
so so this is a simple approximation but
it's very flexible so the Assumption
underlying this is that it's an
approximation that assumes that there's
a certain so let me summarize the
question here the question is the
original computer models are very
complicated just you have replaced it
with a much simpler model so the idea is
so the question is is the simpler model
actually saying that the original
computer model isn't nearly as
complicated as people think and know
that the computer model is still very
complicated but the idea is it's think
of you know a higher dimensional version
of the damp sine curve or something a
little bit more elaborate than that and
a more complicated function of course
but you can still do statistical
interpolation quite nicely with this
this these models even though they look
simple they're remarkably flexible so
the big assumption here is that there's
a certain kind of smoothness as I'm
moving an input space the output is
going to change smoothly so that's sort
of the assumption if there's a certain
degree of smoothness that you're willing
to assume then you have enough
information based on the computer model
runs that you can actually replace it
with this statistical approximation that
still does a reasonable job that's
that's what we're getting at this is not
so much that it you know maybe maybe
emulators the words meetings you know a
little bit confusing news is in my
perception to running they maybe a
different effect right your bdays say
that you're actually taking a whole
bunch of different model runs from
computer mall here and you're actually
using GP
approach to your interpolation between
those based on the parametric space that
you care about I don't think in a way
that gives you an certainty around yeah
far anyway verbally okay that's exactly
it but that's all of emulators remove
exams I know you're making a replacement
for the model but but but in a way when
when you're doing an interpolation you
are making a replacement for the model
right so everything you said is
compatible with with with with what I've
been saying here yes refined version to
see whether or not babies so you use the
computer model to generate these discs
and underlying data and effectively you
want the Jeep is it true that you might
use the GP to model the residual of the
computer runs in the data you have
observed effectively if active
effectively you well not not quite i'm
also well that's that that the residuals
modeling that's going into the
discrepancy function here and it turns
out that that's also a gaussian process
which is why i answered in the
affirmative it first this is also a
Gaussian process but I'm not going to
get into that detail you need enough
flexibility to model of residuals as
well but before I do that I need an
approximate model to the original
computer model all right and I notice
even if first of all this this works a
lot better than you might think it's
it's it's a very simple it's a very
simple-looking approximation but it
works quite well as an interpolator and
what's more because it's stochastic if
you're far away from a point where you
actually have a run you are you sort of
honestly characterizing the fact that I
have more uncertainty with my
interpreter at those points so so this
this this works out well and then we do
this sort of these perfect model
experiments where we we holds you know
do do different kinds of
cross-validation type exercises and then
we add all kinds of noise and there and
we see how it works and it it's quite
remarkable how well this works yes
Delta charm is one discrepancy between
your new model and the emulation model
this is just that you both have another
term for modeling so no sorry it's not
it's not it's best just no no so so this
is the this discrepancy is actually what
what I'm what I've done is I've taken
the computer model and I've replaced it
with this approximate model this is
modeling the distance between the
approximate model and the data not the
not the not my approximate model and the
original model the proximal model and
the data is there any reason not to have
a charm specifically for this currency
between the computer models and the data
no but that discrepancy is already so
the computer model and the the emulator
right that that's already built into
this guy because right through natural
yes yes right but if you saw well let me
see if if I if you actually have points
that look like this if I'm going to try
to exaggerate this just to make a point
at this point my prediction is going to
be pretty much that I get exact you know
I know this computer model exactly so i
might my interpolator might look
something like this I'm and I'm
exaggerating this just just to make the
point so in between the intervals are
going to get very large but the reason
that you didn't see in my toy examples
you didn't see it actually hit these
points there are a couple of reasons one
is you allow for something called micro
scale variation but I'm getting into too
much detail here but and also there's
some computational advantages to
allowing for error even at the place
where you have observations too much
detail but this is to give you an idea
of the fact that as I get away from this
I'm automatically accounting for the
uncertainties here already and then this
is the additional uncertainty that's
explaining the gap between the computer
model and my observations and then this
is sort of the iid noise on top of that
yes
in fact I understand sure no no I like
the question so when you learn the model
right you know that the prediction will
be somewhat accurate as long the test
distribution is similar to the trainings
yeah so when you train the the first
component
you can fit in data to the to the
complex simulator to explore a nice of
possible immigrant but your observations
for the delta terms of limited by
reality yes but actually what you're
trying to predict is what what would
happen when situation changes so if
right you're trying to see what would
happen if the simulation in the ocean it
is powerful or it's going to be to get
hot so hot solid your estimator ah so so
the question was we're trying to predict
in sort of a domain that's outside where
we're at right so at this point that's
not how you want to think about it at
this point we're thinking about
producing inference for theta that
allows me to produce a model that looks
like what I have today once I've done
all up all of that I'm going to use the
physics of the model to extrapolate so
I'm yeah free parameters in the current
current model for insulation and then
you use a full-blown similar to this
that's right calibrate now and I'm use
this to run the model forward that's why
you need these deterministic model if
you did statistical interpolation of any
kind that's just it would just be junk I
mean you're not doing triplet you're
doing extrapolation it's jump so it's a
great question okay so all right uh I
think I'm going to skip over these
details because we've had enough
interested questions I want to get get
through to thee and this is sort of a
high level view of it also but but
essentially that first step that I said
where you're doing an emulation of the
of the two-stage spatial process we do a
lot of stuff to allow it to be flexible
enough to actually interpolate the
process as well so I'm just going to
skip over this this this corresponds to
stage one of the emulation
yes the state is literally one
dimensional here that k p value it's
just have one dimension yes yes and it
was the chili sauce facial very correct
yes right so the inputs way the input is
low dimensional it's one Molly vanish
now you know yes so I so that's that's
those are the details here so this is
the output y 2 and I build as high
akumal white y 1 given y 2 and by doing
this in this hierarchical fashion and I
can put in a lot of flexibility into the
model so i model the relationship
between do you have a similar yes well
y1 and y2 individually are Gaussian but
y 1 given y tu sorry y 1 given y 2s
gaussian and y 2 is gaussian and and
there's a relationship between the two
so let those those are details that i
think i'm happy and maybe you're all
happy that i'm skipping ask me no it's
that the boat that that discrepancy is
accountable for by the the term Delta
yeah this is this thing this thing is
mainly to say that the two spatial
processes are related the and we need to
somehow allow for the fact that they may
be related in sort of nonlinear ways
okay so going back to this the first
step we fit the Gaussian process model
so fitting it means I built this layer
two layered model and I have lots of
parameters and I and I fiddle with those
essentially i do maximum likelihood fit
to get nice parameter values that then
gives me a probability model for the
computer model output but now i want
something that also allows for the fact
that this additional error that gets me
to the actual observations and once i do
all of that I can do markov chain monte
carlo to get to the posterior
distribution that i'm interested this is
the this is the ultimate goal here okay
and then there's a lot of computational
issues so i have to make some decisions
on the fly here so i think i think i
might skip some of these things too and
just again give you a whiff of a related
I
because these are really details that if
you're working on this particular
project may be particularly interesting
to you but most of you know that if
you're if you're dealing with matrix
operations and the matrices are n by n
matrices the operations chalice key
factors and it's all order and cube
right and if M here is in our problem is
tens of thousands and I'm doing and I'm
having to do that at every stage of a
Markov chain Monte Carlo algorithm this
is hopeless so we you actually use a
reduced drank approach based on Colonel
mixing and some of you those of you work
on Gaussian processes no know what I'm
talking about but essentially you take a
very complicated Gaussian process model
and reduce and and make the covariance
into a pattern into a specific kind of
covariance structure that lets you use a
couple of these identities the Sherman
woodberry morrison and silvestris
theorem in order to do your computations
in a much faster slicker way so where
you had order n cube you deal with order
J cubed and I'm just going to skip over
the those those identities and just cut
to sort of the the conclusion at the end
of all of this work what what is our
summary our summary is we get a kv a
posterior distribution the blue guy is
there is a final product that we get
from all of this work and we have you
can see there's a prior distribution the
black one and then the red and the green
come from we fiddled around and see what
you can learn from the individual traces
just one tracer at a time so kV it turns
out what is very important for
calibrating this this model and these
kinds of models but there are other but
there are many other parameters that
those of you follow the climate science
discussions or debates if you want to
call them debates this climate science
literature there's a lot of there's a
parameter called climate sensitivity
that is a huge subject of research and
you know there are lots of science and
nature papers where essentially the
conclusion is a plot like this so and
our methodology you know extends to 22
to working with with any kind of
parameter essentially so that itself is
often an end goal for first scientists
but in this case we can use that to to
build to do projections okay so just to
summarize what we did was we obtain the
probability model that connected the
tracers to kV and the hierarchical model
with with Gaussian processes but pattern
covariances our flex is flexible and
computationally tractable and once i use
the this probability model i can infer
kv from observations and then once I've
inferred kb kv i can then say something
about the MOC and we find that MOC
weakened slightly over the next 50 years
that sort of taking a max budget
estimate and running water order is this
more patient day for you oh so so so
we're doing I skipped over that detail
but we're doing something that's sort of
an amalgam for for computational reasons
and they're identifiability issues so we
do we do some maximum likelihood as part
of it and then the final inference that
we do it this is entirely Bayesian so
yeah the details involve doing some
amount of fixing of parameters at
initial at estimates of pain but for
maximum likelihood and then coming back
and doing base steer racing you know
kind of smooth up at the top at least
you know and you know where even the
blue ones
yeah well yeah I'm not sure that I would
you know if you if you look at
posteriors from from MC MC algorithms
this is this is about as smooth as they
usually get so I wouldn't I wouldn't go
so far as calling this multi model but
but i think i think i did we declare
that make you an immortal poster okay um
alright so let me see how do i do the
time is i have about another since we
started about 10 minutes late i won't i
won't inflict myself on you for too long
but but maybe i'll go for another 10
minutes said okay so i'll just i'll just
give you an overview of this this is
another it's a very cool problem that
that okay cool to me that that we
started working on with infectious
disease collaborators and the reason
that I wanted to talk about this as well
is to give you an idea of how the tools
that I discuss here may be useful in
other problems as well and in sort of in
a surprising way in this problem so just
an overview of this slide these
scientists the infectious disease
dynamics people at Penn State came to me
with this model that they wanted to fit
the difference between this model and so
they came to me with sort of a more
obviously statistical problem they have
a stochastic model we can write down a
likelihood and they have data and they
said we'd like to fit this model and
again they came to me because I've
worked in Monte Carlo methods and do you
think so this this shouldn't be that
hard a problem but the the issue with
their model is and their model you can
write down very easily the the math not
you know it's just a one on one page you
can write down the entire likelihood
function but the problem is that and
this is a model that that you can you
can that that's used to describe how
diseases spread in particular this is
used for measles epidemics for modeling
measles dynamics but the problem is that
that there are thousands of latent
variables and the things that you don't
observe so then
number of immigrants moving from one
location to another if you have measles
and you move to another location you're
going to you're going to probably have
get other people to get have measles as
well to get measles as well so you don't
observe all that information so but you
need that latent process in there to
somehow account for that and if you if
you're stuck with latent variables lots
of latent variables you know that you
need to integrate those latent variables
out when you're performing inference and
if you have thousands of these this is a
this is a hard problem okay and the the
the space-time data set is lovely
because it's very rich and very nice
from from England and Wales so it has
five hundred nineteen thousand points
which is great because you can actually
learn about this disease model you often
that the information is sparse so here
this is great but of course it presents
some computational challenges you cannot
just do something naive and have this
work and this is good to give you an
idea of the kind of data we have many
different cities big cities London and
this is just telling you the number of
disease cases you know it's by weekly
data and these are the smaller cities
and the reason I'm showing you this is
first to give you an idea of what kind
of data we have is time series in 952
different cities 952 plots like this
that's the data and you can see that
it's quite different when you move from
big cities to small cities it's quite
quite widely different and that that
messes up doing the this likelihood
inference that we would like to do with
it yeah this there's a lot of seasonal
stuff here yeah and you can guess what
one of the seasonal things will be
measles has a lot to do with school kids
and so when schools are in session and
there's a spike and so on okay so
there's some issues here the stochastic
model is expenses too it results in
unlikely that's expensive to evaluate
it's not very expensive but it's just
expensive enough that you can't do
cannot do bayesian inference easily and
i'm going to skip over this does anybody
here know about approximate bayesian
computer
no then I'm going to skip over
essentially there's no other approach
for doing this easily so we develop a
very simple solution we thought we
thought we'd solve this problem we do a
grid-based Markov chain Monte Carlo and
we discretize the parameter space maybe
some of you have used these tricks and
so we do a lot of computing in parallel
ahead of time and then and then that
lets us do maximum likelihood and base
but then the problem is so we thought
we'd solve this problem we were done but
we fit this model and it doesn't fit the
scientifically relevant features of the
data and then if we do sort of a
controlled experiment where we know what
the input was and we have the output
corresponding to that the parameters
that were used we don't recover the
parameters that were used so that's bad
news right but the really crucial thing
is it does not fit scientifically
relevant features of the data so what
are scientifically relevant features
these are just a couple of examples I
won't these are things that the
biologists told us they would like the
models to mimic so they have a fitted
model they would like the fitted model
to reproduce a couple of these
characteristics and all I've done here
is I have predictions from our model
versus the actual observations for those
summary characteristics and you can see
that the fit is pretty lousy and so
they're unhappy when they see this and
so how do we solve we thought we'd solve
the problem but it turns out that likely
simple likelihood or base doesn't really
solve the problem so we thought why not
go and work directly with the summary
statistics right instead of the maximum
likely does not understand what features
biologists are interested in capturing
so we thought how about we move away
from maximum electric bass and we move
into this world of sort of this features
or summary statistics based on the data
and so what we then did was
we treated as data the summary
statistics and now you can go back to
thinking about this problem in the way
that we thought about the computer model
problem from the climate science
research which is for each input I can
run this model right at each input I
have a output that I now summarize I
have summary statistics and the summary
statistics are now like my output
corresponding to each theta and then I
have observations and I can use summary
statistics based on those observations
and then now i can use Gaussian process
based ideas just like I did before
that's a bit of a lie there's lots of
details here there's some computational
issues and we use some tricks to to get
the computing to work out right but the
end result of all of that is that we
produce parameter estimates that then
result in the model that fits the data
the things that they care about much
much better why is it not sure you
haven't you care about it like it's also
for instance work for this via the
slices that you have there like slices
in time and space right now you can
create an interpol a TP approach and
take a new others are point in time and
space and a creative value ah is that
the theta you care about okay so so
here's the funny thing we can no longer
our Gaussian process no longer
interpolates the original space-time
process in the previous method we can
actually interpolate that here the
interpolation we're doing of these
features yes so if you give me if you
give me a new parameter input I cannot
give you all of these time series
directly but based on the Gaussian
process emulator but what i can do is i
can give you the summary statistics
corresponding to that but it doesn't
matter because if i actually have the
parameters this stochastic model is way
easier you know i can run the stochastic
model which I couldn't do with the
climate science things if I have the
input space now I have the the
the distribution of the parameters I can
use that to run the original stochastic
model so i can actually still do sort of
predictions or projections or whatever I
want based on those models yes but but
still okay so now you're your GP is on
the summary statistics but still like
what is your deep in prayer even a
Tribble that you're interpolating
between are they time no so that the
thing that I'm doing the interpolation
across our is that input theta so I skip
yeah yeah yeah so I apologize but you
know am I skipping maybe I didn't spend
enough time just just laying out the
fact that theta just like in the other
case their parameters controlling the
climate side of the this process here
again theta the same retinoid red color
things right this time they control the
dynamics of the spread of the disease so
once I know what theta is or have an
idea of what theta is based on data I
can actually go back and fit and say
something about the process and in
likely theta values was there not yeah
just these curses in Patricia you are
these really the primary care about or
are these the sanity checks that they
test these are so this is these are I'm
not sure the distinction review to the
two so these are things that they want
may want to make sure that the model
actually reproduces this because they
don't think they would also like to see
the model reproduce the the time series
but this is if the model doesn't get
this right it's it's it's really missing
up missing what they think are sort of
key features of the biology now I'm
right but they would also like to see
the time series matched but you know if
you think about it if you have 952 time
series you need to come up with some
kind of metric that tells you how well
you're doing and it's not very obvious
how to do that because there's
complications there's lots of small
cities and you can be way off on many
many small cities but do very well on
the bigger cities and still get the
general picture right and you're willing
to live with the variability with the
smaller city prediction so if you did
something naive you wouldn't actually be
fitting it in the right way so which is
why this provides a natural way to think
about sort of a summary measures that
were interested in so and that's that's
why we work with this yes
we prefer to say that just doing ml of
Dave and the original data is equal
saying that you care about blood loss on
the cancer presentation and here basic
incentive to say no the life function i
care about is but matching maximum is in
that that's a very nice summer so it's
such a nice summary I'm going to repeat
it so did he what what what he is
reminding us is that that that maximum
likelihood you know in some sense if you
want to think about it in terms of loss
loss functions on the parameter space
it's telling you it's it's doing
something very naive in terms of
imposing loss right sadly there's a
squared function then that's not
necessarily what you want you would
rather have the lost be placed on these
guys and that's what that's what we end
up doing and so but so so it makes sense
that our model then fits this because
now we're actually trying to fit these
guys rather than trying to fit the
original thing that makes sense but the
very surprising and bizarre thing is
that if we do so this is not surprising
the weird thing is that if we actually
do a controlled experiment where we know
what the input was where we know what
was used to simulate the data and then
we hide it from our procedure this this
approach recovers it better than the the
base are the mat usual likelihood based
approach and that has we don't know
exactly the details why and the
mathematics of it I don't know exactly
how to do how to figure that out but we
think it has a lot to do with the fact
that these smaller cities have a really
disproportionate largely large say in
how the maximum likelihood approach
works and sort of noisy things in the
smaller cities really make screw up your
inference and by looking at the
summaries where somehow making it do
less ridiculous things it this is this
is this is a surprising part of it we
did not expected to get this
but we did not expect that we would also
get better parameter estimates so let me
just summarize enemy can come back if
you have more questions here so just an
overall summary so the Gaussian process
we think I think are very powerful to
when the likelihood is implicit and
simulating from the model is expensive
and these are very useful for
deterministic and stochastic models as
and these one the first case was
deterministic Sagan was stochastic and
we can perform inference based on
scientifically important features of the
data and this is what what I'm talked
about in a second in the disease
modeling approach and their computation
expedient and in fact I didn't say much
about this but except what what I just
said was surprising may actually improve
inference and prediction in sort of the
regular way that you would think about
doing infants and prediction and so this
is just acknowledging collaborators the
guys in blue are PhD students or former
PhD students and and so these are just
some some acknowledging support for all
of this work so thank you very much how
many questions so if you really like to
be early today there couple spots but it
is
be here for the rest of the day and
I said sit one comment I think you kind
of are saying this at the end but the
thing that you saw with the fact that
you're being feted better now with this
model must mean that these summary
statistics are also kind of discounting
those smaller yes yes exactly so it's
something the cleverness of it was
summary statistics however the
reconstructed ends up getting better the
thing and then let's try advantage of
that by looking good that's that's right
and recently there have been a couple of
other people working on very different
projects who have noticed things like
what we've noticed maybe there were
people before who notice that as well
but we do I wasn't aware of it but
summarizing things in a particular way
sometimes even if you know in statistics
if you're in the exponential family if
you're dealing with with with sufficient
statistics then it makes no difference
right but in these sort of complicated
models somehow doing looking at sort of
particular summaries does better than
working with the original data which is
interesting we should await those so
yeah it's the kind of the cool just a
little bit of your result right yeah
it's kind of like I think as between
learning people were kind of like Oh
always go to the raw data that's where
the real truth is that here you have a
case where in fact like people that
looked at and when like no no no this is
a tough that really matters you know
leveraging that is giving you this is
distinct absolutely and in fact you know
that that's that last part of my talk
and disease dynamics I you know I give
one hour talks on that subject alone and
they're one of the things that I would
often highlight is the fact that this
gives us a sort of a nice way to
directly talk to scientists I mean this
disc this gets scientists excited
because it it actually tells them that
their notions of what is important is
being built directly into the inference
again thank you saw</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>