<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Operating System Scheduler Design for Multicore Architectures | Coder Coacher - Coaching Coders</title><meta content="Operating System Scheduler Design for Multicore Architectures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Operating System Scheduler Design for Multicore Architectures</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Rl_NtL3vYZM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
hey thank y'all for coming it's my
pleasure to introduce Simon Peter Simon
is just finishing up his PhD in the
Systems Group at ETH Zurich and he's
going to talk about operating system
scheduler design for multi-core
architectures thanks Zander hey
everybody before I actually start with
the main topic of the talk I'd like to
say a little bit more about the context
of my work which is the bare fish
multi-core research operating system I
assume that most of you already know
about this OS so we'll keep this fairly
brief beverage is a cooperation between
MSR and ETH Zurich and the overarching
problem that we are trying to address
with this work is that commodity
computing is and has been for the past
couple of years in a period of
fundamental change multi-core
architectures mean speed-up through more
processor cores on processor dies
instead of just simply faster processors
now unfortunately this development is
not transparent to software and so we
actually have to write our software
specifically or modify our software to
make use of the additional course in the
in the architecture however at the same
time we see fast pace of quite
non-trivial developments within the
architecture into an AMD are churning
out a new multi-core hardware
architecture roughly every one and a
half years or so so that means we have
to keep modifying our software in order
to make it work on new multi-core
hardware architectures now operating
systems struggle very much to keep up
with these developments especially
commodity operating systems is what I'm
talking about
there's an extreme engineering effort
being put into a commodity operating
systems like Linux and Windows to have
them work right now on multi-core
architectures and is projected that that
effort has to increase even further as
more fundamental changes are being put
into these architectures this is a
structural problem Linda with these
operating systems Linux and windows are
quite complex monolithic programs that
have been developed back in the early
90s with Yuni processors and quite
simple multi processors in mind and
these quite disruptive
just in the architecture that impacts
suffer very much which is not around at
the time now bear fish is a new
operating system that's been built from
scratch for multi-core architectures one
of the in order to address these
problems one of the main problems that
bear fish is sorry one of the the main
ideas that we're using within beverages
to use distributed systems techniques in
order to make the system both more
scalable as we add more cores to the
architecture as well as something that
we call agility with the changing
architecture make the operating system
easily changeable our adapt to different
hardware architectures bear fish is
similar to other multi core operating
system approaches like quarry a Karass
farce and also tessellation that have
been proposed within academia over the
past year or two and so my hopes are
that the ideas that I would present in
this talk are also applicable to the
rest of the multi-core os space now I
have been involved with many things in
the bare efficient operating system and
I encourage you if you get to talk to me
later to ask me questions about other
things that I've been working on as well
and I'd also like to say that we can
make this talk fairly interactive so if
you have any questions just raise your
hand and I can keep track of time okay
so back to the topic of this talk which
is multi-core scheduling why is this is
becoming a challenge now what's
different in the way we need to schedule
applications now and in the future as
opposed to how we use to schedule
applications in the in the past well
it's that applications also increasingly
used parallelism in order to gain
computation of speed ups with the
additional cores that are now thee in
the architecture if I look over the past
couple of years I see a vast number of
parallel runtimes emerging there are
things like interest threading building
blocks
Microsoft has pealing there's also a
lesser well known runtime called conk RT
open and P has been around for a while
but it's seeing increased usage and
there are other things like apples Grand
Central Dispatch for example and all of
these runtimes are essentially made to
encourage programmers to use more
parallelism and programs are actually
starting to use these runtimes in order
to do that there is a new class of work
that is anticipated by many in the
community to become more important for
the end-user commodity computing space
these workloads have an umbrella term
they're called recognition mining
synthesis workloads an example would be
recognizing faces on photos that an
end-user might want to run on their on
their desktop box in the photo
collection the common property of all
the applications in in this class are
that they are quite computationally
intensive while also being easily
parallelizable so they would benefit
quite nicely from the addition of course
in the architecture now parallel
applications like these have quite
different scheduling requirements as
opposed to traditional single threaded
or even concurrent applications and
that's something that's quite well known
for example in the supercomputing
community that has been dealing with
parallelism essentially for decades and
quite adequate scheduling algorithms for
these parallel applications have been
developed in this community however the
focus there is mainly to run a big batch
of usually scientific applications that
run for a very long time to a
supercomputer so you submit that and
then you have to wait usually for you
know hours days sometimes even weeks
you don't really change the system while
you're running these applications and
then you collect your results back later
what's new now is that we want to run
these parallel applications in a mixed
together with traditional commodity
applications so maybe as alongside a
word processor that a user might be
running or as part of a web service so
the main two differences in this
scenario are that the the setting is
much more interactive we now have to
deal with ad hoc changes to the workload
as the user provides input to the system
and is also providing us with a new
metric responsiveness to the user is now
important we have to be able to make
scheduling decisions quickly enough so
that the user can actually feel a
difference in the system as the
schedulers making decisions yes
the word processor and then there's
massive scientific calculation well
they're computer intensive applications
so there's this face recognition for
example is something that is really
compute intensive it takes a while to
recognize faces on on photos but it's
still something that you might want to
run either in the background or you
might want to run this as part of a web
service for example you might just want
to do this when when people submit
photos with this load that's that's true
but I was comparing it to what people do
in the supercomputing community so there
they have developed scheduling
algorithms for parallel applications but
their use cases are typically throughput
oriented batch processing where you have
a big batch of applications and you just
want to and there's no real latency
requirement for a particular job for
example okay so to put that into the
nutshell again the research question
that I was trying to answer is how do we
schedule multiple parallel applications
in a dynamic interactive mix on
commodity multi-core pcs I'd like to
motivate this a bit further and actually
make it more concrete by giving you an
example of what happens when we run
multiple parallel applications today on
the commodity operating system in my
case I ran two parallel OpenMP
applications come currently at the same
time on a 16 core AMD Opteron system
fairly state-of-the-art on the Linux
operating system used the Intel openmp
library which is also state-of-the-art
OpenMP library both of my applications
essentially execute parallel follows
which is one of the main ways in open
impede again parallelism one application
however is completely CPU bound it's
just chugging away doing computation and
it's using eight threads the other
application is quite synchronization
intensive it executes a lot of OpenMP
barriers if if you don't know what a
barrier is I'd like to introduce it real
quick
it's essentially a primitive where these
you have to wait till all the threats
that enter the bear that enter the
barrier have to come out of the barrier
again so it's a coma synchronization
across all the threads if one thread
enters the barrier it will wait for all
the other threats to enter the barrier
as well and then they can all leave the
barrier again this primitive is
something that's used quite often within
parallel computing especially at the end
of computations and of sub computations
because you want all threads to finish
computing their chunk of a parallel data
set and then you usually do an
aggregation operation over that
computation so yes in fact these are
applications that I have synthesized
though they're actually doing some
computation it's not that they're just
going through barriers all the time
they're they're doing some computation
but it relationship yeah but I
synthesized them so that I can show the
effect so what I do then is as I run
these two applications concurrently I
vary the number of threads that the
barrier application uses as both
applications run in order to have some
notion of progress that these
applications are making account the
number of parallel follow iterations
that they're they're going through over
some time unit in my case it was a
second and then I look at the speed-up
or rather the slowdown as we will see in
the on the next slide of these two
applications as they run concurrently so
here's what I got on this graph you see
in the the top the kind of reddish
looking line is the CPU bound
application and then the purple line is
the the barrier application on the
x-axis we have the number of threads
that are allocated at any point in time
to the barrier application remember that
the CPU bound application is running on
eight threads all the time and that we
have 16 cores in total in this machine
and then on the y axis I have the
relative rate of progress that these two
applications are going through
normalized to the two threads case here
on the left-hand side so let's start in
focus
on the left hand side of this graph up
until a bear synchronizing in this case
there's not a very a lot of variability
use barriers yes this is pretty much
what you I would I would argue that if
you use a lot of synchronization most of
these algorithms are actually trying to
optimize for that case that actually
most of the threats are doing similar
amounts of work if you end up having one
thread doing a lot more work than if
you're going through the barrier you
would delay the execution of all the
other threats essentially so yeah yeah
I'll explain it okay so up until eight
barrier threats things look pretty much
benign we have the CPU one application
essentially stays there at a progress of
one which is what we would expect
because it has the same amount of
threads allocated to it and there's
nothing else running inside the system
so the operating system here is doing
something saying and it's scheduling the
CPU bound applications somewhere where
the barrier application is not running
we can see that the barrier application
is actually degrading in its performance
and that's due to the increasing cost of
going through a barrier with an
increasing number of threats you will
have to you have to synchronize with an
increasing number of threats and if
you're going through the barrier often
enough then you can actually see this
degradation so here's why the
application is is degrading but the the
progress that I'm showing you here is
actually over over all the threats so
the I could have graphed it differently
and then it would have stayed up there
so the but the the difference comes from
the from the barrier
yeah sorry I should have probably said
that earlier so yeah it's it's divided
by the number of friends is that the
same curve as you're getting to didn't
have the CPU intensive work
of course yes so you would see this
degradation of every convention
exactly the degradation is due to the
synchronization that the application is
doing over an increased number of course
but there's no no impact from from the
other application and doing memory
contingent exactly okay and actually
I've verified that the operating system
is doing core partitioning here
essentially so it's assigning the eight
eight cores to the CPU bound application
and then it's scheduling the barrier
bound application on the other eight
cores that are still available it's
essentially you're doing a load
balancing algorithm okay now on the
right hand side from nine threads
onwards this is the first case where we
have more threads in total in the system
than we have course in the system the
operating system starts to go to two
time multiplexing the different threads
over the course and with the CPU bound
application we still see something that
we would expect and that is that the CPU
bound application degrades linearly as
it's being overlapped with more and more
threads off the barrier application so
that's quite straightforward however
finally we see that the barrier
application applications progress drops
sharply almost immediately down to to
zero and that is due to the fact that it
can only make progress when all of its
threats are running concurrently if one
thread is being preempted by another
thread of the CPU bound application then
all of the other threads in the barrier
application now have to wait for that
one single thread to enter the barrier
as well before they can all continue
together so this is something that we do
not want and that can be alleviated yes
well sure if you if you had exactly if
you had 16 barrier threats and you had
just one cpu-bound
threat running you would still see a
behavior like this yeah yeah so you
could set up this experiment in
different ways and you would still right
yeah yeah yeah
so there are scheduling techniques to
alleviate this this problem parallel
scheduling techniques essentially one is
called
gang scheduling and that's a technique
to synchronize the execution of all the
threats of one application so you make
sure that you're always dispatching all
of the threats that are within this one
application at the same time and you
never have one threat execute out of
order essentially with the with the
other threads then there's also core
partitioning which is dedicating a
number of of course to a particular
application so there is no overlap with
any other application and these work
however we can't easily apply them in
present-day commodity operating systems
because there is no knowledge about the
applications requirements the
applications today are essentially
scheduled as black boxes using
heuristics by the operating system
completely unaware of what the
requirements are while the applications
and the runtime systems especially
within these applications could have
known that there are threats that are
synchronizing within the application so
in this case for example in the openmp
case interacting threats are always
within an open MP team and so either the
runtime or maybe with help from the
compiler the runtime could have known
that okay I don't know what that was
that these threats are synchronizing but
there's no way to tell the operating
system of this fact also we can't just
go and realize better scheduling solely
from within the runtimes themselves
because it's much more of a problem of
coordination between different runtimes
we might not just want to run open and P
programs maybe we want to run programs
that used other parallel programming
runtimes so how do we do the
coordination between these different
runtimes if we just did it within the
runtimes then several runtimes might
contend for the same course even if they
are very smart to figure out where to
run within the system and how to
schedule if two different runtimes make
the same decisions they might still end
up battling over the same resources and
all the benefits that they have
calculated for themselves might be void
so this is a case where policies and
mechanisms are essentially in the wrong
place
now my solution to this problem is to
have the operating system exploit the
knowledge of the different applications
to avoid miss scheduling them for
example the knowledge about which
threats are synchronizing within an
application on the other hand also
applications should be able to react
better to different processor
allocations that the operating system is
making for example the CPU bound
application could modify the number of
threads that it's using as processor
allocations change with the barrier
application or to give the barrier
application more CPU resource so my idea
is to integrate the operating system
scheduler and the language runtimes
more closely and the way I do this is
essentially firstly by removing the
threats abstraction at the operating
system interface level that doesn't mean
that I'm taking threats away from the
application programmers they can still
be provided within the the runtime this
is something that has been done in
operating systems before there's the k42
operating system in the psychie
operating system and also scheduling
technique called inheritance scheduling
that essentially does not require
threats at the OS interface level in my
case I provide what I call processor
dispatchers at the OS interface instead
and those give you much more control
over which processors you're actually
running on and give you the information
on where you're running on so the
operating system won't just migrate a
thread from one core to a different core
instead if you have a dispatcher you
know that you're running on a particular
core so it's kind of closer to running
on Hardware threads essentially and then
I extend the interface between the
operating system scheduler and the
programming language runtime and yeah
before I get to that I should also say
since extending the interface usually
raises questions with are people
actually going to use this do we now
have to modify all our applications or
to use this interface a claim that now
is a quite good time to do this move
because application programmers are
increasingly programming against
parallel runtimes instead of write to
the operating system interface directly
so we would only need to modify the the
parallel runtimes to inter
more closely with the OS instead of
every single application okay so my I
call my idea end-to-end scheduling
that's not to be confused with the intro
and paradigm in networking I just call
it that way because I kind of cut
through the threads abstraction to
integrate the OS scheduler and the
language runtimes more closely and enter
and scheduling is a technique for the
dynamic scheduling of parallel
applications that interactive time
scales there are three concepts to it
the first one is that applications
specify processor requirements to the
operating system scheduler so the OS
scheduler can learn what is important to
the application the second one is that
the operating system scheduler can
notify the application about allocation
changes that it's doing as the system
runs so the application can react to
system state changes and finally in
order to make this work at interactive
timescales I break up the scheduling
problem in scheduled ed multiple time
scales instead so that we can react
quickly to ad hoc workload changes I
will explain on the latest slide how
that works and why that is so important
okay first of the first concept which is
the processor requirements in my system
applications can say how many processors
they want they can also say which
processors they want they can say when
they want them and at what rate so for
example an application can say something
like I need three cores all at the same
time every 20 milliseconds that is
somewhat similar to Solaris the
scheduler hints though it's much more
explicit the scheduler hands in Solaris
are essentially a feature where you can
say something like I'm currently in a
critical section
please don't preempt me now and the
scheduler can still decide what it wants
to do but it kind of takes these hints
from the applications in order to make a
more informed decision so this is a bit
more explicit than than just that and a
bit more expressive the idea is that the
applications submit these processor
requirements quite infinite and
frequently or at the application start
in my system they're actually submitted
in in plain text so there's a little
snot
language is just more specification IDL
that allows you to specify those like a
manifest I do realize that applications
are going through different phases
especially in this more interactive
scenario some of them might be parallel
phases some of them might be sequential
thing it might be single threaded phases
and these phases might also have
different processor requirements so I
allow the specification of these
different phases in these processor
requirement specifications directly an
example would be if you have a parallel
if you have a music library application
it might do some parallel Audio
re-encoding then you essentially have
two phases one phase is the phase where
you're pretty much just running the
graphical user interface that might be a
single threaded phase and then you could
have a phase where you do the parallel
audio re-encoding and then you're
actually using multiple threads so you
can say in my processor requirement
specifications you can say I need one
core in phase a and I need three cores
in in phase 3 and phase B and then I
added a new system call that you can
call or to switch between these phases
so you can say I'm switching interface a
now switching into phase B now the
second concept is that the operating
system scheduler notifies the
applications of the current allocation
that it has allotted to it this is very
similar to work that has been done
before their scheduler activations and
there's also work that's been done in
the psyche and k40 to replicate
operating systems so I'm applying these
ideas in in my system so the operating
system can say something like you now
have these two cores every 10
milliseconds as other applications are
going through different phases or new
applications start up finally um since
we're in an interactive setting we want
to be very responsive to the yes
os says that a promise or a you'll
probably have them so when the operating
system tells you this then you have them
the operating system has the right to
change this at any time in my system it
will do that whenever an application
goes into a different phase or another
application comes up or an
identification exits so we have these
quite defined phase boundaries
essentially where the operating system
can change its mind and give you a new
allocation yes so it tells you you have
the course then you do have the course
yeah and that library application comes
won't at this time although schools the
operating system told you you have two
minutes you go
no I'll tell you it will tell you things
like you have it every 10 milliseconds
but it won't tell you you will have it
guaranteed for the next two minutes or
something like that so it can still
change it mine it's mine okay so since
we are in this interactive scenario we
need quite small overhead when we are
when we're scheduling we certainly can
take a very long time to change our mind
and this is a big problem because
computing these processor allocations
especially taking into account the whole
architecture different the topology of
the architecture is a quite complex
problem if it's essentially
heterogeneous scheduling on a
heterogeneous platform essentially you
have different latencies of
communicating between different cores
you have different cache affinities
different memory affinities so this
problem is very hard to solve and
actually if you have multiple
applications takes quite a while I will
have in in the evaluation section of the
taco will show you for one example
problem how long it took to to solve it
all so if we're doing things like gang
scheduling and we have to synchronize
this patch over several cores if we do
that every time slice and we're talking
about interactive time slices which are
typically on the order of one
millisecond to maybe tens of
milliseconds this might take - might not
scale with a lot of course so will slow
down the system so in order to tackle
that problem I break up the scheduling
problem into multiple time scales there
is the long term computation of
processor allocations and I expect this
to be on the order of of minutes so the
the allocations that I'm that I'm
providing are expected to stay
relatively stable for at least tens of
seconds or minutes then there's the
medium term the medium term allocation
phase changes that the applications can
go through with their system call and
that's on the order of say deci seconds
I would expect that it doesn't make much
sense to go through different resource
requirements much faster than that but
you want to have something where you
want to be fairly interactive with with
the users so you might still change very
quickly between different phases
and finally I have the short-term perk
or scheduling that's just a regular
scheduling that scheduler that runs in
the kernel and dusty the time slicing of
the applications nets on the order of
milliseconds okay so how is this all
implemented within the bare fish
operating system I realize all of this
by a combination of techniques at
different time granularities first of
all I have a centralized placement
controller that's the thing on the the
right hand side I should also say how
the schematic is built up the we have a
number the course essentially on the
x-axis here and there as many courses
there are course in the system and then
we have some components that run in
kernel space and some components that
run in user space so the placement
controller is the entity that's
responsible for doing the long term
scheduling so this will receive the
processor requirements specifications
from the different applications and then
go and produce schedules for different
combinations of phases that these
applications can can go through and it
makes sense to have this be centralized
because it's doing something that we can
take some time for computing and it also
needs to have global knowledge of what's
going on within the system in order to
do this the placement of different
applications onto the cores and then the
placement controller will download the
produced schedules to a network of
planners that there's one planner
running on each core these planners will
first of all just store that information
locally so that we don't have to do
communication when we're going through
different face changes with the
placement controller then we have the
system call that allows an application
to change a face and when that happens
the application will send one thread off
the application will send the system
call and communicate with the planner to
change with a local core planner to
change the face that planner then has an
ID for the face that it communicates
with those other planners that are
involved in scheduling this application
so it's just one message
being send one cash line essentially
with the ID off the new face being sent
to the other planners and the other
plans just have to look up that ID in
their database of different schedules
that they have for their local Corp and
can then activate that schedule and the
way they do that is essentially by
downloading a number of parameters to
the to the kernel level scheduler for
that particular face
I have evaluated how long it takes in
the worst case to do such a face change
if we change every single scheduling
parameter on in my case a 48 core
machine which was the biggest machine I
had available at the time when I did
this experiment I did the face change
over all of the cores and it took 37
microseconds so since we were speaking
on the order of deci seconds that a face
is lasting this is quite low overhead
for for doing this finally just as a
another number that I have is the the
context switch duration of the system is
4 microseconds which is the other number
that I have there and so doing this
scheduling at multiple time scales
allows me to do something that's that's
pretty cool something that I call face
lock scheduling the way I do this and
this is a technique that allows me to do
synchronize dispatch like gang
scheduling for example over a large
number of course without requiring
excessive communication between these
cores which would not scale very well if
I do this that very fine time
granularities and the way that I do this
is I use a deterministic scheduler
that's running in kernel space per core
in my case I used the our bed scheduler
which was presented by Scott brand in
2003 which is essentially a hard
real-time scheduler though I'm not using
it for its heart region properties I'm
using it for its proper property of
being deterministic and then I do a
clock synchronization over the core
local timers that drive each scheduler
on each core and then I can essentially
download a fixed schedule to all of
these deterministic schedulers and know
that these schedulers will dispatch so
threats at certain times without any
further need for communications so I do
not have to do this
communication over all the course that
will be involved in say gang scheduling
an application every time I want to
dispatch or remove the threats from the
particular course so I have a slide on
this to show you how this works and this
is an actual trace that I took off the
cpu-bound and the barrier application
running on the 16 core system in the
first case we're just doing regular
best-effort scheduling and this is also
to show you the benefits off of gang
scheduling the applications so the read
should probably explain the graph the
red bars are the barrier application and
the blue bars and the cpu-bound
application on the x-axis we have time
and on the y-axis we have the course in
the best effort case the barrier
application can only make progress in
these small time windows here when all
of the threads are executing together
and then you can gain scheduling in the
classical case you would have to
synchronize every time with every core
that's involved in the schedule yes
well so this is where gang scheduling
becomes harder this is why it takes a
long time to compute optimal gang
scheduling is essentially a bin packing
problem you have this other application
that would run in the system and it
might either be also gangster if it just
runs on one core there's no point in
gang scheduling it you now have to
figure out how do I fill this
2-dimensional matrix essentially of time
and cores optimally with the gang and
then this this one application so either
you leave a lot of holes in which case
your schedule is not very good or you
actually trying to do bin packing in and
try to fit everything in so it's it's
quite hard to compute an optimal gang
schedule and this is why scheduling in
in the in the long term this placement
controller is quite a it takes quite a
long a long time so this is why it's
important to break up the scheduling
problem and not do it you know ad hoc
essentially when when the workload
changes okay so in the face lock case
what I'm doing is I synchronize the core
local clocks once for that I need some
communication in order to synchronize
the clocks you can either use a readily
available protocol for that like NTP to
synchronize the clocks or you use a
reference clock in my system I use the
reference clock there's the the pit in
the system essentially that I can use to
synchronize the the core local AP clocks
and then you agree on a future start
time for the gangs for that you also
need a little bit of communication and
then you also agree on the on the gang
period and finally this this will just
schedule itself now each core has two
different as has agreed and has its
deterministic scheduler now you can go
ahead and and gang schedule and we only
might need to communicate again at some
point in the future when we need to
resync clocks i've actually evaluated
whether that is the case on x86 machines
and it turns out that it's it's not the
case it seems that the apec clocks are
all driven by the the same court so i
actually ran a long term experiment on
several machines where I started the
she synchronize the clocks and then saw
if there was drift over a day or two and
there was no measurable drift in in
these machines so it seems they just
have an offset and that's based on when
the core will actually start up but
they're all driven by probably driven by
the same clock okay so let's let's see
it's go back to the original example and
see if n2n scheduling actually benefits
us here so this is the same two
applications just not running on Linux
but running on on barish otherwise same
conditions the barrier implementation
slow and sorry that their application
slows down a little bit faster that's
due to the very implementation that I
had available in the open MP library
that I used within Baro fish obviously I
couldn't use the Intel open appeal ever
because it's closed source I wasn't able
to just port it to to bear fish we also
see a little bit more noise and that's
mainly due to the fact that bear fish
has more background activity there are
these monitors and the planners running
in the background where it's Linux is
typically well almost completely idle
when you when you don't do anything the
pitfall that is still there though
that's the important bit now if I
activate end-to-end scheduling and I
tell the operating system that I would
benefit from gang scheduling from the
barrier based application then it will
actually do so and we can see that the
the pitfall is now gone so the the Barry
application keeps its progress due to
gain scheduling we still see a little
bit of degradation due to the barrier
there's more time taken away from the
CPU bound application and so it slows
down faster that's what you would expect
because we have to take the time from
somewhere that we're now allotting to
the to the barrier application so we're
now much fairer in this workload mix to
the parallel up to the parallel barrier
application okay so so far I've only
talked about cores now multi-core is not
just about course there there are more
things like memory hierarchies different
cache interconnects cache sizes and
things like that and that's another main
outcome of
the bear fish work is the system
topology is very important so how far
away are different course on the memory
interconnect various implemented by
probing shared memory and I did this
measurement on on the 16 core AMD
Opteron system which you see the
schematic of here and I measured that it
takes about 75 cycles to do a memory
probe into a cache that's on a core
that's on the same die within the system
and then it takes about twice as much to
go to a core that's on a separate diet
within the system and so it depends very
much on the the applications performance
that's executing in these various
depends very much on where the app the
operating system is actually placing
those threads if it happens to place the
threads closely to each other than the
application will make more progress if
we place threads on on course that are
more further away from each other than
we will see less progress and other
multi core platforms look different
again so we have to be able to
accommodate all of them in order to make
good progress with the application we
can actually see this in the original
example so this is back on the Linux
system by the big error bars that we're
seeing in the barrier bound application
and you can even see this most
pronounced here in the two threads case
where we really only have these two
threads and the operating system is
making arbitrary decision decisions over
many runs that I did with this benchmark
where in some cases it happened to place
it on the right to court yeah and the
two courts that were close together and
we had very good progress and if it
happened to place these two threads on
two cores that were further away from
each other we essentially had half the
progress in the barrier application now
how do we incorporate that there is a
component inside bear fish that's called
the system knowledge base that's mainly
work done by another PhD student in the
group Adrienne Shiva
the system knowledge base is a component
that contains abstract knowledge about
the machine it knows for example what
the cache hierarchy is and what the
topology is between different cores and
it gains this information either we via
runtime runtime
measurements or weather through through
other information sources like a CPI for
example or cpuid so it's one of abstract
store of all this information and what I
did is I interface my scheduler with
this component so the system knowledge
base is now informed by my scheduler
about the resource allocations that the
scheduler is doing so it knows about
utilizations in the machine also I
should say that the system knowledge
base is a constraint solver so you can
curry it using constrain cost functions
and this allows us to now ask questions
like or ask queries like I need at least
three cores that share a cache
morebetter instead of just saying I want
these these cores and things like that
so I worked with with Adrian and and
Jana Geneva who's another PhD student in
the group to port a parallel column
store it's essentially a key value store
to to Barrow fish as a workload we ran a
real workload on on this application
it's all the European flight bookings
done by a company called amadeus which
is responsible for that so it's
essentially they're interested in high
query throughput to book flights and we
ran this column store together with a
background stressor application inside
the the system again on the 16 core AMD
system and we specified six constraints
for the application there were things
like cache and memory size as well as
sharing of different caches and numeral
locality in there we asked the system to
maximize CPU time and so by submitting
this this constraint query essentially
it took the the resource allocator
several seconds to produce a final
schedule so this is just to give you an
idea of how long it can take to do this
and it ended up deciding that it would
partition the machine between the column
store and the background stress or
application and on this graph down here
we can see the the result in terms of
query throughput the let's start with
the middle one which
the way to naively run the the true
applications by just starting the
background stressor and starting the the
columnstore in that case the contour
just goes ahead and tries to acquire all
the course in the in the system so
chairs its overlaps with the the
background stressful application and we
can see that the queries throughput is
just down there at a little more than
three thousand cruise per per second
there's also a way of just manually
placing these two applications trying to
partition them and then we get a
throughput of five thousand five hundred
queries per second and finally on Barrow
fish we are really close to the the
optimal way of placing these these two
applications the bear fish did the right
decision of deciding to partition the
machine between these two applications
finally I would like to say a little bit
more about my evaluation of facelock
scheduling it turned out that after
evaluating it is not maybe not yet
important on the x86 platform when I
just use synchronized scheduling
synchronizing every time slice
essentially overall the course there was
no real measurable slowdown
up until 48 course which was the biggest
machine that I had available to measure
this the reason for this is that both
timers the the core local epic timers
and inter processor interrupts which is
what I used in order to keep the
schedule synchronized overall the course
generate a trap on the receiving core
and taking a trap takes around an order
of magnitude longer on x86 then the act
of sending an IP I to a different core
so in this case it doesn't make any
sense to distribute receiving interrupts
which is essentially what I do by face
block scheduling now every core uses its
core local timer that generates the trap
however it does use less resources I
don't actually send the inter processor
interrupt to the other course now on the
x86 architecture that doesn't make much
of a difference because x86 has a
dedicated interrupt bus there's no
contention on that bus essentially for
multiple applications trying to send
interrupts but it does produce less
contentions on systems that don't have a
special interrupt bus for example the
Intel signature plow
computer which is another architecture
that I investigated bearish on use the
same mesh interconnect for both the
messages that are being sent between
cores and cache lines as well as the the
interrupt so if you have a lot of
traffic on that interconnect using these
interrupts interrupts could actually be
delayed on the on the interconnect and
then you don't have a schedule of that
as nicely synchronized so the insight
here is that one should always
investigate the cost for intercourse
sends and receives individually that was
something that we hadn't thought of
within bare fish thoroughly before we
always just thought there's one cost
associated with a message that's being
transmitted between two different cores
but we didn't look so much at receive
cost and insane costs obviously this is
only you know not not there there's more
future work with this one thing that I
haven't evaluated so far is how
expressive the operating system and
runtime API should be so far I've only
looked at these exact processor
requirements essentially if one were to
deploy this in a real system you would
probably want to do this expressiveness
analysis first in order not to give the
user this very very expressive interface
to essentially do everything like the
constraint solving and all that it's
nice because it allows you to experiment
a lot with with different things but you
probably want to figure out what what a
adequate interface for applications is
and one thing that I looked at a little
bit and I found interesting was number
of core speed-up functions that might be
provided by by applications in order to
give an operating system an indication
of how well these applications are
scaling with the workload that they're
running and they could do that based on
on estimates and there's actually
related work here there's a system
called Parker that has been presented I
think two years ago in hot par by the
University of California in San Diego
that allows you to do exactly these
estimates so you will give it a work and
openmp workload in this case and it will
figure out how well that work load will
scale and then
you could feed that information into the
schedule and the scheduler could
automatically figure out how many cores
to allocate for a particular allocation
over some other application that might
scale either better or or less well
another thing that I haven't looked at
so far is IO scheduling so for
everything was mainly done within memory
and I just looked at cores and caches so
there's still some work to be done on
devices and in interrupts for example
what if we have a device driver that
wants to be scheduled finally I also
find heterogeneous multi-core
architectures to be very important in
the in the future things like
programmable accelerators are becoming
quite pervasive like GPUs but also
network processors and the question is
what are the required abstractions here
is all the abstractions that I've
presented so far adequate for this or do
I need to integrate more abstractions
okay um I also like to say a little bit
about my bearish related achievements
today throughout my my work I was able
to author and co-author several papers
in top ranking conferences workshops and
also journals with bear fish I
participated in building a system from
scratch I'm the proud owner of the first
check into the code bait base which
dates back to October 2007 bear fish is
a very large system it contains around
150,000 lines of new code if you count
all the ported libraries and
applications then we amount to a total
of around half a million lines of code
it had more than 20,000 downloads of the
source code over the past year and
generated quite significant media
interest being a PhD student I was also
involved in supervising several
bachelor's and master's students the
most important ones were probably one
who implemented a virtual machine
monitor together with me for bare fish
and then the another one who implemented
the OpenMP runtime system that are then
used for my experiments on bare fish I
also had two bigger collaborations going
on one with with with Intel in order to
evaluate bearer fish on the single chip
cloud computer this is how I gained most
of
knowledge there they also provided
several other test systems for for the
group to do experiments on and then
there was a collaboration with the
Barcelona supercomputing Center they
were interested in running a parallel
runtime that was based on C++ and didn't
we didn't have a C++ compiler and
runtime system for forebear fish at the
time so I collaborated with them in
order to essentially port one to bear a
fish so to conclude multi-core opposes
fundamental problems to the design of
operating systems things like parallel
applications as well as the fast pace of
architectural changes commodity
operating systems need to be redesigned
or to cope with these problems I have
focused on process of scheduling where I
found that combining hardware knowledge
with resource negotiation and parallel
scheduling techniques can enable
parallel applications with an
interactive scenario also like to say
that some problems are only evident when
dealing with a real system for example
the problem or the insight that one
should investigate meshes sins and
receive separately and that the cost
difference is quite important on a
multi-core machine the very end I'd like
to say a little bit more about myself
and other projects that I have worked on
aside from barrel fish my main two areas
are essentially operating systems as
well as distributed and network systems
and other projects I've worked on are a
replication framework for the network
file system version for that I did us my
master's thesis this is essentially a
framework that allows you to specify
different replication strategies and
also rideable replication for the
network file system which we're not
around at the time I accomplished two
internships throughout my PhD both of
them at Microsoft Research the first one
was in Cambridge working with Rebecca I
sex and and Paul Barham where we applied
constraint solving techniques in order
to aid solving the problem of scheduling
applications on heterogeneous clusters
in our case it was dry Lync applications
that we wanted to run on heterogeneous
clusters I'd ended another internship
together with Oh far earlier on working
on the Fae cluster tracing system that
got
ssp last year in 2011 phase the system
that allows you to trace through all the
layers of the software stack on a number
of machines all the way down to thee to
the operating system kernel in doing so
safely as well as in a dynamic fashion
finally just before starting my work on
bare fish while I was already on the
ph.d program I did study off timer usage
with it within operating systems that
got published in your asses in 2008
where I found that many programmers
actually set their timeouts to quite
arbitrary values without looking at what
the underlying reason for setting the
timer was in the first place and what
the timer is being used for and I
provide several methodologies out of
that problem in in the paper
finally my preferred approach to to
especially doing systems research is to
find a sufficiently challenging problem
understand the whole space and build a
real system thank you questions yes yeah
so it's based on the Eclipse Prolog
interpreter so not to be confused with
the Eclipse integrated development
environment so this it is prologue with
added constrains essentially it's a we
reported this to to Barrow fish it's a
currently single threaded centralized
component
well they're different solvers inside
the system that you can use to to solve
the constraints you can can specify
constraints like this value should be
smaller than then another value in the
system or smaller than a constant or
bigger than the constraints like like
that I know if you want to know
yes your service reason and all the
applications provided you with
so in the experiments I do that the the
system itself does not rely on getting
information from every single
application it's more of a thing where
application can support the scheduling
by by providing information so if you
don't provide information then the
operating system scheduler will
essentially resort to best effort
scheduling for for you
so I have not evaluated that thoroughly
but I definitely the background services
that are running that are not providing
any information did not impact the the
system that very much I mean you could
see there was there was more noise in
the system so there there is some impact
but for the sake of running the
experiments it was still okay
increasing solicitous
you're the experiment you showed us
yeah that is true and I'm sorry I don't
have one with me for for the risk about
the responsiveness the the the kind of
the idea just I wanted to get across was
that we need to be able to make
scheduling decisions quite quickly and
if we do this you know the constraint
solving for example every time the
workload changes then that is clearly
not responsive enough so and for the
purpose of this talk it's more about
that yes so that's one actually oh good
good and bad points about the constraint
solving so yeah you don't know upfront
how long it might take to do the solving
however the good thing about constraint
solving is that it kind of searches the
problem space so it allows you or it
will give you a quite good solution very
quickly and then it can go on and solve
further and that's actually something
that I'm planning to use in this system
as well that the placement controller
essentially will go off to it's
constraint solving it will start
downloading some scheduling information
as it gets the scheduled and so we can
start using these and then go on and
solve some more the interesting thing
about that is also that it seems to be
trading off quite nicely with different
lengths of phases that you have if you
have an application that has quite short
phases then it will go through these
phases quite quickly and in that case we
might not have the optimal schedule
available at that point however since
the phases are quite short it might not
matter too much to have the optimal
phase allocation available and if you
have an application that has quite long
phases then it also allows you to it
allows you more solving time essentially
to come up with with a better schedule
for these longer phase and then the
decisions will also have more impact
any more questions
today this whole phrases general
festival schedule
by authoring that we should integrate
bank scheduling in this kind of
framework
each
should I put a gang schedule or you
shouldn't put just against it I'm just
saying that the problem will definitely
become much more much harder because of
the parallel applications that you
potentially want to run inside your
system and they do have scheduling
requirements like like gang scheduling
like synchronized dispatch
um that depends a little bit if you it
depends on how much you you really care
about the the best effort applications I
mean if you if you don't care so much
about the progress of these applications
and since there are just best effort
anyway that there probably doesn't
probably matter so much for them you can
just try and that depends on well right
right now I mean that depends on what
the user wants to do with it right if it
if it's if he's actually like actually
using word at this point in time then
it's important that you give them the
service at that point in time so but
it's still a best-effort application I'm
not sure what you're trying to get it
right now Thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>