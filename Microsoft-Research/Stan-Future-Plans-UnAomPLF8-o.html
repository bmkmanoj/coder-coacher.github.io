<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stan Future Plans | Coder Coacher - Coaching Coders</title><meta content="Stan Future Plans - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Stan Future Plans</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UnAomPLF8-o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so Stan 3 and beyond so the first thing
that we're going to do we're currently
at release 24 and we're not done with
the two number two releases yet they'll
probably be a couple more releases
before Stan three comes out but this
will be the third time we did a major
refactoring with Stan to to try to
cannot change any functionality but
actually clean up the code underneath
which is really important if you want to
maintain a big project going forward to
keep things to take things out as much
as you put things in often people think
the third time is the charm and we're
hoping that's true for us that we'll get
it right this time but it'll probably
keep evolving one of our big goals here
is removing duplicated code in the
interfaces right now we have an
underlying C++ API and then a use of
that from Python from our from other
places and we want to remove all the
duplicated code we can they are just to
make things more robust to changes and
keep everything in sync and we want to
minimize the dependencies we have right
now right now we have this very fat
dependency between our interfaces to
kind of know everything about the
internals of everything and we're we're
tightening that up with some more
modularity so we also want to be able to
make a more modular user-facing C++ API
to make it easier for people to do
things like use our automatic
differentiation use our density
libraries and to make it easier to
define interface api's on top of the CP
cpu the design and some coding for this
is is underway right now but it'll
probably be six or nine months I think
before Stan 3 version rolls out sure
whatever you're parsing the program's
into like the abstract syntax tree and
the parser
I'm not sure what you mean like right
now we're parsing into an abstract
syntax tree that's basically boost
variant based that's a pretty
lightweight thing that comes out of the
boost spirit parser so it's reusable but
it's kind of a pain with the callback
methods it's all nice and type safe but
it's like really insanely clever
template programming and lazy evaluation
inside of it so it's using all sorts of
crazy boost stuff that's one of the
things that causes compilation times to
be so slow is the deep template
metaprogramming inside that comes out of
boost to deal with the parser so you
already have two languages I mean so do
radio device tues / but you don't use
speak no I had never even heard of swig
until today so I will go check it out
because it sounds sounds promising mmhmm
yeah so that that sounds very promising
I didn't know anything about it which is
one of the reason to come to workshops
like this stuff I'm more than 10 years
old yeah yeah yeah no there's a lot that
I don't know more I know there are more
things i don't know that i dunno yeah
I've heard that Swedish pretty hard oh
you can make it work
we need to spend more hikers might be
the key years or maybe good to reuse
some of what should we let's be doing a
face maps that took probably yes
Staveley memory leaves and everything
yeah take some of that yeah we're big
believers in borrowing expertise if not
code will borrow any code that we can
manage to fit in to as well so yeah just
getting a hello world that works is
usually half the battle and getting a
compile to the C++ is just the nightmare
compared to Java which I did for the ten
previous years the next thing that's
actually going to roll out because I
hope to finish the final coating for it
this weekend is the differential
equation solver so a lot of models
climate models pharmacokinetic models of
drug transport anything where your
underlying model involves a differential
equation that has parameters like a rate
equation or something and you take noisy
measurements of it typically people will
build an Eau de solver to solve systems
of differential equations into the model
and you can think of that as just doing
the same sort of work as the linear
predictions in a linear model would be
and we've almost got this this done the
big difficulty here was we want to we
had a prototype that we did in a big
project with Novartis that automatically
differentiated the integrator so you
solve these OU des by integrating them
by basically simulating in time you can
automatically differentiate the
integrator but it's very inefficient you
can't control the errors on the
derivatives so what we need is we need
to be able to take derivatives of the
solutions to the differential equations
with respect to the parameters the right
way to do that is to build a big coupled
system of differential equations where
you differentiate the system itself and
Michael Betancourt worked out all the
math for us we've got it all working
we've
a prototype working and we pretty much
got it done so this is this is going to
come up come up soon and we're working
with Frederick blonde Amy resting poon
and Sebastian Weber from novartis and
UTC in Paris on this yeah that's it's
that's a separate issue yeah but we're
not we're not that's not what were you
so you're thinking of like Jeremy's
thing that he's working on yeah this is
slightly different we're actually doing
traditional solutions of differential
equations for which we have noisy
measurements of outcomes of those
differential equations so we want to be
able to fit the parameters we want to
build a statistical model take noisy
measurements and fit the parameters of
the differential equation system so
usually the kind of difficuit here
aren't particularly difficult so it's
not like we're dealing with big hairy
things and this is only going to be for
non stiff equations there's one more
step of needing jacobians and another
couple layer in order to get stiffed if
eq's but we're going to do that as well
because there's uses for it there's a
system that already does this PK bugs
does this already except they don't have
a gradient based system so it's a little
easier for them I don't actually know
that much about PI MC so I can't really
answer that question I'm afraid I mean
maybe somebody else knows more about
both systems from it
having to write down yeah so it's a bit
easier to get your hands on you have to
do that stuff to get your samples from a
model and also do this Hamiltonian
multicolor or not something stuff but so
standard I think we're more
sophisticated in the way
you can bounce some parameters they
provide a patient is very young
yeah and we're specifically aiming at
people who want to write in a little
embedded language like most of our users
or our users or don't know python but
they're happy writing models and it kind
of are like syntax but that's it that's
a different issue we're certainly help
happy to help people build better
adaptation and stuff for their hmc the
next thing we're going to do is hire or
dorado diff this should also be done
this is code complete being tested the
only holdup we have is we can't get
Windows linker to link this thing
without blowing out memory under any of
the windows compilers we have so we may
wind up rolling out higher order Auto
diff for every platform but windows
because we can't make it actually work
and C++ on Windows maybe some people
here could like help us do that because
you guys probably know how to make C++
work on Windows better and we may not
we're probably not going to do all of
our cumulative distribution functions
these things are real the CDF's are a
real monster to differentiate with
respect to their parameters they're easy
to differentiate with respect to the
route come because you just get the
density back but differentiating like
the beta distribution with respect to
the alpha and beta parameters much
trickier and doing we've got the first
order stuff of that worked out all the
iterative algorithms but the higher
order stuff not yet as soon as we get
the higher order Auto diff and that's
all done and we may just punt on the
windows testing if we can't get it to
work because it validates on Linux and
Mac already as soon as that works we've
also got the riemann manifold hmc
already coded and ready to go and the
important part about this is it supports
posterior inference where you have
position-dependent curvature so by
position-dependent curvature if you take
a multivariate normal no matter where
you're at in the posterior the
covariance matrix is the same if instead
I do something like a hierarchical model
where there's a hierarchical variance
parameter and the value of that
hierarchical variance parameter controls
the scale of say the regression
coefficients then what happens is your
covariance matrix changes depending on
where you're at in the posterior right
the kind of adaptation we're doing and
the basic Euclidean hmc is
assuming that the posterior curvature is
uniform everywhere if that's not true
you need to really step the step side
really scale the step sizes down in
order to get into the tails of the
distribution ramon manifold hmc solves
all of this at a high computational cost
we need to compute up to third order
derivatives each iteration and we need
to do things like an eigen decomposition
to smooth out the curvature to get good
estimates of it this is all done you can
read Michael Betancourt's archive papers
it's all implemented he has a really
sweet softabs metric so this isn't the
same as the corolla me and Calder head
Fisher information matrix which is based
on an expectation we're using the soft
tabs metric that Michael invented which
takes the hessian d come I gandi
composes it sort of compresses the scale
makes everything positive definite by
making all the eigenvalues positive puts
everything back together and then it
will work in full generality it's just
going to be about a thousand times
slower per iteration but this gets it
there's some problems that we simply
can't fit without this method so it may
be slow per iteration but there are
things that this will be able to fit
that we currently can't fit and no one
can fit so this is very exciting but not
for all the usual big data reasons we're
also there's another Michael's writing
papers Michael Betancourt's writing
papers faster than we can implement this
stuff and Stan he came up with a very
nice physics principle thermodynamic
sampler so if you're used to how the
current things like simulated annealing
or tempering work they're sort of
forcing the physical system from the
outside this is physically motivated by
applying something like a heat bath and
letting the physics sort itself out so
this is going to be good for the same
sorts of reasons as everything else the
same reasons as tempering and annealing
is so it'll be good with multimodal
posteriors let me just I see that I
should whip through this I have lots of
questions but okay well we can stop now
if you want I mean there's like five
more slides but we can stop at ear
because we're over time no I wouldn't
say any of them are more important than
any of the other ones
okay I'll just hit the titles we're
looking through ensemble mcmc methods
we're going to roll out marginal maximum
likelihood so we can do point estimates
of hierarchical models by marginalizing
out the low level parameters we're going
to with we have higher order derivatives
we're going to be able to do mle and
marginal maximum likelihood error
estimates which all the statisticians
want and then the two big things we're
going to do is ALP here is working on
variational bays and we're going to
build black box variational inference
this is all under development I'm not a
you know it's being prototyped right now
I'm not sure that we have an ETA on
something like that because there's a
lot of research to do with this but the
nice part is we'll be able to do batch
and streaming so this is our road to
doing real Bayesian large scale models
and we're also working on EP versions
with a different set of collaborators
which is going to allow data
parallelization I think that a lot more
easy way than the stochastic methods are
at least from what we know that was the
last slide yes because we don't do any
discrete sampling whatsoever there we
don't really know how to do it we can't
take our language and easily find the
select with a directed graphic modeling
language like bugs or something it's
very easy to find a markov blanket of
everything since we have this imperative
way of specifying the log probability
there's no easy way to pull out the
conditional probabilities the best
thought we have now is we have the user
separately specified conditional
probabilities which is like really ugly
or we create a limited representation
language more like bugs or Jags where we
can do all the computations that we need
if anybody has better ideas how to do
that the only reason it's not on our
schedule is we don't have a satisfying
way to approach it and most of the
problems we deal with marginalizing out
the discrete parameters is better than
sampling them anyway although it's a bit
of pain for users
no parting wisdom face your forth and do
good things</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>