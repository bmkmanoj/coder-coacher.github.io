<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Session three - Artificial Intelligence and Machine Learning in Cambridge 2016 | Coder Coacher - Coaching Coders</title><meta content="Session three - Artificial Intelligence and Machine Learning in Cambridge 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Session three - Artificial Intelligence and Machine Learning in Cambridge 2016</b></h2><h5 class="post__date">2016-07-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jRUJL-JcVfk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'll take that is he is ok so the title
is actually compositional interest in
probabilistic programming and I changed
it at the last minute and let's start
with say telling you with probabilistic
programming is about so the idea is to
make the task of probabilistic modeling
easy or easier because as you know a one
particularly difficult thing about
probabilistic model in case that you
have to do inference and inference is
computationally hard therefore we have
developed all sorts of clever inference
algorithms that help with it but the
problem is that those things are
difficult to implement they're really
painful to the debug and we really don't
want to have to rewrite them every time
you write a new model so the idea is
stuff to have a formal language
specifying your probabilistic models
such that based on the description of
this language efficient code for
inference can be automatically generated
and the reason we call it probabilistic
programming is that we choose the formal
language to be a programming language
extended with probabilistic effects and
in particular a goal is to separate
inference from model development because
most users will not actually even know
or care how to do those things so to
make a discussion more concrete let me
introduce a probabilistic programming
system culturing that we have been
developing for a while and essentially
it extends julia with to FX so one is
assume for drawing random variables from
a distribution so this just says sample
value from D put it in a current scope
with the name x + 2 14 conditioning so
we have this statement factor that's the
factor like in a factor graph and then
there is this sugar observe which
translates to a factor of the
appropriate PDF so here is an example of
a very simple linear regression
model implemented in hearing so sue
sample barometers from the prior and
then you observe a bunch of data points
any questions about this okay so now the
question is so obviously this is very
nice for specifying models is very easy
to work with but how do we actually do
inference and as I said we'd like to
hide it from most users but eventually
someone has to write an inference
algorithm and as we know there is no one
single best algorithm that we want to
apply in all cases so why don't we
design the system in such a way that is
actually easy to come up with new
inference algorithms or complications of
existing class so let's make the task
easy not only for the user but also for
a developer of the inference bucket so
what I'd like to talk about today is
compositionality of inference algorithms
so we know that there are certain
aspects in which inference algorithms
can be combined together and ideally
we'd like our implementation to reflect
that so that if things are compositional
we can literally just put them together
and there are several different things
that can mean so for example we may want
to apply different algorithms to
different parts of the model say I have
something where I wanted to give some
liqueur discrete variables and hmc on
continuous ones and that's something a
lot of people work on but this is not
what we're going to talk about today I'd
like to focus on the other two points
here which is one is that we may want to
apply while algorithm to the output of
another for example i do my variational
inference and i use it as a proposal in
importance sampling and another one
that's a bit less intuitive is composing
program interpretations that we'll get
to later so in order to be able to apply
one algorithm to output of another we
need to have some sort of standardized
representation so that the input and the
output are in the same format for
example if I write my son
they're in such a way that is gives just
bugs just a set of samples then I cannot
really do anything else so there are
different things you can do here for
example you could compile to a factor
graph and then do operations on this
factor graphs but then what to do if my
model doesn't correspond to a factor
graph so we have a very nice
representation for general probability
distributions and does a probabilistic
program itself which leads to the
question of can we write inference
algorithms as program transformations
yes clearly if we can then they
trivially compose and what people
usually mean when they say inferences
program transformation is something that
I'll show you so say we have a situation
like this where we draw a white from a
better distribution and observe a value
true from a Bernoulli with that weight
so those of you who know about
exponential families now that this is
equivalent to this where we sample from
a modified better distribution with
different parameters and then we have a
factor that corresponds to a model
evidence here okay so those two
representations they describe exactly
the same distribution but this one is
obviously better because if you view it
as an important sampler this fing
samples from the prior and the way it is
it is likely hood while this examples
from the posterior and the whitest model
evidence so this is the best important
sampler we can get and this certainly is
an example of inferences program
transformation this is the kind of thing
that people usually think of for example
in compiler optimizations you do some
sort of local modification to the
program where you can show that two
things are equivalent therefore you can
rewrite an expression and that's
certainly true but then there is another
aspect I think we can take this notion
of interest this program transformation
even further than that for example
suppose I have the following not very
interesting state space model and I want
to run smc on that the naive thing to do
will be to take this program a player
and see
smc algorithm that spits out a bunch of
samples but I can actually implement
smcs model transformation so my smc
transforms a program to the following
quad so you can actually read from it
that this represents this is free and
implementation of an smc algorithm we
have a system of particles and for each
particle will perform actions separately
accumulating waits for them where
observes happen and after each observe
we're example the system of particles so
this is straightforward implementation
of smc but you notice that this is still
a probabilistic program because we still
have those assume statements here so you
could say that this is a program in
Turing not a program in Julia and
therefore we can still do stuff with it
afterwards if I want to be really
comprehensive i can add the following
clients and introduce this artificial
conditioning by model evidence or the
pseudo marginal likelihood at the end
and if i do that and i select one
particle at random and the end according
to weights actually this program is
completely equivalent to this one in
terms of denoting the same probability
distribution ok i'm going to let that
sink in any questions about that yes do
we have to assume the notebook particles
is infinite no this segment is true for
any number of particles even one so for
one particle this doesn't do anything is
exactly the same program but for any
number of particles those programs are
still equivalent and if we write an
inference transformation like this we
can actually compose them so this is a
one that I really like there is this
algorithm called particle independent
metropolis Hastings which uses smc as a
proposal distribution for a Markov chain
and actually PIH can be implemented
exactly as a composition of smc with
metro police casings and samples from
i er so this actually works so that's
one aspect of how can we can make
inference compositional by doing it as a
program to program transformation but
there is another one that I'd like to
talk about and that's composing
interpretations of a probabilistic
program because ultimately I have a
program that has some probabilistic
statements in it so say I have a Turing
program what I will want to do
ultimately is to translate it to some
Julia program that I will execute good
and for difference inference algorithms
I will need different interpretations of
probabilistic effects ensuring and those
interpretations are different but they
share common patterns and it turns out
that we can actually design an
implementation that reflects this thing
so there is this very nice tool called
mullah transformers that I love to talk
to you about but I'm afraid that's a
story for another time so let me just
briefly list some of different
interpretations that we might expect so
that you have an idea of the problem
that we're trying to solve so a very
simple algorithm I might want to
implement its importance sampler so I
sample from the prior and I keep the
likelihood that my weight and in order
to implement that I just rewrite those
probabilistic effects in the following
way so for assume I just draw evaluate
random so call my random number
generator to get a value from the
distribution and for a factor I
accumulate way and then my probabilistic
program becomes a funk which is a
function with no argument and every time
I call it I get a fresh sample with a
weight I accumulate the weights and
that's a representation of my posterior
so here the funk that accumulates a
weight if I want to do some particle
methods like smc that we've seen about
the do we do just seen then assume is
still the same we just drove a random
number but for factor we accumulate
weight and then we have to resample so
in order to implement that I will
represent my spray
masako routine which is like a function
but it stops at several times before
it's finished so it goes for a while and
after each statement it actually stops
and it returns the control to the parent
that can perform resampling and then
insert started so you can see that there
is this pattern that we repeat those two
things but then this one is different
again if I want to do random DB which is
essentially a single signature police
case things where I update one variable
at a time then my inference algorithm so
the name a probabilistic program becomes
a function of a dictionary and this
dictionary stores the values of my
random variables from the previous run
so for assume I want to look up the
value if it's there then I put it in the
scope otherwise I draw a fresh one and
then I put it in back in the database
another one say I want to do Hamiltonian
Monte Carlo or anything else that's
gradient-based then what I want to do is
interpret my probabilistic program is a
function from a dictionary of values of
all my parameters to a density so for
each assume i will compute a density of
the parameter because i assume that i
already have a proposition proposal for
all the values and i just want to
compute their density so both assume and
factor will just update my density and
then I run my favorite of the automatic
differentiation tool and get a gradient
so you can see that this is this is like
what we've seen with weight before but
now we also have another component here
and it turns out that we can actually
modularize it this interpret different
interpretations very nicely using mana
transformers so I wrote I drew a map of
things that I have identified and
implemented so far don't have time to go
for all of it but basically the ellipses
are a different
as well different molar transformers
which will stand for different parts of
interpretation of a program and the
rectangles are inference algorithms and
what did you say that for example we
have this weighted which corresponds to
accumulating a weight as we've seen in
the important center there's this
empirical which is viewing essentially
maintaining a collection of samples
rather than one sample and finally there
is particle which says that we have a
computation that stops at certain point
and if we put those two things together
we can get very easy and implementation
of sequential Monte Carlo and actually
we can also get an implementation that
puts those two things together so we
compose all those interpretations and at
the end we get a program to program
transformation so the ultimate
composition okay and there's some other
stuff here as well there's not that much
of it yet this is just things that I
have implemented so far but I think
there can be potentially much more done
here so finally if you know haskell you
can give it a try we have a library that
composes those things using mono
transformers if you don't know how scale
you'll have to wait until we put those
things in during which hopefully will
happen sometime soon okay thanks
again excellent timing we have time for
questions so in by in the diagram that
you had before the operations that you
define could apply to any problems a
program in the language right yes so the
object d the distribution object they
could be itself any problems a
programming language you mean dizzy
right now so this D is restricted to be
a primitive distribution so something
that you can not only sample from but
also compute the density for so those
are also things like normal gamma and so
on but but sort of but but this first
thing I said still holds in terms of
like you can you can do yeah p imhi as a
transform up on any closely criminal
absolutely a program yeah so what if you
want to do more standard compiler
transformations like like rewriting
loops or joisting things out of loops we
can also fit into this framework the
world the metropolitan regions so for
that you need essentially a compiler you
can write your compiler with mother
transformers or you may choose to do it
any other way but it wouldn't fit into
this particular framework you would
probably need a separate pass over your
code to do this transformation first
because you showed this example of
composable inference procedures we now
have potentially a huge space of
possibility procedures
so would it make sense to take a look at
or specific policy program and I really
care about to you know search all the
space for inference procedures which
really work quite well when asked it's
also very roughly run overnight and
morning has been say no here's this
procedure should run and will equate a
new model is that something that's
potentially achievable or potentially
yes at the moment the search space is
not that big so but as we get more and
more building blocks then that is
definitely some people have to worry
about but just going into the future a
little bit I think that once you get to
this point you can write your code that
tries different interests algorithms
this probabilistic program itself and
therefore you should be able to all
right some sort of reflexive algorithms
okay let's not fantasize too much but
yes we will search the space how how do
you find debugging this code of this
transformation in practice is that not
hard to century step through those
different transformations to sort of
figure out what's going on on the
contrary is very easy because you have
different parts that each work to get
each work separately so you can unit
test them if you would rather than
having a big transformation where you're
not sure how things interact together so
if you if you arrange it well as smaller
transformers then you don't have any
unexpected interactions between your
effects that you might otherwise get
models to keep the stage and avoid the
shared memory changes but also the
reasoning behind that yes in particular
modifying a state is an effect that can
work in weird ways with probabilistic
assignment for example yeah an
alternative to designing probabilistic
program is to provide code in an
existing language
samples from the distribution which uses
all the type checking error checking of
the existing language and julia has
probably the best reflection of anything
except possibly f-sharp did you consider
that option so well so you've you've
used Julia's powerful macro capability
to build yet another dsl on top of yet
another language and somehow it feels to
me like there must be some way to write
code in raw Julia so choosing a type tag
is probabilistic and then interpret that
instead yes so this is a very shallow
embedding of a DSL in a sense that you
can actually write any Julia called
aside during and you could probably
write some sort of type system extension
that would allow you to analyze the
Turing code before reexpansion that we
haven't tried that we haven't had any
major problems with debugging so far so
we really think that was necessary at
this point ok can I ask a final question
when will it be available soon so ok
right let's thank Adam again so we
continue on the seam of probabilistic
programming with ND Gordon for Microsoft
Research talking about fabiola
regression formulas as prognostic
programs ah thanks Sebastian
okay so this is indeed continuing the
theme of probably sleep programming I
thought this is partly joint with Adam
who is an intern with us here last
summer and also with colleagues at well
various other distinguished institutions
so what's this about and it's about
learning from the our community they've
got some great rotations for linear
regression that have been completely
ignored in the problems of programming
community and this talk is about
understanding these notations and
putting them into a probabilistic
programming language so the first part
of the talk i'm going to talk about a
sort of formal calculus of linear
regression inspired by our and then in
the second part i'll speak briefly about
how we can embed that within our tabular
language which is where we get fiber
from and also within Stan so let me see
so let me start by just talking about a
little bit about how to represent a
linear regression sort of classic
univariate linear regression inside a
probabilistic programming language and
this isn't this the syntax in values f
sharp style syntax a language called fun
that we've used for some years as a
notation for thinking about the
semantics of provo stick programming so
here I'm doing a standard linear
regression where I've got a intercept
alpha I've got a slope beta and so any
actual point that we're modeling it you
know is it's modeled as sort of x times
beta plus alpha to get to the line and
then adding a bit of noise and if you
write that down in your favorite
probability programming language it
looks a bit like this where we define
alpha beta and I'm the position PI of
the noise by giving them uninformative
priors and then you have an array here
that computes each of the wise given the
exes so I assume have got an array of X
as input and then this syntax here which
is an airship style array comprehension
is producing the output so for every
index said less than the number that the
height of the array which is committed
in my example it's like a set of
students so hence this is the size of
the array for every said we calculate an
alpha plus the corresponding x times the
slope and add in some Gaussian noise and
when I first got into policy program I
thought this is really cool this is
all you two do to write down a model
like that and then I realized that are
as we're doing something even more
compact and I went wow that's all if you
go and use LM or L Elmer in in our
you'll find if you want to do this kind
of modeling all you need to do is say
why Turtles 1 plus X i was a bit gutted
you know it's pretty brief isn't it and
then even worse discovered the the one
plus which is that the one part is
defining the it's saying that there's a
there's an intercept that's optional
it's assumed if you sit if you don't say
otherwise that it's always going to be
there so they've got it down to three
symbols that I thought you know I think
we ought to be able to learn from from
those guys to get some nicer notations
because after all we often want to do
linear regressions in probably
programming and also the new regressions
are often components of larger
probabilistic models so let's let's see
if we can understand that rotation so so
that's that our goal so you want to
embrace and extend our to get better
languages so now what I'm going to do is
go through our notation the full details
of this are in a pop-up paper that was
published this January so that's
available on the web so you can download
it and we also have an implementation
that we've put up on github it's part of
the fabulous system but the first part
i'm going to talk about the calculus so
we have a grammar so overall are so what
I'm trying to do here is define a core
calculus that corresponds to our syntax
for regressions it is not precisely the
same as ours syntax for regressions and
in the paper I describe how we can go
from their syntax to this language this
is like a sort of core language it's a
little bit more verbose than you would
write in our they've got some nice
syntactic tricks and I explained them in
the paper but I'm not going to go
through all the details of that here
okay so so basically the models look
like that so it's projecting why a
regression is one of these formulas so
you can have some noise you can add
together to regressions you can group a
regression by some discrete variable you
can hide regression variables and then
this guy here is maybe the core of it a
predictor where you the regression has
some sort of
a predictor either which is typically a
scalar like we've seen one for the
Interceptor it might be X you know for a
very eight and it has a coefficient
alpha and then alpha itself has a model
so so we actually go beyond what you can
do in our so we want to cover the case
of hierarchical linear models as very
nicely explained in gelman and Hills
book which are highly recommended if you
not read it about hierarchical modeling
and and there the idea is that a
regression you can regressions that the
first order that predict actual data and
then you can regressions a second order
that predict the coefficients that are
used at the first order and so on so you
have a sort of hierarchy of regressions
and that's what this syntax that allows
us where they are and there is the model
of this this variable and it itself
conforms to this grammar so you can have
you know some levels of nesting to be
honest the common case is either one or
two levels of nesting okay so let's let
the grammar I'm going to go and talk I'm
going to talk you through it step by
step I'm going to use this data set as
an example and this is a data set
inspired by Galen Hills book and it's
looking at school test scores and the
ideas you've got a collection of
different schools ABC and D and you've
got a collection of students each row
here is a student and the idea is they
sit a test at the start of the year and
then sit another test at the end of the
year and your and I guess the hypothesis
is that the school is going to add some
value so the school the kids are going
to be better after the year than before
and we're trying to come up with a model
that can explain this and then maybe let
us ask questions whether one school is
better than another or maybe predict how
well a student would you do at one
school and so forth so in the data the X
is the pretest this is the pre year
score the the Y is the the post test
score and our post sorry post year score
and then this is the the a B C or D here
refers to the school and if you pull the
data it looks a bit like this where you
know a particular point is saying this
this child went to the oh hang on went
to I think this is
this is the look this is probably d this
this this sort of what is that yellow it
is d and this child you know started at
say 48 and has come out at 53 okay so
I'm going to come up with a number of
models that use more and more of the
data and for the the kids so the first
one is really totally trivial but just
to show you that we build up piece by
piece you see we just have a single
intercept so this isn't going to fit the
data very well but it's a starting point
so we would write this so this is a
little bit for boss but it's
illustrating the course and regression
the core recursive coefficient syntax
and the calculus we say why regreses
from a projector one so that's going to
give this flat line x the parameter
alpha which is you know which sits on on
the y-axis somewhere and alpha itself
has a regression as a model in this case
it's just going to be a noise term so
this this uninformative model and if we
translate that to our extra up like
syntax you get this that it's going to
define a single parameter okay so they
offer they have introduced a single
parameter with this as its prior and
then we're predicting y so we have this
array comprehension over all the
students that for every student gives
the same prediction okay lying on the
line now this is a this is a very common
pattern so we're just going to allow the
this uninformative prior to be omitted
so if we say nothing we just say V with
an alpha we're going to get that and in
fact it's very common that you want of a
bunch of these and in are you just
allowed to not mention it but you know
you can't give a name to the coefficient
so we want to allow that brevity so you
can just write V on its own and you get
this so this syntax new alpha just hides
the the parameter it makes up a sort of
default name for it okay and indeed so
that would mean that we could just write
something as compact as that to get a
single intercept the next thing is noise
of course you know a key ingredient of
almost every model and we just use this
query here symbol for that which expands
out into something else that I won't
elaborate but but when you go down to F
sharp it looks like this that we we have
some sort of prior on the the amount of
noise
and then the noise term for every why
just as a zero-mean a draw from a
zero-mean gaussian with the the
precision given by that parameter okay
and so now putting those together we got
a plus operator on regressions that just
adds together the outputs of the the two
regressions and so you're getting this
sort of picture and if we translate that
into into our underlying policy language
it looks like this where where we we're
accumulating together the two parameters
that are declared by the left part and
the right part of this sum and then the
sum also gets injected into the
production of the actual outputs from
the model so we're adding together the
the intercept together with the noise
okay and so you see a general pattern
that what does a regression mean well it
means two things we define some
parameters first of all and then we also
produce some sort of output and and and
in this case the overall regression
defines these two parameters and
produces the output in that way okay and
I'll finally get back to the regression
we started with we can write that as y
totals 1 alpha plus X beta plus query
and we're going to get this sort of
effect we've got the we've got a slope
beta and intercept alpha and we've got
some noise added and assembling together
the semantics of these three components
you get this thing so each of them
separately declares a parameter so you
get the alpha from this part the beta
from this part the PI from that part and
then down here in this in the output
part of the comprehension we add
together the the alpha that comes from
this component the x times beta that
comes from that component and then the
Gaussian noise that comes from that
component so you see it nicely assembles
together and that's the whole point of
defining the language we've got a we got
a grammar and then the semantics of it
can be defined and compositionally and
and here we get back to I wanted that we
have said we can always drop these
alphas and we adopt a convention the
plus query is sort of always present
in in regressions you always add one bit
of noise to every query if you haven't
done otherwise and so why Turtles 1 plus
X will give you this so we're sort of
back to where we wanted to be but we can
go further we're in that data set you
remember that we know which schools each
student belongs to when we suspect that
the schools have got different
performance so I'd like to be able to
measure that so one way is to say well
there's going to be a separate intercept
purse / school so we've got this new
operator parís that has the effect of
modifying regression so we got a
regression here that would give a single
line and by saying parís what we do is
introduce Wii Remote we we duplicate the
parameters on the left hand side one for
each school so there was an array of
reels for sorry there was a single
parameter alpha on the left and so it
becomes an array of parameters chosen by
whichever school that the student
belongs to so if we translate that out
you get this this model where the the
parameter now is is defined by
comprehension previously this was just a
scalar but now it's an array so the
parrises had the effect of modifying the
parameter of the underlying model by
making an array of them and then we also
have to modify the the output of each
each model so that instead of there
being a single author there's one for
every school so we need to cheat me to
say sfz so that's giving us back that
which of the four schools corresponding
to say then we use the the coefficient
belonging to it you get this model or
and I've not shown the noise finally
let's put let's just refresh our memory
about the the data we'd like to model
the the the the intercept by data that
we have at the school level okay so
software I've only used the data at the
student level but we've got average
income of parents at the school level
which we suspect me of something to do
with performance of the schools and so
the way we the way we do that is we have
a hierarchical model so we we have a
linear regression for predicting the
coefficients for abs
so the alphas for ABC NT given their
average given the average income in each
of the schools and so we want to learn
that and then use the office that have
been learnt from this process up here to
define the linear regression for the
actual data and so this is okay this is
an example of a hierarchical regression
and this is the first time I've got an
interesting regression embedded inside
another so the overall model looks like
that where I the wise predicted by X and
there's only a single coefficient for 4x
but the the baseline the intercept for
that the used to predict each Y is now
indexed by s as before in the previous
example there was just a uninformative
prior for the the baseline but now we're
saying the baseline is predicted by the
the the the parental income and so we
have an interesting predictor here that
itself is a baseline and it uses you to
predict what the the Alpha is and so you
get you get this model here that well I
won't go through it in all detail but in
exactly the same way as before you know
we've built it up you know step by step
perfect so somebody so far this is the
end of what I want to say about the
calculus and I'll just briefly talk
about embedding within or are used to
this within policy programming language
and then I'll stop so I've explained
this grammar by example and here they're
all examples I've given you and if you
look if you you run these models against
the data you see we get increasingly
better fits which is you know which is
good so this is the first formalization
of these are star formulas and we in the
paper I've got typing rules and theorems
about the the syntax and it's an its
semantics and we also have proofs about
how we can transform a bit like Adam was
talking about transforming probably
stick programs we here have some fucku
worked on this part we've got
transformations on the the the meaning
of the regression shown that we can
actually take any hierarchical model and
sort of squish it down into a less
readable first order and formula okay so
that's the regression calculus how do we
add it to our
languages well tabular is my favorite
prophecy programming language is an
embedding of info don't net inside Excel
and the idea is you take that you take a
data set like this this is I think the
mammography data set from the irvine
repository and you want to have a
logistic regression of of y given x 1 up
to x 6 so if we write that in fabula we
just do it like this so we that in in in
tabler itself we that the program looks
like an annotation of the relational
schema so the relational schema is is
given here where we've got columns exon
up to X 6 which are these reels and
we've got a y column which is the
bullion's and on the right hand side
here we've written some models so I've
got a latent variable Zed that's given
by regression so we just we just add
together in this syntax the the 6 inputs
and then we use that to together
logistic regression to predict Y and it
translates down to this so the kind of
translation i showed you in sort of in
theory in the f-sharp like syntax is
actually implemented in tabular and so
we get this more complicated tab of the
program that wee bit of a pain to write
out by hand but we auto-generated from
this single line and then if you run it
you know it's just going to drop the the
inferred parameters back into the
spreadsheet and we can make predictions
with it so that's in a nutshell how we
have embedded the regression calculus
inside tabular to get fabuleux and you
can go download it and look at it which
is in a nutshell the other point is we
looked at Stan so this is we done a
prototype of this there's a lot of Stan
users there's a lot of Stan models a lot
of them in fact are hierarchical
regressions and we went look to them
we've got there's an awful of
boilerplate that I think makes these
things quite hard to understand and so
basically it's before and after you know
when you this is our syntax where we've
embedded regressions into the language
and you see we get a big reduction in
size and we did a study of about 100
models in Stan and we found I I think at
least 80 of them would benefit from
using regression formulas and will be
much much shorter and so you this is a
sort of example I think this is that
lection predictor and you know it really
just as a simple regression that can be
written down in the syntax I've got so
it's it's much easier in my view to
understand this sort of thing once you
got used to it than the the more
expanded version so let me conclude
there might the point of my talk was to
promote this lovely syntax that r is
developed over the years to argue that
it actually is a nice compositional
semantics in terms of probabilities
because I've been Billy says before are
in fact is not I mean those those linear
regression packages are not generally
Bayesian so they they don't and let you
don't compute probability or
distributions at the end and we shown
here that they it has it does support a
probabilistic interpretation and the
benefit one one specific benefit that is
that we can have more compact
probabilistic models that where a lot of
boilerplate that you're forced to write
the moment can be suppressed and in
favor of this compact syntax so are your
URL is there so go check our papers you
can download tabular if you want to hack
on the source it's on github so I'd
welcome your comments and your
contributions thanks for your attention
yep gentlemen ready so your motivation
was party to make the gravity of the
syntax and make this thing if you if you
overdo it then you risk the code not be
available
you lose the boiler thing but you also
might lose things like variable names
other useful things that make it easy
for teams to communicate program sure
where do you think you are on the
spectrum of pearl and Joe or whatever
the to do screams would be in terms of
readability and communication well I i
would have a keep adding this to those
languages so if it could well be someone
should start off writing things by hand
but then once they get used to this sort
of notation just you know you can then
be briefer and then you couldn't expand
it out if you want to understand if you
forgotten what it means so I don't think
the user has to choose you know I think
once they become fluent with this
notation I think it's preferable but to
start off with they they needn't be I
mean they couldn't they can expand it
out and I guess also if you're saying
maybe in terms of readability of someone
else's model you know I've shown this
this simple compositional translation to
the core language so it's always
possible to show that if I in the
screenshot i showed you i did that by
you know implementation you write the
fabula model is a one-liner and then
there's a button you can click to expand
it out and basically it duplicates a
spreadsheet or that particular sheet but
you get the full expanded version in the
other sheet so you know if you're not
quite sure what it was what is giving
you you can get it from this expanded
version
okay then I want questions and let's
thank Andy again thank you you can't
pass up with a micro plush so next up we
have John home from the University of
Cambridge and you will feel some secrets
of the matrix factorization I can you
all hear me or it's a loss yeah I think
so i'll try to keep it on the 15 minutes
so that we can have coffee is a very
quicker yeah okay so my name is john and
i'm currently closely working with
andrew at microsoft and in today's talk
i would like to briefly introduce you
our recent work on matrix factorization
and emphasize more on the comparison
site where we try to compare between
different algorithms available on the in
public and also between our read arrived
and novel algorithms so just to briefly
introduce you the problem low rank
matrix factorization with missing data
is a low dimensional modeling technique
where we assume that the measurement
matrix which consists of either point
trackpoint track coordinates or pixel
intensities or some ratings in your
recommender system can be approximated
by a product of two matrices and we
assume that these two matrices are of
ranks which are much lower than the
original measurement matrix and such
problems can arise in many computer
vision and machine
in problems but here and also we have
missing data arising from loads of
problems and the objective we are trying
to minimize here is the distance between
the measurement and our estimation and
the W matrix basically sorry sister
laser I was pointing it the other way
right okay so the weight matrix
basically masks out all the invisible
entries so the norm is only penalized
over the visible entries the video I'm
going to show you now is on a non-rigid
sfm problem where the main cause of the
missing entries is due to occlusion of
this giraffe in front so basically we
have this original giraffe sequence and
this matrix is the W matrix each row
corresponds to each point in this video
and over time you see that they go
either black or white so the black bits
means means that it's missing and the
white this means that is filled so this
means that this for example at this
point has been you know it's visit
invisible and then becomes visible and
then invisible and visible again so we
have this video of this giraffe you know
and some parts of it is quite occluded
and thirty percent missing we have and a
matrix factorization algorithm at the
global quotation mark minimum can
produce the result something like this
where the red points describe the
extrapolated point some like this and
here we've assumed that the giraffe
trajectories can be estimated by a
product of two matrices where one is the
giraffe 3d points and the other one is
just a linear basis of this thing's by
won't go through in detail I'll just
briefly mention why we need this global
optimum obviously obviously there's no
guarantee that these are global but this
data sir has been run for millions of
runs over the past 20 years and this
optimum value hasn't been beaten yet so
far and so we believe this is the best
observed optimum for this data set and
each of these lines correspond to the
points that
you've seen in the video just just in
the previous slide and what we see here
for this data set is that the second
optimum which has only 0.6% costs above
the the best optimum has a very bad
extrapolation property like this this
may not always be the case for all data
sets but empirically this is what we've
observed over different computer vision
data sets so here what we are saying is
we because we believe that matrix
factorization algorithms need to run
over different data sets and they need
to perform well we're saying we need to
reach these global optimum or the best
observe optimum for each data set so
some of the known strategies include the
alternation where you just fix one
matrix and optimize over another and
vice versa until you reach some kind of
convergence another thing which was
proposed ten years ago is a joint
optimization where you just stack U and
V together and optimize over both of
them using just the Newton's over as
Newton like silver and the last
approaches variable projection which has
regained attention recently I'll just
explain this briefly what you do is you
first try to minimize this cost over V
only given some value of you and you
have this v-star view because because
this objective is linear in both you
envy that means you can get the closed
form solution for V Star and that means
because this V start depends on the
value of U is essentially a function of
U which means we can now put replace
this original v by this v-star view
yielding a new reduced formulation only
in terms of you and although this may
look complicated because we are
introducing non-linearity we simply need
a derivative of pseudo inverse which is
a written in Magnus book or Tom link as
a matrix effect matrix notation so why
do we need all this so what we did first
was we gathered around all different
algorithms in the literature over the
past 10 15 years and we we realized that
there were some similarities but
because they were all in written and
different multitude of different
derivations we read arrived each
algorithm in a unified notation try you
know the attempt the aim was to try and
see pin down exactly what the
differences and similarities were
between different algorithms and we have
yielded this algorithm chart which
basically now we have a modularized
blocks so for instances compose this
algorithm we just need to do you know
one two three and then we do step 10 and
we yield another person's algorithm so I
mean this doesn't contain all the
algorithms but we have I ran out of this
spaces so I'm only writing some of it so
now the question is you know which
method works best out of all these
algorithms available on in public and so
the standard convention people use is
they just we try and just run the
algorithm on different data sets or has
it oh it's run out of battery i think oh
the strike okay so what we do is we try
to run on different data sets and
different algorithms so the left hand
side data data sets are a bit easier
than the ones on the right hand side the
ones on the right hand side are bigger
as well and we tried different
algorithms over oh thank you okay hey
sorry for the interruption so we tried
running different algorithms and
different data sets from random starting
points meaning we draw use MVS from just
some normal distribution and we just you
know try and measure how many times each
other than converges to the global or
the best observed optimum for each data
set and the green means it converges
hundred percent of the time out of
hundred runs and blues zero and grey
means it's time that will too slow for
instance for these ones it took like a
day for one iteration so I gave up on
those and I they are running at the
moment
so to categorize them the first batch of
the algorithms are the our reimplement
ation / some novelty is added to the
original algorithms and they are
available on github and these are other
of our variable projection based
algorithms available in each author's
pages and these are some other you know
strategy based algorithms which you've
also tested and we see that you know
mainly developer algorithms in terms of
this success rate metric performs better
than the other algorithms well and we
can conclude and say you know Val pros
is the way forward for matrix
factorization but that's not the only
problem you know in practice we also
have to take into consideration of the
algorithm runtime and for instance if we
have an algorithm which you know assume
assuming constant success rate of five
percent if we have you know if it only
requires ten seconds we expect about 200
seconds for one success using geometric
distribution and for algorithm to which
you know even as to seize ninety-nine
percent of the time if it takes one hour
for one run then we expect more seconds
for one success on average so you know
somehow we would like to take into
consideration of the immuno different
algorithms runtimes so our approach was
that you know if we can somehow make
each algorithm work you know hundred
percent or not hundred percent but you
know as well as possible then we can
just compare the runtime of these
different algorithms so we came up with
this very simple it may sound stupid but
a thing called Russo were basically we
run each algorithm until we see second
we see to optimum twice which have to
each of which have to be the best one
we've observed so far so not to confuse
this optimum with the best-known optum
for the data set let's say the
best-known optimum is one but we observe
two twice we still terminate right so
they don't always guarantee hundred
percent convergence but the
the good thing about there is it really
easy to code and also the good thing is
now the success rate for this Russo
metal rhythm goes all while greener than
green Earl and the previous diagram we
now see more gray areas because some
algorithm says have failed to observe to
optimum twice so but then we hope that
because this was limited 200 runs if we
run more runs then eventually hopefully
these will all go to green and we can do
similar things you know run until C and
third optimum and you can see that they
go greener and it's a greener greener
and greener so after having achieved all
this green diagram we believe you know
now speed it'll speed is all that
matters across these different
algorithms but how can we measure this
fairly because you know they all have
different stages in the algorithms which
can be implemented efficiently so now we
go back to this giant table of because
we've modularized each step in each
algorithm we could just turn on and off
different blocks and you know test
different algorithms out and we believe
our implementation of these authors
algorithms are more accurate and faster
than the previous ones because we also
realize some minor tricks of getting rid
of some redundant computations so
reimplement everything and they you know
faster and more accurate and this gives
the conclusion that the VAR pro the dr w
two data sets a dr w so these algorithms
perform better than the other ones and
the dr w two sets which are these to
perform mildly better than the other
ones so just to summarize our work was a
heavy unification of literature and we
propose some benchmarking techniques for
comparing different algorithms and we
claim that the D audibly to set is the
winner I'll cross out of all these
algorithms right thank you
alright and for any people who's doing
matrix factorization the code is
available online so you can use them
great so we have plenty of time for
questions let me start with on so most
of the columns in your table we're
actually computer vision applications
right so are these do you think
these findings are restricted to the
particular nature of these computer
vision applications or do they also
extend to say the statistical problems i
think net maybe means netflix but kind
of other statistical matrix
actualization problems do you think they
would extend to them I think this
actually this paper this RTR MC they use
similar technique to vote pro but the
root the reason we've categorized it
here is they use a instead of you know
computing the full Hessian they use
stripper conjugate gradient to solve
this subproblem and but then in nature
there are similar they have similarities
to the original problem so I think the
techniques on how to solve these
different subproblems may vary but over
all these can be applied to large
problem and also you can add the regular
rises as machine learning people do the
reason why we did include them here is
because that guarantees unique solution
for computer vision data sets as well
about when we tried extrapolating from
those unique solutions they turned out
to be terrible like the ones I showed in
the giraffe data set okay so i think
they are applicable yeah if I'm writes a
couple of your algorithms I write
article is that is that true from the
just a number of spots in there so far
the if we look at the CH lms 5 and envy
auntie and the last one that wonderful
yeah the voice I don't know that one
before that that's it yeah I respond
rapid reference to and read SSA wants
which is the last war column so seem to
have just the same spot see imagine turn
out to be identical oh yes so they are
identical in this way but again as I
said before this one uses
they instead of computing this Hessian
completely they approximated using they
just choose a different technique to
solve that yep rather than analyzing the
whole systems could you analyze the
different elements and see which
elements give you again over other
elements of the algorithm that you've
got there Sarah what kinds only two so
rather than comparing algorithm to
algorithm for the whole lot oh yes could
you break it down say this element of
the algorithm contravenes are going them
such and such oh yes yeah you can do
that the reason why we went for this way
is because you know it was it hasn't
been done before so really we thought it
was you know time to you know you know
truncate all these into into different
categories but yeah I think your comment
is valid so the next thing is to see
whether so actually this number is five
and six these are a particular stage of
doing applying some kind of manifold
optimization now we've written it in
terms of this code detail but what you
can do is you can say you know we've
done manifold authorization does it
improve or does it not improve that's
what you're suggesting right yeah I
think that's we haven't done this that
but we can do this we can do it yeah in
that way
okay any more questions otherwise we are
going off to the coffee break but let's
say John once again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>