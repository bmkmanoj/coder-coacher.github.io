<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Optimizing Trade-offs for Scalable Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Optimizing Trade-offs for Scalable Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Optimizing Trade-offs for Scalable Machine Learning</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hIgfwzgdMrs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
today is a player to have Joseph Bradley
come here he's finishing a PhD from CMU
in the machine in a department yeah and
he can talk about a pleasantry da for
scalable machine learning it thank you
wait so um be right as you said talking
about how to optimize different
trade-offs of quantities in order to
help scale various machine learning
problems and so I'll start out just by
talking about what those quantities are
at a very baby's level view of machine
learning where we'd like to get data
train a model to fit the data using some
sort of optimization and then test the
model in new data and so what the
worries I'm going to be thinking about
our if we have big data a lot of it have
a complex model structured optimization
and all of these difficulties are going
to end up feeding into things such as
issues such as sample complexity how
many training examples do we need to
learn the model a computation in the
optimization during learning and in the
modern world how to take advantage of
things like parallelism to help with
that computation and so these three
quantities are going to eventually of
course feed into the eventual accuracy
of our model on the new data butts these
three quantities which I'm going to be
talking about so in order to improve
scalability you can imagine improving
each of these individually for example
develop methods with better learning
bounds which improve the sample
complexity look at computation by
developing faster optimization methods
in the sequential setting or say do
parallel implementations of existing
algorithms so what I'd like to think
about instead is how to trade these off
where for example a small sacrificing
computation might lead to a big gain in
parallelism
and in the first part of the talk I'll
talk about a method which does this
straight off and the second part of the
talk I'll talk about upsetting and a
method where we can trade off all three
of these aspects so our general approach
to scaling is going to be to take some
complex problem and decompose it into
subproblems which are simpler to solve
in and of themselves then through
analysis we'll look at different
trade-offs in this decomposition and the
analysis will actually help us guide how
we do those trade-offs in order it do
that decomposition in order to optimize
those trade-offs and interestingly we'll
be able to talk about data and model
specific ways of doing that optimal
trade-off so in this talk I'll start out
talking about parallel regression where
i'll talk about parallel coordinate
descent method where we can trade off
computation and parallelism and the
second part i'll talk about parameter
learning for graphical models where we
look at a method which can trade off all
three of these aspects so first looking
at parallel regression so in the
regression problem we want to predict
essentially one label or a small set of
labels given a large number of features
for example and one data set which we
look at the label is a measure of stock
volatility which it turns out you can
predict somewhat unlike the direction of
the movement in stocks and we're going
to be predicting them from a large
number of features which are small
pieces of text from financial reports
the sparsity part of course is that we
want to explain that label using a very
small number of features and of course
this is very useful in high-dimensional
settings where the number features is a
lot larger than the number of training
examples so in this part the problems we
specifically look at are less so with a
least-squares loss and sparse logistic
regression
in general our analysis is applicable to
generalized linear models so in the
sequential setting there are a whole lot
of algorithms which can be used for
sparse regression a gradient descent
stochastic gradient interior point
different threshold in methods one which
caught our eye was coordinate descent
also known as shooting and it caught our
eye because it's been shown to be very
fast there have been a number of
theoretical empirical studies explaining
this and showing this on many problems
but for big problems where you have
millions of features or hundreds of
thousands of examples even this fast
sequential method ah is may not be ideal
and so we'd like to do is take advantage
of parallelism so here I'll be talking
about the multi-core setting where their
shared memory and low latency and an
ongoing work we're looking at
distributed when this in the multi-core
setting we can think of paralyzing a
number of aspects of the problem for
parallel regression first matrix vector
operations many methods such as interior
point to spend a lot of their time doing
such operations and we could think of
using existing linear algebra libraries
to do that however we found this did not
work the best empirically and I think it
was largely because the methods which
could benefit most from parallel matrix
vector operations were not actually the
fastest methods for this particular
problem we can think of paralyzing over
examples such as doing this stochastic
gradient method which has some parallel
analysis but one could argue that using
stochastic gradient methods tends to be
best when you have a large number of
examples not a large number of features
which is the setting we were looking at
and then finally looking at
parallelizing over features for example
shooting or peril or coordinate descent
and parallelizing that we asked the
question which i'll explain in a minute
of whether that should be inherently
sequential but of course it turns out
that it's not and
I'll explain why so I'll talk about is a
method called shotgun which is parallel
coordinates sent for sparse regression
and I'll first show a convergence
analysis which predicts that you get
essentially linear speed ups up to a
problem dependent limit and then show a
big empirical study which shows that
shotgun is quite successful in practice
so just looking at a low background our
problem is going to be to minimize the
convex objective F of W where W is this
weight vector and F of W would be the
loss and the regularization for whatever
lasso or logistic regression problem we
are looking at so for shooting in the
sequential setting the basic algorithm
I'm looking at is a stochastic
coordinate descent method which says
while you're not converged pick a random
direction or coordinate J and update the
weight for j via sometimes a close for
minimization sometimes the line search
so if this were the contour map where
gray is better and we start at some
point this method would optimize in some
direction pick another direction
optimizing that and eventually get to
the optimum so in the parallel setting
what I'm looking at is very naive
parallelization where rather than
updating in a single direction at once
excuse me what date on each of use me P
processors p different directions just
holding pretending that we're holding
the other coordinates fixed so for
example in this setting if we start here
pick two random directions and compute
the minimization xin those and then we
add those updates together we will get
right to the optimum of course this is a
very nice setting where here we have
uncorrelated features and on the
parallel updates are not going to
conflict
now in a bad case with extremely
correlated features if we compute
minimization xin both directions and
dependently and then add those updates
together of course we might diverge and
so you might ask is coordinate descent
inherently sequential well here's why
it's not and those are shotgun
convergence at proof or theorem and it's
essentially stating that if we limit the
number of parallel updates well talk
about the limit on P in just a moment
then we get this bound which states that
the distance from the optimum where W is
the weight vector and WT at T iterations
w star is the optimum that will be upper
bounded by this quantity on the the
right we're on the numerator we have
quantities such as d the number of
features at w star the optimal weight
vector and w naught where we began we're
dividing it by the number use me the
number of iterations t dotted with p add
the number of parallel updates and so
what this is essentially saying is that
we're getting linear speed ups since
this generalizes abound from I shooting
for the sequential setting so given this
bound which states that if we limit the
number of parallel updates we should get
essentially linear speed ups this limit
is going to be essentially d over row
where d is the dimensionality or a
number of features in our problem and
rho is the spectral radius of the
normalized gram matrix X transpose X
where X is this data matrix of examples
by features so intuitively row um
measures the correlation between
features and with proper normalization
of the data matrix row will be between 1
and the number of features so in the
ideal case with uncorrelated features
row will be 1 and that means that our
theory would predict we can update all
the coordinates at once which is what
you'd expect in a very bad case with
exactly correlative features ro bede
which means that we can only update a
single coordinate at once yes 0
whatever's if you subtract two trillion
off of a nap he is subtract a trillion
off of that yeah it should be the same
minimum it should be the same converges
the inventive but the right hand side
becomes like huge negatives I don't know
how to interpret it I see hmm so if you
subtract a trillion off of F um so the
losses we were looking at I guess they
write all the losses we were looking at
we're non-negative um and that should be
I'm thinking of where that would appear
um I think that was implicit in our in
our in our proof and right it's not
stated here they have to be non-negative
n rice / trillion and make that from
your way so if you divide a by a
trillion let's see then oh yeah that it
that it just scales its relative great
okay okay yes did you make it easy
because you scale like AC right so um in
terms of this types of assumptions we're
making um so f is taking the form we're
assuming that we can write out
essentially a second to second order
Taylor expansion and that the matrix
which appears in the second order term
is going in order to UM if it's written
as an upper bound matrix upper bounding
that that matrix is what actually
appears as the grand matrix and the next
part defining row and that limit on row
is what we're that limit on row would be
a limit on this smoothness so i guess if
right so i think extreme values of row
then that row of the spectral radius
would essentially measure that
smoothness as i understand
those right for you so it still up to
140 skip it would still exist um so in
terms of smoothness assumptions um I
guess what we're grapes still you know
that there are some problems for week
right to you so I I think it's really in
what I seen before with being able to
upper bound the change there's an
assumption where we can upper bound the
change in the objective with a second or
something which looks like a second or
taylor expansion and the assumption that
we can upper bound it with that is
what's encoding smoothness and right i
think for some problems certainly you
could not do that great can go outside
again i typed on said cuz i think it
still is strange to me because it should
multiply F by a huge number then you
basically get rid of your W star norm
term so it's so let's see ya at initial
right so it just feels kind of weird
that right um-hmm so I tried to pick up
room so i flew my beautiful ah I am I
unfortunately do not but I think um zero
water
right this assumption this theorem I
think where it would peer perhaps is
that actually hidden and this is really
about for the case of less oh and for
general models there there should be
another term which I think in our papers
on beta but basically it is a constant
which is x it appears in that second
order Taylor expansion like bounding the
change in the objective and if you
multiplied the objective by huge number
then I think that would essentially
appear as a beta x that W star I'd have
to check but i'm pretty sure that
constant which is sort of lost dependent
on is multiplied by the w star term and
for example for for less oh it's like 14
logistic regression I think it's for
something um so so I just kind of hit it
here but you bright I think if you
multiplied by a huge constant then it
appeared there I think that's the answer
okay
right so given that we'd expect
essentially linear speed ups up to some
limit we can see how it actually looks
in practice and so if we look at an
example data set we're here and plotting
on the x-axis the number of like
carefully simulated parallel updates and
on the y-axis the number of iterations
we're both here we have log scales then
if we do a single update per iteration
we require almost 10,000 to converge
extrapolating linear speed ups we'd
expect to lie upon this line and our
theory says we should be able to do
about 79 parallel updates before we
start risking divergence and if you
actually run this in practice then you
see essentially that we're proximately
linear speed ups and then after at the
end of this plot we start hitting gain
divergence and you see similar behavior
on other problems so the experiments
seem to match our theory which is what
we'd hope so as far I've talked about
shotgun as this naive parallelization of
coordinate descent working and showing
how linear speed ups up to a problem
dependent limit actually seemed to
happen in practice but now quickly show
you some results from larger experiments
first looking at logistic regression
where our goal is to predict the
probability of a discrete label we
compared a small number of algorithms
here since there has been a big
empirical study before basically showing
that shooting the sequential version of
our algorithm is extremely competitive
but we did take time to compare shotgun
with this parallel stochastic gradient
method since it was one of the few other
right parallel methods with which we
have not seen tested on these problems
so stochastic gradient was just a simple
implementation where we estimate the
gradient with a single
training example and of course it's
considered to be very scalable and we're
running on a core AMD opteron at 2.69
gigahertz so just quickly showing an
example result this is actually in a
high sample setting at the high
dimensional setting made us look even
better but in this setting we add half a
million examples 2000 features on the x
axis as time and on the y objective
where lower is better and so if we look
at shooting the sequential coordinate
descent it does seem to do recently well
stochastic gradient as you might expect
is faster at first floor later on
parallelizing it helps a little bit
using this method but then parallelizing
coordinate descent helps enormously and
it looks like shotgun is the fastest so
basically paralyzing over coordinates
seems to give big speed ups and we saw
even more extreme behavior differences
in the high dimensional setting its
convex so did you run some sort of thing
like Elliott just you know what the true
objective minimum is right so for all of
these we mean we essentially ran them
for an extremely long time and then
recorded that essentially optimum and
then we would do these experiments
running these until they got within some
percent so we would we actually I think
tried to compute the optimum using um
shooting but in if another that was what
we did in ish alee but then if another
method reached a lower objective value
we had record that is optimum so
it or a little bit below that but right
um we ran some of these for write an
extremely long time until it's our an
implementation right and this is
actually really simple SGD
implementation we also tried ones
specifically tailored for the l1 setting
but those in terms of objective value um
were much slower even though they got
sparser answers just because the the
issues with doing stochastic gradient
with l1 ended up making them less
competitive on these sorts of plots than
our implementation challenge they are
bank reserves also degradation challenge
right so I'm that's a good question and
i'm not sure what their curves would
look like relative to this that would be
good to check it just average is the
eight cores at the end of the procedure
right it does so how do you guys traffic
progress how do you how do you terminate
each of it a forceful rivers oh so the
ideas um at each iteration at suppose we
finna we stop there compute that average
and that's one one point in this plot
and so it's essentially i mean this
Blatt saying suppose we stopped at this
moment then that's what the parallel SGD
would look like much so
that's right so looks like shotgun is
maybe three times faster than shooting
or so i'll show you actual speed up
plots in in a couple slides yeah ok so
then looking at last so quickly um goals
to predict a real-valued label and we
here we hadn't found as many big
empirical studies so we tested a lot
more types of algorithms as well as
large number of data sets with sizes
brain from you know hundreds to millions
of features and examples I don't have
time to go into too much detail fees but
I'll show you two of the four classes of
data sets we looked at the important
thing to note our sweat circled in blue
which is the average predicted number of
maximum parallel updates for the data
set shown and i'll show you in each of
these boxes but the important thing is
basically note that a very large number
of parallel updates could potentially
have been done so in each of these plots
x axis is shotguns runtime why the other
algorithms run time and if something's
above the diagonal line then that means
shotgun was faster so this point is
saying that on this particular data set
shotgun took one point two seconds the
sequential version 3.4 so just quickly
plotting different methods up here we
have shooting l1 LS interior point
method which used parallel matrix vector
operations and then a number of other
methods and most of the dots are above
the dotted line so shotgun did
reasonably well and then on this larger
sparser dataset collection a lot of the
methods actually weren't even able to
finish in a reasonable amount of time
and so shotgun looks quite good so
essentially shooting seems to be one of
the fastest algorithms and shotgun
provides additional speed ups however
speaking of speed ups if we look at the
actual curve
serves of X access number of course
which is also in the number of parallel
updates we're doing y axis the speed up
we got these are aggregated results from
all of our tests and of course diagonal
would be optimal so yeah if you look at
that wall clock time speed up of less so
on average it doesn't look that great in
it varied a lot sometimes it would be no
speed up sometimes almost optimal but
average was there but if you look at the
number of iterations we're doing it's
almost optimal so it is decreasing the
number of iterations and what we thought
we were hitting was at what we believe
we were hitting was this memory wall
we're centrally memory bus is getting
flooded and I think a reasonable
explanation for this is that lasso is
updates are very cheap very low
computation / datum loaded and so it's
rather difficult to hold at to hide
things like memory latency and one thing
possibly supporting this is that if you
look at the logistic regression time
speed up it's significantly better
although still not optimal and logistic
regression uses more floating-point
operations per day time loaded and we
believe that helps hide the memory
latency so yada push okay right so yeah
I think that definitely points to the
need for possibly more opt preps there
are more optimization hardware-specific
optimizations we could make trying this
on other hardware test nyan on other
types of losses which might help hide
that latency right there's a lot to
sound the setting when you're measuring
speed up your old in the problem size
constant
and just increasing the number of or so
are you also scaling the farm website in
anywhere um so this is each point is the
average over data sets of running that
of the right speed up for that number of
course so it's right you're right I'm
not I guess that means that everything
is being held constant is we go across
that that would definitely be
interesting yet the variance in the
actual wall clock time speedups was
pretty big and it would be nice to be
able to say a bit more about you know
wit types of data sets yeah that was
best for me
uh I don't know the number I know the
ranges which were essentially from one
to eight um but one time so I think it
depends on this I'm not sure but I would
think it would depend on the
dimensionality and the sparsity of the
data and I'm not sure right your data in
columns are in rows cuz you're you're
sure you want to access it by column so
that that was how we did it for sure it
is really right ok but they're still
needing different columns and so you're
the memory subsystem ins it has to serve
it's trying to stream through bunch of
stuff but it's jumping well sort of and
in practice we don't choose completely
randomly we did a random permutation and
then went through it and there were
things which we later tried which did
help a bit like trying to carefully
choose a source sort the columns and
choose which ones we handled it which
time um right so it wasn't completely
random but yet they're there were
definitely issues with locality this is
a core just doesn't interrupt it's not
new mom right it's just one pack it do
you need to see a hole cut too
your date's way
the whole column yeah did you seen again
I suppose that the thing is dance you're
good I think that's definitely a
question which interests me whether you
can mix the ideas of stochastic
coordinate with stochastic gradient um
it's not something we have experimented
with really but it would definitely be
interesting yeah so I guess both were
distributed or for like an Ummah system
and be interesting if each processor was
choosing a column from a sub a
pre-arranged subset of columns have you
look done any analysis or look to see
whether it would be just as effective at
each we we have done that sort of
analysis to try to deal with thee I
guess you call the statistical issue of
conflicting updates where we tried to
sort of sort columns based on
correlation between them we haven't I
guess done it as much from the more
system side I guess so in terms of
sorting by correlation there ah there
there can be some benefit to doing it
there there is some recent work by Chad
share I believe which looked at cervix
extending this idea and combining greedy
coordinate descent with stochastic
coordinate sort of where where they did
some smarts wording of comments yes make
sure that I just forgot that what's the
dimensionality and a number of
uh so it varied a lot from 100 to
hundreds of thousands or millions take
the speed up wood putty depend on
another right this is the cash or not
you know this kind of things so that
that is definitely a right mess I mean
remember for shine for like three huge
data sets where you know single box
implementation may not be viable
actually started and then you as far as
single box implementations that I mean
that's something we're thinking about
now this was really targeted at
multi-core I think in the distributed
study and we're having to think about
pretty different approaches yeah in the
experiment with different CPUs
we tried a little bit there there is a
16-core think now um machine which we
were testing with we saw it was a bit
better I think it had right lesson
issues with cash but I don't think you
have you know a lot of business about
memory yeah so so unch in cash and start
to make a shin I mean definitely on a
more souped-up machine that with right
it would help um yeah I think I'd be
interesting to look at other types of
hardware really not that computation
right is that what I think that's
definitely with us so yeah so um just to
sum up this part but we looking at
parallel regression talked about shotgun
this parallel coordinates sent I gave an
analysis showing essentially linear
speed ups course in our experiments we
did not get the ideal speed ups but
especially since the sequential version
was one of the best methods for these
problems speeding it up a bit with
parallelism made shotgun one of the best
methods so so we're going back to the
themes I talked about at the beginning
what we did was decomposed this
computation based on different
coordinate updates and we saw that
basically even though these coordinate
updates would conflict and cause a
little bit of divergence making us do a
bit of extra computation we ended up
getting a big gain and parallelism and
we could sort of optimize this trade-off
by choosing the number of parallel
updates based on the amount of
correlation in our data so there's a
pretty simple example of this these
sorts of trade-offs well I'd like to do
if i have time is talk about parameter
learning for graphical models which is a
case where we can actually trade off
things and much more complex but
beneficial ways
so in graphical models um and motivating
example might be say you want to model
user interests or a behavior in a social
network and to do this you might want to
say model a probability distribution
over a bunch of random variables X where
each X wat say X 1 model is a particular
user user one so given a model for the
probability distribution over all these
random variables you could ask queries
like probability of some set of
variables given another which could
translate to predicting some users
interests given what you know about
others so the general framework I'll
talk about our markov random fields or
MRFs where of course an edge in this
graphical structure will indicate a
direct dependence between two variables
filling out this graph gives this
structure which essentially encodes our
statistical assumptions and then we'll
factorize this model on by writing it as
a product of factors which I'll write as
sigh at and each of these will be
functions over small sets of random
variables which will correspond to edges
in this case or perhaps hyper edges in
our graphical model and so in this if we
fill out the rest of the factors of
course we have a fully specified
probability distribution and even though
i'll talk about MRFs all of these
results generalize to conditional random
fields as well so basically this setup
of course is very principled in
statistical principled statistical and
computational framework there's been a
whole lot of work including a lot from
here and many applications of graphical
models as showing they're quite useful
so in this I'm going to be talking about
the parameter learning problem we're
given the structure wit or given the
structure of this and data sampled from
P star which is this
a target distribution will want to learn
parameters which are the values of these
actual factors so a traditional method
using max likely at estimation or mle we
want to maximize with respect to our
parameters the expectation of our data
of the log probability of seeing that
data point and of course emily is in a
sense the gold standard in that it's
optimally statistically efficient and in
the infinite sample limit I know method
is really going to be better than it but
of course the problem is that in this
loss you have this probability over the
full distribution and computing this
probability is difficult because of this
proportionality constant and estimating
that proportionality constant requires
inference over the entire model all of X
and this has been shown to be provably
hard in general of course be intractable
in some cases such as if the graphical
structure is a tree so given that
inference is hard a question is can we
learn without intractable inference and
in parameter learning there has been
there have been a bunch of works I often
using approximate inference or
approximate objectives but the problem
is that most of these work lack really
strong guarantees for general I types of
models especially if the model is not a
tree or the like and so what our
solution is going to be is to use as a
baseline mle which as we stated has
optimal sample complexity but requires a
lot of computation and as we'll see is
difficult to parallel lies I'll then
talk about sudo likelihood which is this
method which essentially breaks the
problem into a separate optimization for
each variable in your model and as we'll
see it has higher sample complexity but
much lower computational complexity and
easy parallelization and actually our
analysis gives the first finite sample
complexity balance for John
models I'll then talk about composite
likelihood which is a method which
ranges between Emily and pseudo
likelihood allowing you essentially to
choose substructures in this graphical
model to have a more structured
estimator for the parameters without
lettuce do is in many situations choose
this estimator structure in order to
optimize these trade-offs and get the
best of sample complexity computation
and parallelism for many problems so
first to motivate the idea of pseudo
likelihood emily is essentially
estimating this entire distribution p /
x at once and what pseudo likelihood is
going to do is start by observing that
if you take the statistical assumptions
encoded in the graph this is essentially
saying for example the probability of
one variable x1 given the rest of the
graph or its neighbor is proportional to
the single factor sigh 12 so you could
imagine doing regression at which we
talked about in the first part of the
talk in order to get an estimate of this
one factor you can imagine doing this
for every variable in your model for
example probability of x 2 given its
neighbors and we rest amay ting that via
regression would give you estimate of
these other factors and you can note
that we run into some issues with
multiple estimates of these factors but
you can actually show that you can
average these in log space and still get
good guarantees so what I'll call this
is pseudo likelihood with disjoint
optimization where you're aggressive
arable on its neighbors that gives you
factor estimates and then if you have
duplicates than you average them
together in log space also talk about
pseudo likelihood with joint
optimization which is essentially the
same problem but you do parameter
sharing when doing the optimization and
this is actually how pseudo likelihood
was originally presented the key is that
this formulation allows tractable
inference where
each of these subproblems we have the
probability of a single variable given
its neighbors and in order to estimate
this probability or compute this
probability by our model we just have to
compute this proportionality constant by
summing out a single random variable in
this case X 1 so to compare emily with
pseudo likelihood emily estimate the
full model once pseudo likelihood
essentially regreses each variable on
its neighbors emily is going to be
intractable in general as pseudo
likelihood will be tractable Emily's
been shown to be optimally statistically
efficient but people have shown that
pseudo likelihood is often empirically
successful emily has had finite sample
complexity bounds meaning people know
its behavior in the finite sample case
but before this work uh pseudo
likelihood did not and so had often
heard it referred to as a sort of
heuristic but we'll see we can cross
that off yes when you say a martyr vs so
the Mart is the same right so how is the
inference becoming tractable in one case
and interacted with another I me the
model is the same so I'm not I'm talking
about the inference required during
learning not the inference that test I'm
and that is actually a raises a number
of interesting questions um at learning
time though the loss you're optimizing
doesn't have the probability of the full
distribution in it the Marlins oh um
it's an interesting it's it's hard to
know exactly how i phrase it I would
phrase it i guess is changing the loss
and optimizing that loss does give us
guarantees with respect to our original
model
great you can change you can actually
change them all well that we call the
dependency now you just give up that is
another option I mean the way I see it
as you are constraining your your
potentials which essentially is equal
into changing a model that's how I'm
seeing right um maybe maybe just the
same wonderful you normally get a loss
yes okay multiple purposes I guess it's
hard to say I mean you're not maybe I
could raise it under way can you go back
you go back to where you were I'm not
sure what the model would be I guess
which you would which would be listed by
this loss you said you only change the
training only for creating time not
testing time for a shape that's here I'm
not actually optimizing about what suit
I swear you're not doing that that's
true okay right you how does that does
that mean you change the one let me this
I guess it's a semantic i thought i
wouldn't go changing the bottommost
right i mean refugee law serafini
parameters but you haven't read them
there might be something interesting you
said like this is actually optimizing
mle with respect to this model but i'm
not sure what that model would be unless
it right I mean the clothes my guess
dependency network seemed very relevant
but i'm not sure if that's it right um
small-scale problem suppose you
construct something really small folk
wish you actually can compute the
likelihood you compared to like a
diversity how much difference is it
going to be um something like it right i
will show comparisons between like how
accurate the parameter estimates are
from soo likely adversus optimizing
versus optimizing and le if that's your
question i'm more thinking about it
usually
change the loss function right the way
you interpret it and then you estimate
greater from the parameter that you
asking me who dis landscape then you can
compute the likelihood for those
primates they've just been through sudo
again ah yes you can and then the
question how much different is for
small-scale probably public in computers
right and that that's something I'll
show yep good point ok so we're worried
right so talking about sample finite
sample complexity analysis of pseudo
likelihood the result actually a phrase
in terms of emily so first sample
complexity result for a meli which will
be very similar to the pseudo likelihood
one would look like this to achieve a
given error epsilon we need it most in
training examples Britain is ah around
this amount we're here we have epsilon
the l1 norm of the parameter error and
that's actually normalized by the number
of parameters are is the number of
parameters Delta is this probability
that this bone doesn't hold very common
in these analyses and lambda man is an
eigenvalue condition which I'll discuss
a little bit later but first what I'd
like to show is the bound analogous
bound for pseudo likelihood which you
notice is essentially the same except
that actually this lambda men value is
going to be different so before I
explain that I one point out that yes as
I said this is the first finite sample
complexity bounds for very general
models and when you add this together
it's tractable inference for many
classes of MRFs this act and CRFs you
can actually show that this implies back
learnability and basically what you need
to do is control how that lambda men
value grows with respect to whatever
problem parameter you're defining your
class of models with respect to so given
this what I need to do is explain lambda
men
yes what does it remain here error so um
it's the AH l1 norm of the parameter
error so it's we have an optimal
parameter vector and n right and it's
also normalized by 1 over R the number
of parameters so lambda men we'd expect
the number we'd expect the number of
examples to need to be around 1 over
lambda min squared so we expect this
value to be important and I won't take
too much time to talk about this but
essentially it's an eigenvalue condition
which measures the curvature of the
objective where greater curvature and
the subjective is going to make it
easier to learn and important thing
really is how it varies for Emily versus
pseudo likelihood essentially emily is
going to have larger lambda men implying
lower sample complexity but of course
recall requires more computation pseudo
likelihood will have smaller lamina men
higher sample complexity but of course
less computation so we have this sample
computational complexity trade-off and
speaking of trade-offs I'd like to point
out one more which is specific to suit
of likelihood now our call that pseudo
likely Edison is essentially taking each
variable and regressing it on its
neighbors but I mentioned you can do it
but with joint optimization with shared
parameters and disjoint where you
average duplicate estimates later and
the bounce I've been showing have
actually been for joint optimization
which will have lower sample complexity
based on our bounds and with disjoint
optimization we get slightly worse
bounds on the number of samples required
but of course that's completely data
parallel so we have something of a
sample complexity parallelism trade-off
is finally looking at the predictive
power of the bounds on the left here is
lower sample complexity or lower error
for a fixed number of samples on the
right higher and so we'd expect to see
Emily being the best
this respect then sudah likely it with
disjoint optin with joint optimization
and then disjoint and so what I'm
plotting here is a synthetic example
where we can compare with the ground
truth x-axis number of training examples
on a log scale y-axis the parameter
error lower being better and so you can
see with max likelihood we do indeed
learn decreasing the error pseudo
likelihood is a bit worse especially at
the low sample regime and pseudo
likelihood with disjoint optimization is
a bit worse just as we'd expect so this
sample complexity at Union actually
occurs in practice you're physically
good basic you like different can I love
your mobile right have nothing to do
with each other Barnett to get the same
to see them and they're very print like
dependence assumption they don't have
the same sets of variables and so they
it seems that they could get that's very
different potentials there are
definitely cases with where that does
happen and where that doesn't and it's
essentially actually going to be this
like lambda min value which control
which like hides all of that behavior in
the actual proof um you could imagine
the disjoint optimization proof as
looking like a proof showing that simple
regression works and then basically
showing that if simple regression works
each regression will be recently
accurate and so averaging their
estimates together will still be
reasonably accurate and our proof just
use it as simple MC occurrence for
different model they won't be accurate
for the same problem I tell you so x1
depends on X to do and then
that you experiment on a street in terms
of the UM so it would be for the same
model in the sense that both problems
would be using the same set of
statistical assumptions where the
graphical models including these
statistical assumptions and each problem
would be still working with that same
graphical model although with a
different loss i agree but the
guarantees would be worth through thurs
with respect to the same model yes one
thing I'm not able to resolve this how
would this relay given method resolve
fake you know basically what they call
not attractive potentials well you know
x1 x2 should be should be different x2
and x1 should be different as well I
mean you have basically a kind of you
understand what I'm saying like an icing
move yeah basically yeah allegan Isaac
model where where everything is not
attractive those cases are super
likelihood will completely fail hmm so
in that case i'm not sure what the
eigenvalue condition would behave as um
we definitely tested it on models where
there were both um i guess we did not
actually i think testa on models where
they're completely all repulsive
potentials only on once with a mix and
with all attractive but it people have
looked at to the likelihood for
differences on networks of images right
than Ising model and they have actually
found that when the pretend the
potentials on or attractive super likely
wants to diverge quite a bit right so I
mean I think that that's something I
would like to test dunson data with the
ground truth um I think if you have a
model which is right maybe completely
repulsive I don't know I'd have to
explore that more if you have some
repulsive like problem areas then I
think actually the method I wanted to
talk about next would be quite useful
great it's um should know my slide
numbers um right it's so for Emily it's
the minimum eigen value of the Hessian
of the objective the log probability of
our all of X at the optimum for sudah
likelihood it's this minimum over each
variable of that eigenvalue condition
for that local loss for what you're
actually mine so nothing is a measure on
the only one to every problem we always
it to measure on your so this is both
are with respect to UM like include the
target distribution but I mean in terms
of the loss from which this Hessian is
computed that is lost specific that
that's different for the two but both
involve expectations over the target
distribution yes so can that be zero
done is it because I I mean I think so
there are it's actually though the
smallest nonzero I can value if it if
you do ab c easy like that in the case
of like infinite data the pseudo
likelihood is going to converge to the
same parameters so that that's a
simplification really it's that I mean
in in all cases if you have an over
complete representation really what
you'll converge to is something which
can be you know is of the same rank and
can be transformed but is it is going to
be equivalent in terms of the loss
robbers might not be the same but the
probability distribution rustling by the
two sets of parameters will be shirtless
hey right I think you'd always need it
right win wait if you have attractive
live ribs truly the learning has
so there's something intractable and
friends yeah yeah I mean if the
underlying graphical model is is it is
not a cyclic order multiples of local
minimum for the learning so when I talk
about Emily I'm not talking about using
approximate end friends so it would
still be convex um if you threw in some
some types of approximate inference then
certainly you run into problems with non
convex t um right but what about four of
luton videos right so for and I should
have saved perhaps the types of models
I'm looking at our like log-linear MRFs
and CRFs I should have said that arise
if you have latent variables that is
definitely interesting question and it's
something I and as well as the
semi-supervised case which I can't
really talk about here but and would
like to look more at in the future and I
there's some simple ideas which you can
immediately read from this composite
likelihood method which I wanted to talk
about later but yeah it this is not
immediately applicable to that yeah so
other than through something like um
right so right we had seen that
basically um the ordering of these in
terms of error and sample complexity is
what we'd expect ah looking at further
at the predictive power of these phones
you can see that terms of sample
complexity we have our bound like this
and what I'd like to say is that show is
first ignoring the log term which is as
you'd expect not that significant and
practice and fixing the number of
training examples what you might expect
to see if this bound is reasonably tight
is that the error increases at
approximately 1 over lambda men and what
I want argue here is that lambda men is
important in controlling the difficulty
of learning so here if you look at 1
over lambda men so harder problems going
to the right and the actual error on the
y-axis you can indeed see that for what
we'd expect to be easier problems we get
lower air for a fixed number of training
examples and vice versa so lambda men
does indeed seem to be important in
controlling difficulty of learning um
controlling it so basically generate a
whole bunch of random models and each of
those models is a point on this and
generate enough that we get points along
the whole life yeah so you thought about
using a different objective function
here like the error of do you find our
joint probability
the l1 area parameters because at the
end of the day is the probability that
great so our analysis sort does it and
do steps where we bound that parameter
error given number of training examples
and then the loss in terms of the
parameter error that second bound is not
that interesting but I do think that
going directly from bound on the loss to
the number of training examples required
would definitely be interesting to do it
wasn't super clear how to do that in the
analysis but that would definitely be
valuable yeah okay um by the way can I
can ask when should I go until okay
thank you good so in terms of what this
lets us do given that we have this
lambda man values which seems to control
the difficulty of learning family and
suitor likelihood what I'd like to say
is how this varies for different types
of models and so what L do is compare
this lambda men ratio between Emily and
pseudo likelihood so first looking at
model diameter and here I'm just looking
at chains as they increase in length to
the right and on the y-axis this ratio
between Emily's lambda men and pseudo
likelihood we're basically higher means
that emily is better you can see that
other than end affects the performance
of suit of likelihood as you might
expect doesn't change that much as
Jane's get longer so I'll call that not
a problem scenario in terms of factor
strength meaning basically the magnitude
of parameters or how strongly variables
directly interact with each other as you
increase that going to the right does on
a log scale you can see that this ratio
actually does shoot up so you can see
that for very strong factors pseudo
likelihood can actually run into
problems finally for no degree
as you increase the degree of a node
this ratio again increases albeit not as
quickly as with factor strength so we
might call that a problem scenario for
pseudo likelihood but while now talk
about is that we can often fix this um
using a method called composite
likelihood so emily is essentially
estimating the entire model at once
pseudo likelihood is essentially taking
each variable and regressing it on its
neighbors you can imagine the natural
generalization of instead take a chunk
of variables and regress them on their
neighbors and you can see that this
varies this generalizes both mle where
we have a single chunk of variables and
pseudo likelihood where each chunk of
variables is a single variable and so
what we show um are similar sample
complexity bounce and bounce for joint
and disjoint optimization just like you
saw for pseudo likelihood they take the
same form so i won't show them again but
i do want to emphasize that we analyze
structured composite likelihood meaning
that we really paid attention to how
these components were structured
especially with respect to remodel in
data and that's something which
surprisingly you haven't seen that much
I in previous literature in previous
literature often these chunks of
variables were just for example all sets
of two variables or three variables in
your model and it really benefited us to
look at the structure so the obvious
question is how do you choose these
estimator structures and what we do is
look to our experiments with pseudo
likelihood for guidelines for first we
limit each each component to trees it's
so that inference within that component
will be tractable we'd like to follow
the structure of our model in order to
avoid those failure modes of pseudo
likelihood where for example if we have
a star structure we'd like to cover it
with a single component if we have a
strong factor we'd like to cover it with
a single component and we'd also like to
try to choose large components or
minimize the know
BRR of components in order to be
intuitively closer to Emily than to sudo
likelihood beforehand which that it's an
interesting question and it's sort of
ongoing work how you can adaptively for
example get a rough estimate of your
distribution in order to choose a better
estimator and then get an accurate
estimate right um unless you have some
kind of prior knowledge yeah so for
example given this grid we might choose
these two combs to cover it where each
is a component in our composite
likelihood estimator and it sort of
follows our guidelines to the right so
showing some examples experiments with
this here it's a synthetic grid model
where we are learning with a fixed
number of training examples here i'm
going to show increasing grid size to
the right and on the y-axis comparing
the log loss of one method versus Emily
so here saying that pseudo likelihood is
just as good as emily for a single node
since it's the same but as the grid gets
bigger emily is actually going to do
better pseudo likelihood worse as we'd
expect now if we use these two combs for
composite likelihood we make a
significant gain but what's really the
clincher is that if you look at training
time emily takes a lot longer than
pseudo likelihood and using this
structured composite likelihood ah but
limiting ourselves to trees doesn't
actually make us require more time for
training yes approximate inference with
maximum likelihood estimation okay so
this is actually using an approximate
prints with exact it was not comparable
um here it was with good sampling and
there is certainly a lot of tuning which
could be done to ya huh I think it
varies based on the problem I mean
certainly you could do like belief
propagation or something um and yeah
it's it would definitely merit like
further comparisons um it there's a lot
of tuning to be done there yeah Trini
time please all do something yeah it is
but you're using exact for the student
for the composite you're using exact
that's right great so for this composite
likelihood essentially lower sample
complexity without increasing
computation just what we'd hope um and I
do want to emphasize that we estimated
these estimators were tailored to the
model structure and I won't talk about
it here but it's also useful to tailor
it to the correlations in the data the
strong factors where you need either
expert knowledge or some adaptive method
so to sum up here looked at finite
sample complexity balance for general
models and basically showed first that
pseudo and composite likelihood are not
yura sticks you can get real bounds for
those and this allows pack learnability
for certain classes and models and then
we looked at structured composite
likelihood where we gave some guidelines
for choosing estimator structures based
on failure modes of pseudo likelihood
especially trying to tailor those
estimators to the model in data so let
us do this sort of decomposition which
was a model specific and get the best of
all these trade-offs in certain cases so
I've been trying to argue is that we can
scale learning by using different types
of decompositions which let us trade off
quantities like sample complexity
computation and parallelization and
these decompositions used model
structure and locality and the
trade-offs were tailored to the model
and data so for example in parallel
regression we've decomposed over
coordinate updates and we were able to
choose the number of parallel updates
according to correlation in our data and
parameter learning we decomposed the
loss into sub graphs essentially and
we're able to tailor that decomposition
according to with the original model
structure and the strength of factors in
our data and then I also looked at
structure learning in the thesis where
we saw some similar trade-offs but I
didn't have time here so there are a
number of future work things I'd like to
look at first in terms of parallel
regression I've just talked about the
multi-core setting but I'm very
interested in the distributed setting
where you have limited communication
where you both want to limit the number
of messages you need to send as well as
at the size of those messages and
they're really taking advantage of
sparsity in those messages is a
challenging problem I'm also interested
in heterogeneous parallelism where you
might have multi core plus distributed
plus perhaps GPU and you'd like to
paralyze over multiple facets of your
problem for example both examples and
features there also a lot of interesting
questions when we start talking about
for example parameter learning use
phrased as this where you have more
structured objectives and for that I
think a really interesting question is
how did you partly joint optimization
with composite likelihood there I think
methods such as like alternating
directions method and multipliers might
be very applicable to add this so
another thing I'd like to look at in
parameter learning is I've sort of
talked about some guidelines for how to
do these trade-offs but there's a lot of
work which still needs to be done in
order to create serve an automated
learning system which can automatically
choose the structure and parallelization
strategy so there are a lot of
trade-offs in terms of Sam complexity
computation but also things I haven't
talked about such as in parallel ization
how you balance the number of components
you can paralyze the cross versus the
amount of communication required if
you're doing partly joint optimization
I think an interesting way of phrasing
this might be for example graph
partitioning where some questions are
how do you even estimate or compute this
lambda men given a model an estimator
cheaply of course and how do you balance
these various objectives and constraints
in this problem in order to choose
estimators and divide its components
across machines so the interest of time
I'll just get through this and mention
that there are a number of applications
I'm very interested in first in terms of
social network modeling I think that
there is a lot to be done in terms of
applying sort of graphical model
formalisms to social networks and there
i'm really interested in both in first
generative models which model both the
network structure and the semantic
content like text and images in those
networks and there i think they're lot
of interesting questions about both
scalability up to social networks size
and in terms of how to jointly model
those aspects I also interested in
looking at us some work in temporal
modeling and how to modify the methods
I've talked about for the
semi-supervised case I've also done some
initial work which I'm interested in
continuing continuing with machine
reading where an example goal might be
to build a probabilistic knowledge base
from text on the web and there I think
difficult questions are if say you
phrase this as a a big graphical model
how you do focus learning an inference
within that model as since you really
would need to prioritize with that scale
as well as how to do active learning in
order to keep building this knowledge
base but minimize the amount of human
supervision and interference required so
with that I'll leave a summary of this
talk here and thank you very much for
listening
have you ever get sploagies contracting
divergence right I've looked at it some
but I have not done a full comparison
and I think it's yeah it's a really
interesting case where it sort of mixes
ideas of from several cases like
stochastic gradient with approximating
the objective and it would definitely be
interesting to look at generation air on
the pulpit in sales for a distribution
is me but this thing I really want to do
is ask a question about what so maybe
perfect right so you have evidence that
we have lower sample complexity to which
given
um right so in terms of your asking
about going more directly from some sort
of loss to yes what yeah I guess about
is hard but finger right kind of right
um it is definitely an interesting
question i would like to look into it
more I think someone relatedly um would
one initial thing I've been thinking
about is how to make these films not
sort of like wide sweeping bounds over
all your parameters but really be more
parameters specific where you know you
can imagine some parts of your model or
easy to estimate in summer hard and that
in and of itself would let you perhaps
get better bounce with respect to the
Los you care about but also would
perhaps I may not fully understood your
question but I think would help in cases
where what you really cared about was a
particular part of your model and wanted
to estimate it well and then you might
have sort of model part specific bounds
i'm at miss Pelham ists also composite
like it the parallelism is done by doing
you know different part simultaneously
that's right that's how I've been
thinking about it okay so i think it's
not the good of shoe don't like it it
would be more parable that's great but
young anymore great okay so do you have
a kind of curve to show you know what
kind of speed up you get that car look
earlier usual right ah i don't i think
that in terms of disjoint optimization
the curves i showed really are
immediately applicable sense that's
completely data parallel in terms of
cases where you're doing joint
optimization but distributed that that's
something i'm very interested in looking
at UM and there i think really also more
analysis is needed in terms of saying
how a joint optimization behaves and
with the hope that you don't really need
to
um make it fully joint but can you know
essentially stopped early and come on
Dean told that at all in terms of I have
not in terms of parameter learning I
guess that's in a sense more straight
forward since you don't have to do
inference over the full model in terms
of structure and learning that's very
interesting and structure and learning i
was just looking at undirected</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>