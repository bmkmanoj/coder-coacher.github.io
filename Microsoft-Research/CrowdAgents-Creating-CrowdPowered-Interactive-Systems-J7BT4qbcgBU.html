<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Crowd-Agents: Creating Crowd-Powered Interactive Systems | Coder Coacher - Coaching Coders</title><meta content="Crowd-Agents: Creating Crowd-Powered Interactive Systems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Crowd-Agents: Creating Crowd-Powered Interactive Systems</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/J7BT4qbcgBU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay I think we'll get started we're
very excited to have Walter Lisicki
interviewing with us today from the
University of Rochester Walter's a
student of Jeff Bingham and James Allen
and he's no stranger to many of us here
he's done two internships and was an MSR
fellowship winner Walter also won best
paper a twist 2014 and his work is which
you'll hear more about in a minute is
related to continuous real time
crowdsourcing thanks for the
introduction and thanks for having me
out so today I'm going to be talking
about how we can use human intelligence
to augment automated systems that we can
go beyond the boundaries of AI and even
solve problems that neither people nor
machines can alone the resulting
intelligent systems can be used to solve
current and important problems today
such as providing better access
technologies to people with disabilities
but it can also scaffold future
intelligence systems when we are
thinking about how we build and train
them so why do we want intelligent
systems in the first place well we can
do things like have more contextualized
natural interactions with computers we
can offload some of the low-level
tedious tasks that we have to do in
focus more on our high-level goals we
can even convert one form of media to
another such as converting speech to
text in real time I have with close ties
with closed captioning now while these
are really interesting and they're very
generalizable impacts that these systems
can have I'm particularly interested in
how we can use these technologies to
actually help improve accessibility
whether that's accessibility for low
literacy or lowly low technical literacy
individuals who might need a more
natural way to interact with information
than the typical interfaces that we use
or even helping people with disabilities
such as people with motor or visual
impairments who need to control an
interface that isn't necessarily as easy
for them as it would be for us or even
providing captions for deaf and
hard-of-hearing individuals now the
problem with all of this is that this
requires solving a I hard problem
right these are problems like
understanding and tension in natural
language or understanding the content of
visual scene and we're just not there
with automatic systems just yet we need
human level intelligence but fortunately
in recent years can platforms like
mechanical turk have come out and
allowed us to actually access human
intelligence and insight very very
quickly using an API call now we can
start to think about algorithms that
integrate this human intelligence as
part of their function and we'll call
this human computation now I said that
you can do this quickly but originally
actually just took hours or days to get
responses and only in more recent work
by folks at Rochester and MIT has it
been shown that you can actually get
this down to actually just a couple of
seconds to get people to show up at a
task in fact I'd last year I release
some of the first publicly available
tools that allow anybody anybody to go
in and and recruit small groups of the
small subsets of the crowd in advance
that we can have people on demand
exactly when we need them so we can get
people to a task quickly but it's very
different to manage people in a
computational process if you compare
that to you know hardware is kind of
more traditional sources of computation
that's for a number of reasons first a
lot of people really use these platforms
because of the flexibility that they
allow in the way that people work so you
can arrive at any time as or here you
can pick tasks that are really the ones
that you're interested in and nobody's
saying that you have to stay around you
know you can leave whenever you want
which means that we might not know if a
person is going to complete a task or
come back for the next task that we
would think of as being part of the
series an equal once we get a response
it's not necessarily sure that we know
what the skills of that individual are
how much effort they're putting in maybe
they're even trying to game the system
and sometimes people just make mistakes
or have a system configuration that
actually leads to them seeing the
problem differently than we expected
this means that we don't know if we're
going to get our task completed we don't
know what order they're going to be
completed in and we really don't know
what the quality of the answer is even
if we get it so the field is really
focused on how we can solve these
problems by taking a task breaking it
down into small context-free units of
work that only take a few seconds or a
few minutes actually
complete is called micro tasks and then
using that divisibility in that context
free nature to actually let workers on
these platforms take the task in
whatever order they feel is best now
this works pretty well but then we
realized once we get these answers back
we don't have a good way to confirm that
they're actually correct in most cases
it's actually just as hard to confirm an
answer is correct as it is to solve the
problem in the first place so instead
we're going to use the fact that these
are small units of work that are to
actually design easily comparable pieces
and what that means is that we can
actually collect a set of responses to
each of these questions and use a simple
aggregation scheme such as voting to get
a more reliable answer that we can we
can really have faith in now this basic
idea has been used in a wide range of
systems that we've seen so far so things
like coming up with arbitrary images for
arbitrary image labels for images on the
web answers to visual questions for
blind users even finding just the right
moment to take a picture with a smart
camera it's also been used for things
that are non visual so just as an
example and NLP and linguistic task
finding linguistic annotations for text
data sets editing documents or even
translating those documents to another
language and this is really just a small
sampling of the space here there's a lot
of work within the HCI and AI
communities on this these types of
systems and really we've even seen in
the past couple of years the entire
conference sponsored triple-a IH comp
and even a age comp Journal arise to
support these this community but while
micro tasks are extremely powerful they
are fundamentally limiting the types of
tasks that we can think about
crowdsourcing and using human
computation as part of the process is
not everything can be broken down into
these little pieces of work so in this
talk I'm going to claim I'm going to
make the claim that we can actually
create real time systems that support
multi turn interactions even though we
have a very dynamic crowd we can
maintain context and we can maintain
interact with these systems now I'm
assistants builder so I built a number
of large web based systems for
coordinating workers in real time around
a single task over the last four years
and
for each of these systems we kind of
look in a way that we can solve a very
generalizable problem a whole space of
problems while also solving a specific
instance of a challenge that we had not
known that we could use crowdsourcing to
solve before and then for each one of
the systems i've built I've looked at
you know what are some of the other
problems in that space no of the actual
application space what are some
generalizable properties about the crowd
that we can use to inform the design of
future systems and even how do we get
around some of the hurdles that we see
in deploying these systems in real
domains so since I can talk about all of
this in one talk I'm really gonna focus
on explaining how maintaining context is
critical to a wide range of tasks then
i'm going to introduce a new type of
task actually not micro test but a
continuous task that keeps people
engaged for longer periods of time and
goes beyond a lot of the assumptions
that we made for micro tasks and then
i'm going to talk about how this allows
us to start thinking about going beyond
the traditional impetus for
crowdsourcing which is we have a task
that people are good at but computers
really aren't and even looking at tasks
that individual people are not
necessarily good at now I'll talk about
some of where I want to go with this
work in the future so start with
maintaining context and I'm going to use
the example of viswas which is this
system that was originally introduced in
2010 that allows people to blind users
to take a picture speak a question and
get an answer within about a minute
which at the time was with
state-of-the-art I help deploy the
system in 2011 and the resulting data
set of almost 80,000 images asked by
thousands of blind users is is really
fascinating for a number of reasons but
in this data I found a set of cases
where it really shows that micro tasks
are not the end-all be-all of human
computation and I'm just going to give
this one example here where a user asks
what are the cooking instructions on
this TV dinner this is actually a pretty
popular class of questions and
understandably the information needed to
answer this question is very hard to
frame for a blind user there's no
tactile feedback on the actual package
that they're taking a picture of here so
they don't know that they didn't capture
the cooking instructions so when the
system goes out and recruits a worker to
answer this question they say well I
can't see the instructions but then they
usually will try to give a little bit of
feedback on how this
be corrected so they say well why don't
you flip the box over so the blind user
flips the box over again takes a picture
but the system but then they ask how
about now but the system doesn't
necessarily recruit the same worker it's
not sure that that person is always
going to be available so instead it goes
back to the crowd posts another micro
task gets a different worker who then
doesn't know the original question so
they say something very general like oh
well it's a box of food so the user goes
in fixes this realizes what the problem
was they react the question take picture
again um we go the system goes gets
another worker and they say well I can't
see the answer so why did you flip the
box so fur and these kind of thrashing
interactions can actually take a very
very long time in fact even though we
get answers to each question back in
about a minute it turns out that this
typically takes about 10 to 15 minutes
before our user can actually get an
answer to their question or they just
abandon the task all together in this
case so the real core problem here is
that the end user is trying to have a
conversation with the system that
maintains context when the system
doesn't actually support that type of
interaction so I'm going to focus in on
this problem and look at how we can
solve the challenge of actually holding
a conversation that goes and takes
multiple turns and maybe even spans
multiple sessions we go away and we come
back tomorrow and want to continue that
conversation from where we were so we
explore this I looked at a created a
system called chorus which actually is a
virtual personal assistant powered by
many crowd workers behind the scenes now
to the end-user this actually just
appears as a chat conversation with a
single individual they have a pretty
standard instant messenger window and
they're able to interact with the system
now behind the scenes workers also
participate in this process of curating
a kind of working memory so these are
important facts it's kind of a
predictive task where workers try to
speculate on what would be important to
future workers who are completing this
task so that might include capturing
things like allergy information or
location information about the user that
they've shared in the past now they
actually hold a conversation it's a
little bit more complicated than just
justing a chat message instead we
actually of a collective process where
workers are all proposing and voting on
one another's answers essentially have a
rolling voting scheme and to incentivize
people to give us the the right answers
the best answers that they can we have a
mechanism that ask people to propose
answers and if they actually propose an
answer that's correct we'll give them
three thousand points they don't get any
points if other people don't agree that
they have given a reasonable answer to
this task or given the current task now
if since it's a little bit easier to
just vote for one of these answers and
it is to actually go out and find
information and bring it back if a
worker votes on an answer then they'll
get a thousand points if again that's
marked as correct as to make sure that
people don't just randomly guess at what
the answer is if they don't know we also
added in this kind of no op bonus it
gives us a little bit of a gives workers
a little bit of a reward for not doing
anything and of course it's pretty easy
to figure out when people are abusing
this now one of the nice things here
about this incentive mechanism is that
behind the scenes we're actually able to
tune a single parameter to figure out
the the spacing between these rewards
and what that will effectively let us do
is dial in the verbosity of the crowd so
how many different responses do we want
do we want a large set of maybe creative
responses or do we want a pretty narrow
set of highly reliable responses and
while there's some issues if you look at
the actual game theoretic properties of
this mechanism need to find that there
are multiple equilibria but empirically
this actually does work to encourage
people to propose more or less answers
now what does this all mean in the end
it basically means that we can filter
down the set of responses to just the
ones highlighted in blue here and we we
really remove things like like this line
where somebody comes in right in the
middle and says well how's everybody
doing and that's clearly not what we
wanted in the middle of the conversation
so other workers catch a catch on to
this and they filter out that response
so to the end-user we only see this
consistent conversation that is actually
accurate and is sensitive to the correct
points in the conversation that we're
talking about and actually what we've
said in the past and how that how that
together we brought a dozen users into a
lab to actually study how the system
works we gave them a a high level
objective of what they want to
accomplish we didn't give them a
specific script since it kind of defeats
the point and we ran this within
subjects design study now the results
were really interesting and I'm just
going to give one example here to give a
flavor of house of this turned out the
user comes in in this case asks about
some activities that they want to do in
houston now they basically asked what
the prices of doing this set of
activities and after a little bit of
clarification the crowd is able to come
back and tell them okay that'll cost
$150 now interestingly while that's
answer to the question that the user
actually asked a moment later the crowd
I she comes back and says well actually
there's a better answer there is a city
pass available from the local tourism
office and that'll allow you to do
everything and actually a little bit
more than what you were expecting and
it's cheaper so this is an answer that
the request for the end user did not
actually expect to need but it was kind
of Sarah nervously found by the system
buddies collective search process behind
the scenes that can discover information
even if the the end you didn't ask about
it explicitly and because of the
standard is but is information finding
we actually find that a majority of our
participants preferred chorus to a
keyword search like being now more
quantitatively we can also we also say
that this system is able to stay on
topic it's able to give reliable answers
that are very accurate answer what the
what people are actually asking about
and interesting Lee we're also able to
recall eighty percent of the facts that
we're actually from a prior session that
means no workers were present when the
user originally said a piece of
information but they were able to recall
it using this working memory window
eighty percent of the time so we didn't
have to re ask we didn't have to kind of
rehash the same ground so it is possible
to support this type of multi turn
interaction even though we have a very
dynamic crowd behind the scenes that's
constantly changing and if we look at
the roles that we actually have people
completing in the background we have
given a question some people who are
going to propose a new answer and others
who are going to help filter that answer
down so that we only forward the correct
dance
to the end user now it's easy to see
also how we can use the same structure
to include things like dialogue systems
and it doesn't matter that this isn't a
person now if it produces the right
answer then humans will go in filter
this and make sure that we're only
giving good answers to the end user
still but if it produces an incorrect
answer in this case this is the the
wrong City although it sounds like the
right City the same name but it's a city
instead of a region then workers can
check on this find out it's not the
right information actually proposed a
new answer and then filter that so that
we know the human answer is correct and
not only do we prevent the system from
ever giving the end user and incorrect
response we kind of prevent the system
from ever corrupting the as users
experience we also get the advantage of
actually having this as training data so
where we would have normally derailed
the conversation and gone down a
conversational path that doesn't really
fit what the end user wanted to
accomplish we can now stay on track and
see what would have happened even after
the system makes a mistake and yet we
still know we made a mistake because the
system's answer was not voted on and
forwarded so this gives us kind of a new
way to think about how we deploy and
then train AI systems in the wild so now
I want to talk about a new kind of task
a continuous task that goes beyond a lot
of the assumptions of micro tasks and
lets us solve a whole new space of
problems and just as an example of
something that I think we can't solve
using micro tasks alone I'm going to use
the example of driving and driving a
robot right this is very important for
home assistive robotics where you might
have somebody with a mobility impairment
who needs a little bit of help actually
accessing things in their home but how
would you control this with micro test
right it's a naturally continuous space
we have be able to sense the environment
we have to be able to listen to the user
and decide what actually we want to do
about that we want to provide continuous
control to this robot which does not
expect discrete input and we have to
actually be able to respond to events in
the environment as we encountered them
so as a proxy for this home assistive
robotics task I'm actually going to use
this little off-the-shelf rovio robot
where we have you know a little bit of
an obstacle in the way and we have to
drive around the obstacle and get to
target and instead of breaking this down
into little pieces we're actually going
to give workers control of an actual
streaming video interface that's from
the front mounted camera on the robot
and we're going to let them use the the
default controls that the system comes
with which are basically just arrow keys
and a few other simple controls the
problem is if we do this and we give
control to a single person what we find
is something like this where a majority
of the time we actually fail to complete
the task because people will join give
it a good first shot to solve the
problem and then maybe in this case you
know they they fail to complete they
crash into a wall and then they abandon
the task this is a top-down view of our
setup in our lab with it with a trace of
where this particular instance of the
navigation task went now this is because
there are going to be you know there's
going to be some noise in the system not
everybody's going to want to stay around
and complete this entire task and if
it's not easy to do they might just have
a higher rate of abandonment if we use
the fact that we have lots of crowd
workers available to actually just pick
a new one from the crowd and let
somebody else control whenever they
abandon like this we get something like
this we find out there are trolls on the
internet basically so somebody here
navigates the tab the robot almost to
the target it was back and forth and
back and forth and back and forth and
then abandons the task somewhere there
are a few minutes later and finally a
different worker will connect and finish
the task now this of course was
something that raised our completion
rate in this case because eventually
somebody will finish most of the time
but if we could drive this robot off a
cliff that's pretty problematic right it
only works here because you can't
destroy the task basically so what we
really want to do to make sure that we
don't have this kind of malicious input
or kind of otherwise noisy input is
usually aggregate people's responses
right if we use but simple voting in the
case where we break something down into
micro task we want to use something
similar here but a lot of the standard
approaches for dealing with continuous
input don't necessarily work here I use
the example of if this the robot drove
up to a tree it can go left it can go
right those are perfectly good options
but we don't want to average now so
instead we're going to take a vote but
that requires actually discretizing time
so this is going to be a much slower
approach where we need to
for one second of input look at the most
popular answer and then use that to
control the robot and the problem is
still that we're not picking a
consistent stream of options that any
one worker would have selected so we get
a lot of conflict early on here where
when we're trying to decide how to get
around this barrier do we want to go
forward first we want to go to left
first people end up thrashing a lot in
this case so that doesn't quite work so
to solve this problem actually going to
borrow a page from representative
democracy and we're going to pick one
leader who is in control at any one time
but then not make that necessarily the
same person over multiple periods in
time and i'll show you how this works
briefly if we wanted to really solve
this problem we would have to solve this
pond vp where we really want to get
agreement between different workers
policies but of course we can't see what
people would do at every point in time
and in fact there are different
representations in people's heads so we
actually don't even know exactly how the
states line up with the real world so
this isn't very tractable instead we're
going to try to approximate this by
looking at people's input over time
comparing it to the rest of the group
and then picking as are the most
representative of the the group to be in
control so given we have some input and
we have some funny properties of this
input such as we can't really assume
that these two up commands just because
they look the same are not necessarily
the same we can't assume the same
because going forward a moment later
might be a very different action than
going up at the first moment so instead
we're going to bend this over a one
second time period look at each workers
set of inputs for that time period and
then use a similarity function here a
cosine similarity to actually compare
them to the rest of the crowd right so
we get some score how similar they are
to the rest of the individuals who are
contributing and then we'll use that
score to actually update a weight and we
do this for every time point and we do
this for each worker in the crowd which
means then at the at any given time
point we can just pick the highest
weight worker and give them direct
control as the kind of the
representative leader for that period
and then as we keep updating these
weights we will get potentially a new
person in control at
given one second span but we won't be
interleaving peoples and put on it on a
finer-grained scale so the system i
created to do this was called legion and
legion was able to not just control a
robot but actually an arbitrary user
interface by letting you select a
portion of your desktop you have a
natural language command for what you
wanted to complete it and then it would
actually go about the task of completing
that with the crowd we've used this to
control anything from office software to
assistive keyboards for people with
motor impairments they even letting
multiple people play a single player
video game collectively now the idea of
actually being able to keep people
involved in a task for a longer period
of time also points this idea of a new
way to maintain context right if we had
one person here for the entire task then
they could remember what was happening
same thing for the conversation so to
set up a little bit of a test for that I
actually used a video game setup we
created a custom map here and basically
the idea was we had people using lija to
control a video game character they were
walking in the cyclic map it took about
30 or 40 seconds to get from one
decision points of the next and the
decision they made was essentially push
the white button or the black button
depending on some prior instances that
they saw now we actually ran this for a
full hour and it turns out that we could
complete this task even though people
were not present for more than a couple
minutes at a time we could complete this
task consistently and reliably the
entire time here you see an input where
there are a graph of people's input you
see workers on the y-axis there and each
red marking is basically a point at
which the worker contributed some input
to the system and we see that you know
maybe workers stays for a few minutes
and then they leave and you know
somebody else joins for a few minutes
and they leave but the way we were able
to actually control this for a longer
period of time is that if you look at
when people actually join a task versus
when they start to contribute we see
that there's basically this
synchronization period where they are
watching what other people are doing to
learn how to complete the task and then
they use that to continue on with in the
same
and and this is kind of reminiscent of
organizational memory from the behavior
literature where people this is
essentially the process that
organizations and societies actually
used to pass traditions down and other
types of behaviors so this is it really
gives another interesting way to capture
this idea of context I talked about so
far to two very different ways to
accomplish this idea or this goal of
maintaining context over time so I want
to zoom out for a little bit and talk
about some of the more general framework
that this fits into specifically if we
wanted to create these systems what is
the what is the architecture what does
it look like we're always going to need
a way to divide these tasks either into
small pieces or into actually roles that
people synchronously coordinate and
complete we're also going to need a way
to actually collect input from workers
at an interface that lets us get input
from contributors and really
contributors could be human workers or
they could be automated systems that
know how to do either all or a piece of
this task and in either case will need
reward schemes to make sure that systems
are incentivized correctly or workers
are paid fairly for their efforts in the
system then we need to think about the
communication channels between people
and how we actually think about
maintaining context without allowing
collusion without allowing two people to
really coordinate to the detriment of
the incentive mechanism that we're using
and then we're always going to need to
actually aggregate these responses what
they were thinking about controlling an
interface or about holding a
conversation we don't want parallel
threads it exists we really want a
single control string be really want a
single conversation so this idea of
having a single input in a single output
is kind of the same idea is having a
collective that acts like a single
individual which will call a crowd agent
and this idea kind of harks back to this
formalism kind of harks back to the
agent architectures that we see in the
AI literature and Priya prior work now
this gives us a way to really think
about how to design these systems in the
future and how we can kind of design for
interactivity specifically so then I
want to I want to go back to this
example that I started with at the
beginning of providing visual assistance
to blind users and this problem that we
saw where people were thrashing over
kind of Corrections and follow-up
questions and think about how we can
solve this using crowd agent framework
so do this I create a system called view
that actually engages the blind user
with a crowd of workers who actually
hold an ongoing conversation you see on
the left here about now not just a
single image but actually a streaming
video so they can now give multiple
Corrections they can stay around to see
what the result of their a feedback to
the user was and it makes it a lot
faster to hone in on exactly where the
correct information is so if we're
looking at tasks like finding
nutritional information ingredients
allergy information things like that
week actually reduce the time that takes
on average from 10 to 15 minutes with
the prior single image visible system
down to about one or two minutes we're
going to get an order of magnitude speed
up here because of this interaction okay
now everything I've talked about so far
and really all the prior work in the
field is really focused on how do we
complete tasks that people are good at
but I want to go beyond that and
actually look at cases where individuals
might not necessarily be good at a given
task that we want to complete but
computers aren't up to that task either
so we need to kind of start using the
collective ability to go beyond what we
can do individually and as an example of
this I'll use the problem of real-time
captioning which is a very important
accommodation to Ford F of heart
definitely hard of hearing users but
it's really very difficult we need a low
latency only a few seconds per word that
requires typing hundreds of words for a
minute because people speak very very
fast and we need the input to be
accurate so if we take a jet and
generally if we take a person probably
hood is that they're not going to be
able to do this task very well I don't
think any of us in this room could
actually keep up but trying to type this
talk for example dive out this talk now
I'm specifically at a focus on a
classroom scenario we have a single
speaker and at least one person in the
audience who needs this accommodation
and currently a automatic speech
recognition while there been some really
amazing advances recently is just not up
to this task because of the variety of
different speakers the different speaker
conditions may be speaker has a cold
for example um the different lexicons
that we might be using the vocabulary
that we might actually have in a given
lecture and the different acoustic
properties of the environment that we're
not sure of any of this in advance
really makes this a very challenging
problem and automatic speech recognition
does not reach the legal accommodation
under the Americans with Disabilities
Act for being a valuable service here so
instead the current state of the art is
actually using people in fact it's
computer-assisted people professional
cart captionist seeeeee are individuals
who have trained for years to be able to
type hundreds of words for a minute
within a few seconds of each word but
because of this training and because
they're so rare it means that they're
very expensive it costs a couple hundred
dollars an hour to hire these people and
it they're not easy to schedule because
again if you have to find somebody with
this rare skill could take 24 to 48
hours notice and that's kind of the
standard accommodation latency that you
see in like a university setting for
example now this not only means that
it's difficult to afford these
accommodations for for people who have
to actually provide them disability
service offices at universities but it
also means that we can't always access
them not just because we had this this
lead time but because of the cost itself
so a lot of times you'll see that
universities don't actually accommodate
students that they want to work in teams
after class right the in class is
supported but it's not a legal
requirement support after class of this
is not something that that is typically
done or or feasible given the budgets of
these offices what we'd like instead is
for students to have the ability to kind
of walk into a class walk into a group
setting and pull out their phone and get
captions immediately so to do this I
created a system called scribe which
actually streams audio from a user's
mobile device to a server that then
breaks this task up over multiple
non-expert workers I think coordinates
all of their input and aggregates it
back together to provide a single
caption stream within a matter of a
couple of seconds now this means that we
can make these captions available
anytime in fact because we're using
non-experts they're highly available
they're also cheaper we can pay students
in ten to fifteen dollars an hour this
is not something that they've spent
years training for it's just anybody who
can hear and type on sir to get a group
of people to do this it still costs
about a core
the price of a professional and we can
now use students or other volunteers who
might actually have subject matter
expertise so for capturing computer
science computer science talk or
mechanical engineering talk we can find
somebody who actually know something
about that domain rather than has a
specialty just in typing but how do we
actually coordinate people to do this
task the simplest high level version of
this would be to think about round robin
right so we start with one person give
them a few seconds of audio that we want
them to type and then say don't worry
about typing while somebody else is is
taking here the tasks as well hand this
the second person say ok now you have a
few seconds to to type and so on and so
forth and eventually the first person
will catch back up so we can integrate
them back in and have them type a few
more seconds of what they hear of course
this type of coordination requires a lot
of interface accommodations for figuring
out how to coordinate people and
actually prompting people to input at
the right time so I want to show you a
little bit of how scribe does that in
this video and specifically I want to
focus on the fact that typing will lock
in so I ask people type you'll see it's
kind of just go gray they can't go back
and edit because this takes far too long
it increases the latency by too much it
also provides a lot of cues to type in
some feedback in the term in terms of
bonus points and rewards to encourage
people when they're doing a good job and
this can map to to money in the case
that we're actually using Mechanical
Turk workers or other paid crowds to
contribute to this task you also notice
that the volume will increase a decrease
to help more with the saylon see in
these this idea of queuing people to
type at a certain time and that's not
working
okay this is supposed to have audio i'm
not sure why way generally entertainment
oriented and paid for through
advertising although increasingly by
subscriptions now cable television and
finally the current big thing the
internet which came to us starting in
nineteen sixty-nine now the aim that
itself has been alright see ya but less
content than I expected there but it's a
yet the idea we're prompting people that
you have a bit of a salient cue in the
volume as well as some direct visual
cues now as you see here I should people
still make mistakes and it's Prince did
you speak about that okay people still
make mistakes because this is still a
challenging task even for those those
few seconds but somebody speaks too
quickly we actually go beyond sums
working memory and to make this easier I
looked at what people who do offline
captioning actually would would do and
it turns out that they slow down the
audio now if line capturing is the same
task as real-time captioning but
basically without the time constraint if
you give me the captions tomorrow that's
perfectly fine so while slowing down the
audio works in that case it's not
necessarily true that it can work in
real time you'll obviously fall behind
so we'll use the fact that we have
multiple people not just a single
captionist to actually speed or slow
down what one person is hearing to say
half speed that's just for the section
that they're actually typing basically
so if they're supposed to be typing a
certain segment we play it at half speed
if they're not we take advantage of the
fact that people can listen a lot faster
than they can type I'm actually speed up
to maybe one and a half times while
other people are responsible for the
content this means it actually we can
play at the audio at half speed for
everyone while they're supposed to be
typing so everyone hears it slowed down
and you still have that context we can't
remove the context in between these
segments justice that people are
actually worse or almost as bad as
computers when you do this so by
allowing people to kind of hear a more
approachable version of the audio we can
actually increase the recall rate and
the precision pretty significantly on
these tasks and what's even more amazing
is that we actually see a decrease in
the latency so we're slowing down the
audio and yet
getting faster responses and the
high-level reason for that after we went
back into interviews with a lot of the
workers we were engaging for this task
and actually looking at some of the data
it turns out that basically when the
audio is too fast for somebody to keep
up with what they'll do is they'll
listen they'll memorize everything and
then they'll start typing typing after
their segment has stopped playing so
basically in the little context period
where somebody else's typing that means
we pay this shift penalty where we don't
have anybody we don't have the person
typing at all until after we've stopped
the audio clip turns out when we are
able to slow this down it becomes a more
tractable task and people can type each
word as it comes in more often so we get
this decrease in latency you want to
show you quickly what that looks like
this will be another video um basically
the same as before but now with this
playback speed adjustment engagement on
people next network a collision network
which arose during my lifetime
television as you know is a one day
Emily editing nor handed paid for
through advertising although
increasingly advice scriptures now with
Kate floating and finally the con big
time to unit current big thing is the
internet okay so people can adapt very
quickly to this it can follow along and
can actually complete this caption task
easier but there's still a pretty
challenging task here people can still
fall behind they might miss a word they
might not know a certain words it's
pretty hard to caption it but we can't
scale the same approach that we have we
certainly have a lot of people available
now because anybody who can hear and
type but we can't scale this division in
round-robin approach to use too many
people because if you imagine you know a
tenth of a second to a whole large set
of people the tenth of a second of audio
is not make it easier to caption this
this content so instead we're going to
do is rushing it redundant input we have
a few seconds but worker but now we're
going to get a few workers / a few
second segment and collectively this
allows people to very accurately vary
with a lot of high recall excuse me
actually provide these captions a
ninety-five percent of the words that
are said we can capture with
eighty-seven percent precision and we
can she do this in under three seconds
of worker
time for word latency this is actually
kind of on par precision with a
professional and better recall and
latency yeah so the cost of course does
increase we have that kind of a lot of
flexibility in the price we were paying
or we already at a quarter we can do
this with a combination of not just
workers but actually also ASR which I'll
get mentioned a little bit but it's
still much cheaper than providing this
using professionals is much more
available and and higher accuracy
because of this expertise or subject
matter expertise specifically but of
course the problem with it is that now
we have multiple streams of captions
which as we talked about before does not
work for many applications and turns out
if you bring students in to actually try
to read captions read you maybe 10
seconds of captions it takes them 45
seconds if those are parallel captions
they can reconstruct what was being said
but it's so slow that we lose our real
time constraint just on the reading
alone so instead what we want to do is
actually add this combiner phase where
we're going to merge all of the boards
that people have said back together and
and create a caption that is kind of
easy to read and a single caption and we
need to do that using multiple sequence
alignment which is this process that was
originally used in computational biology
actually to realign genome sequences and
while this will figure out where we have
gaps and where we're missing words and
maybe even allow us to align words that
we want to compare and and denoise in
case somebody had like typo for example
it unfortunately it all of the existing
work on this has been offline so these
are kind of dynamic programming
algorithms that allow us to do a late
binding answer but we have to have all
the input that we want to align first
which doesn't work for a real-time case
so instead I came up with a graph based
approach that actually construct a graph
as we see words based on which words we
see immediately next to each other in
different workers input weights the edge
and we can actually go back with the
language model Andrey wait the edges so
that when we find the highest
probability path this results in a
pretty reliable caption and will this
work pretty well and actually works at a
much much more general constraints more
recently we have been using this a
star search based approach with a beam
heuristic that actually lets us more
accurately control the computational and
time resources that we use for a given
alignment and more integrate the
language line alone more principled way
so this gives us our single caption and
we actually been able to show that this
is very useful and even I use this in
real domain so we captioned a number of
conferences including w-4 including
assets last year which you see here this
basically are our screen on the left
we're captioning for the entire
conference and the speaker's off to the
right somewhere there and interestingly
instead of hiring Mechanical Turk
workers instead of bringing people
explicitly into this session to provide
captions we were able to go into the
audience asked for volunteers and get
five students who had never used the
system before about five minutes in
advance of this session they all sat
down and they were able to actually
create their produce pretty good
captions even with only a few minutes
notice this really points to the idea of
kind of democratizing access access
technology right now anybody who's
interested friends peers family members
can help with this task whereas before
it really wasn't viable to do so yes
people were in the sessions eventually
wear headphones or something so they
could hear this slow right in this case
we weren't using the time war for the in
class or in session participants but you
could imagine doing exactly that I only
seen headphones yeah so there's a little
bit more challenging a test we didn't
get that one well then and mostly we did
that because there is still in this set
up some server latency basically we have
to stream the audio to our server that
warps it and send it back so we didn't
want to add that like one second of
blatant see ya and presumably because
I'm attending to talk it's something
that I'm interested in but did you get a
sense of the people that were doing it
how much it detracted from their
experience yeah that's a that's a very
good question we it is often difficult
to follow content we r 0 than using five
people here so that's still a reasonably
intensive task on one way that we've
looked at so here you do see a little
bit of loss if we're using for P
it would be significantly more what we
found is basically if we want to use
volunteers in a classroom where we
actually have students and turns out of
vast majority of students would be
willing to help appear as long as it
didn't hurt their understanding of the
content so what we can usually do is
find Neil 30 students in the classroom
and make it so that you only have to
type a few seconds every couple of
minutes which then makes it very easy to
follow along and it doesn't detract at
all and and the the general approaches
that we use here can actually generalize
the things like coding behavioral video
in a much shorter amount of time then it
would usually require if you were using
an undergrad so a couple minutes instead
of a couple of weeks in this case and
even to activity recognition settings
where we actually need high-speed action
labels to ensure we understand what's
going on in a environment in real time
and because the high-level takeaway is
that instead of selecting a single
person or a single answer to actually
include in our final output we want to
synthesize our answer from a set of
different people and this allows us to
go beyond what one person is able to do
and I should stitch back together
something that that is a higher quality
so now I want to talk a little bit about
where I want to go with this work in the
future I really started this talk by
describing how micro tasks is allowed to
still use human computation in settings
that computers can't operate a loan you
can get labels to images we can answer a
lot of these simple task it works great
for batch process tasks or anything
where we only need a single response and
in this talk I've shown how we can
actually greatly expand the space of
problems we can use human computation in
by looking at continuous tasks now how
we support interaction over multiple
terms of interaction with an end user
but we're still looking at these systems
that we create these intelligent systems
that we create as tools and I really
want to look at in the future how we can
use kind of mixed initiative principles
to go beyond this idea of asking a
question and getting a single answer and
actually work more deeply with the crowd
actually get the crowds insight into
problems as we work even when we don't
know that we had a question that needed
answering and it's just a one space that
I'm going to explore this in I'm gonna
use an example of smart prototyping so
basically taking an initial
napkin sketch and seeing how fast we can
turn it into a functional prototype so
in in work that's appearing at Chi this
year I built the system apparition you
see a little video here and it's
basically a platform for exploring
intelligent prototyping tools the user
can sketch a rough version of what they
want while describing it out loud and
the system will automatically convert
their sketches to real elements as as
you go so this is basically like
describing on a whiteboard and ending up
with a more realistic interface here
we're prototyping a simple like
platformer game you can think of Mario
and the user is able to describe some
things sketch some things and the crowd
is working behind the scenes to actually
make this into real content or or update
the grass that it's green in this case
now and after about a minute we have a
sketched prototype but we want to add
functionality so user creates a
character describes a basic behavior so
it should follow where they click and
within three seconds the character
actually has that behaviors you can see
they click and the character follows
where they said to go now they can keep
using this behaving they don't have to
keep Reese PESA fying it every time they
interact with the system the system
remembers how it's supposed to work you
can see here they play a simple game to
get across the other side have
programmed code for making a fast answer
is that because the crowd workers are
doing like Wizard of Oz so here we're
looking at basically a collective Wizard
of Oz pro process where people are
coordinating to control different pieces
of the interface and in fact we have a
lot of new interaction techniques that
make this possible where prior
synchronous drawing systems actually
didn't support truly synchronous editing
right so things like Google draw for
example don't really work in this
setting there's a lot of different
things to coordinate workers but it is
kind of a more freeform task than
typical and um we're able to actually
get some more flexible system support as
well so not just workers coordinating
but actually also the system tries to do
some gesture recognition for the drawn
elements and actually will post it to do
item here on the on the side what it
doesn't know so that work
can take that task in and help complete
it so this is really cool we're still
again this is this is still using the
idea of using the system as a tool so I
want to go beyond this and look at how
we can take some of these prototypes
we've created and let the crowd helped
us create improved interfaces and that's
maybe in this case importing more
thematically appropriate content so we
don't actually have to go back but the
crowd can help us figure out what makes
the most sense in the setting we
described and this can actually be very
useful for blind developers who might
actually need otherwise help from
sighted peers to come up with like their
finalized version of the interface even
though they have an intuition of what
they want we can also use you the fact
that the crowd themselves understand why
certain things happen you can see and
and start to capture that idea more
formally so maybe things like kaleido
balsamic the character can't pass
through but but actually the sun's in
the background and people understand
that intuitively but the system just
sees polygons we can also capture the
idea of playable characters and maybe
some of the relationships and actions
that they can carry out and even what
happens when those actions are taken and
they interact with one another and we
can use this idea of using guy was a
formalization layer that helped the
system better understand what's
happening in the world to actually start
to predict what might happen in the
world and if we keep a small frontier we
can start to borrow a page from a kind
of predictive parallel computation and
try to guess what will happen precompute
this using people on in the crowd so go
ask what might happen if I reach this
state and when the user actually gets to
that state we can actually have a zero
latency response where the computer
already knows it just has to apply
what's about to happen so we can start
looking at zero latency crowds in the
same way that going from hours to
minutes in the minutes two seconds open
up whole new spaces of applications I
think actually zero latency or a few
millisecond responses will do exactly
the same and then kind of behind the
scenes how can we use this understanding
about the system and the crowd has about
the application domain to actually
template out code or generate code
either from scratch it from
demonstrations or
or even from or even help p of workers
and users import generalized examples
from offline crowd sourcing platforms
and community exchanges into specific
examples that we want to work
automatically and I think these mixed
initiative systems will help us go
beyond just a naive mixing of these
three incontri contributors and actually
look at how we can best take advantage
of the strength of the crowd the
creativity of the crowded creativity at
the end user and some of what i can do
to understand and reason about the
domain so I've talked about how we can
support ongoing interactions with the
crowd in this talk and really focused on
the fact that consistency is key to
success we need to be able to do then
and all the systems i showed you need to
have consistent output that doesn't
conflict' with itself but actually and
respects kind of the the progression to
get to that point in the interaction and
to design systems that do this I've
introduced the idea of real-time crowds
actually as a crowd agent and introduced
an architecture for how we can create
these systems and I'm really excited
about how we can support richer and
richer interactions with the crowd via
things like mix initiative principles
thanks
yeah so when I kind of budget kind of a
comment see I saw your attractive crab
powered systems when you were driving
the robot around and in many ways felt
like twitch plays Pokemon um did you
find that there was a way to kind of
exclude rolls out of your system
automatics yeah so so one thing that
drove me nuts about twitch plays Pokemon
well it's an awesome example basically
this is a game that was set up I guess
last year yeah who's that allowed a lot
of people to play basically a gameboy
version of pokemon but it was 10,000
people all playing at once and kind of
just throwing input at the system and I
think it had one of two modes right it's
kind of mob control and a vote although
it wasn't clear exactly how they were
voting i think they were also binning by
time so this was basically two of our
base lines one thing I didn't show is
that mob was actually something else we
tried that doesn't really work because
you again lose consistency over time
what we see is that when we're actually
learning the weights and using the the
crowds collective and voice to actually
vet workers we are able to very easily
identify people who are trying to to
differ from the rest of the set and we
can't guarantee that those people are
malicious but since things like pokemon
or this navigation task or are
relatively straightforward in terms of
making progress people who are not
contributing that direction are pretty
easy to detect so yeah we can do that
well yeah to find again what exactly
meant by message of Christ were saying
because I wasn't totally clear on why so
the earlier systems you talk about
didn't count as mixed initiative like
it's like the one worth it find people
took video while the course of workers
worth I like them seems similar to know
the yes I example so that is that is
actually kind of a nice example of a mix
initiative interaction but it's not
something we necessarily designed for it
was kind of a byproduct of the way we
designed the incentive mechanism and
because we had this rolling voting
process where we didn't just restrict
people to kind of a single contribution
we were able to get this inside
same thing with the chorus example we
would come back and actually suggest the
citypass even when the user didn't ask
for it so I really think it's about
focusing on trying to design the systems
rather than taking advantage of kind of
what naturally occurs right so so yeah
in those systems we weren't that wasn't
an initial attention and do you think
that in systems like that is it
important that the the end user who use
that they're actually attracting with a
single actor or with an intelligent
computer system versus that they know
they're interacting with the crowd of
workers is it important to hide that
effect from that word oh um so we were
mostly focusing on hiding this fact in
the context of trying to make sure that
we didn't have multiple threads of
interaction right it's confusing to
actually hold multiple conversations at
the same time so from that standpoint we
don't want to feel like we're talking to
a set of people but at the same time
once we have some of this filtering it's
actually it turns out that it's actually
useful to end users to include some
information about how confident the
system is so we can tell people yeah
there's multiple people hide the scenes
and this is like the level of agreement
on to give an answer and people find out
informative for selecting so the
combination yeah so crash with some
research has exploded over the past
several years what do you see going
forward you see that the types of tasks
and activities that people are applying
to crowdsourcing staying somewhat
consistent or you anticipate that we're
going to see this would deviate off in a
new direction um I think we've only seen
a fraction of the actual applications
that we could create even this idea of
moving towards more mixed initiative
systems and I should collaborating with
our intelligence systems is something
that really hasn't been done yet even
though we keep claiming okay we have an
intelligent system right it is very much
designed from the tool perspective so I
think there will be a lot of brand new
applications not just kind of
solidification of prior work I also
think that hopefully the platforms will
become better and make it a little bit
easier to to work in the space actually
what did the platform's being a veteran
yeah well right now for example
Mechanical Turk really doesn't have
great real-time support we were kind of
hacking the system a little bit to pre
recruit our own crowd that has these
properties which is very hard to scale
and very hard to it's actually it's
interesting it's hard to scale and it's
hard to one off there's coming like
middle level where we need some set of
workers but we don't need too many and I
think that if the platform actually
natively supported this kind of task
routing to individuals who maybe are
available at at a given time or at least
put a lot of people to actually be kind
of on call um we would see more people
doing work in the space actually yeah a
lot of the works he presented kind of
invented crowd and from the users
perspective pretty intimate settings
like in for blind people if i did a
bedroom or in the transcription since
the content might be something that you
don't necessarily think this want to
spread to the entire crowd have you
thought about my privacy yeah yeah so
we've actually done some of the first
work in this space with Jamie and AJ
actually where we were looking at how
much you attack these systems how might
malicious workers in these system
actually be a threat to the end user and
while we're not seeing high levels of
kind of malicious users of people who
would actually do bad things with the
information now certainly that will grow
over time it's it's also true that a
single individual is not necessarily the
biggest threat because a lot of these
denoising properties can can at least
prevent bad answer so again the the
blind user case where we actually want
to make sure that we don't miss inform
the end-user that often gets filtered
out with kind of the same approaches
that we're using here if we want to
prevent people from seeing it we're now
looking at how we can create intelligent
filters that I should use people but
never reveal that information but I can
talk about a little bit more offline if
you're in you're curious but
you you mentioned zero latency proud
how's that possible but my gosh they're
very right so the idea here it would be
that as we start to formalize more and
more of the information about the domain
that in this case we've just created we
just sketched out the system can can
start to basically run a planning
process over what might happen so where
could we go from this point that the
user has reached where do we expect to
go kind of based on prior interactions
and once we can start to narrow down
that the size of that guest set we can
start actually pre-compute maybe on the
3 2nd frontier that's our current
latency what might happen get that
answer and then have it ready when we
reach that state to immediately fire so
we're talking about a few milliseconds
or ever it takes the computer to now
respond rather than whatever it would
take to don't be surprised by that new
state then go out get an answer for that
exact moment yeah that's somewhat
related much different not really chorus
point about where these things are going
right so one of the things you talk
about a lot in is real time and the
effects interactive systems is things
right and trying to get that down way to
zero and so I guess also I've seen in
the in the number of different things
you've done is that some will still feel
like you have to do specialize the only
this problem and this time work in kind
of solution and find the right way to
finish these are the problem is
different like do you envision that we
will have like sort of more general
solution to this problem reducing
latency or is it going towards just
building like to settle 58 different
little kinds of things that we like you
know like are there generalizable
solutions is it kind of every new
problem is still a new one are doing
this exploration phase like what's your
sense of yeah so I think what I focused
on here is it kind of in the broadest
sense focusing on the types of input and
output so going back to this crowd age
and architecture what types of input can
we can deal with so if it's a stream
string that's faster than people can
individually contribute to that kind of
thing so I think there are those broad
classes of if we see the same problem or
the same kind of
properties of the problem we can use the
approach there are also going to be
these like more fine-grain optimizations
like time warp where we look at how
people complete this task or some of the
specific human factors to completing
that that task that we can then talk
meant this with so described is a good
example in that it's a reasonably
general approach actually something
we've done very different things like
activity recognition using a very
similar process but of course there we
didn't use things like Time Lord I'm
gonna take it in a different direction
um to mechanical turk is related
crowdsourcing crowd workers so it's
about getting the crowd to work to help
me complete a task now the robot driving
is a little different this one with the
twitch pokemon game that's more sort of
crowd participation so it's not that I'm
doing a task for you it's like I want to
play this game with the crap yeah has
there been much of those types of
activities where it's not that I'm just
looking for somebody that I'm I'm doing
this for my own enjoyment with other
people and yeah so not so much within
the computation space you do see kind of
other things in crowdsourcing that maybe
or you can even think of things like
under sourcing or community events right
as a type of this right we're all
working towards a common goal but
there's not necessarily a computational
process involved in that I think that it
is interesting that the like Jolie play
a game set up in Legion is a
self-directed it's not a signature that
the robot driving was right so there we
actually were specifying the goal so
we're looking for a process yeah but no
but the gaming is a very good example
using the same system and there we were
actually course not hiring people but
lighting people you kind of saw in the
picture uh sit on a couch right and
collaboratively play this game so I
think that we can transplant some of
those ideas from the computational space
to can the more general self-directed
crowdsourcing I also think that the
systems like apparition are trying to
use this like one level abstracted
conversational process where we know
people are computing
something computing here using air
quotes basically just as their mental
process to get to the response that we
need and that's integrated in a certain
way but there it's much more undirected
and what we can learn from kind of team
work settings informs how we did the
coordination in settings like a Persian
where we don't have i know i need to do
X right it it's kind of up to the
workers to figure out what they need to
do yeah last comments okay last time it
was something that just sort of made me
think about AB do you have thoughts from
the workers perspective of what it's
like to do these small tasks or
participate in these things you know
because when you're thinking about
playing the games that sort of us being
it rather daily right right right
external work um yeah I mean there is a
lot by type of task so what we see is a
lot with a lot of the assistive
technology work people are actually even
more engaged than by micro tasks and
feel like they've contributed a bigger
piece when we use continuous task but in
general knowing that they're doing good
is is something that helps I think that
in general from the worker side kind of
this line of work we're looking at a
more continuous interaction with the
system also avoids a lot of the
interruption problems that we see in
traditional micro test crowdsourcing and
actually there's a paper at Chi this
year that looks at some of the kind of
detrimental effects on workers to having
their workflows constantly interrupted
in different ways that are pretty common
currently when we use micro tasks and
the punch line is I can take people up
to twice as long which of course they're
not paid differently for so with just a
little bit of the wrong routing it can
essentially cut their their effective
pay rate in half so this is trying to
get away from some of that some of those
problems thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>