<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Plenary 4 Keynote: Data Challenges and Opportunities in the Next Decade | Coder Coacher - Coaching Coders</title><meta content="Plenary 4 Keynote: Data Challenges and Opportunities in the Next Decade - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Plenary 4 Keynote: Data Challenges and Opportunities in the Next Decade</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fvm7AEXr6yQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so welcome to this session this is the
only panel discussion session of this
summit it's called data challenges and
opportunities in the next decade but I
emphasize to the panelists that they are
not restricted to just talk about big
data but in fact anything about machine
learning where big data may play
important role and in fact to take a
step back and look at the future look at
the next 10 or 15 years on the format of
the panel is that we have four panelists
each is going to speak for exactly five
minutes and then we will have about 15
or 20 minutes to have the panelists ask
questions of each other and then we'll
have about 15 or 20 minutes to have
questions and answers from the audience
so I'm going to in order to save time
because I know some of you are pressed
for time to leave early i'm going to
introduce all the panelists at once um
before I do that let me just share with
you the three questions i asked the
panels to think about and i would like
to ask the audience to think about
questions as you're listening to the
panelists that you would like to ask
them and during the Q&amp;amp;A so our first
panelist is sitting to your left
Michelle paws nard is the chairman and
CEO of inria um he is his research
interests are in algorithms parallel
computing grid computing networks of
automata and neurons now all these
panelists have multiple awards and
honors and titles and so on so I'm just
going to say one or two about each in
order again to say Tom I hope the
panelists are not offended I so uh
Michelle is a one the Charles Babbage
award by Triple E and he is a night of
the French Legion of Honor
I want that tight okay Lana Tarasenko is
sitting next to Michelle he is chair
professor of electrical engineering and
Oxford University his research interests
are in signal processing techniques
especially as applied to diagnostic
systems for example for jet engines and
health problems he's a fellow of I EE
and a raw and the Royal Academy of
Engineering um he received the british
computer society medal for work on
neural network analysis of sleep
disorders so that's Lionel commander of
the British Empire Michelle ease night
initial thought I should mention the
command of the most excellent Order of
the British Empire CB next is Ian book
 who is the clinical professor of
public health informatics at Manchester
University his research interests I have
to read this you'll understand why later
are very broad in particular and
bringing together statistical and
mathematics biomedical social economic
and computational thinking to health
care especially in epidemiology he's a
fellow of faculty of Public Health a
fellow of the royal statistical society
and last is Eric Horvitz who is a
managing co-director of the Microsoft
Research Lab in redmond his research
interests or a machine learning and
intelligence with length to decision
sciences human-computer interaction
biomedical informatics and information
retrieval he's a fellow of many
societies including having just been
elected to the National Academy of
Engineering and recently elected to the
chi Academy so that's Eric
so we will start with Michelle and I
forgot to introduce myself I'm Jeanette
wing I'm head of Microsoft Research
International okay some that's our
challenges and opportunities next decade
so first I just want to what one or two
slides on inner yeah so you know it's a
french institute for research in ICT and
while doing research development
education training and transfer
innovation and it's rather large
Institute institution since we have more
than 3,500 scientists so why big data
today so I just want to say again what
Rick didn't yesterday so then the
overwhelming amount of data are
generated by all kind of devices is all
networks or services so so it's a it's
something that is is coming that is
present and and we have to deal with and
the challenges we see with big data is
easily scale to 22 velocity to speed
because there is a continuous stream of
data datura not a very rajan 80 and and
also complexity of this data so i was
asked an example of machine learning in
practice so i would point out one work
that has been done at ino as are several
on searching in image databases and and
we are now capable in lesson in a less
than one second to to unser carry on
more than 100 million of images and
that's clearly a big change
opportunities over the coming year so of
course the opportunity comes from the
difficulty so there will be a
more and more data and these data will
be richer and so just 12 22 points that
I want to underline the linked open data
on the web and and the data amount of
data generated by all kind of devices in
what we call internet on earth what are
the greatest research challenges that we
see I just want to point one specific
domain is to solve the two opposite
requirement in terms of human workload
so we need of course weekly supervised
learning in order to deal with the large
amount of data and from the other size I
mean data is nothing if you cannot
transform it into reliable and trustable
knowledge so these reliable and testable
lineage Lee needs the human validation
so the trade office is her how much
human knowledge you have to put in this
and three specific domain I'm a
underline is a knowledge discovery
visual analytics and geometric inference
so one slide on each of them in
knowledge discovery we have to to deal
with knowledge representation and
reasoning and we need work on adapted
algorithm for these in visual
exploration of big data is a big domain
and and interaction with this
visualization is quite important and new
and also geometric inference and
topological data analysis it's quite
very important so the last word in sin
in synthesis big data is not the prime
of of of the amount of data is that it's
correspond in fact two big challenges
and I we can discuss this later thank
you
so i can start whilst Jeanette is
loading up the pictures because we are
on the type time scale in terms of
prediction and with niels bohr
prediction is very difficult especially
about the future so I'm going to talk to
you about some things we've learned over
the last decade we shall probably map
out on to the next decade secondly I'm
an engineer so I'm passionate about
getting real world non-consumer and
you'll see why non-consumers different
from what i'm calling the real world i
hope by the end of my five minutes
applications of machine learning
community of jose and this day of am UK
laundy school so a picture's worth a
thousand words and they i don't know how
machine translation would deal with that
from the original in French but i'm
going to show you lots of pictures
because that means i can save Huard so i
started working rolls-royce in 97 on the
trend 500 and we did machine learning
for sensor data fusion these are the
fuel oil flows temperature pressures
vibration sensors on the Trent 500
engine we analyzed data from the first
ever run on the testbed of the trend 500
as well as thread 500 we've done the
trend 900 the Trent 1000 rose rose
doesn't care who wins in the aircraft
walls between Boeing and Airbus because
it supplies engines to both supplies the
trent 900 to the airbus a318 and the
trem 1000 to the Boeing 787 Dreamliner
so what you see in the middle here is
actually the first endurance flight to
Australia of the Airbus a380 where we
analyze the whole of the data and the
whole of the endurance data so we've
built real systems based on machine
learning for rose royce which are used
today in in their various data centers
and and in fact actually rose rose don't
sell engines they sell you told total
care package these days because of
course it's very important for the
airplanes to have as few engine failures
as possible and therefore they have and
they mention here in this advert from
rolls-royce half a billion reports being
streamed back to the operation center
annually but actually you'll be
surprised
little machine learning there still is
in their data center and the reason why
is that in order to do machine learning
you need to be able to do for reasons I
can explain the discussion you did you
need to do it on the wing to did on the
wing however you have onerous
regulations whether in us to the Europe
about actually having algorithms that
analyze airborne data and the
certification of on wing software is the
lengthy process we only in the second
iteration of the systems we designed for
Roger so key issue if you want to do
machine learning from the lab into the
real world applications in this domain
is certification that's also true in the
medical area where we've been designing
patient monitoring systems we believe
that our system was one of the first if
not the first machine learning
application to be approved by the FDA we
had to give three predicate devices to
the FDA in order to get that approval
and secondly to get the alerting
software approved we are to do a
clinical trial so certification in
health care whether big data otherwise
will require regulation by the FDA will
probably require clinical trials
demonstrating improved patient outcomes
big data in healthcare I think two
things that perhaps haven't been
mentioned very much this meeting sofa is
the time scales that you're looking at
10 orders of magnitude from 10 to the 9
roughly the number of seconds in a
lifetime for genomic data all the way to
acute hospital data once per second in
between there you got self monitoring
data and I pro finally disagree with
Herman Hauser about what he said into
the self monitoring data we can come
back in the discussion you can ask me
why I disagreed about what he said on
self monitoring data and a very
important we mentioned a few times maybe
he's being slightly forgotten how do we
incorporate prior knowledge into our
machine learning algorithms couple of
examples very simply I'm not going to
explain this slide this is the slides of
the vital sign the physiology of the
recovering patient after surgery it
happens it's not the left hand side that
matters the parameters themselves but
the variability at the right time scale
because it's all about recovering
homeostasis after surgery and that's
indicated by whether you
main highly variable or not or you're
settled down gradually as in the green
traces for those who are going to
recover and not end up back in intensive
care self monitoring of patients I'm not
going to explain this except the red dot
this is patients we asked to monitor
their conditions because they've got a
significant respiratory condition COPD
we can get false order simply because
they decide to monitor at a different
time of the day that they normally do
die innovations may be bigger so how do
you incorporate that into your machine
learning algorithm only by understanding
enough of the physiology of the patient
that's and I think that's something very
important and I think we need to focus
on the incorporation of prior knowledge
into our machine learning algorithms
deal with multiple time scales and if
you want to get it I there in the real
world certification is a big issue thank
you very much
Thank You genus
so I'd like to take a public health
perspective and start by asking the
question were does public health and
health care need machine learning and
point to some hard problems that i don't
see addressed at the moment big need
current medical evidence predicts less
than a third of what will happen to a
patient when treated or what will happen
to the average patient in the average
treatment consider mr. Jones over here
type 2 diabetic high blood pressure
chronic kidney disease goes to see the
note goes to see the dermatologist so
the diabetologist focuses on his blood
sugar focusing on the blood sugar put up
his weight putting up his weight puts up
his blood pressure putting up his blood
washer put stress on his kidneys so he
goes to see the nephrologist with
advancing kidney disease the
nephrologist focuses on blood pressure
control to control points three
different care pathways one indicating
how to control mr. Smith weight from
family medicine but mr. Smith is not the
sum of three different sets of
guidelines or three different rule bases
is the union of those highly interacting
factors and the interactions point to
two-thirds of the evidence base that's
not covering his needs so some myths how
do we bridge that two-thirds gap the
ingredients we needed data models and
expertise we have a tsunami of data we
have a blizzard of models methods and
peck through papers in the medical
literature but we have a drought of that
precious resource for abstract reasoning
expertise to build the usefully complex
models in the big data so in the health
science community you could say that big
data is the solution is a common myth at
the moment you could also posit that in
the context of big data in health
science the idea that science will
provide all the models to transform the
data is an unrealistic expectation and
that clinicians will continue to be the
main source of data is also prominent in
the mind of many
scientists we give some examples of the
myths so here we have type 2 diabetics
on the vertical axis we have the blood
red blood cell concentration people down
here a fing tired because they have
anemia they're feeling miserable and
tired with a low red blood cell
concentration this is due to their
shrinking kidneys sending less of a
signal to the bone marrow to produce
more red blood cells if we want a large
data set to look for this tipping point
between shrinking kidneys and reducing
red blood cell concentration we have to
get an indication of this kidney
function that is commonly done through a
blood test creatinine agent sex but we
don't just need a larger database
because the transformation is nonlinear
one with age sex and creatinine into
estimated glomerular filtration you need
to know the lab assay to get the formula
right for the transformation common
metadata that are missing from big
databases borrowing strength reaching
outside science into the data that's
being generated increasingly in the
service so this is from a scenario from
the world's largest respiratory trial
here's a drug company this is
glaxosmithkline over here putting
forward for testing a new beta agonist
combined with steroid help people
chronic obstructive pulmonary disease
that's a once a day dose versus a twice
a day dose more convenient business case
predicated on that convenience also
interacts with the deprivation that the
patient experiences the public health
team over here know that the deprivation
scores in sulfur dwell this trials
taking place are unreliable they've
recomposed them ideally we'd have an
Amazon like environment where users who
selected those data also did these
things to them we'd be able to borrow
strength from multiple data
transformations touching the same data
people generating data from their own
experiences here's a trial of an at
least gets a free Nia to try and reduce
the relapses gets free nearby be giving
people a heads up there they're going
off their medicines that interacts with
the set of care pathways but to make
this truly engaging and not irritating
and switched off you need an underlying
cognitive model so this is a real
application of machine learning with
colleagues at Microsoft to some of the
data
trying to take an alternative approach
to the usual either or hypothesis driven
or data driven to take a hypothesis
shaping approach to debug a comment
debunk a common myth the common myth
here was that children were either those
are than allergic tendency all those
without an allergic tendency and there
are many diagnosis in medicine for
example asthma there's not one diagnosis
it's many underlying conditions so we
take Lauren with a bank of allergens at
age 13 five and eight we're trying to
infer here with infertile net the
probability of gaining or losing
sensitization and by looking for the
structure in the data in this reasoned
way we came up with five classes instead
of to those predominately non dust mite
does might multiple late multiple early
sensitization just know that this has an
odds ratio multiple early sensitization
the subsequent development of asthma of
around thirty one very large signal in
the data was not anticipated at the
original stage of hypothecation so are
shaping hypotheses so I'll finish by
putting forward some challenge areas I
see for machine learning to focus on
areas of public health need I could use
a portmanteau term here of assisted
reasoning we need machines to infer that
people are touching data in ways like
other people to connect their abstract
reasoning to build those usefully
complex models in ways that do not rely
on the stretcher in the data alone thus
we get this heuristic in health science
to discover those underlying subgroups
for more personalized care as ender
types we need machines that can give the
heads up that someone else is touching
the data like you and not only that
these factors that may not be in your
shopping basket of data might be
relevant to your investigation but also
this type of transformation can add
value to your interaction with the data
and that will create a series of
usefully complex models we then need to
hinge that useful complexity between the
N of 1 model of my health
how I responded previously and the
average patient model which is the
current medical evidence pace thank you
for listening so I've been reflecting
about the genetics questions last a
couple of days here I have been
particularly excited about four years
now about systems that are immersed in
real world settings that continue to
learn in an automated manner over time
and that not only observed but leaned
forward using active learning to
understand the best way to learn and
they continue to probe at the borders of
their own competencies and this includes
work that we're doing where systems are
online and reacting to shifting sets of
data over time buzzing adapting leaning
forward where most of that work right
now is in the lab but some is out
already we have systems around the world
that are doing reasoning and learning on
in a semi automated way about patient
outcomes the nearest by here is
Southampton right now up in Southampton
this is happening as we speak of our
traffic prediction work is doing the
automatic work in general having systems
that can reason about to do this that we
thought the perceptual skills and the
intellectual skills of both people and
machines together again it's another
challenge area that working on in this
idea of leaning forward and buzzing on
multiple opportunities ahead over the
next decade I'd like to see us takes
more seriously decision making in
machine learning and I don't just mean
adding a loss function as some people
think decision making and learning is
it's really deeper integration of action
into our models and back propagating
that to the methods and the focus of
attention of where our algorithms do
their work with data it's great to see
Francis Bach here former intern on our
group one of our best in terms of all
time his microsoft research project was
learning directly the receiver operating
characteristic curve and optimizing the
area under the curve directly and
propagating the actual effort into the
algorithms there it's also great
opportunity to develop
situated autonomous learning systems
that continue to adapt to decide about
scientific problems that help human
beings in the broader endeavor of doing
science that even a proposed and pursue
scientific hypotheses that do
discoveries multiple data sources and so
on Arnie remember applications there's
quite a bit to be done on learning about
people to support them to enhance
interaction to kop them at them in
numerous ways think about systems that
can learn about what will remember and
forget what are attentional focus is at
any moment what's a display when will be
surprised this is a very rich
opportunity area now I thought I'd end
with a grand challenge as a broad an
important challenge instead of
technologies and principles we need to
develop as a community and I'll allude
to the intuitions and passions of a
great Swiss French scientist and a great
Parisian of the past Jean Piaget there's
a great opportunity and anybody who had
kids sees that what we're learning and
what our field learns in terms of
classifiers and predictions is quite
different than what our children learn
out of the box or into the into life say
quickly and there's opportunity to
master methods for moving beyond
classification and pushing on
likelihoods and inferences about classes
to learn about rich actionable knowledge
in the world of interrelated concepts
and this is going to take advantage on
several fronts but I see glimmers of
directions here in terms of transfer
learning multitask learning causal
inference and quite likely by folding in
natural language processing on concepts
but powering up an insensitive incessant
active learning on a path to learning in
a cross-domain manner leveraging the
different domains as language models for
one another now as a sample direction
I'll stop with you wanna get more
concrete of this grand challenge how can
we move from the current focus on
competencies with the task of
classifying images in imagenet tens of
thousands of categories to move beyond
is a hierarchies to move beyond
inferring maybe objects that might
co-occur
statistically to a rich understanding of
ontology of relationships dependencies
and causation in these images among
objects in the scenes will likely have
to leverage distinctions and fundamental
knowledge about human centric action
affordances we'll just have to
understand the nouns of our world the
verbs prepositions the knowledge of that
comes from social relevance and setting
even defined why a picture was taken
what was going on in that scene there
break these ahead I'm optimistic we're
going to develop these deeper more
comprehensive and more holistic
learnings about actual knowledge across
domain and I think when we get there
when this community gets there we look
back upon the last 25 years or even 30
years of work and think a lot of these
pieces were enabling but that really
wasn't learning I'll stop there can you
hear me ok now I'd like to turn to the
panelists and ask them if they would
like to comment on each other's comments
so we can start with Michelle and one
question 22 you know you you you tackle
engineering and health and in both cases
we can discuss the role of the human in
the loop if you take the airplane is it
necessary to have a pilot or do we do we
need a pilot and the same for healthcare
I mean we still need doctors well I'm
sure you know plane that flies from
Paris to New York is actually flown by
the autopilot for all the flight by
about ten seconds so and in fact this is
the very shortly after takeoff the
machine takes over from
from the pilot and although the pilot
lands the plane there's actually no
reason why the machine couldn't land the
plane so I think it's also to do with
human psychology we've got the Google
car which is driverless we've got in
Oxford the robot car which is also
driving us and it's it's human
psychology how would we feel first being
in a car that has no driver most of us
in machine learning community you
probably be reasonably happy as long as
we knew what algorithm was being used to
had deep or shallow it was to be in the
car in a plane I think we may still have
some reservations but we're people who
understand even what the word algorithm
means the general public out there you
know in terms of the psychology and as
that's what i wanted to mention about
healthcare there's not only the patient
physiology that matters but also the
patient psychology and because when the
point I was trying mate implicitly is
actually we talked about algorithms we
talked about computing resources the
most costly aspect in 10 years from 20
years now will not be the computing
platform will not be the time spent
developing the algorithm it'll be
actually acquiring the data acquiring
the data unless you're looking at you
know social media applications the way
people interact with technology but
actually in healthcare or even on a
plane and so on acquiring data becomes
perhaps the most expensive part of the
system and that lesson aside in terms of
doctors and so on if you have enough
data then you can make pretty good
inferences but I think we always want to
have a safety net it's a human reaction
do you know that if something happens
that the pilot can take over from the
autopilot and likewise it is probably
true that most machine learning
arrogance would do better than the
average primary care physician but we
still need that face-to-face interaction
in order to be told what's wrong with us
so it's more psychology than feels I
mean
comment about these day of the data
being expensive yes in a world full of
data we still have the challenge of
expensive data and acquiring it cleaning
it using it I don't see very much at all
at this meeting on active learning and
when you do see active learning it like
an icml meeting it's often very
simplistic we just published paper last
night actually on the web that looks at
active learning as a rich planning
problem and so making our systems aware
and even with sparse data giving them
the ability to go out and figure out
which data they want to grab given the
cost of the data healthcare for example
very critical especially if I can
interview maybe people about what
happened in the past it's not encoded in
an EHR so I think this is a really rich
area to push on hard making our systems
aware and active and understanding where
does sort of extend their tentacles yeah
the information content the data that
you don't have right how'd you grab that
data is the cost in there good enough
modeling it's an optimization point
between getting in an accurate measure
at one point in time and getting a less
accurate measure but more frequently you
did true utility of longitudinal data it
has not yet been fully exposed and the
models to get that optimization I showed
a smartphone app there that had one
element of a questionnaire so there's a
trade-off there between asking the user
to respond to too many elements to the
point where it annoys them and I'll turn
it off to having an incomplete response
to that psychometric instrument and then
supplement that with with with further
questioning it's a very complex temporal
structure open in that modeling problem
one of the illusion of human and expect
and especially expert that is there is a
they are trust in themselves that they
are right when you when you go when you
see a pilot or doctor or any expert it
will tell you this is the truth and
unless another expert says you're not
rising and what a machine a machine is
not doing machine is just taking one
hypothesis among a set and and giving
probability to losing side of things I
think that we should overcome this this
point and this is a this will be such a
transition in health system and rather
than consulting one specific doctor or
surgeon to have a team we're a team with
machine and where the patient or the
group of passion could have a series of
hypotheses on his diagnosis and on the
treatment so that it can choose and this
is the point I want I want it to I a
related question which it has to do with
trusting the data how do we we are going
to be building these applications in
safety-critical domains like health care
and jet engines and and they're going to
be based on analyzing using
and so on on data so how how do you
address the trust issue for data and
beyond trust we have applicability if
you anybody gets a 23andme study and you
look at the risk factors they'll publish
on your on your single nucleotide
polymorphisms and go down they'll tell
you each study they've done and why and
it'll be like an Asian data set which is
being used to tell me what my stomach
cancer risks are and they'll just put up
a graph and say this is what we have so
far and and they'll assume
transferability and portability for you
to pearl and so the idea of it not even
beyond trust how do we know we have
representative data for the situation we
care about I'm not saying that the two
things very briefly before we are videos
I think you're right al a lot of people
now go to the doctors selling the UK
after they've googled their symptoms and
and their disease so in fact it's
becoming the consumer driving as
consumer of health care so if you talk
to the average primary care physician
the site they hate most is google
because it gives them a lot of
difficulties because patients come
either with some kind of grand truth or
and then the whole consultation is spent
trying to say is the doctor bed and
Google or vice versa so this is
happening more often but it's driven by
by the consumer in this case and I think
it will happen more I think what is
needed on those and I don't know if
anybody's proposed is some kind of
external validation of the healthcare
information because there's some good
websites for example diabetes the
diabetes UK website is good bits plenty
of websites that actually tell you
information which is wrong because it's
not regulated so I think some validation
that information maybe by the experts or
some other means would be useful in
terms of trusting the data as well I
think we need to go back to what we're
doing the 70s and 80s sensor modeling
you know to trust the data you trust a
sensor in the first place and the other
one is data fusion so if you have
multiple redundant sensors if there is a
sensor problem then you'll be very clear
that that sensor is giving erroneous
information so i think sensor modeling
and some form of censored data fusion
helps to build confidence in the data
how about you
redundant machine learning algorithms on
different algorithms on the same data
set so they talked about trusting the
algorithm I guess there are some methods
got a control theory and decision
science on stability and robustness and
one way would be to look for sensitivity
to things like algorithmic changes and
to using different kinds of data and as
you put it back on the data or the
feature set how stable are the
inferences how deep are you into an
action space that's not changing as you
as you change the algorithms or other
parameters you're using versus sitting
on the edge of it or a cusp where as you
shift aspect of the analysis or the data
you're changing the recommendation
recommended action in the world and you
know sensitive at that point it's you
know you need to sort of be more careful
you can inverse problems to the trust in
the information on the mechanisms of
producing information is is the public
good the contribution of your own data
your personal data to the public good to
sure up some of the weaknesses
particularly expensive areas of capture
let me give you a personal example my
both of my parents have pulmonary
fibrosis and my mom recently died of
lung cancer both in pulmonary fibrosis
and lung cancer there is uncertainty as
to whether a food supplement
n-acetylcysteine is helpful or not and a
lot of physicians around the world will
recommend patients and say the trials
are equivocal on whether the
inflammation that causes pulmonary
fibrosis can be dampened by taking an
acetyl cysteine it's not prescribed a
lot of the time the patients ask to go
to the health food shop there is new
biology just come out that the end
stages of cancer so-called cachexia when
the body is consuming itself can be
dampened because mitochondria will be
inhibited by an acetyl cysteine so there
are natural experiments taking place out
there all of the time I was certainly
myself and my family be very grateful if
the data of from from our family we used
or contributed to the public
good but we need beyond the clinically
coded data to mobilize public support in
those natural experiments and just as we
talked about an instrumented web and
future systems that reveal how our data
are being used in a very connected way I
think the evolving knowledge that is
highly motivating can help mobilize some
of that very precious data capture that
revealed one comment on your example is
that there's a chasm between the formal
clinical trial which are expensive and
you know you involve small n and low and
the larger informal platforms like
patients like me right now that are wild
west and what I would love to see and
I've talked to Jamie who founded
patients like me our folks in this
community for example and people in the
related clinical communities working
with these scruffier platforms to
introduce statistics and methods that
can really mine from the larger data
sets to bridge this shared crowd-sourced
healthcare data with the formals you get
a verified clinical trial ok I think I'd
like to open up the
I'd like to open up to the audience now
for questions and answers by the way it
i think it's purely coincidental that
three of the four panelists are experts
and have interest in health care so I I
don't think we're limited to just
talking about health care but um we
might as well take advantage of that
expertise so let's start with Rick well
actually I was just going to ask a
question that on a different topic which
is I've noticed in several sessions
we've talked about applications like
health care we're up engineering you
know we haven't really talked about
education but that seems like a place
where where there's a huge opportunity
for machine learning to be able to build
systems that could educate people better
than people do or at least in certain
areas to do a potentially better job by
being able to diagnose what individuals
know what individuals don't know present
them with the best possible material so
i'd be curious if the panelists have any
any thoughts about that we're talking
about getting getting rid of physicians
and getting rid of pilots what if we get
rid of teachers or at least help them I
mean I think the with the work going on
in online courses and with flipped
classrooms is a tremendous opportunity
to collect really valuable and large
amounts of metadata about the process of
learning and education there's an
ongoing discussion about whether or not
the various entities like EDX and
Coursera will be releasing their data
making it available even to the teachers
of those classes and I have to say that
I just saw the first paper from
published by Coursera I think Andrew and
Jonathan Wang and others one of at
daphne's postdocs at Stanford with very
promising results on peer grading and
peer assessment from Coursera data so
we're seeing some good work in that
space starting I think it's a great
topic area yes a comment I mean
in this type of in education and
learning there is a passive learning and
active learning and of course for I
think four passive running I mean get
rid of the of the professor in the butt
for active learning it's quite different
because in active learning you need
maybe it could be done by a machine but
it's it's a more clever task because you
need to be to be to be correct corrected
so it's a role the role of the of the
parents in the in the in the children
education you know could could a child
be educated only by machine I don't know
not quite sure but Michelle I mean I
mean the the biggest piece I think of
act the active part of learning is
motivation and engagement we could learn
about that of course so I think
obviously the US is leading in this and
i have several mike postgrads taking the
courses that some likely on Coursera and
so on and the experience has been very
good I think what I would say about
education as indeed with healthcare is a
lot of our machine learning is based on
population based models and one things
you have to worry about other tales and
there's often something specific about
why that person is either doing
extremely well or is doing extremely
badly and often these models because
they're based on large numbers of
samples and effectively a modeling the
population and may not be so good so I
would say yes less by all means continue
to do this but let's also improve our
modeling of the tales because there are
some human stories in each case both in
disease and in learning both the top and
the bottom end that may need human
intervention to deal with the tell so
the large proportion say you know within
three standard deviations the me neither
side will be fine augmented by
occasional human interaction because you
know your fees have got to go somewhere
not just the and I know the fees in the
u.s. probably even higher than the fees
in the UK for for education so
there will be some financial argument
that about interacting with the
professors but I think also the
interaction about why these are those
people are in the tails which may not be
revealed simply from the online
interaction so basically carrying on
from this sort of point that you made
about this Black Swan effect right where
you don't see the data in the table now
Eric mentioned that there could be a
more active learning approach where you
actually go and search for that data
which is missing right but in certain
cases it might be very difficult to get
those examples now if you are training a
self-driving car you need to be able to
see you what sort of things cause
accidents right the variability human
behavior and to basically sort of info
intent of humans and that seems to be a
very very difficult problem because
there's so much variability in those
observations right so do you have any
sort of thoughts on how one would go
about sort of making sure that we can
sort of certify that my machine learning
system is going to be robust against
those sort of observations and those
those are a phenomenon well I'm asking
difficult i think we get into almost a
domain of extreme value statistics so
effectively these are extreme cases and
there's two schools of thoughts you can
do extreme value statistics from
learning the distribution of normals and
studying the tails using weibull
distribution with assumes gatien's for
normal or you can throw where everything
you know by the normal and concentrate
on is very rare events I tend to be at
the latter school because I think
especially in health care there's no
reason why the tails are actually
anything to do with a normal
distribution so how do you concentrate
on those rare events and acquire that
data but I think it is very important
because you probably can't infer very
much from normal behavior it's not just
you know the tales of distribution it is
quite a different model all together and
I think you probably do need to acquire
that data passively I mean I think in a
car be reasonably easy to acquire black
box type data as you do in air crashes
to try and work out why the rare event
has occurred but I don't
you can infer it from the 99.99 % with
several lines of the data that you may
have in your general model as a as a
gratitude in my second adviser George
Danzig turns me knowing that I gonna get
md as well as a PhD and said I was I've
just been told I need to get a cardiac
surgery for an aortic valve go do a
decision analysis for me and let me know
what I should do should I believe these
surgeons to say done immediately I
couldn't find any place where no one
list where people didn't listen to the
doctor and that was the Lord everyone
got it done immediately so I had to sort
of figure out how to generate those
cases and in the decision analysis
sometimes the data is not there because
of policy so you asked a very hard
question there's a number of ways we can
talk offline about research directions
on that another question player is very
nice and interesting panel but the point
is that we are talking about machine
learning and making machine intelligent
and date house and all of this there is
two point that I am missing in the
discussion one point is about the fact
that with this amount of data and
meshing becoming intelligence how can we
get around ethics and how can we get
around problems like privacy and
problems like y'all the use of this data
and think that we have a big challenge
as a community of computer scientists
that we are not used to have such a
power on the life of people to begin
also to think about how can we educate
ourselves to this kind of philosophical
concepts and kind of correct behavior
because more power means more
responsibility and that's true with any
technology and it's been true that way
for decades and decades and decades as
people have innovated and come up with
new ideas and technologies that
influence society in a variety of ways
but first reaction to your comment is
it's a very important comment but in
some ways we want to
developed and watch and react and with
care as we go as opposed to picking in
advance about all possible ethical
issues that might come up it's going to
be an incremental process I think with
some bumps in the road for example in
privacy there is due respect I disagree
a little bit I think that we are at a
point where technology is now very
interestingly fusion with with humanity
with society it's no longer you know
technology for engineering no no it's
changed to society itself so I think we
should change our mind and incorporate
these questions by design so I think
that the system of the features should
incorporate privacy concern security
concern first concern even I mean the
fact that maybe you will you want or
your euro your children we want to to
that some of the information that they
put be taken off I mean a human can I
mean for us sometimes wants to to to
change
so the way our hacks and I think this is
it is quite important this this are
incorporating this concern at the very
beginning by design it is quite
important and I think that second answer
is that we always think that machine
learning is is an a volatile evolution
process in terms of software but based
on a machine that does not change I
think that we should also think of
embodied intelligence or so so a machine
that will evolve because because this
machine will be within the society some
in and this makes the things even more
difficult in the sense because you
cannot believe that the the machine that
will that will use the software will be
always be the same so I think that even
in the in the odd where this concern
should be there should be taking so I
think society is suffering more from the
fear of privacy concerns than the actual
threat and the irrational responses to
the fear of concerns I think one of the
most helpful things we can do is to
discuss privacy thoroughly but always in
a risk-benefit context never in a risk
only context because that is completely
artificial it goes right back to my
first point about the biggest risk for
society is to not to address that
two-thirds gap in unpredicted outcomes
from health care interventions many
other examples can be put there for
example the the privacy around a piece
of research that's really useful
specious privacy debate that a blanket
publishing response to lock down the
whole process that led to that end
published result now that reduces the
utility of the linked data the
potentially open linked data for
education for inference that someone
might find that active example
of transforming they'll be really useful
in their educational context so with all
due respect with all the respects to the
panel's about privacy so i think is an
incredible opportunity to a lot in the
general for some buddhist from general
concerns about about anonymizing data
and making it available for studies
there is a number as a constellation of
technologies coming out of computer
science at the at the intersection of
computer science and policy that can go
a long way now i also referring to
privacy as being a wait-and-see kind of
thing that's a given we have to adjust
that issue I was talking more broadly
about a variety a constellation of
future challenge we're going to have
with intelligence is in our midst and I
mean rather than trying to design or
think ahead about those kinds of things
I'd like to sort of you know watch them
evolve with them and work with with
challenges as they come up but I think
privacy is is something even given a
risk-benefit and skeptical I to be
concerned about that can be it needs to
be addressed urgently but it's like a
solo she's like please it's not it's not
an autonomous encapsulated entity it's a
social Michelle I think that time just
about run out because I think people to
go to cash air planes or trains but and
I won't say anything about privacy given
there is any time but I think we could
have had the whole debate about the two
questions you raised I think ethics is
important I think people are beginning
to address it in a similar context for
example the royal academy of engineering
and the academy of medical sciences in
the UK have got together to think about
cognitive enhancement so we would all
agree with the artificial cochlear
enable people who call here to hear the
work that's gone about artificial eyes
and saw what about further enhancements
of cognitive capabilities what are the
ethics are providing human beings with
artificial means of enhancing their
cognitive capabilities so two academies
in the UK actually at the moment sitting
down together and at trying to write a
joint report on this I think there is
work going on and it is an important
issue I think we need to teach ethics to
computer scientists and engineers in our
second or third year
in our engineering course we have an
ethics module and I think we also need
interesting peri Institute so the Oxford
internet Institute only by half the
people there are computer scientists
they're philosophers they're ethicists
and so on you have to teach the non
computer scientists or the non machine
learning experts if you are enough about
computer science of machine learning for
them to be able to understand what the
issues are and then to make a
contribution but it has to be dialogue
so a multi-discipline these Institute's
is very important and yes the computer
science is going to be in the middle of
it but we have to have these other
branches so I think it's something not
to be swept under the carpet but there
is hope that it is being addressed oh
thank you the last word concerning the
equation i mean i just wanted to cut
this morning answer by elman Orser
concerning the saffir and it was
mentioning drones and remember is a
chasm of rules one of one of the first
true love concerning robots for sarcasm
of is that robots are not allowed to
kill human do we allow machine to kill
human by design great last word</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>