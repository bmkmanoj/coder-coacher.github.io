<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Visualization Clusters: from Tiled Displays to Remote Visualization | Coder Coacher - Coaching Coders</title><meta content="Visualization Clusters: from Tiled Displays to Remote Visualization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Visualization Clusters: from Tiled Displays to Remote Visualization</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iOKuE7bDklo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so I think we can get started now
sorry for the interruption or the delay
would ya
so I'm Erin Smith from extreme computing
group and it's my pleasure today to host
Paul Navratil from attack the Texas
Advanced Computing Center so Paul and I
have known each other for over a decade
so I remember when he was using small
displays in single course so attack he
is managing the visualization software
group and he is visiting this week
because of SC so I talked him into
coming over here and giving us a talk so
I think he's gonna tell us something
about their new display technology and
the software they've been building
thanks Aaron and thanks y'all for being
here and thanks to whoever is watching
on the web so my name is Paul Navratil
and part of the visualization group
attack which is the Texas Advanced
Computing Center it's part of the vice
president's of research portfolio at the
University of Texas and we have a
mission to service the advanced
computing needs of the University the
Texas System and through primarily NSF
funding the national cyber
infrastructure and so what I'm going to
talk about today is some of the
challenges we face as a visualization
group attack and some of the ways we're
solving those challenges both in our
remote computing platforms and in our
local large format displays okay I don't
have slides for that but I can give you
just a off the top of my head we're
about a hundred people all almost all at
UT Austin at our research campus about
10 miles north of main campus we do have
a few folks in remote offices at Office
of Naval Research down in Medical Branch
in Houston for UT so we have just you
know a the beginnings of a branching
outside but otherwise we're divided into
visualization group high-performance
computing advanced systems that keep the
big iron running user support and
outreach and so as part of our mission
our director JBoss so it really has a
emphasis on giving back to the community
you know
only the scientific community but also
the larger community and both to inform
the public and to inspire the next
generation of scientists that will take
our places okay so basically what I'm
what I want to talk about is how we're
using the cluster technology that
predominates high-performance computing
today and applying that technology to
visualization because as you'll see once
a simulation that runs on a very large
cluster a hundred thousand cores for
instance that data produced by that
cluster really can't go anywhere else
and so that cluster has to become the
visualization and analysis instrument
and I'll talk a little more later about
what that looks like like in terms of
what we do so I'm also going to contrast
the visualization workflow with
traditional high performance computing
if you think about say a fluid solver in
massive parallel you can just divide up
the domain evenly or say if you're doing
a ionization study of dark matter you
can take each of your stars that that
emits the ionization and duplicate the
entire universe that you're that you're
studying across different across your
across your cluster but for
visualization the computational workflow
looks different and the results and the
demands placed on those computations are
different as well and we'll talk a
little bit more about what that means
then I'll also describe some of the
solutions that were pursuing attack of
course I'm being biased towards the one
I'm more familiar with than not so
you'll see a lot of the software and
some of the hardware that I've helped
design and I'll talk a little bit at the
end about motivation for the work of
future clusters okay so let's look at
what a typical HPC workflow looks like
say you have some parameters that you
input into an equation so this could be
say the initial conditions for a weather
simulation temperature cloud density and
flu fluid in the atmosphere you might
have some initial conditions some
starting points and you ship those into
your supercomputer and run a simulation
and then you get some results typically
in the form
of some graphs or maybe just even a
single number or set of numbers and you
might also get some time steps out that
you either feed back as new initial
conditions or you do later analysis on
and so most of your work goes happens at
the Machine and it's in terms of inter
and intra process communication that
might take the form of MPI maybe
pthreads maybe some combination of both
nowadays you're also talking about
hybrid technology so GPU computing
Intel's making big announcements about
Mike this week there many integrated
core product project that came out of
Larrabee development if you're familiar
with that work so visualization workflow
takes these time steps that were
typically generated in from some
simulation it could it could also be
from an instrument say an MRI scan
electron microscope and runs
visualization algorithms on some
high-performance or advanced computing
Hardware and then you get some geometry
for instance that you're actually going
to render into a picture you then have
to feed that geometry back into some
sort of hardware to perform rendering
that creates the pixels of the images
that you create and then you have to
display those pixels somewhere and that
can be an online process for interactive
visualization that could also be offline
say if you're rendering frames to create
a movie and typically this process is
iterative so say you're trying to do a
visualization and you don't like the
technique you use maybe you did iso
servicing you want to do volume
rendering instead so you change the the
way you're generating the geometry maybe
you didn't like the color palette you
use so you have to create a different
image maybe you have point data and you
need to resample it into a grid so that
you can use more visualization
algorithms on it so you have to actually
manipulate the data itself then you
typically also have a process where
you're creating your rough
visualizations just for your own
edification then you polish it up a
little bit and you show it to your
colleagues and then you polish it even
more and you put it in your publications
or in your talks and so there's another
axis of iteration that comes out of the
screen
so let's what what does this
communication pattern look like and I
put this in quotes because I mean
process inter process and inter-process
communication similar to HPC but I also
mean the communication that a human
gives to the to the simulation or to the
to the process and that comes back to
the human because there's typically an
interactive component to this so there's
the same inter and intra process
communication there's interactive
algorithm manipulation think of changing
the ISO value in an isosurface
and then there's also interactive
display the data so all this process
typically has to happen in say a
twentieth of a second for just
interactive or a sixtieth of a second if
you're if you want really high
performance interaction okay
so the algorithms tend to be more
demanding on the hardware than a typical
HPC algorithm for a couple of reasons
first the calculations tend to be more
irregular
there's data-driven calculations think
traversing a tree or searching for a
particular ISO value in a data set you
also generate new data so if you're
doing that weather simulation the region
of the country you're simulating never
changes right you may change the
gridding of it but once you've formed
that grid it's constant through the
calculation in visualization algorithms
you're returning new geometry or you're
generating images that you'll return so
you have to buffer that data in memory
also you're interacting with someone
controlling the algorithm and
controlling the the calculation and so
if you're changing that ISO value then
your parameters change and you have to
recalculate very quickly and the users
expect an interactive response they can
tolerate up to say 10 frames a second
but if you start getting into seconds
per frame where they have to go get
coffee or check their email between them
you've lost it and also interactive
display of data and particularly for
large simulations as the pixel count
grows
it's more and more difficult to ship
those pixels to in a in a meaningful way
to a display okay so let me give you
just a quick example if you're not
familiar with visualization algorithms
some work that we've done in GPU based
ISO surfacing so isosurface just means
you're looking for a continuous value in
the data set think the temperature lines
on the weather map extrapolated to 3d
and a classic way to do this is with
marching cubes where you find where the
value crosses in a cube and then there's
a lookup table to determine how the
geometry is placed and so say if our
value crossed the line four seven four
five and four zero then we go back to
the lookup table and we put the triangle
at the cross point through tri linear
interpolation okay
and so the way this is done on the GPU
is actually a three step process they
classify the voxels find out which
voxels have your data value in which
don't you do a scan to determine where
those voxels are and then you compact
them into an active box a list and
generate the triangles
okay the challenge here is that you
don't know how many triangles you're
going to generate ahead of time and on a
CPU based algorithm that's fine you just
create a new vector or you create maybe
an STL vector and just push onto the end
of it on the GPU that's harder because
until recently
you couldn't allocate new memory on the
device side and in OpenCL that's still
true CUDA now gives you the ability to
do that so but either way you have to
buffer some of your available memory for
the for the data that's going to result
and so you can see that the execution
time is really bad in this space because
one of the multi pass process and two
because of the amount of global memory
that you're accessing in that compaction
step and so one one of the things we've
done is we've whoops that was back sorry
is that
we've now moved this that this is just
summarizing the the issues that I
mentioned previously that you don't know
how many triangles you're creating
initially but also you're generating the
triangles in parallel and you're writing
them to a single buffer and so the
multimap of multiple threads in the GPU
have to synchronize down into that
single buffer so think of it as a
reduction but with more data so then
what we've done is we just take a
classic you can think of it as an as an
octree approach from graphics that we
just four instead of dealing with
individual cells we create meta cells
that help filter out regions of space
that don't or regions of the data that
don't have interesting numbers and so we
can now in a spot that was ten mega
milliseconds we can reduce that to three
for about a two and A two to two and a
third time speed-up and we can also
because we're operating on fewer cells
the classification step becomes a little
faster but so I hope that that motivates
just a little bit and if we can talk
more about that work afterwards I know
I've given it just a very quick
treatment but basically the takeaways
are that we need more memory than HPC
algorithms and that amount of memory we
need is dynamic or can be and that the
large problems need significant
computational resources that may be
larger than what is evenly divisible on
a single node for instance we have done
some work with the dark matter
simulation where each individual time
step is 650 gigabytes and that is the
memory footprint for the HPC simulation
itself for the additional structures
that the visualization algorithm needs
that expands to 3 terabytes and some of
that is because the visualization stays
vtk is only now being concerned with
memory efficiency but some of that is
just you need the extra space to allow
the algorithm to work and so if we need
if we need what we
doubt is when we first started working
on this system our largest visualization
resource had only a terabyte of memory
so we had to go back to the HPC resource
but we were allocating nodes now for
memory instead of for processing power
and because a libraries like vtk aren't
multi-threaded we would allocate the 32
gigabytes per node on Ranger one of our
machines and 15 cores would sit idle
so there's definitely an opportunity and
this is some of what we're working on
attack is to paralyze this so that we
can make use of both of those cores even
when we're allocating for memory ok ok
so the original solution to all this was
to move in our the data to a separate
machine to do to do this interactively
for instance date some datasets that
were small enough that you could move to
your own desktop or to your laptop
however as we're getting larger datasets
moving them off the machine they were
generated is becoming untenable so even
if you have a 10 gigabit connection a
terabyte go get a cup of coffee a
petabyte go on a vacation right and if
you have to do this over wireless forget
it
it's just not going to happen we even
today work on datasets where it's easier
to put the data on physical drives and
nail them rather than try to do the
transfer ok so it's so this is only
going to get worse because as the
machines grow the datasets grow but the
disk technology and the network
infrastructure isn't there and this is
suffers from a last mile problem so at
UT we have 10 gigabit connecting our
machine room to the main campus but if I
try to send that to UT El Paso it's
going to pass through a thin pipe and
that's only as fast as that transfer is
going to go ok so what we've done is
we're moving the visualization resource
to at least the same machine room and
ideally moving forward on the same
machine so our first system like this
was built in
2008 spur which was an attachment to
Ranger eight nodes 32 GPUs and a
terabyte of aggregate ram and each
single node had at least 128 gigabytes
of RAM because there are still some
legacy shared memory codes that need
that much that need that much Ram long
horn is the machine we just put out in
2010 256 nodes 512 GPUs and thirteen and
a half terabytes of aggregate Ram and
because it's in the same machine room we
can put a very high bandwidth connection
to the ranger file system and and
operate on the data without moving it
and now our lone star system that we
just released also in 2010 has right now
eight nodes each with two GPUs but we're
going to expand that by the end of the
year to 72 nodes with GPUs so we have a
smaller version of Longhorn sharing the
same interconnect and disk as the the
larger nodes of Lone Star Lone Star's it
is a 20,000 core machine in our new
machine that was just announced stampede
which will the attend petaflop resource
released at the beginning of 2013 and
yes everything has a Texas tie-in to the
naming scheme so this will have 128
nodes each with probably a Kepler GPU
from Nvidia and also this each node will
have an Intel mic across the entire
system which will be on the order of
thousands of nodes so that'll be
interesting to to experiment with and so
for problems that don't operate on the
VIS subsystem we use software rendering
and we move it back to the HPC cluster
okay so what about shared memory there
there are some in the community that
thinks shared memory it is and always
will be necessary for visualization we
we've done the experiment ourselves and
spur was actually a replacement for
maverick which was a 512 core hat I'm
sorry 100 101 cake ORS half a terabyte
of shared memory and what we found is
that we were able to build spur with
more capability for
cheaper and easier to maintain and the
use of spur was actually much higher
than then of the shared machine maverick
and the nice thing is that if you go to
a distributed memory model you can get
much more aggregate RAM than is possible
on a single machine today so you can get
an order of magnitude more and there are
single nodes that you can get now that
loan start and Sanpete will both have
that have a terabyte of RAM so you can
still you can still get a significant
shared memory resource even in a cluster
environment yeah sort of minor of why
shared memory feel for you sure I think
because the the environment is harder to
control because under underneath you're
either having an uma access that you
don't have control over or you're
slowing everything down to the least
common denominator and it's ultimately a
shared resource in our clusters we can
give everybody exclusive access to their
set of nodes whereas on the shared
machine everyone's playing in the same
sandbox
and so also this is just following
trends viz machines have always followed
the the path of HPC machines and
clusters have definitely won out
I think 480 some odd of the top 500 our
cluster machines and that's just going
to grow so so we we're trying to bring
the community into that into that role
ok so there are some tools out there
some some tool kits like pair of you and
visit - - large shared memory plan and
I'm sorry large open-source platforms
based on vtk to do visualization and
what they've done is they use a fat
client model where the geometry is
generated on the server and then shipped
to the client and some of them visit in
particular tries to be smart about when
it ships pixels versus geometry but what
we've done is so we've seen that the
data traffic especially doing managed
communication within the the within this
software can be too high for low
bandwidth conditions and also the
connection options are still lagging a
bit behind sometimes they just assume a
 a large shared memory system that
you're connecting to even remotely what
we've done instead is we push everything
server-side and now we just use a thin
VNC client to interact with the server
and that's been successfully used from
tack machines literally half a world
away we have collaborators in the Gulf
states working on our visualization
system remotely and there's definitely
latency in that model but you're going
to experience the latency either way and
this allows the computation to move
forward while just moving keyboard and
mouse movements from the user and
pushing pixels back and with the new VN
sees they're doing smart things like
only updating the change region of the
window rather than pushing an entire
window across and so this really
minimizes the bandwidth and if you want
we have full-featured gnome or KDE on
the back end
or we I tend to use just EWN now that's
ancient and Spartan but it gets the job
done and it minimizes even the overhead
of the windowing system ok so let me
show you just briefly I won't play this
entire video but this is our web based
interface on to long horn
it's called envision and so someone
through the web can either get that VNC
window I mentioned or go through an
interview process to do their own
visualization and this is I think this
version is vtk based but it has tie-ins
where we can use other rendering
solutions and so you just have an idea
that this all happens again in in your
browser and this is a mummy MRI that
they're playing with so this is actually
this is all on the browser so the vtk is
rendering server-side and then the
pixels are being shipped yep so it's not
even vtk window okay so let's talk about
image display because generating the
images is only part of the solution you
still your analysis of a bit those
images are limited by the pixel count on
your display you're using and if you
have an electron microscope that is
producing an image that's twenty-five
thousand pixels by twenty five thousand
pixels you're either zooming and panning
in something like a Bing Maps interface
where you're zooming in to what you want
to see you're pulling back but you only
can you have to trade-off context for
detail and so there are other images
like NASA Blue Marble has a three point
four gigapixel image of the entire Earth
Google Art Project has the the large
scans each museum in the project has
donated a piece of art to be high
resolution scanned it's the equivalent
data this is at point at half kilometer
resolution per pixel and it's equivalent
to the entire earth scanned at kilometer
resolution per pixel is how much data
are in one of these one of these Google
Art Project scans so very high
resolution resolution and again the
electron microscopy
is at half a gigapixel already and so
it's nice to have both resolution and
size resolution to see the details size
to see the context and so we have
multiple display technologies in our
visualization lab that give you an idea
that allow you to choose the the
technology that's right for you and this
is a brief view of the viz lab from the
Longhorn Network this is the all of the
Longhorn Network I've seen I don't know
if you're following that saga but
apparently there's a neighborhood in
Dallas that has a long horn Network and
Austin does it so this is that was our
307 megapixel display that's a 12
megapixel touch display and we actually
have that touch display in our booth at
SC so if you have an opportunity to go
to the exhibitors fair it's at booth 323
and so this is just a minute you can see
high-resolution photography this has
actually the lab has allowed us to
expand in beyond the traditional STEM
fields science technology engineering
mathematics we've gotten fine arts
architecture humanities in here to take
a look at we've actually had artists
build pieces for the biz lab and so what
we found is by working with the artists
they have the vision of how it's
supposed to look and they challenged our
technology to reach it then we can take
that new ability and bring it back to
the science and engineers to expand what
they can do and so we we find it's a
virtuous cycle to work with these folks
this is a 3d display this is just an 82
inch commodity television and driven by
a Quadro graphics card and so what we've
done this replaced a barcode projector
solution 2,900 square feet and in the
space that that projectors that single
display of projector technology took up
we have a meeting room and six high tech
displays now so we still have a
projector but we also have so much more
that we built and we maintain and and
the the power of the commoditization of
this hardware has really come come to
the fore okay so
let's I'll this was our first tile
display cult 3x3 these are 30 inch LCD
monitors and we built the frame ourself
just any drafting program that gives you
actual measurements you can design it
send it to a company it's called 80/20
the industrial erector set and they'll
cut it send it back to you iodized it
any any color you want so as long as
it's black so this is the 307 megapixel
display and that's showing Mars in the
background plus some visualizations on
top of it and the nice thing about this
is you can either show very large data
or you can show multiple views of data
or a correlated data and so you can have
everything up at once rather than having
to switch between windows on a smaller
display our work here has also been
starting to expand beyond just our
Center we run a lab in the college of
education at UT now that allows them to
work on their information visualization
problems that we have a partnership with
such as the scores for every student in
California in 1995 the test scores for
them and so those are the types of
problems that they're trying to
visualize we've also consulted on tiled
displays at UT San Antonio ut El Paso
and I believe also in Colorado and so
that's broadening out and again built
and maintained all by Tech staff and
using commodity equipment so also at a
fraction of the cost this is a schematic
of Stallion you don't have to read it
the takeaway is that 75 displays 30 inch
there's a three by five hotspot just by
a feature of how the hardware worked out
the outside that hotspot it's each GPU
drives two displays inside the hot spot
each GPU drives one and so you can have
it there's a note of notable rendering
performance boost in that hot spot
twenty three workstations and these at
the time this was built in mid 2008
there were no server class machines that
could that contain GPUs there's
but at the at the birth of the gpgpu
revolution so we just got gaming boxes
and these are GA T g-force machines
driving it if we did this today we would
use server rack mounted machines that
contain GPUs and we'd use quad rose and
we have SDR InfiniBand just for the to
have high bandwidth among them and
that's it we also have a 5 terrible
system that's in the process of being
expanded to about 50 terabytes ok
displaying data so that Square is a
scale representation of the 30 inch
monitor
compared to stallion okay and then that
blue marble project I mentioned so this
is to scale 3.4 gigapixels versus 300
mega pixels versus 4 mega pixels on the
single display so you can't see the
entire image but which one would you
rather look at that image on if you had
the choice of either in your office and
so the point being you can't see all the
data but you can see more of the data at
resolution so that you can see not only
the details but get the context you can
almost see the great wall on that on
that it's pretty cool okay so display
software has been stick is not
standardized at this scale the Kaua'i T
squared team has built CG LX those were
really the first folks and then folks at
Chicago have built sage but they're now
both closed source and that's a problem
because some of the configuration
assumptions are baked in when we first
got sage we had to modify it it was open
source then and we have to run CG Alexan
integrated mode because it doesn't it
doesn't accept our hardware
configuration naturally and
full-featured windowing environments
like XDM x don't scale there's actually
hard-coded limits in in the code at 16
displays and so 75 we just uncommented
it and see what would happen bad things
so there's there's definitely a need for
folks to do X
right and because outside of a
full-featured environment your your
ability to have software hosted on the
on the large display is limited as well
primarily images and video streams
because that's the first thing people
try and it's relatively easy you have an
API for third party software but if you
want to run a closed source proprietary
piece of software or a large software
base you would have to go in and modify
it you can use something like chromium
to just sniff OpenGL and map it across
but that chromium stop development in
the mid to late 90s so it only supports
OpenGL 11.3 and it's we're at OpenGL
four or five now so so a lot of stuff
that it doesn't support and then to give
you that pan and zoom feature for very
large images there's a separate
application in sage and CG OLX doesn't
have support for that at all so what
we've done instead is were we have now
beta release our own display cluster
software that will remain open source
and it combines the features of sage cgo
X and magic carpet is that pan and zoom
for large data or for large image and it
has all those features plus it allows
you to toggle between choosing network
bandwidth versus disk bandwidth for if
you say we had a development event for
before a UT football game and we
streamed 75 football historic football
games on the tile display which was
really cool and it was kind of amazing
they gave us the copyright permission to
do that but it it really taxed the
ability of the other software programs
to do that and so we had to run it in
our own software we all are also
exploring a touchless interface and
you'll see a video of that in a moment
and we also have tu IO Bluetooth
connectivity so you can use smartphones
and tablets to control the display and
we also giving a Python scripting
interface so that you can can
demonstrations or do advanced scripting
for interactions across the display this
was really motivated and
actually by our artists who wanted
particulars to come on display at a
particular size at a particular point
and the other the other windowing
environments really couldn't handle that
okay so let me show you a little clip of
this and so this is showing that pan and
zoom that we that you would normally
have to have to a separate application
open for but and now in display cluster
we can show multiple images and zoom
into them simultaneously these are some
of those Google Art Project videos
that's van Gogh's Starry Night this is
the ambassador's from I think the 17th
century and it the nice thing about it
is everything's public domain so okay we
can put this up here's the Connect
interface we liked that so this is a
piece of 17th century Russian art and we
were amazed that that face looks a lot
like Robert Downey jr. particularly
after a bender it's a little green
tinted and so again part of the
motivation for this is the College of
Education wanted to use Mac's to run
their cluster because they have a
relationship with that and so CG LX and
sage wouldn't run on the Mac and so that
really motivated us we talked about it a
long time but once we had a tangible
problem to really address and fix that
that's what motivated us to do it and
we've already had a lot of interest from
all the UT institutions that have tile
displays folks at Stanford are
interested University of Central Florida
University of Michigan so I think that
folks are using what's out there now
because that's what's out there and so
hopefully we'll we'll have some nice
moving forward experiences about about
how how our stuff works so this is the
touchscreen display this is I think
that's Bing Maps being over the
University of Texas this was designed
in-house by from 6:46
inch Samsung LCDs a PQ lab 32 point IR
frame we also have a pane of glass in
front to give a smooth swipe experience
over the bezels and then we also have
the Kinect to do a touchless interface
and this is really working as a testbed
to then take those interface designs to
the larger displays and so there's a
nice feedback up and it's driven from a
single node with an AMD Eyefinity six
port GPU that really allowed us to keep
the costs down I'll talk a little bit
more about this later the challenge
there is that it has really reduced
rendering capacity because it only has
four gigabytes it's driving six displays
at two megabytes each and if you're
trying to stream say a video on each of
those displays things break down pretty
quickly but it's also do bolt into a
Windows 7 and in Bude - ok I think this
is the last video but this gives you a
little sense of how the display works
so this has been motivated by a project
with the National Archives this is
showing the digital holdings in a tree
map view and the National Archives are
really struggling because they just
don't understand half of what they have
in digital form much less how to respond
to a Freedom of Information Act request
and so this is operating to allow those
folks from from the school of
information from National Archives to
really interact with it in a more
dynamic way so let me get to some of the
interaction that's just a one of the
demos
there's multi-touch map navigation you
can see the whole video on YouTube
so that's being there that's I'll tell
you that's one thing you have to
actually have that hold that and then do
the rotation Google Maps lets you do a
gesture like this to do that rotation so
any Bing developers in the audience
that'd be a nice feature to add okay so
let me talk a little bit now in the last
few minutes about what the types of
things that we're doing on these
displays and some of the user successes
that we've had so this is a really
something that could only happen at
Texas I think is after the BP oil spill
we had scientists who are who have
models of how the Gulf Coast I have the
Gulf the the the patterns of ocean
currents in the Gulf and they then just
modified that code to track where the
oil spills are going where the oil
particles are going so here's a lie that
wasn't the last video whoops pause it so
here's a visualization of that
simulation and these were done the
simulations were done in real time so
that we could give the responders some
idea of where the oil might be going and
so you can see how this is the end of
Louisiana this is the Mississippian
coming into the New Orleans area
and so the simulation was also run at
multiple resolutions so you could get
the coastal effects and then also track
more broadly in the Gulf so this on this
video itself or just in general yes so
you can zoom into the yellow really fine
grained now the the simulation has
granularity limits because it's a gulf
simulation and so that the coastal
elements are rather coarse but but this
but the simulation itself is pretty is
pretty was pretty accurate in terms of
where the where the oil was going
and so this is in the spirit of what we
do with Noah during hurricane season we
have several hundred thousand hours of
compute time set aside so that when the
plane flies through a hurricane
they can take those immediate readings
and then they run what's called an
ensemble simulation where they run so
really about a dozen different hurricane
simulations with slight parameter tweaks
or slightly different implementations
simultaneously and that's where they get
that cone of probability from they just
average the simulation results together
back in when Ike came through in 2008
our supercomputer actually predicted
that at the moment that the storm was
gonna go over Austin and cause the UT
football game to be moved a month later
and then storm took a right turn and
missed us entirely but it was it was our
claim to fame that we we got the game
cancelled so we've also studied h1n1 not
only the molecule itself and how the
virus attacks cells but we've also
looked at the epidemic epidemiologic
effects and this is a nice web interface
that Greg Johnson on my staff has
developed that part really isn't
exciting so here I think we're getting a
little choppy but you can see the
counties of Texas and then once you zoom
into a particular point you can run look
at the the results of the simulation
I wonder if I broke things yep so you
can see the the counties are changing
color by how much infection is occurring
at a particular time you can see how it
radiates out across the state from a
particular input so that was a a disease
that was breaking out in Travis County
where Austin is and then how it slowly
follows the population around in the
state and this is something that was
commissioned by the Texas Department of
Health so that you can so that they
actually want to use this to track Texas
EMA death mix and to prepare for future
ones and what you saw there was then the
graph graphic output of amounts of
antivirals amounts of folks who are
susceptible to the disease and who have
recovered or the mortality rates and so
this is a really powerful way for those
folks to really learn more about what's
going on in terms of Texas Health okay
we've also worked on hot on
high-resolution mantle convection and
what that allowed us to do this work was
featured on the cover of science not too
long ago that allowed us to then with
the same team do a simulation of the
Tohoku earthquake whoops and so what
you'll see here this is work that Greg
Abram did is the seismic wave
propagation that originates in Japan in
the top corner here so there's Japan
assume this is wrong looks like the
screen for us
think the system's having a little
problem with it the video should be
smoother but what you'll see is that
here as the earthquake waves radiate out
watch in that Center here the waves
actually hit the core of the earth and
reflect back up and you can you can see
that in the simulation eventually and so
here it comes there's the shadow of it
right there
and so that's part of the reverb ative
effects of the of the original quake
really rings the earth like a bell and
you can see that waves propagating all
the way out this is through Alaska down
the East the western coast of the US
through Greenland and into Russia okay
and so this is another piece of
visualization from that Nara project
with the National Archives and so this
is a 3d 3d visualization and our 3d
imagery that the National Park Service
has plus photographic tours and so this
allows them to see where those
particular files are located
so to summarize let's talk about the
trends that visualization systems track
HPC systems both at the large scale and
at the small scale the reason that we
could build a system like stallion or
like the touch display is because of the
power of individual workstations
nowadays and that vis this systems must
be able to scale to HPC size either in
the raw hardware or in the software that
has to run on the visualization systems
so for instance a multi-threaded Mesa or
other software rendering package would
be would be excellent to for to develop
and advances in the GPUs themselves not
really not only in the compute where we
can move the visualization algorithms on
to the GPU but just in terms of their
display power allows us to really
increase the density although as we
increase the density it limits the power
of the display and so in terms of
expanding a single window for like Bing
Maps or for Google Maps that's fine but
to do high-intensity video streaming
that's either CPU or GPU intensive a
single node does show its limitations
and but that powered allows you to
trade-off and so you can design your
system either with many nodes to make it
a proper compute resource in addition to
display or you can minimize the nodes to
reduce the cost and still have a very
large format display for instance if we
were to reduce tallien today we could
span anywhere from seven to thirty eight
nodes depending on the amount of compute
power we wanted behind the display and
so for future work we really as a
community need to address the
algorithmic in efficiencies in in terms
of the visualization stack memory
efficiency vtk still has an annoying
habit of moving the data set into an
unstructured grid even when it doesn't
have to especially when there's a
structured grid algorithm that can be
more efficient the raw BTK out library
still may do the dumb thing both the
visit and pair of view teams have
reimplemented a fair amount of the vtk
stack to avoid situations like that and
to use the fast case when possible we're
just starting to explore as a
visualization community how to include
accelerator support we're already using
the GPU for rendering if we can move
more of the algorithm down there and
generate the geometry that we're going
to render already then all more to the
good and again improved software
rendering particularly for HPC systems
that don't have graphic support right
now we really only can use the Mesa
library and that single threaded and in
terms of the usability for large-scale
displays the windowing interfaces are
still limited though we attack are
working to fix that distributed
rendering support chromium is the latest
effort that I'm aware of and that was at
the end of the mid-90s
so so that that's twenty years old or
almost and then better usable user
interaction and that's what we're
working with both in touch and touch
list interfaces or using the interfaces
that people carry around in their
pockets or smartphones to actually allow
us to use the displays and with that
I'll open it up to questions thanks for
your attention
yeah
what exactly can you do with that
because I'm the video right the guy's
kind of an ancient Oh so yes that's what
the touch with the Kinect that allows us
especially on a display like stallion to
have a centered control point and then
expand more of the display we're also
using investigating using multiple
connects to do three-dimensional sensing
not just in front but also getting a
side sense and then doing a chain of
connects to have control through the
expansive stallion instead of just one
in the center so you could be at there
at the side of the display and still
have effective control part of the
challenge is determining clicks so if
you want to grab a window and move it
you have to have some sort of gesture
maybe you know closing a hand so the
surface area is reduced
I think the gesture we're using right
now is kind of a Pac Man you know I want
to grab this and move it so that that's
part of a challenge to there's work out
there on using gloves with or some sort
of you know color coding system and
cameras we're trying to simplify the the
equipment needed you know we don't want
to have Michael Jackson gloves for
everybody yeah experience if they notice
it at all they notice it the first time
and they grow increasingly resilient to
them we're used to looking out of things
like windows and if you're looking out
of a window and you want to see
something behind it you move your head
here you just move the data what it
doesn't do well is PowerPoint if that
text is behind the bezel you're not
going to read it and so we have a 4k
projection system that gives us that you
know slideshow I did my dissertation
defense in there on the 4 megapixel
screen we had another dissertation
defense with the four megapixel slides
and then the data all across stallion in
in terms of videos stills and it was ten
interactive potent interactive but ten
video posters of his work
and it was you know very compelling the
bezels give us also some other
advantages initial purchase costs
because these are commodity we don't
have to do any modification we just
stick them up they're a replacement cost
we actually have you may have noticed in
the video some of those workstations
those are the same monitors as in
stallion and so we have hot spares that
we can essentially just pop in there the
only caveat on that is you want the same
manufacturer a lot because the
fluorescence has slightly different
temperatures and that that gets messy
let me see and then the construction and
maintenance ourselves also if you ever
come to Austin and are right up with
stallion it's not a precision fit and so
that bet those bezels buffer us a little
bit and so we can just put them together
us you know without you know without
shimming it or doing any sort of of
touch to it whereas the barco system
that we replaced we had to pay a five
figure contact for someone to drive up
from Houston every six months and
readjust and realign to account for
building shift thermal expansion things
like that another thing is those Barco
systems had be kept at about sixty
degrees and so the students would come
in and their winter jackets wouldn't
want to stay there the entire walls were
painted black to have this immersive
experience here we've made it warmer
people want to be in there and because
it's all commodity parts it's used to
being in the same environment as humans
it's made to be in an office so we can
keep the room at you know a balmy 68 or
70 and you know people can stay in there
they're actually there is a line as you
move forward we we adjusted the HVAC so
that the it now has a row of registers
that blow straight down on the on the
display but the 2100 square feet it it's
it's pretty reasonable it doesn't it
doesn't get too hot either
so each so I I know the stats in terms
of amps just because when we were doing
the circuitry no you know we want to
make sure we weren't going to blow
anything the Machine the display is
determined by the brightness of the
monitors we have it probably at about
1.5 which is just the the default turn
on it can if you max the brightness it
goes to about 1.8 amps if you minimize
it you can go it down to about point 8
amps we as part of the renovation put
glass doors in so that people could see
the display as they go by in the hall
so we tend to keep the display on with
interesting things and it's great you
literally get these you know pauses and
walk backs for people wondering what's
going on in there
the machines themselves are rated at 8.3
amps
Cole just warm boot is about if I
remember correctly
maybe 1.2 amps and even running it
wasn't Linpack but it was another stress
test on it never got above 2.5 amps so
that 8.3 I'm not sure where they get
that but that is extreme crazy doing
something so are our normal operation is
is I'd say for the entire system
we're probably under a hundred amps
which is still a lot but you know it I
think for the size of the display it's
pretty reasonable and compared to a data
center it's it's not register it's not
moving the needle good question yes
and I can imagine to certain extent that
they can't do their typical things on
here so they've got special apps and
sign up and visit mostly sort of a
showpiece right now or do they actually
get something there is there's a lot of
outreach that happens there you know an
education coming through for instance
the electron microscopy the lab came in
they loved it the pea I was you know a
40-ish you know woman and she'd never
touched a game controller and it took 15
minutes to pry it out of her hands
because she was zooming around and you
know exploring it great you know their
3d reconstruction technology what
they're actually the signs they're
actually doing is like a key framer they
stack these slices together and then
recreate the neurons in 3d software is
single threaded runs on a single
workstation doesn't utilize the 23 nodes
of Stallion and so while we could bring
up those images at the flick of a button
which they were they loved their actual
software didn't run there and so what we
do as the center is we do have funding
to do advanced user support projects of
porting that over but there's also the
you know last mile problem of this isn't
in my office this isn't in my building
I can't walk outside in the Texas summer
you know so there has to be some
motivating factors for that but we do
have folks using we're actually
expecting to get more out of the Fine
Arts side because to be able to zoom in
where a brushstroke fills an entire
screen especially of something like The
Birth of Venus that's in Italy and it's
a fresco so it doesn't travel you know
it that allows folks you know doesn't
replace interacting with the piece but
you can get a much different experience
with it and then the science follows in
terms of an analysis rather than big
data I think we have the multiple views
gets more use where for instance for
hurricanes you can put a path of the
storm you can put the ensemble
forecasting you can put the storm surge
all very large and together and so they
can analyze it as a group and then they
bring in the news media and you know
they it looks great there too
if you if you go to the Texas website
just the homepage the new campaign video
for for development was shot in the lab
and so we have the university president
we have all the students and probably
about 60% of that footage was was in the
lab and and even though we're a
ten-year-old center we have you know
top-ranked machines in the world they're
still folks on campus who don't know we
exist
and so in terms of a ambassador and an
outreach piece it's already you know
worth worth what it can do and and the
signs we can get out of it is in some
ways a bonus yeah yeah yeah and so we
can we can do that as well
we that's one of the things we wanted to
design into our into our software we had
one of the one of the lead developers
for sage was on our staff for a time
he's since gone to slumber J but at
supercomputing in Portland we had a live
video stream to our small display cult
from Australia but that comes back to
the bandwidth problem we had to set that
up with national lambda rail to get a
dedicated 10 gig connection a whole lot
of Hoops to jump through rather than to
just you know use Skype and call you
know call Erin on a Sunday right so yeah
so in terms of highest high stream data
the the interesting out takeaway from
that experiment is that the compressed
video actually looked much better than
the raw we did compressed and raw HD
stream and just the amount of packet
loss in between made the made the raw
stream really noticeably janky whereas
the compression actually yeah that's a
technical term but yeah the compression
actually recovered from that a bit
great thank you very much I appreciate
it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>