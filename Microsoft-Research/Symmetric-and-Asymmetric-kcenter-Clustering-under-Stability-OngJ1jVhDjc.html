<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symmetric and Asymmetric k-center Clustering under Stability | Coder Coacher - Coaching Coders</title><meta content="Symmetric and Asymmetric k-center Clustering under Stability - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symmetric and Asymmetric k-center Clustering under Stability</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OngJ1jVhDjc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so let's start today's talk it's a
great pleasure to introduce emeka hawk
Talib she's a graduate student at
Carnegie Mellon University advised by I
real book sorry averey emblem and idea
of ricotta Nikki works on learning
theory game theory algorithms and she
already has some very exciting results
and today today she will tell us about
the key Center problem thank you I'm
going to talk about symmetric and
asymmetric a center problem and
specifically how they behave under
stability conditions let me first start
by introducing my co-authors this is a
joint work with nina belkin and colin
White I've tried very hard to include a
lot of pictures and figures in the
stocks and since I clustering talk but I
don't think any of them are going to
look as good as these pictures do but
let's hope good so we want to talk about
k center clustering k center clustering
is motivated by a very simple and
natural question which is if we want to
for as an example if you want to assign
k locations in a city to fire stations
how can we do that and where should we
put these fire stations but we should
first think about what our goal is when
we are putting fire stations we really
want to reduce the travel time for the
worst location possible in this city so
if the worst location was going to be on
fire we want the closest fire station to
be able to send a fire truck and resolve
this issue very quickly that is what k
center clustering is going to do more
formally for a set of points s and a
distance metric D K clustering problem
is a problem of choosing k of those
points and assign them as centers and
then partitioning every point and
assigning it to is closed
Center and with the goal that them to
minimize the maximum possible radius in
this sort of color even more formally we
want to choose a set of centers for the
worst point in this set we want to have
the best distance being minimized we are
going to call this as a cost of the case
enter clustering and we'll refer to that
on our store as an example let's have a
bunch of points we're going to assign
the center's as the middle points and
the r star is really the cost of the
largest edge here the partition that's
induced by this sort of assigning each
point is closed the center is going to
be the case on turquoise drink good um
this is are not always symmetric they
could be different in different
directions for example when you're
driving you have one bay road so the
distance to and from a location can be
very different we're going to consider k
Center on there are two different
notions of distance one when it's
symmetric and 1's minutes asymmetric for
the asymmetric setting we still have
some axioms about the distance which is
the distance of any point to itself is
zero and the director triangle
inequality holds in addition if you're
in symmetric setting you also have that
for any point I need to point the
distance in either directions are the
same our objectives say is the same we
want to be able to minimize the distance
from the center to the points so this
direction we care about the backward
direction we don't it really doesn't
matter you could switch this as long as
we stick with a consistent notion
everything is good but so for this stock
we're going to stick with distance from
the center to the points
there are well known worst-case
approximation results for both symmetric
and asymmetric a center for the
symmetric case Center in 85 Gonzalez
showed a tight to approximation
algorithm for the symmetric case center
it's very simple and you cannot do
better than that any two minus epsilon
approximation is going to be hard for
the asymmetric version in 96 and then
later in 2001 there was an approximation
algorithm for off laga star of n and if
we need to remind ourselves what locks
torres it's a super constant function if
you write n as a tower of powers of two
the lengths of or the height of this
tower is log star n so it's a very
slowly growing function but nevertheless
is not constant so there was an
approximation algorithm for off log star
n and the gifts at that point was that
this cannot be tight there is it's not a
very natural number to come up with but
as it turned out it was site there was a
matching lower bound for the asymmetric
a center of again or flux RN that was
shown later questioned certain others
there are many others unhappy and this
is a breakthrough result good so in many
situations having a constant
approximation that along a super content
approximation is not very desirable this
is really a cost you are paying so if
you're paying double the cost you might
not do I not be happy with that but
unfortunately this existence of tight
approximation guarantees stops us from
going and getting better results but the
main problem here is that these tight
results are really just for the worst
case situation and in many situations in
real life we are not always given this
worst-case scenario a lot of times it
would be very pessimistic to assume that
every day we are solving a worst-case
instance so we want to go beyond this
worst-case analysis and focus on
instances that can
surely occur in real life what does that
mean it means that we assume certain
natural assumptions natural in double
quotes and then we are hoping that under
those assumptions we can get better
results hoping that either we can reduce
these approximation guarantees or we can
actually find the exact solution there
is a problem for which I simplest case
being sense is typically case centered
but your first example where it is short
yes so maybe when I show you the type of
national assumptions we make you will
see that it does make actually a lot of
sense so maybe I'll come back to this
question in two slides or three slides
good okay so one of the notions that we
are going to assume here it's called
perturbation resilience and what it
tries to capture is that in real life we
make some mistakes we make their measure
things that we measure can be a little
bit off or there can be fluctuations for
example in this case the traffic on one
day is higher than the other day so the
travel time changes from one day to
another and this small changes small
fluctuations in the measure our hope is
that is not going to entirely change our
solution so if we decide to put this for
that the fire stations at one place
after six months hopefully we are not
going to come back and say oh the
traffic has changed why like a epsilon
and now we have to destroy all the fire
stations we had and come up come back
and put like a different fire station
somewhere else so that is what a
perturbation resilience trying to
capture that small fluctuations don't
make a big difference more formally
actually not more firmly yet
perturbation resilience for the most of
this talk I'm going to assume that small
fluctuations don't change the solution
at all so not that they don't change it
drastically but they don't change it at
all I'll make it more robust at the end
and this implies some more structure and
the hope is that this more structure is
going to help us find the exact solution
let's look at this example so what is a
perturbation or fluctuations in the
distance are going to look like
something like this and in this specific
scenario there's a lot of structure
presents so the clustering doesn't
change at all so this instance would be
perturbation resilience more formally
perturbation regime was introduced by
below and Danielle in 2012 and what it
says is that for an instance and a
distance metric it is alpha perturbation
resilient if I blow up the distances by
at most a multiplicative factor of alpha
so this is what that is then the
clustering or the solution does not
change at all so that is being alpha
perturbation resilience one very natural
implication of this result is that
actually the optimal clustering is
unique already because optimal
clustering you're not making any extra
perturbation to it so if there exist two
optimal clusterings one has to be
different the other thing is that to
note it's fine if the center's change
all we care about is that the partition
does not change under the new distance
metric it's okay if it is this Center is
do change is there any question about
perturbation resilience ok the next
natural assumption that we want change
well we are not introducing that I want
to mention here for this talk it's
called approximation stability and what
it captures is that if you are
approximately the cost of a function of
a clustering in this situation you're
also approximating the membership or the
partition itself so if you move within
the cost for for a partition that has
better cost the partition is also closer
and membership to the actual optimal
solution this was introduced by Vulcan
Blum and Gupta in 2009 and more formally
what it is is that if we have an alpha
approximation of the cost which were
case in here is alpha R star we have an
excellent fraction of the points being
different than the
simo so that's alpha epsilon a very
simple result is that if you have an
alpha 0 approximation stable instant you
have an alpha perturbation resilient
instance this is relatively simple so
what it means is that approximation
stability is actually a stronger notion
of stability than perturbation
resilience okay so at this point is
there so some worries about this or it
is something that we decide it's an
assumption that we make about well
behaving instances so power work about
preservation museums and approximation
stability so bill and Danielle
introduced this form ax cut problem and
they showed that for a square root of n
perturbation resilient they can find the
exact optimal solution i Wasi Blum and
Shefford showed that for center based
clustering which is K Center comedian
k-means a bunch of other clustering
notions under three perturbation they
can find the optimal clustering Vulcan
and the yang improved this to 1 plus
root 2 perturbation resilience mockery
of mockery chip vijayaraghavan which is
cos newspaper introduced that improved a
max cut from below and danielle's and
also showed that for a min multi-way cut
on the four perturbation you can find
the optimal solution for approximation
stability button blonde book tour showed
that 4k means and comedians and also
meant some if you have a one plus Delta
approximation stable instance then you
can find the find the optimal result
quickly
in this talk I'm going to first improve
on this notion which is the three PR and
1 plus root square root of two PR that's
for the symmetric case i'm going to show
that 4k center at two PRS to
perturbation resilience is enough on the
other hand for a symmetric i'm going to
show that if you have three perturbation
resilience you can find the exact
optimal solution and if you have two
approximation stability you can also
find that in the exact solution we also
show that but i'm not going to go
through it in this talk as much that
both of these are tight so anything
under to perturbation resilience or two
approximation stability 4k center both
symmetric and asymmetric stays hard
finding the optimal solution stays hard
to put into perspective what we're going
to show remember that before we have any
stability assumptions if we don't have
any stability assumptions I said that
there is a tight two approximation for K
center and a tight locks are in for a
symmetric version of it under stability
the story changes a lot we have the same
to PR the two perturbation so we can
find the exact solution to to
perturbation but for the asymmetric
perturbation resilience changes the
story a lot all of a sudden from that
locks are angry we can go down to it a
constant alpha and that's pretty cool
for a summit for approximation stability
that even goes farther and in fact oh
sorry and in fact for both k center and
asymmetric a center we will have a two
approximations ability which is for this
case is pretty trivial for a symmetric
not so much so it's really interesting
to see that under these stability
assumptions stood without them two
different problem could be very
different different in terms of
difficulty and the width emm they could
look very similar good so I want to talk
about a symmetric
version on their perturbation resilience
one of the challenges when you're
working with asymmetric clustering in
general is that there are points that
are very their distance in the two
different directions are very different
this is especially a problem when a
point is much closer to a center of of a
different cluster so this then oh sorry
and this is just showing that the two
versions are different a point is much
closer to a center of different cluster
than its own Center the problem here
would be that if I draw a ball of radius
R around each point a point might not
capture any of its own points in the
same same cluster so these points are
not really good we don't want to deal
with them or we want to sort of filter
them and not worry about them too much
for now I'm going to introduce a set
called symmetry set which captures this
notion it's going to include points that
they're forward and backward distances
are almost the same to some threshold
are sir formerly a point P is put in a
symmetry set if for all the other points
if that other point to pee has distance
less than R star p also has this
senseless on our store to Q or even more
formally if this statement holds if Q to
P is smaller than R star this implies
that p to q is also less than R star so
this means that in two directions there
are almost they are both under our star
let's look at an example of this so I'm
going to put the definition of a up
there but if there is any question about
it please let me know and I'll explain
that let's look at this example so for
the maroon point the only other point
that we have in this set is a point that
has a distance to our to it so for p
equal
to the maroon point and Q equal to the
beige point this is always false because
there is no point that has a distance of
less than R star to pee so because the
premise is false this statement is
always true and our point armor one
point is in the set a for the beige
point however there is a we have a
distance of ours are going into it but
we have a distance of two are start
coming out so that's not good so that
point is not going to be in this
Saturday one more example we have for
the maroon point we have a point we have
a distance of our store going in and
again a distance off tourists are coming
out even though they are from two
different clusters and again that's not
good so that's not going to be instead I
either so I just defined a set a so
there has symmetrized points in it so it
would be nice if set a is not empty
because if it's empty I just what I did
was completely useless so I'm going to
tell you about three properties of set a
and these three properties hold if we
have three perturbation resilience the
first property is that all of the
center's are in set a and this is really
nice at least one point and the most
important point of each cluster which is
the center is going to be in this essay
the second point is that if I only look
at set a two points are written the
distance distance R star from each other
then they both belong to the same
optimal cluster note that as long as we
are in set a if two points in one
direction have distance of our store and
the other direction they also have
distance of our star so when we are
considering that a symmetry tells us
that I don't worry about the direction
for the third property now looking
outside of a if I take a point outside
of a the point in a that has the closest
distance to it and that point both
belong to the same optimal
cluster again so these three properties
show that there is a lot of structure
present in the settee not only centers
are part of it and if you look at any
our star threshold of the set a the
points belong to the same set but also
even if you don't look in a and you're
looking outside the set a is going to
guide you and tell you how you have to
put these points in the same cluster so
if we have these three properties then
the algorithm is very natural so our
algorithm first starts by creating the
set a so the blue points are going to be
in the set a and then you're going to
partition it based on our star which
means that any two points that are
within our star away from each other
they are put in the same partition or
you can think of it as draw these are
star edges and the graph that the
connected components of these graphs are
the this graph is the partition after
doing that for any point that was not in
a we are going to connect it to its
closest point in a the claim is that the
clustering induced by this partition is
the optimal clustering I'm going to give
you one minute proof version of this
while going through the algorithm one
more time we are going to create that a
again at this point what do we know it
has all the centers and that was by
facts one so if fact one was correct
then a has all of the sensors then we
are going to partitions that a based on
our star so at this point we know a
couple more properties that because all
the center is very an a and a center
does the distance from a center to all
of its points is our so the each Center
and the points in its own cluster are
now put in the same partition so that be
now we also know that and that's just by
the definition of the radius we also
know that no set has two points from two
different clusters why is that that's
why fact to fax you said that if you are
distance of ours are then you aren't
part of the same cluster so if i made
this partition by our star then by fact
2 i'm not mixing or merging any of the
two clusters two different clusters
together good and what do I know at the
end I know that I connected each point
to its corresponding closest point in a
which also has to belong to the same
same cluster by factoring so if facts 1
2 3 hold then this is this algorithm is
correct leave it affects 1 in 3 and then
you weren't still Nicholas I mean when
are you using there because because the
point p for the second part so i don't
have an example here but let's assume
that there was a point here that its
closest point was this guy not or was a
point that was not the center itself
actually this is a pretty good example
this guy for closest point is this guy
that's not the center so if i don't know
that these two have to belong to the
same set then that's a problem
especially because fact three is defined
for outside of a not inside of it so you
don't need the three fact so it's the
algorithm and the overall proof clear
yeah maybe some things how do you know
our star good you don't need to know it
you can guess it we are in this you're
not in a Euclidean setting right you're
in its setting when you have endpoints
and you have n squared distances guess
our start has to be 1 up to n square
this census I'm run this that's one way
of doing it so if you don't know the
optimal you can always guess it because
you're in a very discreet setting
correctly then I'll get what the wrong
number of clusters or what yeah did you
get the wrong number of clusters it's
necessarily yes yes because this is like
monotonic inkay I guess we go to the
smallest guess where you get to like
he was awesome I nice person good okay
so I want to give you a proof of fact
one and two and fact three we are going
to ignore the proof so sorry how did you
find the sector's sensors I'm not
finding them the center that's the same
to support you see it is clusters huh
exactly so once you have the cluster you
can find the sensor so I'm saying this
partition is the optimal clustering the
fine doesn't receive on the partition
good ok so I'm going to give you a proof
of fact one and two especially because
it's going to use a perturbation so I
want you guys to get a flavor feel of
like how these perturbations are
important factory will ignore for now
okay a useful lemma this is by
definition a useful lemma that is for
any point the distance of that point to
a center that it's not its own is more
than two hours so I want to prove this
and once I prove this fact one and to
follow very quickly from it let's well
let's assume that's not the case so we
have AP and a Q such that P Q sorry
Peter that Center is to our sir so
reversing this so we are assuming that
there is a to our store distance between
P and the center so the distance between
that Center and Q is by definition our
star therefore the distance of P to Q is
at most three our star so the distance
of P to any point in the cluster that
it's not its own is now at most three
our star I'm going to make a
perturbation this perturbation is going
to increase all of the distances by the
most by multiplying it by three except
exactly the distances between P and this
set it's going to increase it to up to
a3 our star
above that we are not going to increase
it so this is going to look like kind of
like this but it's very hard to show it
in fine but what happens at this point
we know that in this new distance metric
the distance between P and any other
point is still the same three our star
so if i were to cluster this this way
the cost of this clustering would be 3r
star and the cost of this questioning is
no more than our star so this is a
three-hour star up approximation in
essence it's not not really
approximation but the cost of this
clustering is three our star in the new
distance also in the new distance what I
did I increase all of the all of the
distances that there are store before 23
our star now so there is no way that my
previous clustering now has a cause that
is smaller than three hours sir so if i
were to cluster the way that it was
before that cost of that clustering is
at least three artists are so i have two
clusterings that both have cost three
our store or the other clustering is
actually better than it so the optimal
clustering is not unique anymore this
would be a contradiction so I created a
perturbation resilience and i created a
perturbation and i showed that the
optimal clustering is not unique anymore
good so this is very simple and now
let's use this lemma which says that
your distance to any sensor that's not
your own is more than to our star to
prove back to an answer the definition
of a there if we need it so we want to
show that all of the center's are in
today consider one of the clusters on
that Center for the points that are part
of that cluster we know that the center
to cluster by definition has distance of
our son so no matter what the backward
dimension is the the second part of this
implication is always correct so the
center is naturally always in that
and that in that set except if there was
a point that was also not in the same
cluster but pointing to you with a
distance of our sir but our lemma says
that this just cannot happen because the
distance of such a point is at least two
our star so any center always satisfies
this implication and any center
therefore is always instead a fact two
says that if you are in set a and if you
are within our store of each other
you're both end up in the same optimal
clustering assume not so we have a P and
Q such that they are within distance of
our star from each other but they're in
two different clusters so then starting
from P going to Q I am paying a distance
of our star what about starting from Q
going to the center q is in the set a
because it's in the set a and because
the center to Q has a distance of our
star the cutest Center also has to have
a distance of our star so the distance
that Q place to go to to the center is
also at most our star given that using
triangle inequality p to the other
center is now at most to our star which
contradicts of them so these two facts
follow very naturally from the lemma
daddy are there any questions about this
so this proves that if you have three
perturbation resilience instances for a
symmetric case center you can find the
exact optimal solution with a very
simple algorithm that I showed you let's
let's go to the symmetric case so for
the symmetric case what you have is that
I claim that a to perturbation
resilience is enough and that's because
there is extra structure in case enter
like symmetry for example that is going
to help us we are going to use an
algorithm that was introduced
swept by balcony Liang when it was
introduced it was shown to have 1 plus
root 2 perturbation resilience to work
for 1 plus root 2 perturbation
resilience we are going to show that 4k
center it does better only to
perturbation residences and off boards
and I'll just give you an overview of
why there are two key properties when
you have to perturbation resilience in
symmetric instances I'm not going to
prove them to you but I'll tell you what
they are the first one is that if you
draw a ball of radius R star our officer
of radius R I at the center then you're
capturing points from that cluster and
only from that cluster which means that
this is not an instance of this property
that I'm telling you because I drew an R
star ball and I captured another point
but if there was a little bit more
structure which is that then such an
example satisfies the coverage public
property draw a ball of your radius
you're capturing the cluster and exactly
the cluster the second all right is it
the radius of hostage I yes so if this
is CI and this is Big C I cluster I RI
is the radius is that the second
property that we call week center
proximity is that each point is closer
to its own Center than any other point
which means that if i take p and take it
to its own center and i take it to
another point in a different cluster the
red edge is always smaller than the blue
edge so these two properties I can show
who that hold for any to perturbation
resilient instance of K center they
don't necessarily hold for other
center-based clusterings actually so if
you have to perturbations resilience
here for K medians for example the
coverage doesn't work so this is using K
center very much like using the
structure that's that exists in case
center
based on these two properties there is a
notion called a closure distance notion
which says that for any two sets that
linkage the closure linkage distance
between these two sets is defined as
follow I will choose one Center and I
will draw a radius of our ball around it
to properly should hold the first one is
that this ball should capture both a and
B and the second property is that
anything else that also it captures
including a B and more should be closer
to this Center than to any other point
outside outside of this well this
already sounds very much like the two
properties that I explained to you and
there is a reason otherwise they
wouldn't have come up with this notion
of distance let's look at an example I
have a set ABI want to see what their
closer distances so i will pick a center
and i will draw a radius of our around
it whatever ideas this is our so do I
have the properties or not so a and B
are definitely captured so that's good
but there is a point where the red edge
is bigger than the blue edge so this
cannot be our star this cannot be the
closure distance so i have to go
slightly further so now i go further to
capture that bad point and now any point
that is within here is closer to the
center than any point outside of it so
this is going to be that the closure
distance of set a and B
let's do one more example i want to show
you that if i take two subsets of the
same cluster and i make sure that one of
them has the center of the cluster in
there then the closer distance of these
two is at most are I which is the radius
of that sensor
okay so we have the sets a and B if it's
not clear a and B are the bigger sets
I'm going to draw a ball of radius R I
at the center and i'm going to show you
that it has the desired properties so
then it has the first property because
we said by the coverage property if I
drew a ball of radius R I I capture
exactly the cluster so that's good has
the first property for the second
property let's assume that it doesn't
have it then we'll actually what by the
by the big Center proximity by
definition you have to be closer to your
own Center which happens to be the
center of the optimal cluster then to
any points outside of your cluster this
is exactly what we sent your proximity
says so if you have two subsets of the
same cluster and you make sure that the
eye is in one of them the closer
distance of the two is at most are I
good now I'll tell you what closure
distance algorithm actually does it's a
linkage based algorithm it's very simple
it starts once you understand the
definition of closure link it close your
distance is very simple it starts with
all the points being laid down like that
and it's going to consider them each
being in one set and now it's going to
merge any two sets that have the
smallest closure enclosure linkage
distance so this is going to be an
example of it and it will continue it
won't stop at k we'll just build this
tree until it there is one cluster left
so this is the hierarchical clustering
of this instance but we want to exactly
K centers so we are going to find the
best pruning and we do that by dynamic
programming what is a pruning the
pruning is that will cut this tree at
some point and this would be the
clusters left for it we're going to do
that and find the best cost cluster of
size K that's left another pruning could
be dis and dirty requesters left
this point so this this algorithm works
given one condition and that is if I did
not merge any two clusters that were
incomplete together at some point so I
want when I look at one cluster at this
point it's either a subset of an optimal
cluster or it has multiple complete
clusters in it it cannot merge half of
one closer with another half of another
cluster so if such a condition holds
then the algorithm works by intuition
what I want to tell you is why it makes
sense that this condition also works I'm
not going to give you an a real proof
but I give you an intuitive proof let's
take the simpler case which is we are
taking two subsets from two different
clusters and I want to tell you that
this is not going to merge like this
merge is not going to happen at any
point in the algorithm why not so a
might have the center in it or might not
have the center in it if it does then
good if it doesn't let's take another a
prime that does have the center of CI in
it by what I explained to you two slides
ago we know that the closure distance of
these two is at most RI if you want to
remember it's because the coverage the
ball keeps all of the points and bye
week center proximity nothing is closer
to point out side down the center so
definitely for a prime na the closure
distance is RI what about 4am be okay so
let's see what they caused the closure
distances will take one Center will draw
the point the the ball and now we look
at the center that was in a what happens
the center it's definitely closer to its
points it's in its own cluster than the
points outside this is why the coverage
we said if we draw a small ball I only
capture my points so the center is
closer to a point outside so this cannot
satisfy the closure linkage so if i want
the closer distance I have to go even
farther and capture the whole set a
which means that now I have to pay more
than our I this was just a an example of
the proof this is not a total proof
because there are cases we didn't cover
their cases like so what if one of them
is a complete cluster this is a much
more challenging situation so I'm not
going to cover it but intuitively just
because the larger the cluster is the
larger is going to be the good quadra
distance and the property is that they
had weak center proximity and the
coverage is going to help us with that
so this shows us that we have to
perturbation resilience is enough for
case center we also have lower bounds he
said that there is no polynomial time
algorithm for symmetric instances of any
two minus epsilon approximation stable
cluster so let's remember that if we had
alpha approximation stability we had
alpha perturbation regime so our lower
bound is for this stronger notion of
stability than perturbation ruling so it
implies perturbation resilience lower
around as well it's also for symmetric
any symmetric instances a symmetric as
well so this result says that bunch of
our results are tight which is the two
perturbation resilience for symmetric
that I just showed you the two
approximation stability for asymmetric
that I didn't show you what is very
similar to the three perturbation way
what is not known to be tight is the
three perturbation resilience for
asymmetric it will be very interesting
if we figure this up but we don't know
right now if the two perturbation the
algorithm we have is tight or not
I said that perturbation resilience we
are considering this scenario that the
fluctuations do not make any difference
in the clustering it's very this is not
a robust nauseous so we want to work
with a more robust notion of
perturbation resilience which we call
which is called alpha protocol alpha
epsilon perturbation resilience which
means that alpha perturbation you're
allowed to be to change by an epsilon a
little bit of change in membership as
long as you have alpha perturbations in
the paper we show that a 4 epsilon
perturbation resilience for Chase Center
is enough and the nice thing is that
actually single linkage will give you
that it's an elaborate proof I thing if
you have like six epsilon is very
trivial but for epsilon is it takes a
little bit of time so you you need your
clusters to be larger than like
excellent and like two epsilon n or
something like that you need some
largeness of Questers assumption
actually we have lower bounds us as if
they are not large and it's already
difficult to spot you get big data yes
you get the exact option we have more
results for the robust notions of
approximation stability that I'm not
going to mention let's go back and think
a little bit about what we showed that
we started working with these like
natural stability notions like
approximation stability and preservation
resilience where the hope when these
notions were introduced was that if I
have 1 plus epsilon stability or 1 plus
epsilon perturbation resilience and I
can still find either approximate or
exact solution that was the hope so our
lower bound kind of was negative in that
sense even though so upper bound we
showed that great you know we can take a
problem that doesn't have a constant
approximation ratio and show that three
perturbation is enough this was a very
strongly positive result our lower bound
is somewhat negative that says that even
a to perturbation resilience or two
approximation stability they're simply
not
enough structure and to find the optimal
so that means that going back we need to
think hopefully this won't happen for
cami's and comedians case center is much
more sensitive to distances so the hope
is that this will not happen for k
center anton for k-means on comedians
but it would be very interesting to see
if you can even get less than 1 plus
root 2 that's the best known result for
it right now and if it happens for
k-means or k medians or if not in
general for k center we do need a
stronger notion of stability what would
be a reasonable nor sure that's not
assuming way too much so something
should blow up as actual ghost mm-hmm
but I'm not question was like well I
mean cuz you're not supposed to know if
someone right orissa you're so for if
you look at the so for example 1 plus
epsilon approximation stability results
the balcon blummo the wrong time is also
in epsilon and also that the restriction
on the cluster size they have rate is
based on epsilon so there are definitely
things that won't work when epsilon is
there so y'all know I'm so I don't know
about their case um you have multiple
results I don't know yeah but generally
the hope is that you want to be able to
say I mean to perturbations to
perturbations is kind of a large
perturbation but 1 plus epsilon you can
justify very easily so that's sort of
the hope and this lower bound is
somewhat negative because of that so we
do need if we want to consider k center
or even came k-means and comedian
depending on what we get is that what
would be and more natural stable
sibility result that's not a selling way
too much
to conclude it would be interesting to
see how a symmetry and stability work in
general so we showed that we showed that
for the asymmetric clustering objective
how things worked but in general there
are other asymmetric clustering
objectives like what happens with like a
symmetric Amy it means or asymmetric
comedians that will be interesting it's
also very interesting to see if the
hardness that we showed is it tight for
also three perturbation asymmetric
situations or not because there's a gap
of 2 to 3 4 perturbation resilience
considering a symmetric so it will be
very interesting to see what that actual
values and with that I want to thank you
are there any questions you'll be
allowed in some sense are using this
property right mapping the doctor you
are is an internal motivation to some
extent because this is yes so r example
is this cool let's say like this would
be to say that how said not 4k center
particularly nobody of this fault one
national approximation our lives are
just you solve a matter of like a packet
one realization typically the LP and
tied around it can you say for example
under your assumption is insistent that
we saw their people actually a solution
will be integral because intuitively the
picture in mind is that these clusters
are so far away from each other right
like your other thing can easily find
out because they don't really attack
because if they attracted or they close
finally like one of the point changes it
put it put it a different bus and you
know the optimal solutions are not
changing unless we should do tell you
what the right answer is oh I'm not sure
I'm not sure if I can guarantee that um
the algorithms for example the closure
distance algorithm is like given the
property is it's sort of a natural
notion of distance but if you don't know
those properties doesn't seem like
something very nice
so I'm not sure if it will imply
anything nice about the lp solution but
the properties that I prove definitely
or showed they only hold for two
perturbation resilience or three
perturbation for that situation because
if you go and they're tied if you go
below that value those properties break
and in fact we have a section in our
paper that is trying to address so 4k
Center sure we got down from 1 plus root
2 22 what about k-means on comedian so
we are trying we we noticed that these
properties break so we try to replace
these properties assuming some
additional things so maybe that's sort
of like a first step to moving beyond k
center and moving beyond 1 plus root 2
in future but it's not very clear that i
don't know i don't think it would be
easy just to show that the LP is all of
a sudden behaving nicely like you have
to analyze that separately i think one
of the issue of these i wasn't it that
they are not very robust innocence so
for instance you talked a little bit
about this alpha epsilon yes resilient I
mean that seems much much more
interesting ah but what you showed us is
nuts I'd by epsilon is it that's true so
the the Alpha Epsilon are the
implication of it is much more
interesting I agree the results are much
less crisp that's why I didn't show it
to you and like do we know there must be
a separation so between our 50 n alpha 0
no I don't me don't know my separation
for it so the best result we have 4k
Center is what I have wishes we have
issues the for epsilon and I guess my
like we think you can do 3 epsilon but I
I don't know what evolution
that she's so the lava the robust
algorithm for the for the for epsilon is
single linkage it's yeah so it's really
it's showing the showing that their
property is at home okay oh no no single
linkage okay but we're stopping it but
in legal options to the clusters of a
three yes so that you have to do if you
don't see how partners result but it
means it's a natural rhythms people why
she run so that's yeah that's nice
that's a good thing that's exactly yeah
we have other sort of linkage style
algorithms that like single linkage
cross some prepares pre-processing or
post processing that also shows i think
what is interesting this it's real
algorithm no it'll be very surprising if
there'll be is not indeed all right
because if these clusters are very far
away dearly there'll be can't cheat it
has to return right to be this is 510 I
don't know what but of course the game
is not a plane to foster dog make sure
they're 15 teen rating when you
practice with your sake of you would you
actually run something like single
linkage the other is doing for that I'm
glad you probably you can implement I
shall get it move yeah these are like
ants for Matt Morris is highly one point
of that typically like if this way you
will do if you have this instance a is
that was what we're going to do more
yeah that's a loss of you get awesome
easier ones don't get kind of some kind
of trade-offs between if for example you
care not about David the exact solution
but just said somehow trade-offs between
yeah keep the art studio two
approximations can go to Memphis look
you can't be happy how one is also
platform that you get a two
approximation assuming i don't remember
exactly which one of this but yes
there's a setting that you don't get an
exact you get some approximation for
slightly better part of it perturbation
Rizzoli so so you can definitely do that
our heart nurse results of course don't
cover that so I think it would be doable
I don't know if a nice connection
between them though that would be cool
more questions and let's think this
trick again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>