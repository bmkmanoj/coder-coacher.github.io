<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>On the connection between the Kannan-Lovasz-Simonovits conjecture and the variance conjectur | Coder Coacher - Coaching Coders</title><meta content="On the connection between the Kannan-Lovasz-Simonovits conjecture and the variance conjectur - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>On the connection between the Kannan-Lovasz-Simonovits conjecture and the variance conjectur</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/rxvDUuXNTqI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay good morning everyone we're happy
to have run and all done from Tel Aviv
and the Weizmann Institute tell us about
the canal novice similar which
conjecture and the variance conjecture
all right thank you very much for this
invitation and this opportunity I hope I
can make the stock work despite of the
jet lag so I want to talk about the
connection between the kls or command
love us shimano fates conjecture and the
so-called variance or thin shell
conjecture so I want to start with our
main topic will be isoparametric
inequalities on convex sets or on
uniform measures on convex sets and will
actually want to consider sir a slightly
bigger family of measures namely log
concave measures so on I want to start
with a few definitions the first one is
just the definition the basic definition
of a local cave measure that's just the
measure whose density has a logarithm
which is a concave function so a typical
example of a low concave measure is just
the uniform measure on some a given
convex body another typical example is
the Gaussian measure if you were if you
feel inconvenient with considering glow
concave measures you can always imagine
this thing is just uniform measure over
some convex set and will need a certain
normalization for measures will see why
in a while so we'll just normalize the
measures to be isotropic namely will
define be of MU as the berry center the
center of mass of the measure and curve
of me was the covariance matrix of the
measure
and we'll say that it's isotropic if
it's centered at the origin and has the
identity covariance matrix it's an easy
exercise to see that any random vector
can be under some fine transformation
with a little-known degeneracy condition
can be made as a tropic all right so
we're talking about isoparametric
problems for this we want to define the
surface area of a set with respect to a
measure so let's imagine our measure as
the uniform measure on some context body
and then we have some subset of this
convex body let's call it t then the
surface area measure of okay here it's
called a the surface area measure of a
mu plus of a is just you take a little
extension of a yeah I guess so you just
take an epsilon extension of a you look
at the measure inside this extension and
then you take the ratio between this and
epsilon as epsilon goes to 0 this will
be called surface area measure and our
main interest will be the following
constant which I also defined here for
your convenience so out of all possible
isotropic low concave measures and all
possible subsets of measure exactly
one-half so we're talking about
probability measure so you can just
imagine this is the most efficient way
to cut an a convex set into two parts of
equal mass and the minimal surface area
would will be defined as GN to the minus
1 and we are
looking where what our main objective
will be to try to prove an upper bound
for this constant GN the significance of
this i'll talk about it a bit later now
let's try to understand why I want this
normalization that the isotropy City why
do I ask my measure to be isotropic so
first of all just notice that if I don't
have this constraint then it's a bit
pointless even if I want my convex body
to have what volume 1 say I can consider
bodies which look like this for example
very long and thin and then I can always
cut them into two halves using a very
small surface area and well obviously
the definition would be wouldn't make
any sense now I isotropy city so this is
a natural way to understand it for those
who of you know the proper landlord
theory me just state that marginal is
one-dimensional marginals of low concave
measures are also low concave the
marginal will also be isotropic now this
family of one-dimensional low concave
isotropic measures attain some
compactness well I want formally define
it but they are compact enough for
example in order for the density at the
point of median to be bounded between
two global constants to universal
constants this means that if you think
about the geometric meaning of this it's
just that if I cut my body using a
hyperplane if my surface is flat then I
know that this surface area will be
between two universal constants so
another way to think about
sighs otro p city instead of writing
Kyra is a tropic I could have written
measures such that all hyper plane cuts
into two halves have some surface area
bounded from below by one say all right
now it's well known that this constant
GN has many equivalent up to a universal
constant equivalent definitions so by
works of Boozer chigger room of minimum
LeDoux and de manuel minimum sum of the
equivalent definitions of gnr the
following so it's the optimal spectral
gap of the noi minh laplacian on an
isotropic convex domain in RN and we
know that Lipschitz any Lipschitz
function admits exponential
concentration around its mean with
exponent equal to G n to the minus one
so many proofs of facts using the
concentration phenomena in high
dimensions actually rely on things
related to this constant the gnomon a
spectral gap is also I guess all of you
know that it's highly related to the
mixing time of random walks on convex
bodies and this is the reason why this
constant GN was also interesting first
for some computer scientists working on
algorithms related to high dimensional
convex set so I'll just mention that the
first I think the first result where
random walks on convex sets were applied
was this breakthrough result by dear
fritz and canon excuse me if I don't
pronounce the names right who
demonstrated that rather
privately the volume of convex body in
high dimensions maybe esting
approximated only in polynomial time and
the way to do it relies on the fact that
the random walk in a convex body has a
mixing time well it relies only on the
fact that it's polynomial but the
complexity is actually a direct
derivative of the mixing time and this
is why it's interesting for us okay
because this is an interview I was urged
to mention here that one question asked
by lavas was whether this polynomial
algorithm is possible if you're just
given random point from the convex body
and in 2009 I proved that it's actually
impossible so this algorithm relies on
the fact that your Oracle to the body is
the membership Oracle you give a point
and it answers yes or no to the question
whether this point is in the body all
right some other algorithms related to
convex bodies also rely on these mixing
random walks for example if we want to
optimize low concave functions over the
body or do a PCA just calculate the
covariance matrix of the body so these
to rely on sampling which in turn relies
on the make that the complexity of
sampling is just the complexity of how
much time it takes for a random walk to
mix so these are all related to this
constant GN all right it was conjectured
by canal novice and Simone of it that
this one this GN is actually bounded
from above by some universal constant
independent of the dimension in other
words what the conjecture is that in a
convex set that in order
to cut it in two one half I can
equivalently just check only flat
hypersurfaces and if I know that all of
those guys are bounded by some constant
then actually it will satisfy an
isoparametric inequality with
essentially the same constant this is a
pretty bold conjecture it right now it's
known for very few classes of convex
bodies in the original paper they only
proved this dependence not up to some
constant but they have this extra factor
of square root of n they use some
localization method we'll talk a bit
about later but right now if we believe
that the the GN is also equivalent to
the optimal exponential concentration
constant then this can be proved rather
easily we just note that by the
definition of isotropy City the expected
norm square is just equal to N and then
we use Markov inequality to say that
most of the mass of the bodies in
distance no more than faith ten times
root and from the origin and then we use
a classical theorem of Burrell which is
actually just a straightforward
application of the brew Minkowski
inequality that says that once the mass
starts decaying then it continues the
king exponentially and well it follows
from these they think that any Lipschitz
function will admit an exponential
concentration of this type and of course
i have this factor root and here which i
want to i mean the kls conjecture search
that it's true without it ok I want to
talk about a slightly weaker type of
concentration on convex bodies namely
and maybe more relaxed isoparametric in
a quality called with called thin shell
concentration so as we saw when the when
X is isotropic the expectation of the
norm square is just em and we define
this constant Sigma and which I also
defined here for your convenience as the
supremum of the standard deviation of
the norm and it turns out that the
following thing is true the standard
deviation of the norm is all always much
much smaller than its expectation this
quantity the norm of X is always
concentrated around its expectation if
you think about the geometric
interpretation of the sink it just says
that most of the mass of my body is
concentrated in a very thin spherical
shell because this radius here of the
shell is presumably much bigger than its
thickness okay the first type of results
like this was proved by both claw dog in
relation with the so-called central
limit theorem for convex sets which
unfortunately I don't have time to talk
about several alternative proves and
improvements were introduced by some
other guys and currently it is known
that this constant Sigma n is smaller
than n to the 1 over 3 so this distance
is n to the one half and the thickness
of the shell is n to the 1 over 3 which
is much smaller this might be surprising
for people who are unfamiliar with high
dimensional convex geometry but this is
true and it's actually conjectured in
two different papers by until abhorrent
person akhiyan Bobkov called upski the
dis constant Sigma n also can be bounded
by some universal constant this is
actually known for
several classes of convex body's already
and let understand why I called this a
relaxed version of the isoparametric
inequality so as I mentioned above the
isoparametric inequality corresponds to
a spectral gap which in turns gives
which in turn gives us that one Quran
equality like this now if we use this
point Quran quality just with the
function actually forget the square here
just with the norm of X then the
gradient is always smaller than one here
we get GN square and here we just get
the variance of the norm so we see that
well the ISO perimetry immediately
implies finchel concentration a nice way
to think about it it's just the
following either perimetry means that
the surface area of any surface cutting
the body into two halves is small and
Finchel just means that one specific
surface namely the euclidean sphere
taken to be with the radius that divides
the mass into two halves doesn't have a
big surface area so this is just well
it's either perimetry for a specific
case ok so we just saw that the kls
conjecture implies the thin shell or
actually I didn't mention this it's also
called the variance conjecture the thin
shell conjecture now what about the
other way around so using more less the
same lines of the proof of the kls paper
itself Sergey Bubka managed to show that
under some failure Finchel assumption ok
actually we get this kind of estimate so
what's on the left side here is just
that one correct constant it can be
replaced by the optimal constant in the
pond Carini quality here we have the
thin shell Khan
stand but there we have this extra
factor which is equal to square root of
n if our random vector is isotropic so
this implies that we have this relation
GN is smaller or equal to n to the 1
over 4 times root of sigma n we just saw
that sigma n is smaller or equal to GM
so we do have some some bounds on GN in
terms of Sigma and already and under the
finchel I patha this if it's proven then
we'll actually know that we're halfway
to attaining the kls conjecture so from
n to the one-half we got to enter the
one over for the main theorem I want to
talk about now is the following we
actually have a logarithmic dependence
between the two constants so we actually
prove something slightly stronger and
maybe this is a nice way to formulate it
up to a factor of square root of log and
the worst isoparametric sets or
ellipsoids so it's conjectured by kls
that the worst isoparametric sets are
flat or hyper planes the ISO perimetry
is for any surface and well according to
this theorem if we're willing to give up
this root log and factor then it's
actually attained the worst surfaces are
actually ellipsoids now along with the
best known fincher bound we also get
some improvement of the best known and
kls bound using this theorem it's
important to mention that the bound we
get is actually global and not local if
we know the venture for a specific
convex set a specific local cave measure
it does not unfortunately does not imply
an isoparametric inequality we somehow
have to use you'll see
this in the sketch of proof soon we
somehow have to use thin shell for an
entire family of low concave measures in
order to prove I supper imagery this is
a good place to mention that inner in a
joint work with Bossk leg we also shown
that the thin shell conjecture implies
this though the so-called hyperplane
conjecture or the slicing problem which
is another central problem and I
dimensional convex sets so if you don't
know it's just conjecture about the
maximal sorry minimal possible volume of
the convex set in isotropic position so
okay I just sketch this to to make some
or to give some order here so we have
the kls finchel and hyperplane
conjecture by chiggers theorem kls
implies thin shell this is bob coves
theorem I just mentioned this is the
theorem we're talking about now which
only glue works globally unfortunately
together with Baz Claire tag we know
that it also implies the hyperplane
conjecture and there's also a wrecked
implicate a reduction of hyper plane to
kls by ball and angwen but unfortunately
on only with exponential dependence so
currently we have all these connections
known all right so I want to start with
sketching a first attempt to prove our
theorem so this is in some way based on
what kls did in their paper this is not
with what will actually use but let's
try to go for a first attempt how to try
to prove some isoparametric inequality
so i'm starting with
convex body k and let's assume that it
has some subset T of measure one-half
and I want to do the following process I
choose a random direction theta in the
unit sphere let's say theta points this
way and what I do is I cut the body k
into two halves through its berry center
with a hyperplane whose random direction
is Theta and I call I ignore this half
and I call this fk1 and then I continue
this process i generate another random
direction let's oh sorry i generate
another random direction and then i cut
the body k 1 in its a berry center again
only this time I cut it I well okay if
instead of this formula what what you
can try to think about is that at every
step I don't generate the direction from
the unit sphere but I generated
uniformly from the ellipsoid of inertia
of the body i'm currently looking at so
this normalization here with the
covariance matrix just makes my process
into a markov process on convex bodies
in some sense i'm i'm always looking i'm
doing this cut in the world where my
current body is isotropic and i continue
this process on and on i cut again and
again and again until I get a localized
version of my body some much smaller
body and let's try to think what this
may teach us so one observation we can
make is the following so if I choose
this direction theta from the sphere i
can define this function f as a function
on the sphere which just
measures the relative volume of tea with
respect to K after I make the cut now
it's not so hard to say that this
function will be say ten lipschitz I
mean where 10 is the constant
independent of the dimension now it's
well known that 10 Lipschitz functions
on the sphere are actually very
concentrated around their mean the mean
is definitely one half and this means
that I'll get something like one half
plus minus ten over square root of n
something very close to one half the
expectation is one-half and this means
that we can iterate this process more or
less n times we can cut again and again
and again and still have some
non-negligible probability that the
proportion of t from what we get so we
have a non-negligible probability ending
up with something that t is more or less
one half of which ok it's not that will
always end up with something located
here or here that doesn't see the
boundary of tea at all we have a pretty
good probability of seeing the boundary
of tea after making many cuts and this
is pretty good because if we knew
something about what the body k and
looked like if we knew something like
what about what happens after n cuts for
example if we know something about the
renewal time of this Markov process
maybe this body doesn't even depend on
what we started with so much or if we
know some isoparametric and equality
about it this would in turn imply that
the the surface of t is quite large so
we might have been able to say something
but unfortunately I can't find any way
to say anything about those bodies km so
let's try
to consider a better I mean at least for
our purposes may be a better
localization some a slightly softer
process instead so I pick some constant
epsilon which will be small and instead
of try truncating an entire half space
every time what I do is I just take my
measure which I now consider as a
measure in the class of low concave
measures but not as a convex body I take
my measure and I multiply it by a linear
function which is equal to one at the
berry center and the gradient of which
has a random direction so instead of
killing the entire half space they just
give a little more mass to one of the
half spaces and I continue this process
again and again I * more and more linear
functions and I do it again in a
Markovian manner I normalize my my think
to be isotropic and at each step how is
this better so it turns out that here i
can actually maybe say something about
what i get after many iterations so
let's let's consider the one-dimensional
case for a second if we in the
one-dimensional case after doing this
think twice i get some linear function
but i also get some quadratic term right
then it's the product of two linear
functions and if we do something like 1
over epsilon square iterations then what
i get is the product of many such things
which is roughly an exponential a I mean
the exponent of some quadratic function
and in a higher dimension I will I will
expect my measure to be more or less x
some Gaussian density
now measures of this form a low concave
measure time some Gaussian density for
those who know the brass compli borba
Creamery results for instance which I
will mention later those kind of
measures already attained some a some
spectral gap so I can already say
something about them only provided that
I know that my quadratic term is large
enough so if I know that this matrix a
is quite large I mean it's bigger than
say 1 over 10 times the identity then I
know that this measure said already has
some good concentration properties and
I'm kind of done ok but unfortunately I
have no way to quantify what is to
analyze what this matrix a looks like
but what i can do is i can change this
process such that i will be able to say
something so what I'm gonna do is I'm
going to define a continuous version of
this process so maybe I'll write it down
on the blackboard because this is
actually the main tool in our proof so
I'm starting with a Brownian motion WT
and then I solve the following system of
of stochastic differential equations f 0
of X is just the function 1 at x 0 and x
1 and DF its time T will be at the point
x will be ft at the point x times some
linear function so again I have this
random gradient and I have this
normalization and my function little F T
will just be my initial function times
this function capital F so I get a
one-parameter family of random functions
and here little 80 is the berry center
of ft and capital 80 is the covariance
matrix of FD so this is our process and
let's try to see how it can be helpful
to us let's start with some basic
properties of it so first of all you see
are an infinite I'm in a system of SDS
for every X you have an FB but it can be
shown that these are actually solvable
it has a unique solution and it's finite
and not non negative for any time T as
you see here F at any X is a martingale
it's only as a martingale term here
which means that if I want to for
example measure the surface area of some
set e what I can do is just take the
expectation of the surface area at some
positive time T
it's pretty easy to calculate thanks to
the fact that we're multiplying by
something equal to one at the barycenter
then we'll always have a probability
measure it's a simple observation and
also thanks to this normalization here
this is a semigroup so again it has the
same or covent property we can run it
for one second and again one second in
its run it's like running it for two
seconds thanks to this fact essentially
by ideologically by this using with the
same ideas of compactness we seen before
this fact will make sure that we can run
this process for a rather long time and
the proportion of some subset e from all
of our measure will be kind of close to
one half so the idea we used before
using the fact that we are the Markov
process we can still kind of using here
using more or less the same ideas okay
yes is that this is sort of the
continuous version of the PV yes yes yes
yes ennis it sort of is it exactly what
you would get if you somehow took lots
of very yeah well if if I look at this
and I take at epsilon 20 ideologically
what i get is this yes yes yes okay and
now for a very nice property of this
thing if I if I apply it owes formula to
the logarithm of F this is just a
straightforward calculation what i get
is the following I have this diffusion
term which is just a linear function and
then i have this ito term which is a
quadratic function so what I actually
get now I now I mean what we
approximately guess
we would get in the non-continuous
version here we actually get that at
every time step at every time T we what
we do is we actually multiply our
density by some Gaussian density and as
we said before we somehow want this
quadratic term to be big we want to it
to have a high curvature in some sense
so here we actually know exactly what
our quadratic term will be it just turns
out to be from what you see here it's
just the integral between 0 and T of the
covariance matrix of the measure we had
along the process now as I mentioned
before by results of Boozer brass camp
lip gross Bakri Marilla do etc I guess
the what we actually use is the hype so
called hyper contractivity principle by
Bakrie emily and what it says is the
following if BTW is larger than some
multiple of the identity then the
function f which is then actually any
function which is a low concave function
times this function attains a respective
isoparametric inequality so maybe I'll
write it down here so if the function H
of X is of the form some low concave
function I don't care which as long as
its low concave times e to the minus
alpha x square then the spectral gap of
H is at least ARF
so great so what we learn from this is
that if BTW is rather large in other
words if i managed to keep this matrix
rather small if i managed to keep the
covariance matrix along the process
rather small then for free i get some
isoparametric inequality for the measure
i get after say one second so if we
connect all of this together what we get
is the following if we if the measure
along the process we call it mu sub T
then the last equation with say the
Bakrie Emery result shows that if the
operator norm of the covariance matrix
is smaller than some constant alpha
square for a large enough span of time
then the function ft will satisfy this
so I I will have an isoparametric
inequality for any set which is not too
small and not too large this can be
ensured using the this compactness I
talked about before so all we really
have to do is make sure that this
operator norm remains small for for long
enough so at this point we reduced the
isoparametric inequality to just saying
something about the norm of some a
matrix valued stochastic process okay
and it turns out that actually this
process can be analyzed using some
stochastic calculus related tools so
I'll just I have 10 more minutes right i
think i have till 11 25 okay so i'll
just briefly go over how this can be
done so this matrix
80 which is equal to so 80 at i J is
just the integral of x i minus 80 XJ
minus 80 I i have this covariance matrix
and if i want to try to analyze how its
entry is very so its entries there it's
actually easy to see that there are ito
processes i can in order to know this
differential i can just differentiate
inside the integral and i get this
linear term i actually have to also
differentiate little 80 but after doing
some calculations and estimates what we
get is that roughly the differential of
the entries of the covariance matrices
are just integrals of the grief free
monomials over our measure this is not a
very surprising fact just because what
we do is we multiply by linear functions
and the nice thing is that thanks to
this normalization that we had what we
get is I won't get into the formulas but
what we get is that somehow we integrate
three third-degree monomials over some
function which is always normalized to
be isotropic somehow we always normalize
such that at any given time the measure
we considered is is an isotropic measure
and what we conclude in the end is that
the differentials of the entries of the
matrix are just some vectors defined by
integrals of third-degree monomials over
some isotropic measure the Tropic
measure is just more
original measures times some Gaussian
density actually and normalized began to
be isotropic and well these things can
be estimated using everything we know
about as the Tropic low concave measures
so now what we can do is we can
differentiate we want to bound the
operator norm so we can differentiate
the eigenvalues with respect to the
entries well more or less up to some
cheating we get that the these things
are close enough to be it'll processes
there it'll processes whenever all of
the eigen values are different actually
but it's good enough for us and their
ito processes whose diffusion term
corresponds to those vectors III hands
the diagonal diagonal terms of this guy
and we all we also have some repulsion
between the eigen values which also
corresponds to some i mean the the
magnitude of repulsion can be bounded
also buy vectors of this form and okay
so this is what I just said and in order
to bound the operator norm what we can
do is we define an energy term like this
and this actually turns out to be
exactly an 80 process and it turns out
that the repulsion between the eigen
values is actually the significant thing
the diffusion gives us some logarithmic
term and the repulsion is somehow well
bigger unless we knew something about
the thin shell conjecture and what what
we get is that the drift of this guy can
be bounded by an expression like this
which is just well okay at this point we
see that already we
reduced the kls conjecture to saying
something about the expectation of some
monomials of degree 3 instead of well
the kls conjecture talked about all
Lipschitz functions now we only have to
know something about monomials of degree
3 and the thin shell conjecture is just
saying something about so it's it that
the variance of the Euclidean norm so in
order to know that we have to know
something about monomials of degree 4
right because the Euclidean norm is just
a polynomial of degree 2 and I won't get
into the details but this guy here can
be rather easily bounded by Sigma N and
this more or less concludes the proof so
I want explain out to do it but I do
want to talk about one maybe a
algorithmic application of this method
so say I have a convex body and I want
in polynomial time to say something
about to say whether or not this body
attains a spectral gap so what this
method can give us it can find the lower
bound for the spectral gap in polynomial
time now giving a lower bound is rather
is e0 is always a lower bound for the
spectral gap so it's not it doesn't say
much but what we actually give is that
is a lower bound such that if we have
some convex body that we have a
non-negligible chance of such that that
the algorithm will return something very
low then we will know that the kls
conjecture is force so somehow if we so
one thing you can do with such algorithm
is that if
you have somebody and you think that
this is that this body is a
counterexample to the KLS conjecture
then now instead of checking all
possible Lipschitz functions or all
possible subsets which is something that
definitely needs exponential time right
now you don't need exponential you only
need polynomial time in order to to
verify that the body or you're holding
in your hands is in fact a
counterexample to the kls conjecture and
well how do we do that based on the
method we've just seen so what we saw is
that in order to know that the body has
a spectral gap we we can it's enough to
check that the operator norm of this
matrix along the process is not too big
so what we do is we just discreet it we
do it we make a discrete version we run
a discrete version of this localization
and we test this matrix 80 along the
process and well in order to estimate
this matrix 80 we can actually do it in
polynomial time why because our measure
is just some Gaussian density restricted
to our convex body and we can actually
sample from this measure by the standard
a means of reef love taking your
reflecting random walk but since we have
some density then we'll also have this
drift term but we can still do that and
in in this way we can actually run this
localization on a computer and somehow
test what happens to this matrix and
this gives us this theorem all right
I'll end here
offensive Last Theorem you need the
sequence of convex bodies no no I am
given one convex body but I what I get
is a sequence of low concave measures
which are obtained by taking this the
uniform measure on this convex body and
multiplying it by some Gaussian measure
and these measures are attained randomly
i mean i calculate i take a random
gradient i multiply by some linear
function and if i do it enough times
i'll get something which is
approximately a sequence of the of some
Gaussian measures restricted to my body
rhythm well okay if I assume the kls
conjecture then the algorithm could be
returned one or something like that but
because the careless conjecture just
gives and well okay not exactly because
I don't know that the body is isotropic
but what the algorithm would do is it
would check the position of the body
calculate its covariance matrix and
judge and then just output the smallest
eigen value or something like that yeah
yeah yeah yeah but I am actually glad
you asked me that because even if the
careless conjecture is proven then it's
not our to see that this algorithm would
work for not only for convex bodies but
also for measures which are not tulo
convex any measure whose logarithm has
Hessian bounded by some matrix which may
may be positive but not so large this
algorithm also works on it so well it it
actually works for a larger
class of measures on which there is no
hope of proving an isoparametric
inequality so this can still be useful
so even without the GLS conjecture the
tableau Griffin give an approximation
for it a draw spectrograph of cage well
the thing is that it it it kind of if if
k doesn't have a spectral gap you will
know it but if it does it might still
give you some answer that seems like it
doesn't have a spectral gap and this is
because one of these measures along the
process doesn't have respect or gap so
somehow what it gives you is is the
spectral gap of of a certain family of
measures which kind of implies the
spectral gap of K I I mean well if you
really want some some approximation than
you you also need an upper bound somehow
and this I don't know how to prove you
mentioned in the instructions on
interleaving the stability result give
you what
yeah okay so this has nothing to do with
this the impossibility result is just
take a convex body k and a take aunt n
to the 10 random points from k based on
this and to the 10 points it was not
known whether you can say what the
volume of k is SE up to a constant of i
don't know 50 a multiplicative constant
of 50 and the impossibility result is
just that this is impossible there can
be there are two bodies with very
different volumes such that if you look
at the total variation distance between
the random points somehow got from these
bodies it's it's very big no no
algorithm can distinguish between the
two this learning problem is only
possible to supervise
exactly so it yeah I mean there are many
supervised cases we're talking about the
specific Oracle where you you give point
to your black box and it tells you yes
or no whether it's in the body or not
yeah
again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>