<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Abstract Conflict Driven Clause Learning | Coder Coacher - Coaching Coders</title><meta content="Abstract Conflict Driven Clause Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Abstract Conflict Driven Clause Learning</b></h2><h5 class="post__date">2016-07-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/E2GMQ_O4URs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
hello everyone thank you for coming so I
am very happy today to introduce the
talk / Leopold other Wis PhD student in
the group over then your crowning in in
Oxford and it's going to talk us about
some very interesting walking as made on
an abstract complete drivin closed
learning it's a ready so I part of the
stork in SES last year I was very
interesting about it i think is very
interesting so let's know more about the
particular thank you thanks very much
yeah oops so on I mean just really talk
about my PhD work its work on unifying
some working abstract reputation and
decision procedures which are most
notably done with video de Silva but in
the meantime a lot of our people have
been involved of course my supervisor
Daniel crooning about to greet you
recently who helped us out with mastered
implementations and Michelle couch niku
also helped us with implementations and
so when I first came to the UK to start
my degree is that my PhD I ran in a
couple of communication troubles and
which the bad thing gives communication
is one is a large part of research and
the problem is that when bridge person
says something what they say and what
they mean is quite different or what
they saying what others understand is
quite different so a British person
might say oh by the way which which
allows them to introduce the main
purpose of the discussion but for
anybody else it means all incidentally
it's not very important but i wanted to
point out so when I drifted off into
abstract interpretation at a bit of a
more practical background so I can come
from a sad sad specific background when
I drifted off slowly into abstract
interpretation I Stanley found myself in
a similar situation when I went to
scientific conferences because I was
very excited I would say oh isn't this
an instance of abstract interpretation
and I think this is something a lot of
people don't like to hear because what
they hear is all this is a trivial
consequence of upstream patient baby we
knew this since 1979 and what I actually
meant was oh I think there's a very
simple top-down characterization which
might be very useful if we use the
language of algebra fixed point since
obstruction because it allows us to
relate it to lots of other work and so
on or another thing I would say well
technique X computes an abstract fixed
point and what I meant is that
rich body results that that we can apply
to this if we viewed from this level of
abstraction but what others might
understand is that I don't care about
the details so I got a lot of glazed
looks when I mentioned the word abstract
interpretation before I learned that
there was a bit of a communication or a
little involved here so in this spirit
everything is absolute invitation maybe
not everything but a lot of things
abstract interpretation but SATs are
there from the abstract interpretation
so we are we have paper on this upcoming
in SES and this talk is going to be
about the more practical consequences of
this view so it's not just a nice
theoretical conceptual view but it's
also nice view because it allows us to
generalize existing techniques okay what
is the SAT solver forever in this room
knows this but it's all for the
propositional satisfiability problem so
we have a propositional formula and just
in case somebody's not familiar with all
the jargon propositional formula and CNF
is just the conjunction of clauses
clauses disjunction of literals and the
literal is just a proposition or its
negation and not a question we are
asking is there a propositional truth
assignment which makes the formula true
so modern solve is a based on so-called
conflict driven clause learning
framework short cdcl at least the most
efficient solar today cdcl is the basis
of modern smt solace or SMT solvers are
solvers for some restricted first-order
logic with regards to don background
theories and they're also very critical
components as everybody will know here
in program verifications everything is
based on interpolation is somehow
usually related back to the CCR
algorithm why is the cgt algorithms
interesting because it's extremely fast
so here's a graph from a survey paper
which plots the decrease in runtime from
two thousand to two thousand seven so
this is over the same instances over the
same machine it's just kind of
algorithmic advances that we see here
are advances and data structures and I
think this went on before that for a
while and it's still going on from what
I know so why is it so fast a couple of
reasons um one very important reason is
that the engine engineering is very
efficient so a lot of people have spent
a lot of time and finding efficient ways
of encoding data structure
making data structures just big enough
so they fit in the cache and another
reason is that the algorithm behind it
is is very ingenious so a CCL algorithm
is a refinement of the original deep
Adele algorithm which was also a
satisfiability algorithm DPL goes based
on on something like a split so it's
it's it's a bit like a depth-first
search so we're searching for
assignments we don't find one we
backtrack we try something else ccls is
significantly different from that
although it's a bit hard to see this
when you kind of approach it from this
historical perspective because many
people view it just as a refinement of
ppl but it's a very different algorithm
because what it does is it probes the
search space so it does this depth first
stepping down the search space but
instead of doing a backtrack and trying
something else it actually learns
information about the search space if
that fails so it's an alternation of
model search and then a a compaction of
the search space with with the learned
information um one of the open questions
are one of the questions that are the
focus of satisfiability research is
whether we can lift the success of CD
sell to other domains and the most
popular research area that concerns
itself with that is smt and the most
popular architecture for lifting a CD
set algorithm is the DPL entity
framework so what is smt it's souling
satisfiability of typically a quantifier
free but not always quantifier free
first or the formula or with respect
some background theory so we use
interpretations for plus Andy qualities
and then we ask the question whether
there is an assignment to the variable
the first order assignment to this
variables which makes the formula true
and the most popular framework for for
approaching smt is the DPL LT framework
or various instances or variations of it
which basically says well in order to
solve a problem such as that one what we
do is we first consider the
propositional structure so we just say
well this is P this is q this is RSS and
we run a SAT solver over it and then the
setup is going to give us candidate
assignments for the propositions and
using those are will create a theory
solver whether that's actually
consistent assignment so this is kind of
a very naive version of this framework
and what's nice about is two things
really it's a way of implementing SMT
solvers
efficiently so it gives you actually a
nice algorithmic framework with a clean
interface and then you can build solvers
on the other end is also a kind of a way
of reasoning it gives you a way of
reasoning about solvers that follow this
structure so it is kind of like a
mathematical recipe as well one way to
view what smt does really is is as a
kind of partitioning so you can view GPL
to add ability to partition the space of
potential models according to the
propositions that occur in the formula
so for example if you have a formula
here where we say x is e 20 or two or
four up to some 2 K where K is a
constant and why is it a 02 for up to 2
k then tip lol will in some sense
explore this kind of partitioning or
they say well what is X and what is y
and X can either be 0 to 4 or like left
out a couple here and why can be 0 to 4
and then in each of those little
partitions your theories although we'll
be able to give you a precise answer
whether there is actually a solution to
the formula or not in this case that
won't be a solution because I we also
require that so x and y are always even
numbers and we always require that X
plus y is an odd number so this is not
going to work out so now if we use a
very naive version of the tip by loyalty
framework we're going to run into
enumeration behavior now because an smt
so basically check a tipple LT based smt
so basically check that whether x equals
0 and y equals 0 is consistent and it's
not consistent so you'll start and try x
equals 0 y equals 2 and so on so you go
through all the possibilities unless you
can learn something that tells you that
X will always be an even number and do I
will always be an even number and if you
add the two you'll always get a an even
number again which is inconsistent with
this one so now instead of partitioning
the formula according to truth
assignments two propositions something
we could do is we could use a different
kind of partitioning so for example we
could say well what if we just track for
each variable whether it's even know odd
so now we have X's even odds why is even
odds and in fact just looking at formula
statically we could determine that x and
y must always be even because they
always assign even values and we can
immediately deduce that this formula is
unsatisfiable
alright so this enumeration behavior is
a well known issue in smt and one of the
ways that people are trying to deal with
this is our so-called natural domain SNT
procedures so the idea here is instead
of armed instead of using a
propositional algorithm in conjunction
with some theories over we're trying to
lift the steps off cdcl directly to to
Theory domain at the same time there's a
push in smt to kind of integrate program
analysis more strongly so for example by
having smt procedures that somehow
operate over programs or also just by
adding fixed points to smt theories so
graphically to make the distinction
clear between what deep llt does and
what the goal of natural domain smt is
considered this here so deep a lady
really has this propositional solver and
it's connected to a three years old when
inside the propositional so we have some
model search and you have some conflict
analysis now the idea of natural domain
smt is to do away with the separation
and to translate those internal steps
directly to the theory domain so we have
a theory model search theory conflict
analysis we have Theory learning
everything's happening in the theory and
this is quite an active research topic
right now so I've left out a lot of
stuff here so it's just a small listing
of natural domain S&amp;amp;T procedures and now
there's also big push of getting fixed
points into SMT solvers at the upcoming
smt workshop there's going to be two
papers about this their paper cup about
reach ability modular theories so
there's a lot of work on there so that's
there's some confluence with computing
fixed points and of course I when you're
computing fixed point that's good good
time to talk about abstract
interpretation I'm going to show us just
by example for four intervals what we're
doing here is we track the possible
ranges of a four variable and I'm going
to show to analysis that we could do on
the basis of intervals one analysis
would be to say we do in all approximate
analysis with a strongest post condition
so this is very simple we start we know
nothing in the beginning now we say that
a is five after the if statement we know
that B is between minus three and three
because you don't know which one we went
down and then when we add this number
here we know today is between two and
eight which allows us to prove the
assertion correct
another thing that we could do is under
proximate under proximate analysis with
Vegas precondition so now I now
understand that in a in a standard
abstract depredation keluar connection
setting intervals are not proper over
approximation but we can do I saw
another proper under approximation
apologies but what we can do is we can
in order to get a proper an
approximation we can just consider sets
of intervals so here for example we step
back we say assertion holds if a is
smaller zero or if stay is strictly
greater zero now is that back this would
be very hard to represent it precisely
as I said the weakest precondition of
this statement so instead we we make
some guess we just pick one possibility
and here we say well it works if a is
greater than for grading to the four and
peace between minus three and three
luckily be is between minus 3 and 3 is
actually established by this block and
then we just have to guarantee that a is
very equal to 4 and then we find that in
fact every execution is safe so it's
another way to prove safety of course
this is sound but incomplete so here we
have some imprecision that was
introduced and this could lead to
incompleteness and here we have to make
a guess which could lead to make it
efficient which would could lead to
incompleteness formally we have a
concrete lattice which is just the set
of states in this case our formula is
under the set inclusion and normal
intersection and joints we have an
abstract lattice and then the canonical
setting with a Galu our connection
between the two so we have an
abstraction function which tells us if X
is 3 19 the best obstructions X is
between one and nine and if you have X
goes to 46 then we can concretize this
and say well X is e or for its five or
six more of what we have we have
concrete transformers so this is just a
standard standard transformers like
strongest postconditions weakest
preconditions and then we have sound
abstract transformers which are
pointwise larger which point 0 is over
approximate the concrete operators and
so of course are the main result of
abstract invitation is that we can do
fixed point transfers so when we have a
fixed point like here which would give
us the semantics of the program set of
states that can be reached by the
program instead of computing it in the
car
Crete we can move to the abstract and
compute the fixed point there and now if
we move back to concrete while
concretization we actually going to
soundly over approximate the concrete
fixed point but sometimes we might get
bored with a fixed point computation
because it might take too long or it
might not converge so here's an example
here we have X is equal to zero and now
here we loop that counts X up in each
iteration so now we collect the states
that are reachable the loop entry point
we have first 0 0 then we've 0 1 0 2 and
so on and we will have to continue to
see until we reach a thousand or in
other cases this might not at all
terminate so in order to achieve
convergence there's two tricks that we
can apply if you have a least fixed
point computation we can jump over the
least fixed point to enforce convergence
faster for example here the loop entry
point I might just say well let's try to
loop God and see what happens then so I
met say well x-small thousands as an
interval if you're computing the
greatest fixed point and we're gonna
stay soundly above it then we can use a
narrowing operator so normally will go
down here but we don't want to go all
this way so instead we kind of branch
off and and choose something that's less
precise and stay soundly about the fixed
point all right so now how does this
apply to logic let's say we want to we
want to take a logical formula and want
to play abstract invitation to get some
information about it a very simple way
to think about this is to translate it
back into the world programs which means
we can use all our program analysis
intuitions so for example if you want to
check the satisfiability of a formula
here we could instead just prove the
following program safe so we replaced
all the propositional variables by
bhuiyan see variables and now we say if
p is the case is first Clause then if
not p or q is the case and so on and if
we can go through all those clauses in
the program then we say assert false so
now the program is the formula
satisfiable exactly if this program is
unsafe and in fact we could see what
happens if we run a normal abstract
interpreter on this here for example we
have a boolean constant analysis so just
constant analysis for boolean variables
initially we know nothing here we would
learn that P must be true
all p must be true and we enter this
thing here we know that you must be true
as well and finally if we are here with
P must be true you must be true and you
enter here then we reach bottom because
it's impossible that those to us through
at the same time which means that the
formula is unsatisfiable and the program
is safe so the more formal picture that
when we have logic what we have is we
have a syntactic set so we set a formula
and then we have a set of semantic
elements which are structures first of
all the structures and we have a
semantic entertainment relation between
the two which tells us which structures
satisfy which formula the concrete
domain is now the power set of
structures and which gives us the
lattice again of course and as an
example if you have a propositional
logic then we we have two formulas just
a set of clauses if you choose a set
based formulation and the structures the
semantic elements are just the functions
that map propositions to true or false
and of course Samantha containment we
have structure entails a formula if
every clause has a literal which is
being made true by the assignment
alright so for abstract invitation we
also need transformers and over logical
formulas there's two transformers are
that I want to present that that makes
sense so one of them is essentially it's
the postcondition strongest post
condition of an assumed statement it
says give me a set s set of structures s
and return all the structures in S which
satisfy the formula so we filled out the
things that are contradicting the
formula the other one you could call
conflict collection transformer so we
start with the set s and we actually set
all the things that contradict the
formula fire which is essentially the
same thing as the weakest precondition
of an assume statement
all right now we can use these and
specifically abstractions of these to
check satisfiability of formula so if
you have an approximation of the
abstract model transformer so on sorry a
no approximation of the model
transformer what we can do we can
compute the greatest fixed point over
this thing so basically come from the
outside we say give me increasingly
presides pounds on the set of models and
if we reach bottom so if you say that
actually are there are no models then we
can deduce unsatisfied bility dually and
this is a really the same thing with
with your head turned upside down we can
compute a least fix point of the
conflict collection transformers we
start with a set of non conflicts and
then we grow the set and we find if you
find that everything is conflicting if
everything is contradicting the formula
then we also know that the formulas
answers viable alright so now this is
where I want to bring the two together
as we talked about natural domain sm
team we've talked about interpreting
logic using abstract invitation and the
argument i want to make now is that you
can actually view cdcl as a natural
domain smt procedure whole proposition
logic which uses abstraction so natural
domain smt procedure for propositional
logic low sounds odd but the ideas that
if it clearly separate out various
issues then we we have easy
generalization to reach your structures
and this specifically involves what
model search means with respect to an
abstraction and what conflict analysis
means with respect and abstraction so
first talk about model search model
search and SAT solver is an alternation
of two steps we have deduction which in
first variable assignments so first that
X must be true well I must be false and
then we have decisions which guesses
variable assignments once we can't infer
any new information now the first point
to notice that SAT solvers actually
operate over abstract domains so here's
a bit of mini-subs implementation
minister to I believe and the core data
structure of Manisa to is a partial
assignment a partial assignment Maps
partial assignment naps variables here
presented by indices to assignments
which are true false undefined so three
value logic and there's a special flag
okay which is false if this always
contain conflict
so essentially what you have here is
exactly the structure that we saw
earlier the boolean constant domain
because for each variable we can for
example here we know that p is true and
we don't know the value of Q which will
correspond to an assignment from P to L
true and from q2 L under and so on so we
can have that both are true we can have
a conflict here we can have an empty
partial assignment up here so in fact
SATs always operate exactly over the
abstract a name that you would use in
boolean constant propagation furthermore
deduction and subsolar compute the
greatest fixed point so subtle uses the
unit rule for reduction or the unit rule
you can view as a mapping from a partial
assignment to another partial assignment
with respect to Clause so here for
example you have a clause p or q now if
we know that piece falls then we can
deduce that q must be true that's
basically the unit rule says because
each clause must evaluate to true so the
only chance of this happening if piece
falls is that Q is true and in fact if
you think about in terms of the
abstraction then the unit rule over
approximates the model transformer in
fact it is the best a model transformer
that you can have over the partial
assignments domain so unit rule is the
best abstract transformer of the model
transformer over partial assignments and
now of course if you just repeatedly
applied at the unit rule to deduce new
information like here what you're
actually computing is greatest fixed
points because we're getting more and
more precise until we reach a point we
can deduce nothing more which is our
fixed point B exactly someone
characterization of putting constraint
propagation is just that the greatest
fixed point of the unit rule in this
case the unit who lifted the formula
also decision making is corresponds to a
well-known concept in abstract
invitation all the one that is applied
in a somewhat unusual way a decision
just says one's no more no more new
facts can be deduced Wii U rishta leap
ich a truth value for propositional
variable so here for example consider
this formula here we know nothing at
first and we could introduce that p is
true and then we can't reduce anything
else so now a decision would jump under
this greatest fixed point so we suddenly
decide that Q is false and from there we
reach
conflict so if you recall widening were
used to jump over a least fixed point
and what a decision does is it jumps
under greatest fixed point I mean what
is unusual about this application of
dual widening if you want is that it's
actually unsound so you lose soundness
usually you only apply widening in a
sound way but there's an application of
widening that leads to an unsound result
all right now if we consider conflict
analysis we can split it up although an
implementation this is often not
reflected we can split it up into two
different procedures one of them is
abduction so it's why a procedure where
you can find a set of possible conflicts
and they might sorry set of possible
generalizations of a conflict so in some
sense explanations of conflicts and
another one where you choose one uris
t'kul e are the reason is that in a Sat
solver when you reach a conflict there's
often multiple incomparable reasons that
you could extract from run to tell you
why you reach this conflict alright so
one example if this is implication graph
cutting which is just one of a couple of
techniques may be the most important
technique that's used in such solving in
order to conflict analysis so here we
have a data structure called an
implication graph which SAT saw was used
to record the doctrines that were made
so for example here we had the truth of
P which was given to us by a decision
and frumpy Winfred using this clause
that few must be true so we draw an
arrow from P true to Q true and we also
draw an arrow from p true to q2 are
false and from those two together using
this clause we could deduce that s must
be false until we finally reach the
conflict so now a Sat solver applies
technique called conflict graph cutting
which basically tells us that if we
disconnect the decision notes from the
conflict node by a cut then we get a
sufficient reason for the conflict to
occur for example we know that whenever
we assign s2 Falls will reach the
conflict or we can also say that
whenever we apply Pete assign Peter true
we read your conflict and here's exactly
what I mean so abduction would say well
you can cut here here here and get
different reasons and then there's your
stick choice involved which tells you
well choose this reason or that one
according to some to some criterion so
basically we could formalize this as a
funk
application where we start out with with
the singleton conflict pie which was
kind of this starting out conflict or at
the full set of all assignments being
made here and then with the cut we
managed to generalize this statement pie
to either P goes to true q goes to trial
goes to false or s goes to false and
then this choice involved which tells
you well picked this over this well
essentially an abstract imputation terms
what's happening here is that you're
suddenly operating over the disjunctive
completion or with our partial
assignments domain because you have an
option you have various partial
assignments that you're working with so
you're essentially working with with a
kind of power set domain so it's are
read the extension with disjunction so
we can say this partial assignment or
this partial assignment and just to tell
you what this visually means on so
originally when we are going to abstract
the set in the partial assignments
domain we said well give me the the
smallest enclosing partial assignment
that doesn't miss out any objects in s
and now if we under proc is mating it
were using partial assignments or using
these junctions of partial assignments
what we say is give me all the partial
assignments that fit inside my set and
now the cats transformer is just the
transformer over this domain here and
essentially it's doing abductive
reasoning because it tells us given that
we have reached a certain result was the
reason that we reach this result and
it's in a such abduction transformers
are under approximations of the conflict
transformer so the conflict transformer
applied to this the best abstract
conflict transformer would basically a
doll partial assignments would add all
possible reasons that lead to a conflict
and is very expensive to compute so
instead you choose an under proxxon a
shin for example the one you obtain from
your from your conflict graph cutting
also the heuristic generalization with
choice is dual narrowing so for example
you might have your original conflict
here and then a graph cut told you that
you can either go here or here and then
you can further do something like Klaus
minimization which is a technique to
further reduce the size of a conflict to
either see that our is false is
sufficient to get conflict or SS true is
sufficient to get a conflict so one
thing you could do in your SAT solver is
now to collect all of them and to
basically
learn from each of those reasons but
that's over typically don't do this so
we would start with the original
conflict then we go to something more
general which says you can go here or
here and then we're getting more general
by saying well actually you can split
this up and each of them independently
sufficient to produce the conflict and
just come to the most precise
information that you can get out of this
set of possible generalizations but
instead well as arts or does it just
chooses one reason so we'll sell out the
original reason and then it gets
something that's smaller than this
object here and they get something it's
small in this object here and will
converge early so will converge below a
list fixed point so again it really
called it narrowings used to converge
above greatest fixed point what we have
here is a dual narrowing operator
because we're computing a least fix
point where we collecting conflicts but
we're not being as general as we can be
for reasons of efficiency so what this
means overall is that we can basically
find an abstract the recipe for deriving
natural domain S&amp;amp;T solas from abstract
domains by just making the abstraction
leaving the abstraction abstract so
leaving the abstraction as a parameter
and now search just compute the greatest
fixed point which do a widening and
conflict analysis computes a least fixed
point with dual narrowing and what we
need for this is some ol approximating
demain on this side and some under
proximate in the main on this side
although if you want to do Clause
learning those have to have some precise
relationship with each other and
essentially what we what we exchanged in
here is when we fail at modern stretch
which we can because this dis widening
is unsound then we transfer this over to
the conflict analysis domain and once
the conflict analysis domain converges
we can learn a transformers we can learn
a refinement of the model transformer
here in visual terms what this means is
we're starting out with kind of this
green area we'll see we have some cost
over approximation of our set of models
in this case everything and now we're
refining this down iterative Lee once
you can't we find it down anymore make a
decision and then apply reduction until
we reach a conflict is the point where
we have realized in our obstruction that
those two sets are actually use drunk to
there's no model of fire in this set
here now what you're doing is abduction
which tells you all you could go here
you could go here or you could go here
and essentially you could follow up on
each of these but for efficiency we just
choose one now we generalize this
further and make sure we stay outside
this region because sound and
approximation of the set of conflicts
and we choose one here so one thing
that's missing now from this account is
we have model search and we have
abstraction and we have conflict
analysis with abstraction one thing
that's missing from this account is how
we're actually going to do learning how
is this going to be performed and it's
it's a very general questions and there
would be a lot of answers to this I'm
going to focus on the one that the SAT
solver chooses so first before we go
there actually you can consider a very
simple form of learning so it's kind of
the simplest form that you can think of
which will be taboo learning so which
means that when we found that C is a
conflict and we find that we actually
reentered the conflicting region we
immediately go to bottom so basically
here we find that green areas inside the
blue area so we immediately deduce the
bottom is the case which you can sum up
as a rule like this here what's nice
about this is that you don't need any
prerequisite so you can basically do it
over any lattices because all you need
is this check which tells us are we
inside the conflict in region region or
not but a propositional SAT so does
something smarter in some sense because
what it will do is it will check whether
something is nearly conflicting and if
so it will drive it away from the
conflicting area of the conflicting
region of search space so if this is
kind of a your previous conflict and
this represented partial assignment you
start so will notice that you nearly
except for one bound inside this area
and then it will immediately deduce that
you must stay outside there so you don't
get lost searching in this space and
ending up in the same conflict again so
one way to to write this down is now
given that our conflict was something
like P goes to true go q goes to true
and r goes to false we can actually
decompose this into its singleton
assignments so we can compose every
partial assignment into meets of
singleton assignments and what's nice
about these singleton assignments is
that they have compliments so partial
assignments don't have compliments in
general so the compliment of this set is
not expressible as a partial assignment
but each of those individual objects
here has precise complements
and now we can do is that this
decomposition allows us to express what
it means to be nearly conflicting
because you can check whether we have
three elements here and we can check
where our current element is more
precise than each of these elements but
one so here for example we say well if
we know that Q is true in pie and rs
falls in pie then we can deduce that p2
is not the case and so the compliment
property allows us to drive the search
away from this conflicting area so
essentially we acquire this reef
complementation property that every
element in our lattice has to be
decomposable into needs of small
elements which then have compliments on
this is a fairly common property in
lattices so to give it two examples if
you look at intervals octagons they have
this property because an interval
doesn't have a precise compliment and
octagon doesn't have a precise
compliment as an octagon or not everyone
every octagon but we can decompose
intervals and octagons into half spaces
which are again intervals in octagons
and each of those half spaces has a
precise compliment so this means that we
can do Clause learning over intervals or
off the guns or to give you a slightly
different example let's say we're doing
some kind of a trace trace abstraction
for program analysis and now we have an
abstraction which tells us well we have
three branches in our program and for
each one I keep track of where i took
the left branch the right branch or
whether i don't know which branch i took
ok so now if I know that this path is
infeasible where I go right left right
then I can decompose this again into
three decisions so I'm first made the
decision to go right which has a precise
compliment then decision to go here
which has precise complement and so on
here so final example is equality dis
equality graphs so if you have a graph
structure sort of the nodes here
represent variables and we draw a
straight line between them if you have
an equality edge which tells us the two
variables are equal or dotted line if
there are unequal in which case you have
this equality edge now every graph can
be so the graph itself can't be
complemented but every graph can be
decomposed into its component edges in
each of those edges can be precisely
complemented which again means we can do
Clause learning over a quality graphs
so what this gives us is a generalized
unit rule so for example for intervals
we know if you have a conflict here and
we have an element green element here
which is nearly conflicting which means
that according to the decomposition were
inside all bounds but one then we negate
the last bound and flip it over or we
can do this here so if we know this is a
conflict if you go left left left and
then be an element that nearly always
goes left then we can know that actually
we can immediately reduce we have to go
right here which pushes the search away
from this conflicting area all right so
finally what I want to talk about some
instances and applications of this and
this is something we worked out without
about to grid Joe we have an
implementation which are basically takes
the cdcl propositional sub framework and
what it does is it tries to separate out
the obstruction from the actual
algorithm so you have a clear interface
which you can use to instantiate so to
put in your over proximity domain you
need some decision heuristic which makes
sense in the abstraction and at the
other side what you need is an under
proxima ting domain with clear interface
and then some heuristic which tells you
which generalizations are to be
preferred and using this you can
basically instantiate all these
components and then we've got a decision
procedure which uses natural domain
decision procedure which internally uses
your abstract domain as a data structure
so what you can do is you can actually
generalize the the conflict graph
algorithm to work over domains which
have those complemental decompositions
as an example if you consider this here
here we have formula x equals y and x
plus y is ready put to 10 so now
assuming that we make a decision where
we say x is smaller equal in zero what
you can do is we can deduce that y is
small equal than 0 and from this we can
deduce that this last thing doesn't hold
here so reach a conflict because X plus
y is then definitely also small equal to
0 so it's not so will now tell you that
you can just cut the graph to get a
sufficient condition for conflict but
essentially what we're knowing we know
that we
are computing something like the weakest
precondition here so where you can do
what it starts work on to because we
have a richer domain structure is we can
look at each element and we can try to
generalize that for example we can say
well if Y is up to seven this thing
still works so we can still the juicer
conflict and now you could say well okay
then you can move X up to 2 because 7
plus 2 is 9 so this will still be
contradicting and you see that this in
addition to a subtle which has to choose
between various cuts through conflict
graph the choice here goes into the
generalization itself because we could
have chosen we could have distributed
the slack between x and y in any way
that we wanted and are so we could have
for example chosen 55 or switched around
had 72 and good stuff being able to
deduce the conflict and now once we've
done this we can choose a captain for
example learn that this will always be
case when X is small equal than two all
right so the general pattern here is
that you can generalize each node so you
can have something like a generalization
of the first year p algorithm let's use
the Mossad silver to do this and then
you can cut the graph and you get a
generalized reason and this works
actually very well so we have the sense
denshi ation with floating points and we
compared to as III I have to point out
that pitch vector encoding is generated
by mouth shut because said three doesn't
support flowing points at least not in
the release version so first translated
to big collectors and then fed it to set
V and this performs very fairly well as
you can see because basically the big
vector encoding loses all ability to
reason to reason about it original
numerically which is a bad thing if
you're floating point because instead
you have to reason about those massive
bit blastings of floating-point circus
which is very inefficient also one thing
I have to point out here is that it
would be very easy to find examples
which wouldn't work for a technique at
all because we're still based on
intervals so for example if you try to
do something like an equivalence check
this is fundamentally beyond the range
of what intervals can do because
intervals can never express that X or Y
have an equal value so these are
basically problems derived from bound
checking those problems where intervals
make sense but intervals alone are not
sufficient to do it
another thing that's nice that falls out
of this is that you can basically do a
abstract conflict with and Klaus
learning for programs and the trick here
is to say that we treat our program as a
kind of logical problem so we say we
have a set of traces that's now our
semantic set and we have a set of
structures which is just our set of
programs now we can say that we can
basically say that piece of pie
satisfies a program if the trace pie is
an erroneous trace that is generated by
a program now if we instantiate this
framework now I've been stance yet our
framework again we can ask what does
this mean now what does it mean to
compute the abstract model transforma
given this definition of satisfiability
given this definition of satisfaction
well the model transformer remember
throws out all the things that don't
agree with this where it just doesn't
hold so essentially what we're computing
here is the set of traces that is
generated by the program and is
erroneous and there's two ways to
characterize the set or you can just
yeah there's two is characterized I said
one of them is to start with the
ignition stays and to at least fixed
point analysis with the strongest post
condition and I wanted to start with the
arrow States and released fixed points
from there so basically what this means
is that we computer greatest fixed point
but it each step of the greatest fixed
point we compute the least fixed point
inside which gives us the behavior of
the program at the same time the
conflict transformers now the transform
it adds everything that doesn't agree
with this way this doesn't hold to your
set so basically what we're collecting
is all the program traces that are safe
or the program traces that are not in
fact generated by the program and again
you can get this from now at least fixed
point or the greatest fixed point of
weakest precondition and universal post
condition and what's exchanged between
the two is now here basically a partial
safety proof so we're trying here to
find erroneous traces if this fails we
get a partial safety proof which is
correspondence to a conflict and start
solving and what we get back is a
refined transformer so we get some some
basic refinement of for example
strongest post condition which keeps in
mind satisfiability information
alright so i can show an example of this
here's the program that you can't solve
purely with interval propagation what it
does is it first checks the value of
eight and according to the value of a
science be to a certain value in this
case between minus 2 and 2 but it can
never be zero and then you want to prove
the p is not equal to 0 so you could do
a standard interval propagation you'll
just end up with bees mind between minus
2 and 2 and you can't prove this correct
so now we can basically lift all the
steps of the subsoil lifter the conflict
graph construction cutting to operate
directly with the obstruction so now we
start out the decision level 0 which
basically means that we don't make any
assumptions so we just did use whatever
whatever it be know for sure without
making any further assumptions and also
remember that we build our graph over
those complemental elements so we
decompose all the things that we deduce
into their components and now over this
domain here are the meat irreducible so
that it's kind of like minimal
complement able elements essentially
elements that tell you that at a certain
location for certain variable there's a
certain bound so here for example at c2
which is here we know that a is small
equal to minus 1 and we also know that
it's great a equal to minus 1 because
here we have a check which makes sure
that a is exactly minus 1 and so on so
now this base is the result of a
standard abstract interpretation run I
just split split up into its component
parts and at the end we actually find
that B is between minus 2 and 2 at this
edge so we know that B is equal 0 but we
can't prove that it's unreachable so now
we do what a subtle to make a decision
and the same way in the SAT so it
doesn't really matter you don't have to
make a great decision because you can
recover from it later in this case we
can make a stupid decision the stupid
decision is that we say well what if a
is smaller equal to minus 42 which is
just something that is randomly chosen
now what is it happen is this would
propagate down wards and we find that at
c1 a is still small equal to minus 42 so
we agree with the bounds we go through
here but a small equal to minus 42
actually contradicts all those bounds
there so we can deduce that c2 c3 and c4
are all unreachable now from this
information
that those unreachable we have this here
we can deduce that at this edge we only
have this assignment that is incoming B
equals two so we know that B is greater
equal to two and we can ascertain that
the program is safe because you find out
that you can't actually reach the arrow
location so now it's the same thing that
we talked about earlier now we can do an
approximation and we essentially using
the weakest precondition to do this to
further generalize the graph so from our
locations conflict in order to guarantee
this result it's actually sufficient to
guarantee here that p is greater equal
to one at this point when you come in
here furthermore we don't have to
guarantee anything at c1 as long as
those still unreachable we'll know for
sure that add into this thing will hold
and in order to guarantee this result we
can just say that a small equal to minus
two and it's exactly what I meant
earlier so it like in the SAT so it
doesn't really matter that we made a bad
decision because our generalization our
learning will recover from that and now
again so there's various options by crew
cut so one one possibility would be I
could cut here and say well you learn
that in the future a must always be a
strictly greater than minus 2 but we
choose a cut here so we say that
whenever way 10 20 we only consider
things where be small equal to 0 because
we know that for everything else we've
already proven safety and basically this
will allow us to prove safety in one
step now because in the next step will
be able to exclude p is minus 1 and b is
minus 2 sorry to exclude B's 1 and b is
2 and what will remain is BTS men minus
1 and minus 2 and we'll be able to prove
safety because we know that at this
location it's between a minus 1 and
minus 2 and we can exclude the zero case
so what all is that you have basically
something like an intelligent
decomposition of the program so one
naive approach would have been to go
through all parts separately but with
this analysis we actually found that you
only need to do 2k splits a bit naman
numerical is the following so here's an
example of a program which tries to
compute the sine function so the ideal
sine function is plotted in red here
and with program output plotted in blue
so it's some approximation and now we
want to check whether the output of the
program isn't a certain range here if
it's in a certain input range so for
example if we say it so normally if you
do interval propagation you basically
find that the result is between I think
minus 2.3 and 2.3 that's the precision
of intervals for the program but now if
we enforce that we want to check that
the results between minus 2 and 2 cdcl
will automatically introduce those
petitions for you so here we have one
large partition because we are far away
from this bound here so we can we can
basically admit more errors and towards
the edges where you need to be
increasingly precise you see a
refinement and this by why does this
data taken from our answer is not
something I made up but this is plotted
from a run of tool now if we decrease
the if we decrease the bounce that we
want to check so if we make the problem
harder you see how those partitions
increase and move towards the edges and
we can continue this game until yeah we
get this thing here so this is still
there's a lot of iterations involved but
it's still much faster and fully correct
with bit blasting because again it's a
floating point problem which is very
hard to get lost until we actually reach
this case here where we have a count
example and the reason we can find
counter examples is that this is a loop
free program so you can basically refine
all the intervals down until they are
Singleton's and then we have a single
trace that goes through the program so
here you see again it's basically
something like an intelligent
decomposition of the analysis so now we
have some experiments we compare this
with a standard interval analysis and we
compared with bounded model checking and
our technique is in blue here so you see
that standard interval analysis is a bit
faster but we have some false alarms
also we can't distinguish with basically
don't get a counterexample with the
interval analysis of course if the
program is actually buggy he abounded
mulch ecking our times out quite a lot
of times with time about half an hour
and we get a result in in about at most
10 seconds for each of the programs
again this was taken from bounce
checking it could very easily kill it if
you gave it an equivalence
Eustace doesn't work within to us sorry
i don't know i think it's just you know
this is still a couple of seconds so
it's it's not our implementation of
interval so there's different
implementations of so we use the
difference we used us pray for this here
for the green line and for this line we
used our auto so I'm not sure I wouldn't
I wouldn't take it too seriously I don't
think it's it's a significant difference
sorry x axis is the number of the
benchmark so basically what we have here
is various benchmarks and you can see
for each benchmarks how to compare so
actually the line doesn't we in anything
here yeah yeah no there there various
bound checking some of the Mahan made
some of them are from kind of static
analysis examples so it's all bounds
checking basically it's always you
compute some function you place bound an
input range and then you have bound on
your food so it's very sorry is one of
those times one I don't know seven no no
no actually i mean i can tell you can
remember i can't find it on a graph
again but i remember that the sign one
was one which i think if you if you run
down with malu checking it depends on
the bound how long it takes so we're
very fast on this example because it is
relatively simple we can always solve it
and let less than 23 seconds we found a
model checking it depends on how close
to you you get to the bound so if you're
further away it takes like half a minute
to a minute and you decrease the bound
it gets close to the timeout so it takes
quite a long time although it's just a i
think a three or four line c program
that generates it but its massive
explosion the big blasting
alright so the final thing I want to
talk about is a conditions for
completeness and what I'm going to say
here is just a something very simple
because what I'm going to do is I'm just
going to lift the completeness
requirements that you have in the
propositional case to the abstraction of
course or you could do much more what
this basically does it doesn't place any
assumption on the strength of your
learning it just says well if you have
your procedure when can you be complete
without without placing an assumption
the generalization step in the learning
and essentially what they have in a
propositional case is you have a finite
lattice you have the complement will
meet irreducible switch are up here so
each of those has a precise complement
this allows you to construct clauses
which gives you back jumping algorithm
basically and then finally you have
precise deduction on atoms what this
means is that atoms are just the thing
in your lap is that sit just above
bottom and what we need here is that
when we get those to those minimal
elements whenever whenever actually the
concrete model transform all tell us
there are no models inside a then we
need to be able to deduce this in the
abstract which is actually fairly
similar to the SMT debility style
requirement that when you have a
conjunction then you need to be able to
check you kind of an incomplete so what
it doesn't tell you anything about
conjunctions this will work alright so
the springs talk to an end I didn't
realize that I had one and a half hours
so I prepared for about 50 minutes all
right so this is kind of a slight that I
assume that's some smt people will be
here so but it seems most of them are
gone basically uh the main thing is that
I mean you have this cultural difference
between between people who do abstract
implication people do smt and now you
see that yep in S&amp;amp;T we're trying to add
fixed points to big SMT solvers or we
have program analyzes that kind of look
like is that so there is a natural
convergence happening there and I think
if you take the abstract interpretations
perspective seriously you get a couple
of things out of it one of them is that
you can actually take algorithms that
you haven't satisfiable d decision
procedures and you can lift them to
program analysis procedures so that's
kind of one way of technology transfer
but I also think that the abstraction
perspective is fairly important or could
be fairly important for for the SNT
community
especially when you build fixed point of
course and then it's it's I think
essential that you start thinking about
abstract invitation but also just when
you think about natural domain s and T
procedures because you actually want the
solver that grabs inside your theory
domain with both hands and how are you
going to manipulate that and I think
abstractions good way to actually
manipulate the theory domain to talk
about semantics and talk about search in
the space of models and basically we
have a community that spent the last 30
years thinking about how we can think
about domains and reason inside domains
so I think there's some possible
technology transfer that goes both ways
here all right so thanks very much for
your attention and we have you down send
questions which benchmarks now I have to
set aside one from sorry so the program
once whereas the design isn't Iliad
format sorry de prom the we have an smt
so if benchmarks were adds a separate
implantation from the program analyzer
and the 120 are an smt lib format and
we're actually going to make them
available soon our C programs so we used
those implement and Daniel see proof of
framework so it's same thing as CMC so
you basically an abstract interpreted
runs on top of control flow graph which
is generated from C program yeah here
exactly
yeah no no you can't have Luke's as well
I mean the ones the ones here the thing
is we compared to bounded model checking
and the thing is this now so we we can
support loops in the same way than
abstract interpreter can now we don't
have anything fancy non-abstract
interpreter at hand these loops but if
we had we could apply the algorithm with
the same interface to work over that so
now intervals basically one thing that
you can't do in normal intervals you
can't look inside a loop so i can
restrict the initial condition of a loop
and then compute a fixed point from
there and then try and prove the program
safe but is that not sufficient I can't
look inside that the reason is that in
order to instantiate the South algorithm
you need to be able to basically cut the
set of traces in half so for example if
at the initial state I say x is small 0
X is Grady equal to 0 that this cuts the
traces in half that are going to go
through but if you go to a point that's
inside a loop and you make a decision
you say well X is smaller 5 well
sometimes it might be when you go
through loop and sometimes you might not
be so you don't actually partition the
set of traces using something like this
and this doesn't mean there's not a
limitation of kind of the framework it's
its basic limitation of the underlying
abstraction that you use if a new
abstraction for example you able to
express something like consider only
even numbers of iterations of loop
considerably odd numbers of iterations
who don't make a distinction then you
can instantiate this algorithm to make
decisions on that or loan closes over
that you could learn for example if you
have reduced products of intervals and
number of times through a loop you can
learn something like whenever you have
an even number of loop iterations then X
must be small or five hear something
like this yeah
which
yep
yeah yeah
yeah it's not said that he antenna
depends how many questions are basically
what I given here was a was a very close
completeness requirement which is really
just leaves it from propositional case
so if you have a finite lapis and
essentially what you can we can always
enumerate all the simpler Tommy cases
now the question of when I go and fall
into this enumeration behavior and when
are you going to be able to prove things
more efficiently as very interesting
question I don't have a good answer for
it but it's I mean the intuition is this
you can already see it when you run up
when you run an interval bounce checking
your bounce problem then depending on on
how close you're a check bound is to
your actual bound you see the number of
iterations shooting up so it starts
enumerated very close to the bottom to
get more precision now I can go to
completely other way and you can try and
prove an equivalence checking program
then it starts enumerated from the
beginning it does nothing but enumerate
because you're using intervals as a
basic format but I think it's very
interesting because this is something
that happens in S&amp;amp;T and SAT solvers as
well it'sit's if you have a certain
problems SATs always tend to enumerate
them because they don't have to kind of
write language to properly think about
them in some sense so I think the same
thing that's happening here
yeah yeah yeah exactly and I mean this
something is very useful it's also in
order to get this complementation that
you need it's it's may be useful to run
an analysis first because you might not
have this complementation property but
when you can make certain assumptions
you might be able to complement inside
those assumptions to something like this
so I think there's still value in doing
this pre-processing yep imagine going on
what if I agree I need a divorce
if you know the quantifier free condoms
just are commonly quite a comment
regarding the state of S&amp;amp;T procedures
but most of them tend to deal with 25
projects but not all now I mean in
because we're computing post operators
directly no obstruction we don't
actually need quantifiers each other I
mean you know don't explicitly need to
handle quantifiers because in logic the
quantifiers I needed to do some
character the program to compute
postconditions preconditions
ship on operators
aeromotive like this yeah so you mean
now to handle a quantification somehow
warrior doctors some kind of connotation
or download qualifies that the formula
that has to be a seesaw yeah no I mean
in general notice that there's nothing
holding back basically it's again the
strength of the abstraction your bill to
compute to approximate results over
example you have a statement that says
for all X and then array array of x is
equal to five and the abstraction can
actually read this then you're fine can
handle it there's nothing at all you got
computers but that's right there
complete you are always
without but without going to you mean
your computer set up you kind of make up
the receivers yeah you mean now that
your physical give the semantics of a
formula relative so the app structure
yeah sure I mean I mean this is so much
work that's also just published the SES
because you're not solving I mean your
soul satisfiability as a side product
somehow but really what you're doing is
making distinctions until joining
together against your learning can vary
insulin formula if you like so you're
learning you getting increasingly
precise approximation
so
yep
Ravi</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>