<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Random Functions for Dependence and Component Analysis | Coder Coacher - Coaching Coders</title><meta content="Random Functions for Dependence and Component Analysis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Random Functions for Dependence and Component Analysis</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/0H_aDyBtGrI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright so in this talk I'd like to
review two algorithms that we have
recently proposed based on this
framework of random functions or random
features so the mindset here is that you
have millions of points millions of
features and the processing capacity of
your smartphone and this is joint work
with Philip panic Bernard Shaw cough and
stupid stuff from the Max Planck
Institute for intelligent systems Alex a
smaller from CMU and zooming ghahremani
here in Cambridge ok so let me begin
with some a very classical problem that
we all faced at some point and is the
case of nonlinear binary classification
right so in this problem you're given a
bunch of blue points I blank a bunch of
red points and your professional career
will be summarized to find an algorithms
that will perfectly separate these two
clouds and as somewhat useful way to
proceed in this setup is to map your raw
features or explode them into a very
high dimensional space on which we hope
to find a linear separation boundary ok
so in this case we are creating a third
dimension that will go that will elevate
each of the points to an altitude
proportional of their distance to the
origin all right and doing this one way
of doing this is to use what we call a
kernel function which is a function that
will tell us how close together two
points are in some feature space that we
don't necessarily need to know right so
in this case this feature space work
pretty well for a problem we can
separate the two classes of points with
a with hyper plane and the induced
decision boundary in the original space
would be this the circle ok but the
problem today with millions or tens of
millions of points is that you have to
deal with these objects so the so-called
kernel trick can be the colonel trap
right you have to build this kernel
mattresses which will be for a million
points a million times a billion this is
an operation squirt in the number of
points you have and in many scenarios
you even have to invert these monsters
right so that's even cubic and all this
effort is to capture exactly the essence
of the kernel which is the square is
potential and then we
will this full rank matrix to perfectly
capture the feature space associated
with this girl and my message here is
that you can do much better by just
building a randomized an approximate but
explicit feature space that will capture
the essence of the colonel not exactly
but in a much nicer way and one smart
trick to do so is to use a classical
result from Freudian analysis called
Buckner's theorem this is pretty old
result but it was not until 2008 when a
lead rahimian Benjamin wreck decided to
use this in the context of kernels and
the theorem goes as follows bear with me
a little bit of math with any shift
invariant kernel which means that i can
express my kernel with only one argument
which is the distance between the points
i'm evaluating and by the way i think in
the talk from zooming and james i think
most of the currents are shifting
variants so you could use these tricks
to scale an automatic the statistician
to me in your support any kernel of that
form can be expressed by its Fourier
transform which turns out to be if the
colonel is positive definite up a
non-negative measure all right so
ignoring normalization constant we can
sample from that measure so let's press
the kernel as a Fourier transform and
then instead of computing this integral
which will take us to the exact
reproducing kernel hilbert space but all
this kinda matrix business let's express
it as a finite term some of deterrence
okay and now this is not equal it's
approximately equal with an error term
of 1 over square root of T just
approximating an integral with Monty car
the cool thing about this representation
is that I can separate the things that
depend on X from these things that
depend of Y can cut the net them in a
vector and then I will have a randomized
but explicit feature space and now I'm
going to step back and instead of using
dual algorithms i will use linear but
primarily teams on this set x
representation and this will allow me to
do for example nonlinear regression in
linear time complexity with respect to
the sample size all right so there's two
applications of this idea that we've
worked in the past two years and the
first of them is to develop a non linear
dependence of measures are not linear
measure of dependence and we call it a
randomized dependence coefficient and
the task at hand here is well you're
giving a sample that
can be the circular pattern plus noise
and you're interested in characterizing
if there exists a dependence between x
and y or not so in the case of using a
linear measure of dependence like
correlation you will get something close
to zero because there's no linear
direction in this sample that we explain
the variation in the data but we want to
to have something more general and we
will use these random features to do so
so the method works as follows the first
step here is kind of a pre-processing
step on which we will make all the
marginal distributions all the features
and X all the features in why look
uniformly distributed and this is called
the copula transformation of our data
this will make or depends the statistic
environment with respect to
monotonically transformations in your
data Miguel in the afternoon session
we'll talk more about this this is a
logarithmic time transformation so we
don't lose much time much time doing
that the second step will be the random
feature step so in in methods like kcca
your rate cecum or some other
state-of-the-art dependence measures you
will build a kernel matrix and then do
some spectral analysis on it instead we
are going to build this approximate but
explicit featuring space we're going to
do it for X we're going to do it for y
separately and now instead of the scalar
random variables we're going to have K
projections of XK projections of Y and
then or dependence a statistic will be
the linear combination of X and
simultaneously the linear combination of
Y such that we reveal as much
correlation as possible so in summary
we're given some raw data and we have
computed these transformations here such
that we push X through these
transformations we push white through
this one and then we get something that
gets the correlation rebuild and that's
our dependency statistic ok it turns out
that we are approximating a very
classical dependence measure but this is
incompatible in general is searching
over some binaca spaces from to general
functions F and G you can plug this into
a computer without further ado berlin
station and we have reduced this problem
to a linear thing which is the CCA step
so if you want to know more about this
there's this has been published in last
nips
the first new thing about using this
this random features is that you can
really do a theoretic a clean
theoretical analysis everything becomes
a random variable everything becomes
independent so making claims about the
algorithms is pretty straightforward
we've been able to show that this RDC a
statistic of ours can bridges to to what
we have presented before well when
respond when there's when this HDR
statistic is restricted to some
functional class more details of line
and then the convergence rate is one
over square root of n like any
statistical method plus an additive term
which is one over square root of k where
k is the number of random projections
and then you can understand k here as a
trade-off between speed and accuracy
depending on the set up you're working
then the computational complexity is
rather roughly angle again in the in the
sample size although you can improve on
that and then you can also identify some
limiting behaviors of of the of the
statistic with respect to its
hyperparameters the second neat thing
about the random features is that they
allow you to code your yoga rhythms in a
bunch of lines so this is the our source
code for for everything I've just
described it's just five lines and it's
pretty fast for a million data points
for example we were running in four
seconds and previous the state of the
art like a CCA will take 43 seconds to
run 100 100 thousand samples and we take
a hundred times less and you might be
worried that the accuracy has been
dropped but this is indeed not the case
here we are depicting how well the
dependency statistic is capturing each
of these patterns depicted in little
boxes here and in the x-axis we are
adding additive noise to these patterns
so by the end that we are placed in this
region it's just a cloud of points so
really none of the methods can identify
the underlying dependence we also did
some feature selection experiments this
is a Gaussian process regression when we
select 246 etc features greedily by
maximizing the dependence between the
features we select and the output that
we are trying to progress to and also
the the perform
mascara surprisingly good and that's the
first part of the doctor if there's any
questions about that no okay the second
application of the random features that
we have done is to component analysis
methods things like pca CCA we have we
have taken the kernel version of these
algorithms apply the random feature
trick to them and see how well they
escaped and analyze some theoretical
properties of them so in the case of
KCCA the solutions of current ka pcs re
the solutions are defined by the
eigenvalues and eigenvectors of the
kernel matrix K and in our case we are
going to approximate this matrix by K
hat where K hat is now not going to be
full rank it's going to be the sum of em
random features of this form and each of
them will be dot product against itself
and form our rank one matrix okay and
the the basic result that we've been
able to prove is that you again achieve
some kind of inverse square root the
convergence rate with respect to the
original method and and this result is
in operator norm so it's it's very
expressive in terms of all it's
controlling all the marginals and this
will be considered like a major result
some years ago but thanks to them
recently developed matrix concentration
inequalities by Lester Mackay and
colleagues this was a joy to proof
because all the features are independent
you have a sum of independent matrices
concentration of measure happens very
very nicely the same thing for canonical
correlation analysis all of this was
slightly more tricky to prove because
there's some embraces floating around so
we are we have only results for the
regularized version of canonical
correlation analysis again the
boundaries of the same characteristics
as pca plus this multiplicative term
which is an inverse linear effect with
respect to the regularizer so we cannot
make any claim for unrealized CCA and
this is something that I would be very
interested in discussing
again coding these methods you can do it
in a slide which is something pretty
weird right by the way who uses are in
the audience oh okay that's surprising
um yeah the code here is for CCA and PCA
and in terms of accuracy we have done
some experiments with CCA in CCA in a
sentence takes two views of the same
data and finds a linear crew a linear
subspace of these two views that is
maximally correlated in the case of
currency CA we are not talking about
correlation we are talking about
dependence so it's much more expressive
this dcca stands for deep canonical
correlation analysis essentially you
take the tube use of your data you push
them through a deep neural network and
well and you develop a back-propagation
rule that maximizes the correlation of
the outputs of the network this guy here
is coded in c with some parallel
libraries and took like 12 hours to to
run on this data set a hundred have some
points or something like that and then
the proposed method in the previous
slide which is our CCA and achieves with
enough random projections the same
accuracy took like two minutes in my
laptop so the the computational savings
are quite huge this data set if you're
interested is two views consistent the
first one is a speech and the second one
is the tongue placement of the speaker
so you want to find correlated
representations of these two views and
this is the the amount of extracted
correlation in some test set another
application of this method that we have
tried is this learning using privileged
information again you have access to two
views of some data but one of them is
going to be missing a test time so in
this case we have pictures of animals
and for each of the pictures we have
some summary like this is the animal
black is the animal wide and so on but
in test time we assume that these binary
attributes will be missing and we will
only have the pictures to train our
classifier with but still we want to
strike some information between these
two views because these high-level
attributes for example when you train
with them you will achieve hundred
percent accuracy and when we when do
Turing on it with images we get 55 or so
so we use the CC a method again and
using sir features for damages achieve
something like fifty five percent
accuracy and using this common subspace
of nonlinear random features improves
the classification of course about five
to ten percent I think that's pretty
much it and the takeaway message here is
that randomness provides easy
theoretical analysis implicit
regularization and we have seen this in
random forests and drop out neural
networks for example scalability and
little awesome performance in many
scenarios and also some research
directions we have pointers on how to
tackle these problems but I'm very
interested in discussing with you is can
we use these random factions to perform
density estimation which is a much more
challenging problem can we come up with
better design issues for sampling the
random features if we want approximate
the Gaussian kernel with something from
a Gaussian right but if there's
something more complicated and can we
come up with with some other designs
does it help to build deep randomized
architectures on how to do so can we
incorporate the sparsity assumptions in
the random projections in some way or
for example can we build incremental
methods for Mother's selection I build
them regressive with a thousand features
that then maybe it's not good enough if
I want to build a regresar with a
thousand one hundred do I have to
rebuild the whole thing again or can I
up at the features on how and that's
about it thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>