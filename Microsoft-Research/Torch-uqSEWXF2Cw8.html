<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Torch | Coder Coacher - Coaching Coders</title><meta content="Torch - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Torch</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uqSEWXF2Cw8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright so I'm here to we are going to
do this in two parts i'm here i'm going
to present the general things about
torch and then one of my friends from
Facebook is going to specifically talk
about the kinds of extensions and the
kinds of things that they have been
doing recently in facebook so torch has
been it's a framework for scientific
computing for lure or legit it's a
debate but logit is a very efficient
very efficient version of lua it has it
has too many advantages it has a very
small number of disadvantages depending
on what you want to do most of the time
you want to use legit and gain all those
advantages so torch has been around for
many many years it has actually been
started by semi banjo and run on colbert
mostly and Ronon has been always the
main developer for many years i think
since 2000 or 1990s i don't exactly
remember when he's very old and there
was versions like I think 135 and this
is version 7 for some reason it's always
odd numbers and the version 7 is the one
where it's actually actively pushing for
Lua and blue edge it being very much
integrated into the into the Tosh
framework there's a website of course
toast at CH and then everything goes on
in the Indian on github it is it is
structured such that it is actually not
just one project it is tons of projects
those are all tons of the lower packages
and you get one you get all the
dependencies just like any package
mechanism there's a good cheat sheet
that you might want to check out submit
chintala has put together this it
basically summarizes everything that you
would want to do and then how we would
do that in torch and then there are some
tutorials and maybe most importantly
there's the laws and large it's main
website or you want to learn about door
and budget okay so I mentioned several
things here so torch is always being a
a very sort of open source and like
company friendly because it has bsd
license so that companies and people in
industry can use it it has lots of
little packages but main main focus has
always been large scale neural networks
and online learning and so far I think
Clemence gathered these statistics that
has been more than like fifty thousand
downloads there are many I wouldn't say
many there are several universities that
are using this and major industrial that
so I was part of the deep lines start up
since we got bought by google now inside
Google there's we are using this
officially something like fifty seventy
percent research group uses it I think
Facebook uses it right now for research
and then now Clemence join Twitter
layers that they have started using it
so we are seeing some sort of good
industrial support is the Nicholas from
facebook is going to give you some
examples of what they have been doing
and okay so I want to talk about so the
torch is not much different than when
you think about it on the face of it
it's not much different than you would
have something like Python and numpy and
scifi or like people who come from
academia matlab so why first of all like
why a mixed language approach because
John just talk about the wall palette
which is one big application the reason
for that is we want to create complex
applications not just on one domain it's
a general-purpose system you want to do
anything and you want to have a
scripting language so that you can
interact with it you have data sets you
want to play with those you want to look
at your data you want to do some
exploration around your ideas
interpreted environments are really nice
for that and legit and lure gives us
that though I is a very very simple
interpreted language it's actually I
don't I haven't seen anything simply
like it I think it takes two days to
understand what is going on with the
language but once you have this what is
important is you need to have a you need
to have a good back end because just
because you do you
you interpreted development you do
scripting it doesn't mean that you want
to compromise on the speed right so you
always want to have your backhand the
algorithm implementation fast and then
with luanne legit this gives us this
gives us a perfect perfect space because
it is very easy to interface with C and
C++ from Lua and especially for an
especially from budget so we touch what
we have is we have a good back end which
is a CMOS almost every numerical routine
is implemented in C C++ we have a
something like an equivalent of numpy
and then there is the there's the CUDA
port and we use openmp for certain
things and then and then the advantages
of logit one of the main advantages is
it is I think still the fastest
scripting language that is out there for
certain things actually there are
benchmarks showing that it is actually
as fast as C and then if you have a data
that you say you allocate it in see just
the high level say something like a
numpy tensor you can actually loop
through it in the space as much as you
would look through that in see certain
operations you can do that fast this
gives us a great ability to actually
stay at the high level implemented
Gotham's very quickly and then don't
compromise on the speed it is very
simple it's very readable as I said and
then the interface to see is very very
clean especially with something like
like FF I rajat has a is on FF I
implementation of course sometimes there
is no interface it's just you get the
pointer to your data and you use it
directly in Lua and another big
advantages Lua Lua is a very very small
language it is it is several tons of
files couple kilobytes and when you
compile everything together you can you
can you can embed it in iphones and
video games and everything actually law
has been around for a lot for a long
while and it has been very popular with
video games because those guys want to
have good scripting languages that they
can embed one design choice that is
important that has been for Lua is Lua
doesn't need to be the main application
that you are running it is actually by
design is an embeddable language so
if you have a C++ application laura is
just the just the library that you link
with and you can start actually
scripting even from your C++ application
which this in turn means that everything
that we do in touch becomes a library
that can be linked into any other
application okay so this is a I think
I've answered many things here but this
is this always comes up why didn't we
just use Python and then 11 main
advantages as I said speed the other is
we wanted to build applications so that
it's self-contained it's there porting
it's very important and we want to be
able to sort something like FF I as you
can imagine the advantages translate
into being able to wrap libraries very
very easily one thing that you won't
help with lower compared to python is
the millions of packages that people
have done for you so most of the time
you would find yourself that like the
the availability of packages are not as
good as Python this is it this is the
reality but then you wanna you want to
wrap something you want to wrap a
library and use it right it is very very
easy I'm not sure if it gets easier than
that but people can check out examples
of what we have in torture like that is
around okay so I'll just um talk about
Laura a little bit it's less it's a very
simple language and it has only one data
structure and it's a table and that's it
and a table it also it's it's at the
same time same array it's a it's a hash
table and it's a strap it so this is
everything so you can you can have
simple numbers my table 1 2 3 and then
you're going to be able to index it web
with my table of one my table of 23 or
you can you can you can have a string
string elements or you can combine them
and then you can you can index the table
with anything functions are which is
very important functions or first level
seasons of the language so you can even
have keys of your table as your
functions so that this means that any
object that you have in nua you can have
a hashmap from that object to any other
object this is a real power
construct and once you have this it is
actually very very hard to let go and
then the other thing is to complete
everything of course it supports
closures and with these two tables and
closures with tables as powerful like
this and closures you can pretty much do
anything you want the language itself
Ethan doesn't have any object oriented
support in itself but by by using these
these mechanisms there are many
extensions that is out there which is we
also have a particular one in touch that
you can actually emanate objector anted
programming and we baby keep writing
classes and inheritances and things like
that but india and those are just tables
and functions which make me actually
feel better okay so torch as I said
there are tables but tables in the end
are not the are not the real solution if
you want to do large-scale machine
learning and if you want to play with
neural nets and everything right so what
torch provides is is one at the core of
it it's a very crucial component it's at
answer object it's an N dimensional
array and basically a tensor is a view
over a chunk of memory you have a memory
and think about this in MATLAB you have
multi-dimensional arrays right but in
the end in the memory space it's just
one-dimensional memory and as opposed to
MATLAB what you have is that not every
tensor has to have a distinct memory you
can have a chunk of memory and you can
have any number of tensors viewing that
memory or even parts of that memory with
different shapes this is this is
achieved through so you have a you have
a memory space we have a storage object
that wraps around that memory space and
it sort of manages it in the sense that
you can you can ask the store objective
locate more memory or less memory so
this is your memory interface and the
tensor is a structure over that where
you have your storage you have your
offset so that ok i'm going to start
from this location in my memory and i'm
going to have say a six dimensional
array and then strike this something
that gives you the ability to say that
in each dimension i'm going to jump this
much in my memory so with this with this
you can actually construct any n
dimension of the race i think this is
not this is not something new i'm leon
and
young and they did their lush they had
exactly the same structure and then I
think it is pretty similar structure in
numpy and then this this structure is
very powerful and it is at the core of
everything and then you can imagine that
we have we have doubled answers for
dancers integers by its kudat answers is
just another type of cancer and then
what you do is you implement all your
all your linear algebra operations in
terms of these tensors and provide a
provide the interface to to these in the
loft space so letting the loft space no
matter what type of tensor you want to
create you can create it and what type
of linear algebra operation you want to
do matrix operations vector operations
convolutions all sorts of things you can
do I'm actually not able to keep track
of time here so I don't know how am I
going ok ok ok so we have a bunch of
packages of course and then like
packages for image manipulation plotting
random numbers and any little package
that you can see out there you can use
it because as I said torch and all the
other packages that depend on tours that
we wrote our just lower packages there's
QT interface that was written by Leigh
own and then it provides you to create
GUI applications which at least in deep
might we use it everyday and then as I
said the structure like you have your
low-level packages there's the tensor
library on top of it and then there's
the lower capi that you have we have
another library that provides high level
functions over it and all the packages
that you write basically uses these
three libraries and do their job they
can be all in the lower level they can
be also have c implementations they can
have GPU modules and everything but
these these doubles provide everything
if you want to do more of course you can
do more you can go directly to your own
library and everything but I think this
provides is provides everything there's
a package manager of course and this is
a generic package manager that we didn't
write this is the lorax package manager
that is available for Lua and we just
provide a set of packages that are
related to torch
that's what we do so I want to talk
briefly about the NN package because
this is this is like the core of why how
tour started like at least the recent
version and this gives you the ability
to create neural network a barrack any
any type of neural network you can
imagine I can I think I want to claim
that you would be able to create with
this okay so it is it's a very simple
interface basically you have a module
any neural network layer is a module and
what you have is you need to implement
three functions and then with those
three functions you are going to do your
forward evaluation of your module and
then the backward which is going to
calculate the derivatives with respect
your input and the derivatives with
respect to your weights in your module
and basically what a neural network is a
combination of these modules put
together in some form and then you
connect these and then you do gradient
based optimization you go forward
backward you evaluate your network and
then you go backward depending you
calculate your loss and you calculate
the derivatives with respect to your
loss and everything everything is simple
from that point on in the neural network
library as I said we had we have
optimized backends when we use cpu we
used OpenMP and sse so that you have
parallelization in your convolutions and
many other operations and then they are
efficient and when you use GPU we use
cuda there are two packages as I said
torches packages lots of packages q
torch is a package that gives you the so
the dancer library is by default on on
cpu there's a version that extends it
the GPU it is Q torch and qnn is the
version of an end that extends it to GPU
and I'll show you a quick example
showing that switching from cpu GPU is
basically say you create an object it
leaves on the cpu space you say column
cuda and then it shifts the GPU and then
from that point on everything that you
do with that happens on GPU anyways
performance is important right so sumit
has created a set of benchmarks on this
website you can check all those
benchmarks and then see how things are
err my mum okay i think i'm gonna skip
these parts i want to show this quickly
which is how you would create a a
standard convolutional network okay whoa
the second loose this is a container
object it's basically going to put
blocks on top of each other we're going
to create commotions tan H max Poulenc
normalization conversion 10 h max
pooling normalization and then linear
layer and locks off max and I'm going to
minimize it with say some loss and this
is basically it this is how you would
define a standard feed-forward neural
net when it comes to more complicated
neural networks the code becomes a
little bit more complicated but in the
end this gives you the high-level
ability to construct neural networks
very very easily there's all sorts of
nonlinearities and standard modules that
you can find in the neural network
library and extensions of it that I
really think you shouldn't be able to
write anything more and these are I'm
not going to go into details I'm going
to mention another thing some neural
networks are harder because they don't
fit into these some Lego like blocks
where you can put blocks on top of each
other like feed-forward saw parallels
and things like that there's a graph
library that you can basically create
any sort of connections this is the
implementation of the LST I'm recurrent
neural network I don't think it's much
longer than how you would write it on
the paper and then these are all things
modules that are already in the in the
neural network library you just write
them as you would write the functions
and then you would get this sort of
structure which is going to be
implementing your last year and then
CUDA the last point I wanted to make say
I have a network I put the network on
CUDA from that point on I create by
input on the on the on the on the device
on the CUDA tensor and I do model
forward input this runs on your GPU
right and this is basically the ability
to switch from cpu GPU is this much if
you don't need to write any new modules
which i am saying claiming that
ninety-nine percent of the time it's
going to be that there
this is this is this is basically that I
think at this point Nikolas should come
because he's going to talk about things
that they have been doing in in in
facebook one thing I want to mention is
we have been doing reinforcement
learning in deep mind using torch a lot
which goes back to John's point about
causation and exploration and it is it
is a very important part of the research
domain nowadays and there are many
libraries that we haven't made open
source yet maybe we should okay so I'm
going to yes hi everyone I'm Nicholas
I'm from facebook so Facebook has
started using torch earlier this year so
we on electron joined facebook in
December last year and ever since we
started playing with torch and here's
currently what we what we're doing so
first of all we've been doing a bunch of
open source contributions that I'm going
to talk about and the other things we're
working on is mostly performance so
parties on multi-gpu and better
communications for for bigger parallel
computations as well as Colonel speed so
everything we've been releasing is going
to this facebook FB lower lip github and
so i'm going to talk about these open
source contributions so we released a
bunch of packages so here's the list of
everything we release so there's a
thrift shared ization library that i'm
going to describe a debugger 44 lua a
bridge between LuAnn Python which allows
you to write Python and inter inter
operably interoperate with them with lua
some c plus plus utilities that has been
quite helpful for us to write also unit
tests and things like that and different
utilities and and also some abilities to
write Matt
matlab files without even having MATLAB
installed for better collaboration so
I'm going first to talk about FB thrift
so thrift is a multi-platform
multi-language serialization that we use
in production at Facebook so actually
the servers the big Facebook servers run
thriftier ization all the time it has
built-in optional compression and with
thrift we've basically built realization
and digitization of objects and we
support i would say basic types and most
importantly the torch sensor which is
the main piece of data that we're
working on and any arbitrary cyclic
object graph composed of these things
and it's been pretty successful in the
sense that we do observe speeds between
three and eight times faster than the
tour serialization so it's pretty
convenient to use and so how would you
for instance sterilize and dis yer eyes
an object so you would require the the
package and then basically load an
object and just turn it to a string and
that's sir eyes and you can sterilize
deserialize dumped into a file get it
from a file get the object back and
everything is done transparently you can
activate compression if you want and you
can even use these things to send them
over our network and everything that
comes with a sterilization using thrift
so the second piece that we've been
releasing is this debugger which is
really a full-featured source level lua
debugger so you don't require torch is
really a low aside debugger and it works
really two way so it's built on top of
gdb and has very similar interface and
so you have two ways of using it so
either you just embed some codes just
say debugger enter at a particular piece
where you want to inspect your you code
or you can also have it launch
automatically as soon as you
an exception and that's very convenient
to actually get from line to the C code
and see what's what's happening if
something bad happens you can debug the
library you've been writing things like
that also plugs nicely with the kadagidi
be it's it's all automated by using this
lure debug on error flag and so the last
thing I wanted to talk about is this FB
dot by thumb sorry which is really a
bridge between one Python so really you
can communicate between the two
languages you can write a mix of both
languages so you can really use Syfy
with the eluate answers and benefit from
all the optimization execute on CUDA
writing Python code which type i and you
just work as if it were numpy arrays and
there's an on-the-fly data conversion
that happens for you the way it works is
basically we have two main methods
there's this pie exec and pie eval that
you call from lua which specifies on
python code and you can execute that
code in Python from lua and pass objects
around and so there's some
technicalities with the data model just
talking about them very high level but
they're all described very well in the
on the github basically LuAnn pison
don't have the same data models you need
to try to translate between between the
two there is it's not it's not a perfect
conversion so it's an approximation it's
a heuristic and sometimes you need you
need more so we have solutions of also
for that but basically the data are
always transferred by value the tables
are copied deeply except the tensors
which share the data so everyone
accesses to that same pointer that Cory
was talking about which contains the
data but then the view is translated and
the view is peanuts it's a few bites and
effectively what what what this means is
you can allocate create a new num pie
array and then pass it to Laura and run
torch libraries on it without making any
copy just a small metadata that's very
very fast and efficient
so for more complex types we also have
these opaque references which allows you
to basically pass data around in a chain
manner within within Python different
Python methods so i will show you about
this okay so i'll go very quickly for
these things so there's examples you can
find them also on the github but
basically you can run python code you
can import some numpy transpose it eval
and then tokenize from wawa using python
data structures and that's pretty much
it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>