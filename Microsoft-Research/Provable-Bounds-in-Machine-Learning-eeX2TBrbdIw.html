<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Provable Bounds in Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Provable Bounds in Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Provable Bounds in Machine Learning</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eeX2TBrbdIw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
to have encore moitra here he many of us
know him he was an intern with us a few
times and he will be speaking about
provable bounds in machine learning all
right great thanks it's great to be back
so i know many familiar face in the
audience but maybe you haven't seen me
in a suit so this is me in a suit I and
today I'll be talking about provable
bounds in machine learning so the folks
of this talk will be on revisiting
problems in machine learning from the
perspective of theory and I'm going to
try and actually talk and not just mime
title as I hear Adam did so let me start
off with an example there's a problem
that's near and dear the hearts of
machine learning researchers called
topic modeling and for now I'll define
this informally so suppose we're given a
large collection of documents say all
the articles written in the New York
Times in one calendar year we'd like to
somehow make sense of this collection
we'd like to automatically organized
them based on what they're about now we
may want to do this for a variety of
reasons maybe to help a user browse the
collection to be able to suggest
articles that you might be interested in
reading based on what he's read so far
or maybe we want to do this for purely
scholarly reasons to better understand
what happened in the given year in any
case what we're now faced with is a
problem in unsupervised learning so none
of these articles are labeled with what
topic best describes them instead all
we're given access to you is a massive
amount of data and we'd like to find
some hidden structure in this data and I
want to emphasize that actually this is
just one example of a much broader
challenge in machine learning where the
goal is to develop tools for the
automatic comprehension of data this is
really a Holy Grail in machine learning
and the algorithms and ideas that I'm
going to present in this talk even
though keep coming back to this example
they'll have applications not just the
newspaper articles but also to other
data types things like web pages
images genetic sequences and even say
user ratings so let me tell you
something about the prevailing approach
for this problem so the standard
approach this is initiated by sultan
many decades ago is to organize all of
this data into a very large very sparse
matrix this matrix is called the word by
document matrix and it's exactly what it
sounds like it's a matrix where each row
corresponds to a different word that
occurs in the collection and each column
represents a different document and then
the entry in row I column J is just the
frequency of occurrence of word I in
document check and actually throughout
this talk it will be much simpler to
think about this entry as being a
relative frequency of occurrence so I'm
going to normalize this matrix so the
column sum to 1 or equivalently I'm
thinking about the documents as being
distributions onwards so this is the
matrix we have in hand it's a very large
very sparse non-negative matrix and
they're in fact two different schools of
thoughts for how to perform things like
topic modeling one will be based on an
optimization framework another will be
based on sort of probabilistic methods
invasion perspectives I'm going to talk
about both in the stock but I'm going to
start with the optimization so in fact a
standard approach and machine learning
is what's called non-negative matrix
factorization and I'm going to spend
some time with this definition and then
I'm going to interpret it because i
don't think the interpretation makes
sense without really thinking about it
but now we have this entry wise
non-negative matrix m we'd like to write
it as the product of two other entry
wise non-negative matrices a and w and
moreover we'd like to try and minimize
the inner dimension the number of
columns in a or equivalently the number
of rows and w so now just to put this
definition in context what happens if we
remove these non negativity restrictions
then we have a classic definition in
linear algebra because the smallest
inner dimension we can obtain is just
the rank but really the problem changes
drastically when you require these
matrices to be non-negative and the
smallest thinner dimension you can
obtain is then called the non-negative
rang so the rank on the non-negative
rank can and do behave very differently
on matrices but may
I'm getting ahead of myself let me now
start with an interpretation of what
this is so why is it natural to require
these matrices to be entry wise non
negative and the very quick answer is
because we can now interpret this
factorization probabilistically so
recall each document is a distribution
on words and it turns out that actually
miss factorization we can always assume
that the columns of a and the columns of
W also some 21 so what this means is the
columns of a sum to 1 so their
distributions on words so we think about
these intuitively as being topics things
like you know a topic might be something
like personal finance where when I'm
writing an article about personal
finance there are some words that are
more likely to use things like stock and
trade and broker and each topic we're
thinking of as being associated with a
distribution on words so we're hoping
that when we find these non-negative
matrices the columns of this first
matrix encode interesting topics that
have some sort of intuitive meaning for
us and now similarly these columns and
WR distributions on a much smaller set
their distributions on the topics so
what they actually are this matrix and
codes are representation of each
document as a convex combination of
topics or to be totally formal about
this since this is really crucial for
the talk think about this column in the
matrix in this is a document it's a
distribution on words and moreover we
can represent this distribution
equivalently as the product of this
column times this matrix so it means
that this document is this convex
combination of these topics and this is
precisely what makes non-negative matrix
factorization so interesting throughout
machine learning is that here in this
example what it's trying to do is it's
trying to take a very large number of
observed variables these documents and
express them much more concisely by
finding a small number of hidden topics
that explain everything that explain all
of the documents at once if
sweetness and clustering is that a
clustering a would be 0 or 1 exactly yep
I can be approximate as well so there'll
be approximate versions of nmf and
they'll be exact ones as well so i'll
focus i mean uh you know I I don't want
to get into sort of the error bounds of
the algorithm so I'll focus on the exact
one but our guarantees also hold for
approximate ones is going to life you
never would you wouldn't get get a small
R would you know right but what you want
though is you would ideally want the
smallest are and you know I mean so
you'd want once you fix our to find the
most accurate one but for starters you'd
have to be able to tell whether it's
exactly are or not right that's why I'm
gonna focus for now on this exact
problem are there any other questions
about this definition so now actually
let me tell you something about so let
me see so in general you're learning
problem would be to fix arms and do some
things minimize to norm of the
difference of a and mo watch out yeah
yes in fact there are many norms that
make sense for example I could choose
something like the Frobenius norm or the
spectral norm or any of those things but
actually I'm to get to this in a in a in
a slide so let me defer that point
slightly but first I want to tell you
about this problem is actually a problem
that has applications throughout machine
learning even throughout theory and
beyond so let me give you an abridged
history of this problem so in fact some
of the most famous applications this
problem are in machine learning and in
statistics so this problem was
introduced by Lee and son and really the
pattern in which it's applied is the
same way that I just described the idea
is always to use this optimization
problem to somehow extract some latent
structure in your data to express a
large number of observed variables using
a much smaller number of hidden
variables and actually in this way it
has applications not just the things
like text classification and information
tree ville but the same two-level
probabilistic model is also use them
things like collaborative filtering in
describing users preferences as being a
distribution over interest they have
which in turn is a distribution on items
they might buy or move
they might watch actually interestingly
enough this problem has been introduced
at least three times so the second time
which is actually before this was by
Yannick caucus and in fact non-negative
rank plays a crucial role in a number of
combinatorial problems or problems in
theoretical computer science as well
it's actually Jana caucus introduced
this notion of non-negative rank in the
context of proving lower bounds against
linear programs so if you're given some
pollito p with many facets you'd like to
know is there another polytope q with
many fewer facets that projects exactly
to be and turns out that the smallest
number of facets you can get is the
non-negative rank of some appropriately
defined matrix so this is in fact the
direction and a connection that's been
very popular recently I've actually done
some work on this with mark Braverman on
showing lower bounds with a clique
polytope which I won't get into in the
stock in fact it even has applications
like the things like the log-rank
conjecture which is this famous central
conjecture and communication complexity
which can be thought of equivalently in
this language as what's the relationship
between the rank and the non-negative
rank for boolean matrices its equivalent
in this framework this is ranked where
you find so in other words given any
match its kind of always factor looks it
given any non-negative matrix you always
can because I could take means entry
wise non-negative because for my first
matrix I could just take the matrix all
the matrix is I saw tits+ into it's an
operator sense no no its entry wise
non-negative always entry wise
non-negative from the probability of
finding this all right awesome entry why
yeah in fact there are sort of many
different there but yeah and there's
sort of many different views of this so
you can see it directly that it's always
well defined or it'll turn out that
they'll be sort of more polytope
versions of what's actually going on
then I'll play a role in some of the
algorithms I'll describe so this should
hopefully become even clearer matrix
else would not have been enough um
strict entry wise positivity are you
talking about operator norm
not enough for their something well
that's not what i have but i think it
would be it would have been enough
negative matrices it has to be
non-negative ngos yeah so it cannot be
enough I think she's using the operator
norm definition yeah but it's it's not
so crucial actually interestingly
interestingly enough historically the
first introduction of this was in the
70s was in something called physical
modeling where was called self modeling
curve resolution where the intuition for
some of these applications and physical
modeling is in some case you have a
system which you observe some properties
and you know for a priori reasons that
the interaction of the components is
additive so when you're trying to
explain data without using cancellation
again you get back to non-negative
matrix factorization so in this way it
has applications the things like Kemet
ryx and biometrics and even modeling
things like marriage dynamics and
economics so this is my abridged history
the problem and hopefully I convinced
you that it's something that you know
connects many different areas but maybe
before we should we get too excited
maybe we should ask some basic
theoretical questions about this so can
we actually compute this so what can we
say about algorithms for computing this
factorization so in fact as I mentioned
nmf is something that's used in many
applications and machine learning so you
know what do practitioners do when
they're given a particular non-negative
matrix and like they'd like to write it
as the product of two other non-
matrices well in fact what they do is
you know it's it's I mean a good example
for it is local search it's oftentimes
heuristics without provable guarantees
so I'll describe this informally because
many of you probably seen this in other
contexts is we're faced with a non
convex optimization problem and the idea
is if we knew one of the matrices it
would be convex so the idea behind local
search is you guess a you compute the
best w you treat w is fixed then you
compute the best day and you alternate
back and forth so you know this is an
approach that has applications also to
things like you know local search is
used for things like k-means clustering
and
mixtures of gaussians but you're like in
all of those other applications it's
just a heuristic it fails on worst-case
inputs and it really does get stuck in
local up in fact even worse there are
more serious practical issues with this
namely it's highly sensitive to things
like the cost function as we described
do I want to measure how close a times W
is the M using the Frobenius or the
spectral norm these change the output of
this local search procedure that's very
undesirable because these details that
shouldn't change things change the
topics that you find so do things like
given than a and then M it's easy to
compute the W that would be non-negative
yeah see just nearest no yeah yep and
non negative as well so you can do this
via like convex optimization or you know
and the gradient descent is a sort of
good way to do this as well under- i
will just start from the actual natural
like rank decomposition and then try to
kind of make the vector slightly well
that's a little bit tricky because you
know if you start with a matrix which
the non-negative rank is very different
than the rank and starting from the rank
factorization is sort of starting from
something not on the right track so to
speak so the fact that there are
matrices which these things can be very
different it's not only crucial in some
of these combinatorial applications but
also is sort of a barrier to starting
with the rank factorization in the first
place so my goal in the first part of
the stock is to give you a relatively
complete picture of the complexity of
this so I'm going to start with maybe
the most optimistic question we should
ask the most optimistic thing we can yep
computational question so there it's
solved right so for example SVD will you
know find factors which are you know so
that's when you dropped this non
negativity restriction so the question
there is just how fast can those
algorithms work but actually the point
so that polynomial of us is not buddy
know me yeah I mean the other thing is
that so non-negative what makes an
interesting is the fact that it you know
in many applications that produces
better results
so the fact that an SVD the topics you
find are orthogonal is you know very
unnatural so these topics are difficult
to interpret but when you find topics
that are entry wise non negative i can
interpret it as a probability
distribution so I mean so the when you
drop non negativity it's entirely solved
the question is you know what happens
now with the slight twist so you say
it's high as a sensitive to the cost
function do you know that the exact
solution is not highly sensitive to the
cost function know the exact solution
could be highly sensitive but for sure
it won't depend on things like the
update procedure anyways I'm not going
to come back to local search my I mean
this is not even a simple algorithm if
you think about it because it really
depends on like the structure of local
optima so even though it's simple to
implement understanding it is you know
beyond that instead of owing to present
other simple algorithms but first
actually I want to start with this you
know even more optimistic question maybe
we can hope for an algorithm that works
on all inputs so first let's understand
the worst case complexity of nmf and
actually I only have one slide on this
because this will not be the focus of
the talk but I want to give this sort of
it gives a complete picture of what
often happens in learning that the
optimization problems that we abstract
away from these learning applications
unfortunately their heart in the worst
case this happens time and time again
maybe the first few times it happened
we'd be surprised but by now it's almost
not surprising that the theory would
predict that these things are very hard
sofa vases indeed showed that it's NP
hard to compute nmf and on the
algorithmic side cone and Roth Blum gave
an algorithm which ran in exponential
time based on based on quantifier
elimination and things like this but I
want to emphasize that even here this is
not the end of the story we should
actually see notice that this cone and
Rothbaum algorithm it actually is
exponential even for small values of r
so it's still running in time
exponential on the size of the matrix
and yet the first cut of you know what
instances are interesting of nmf you
know the most interesting instances are
when the non- rank is small that's when
you really can
take a large matrix and express it as a
small number of hidden topics so even
here we should ask a more nuanced
question namely what's the complexity of
NM f as a function of the number of
topics I'm looking for so what's NM f as
a function of all and in fact in joint
work with sanjeev Aurora wrong g and
ravi canon we showed a nearly optimal
algorithm fits so we showed an algorithm
which runs in time n times m to the r
squared and on the hardness side any
algorithm that runs in time n times m to
the little o of our would violate the
exponential time hypothesis would yield
sub exponential time algorithm for three
sets so if you believe ETH this is
roughly the worst-case complexity of
this problem actually for small values
of R this is an exponential improvement
over the previous algorithm and I want
to mention that actually hear this
question has some beautiful connections
to algebraic problems that it turns out
that these algorithms and hardness are
better understood in the language of
systems of polynomial inequalities
because instead of I can recast this
optimization problem as asking for a
solution to a system of polynomial
inequalities I can treat the entries in
the matrices a and W as being variables
and then the constraints that these be
non-negative or linear constraints the
constraints that a times W equals M is a
set of quadratic constraints and we
could ask can we solve the system of
polynomial inequalities to find the
factorization and turns out that the
running time of the solvers always
depends exponentially on the number of
variables so we get as an algebraic
question out of this is is there a way
this this is exactly why the algorithm
Conan Rothbaum runs in this time we get
an algebraic question about whether
there's a way to express this
optimization problem using fewer
variables so is there a way to get the
number of variables we need from being a
function of N and M to only a function
of R and actually turns out that you can
and actually r squared is the right
answer for this algebraic question and
that's what yields this algorithm
for about this question of knowing the
matrix isn't exactly these work for some
notion of approximation but it has to be
pretty close to a true factorization so
it has to be you know within some sort
of Epsilon ideally what you'd like so we
have some other algorithms which work
even in the approximation setting the
problem then is that they'll lose
something in how close they are so if
there is something that's epsilon close
they'll find something like are the
one-fourth times epsilon but regardless
this type of hardness result is also a
barrier to to you know doing the
approximate version you better be able
to solve the exact one so I'm not going
to go into what our approximate ones are
but they're based on things like SD peas
and SV DS as well but now so this is a
relatively complete answer to this
question but instead what I'm going to
focus on in this talk is now so this is
the worst case complexity what I now
want to focus on though is what if we're
not willing to settle for these types of
algorithms so we're going to have to
move beyond worst-case analysis somehow
and really the burning question in a lot
of these learning applications is what
makes these instances that we really
want to solve easier and so the focus of
this talk will be on a natural condition
which I believe is the right notion and
the context of text analysis where in
fact we can give a simple algorithm that
provably works and it works quickly
works in roughly quadratic time in fact
their work so fast that we'll be able to
use this algorithm for a number of
machine learning applications and be
able to get even highly practical
results which get as good quality as
existing best algorithms out there and
are much faster so that's informally
what the focus of this talk is so let me
tell you about this condition on
non-negative matrix factorization which
i think is the right notion in the
context of text analysis in fact this
condition which is called separuh bility
was even introduced in the machine
learning community a decade before us so
it's already something that's been
identified as being something that's
natural and indeed will show algorithms
under this condition so what I'm going
to do now is going to tell you about
this condition and there's a related
notion called anchor words so let me
define this now so let's take our
topic matrix a so each column in this
matrix represents a different topic and
each row represents a different word and
now throughout this talk I'm going to
use colored squares to denote on nonzero
entries and white squares that are no
zeroed entrance so all I'm going to be
interested in is the zero nonzero
pattern of these topic matrices now let
me define an anchor word informally an
anchor word is a word that when it
occurs you know what topic generated it
so let me give you an example so take a
topic like personal finance as I
mentioned their words that you're likely
to use when you're writing an article
about personal finance you know things
like stock and trade yet these are words
that are shared with other topics
because they can and do get used in
other contexts but if you think about a
word like 401 k then this is a word
that's fairly unique to this topic it's
a very good indicator because it's a
technical world or to put it
combinatorially 401 k is an anchor word
for the topic personal finance because
the only nonzero entry in this row
occurs in the column corresponding to
personal finance so this is the
definition of an anchor word let's see
another example what about a topic like
baseball again you have words like home
run and pitcher that cannon do get used
in other contexts in a home run is
frequently used in business to say good
job yet if you have a word like bunt and
this is a very technical word that's
specific not just the sports but the
baseball so this would be a good anchor
word or something like Oscar winning
this is great for movie reviews so this
is the notion of an anchor word and now
the notion of separable ities just that
every topic has an anchor work or
combinatorially for all columns there's
a row where the only nonzero entry is in
this column and I want to emphasize that
I'm not going to assume that we know the
anchor words we don't note but just the
fact that they exist when our algorithm
finds them it'll be like a trap door to
figuring out this factorization once we
know the anchor words we can zero in on
these documents where they occur and
it'll be much easier to
you're out the topics from just those
documents so I just because you're in
the exact case you're assuming that
every document that's about baseball has
the word budget yeah we're going to move
on later to probabilistic one where
that's not true so can I table that and
then we'll discuss it there it has so
we're in the exact case right now and
later we're going to move to a
probabilistic framework exactly where
what you're given is not the true matrix
M but samples from it in which case
words that are you know documents about
a topic will not contain just a clear
like a feature 35-percent about baseball
and twenty-five percent about personal
finance then you have your ten percent
whatever the P is for buns you have
thirty five percent p exactly in that
one category and twenty five percent I
mean faced it if you do if you knew at
anchor words were it would be trivial
with the problems that you actually
don't know what these anchor words are
because you don't even know what the
topic sorry are you also assuming VIN if
it's not about the topic it doesn't have
the a chord that's what I yeah exactly
because it's the only topic that could
have generated that ankle so let me
actually table Adams question that's a
great question but you're jumping a bit
ahead this will be like an extreme
version of the probabilistic model where
documents are really want but when we
move to when documents are short that's
exactly what makes it interesting is
that you don't need that assumption I
guess I don't know if you'd see in real
life make sense but it gets in peaceable
you could also if you don't have em co
words maybe create and copious like yep
maybe there is not one word rephrase for
example blank your phrases yes and I'm
not going to get into for example it'll
turn out that we'll be able to work with
even approximate anchor words for some
notion of approximation but I don't want
to get into how the error propagates in
the fact that there aren't perfect
anchor words but there is some way to
quantify that and later what I like
actually about this assumption is
oftentimes were faced this conundrum
that you know how do you formulate good
models that move beyond worst case well
what I like about this is that this is a
type of assumption that can be falsified
in the sense because I can run existing
topic modeling software on real data and
check our the topic matrices separable
now there'll be no nonzero entries in
that but they'll be closed in
it turns out being separable that our
algorithms will work and in fact we'll
even see this with experiments that even
when we run it on real data these
algorithms will perform as well as the
existing vest okay good this was a slide
that was supposed to get a lot of
questions so now let me tell you about
the main results which I'm going to
present you in this talk in fact all the
algorithms will be really simple and
very simple to understand as well so the
first algorithm is that separable ax t
really does make this problem much
easier there's a polynomial time
algorithm when the topic matrix is
separable in fact if the number of
topics you're looking for is a hundred
say them this is a quadratic time
algorithm and you really can run this
here there is none because what I'm
going to get to now is a stochastic view
of the problem where then it will depend
on P because you're only getting random
samples from it so it'll always be
inverse poly with it the bounds we
proved don't look as good even though
they're polynomials but in reality when
we actually test this out that's not so
bad yep regular matrix factorization
what is the complexity of the best
algorithm right now ah so it's well I
mean so computing something like SVD you
can do uh you know the dimensions of the
matrix times the rank you mean NMR yeah
it's the same thing yeah I was under the
impression she's more like an odd I mean
if it's like a square matrix it's closer
to NR and poly log and necessity I don't
remember exactly that's what attracts me
okay yeah see the thing is that for make
see for like SVD types of things it's
tricky because there are things which
are in principle constant time but what
they're doing is getting an additive
approximation and for vinia storm and
they're based on sampling from the
columns according to their l2 norm so
like they're all these papers of Kanaan
at all so it's very tricky to compare
the different things because they depend
on what Oracle you're given access to
whether you're given an l2 sampling
Oracle for the matrix or not
but you know the best general-purpose
thing is roughly you know there's this
demo at l algorithm 2007 which is the
best I know if for computing SPD that
said there are things like the Clarkson
woodruff algorithm which are now input
you know you know running Oh tilde of
the input length for example so they can
be parametrized by the sparsity pattern
of the matrix which this is not right so
as I mentioned the second half of the
stock will now be a probabilistic view
instead of the same type of problem
where I'm going to define this still
informally because it's a lot of baggage
but what it is roughly is that documents
are now generated stochastically as
convex combinations of topics or put it
another way you don't have exact access
to the matrix m instead you have access
to random samples because the words you
observe our samples from the underlying
distribution defined by document and you
could wonder in this context whether you
can still handle very incomplete data
the fact that the term by other word by
documate ryx you get is very sparse so
in fact it turns out that through a
clever use of this algorithm we're going
to be able to get a polynomial time
algorithm for learning the parameters of
any topic model provided just that the
topic matrix is separable and I want to
emphasize that what I particularly like
about this theorem is that it'll really
work for a broad range of topic models
in fact topic modeling is a gigantic
literature with in machine learning
where they you know keep proposing
progressively more and more complex
models that are closer to reality you
know lda said at the beginning of these
things and what I like is that this
algorithm actually works with any of
those so it works for you know in a
fairly oblivious way to how you're
generating the document says convex
combinations of columns so maybe that's
what eventually makes this algorithm
actually practical is that it turns out
that when we implemented this algorithm
it runs fifty to a hundred times faster
than the existing best algorithms out
there things like the mallet toolkit
with nearly identical performance on
every metric we tried everything from l1
to log likelihood the things like
coherence
are there any questions and I want to
mention that there's actually been a lot
of beautiful work on topic modeling over
the last year so Sean's in the audience
and a lot of people in Microsoft have
been doing great things where they give
Daniel and give great algorithms for lda
which are based on entirely different
techniques are based on tensor methods
instead so these techniques are very
different than the guarantees of sort of
incomparable so I it's a little bit too
much of a tangent about it right now and
i'll talk about it towards the end or
maybe we can get Sean to shoot I'm just
to make sure I understand what is this
mr. Cassidy generation process so if you
mean once you pick the collection of
topics that are document is supposed to
be about you get a distribution if the
distribution for zero probability on a
word you will not see that work right
yep okay so so it's not possible for
some radius erroneous words to appear no
no but the problem is that now what you
observe when documents are short when
they're only a hundred words long and
your vocabulary is you know 30,000 you
have a very sparse matrix which the true
matrix m is very dense potentially so
can you even handle this level of
uncertainty but we're getting ahead of
ourselves because i'm going to formally
define this later and i want to focus on
first this algorithm right here this is
very simple the first one ok so i guess
some let me move on to telling you I've
told you this algorithm is simple let me
show you how simple so I guess maybe the
first question I should address is how
to anchor words help so here I'm making
the assertion that once you assume there
are these anchor words out there that
the problem becomes much easier not
exponential in the topics but quadratic
time so how do you anchor would tell and
actually so I want to emphasize this
this is important even I've already said
it we don't know the ankle weights even
though this matrix is separable we don't
know these green sports yet the fact
that we're multiplying this matrix a by
W what it means is when you take an
anchor would say the second row of a it
picks out the second row of W scales it
puts it in it
so this is what makes the problem much
easier is that part of our factorization
is hiding in plain sight the matrix W
it's part of our input we just need to
figure out which rows correspond to W we
just need to identify the anchor words
so how can we identify the anchor words
pure semi one and cold you might as well
assume 203 right per topic yes you could
do that yep see repetitions so you'll
actually see the petitions in the Indus
matrix like almost you could yeah you
could in fact um again let me take all
that you know because when I go to this
topic modeling this probabilistic
interpretation will see a good way to
use even non anchor words and learn
something about the topic matrix so
that's sort of a generalization of your
question is how can you even use
productively words that are close to
anchor words so how can we find these
anchor words so we want right now is an
algorithm when I give you a road to tell
me is it an anchor row and now this will
actually just come from a very simple
geometric picture that I'll use not only
here but throughout this talk so
consider a row that's not an anchor row
so take the first throw in this matrix a
it's not an anchor realm and we multiply
it by the matrix W we pick out the first
and third rows of W and add them
together and we get this first thrown
out so what this means is that when
you're not an anchor row you can be
expressed as a convex combination of the
anchor rose if you do the normalization
right so it's a very simple geometric
picture now what are anchor words
they're the vertices of a simplex there
are extreme points that once we figure
them out we can express everything else
as a convex combination so now the
algorithm is very simple let's get a
geometric condition which helps us
identify whether a word is an egg word
or not but what happens when I delete a
row from em and it is an anchor work
then the convex hull strictly changes
because we've deleted a vertex these are
points how many dimensions these are so
right so I'm taking the rows in em and
plotting them and I'm normalizing
so they sum to 1 i'll have to sort of do
some funny business with normalization
throughout this talk but you can just
ignore it if you want that's to make
sure that the convex combination and not
a non-negative combination right so I
mean once I delete an anchor row I've
changed the convex hull so I've deleted
one of the vertices of the simplex and
yet in comparison when I delete a non
anchor rope the convex hull doesn't
change so deleting a word changes the
convex hull if and only if it's an
anchor work and now there's actually a
very simple algorithm we can for
simplicity do this with linear
programming although there are much more
clever and faster ways to do this well
when we want to determine whether arose
an anchor row we delete it and we check
can we express it as a convex
combination of the remaining rows if we
can then the convex hull hasn't changed
and it's not an anchor row if we can't
then the convex hull has changed and it
is one so now I can tell you the full
algorithm I mean I've already analyzed
it free we find the anchor rose let's
say by linear programming let me come
back to this in a second then we paste
these rows in em we paste them into W we
know one of the factors and now we only
have one missing factor and it's a
convex optimization problem so we just
need to find the non negative a so that
a times W is as close as possible to amp
so this is the algorithm in fact how
fast does this algorithm so let's do
things a little bit more cleverly here
so what I'm really trying to do with
this anchor would step is I'm trying to
find the vertices of a simple yet I can
do this without linear programming
imagine I found one of the vertices now
I can greedily look for the point that's
farthest away from that that will be a
vertex of the simplex then I can
greedily find the point that's farthest
away from those two that'll be another
vertex in the siblings and I'll greedily
find the one that's furthest away from
that so in this way we can greedily in a
very combinatorially way find these
anchor words in quadratic time in fact
you can even analyze how the error
accumulates in these types of procedures
like if you're not really given a
simplex you can bound how the error
accumulates in the words that you find
so you know
coming in so you're in the number of
dimensions is everywhere it's in the row
documents the number of documents
because we're taking rose and M and even
this last step
probably I mean you have to make sure
that when you paralyze it you're not
finding the same anchor word over and
over again what you could do is for a
well-conditioned simplex you can just
try and find one of the vertices many
times in parallel and just by like you
know I mean you just merge the lists and
you'll find all of them even if you find
it repeated many times so there is a way
to paralyze it I mean it won't get as
good error bounds in terms of how the
error accumulates but it'll still be
paralyzed abul in this way so in fact
now what I want to do is I want to take
a probabilistic views this is something
I've been sort of hinting at but now I
want to understand whether this
algorithm can be made to work with very
incomplete data so when we're not given
access to the matrix M but we're given
access to random samples from it and now
we can finally formally introduce this
topic model although mother's sort of
already defined it so it's very similar
to non-negative matrix factorization we
have a fixed set of topics but now
there'll be two crucial differences the
first difference is that this matrix W
that encodes representations of
documents as convex combinations of
topics this matrix will be
stochastically generated so there's some
distribution that generates the columns
of W so for example maybe when I sample
from this distribution the first
document is entirely about personal
finance and then I get this column in em
and the second document is half about
baseball in half about movie reviews and
I get the second column pen that's the
first differences w stochastic now the
second difference is the one that's
crucial this document is defined by
distribution onwards it quiver but I'm
not given access to that distribution
because the document is not very long
instead what I'm given access to our
random samples from so for example if
these documents are only two words long
I might just see these blue boxes
instead it was you could have stuck the
way in use of these of independence up
this yeah it's right that everyone does
so right so I want to emphasize that
here the challenge is that this word by
document matrix that I observe
curve is very sparse you know if
documents are only like a hundred words
long like in abstracts and this is like
15,000 this is a very sparse matrix and
you know compared to the true matrix m
and yet I still want to be able to
recover the number you know the topics
correctly the thing I have freedom to
play with is that I can potentially take
more documents the question is how many
documents do you need to be able to
actually figure out the topics so this
is a fourth Idol but I'm like roughly
four linked lists of documents like what
are the what's the actual drink and
let's actual life non-negative rank of
like I know Wikipedia or you're
constantly not Wikipedia sorry you know
I knew like roughly what I mean I think
you should ask how the approximate rank
and how the approximate non- rank or
whatever 90 I mean yeah uh I mean they
both actually seem to decay it not such
different rates but they don't have any
sort of sharp threshold we're beyond
that you know there's some like drop off
so it really is this game where you have
a little bit of freedom to choose how
many topics but then you know I mean
they will produce good topics actually I
mean it's it's sort of this trade-off
that is you increase the number of
topics sometimes you have a topic that
fractures and do a few different topics
for example so it's sort of thing so
that is not negative bank is
dramatically higher than the bank it's
just that you get better results if you
still not negative yeah that's right you
get much you get topics which are much
easier to interpret all for starters so
the issue is that you know when this
thing is allowed to be orthogonal then
when you're projecting onto the space
documents are are said to be more
similar even based on words they both
emit because this has negative entries
so that's sort of the intuition for
where this came from is because there's
a long history that holy starts with
Hoffman and probabilistic latent
semantic indexing if you be practiced
you don't see that big of a gap between
how well you can you know I mean you see
a fairly constant gap and you know how I
mean they don't behave differently
qualitatively in their assets
onyx but you know the quality of the
topics looks much better okay so just to
refocus now actually this is a very
general problem I've described in fact
there's a long there's a very large
literature on topic modeling where there
are just different ways that generate
the columns of W so for example there's
this very popular model called latent
garishly allocation of blinding and
Jordan that's just a particular way to
generate the columns of W using a darish
light distribution and this is some
distribution that favors relatively
sparse combinations of topics but of
course there are even more complex topic
models things refinements on top of this
like the correlated topic model of blind
Lafferty which then allows certain types
of structured correlations and anti
correlations between topics and this the
columns of W are generated by a logistic
normal and they're even fancier ones
like the pachinko allocation model where
these columns are generated by some
multi-level directed acyclic graph that
sort of intuitively tries to capture a
topic art yet my point here is not to
emphasize that these you know I don't
want to dwell on these differences
between these models but my main point
is that even the differences between
these models can be thought of all in
the same framework as just being
differences and how W is generated in
fact in light of this I want to make the
case that you know what we should strive
for algorithms that work for broad
families of topic models so that their
analysis of these algorithms rests on
fairly general principles and is not
tailor-made to some of the details the
model so that way even if you think the
world is generated according to el dÃ­a
you'll have an algorithm with guarantees
or even if you dream up some much more
complex model you'll still have
something you can say algorithmically so
this is yep documents might so you get a
sample of words let's say it's a single
topic document you get a sample of
associated with that topic so how is
this assay messing w is generated
because then your topic distribution is
uh no these are two different these are
two different changes for the nmf model
Oh see for example you need W to be
stochastically generated in some way so
that you know you don't just end up with
like purely you know on a single topic
or something like I mean it's it's just
an extra modeling feature that's the
thing is that what you learn is the
topics you might never be able to figure
out what topics describe it document all
that well right because if there are
only two words that you get per document
even if you knew what you're looking for
the topic matrix you still have some
inherent uncertainty on what the topics
are that describe it so the point is
that you know you're not trying to get
every document perfectly right because
even the model can't figure it out but
you're still trying to learn the topics
that's sort of the crucial part right so
now this last part of this talk is now
focusing on this question really at an
abstract level of what if the documents
are short can we still learn the topic
matrix even though what we're given as
very sparse and actually this algorithm
will be equally simple to understand and
analyze and the crucial observation is
that maybe the term by document of the
word by document matrix is the wrong
matrix to work with let's instead work
with something called the grand matrix
which is just the word by document
matrix times its own transpose so you
know so this is a word by word matrix
and it measures the co-occurrence of
different pairs of words but you know
the first reason to even consider this
matrix is that now as we increase the
number of documents the dimensions of
this matrix are not growing unlike the
word by document matrix which keeps
growing out this converges to something
right so it converges to its own
expectation which I can now pull out
these topic matrices and inside I have
the stochastic matrix w w times w
transpose also converges to something
this is a measure of the different pairs
of topics how often they co-occur and so
now the entire point of this picture is
just that these are three non-negative
matrices so I can group together these
last two it's a non-negative matrix and
this is
so we're again inseparable nmf now
there's a slight twist here which is
that this matrix the normalization is
not what it was in the original nmf
problem so it turns out what will
actually be able to get is just the
anchor words from this they'll be
extreme rows of the gram matrix well you
take em had em hat transpose and you
look at the rugs and you normalize them
supply some 21 there'll be vertices of
ass implants so they'll be extreme point
Rose you mean the extreme right the
extreme points of their corresponding
soon well stuff to do just a slight more
a bit of work to find the rest the topic
matrix since I'm now going to describe
but the point is that given enough
documents we can still find the anchor
words in these models and now the real
question the last technical part is how
can we use these anchor words to find
the rest of the topic matrix and this
will actually come from first a
reinterpretation of what it means for a
word to be an anchor work so for example
if a word occurs in a document I know
nothing else about that document I have
some posterior distribution on what
topic generated in fact a word is an
anchor word if and only if this
posterior distribution is supported on
one topic that's the definition of an
anchor word is that when the word occurs
you know what topic generated it and now
the key point is that once we have the
anchor words will be able to use them to
find the other posterior distributions
for all of the other words and this will
come back to this geometric picture now
right I have these rows of em had em hat
transpose which the anchor words are
these extreme points and now if I take a
non anchor word word number three I can
express it as a convex combination of
the anchor words it's half anchor word
two and half anchor with three but what
does this mean it means we're taking the
co-occurrence of word number three with
all the other words and that's the
vector in em had em hat transpose and
we're expressing that vector as a convex
combination of the co-occurrence of
anchor word to with the other words and
if anchor with three of the other words
so if you work it out what this means is
I can express the posterior distribution
on topics for this word as this convex
combination
posterior distributions on topics for
these anchor words so we actually know
the posterior distribution on topics for
anchor word three it's the same convex
combination because anchor words a very
simple posterior so the posterior
distribution forward number three is
half topic two and half topic 3 and we
get this from just the geometric
viewpoint of what's going on so now what
we have is we have probability of topic
given word and yet what we really want
the entries in this matrix a probability
of word given topic all we have to do is
use Bayes rule the switch which one is
which or to put it another way it's this
formula and the key point does that
everything on the right hand side we
know we know either we know probability
of topic given word from the geometric
picture and we know the probability the
different words just by looking at our
data so we actually can use this formula
then to go to what we want and that's
the algorithm so we form the grand
matrix to find the anchor words there
are extreme points again but now if the
graham matrix and we can use a
combinatorial algorithm to find them and
then we write each word as a convex
combination of anchor words this gives
us these posterior distributions and now
from these posterior distributions for
each word we can compute a using this
formula is not exactly right but you can
bound the error accumulation of us and
then it will give you effective bounds
on how many documents you need to get a
certain amount of accuracy and this will
then depend on things like the P
separate e it's it's I mean it's improve
Lee very similar yes and this you know
now you know how the error you know how
many documents you need will play a role
in what error you have for each of these
steps and then there'll be some
compounding in the error that you can
then analyze but there is a provable
balance
and in fact this algorithm now works
without any assumptions on how this the
casting matrix is generated other than
this matrix are needs to be non-singular
so there has to be no topic which is
exactly some linear combination of other
topics and how often it co occurs and
this works whether you think the world
is LD ace correlated topic model or
pachinko allocation model or even more
complex things you can dream up provided
just that the topic matrix is separable
and actually I want to tell you
something interesting about this
algorithm that it was actually inspired
by experiments which is somewhat unique
what I mean by that is we had an initial
algorithm with provable guarantees that
was based on the same pattern first find
the anchor words then find the rest of
the topic matrix but it used matrix
inversion and that's an inherently noisy
step which then actually produced many
small nuttin you know negative values in
the topic matrix yeah for small number
of top four documents yet what we're
looking for is a distribution so this
was the impetus to actually look for
something that used the fact that what
is looking for was a distribution that's
what led to a way to get around matrix
inversion based on Bayes rule instead so
this is I think somewhat unique that
both of these algorithms have provable
guarantees and yet as we'll see in the
real data this is the crucial difference
that made this algorithm from useless to
very practical is that it started to use
Bayes rule so let me tell you about
actually experiments and is the first
time I've described experiments and a
talk but I assure you I didn't code it
so you know how can you go about testing
these algorithms now we have provable
guarantees for them so we know that
given enough documents a polynomial
number of them and the parameters we're
looking for that we do get the you know
true topics we want to test how fast
they really work how many is that number
of documents huge or is it tiny for this
algorithm to do well so in fact you want
to test these algorithms in a setting
where you know they provably work and
you're going to test how fast they
really do work and so one way to do this
was actually novel to this paper is
learning a topic matrix on real data
using other existing software so you can
take like New York Times articles and
fit a topic model to it can take nips
articles and fit a topic model to it or
clinical data and fit a topic model did
and then you use that topic model to
generate further data that's why you're
assured that you know the matrix that
you're trying to learn is somewhat
realistic and we found that our
algorithm is fifty to a hundred times
faster and let's check the performance
on other data but I want to emphasize
that it's actually not surprising that
it's faster because existing approaches
out there use things like you know mcmc
methods and this is entirely based on
simple linear algebraic stats so this
should of course be fast so people who
actually do this use mcmc methods yeah
Gibbs sampling yep this mallet toolkit
that's very popular from UMass Amherst
in fact we we wrote a paper with someone
who helped write mallet so David min
Nell wrote melon no yeah so it was
someone at UMass Amherst and MIM no
contributed a lot to it but so the
timing results are not so interesting
because the algorithm is fast so this is
this recover algorithm based on matrix
inversion and this is the one I
described recover l2 is the one that at
its heart is using Bayes rule and so
it's very fast compared to Gibbs
sampling yet what I found surprising was
how accurate it is so this is measuring
the l1 distance you know the error
between the topic matrix we found to the
truth because we are generating the
documents from some truth and in fact as
you see this recover algorithm is pretty
terrible this is the one based on matrix
inversion it's not much better than
random guessing its l1 distances above
one or you know quite some time right
and yet look at recover l2 now this is
this this color right here and actually
not only does it get pretty quickly to
the same level as Gibbs this we do it
either way but this is the one based on
Bayes rule instead of matrix inversion
and it actually gets better than gives
eventually but faster and better
according to this metric
and this is also true for log-likelihood
again recover is terrible and recover l2
is is very comparable despite being much
faster I I would be shocked if they are
but thank you for bringing that up I
almost forgot to mention so what's
remarkable about this is that these
topic matrices are not separable right
I've just learned them on data I haven't
been forced that they're separable and
yet these algorithms still work does it
mean that they're probably merely
separable they're merely separable
they're nearly separable but you know I
mean it's actually hard to say you know
how how well we can prove bounds for how
close they need to be to nearly
separable for algorithm to work is much
more pessimistic than apparently they
need to be geometrically the separate
ability said that you have endpoints and
n dimensions with the they're convenient
contained in the convex hull of our of
these and when you say these are nearly
inseparable what does this condition
translate to so so um you're almost
right I mean you're probably right but
let me check on this so even though
there's a convex hull of our points
that's actually the general nmf problem
the difference is that part of your you
know in NM f what makes it hard is that
you don't know the vertices that simplex
these are the topics yet what makes this
much easier is that these vertices are
sort of present in the data already so
when you plot your data not only is it
contained in a convex hull of our points
but these are points are either present
than the data or nearly present in the
baby right so that's what the notion of
near anchor words is that they're nearly
present I see because that's actually I
mean that's a very good intuition for
what makes nmf so hard in the worst case
is that you don't know where these
missing our points are that magically
contain all of the data very okay yep
okay so we ran this on real data and in
fact you can run on Fringe thousand York
Times articles in 10 minutes instead of
20 hours and now i can tell you
let me just say where this fits into a
broader agenda so I think a lot of my
working learning can be thought of as
being organized around this question is
learning computationally easy so when we
abstract out these hard optimization
problems from learning applications are
they really a barrier the further
progress in learning one direction to
attack this question as new models is
what I described in this talk is this
notion of separate II then leads the
theoretical questions about can you
design good nmf algorithms under it and
topic models and we use tools from
computational geometry and actually an
important piece of the picture is
experiments here is the way that
experiments can help corroborate these
new models and you know also be able to
test out these algorithms it helps you
figure out whether your new model is
worth anything yet another direction for
attacking this question is oftentimes
these computationally hard problems come
from choosing the wrong estimator so
when you use maximum likelihood you
invariably encounter hard optimization
problems another direction which you can
organize some my work on is in finding
new estimators that avoid maximum
likelihood so like in joint work with
Greg and Adam we used method of moments
to learn mixtures of gaussians I guess
since we're running short on time i'll
just mention that we gave a first
polynomial time algorithm to learn the
parameters of any constant number of
mixtures of gaussians and i've also been
working on questions like can we make
these estimators robust so maximum
likely it is something that inherently
is very robust the model miss
specification so something we shouldn't
lose sight of when we're trying to find
surrogates for max likelihood is to make
sure that these algorithms are not you
know robot you know hopelessly not
robust outliers so I've been trying to
understand you know what the interplay
is between robustness and efficiency and
i can tell you maybe about some my work
on doing this for robust linear
regression with more its heart we
resolved what the right trade-off is
between robustness and efficiency and
i've also done some work on population
recovery and deep learning that i'll be
happy to tell you guys about and i guess
the last thing is that i guess there's
just one piece of
I work on I work on algorithms more
generally so my thesis work was actually
on approximation algorithms and metric
embeddings is about taking a very large
graph and finding smaller
representations for its communication
structure there's something that uses a
lot of tools from say discrete geometry
lately I've been thinking about some
uses of information theory to lower
bound linear programs the show that
linear programs can't you know be small
and solve click for example its joint
with Mark Braverman I've also done some
work on combinatorial and things like
smooth analysis like this past year I
had a paper with Noga and Benny where we
showed the first dense graphs that have
large induced matchings and that's it so
I'll summarize that often these
optimization problems that we abstract
from learning applications are
intractable like they were for nmf and
for things like mixtures of gaussians
are there new models that better capture
the instances we really want to solve
these new models then lead to
interesting theoretical questions which
in fact can lead to highly practical new
algorithms and this is really just the
beginning I think there's been a lot of
exciting work in these areas also
mention shaman Daniels work again as
being some of the other people have been
working a lot in this area and Greg as
well and that's it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>