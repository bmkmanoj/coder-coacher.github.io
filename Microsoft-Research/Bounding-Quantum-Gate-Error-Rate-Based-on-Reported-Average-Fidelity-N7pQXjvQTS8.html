<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bounding Quantum Gate Error Rate Based on Reported Average Fidelity | Coder Coacher - Coaching Coders</title><meta content="Bounding Quantum Gate Error Rate Based on Reported Average Fidelity - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bounding Quantum Gate Error Rate Based on Reported Average Fidelity</b></h2><h5 class="post__date">2016-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/N7pQXjvQTS8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
okay so we'll we'll continue on thanks
rod for a great first talk now we'll
move to Barry Sanders Barry's gonna tell
us about bounding quantum gate error
rate based on reported gate fidelity
very Thanks okay um so first I'd like to
thank Kristen everybody else for that
I've organized this event for the
opportunity to be here it's my first
time at Microsoft and the faculty summit
I found really great and I appreciate
the chance to talk to to all of you
about this work the work I'm going to
talk about here it's on the archive back
in January as it wrestles through
referee processes the idea the
motivation of this work is that as I'm
sure many of you know there have been
very big announcements about great
progress in fault-tolerant quantum
computing and I and so last year I was
staying at University of Waterloo Anna
during my sabbatical I stayed with my
son he's the first author and so we just
spent all our time trying to figure out
how can we take a reported gate
performance you know if an
experimentalist and they're doing great
work there's no disrespect to them but
if there's some really good reports like
ninety-nine point nine nine percent
fidelity gate and the idea is that
that's then sufficient to move on how
can we then make use of that number to
try to assess on a rigorous level
whether the whether it is scalable so
it's really the question it I kind of
try to think like an investor you know
if I have money to invest and somebody
announces the gate and they set up a
company should I put my money in the
company or not so this talk is just an
exercise in trying to take what's
reported and put it into a meaningful
quantity with respect to rigorous bounce
unfortunately the numbers don't
translate into very good numbers so our
work has turned out to be fairly
controversial for a couple of reasons
one is the numbers that we get and i'll
show you later on look quite bad and the
second thing is that the numbers are not
necessarily
so if we get bad numbers so we try to
tell people we're taking good numbers
turning the crank to get numbers that
are rigorous but if the numbers are bad
it doesn't mean the experiments bad so I
just want to get that across right now
that if somebody reports 99.99 percent
fidelity and we find that the
performance is ninety-nine percent
possible error rate it doesn't mean
that's what the experiment does but kind
of for six months we've been arguing
with people who for one reason or
another don't agree with don't follow
that statement so that's it so now I've
got your adrenaline going so and the
work is done so you've all Sanders the
first author is a PhD student University
of Waterloo Joel wollman is a postdoc at
the University of Waterloo these are my
affiliations for that so i just
mentioned briefly i'm at the university
calgary but i've also got a thousand
talent chandran position at the
university of science and technology in
china and we got various funding
agencies to pay for us to do this ok and
then others the cities i live in and
then on the left you get an idea of the
talk so i'll just keep it simple i'll
introduce the basic notions i'll talk
about the threshold theorem which is 20
year old knowledge so i think the young
people need to hear it possibly the
first time and the old people like me
need a refresher so i'm going to take a
bit of time and go through it then this
particular part of the talk connecting
fight to pee it's the back slash WP and
latex i don't know how to say it so i'll
just call it p but the idea is that this
is the average gate fidelity that
experimentalists report this is the P
that we plug into the fault tolerance
theorem and so the whole talk is just
about if somebody tells you this how do
you get that now you can't take this and
get that but you can get a bound on that
so given one reported how do we get the
other and I'll just be a little bit
clearer there's another controversy
which i'm not going to delve into in
this talk and it's whether
experimentalists are actually reporting
this so the idea is that an
experimentalist will do the experiment
and get some number they'll call it the
average gate fidelity which is then
reported so whether the interleave
randomized benchmarking or whichever
technique they're using represents that
number I'm not going to worry about so
if an experimentalist tells me this
is the average gate fidelity I'm just
going to believe it I really don't but
I'm going to believe it for the sake of
the top then I'm going to take that
belief and translate to bound here okay
that's the concept now just to give you
the context so in the fault tolerance
theorem the concept is there's an
existential proof and I'll be clear in a
moment but the exits existential
statement is that there exists a
threshold on errors and it could be
preparation errors measurement errors
gate errors noise in the system all
kinds errors but there exists a
threshold such that if the performance
and it's a gate error rate threshold if
the performance is better than that
threshold value then there exists a way
to make it scalable in the sense that
with polynomial overhead with faulty
gates and noise and local noise and all
that stuff there's an efficient way to
make the circuit make the make the
entire circuit work the problem of
course is that the proof is existential
so the way that it's approached in
practice is to try too often it's
tackled numerically Krista the chair did
worked a number of years ago and a few
others have gone through this you know
just using numerical techniques to try
to find it a typical approach is to try
to then find error correcting codes so
you look for an error correcting code
and you can establish you can establish
the error-correcting codes that you've
met some threshold that's the idea okay
and I've mentioned I'd belabored the
point about the aim of this work and
it's simply taking the average gay
fidelity Phi now I said so the idea of
the error of the fault tolerant fault
tolerance is that it should really be an
error on the set of a universal set of
gates you know we want to make a quantum
computer its Universal set of gates but
the way things are reported now
experimentally if somebody has made a
gate and reports it on that gate so I'm
just want to be a little bit clearer all
I'm going to worry about for this is if
I'm told that a particular gate has an
error rate we're just going to assess
that but if you want to assess the full
computer you want to look at all the
gates and find the worst one
and then assess that so if somebody
makes a gate and it's good it doesn't
mean the other gates are good okay now
I'll try to make the claim clear so a
lot of the work I'm going to present
here is very derivative in the sense
that it's using previous results and in
particular we like the wallman Flemmi a
result I'll tell you a bit more about it
so Joel Wallman is one of the authors of
our work and Steve Flemmi is now at
University of Sydney and so they come up
with a bound on the worst case quantum
they come up with a bound on the quantum
gate error rate and then what we do here
is we just proved it's tight in a sense
so we find a couple of examples and show
that the bound that they find can't be
improved now when I say it can't be
improved it's with respect to scaling in
terms of the dimension of the gate so if
you have a two qubit gate you have four
dimensional space 3 cubic gate eight
dimensional and so on so it can't be
improved with respect to scale and with
the dimension of the gate and and with
respect to the scaling in terms of the
average fidelity so our goal is simply
to show ultimately we show a tightness
of their scaling okay and then there's
some technical details on the threshold
theorem and the importance of fault
tolerance okay then I'll do it and I
think in this audience a lot of people
know circuits better than I do but I
just have a slide showing circuits just
to explain a few key notions that will
appear on the next slide so this is a
classical circuit some of the issues
that arise it's nice to understand it
classically first and so this just
represents the circuit and then we have
gates and gates can include the initial
the final the fan outs readouts and so
on and then if you take that particular
circuit and you want to know how big it
is the size of the circuit is the number
of gates in the system including all
those points I said then there's a gate
depth which is the longest path from a
given gate to the output there's a
circuit depth which is the maximum
length from the input to the output and
then there's a circuit width which is
the maximum number of gates have given
depth so if the given if the you know
given depth of four
has the most number of gates the width
corresponds to it so there's just
classical ways of counting the cost of a
circuit and they become important now
what's happened is so we have this fault
tolerance theorem it's a very important
theorem and actually we've been spending
a month digging through trying to get a
good statement of it and so there are
different statements of the theorem and
some are very complicated written and
some are too easily written so they tend
to be a bit sloppy and so I would kind
of use a Goldilocks principle and so the
author ona from ben or one we find just
right we don't nothing against any of
the others but this is the one we kind
of used now to make sure that we stay on
track on what the theorem is and I've
put verbatim the statement of the home
run off of van or theorem their theorem
actually came out 1999 but it was
finally published in 2008 so I don't
know if they had bad referees or they
just let it sit or whatever but but it's
an older result that was known but it
came out and they refined a bit ok so
the statement just to be clear is that
as I said before there's a threshold
beyond which scalability is achieved so
they talked about the threshold they
write it down with respect to eat or not
the circuits allowed to execute with
some error so epsilon greater than 0 is
the allowed error and we need to talk
about the metric for the errors then
they talk about Q as a quantum circuit
operating on n input cubits 40 time
steps using s 2 and 1 cubed gates then
they say there exists a quantum circuit
Q Prime and now in the strict sense Q
prime is a simulator of Q so there's a
quantum circuit Q Prime with and then it
talks about the overheads that I've just
explained in the previous slide depth
size and widths and so the depth size
and width scale poly logarithmically
with respect to and that's the number of
qubits and effectively the problem size
asses the number of gates t is the time
runtime one over epsilon inverse of the
air okay and they say such that in the
presence of local noise of error now how
to define local noise of course that's a
bit of an art and there's nice work I've
seen out recently even
making fault-tolerance proofs that
generalize how local noise has to be so
noise can be local but not totally local
to each cubed or gate and there's growth
allowances so noise can grow during the
process so even that part's a little bit
complicated to try to assess what noise
is allowed but it says that Q prime
computes a function which is within
epsilon total variation distance from
the function computed by Q so this is
the key element here the total variation
distance is a probabilistic quantity so
it's it's a classical statistics /
probability notion and this is what they
use in the proof so the total variation
distance establishes the ground level or
the foundation for a rigorous analysis
of the fault tolerance there it's not to
say that epsilon could be determined
with respect to another distance and
experimentalists using the average gate
fidelity it could be meaningful but in
order to connect with the fault
tolerance theorem we have to be able to
make either use a total variation
distance or some relative thereof or
make a new theorem and I'm all for a new
theorem I just haven't seen one okay now
just to be clear Q and Q Prime
stimulates so I just put down here the
notion of simulating so in some sense
it's really that the fault tolerance
theorem just says that there there
exists a theorem with faulty things that
will simulate efficiently the one that
you want and then the concept of error
is actually important we talk about
errors but it's important to get the
concept of error right if one wants to
then tweak what the the total variation
distance and so the concept of the error
that we use and we've been talking about
this for months amongst ourselves and
with other people but we see that a gate
produces an outcome classically you hope
it's perfect but in a noisy world or a
quantum world it's not and so the actual
measurement outcomes sample oh not
plugged in so it goes blank fast
oh so maybe after unplug and replug
portney yeah I know it's blank yeah
yours blank but I'll just get the plug I
should have plugged it in but now that's
gone blank I don't know yes either okay
so i'll just keep i'll have to wiggle it
every few seconds all right um okay so
the idea of the error is you have some
error prone thing it produces a
distribution of outputs and then the
question is whether the output that
you're producing as a distribution
matches the distribution you want to get
and that on a fundamental level is why
the total variation distance is used so
the total variation distance is
operational and it if you define air in
this way then it's then it's meaningful
so here's the here's how the total
variation distance works this is just
taken from a standard textbook markov
chains in mixing times and so the idea
is suppose you have two distributions
labeled here mu and nu then the total
variation distance is given by these
expressions but you can see in the
picture it's very clear that it's really
the difference between this distribution
one or distribution to in the area one
in the area to ten are equal so it's
really that area that is the total
variation distance between the two and
operationally there's a way to think
about it where if if I have a device
that produces outputs according to
distribution new and I want you to
believe it's mu then and I run my box
and every and then I throw away events
so that I only transmit to you ones that
will convince you with new there's an
operational way to think about it where
there's an overhead and throwing away
events so that I can lie to you so I'm
you know it it cost me to be a liar and
the worse the distance the bigger the
liar I am so in that operational sense
it's kind of where the fault tolerance
theorem comes in now we need to quantify
this so there was a beautiful work
originally by kid I
and then elaborated much more by John
watrous and John watch vs notes on this
our standard fare for people working
through the quantized version so the
idea is that we need to go from a total
variation distance to the quantum
version and the answer turns out to be
to use the diamond norm distance and
over here I've just written the
expression where and I've tried to
explain it there but first we talked
about a distance between states and rod
just told us all about what a density
matrix is so I don't need to even
discuss what this means and I could well
I probably agree with you um okay so
then it's the maximum over all channels
or observables in this case and then
it's somehow we can reduce it down to
the trace distance now what we want to
do is to deal with the distance between
channels so we think of each gate that's
imperfect could be perfect it's a
channel so then the idea of the
difference between two distributions
becomes a difference between two
channels and the diamond norm is the one
where we say take the channel and take
other degrees of freedom that we don't
want to touch so if you're going to
execute a gate you want nothing to
happen on the rest of the system and you
want the supreme um k where k indicates
the size of that larger system acting on
the state of all states and then you
take the worst case with respect to all
states so the diamond norm distance has
very nice properties and the end these
relations here were established by folks
in Van de Graaff back in the 1990s and
so the concept is that what evolves out
of this is the total variation distance
or alternatively through the diamond
norm this is the way that allows us to
think rigorously about fault tolerance
so you but you get the point that let in
the way that current experiments are
reported it's with recourse to a
different measure and so and again i
emphasize because i've been arguing for
months about this but i emphasize that
it's okay to use a different measure as
long as it has meaning rigorously with
respect to the scalability question so
i'm not against it but on the other hand
we need to
be much more careful about the way that
we report so then this slide is just
telling you kind of just a rambling
about requirements to be able to report
or to claim fault-tolerant threshold so
the question is then you know given an
experiment comes out it reports that
it's reached a point of fault tolerance
you know we built to see not gate now
let's build the next thing right now
it's okay like in some sense we should
just build the whole damn thing and
check if it works but on the other hand
if the claim is that it works we need to
be careful about what are the
assumptions or requirements so one of
the things that's that is required once
we still don't deal with total variation
distance is we start to simulate error
if we simulate the error then we have to
be very clear on the promises that are
made of what are the contributors to the
air and make sure that those are all
laid out you know and this is something
I don't see yet in a lot of cases that
that the that their results that say
it's fault tolerant because our model is
agreeing in some respects with
experiment okay now so a valid claim
that fault tolerance holds even even
with respect to the theorem is that
noise has to be local so we really have
to know about the noise and the system
and be sure that the noise and the
system behaves in the way that we want
now if we wanted to do in a full
experimental test that's rigorous the
reason experimentalist don't use this
technique is it's just too hard because
within the expression on the previous
slide it shows you that we have a lot of
observables and a lot of inputs you know
we're gonna do a supremo it could be an
uncountably infinite set but it's going
to be large in any case even if we take
shortcuts so it's experimentally
difficult another technique that's used
is to numerically evaluate and see if
the model is reliable with respect to
aspects of the experiment so can we put
faith in the model the model is subject
to a lot of tests and that we hope that
it's tractable on a classical computer
to be able to establish that what's
happened nowadays is the interleave
randomized benchmarking that uses random
Clifford operations and assesses that
way has become very popular so there's
now a technique that well it's been
around for a few years but it enables
experimental characterization in a way
that is entirely feasible so I think
that if something works well in the
experiment they should use it it's not
like I'm saying don't use that go away
do something else use that but let's
understand what that really means so the
average gate fidelity it's kind of
interesting that the average gate
fidelity I'll skip some of this stuff
but there's a the concept with the
Clifford operations it mathematically it
can boil down to the following so I've
just got the phi g is the is a channel
averaged over harm measure with respect
to the states in the system and DG so
when i write the natural subscript G
natural i'm referring to the perfect
gate so given a perfect gate that's
unitary it would be G on row so the G
natural is the channel version it's a
unitary channel based on that G and then
we just compose it so we talk about a
discrepancy channel which is the case
that suppose that you took your state
you sent it through a perfect channel
and then you send it back through the
channel you've got if the channel if the
channel you make is perfect then the
discrepancy is the unity ok so this is
really just convenient because it means
we could write a lot of unities and then
mathematically what that average gate
fidelity really deals with is in some
sense we can construct XE which is an
operator that projects on the state or
on the householder reflection with
respect to the state and then there's
some averaging over it and so
operationally instead of taking the
randomized benchmarking approach as a
definition we can use this and then test
whether what they do match is that and
again I said like for the moment we're
going to leave that aside we're just
going to believe that the experiments
are really doing average gate fidelity
but i just wanted to kv it in there to
make you worry about that ok now in
recently Joe Emerson's group has
been playing a key role in trying to
connect the gate error rate with the
average fidelity so there's a paper out
in viscera of a by magazine gambetta and
Emerson they're both at IBM now
Emerson's at University of Waterloo and
so they got a lower bound on the worst
case gate error rate and magazine and
his PhD thesis and to my knowledge it's
not published in any peer-reviewed paper
but it's good believe it's correct
anyway that's even something what was
tested by PhD examine suppose so then he
also puts in an upper bound there but
the upper bound I don't think was
published because it was improved a
couple years later by Joel Wallman who's
a co-author of our work and Steve Flemmi
at the University of Sydney and so they
got some constant out of there so this
is the state of the art the lower bound
and the upper bound on the worst case
gate error rate and then our goal is
like we're interested in the question of
whether you can do better and of course
if you can do better there are two ways
to test it one is to do better and the
other is to find an example that shows
that it's tight so we do the latter so
we show that they've done the best they
can maybe not necessarily with respect
to a constant but with respect to the
function the functional dependence on
the dimension D of the gate and the
average fidelity clarification is that
all based on the trace distance or do
people actually compute the diamond
distance which minus thing is much more
much more difficult to compute diamond
trace distance but I don't have the
proof trace distance-based I think I
don't know the proof of theirs but isn't
it an issue that trace distance does not
compose when you use it in inside a
bigger protocol where is that the
diamond distance would compose I have to
check so you know it are you familiar
with the proof for you I discussed with
Dave quarry in the context of a project
by me and he was saying honest reporting
should be based on the time and distance
yes and but it's caught it's hard to
compute you have to solve no but I'm
asking I'd have to go back and check
that but you know their proof or no okay
well I know some work when people looked
at channels which can be simulated
efficiently Clifford who could be a
Clifford cross operators and then you
you
look at the closest channel and try to
connect the distances yeah I or didn't
just bring this no I forgot to wiggle
this thing yeah come back at me I don't
know I I'll keep answering the question
is that there's a difference in the
scaling with 1-5 of the upper and lower
bounds its quadratic I'm curious if
you've got an intuition about which one
of those two is tight for the difference
between if at this concert which one no
no the far right in the the upper bounds
it's a square root 1 minus 5 lower
bounds it's 1-5 i'm curious whether you
got an idea which one of those is the
kind of the optimal scale oh you mean
this versus that yeah yeah but what the
lower bound the others are upper bound
yet it never mind back on track I later
I appreciate you yeah relate to Martin
Martin question so this average great
fidelity you don't consider any on
Scylla right so it is like the tempest
using trace distance so do you have
results when I like we apply the gates
and I used to have part of the states
that is untouched like they call I can
take my fidelity do you know anything
about it sorry you're talking about the
calculator you talk about your fingers
married you have this fight of cheap
rather you're asking about taxation the
fighter not the definition in the fire
Chi you just do this average / input
states right yeah but if you wanna
preserve and a move of the rest like you
know what he says of the diamond norm
you should have some rights among civil
that you don't touch right you should
have like the gate xmg and they state X
on jim g prime for example yeah do you
can you handle this or I'm so not sure
why you're asking that I mean so you're
you're here at the beginning or that I
don't know but okay no because I think I
covered I tried to cover very carefully
the point that we're just taking the
average gay fidelity as it's used uh-huh
so my talk is not advocating the use of
the average capability it's taking the
average gate fidelity as in the ideal
it's reported in experiment and then
we're looking to convert it
to the diamond norm based gate air raid
also so when you ask we can handle it
i'm not trying in any way to improve
average gay fidelity okay what I'm
trying to do is to use the reported
experimental quantity and seeing what it
means rigorously with respect to the
diamond or based agri era in the fault
tolerance theorem okay yeah thanks sure
okay so thanks for helping me give time
to get this lit up again I've got to
plugged in so I think it will not do
that again all right um okay so here's
here's the upshot we just first of all
we just plug in the numbers so it's just
kind of straightforward to just take
reported numbers I don't put we've done
various looked at experiment
superconducting ion trap etc but instead
here i just want to show you what
happens so if you take just plug it
straight forwardly and you take a 1
cubic gate like not to cuba gate like
see not three cubic gate like controlled
swap or Fred Caen and then if the that
column the first this column here if the
fidelity is better than average gay
fidelity is better than ninety-nine
point nine percent then the worst case
then the worst case Gator rates terrible
it means that it succeeds with seven
point seven five percent 14 sorry
failure seven point seven five percent
14.2 percent and twenty six point nine
percent of the error rates okay so now
as I as I warned at the beginning we're
not saying that you know if somebody
tells us that the average gate
Fidelity's ninety-nine point nine
percent they make a scene hot gate then
the error rate will be 14 for two
percent all we're saying is that's a
bound that we can establish okay so one
of the sensitivities that's come out in
the community is that we say that they
argue they say well we believe we're
doing much better than a 14-point two
percent error rate and then my response
is okay you know I I trust you can you
explain to me the assumptions that
you're making that allow you to do
better because we can't do it and then
this is where all the difficulty comes
in is that I'm only asking for a clear
set of statements that I can follow so
that we can tighten our bounce I'm
trying to make it more mathematical you
know you tell me what you're doing will
tighten it I'll show you
words the end that we've done the poly
noise channel so if its promise to be
Polly then we get a better scaling okay
I think I just repeat myself at the
bottom there and then our claim here is
that what we find and what we think is
that that then I'll show you the
examples we find this optimal scaling
and so we believe that those numbers
can't be improved without promises on
noise so there needs to be some rigorous
level where we're able to do something
but the only case we've been able to
crack so far is if its promise to Polly
noise and so far experiments don't even
like they're they're not proven people
talk about that but they're not proven
okay so the way that we try to do a
tight scaling with respect to with the
wallman Flaminio bound is we find two
examples one is to show I'll just show
you the functional form again one is
that we have the wolman Flemmi a result
and then we want to say if we hold Phi
constant can it scale better with
respect to D or if we hold the constant
can it scale better with respect to Phi
so that's the question that we want to
deal with and so in this case we just
find an example this is a unitary gate
and it's given as a diagonal matrix with
all one's in the diagonal except an e to
the I theta so this is a multi
controlled phase gate and then if we
wanted it to be local equivalent to a
multi controlled not we were just that
theta equal PI onto so we go through
some basic calculations we calculate the
worst-case gate error rate expression we
get the one minus the average fidelity
which we use the other formula to
calculate and then what we find is that
the in this case the worst case then the
least upper bound is larger than the
minimum of these two quantities so
effectively we're finding the same
scaling up to a multiplicative constant
as what wolman inflam you found so
we get for fixed d so we fixed the
dimension in that case and then we just
get and we kind of write as an inverse
because we're using a theta argument so
a lower and upper bound and theta
arguments make sense if quantities get
large so this is going to be small so we
just wrote it in the inverse way so you
could write on top and on top but
strictly we have to turn them over ok
and then oh there's a formal argument
this is you've all gave the talk and Joe
Broussard was there and she was very
careful about things so this is this is
the side that we keep if Joe broussard's
ever in the audience to try and make it
very clear ok now the tight scaling and
dimension we just consider a different
gate so here we have the channel we have
some parameter lambda between 0 1 and
then we have the identity channel and
the sea to the N naught is the multi
controlled not gate so it's got n plus 1
cubits the seat of the end not gate
there doesn't represent the unitary gate
but rather the channel the unitary
channel from that gate and then by
straightforward mathematics we can
calculate the worst-case terror rate the
fidelity we get this result and that we
find that we get tight scaling for fixed
d centering I think it's sorry that's
fixed e that should be four fixed fie
sir typo there ok so those two examples
show that the scaling they get er are
tight we found two examples some people
object to the examples we find because
so here's how the argument goes they say
well the examples you find at least this
example is a unitary operation that is
over many many qubits this is what so
some people call this a coherent error
we call the unitary error and then so
the argument goes as follows you know
you're using a case that experimentally
we can test for and reject so there are
efforts to put noise and channel so if
you think that your problem is a unitary
channel and you just add a little bit of
noise to the system you kill that but
the logical fallacy is
just because we find an example that
shows that it's tight doesn't mean if
you kill that example you've proved it's
better right so this is the argument
that we have is really just a basic
logical argument you know you can kill
our example but it doesn't change the
argument sure Oh microphone must be Mike
somewhere huh so your examples are
essentially testing their sympathetic co
you're changing the constant and this is
fine except when you say that fourteen
percent cannot be improved this is not
an asymptotic statement this is a very
specific numeric statement correct so
how do you kind of reconcile one of the
other well know it so I'm not yeah
you're right i mean what you're saying
is okay fine if you make an asymptotic
proof that scaling is good how can that
prove the 14.2 percent and if our proof
doesn't show up to some multiplicative
constant that's correct yeah so we're
just kind of arguing that the tightness
of the scaling asymptotically indicates
that but we're not saying we're saying
that we think that further understanding
of noise is required to be able to do
better but you're right the 14.2 percent
can't be written in stone because the
proof of the tightness doesn't
incorporate the constant and for fixed
size you know it doesn't prove it so
yeah your points correct but we can't do
better so you know we're just analyzing
as we are and we're not able to make a
stronger statement okay um say it
argument all right and then finally we
do a test for poly channels so I think
people here have a good idea what Polly
channels are but i'll just mention so we
have the poly operators XYZ and you can
throw an identity
so then with so over here we consider a
twirl operation that's that discrepancy
channel send it down a perfect channel
send it back through the one that you
actually make in order to keep the math
a little bit more elegant and then what
we do is sum over all the poly operators
the identity in the XYZ and that that
becomes what's known as a twirl
operation and then we calculate from
that the worst-case gate error rate and
Delta Delta here is the diamond norm
distance between DG and the twirl so the
Delta here that we introduce is a
concept we think of as Paulinus so it
gives you a diamond norm distance of how
far your channel is from the nearest
poly channel and that delta when we
calculate the worst case gate error rate
as diamond norm distance between the DG
the discrepancy channel and the identity
we find through a triangle inequality
that we get the Delta term as defined up
there plus one-half the diamond norm
distance of D twirl minus the identity
and then we can calculate this quantity
here and we get a result there which you
I hope you can see doesn't have the
square roots so remember up here we keep
having square roots over things the
square roots vanish so if you're
promised that the channel is poly then
then the 14.2 percent etc and I
understand you know it's not we have to
be careful in those statements but
essentially the scaling becomes better
so we expect the numbers to be better
and we can plug in the numbers I don't
show a table for that but the
performance is better it's not something
that with respect to the worst case gate
error rate plugged into fault tolerance
theorem is not wonderful but it's but
it's certainly better so the message
here is really that by plugging in the
by making assumptions about noise being
careful we can get better bounce and
that's that's what we do okay and then a
last slide so I'll conclude so first of
all we we've gone back to fault
tolerance theorem we've made sure that
we have a good understanding operational
if what it means we kind of look at it
as a an adversarial approach where
somebody is lying about the gate the
overhead to be able to lie so if you
make a faulty quantum computer it's
going to faulty quantum computer it
might still do all the computation but
you might have to run it more times and
delete outputs to make it look like it's
doing its job so we have this very
operational way of thinking of what the
error is we also deal with an
operational approach the average gate
fidelity to make sure we at least have
that understanding of operational
definitions of the two quantities we're
converting we calculate the fidelity's
and the gate error rates with respect to
wolman fleming and we show that that's
tight in scaling with respect to D 45
fixed and vice versa we then went
through and considered what the way we
interpret some of the experimentalists
arguments that the way we think about it
is they're promising us Delta
approximate Paulinus of the channel so
this is I won't mention groups but this
is what once some groups are telling me
and I listen for an hour I am able to
condense it down to that line and then
and then what we do is we find a scaling
that works in this way so once they do
it we're able to then take the result
and do our conversion and that we we
think that we could do better if there
are better things with noise but the
bath in this work is simple and to do
better I think we have to be better
mathematicians which I'm not so my
collaborators are and then and I think
that and then this kind of takes us to
the future outlook and that's it you
know we're getting to a point where
they're very optimistic predictions
about how well quantum computers are
going to perform we're not saying
they're not so our results that look
like poor performance doesn't mean
they're really performing poorly it just
means it's the best mathematical result
we know how to get at this point and
that there's some work to be done one is
really to get the noise models
established and trusted and then this
second is to find ways to convert those
quantities that experimentalists find
convenient to perform you know we don't
want to you want to tell them if
something works do something else you
know if something works we got to find a
way to be able to take that result and
then to convert something meaningful
thank you yeah I have one question so I
guess a curious sort of reverse question
so suppose you know experimentalist has
a circuitous system and doing some extra
can do like all possible experiments
with it right so i guess like natural
question as a mathematician for me to
ask actually how good he can learn from
you know all this kind of experiments to
actually establish bounce on such
quantity right thing suppose an
experimentalist does three cubed like a
tough legate yeah so suppose
experimentals has a freedom to do a nice
tribute experiments right on his system
right so how close you can get to
estimating how this P right because you
know I mean can he do but in principle
batches I'm just you know measuring
fidelity through a benchmarking and
anything else right because is it just
like because he just kind of doesn't
have access to you know it just kind of
a question of you know lack of knowledge
from experiment a lack of possible
knowledge of from experiment or is it
just a question of not doing right
experiments yeah no I understand well so
right now I think the way that like I
feel like the experimental approach is
coalescing on on the optimum you know
that the technique of randomized
benchmarking kind of makes sense you
know if you really want to do things
rigorously you go to an experimentalist
and say you know do this over every
input which is uncountable and you know
like there's a set of instructions we
might give based on the math that are
not possible so if an experimentalist
builds some or any three cubic gate I
feel like the way they're doing it which
is
trying to do randomized Clifford's and
that seems it seems like a good way to
go in that there's time scale limits and
practicalities is that you using delphi
yeah well I guess yeah I mean you know
if ever like to try to prove some kind
of impossibility result say I would just
say okay you know here's like so
experimental skill can can do this my
knee operation since in this time right
for example how much she came back from
its raison you know maybe this kind of
question I would initially asked racers
yeah right and then you might do it like
instead of a uniform prior you might do
a non uniform prior over inputs and all
that kind of stuff there might be better
ways to do it but I understand what
you're saying you're saying given that
the experimentals has what is the
capability and then design instead of
using average gate fidelity find the
best way to report right now or you know
even like nice just proved it bound
right so because you know if you just
can prove a bond even without finding
this way right so maybe is this bounce
will be just pretty close to what you
get from Fidelity's and you just don't
need to search for new ways which is
kind of okay here's a best possible we
can do right or this is what we get to
randomize benchmarking you know the
difference is actually not that big so
you know we can actually kind of
establish as we can actually do but the
best thing I agree with everything
you're saying thanks yeah I just kind of
curious a few thoughts opposes there
another question okay great so well
let's think very one more time and</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>