<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Multimodal Gaze-Supported Interaction | Coder Coacher - Coaching Coders</title><meta content="Multimodal Gaze-Supported Interaction - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Multimodal Gaze-Supported Interaction</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/O8gTJOIV5gA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so can everybody hear me first yeah okay
so yes my name is Sophie and s and who
already introduced me I have a couple of
months ago I have successfully defended
my PhD Jesus at the interactive media
lab and racing and today I will talk
about a bit of my work that I have done
during my PhD about multimodal gays
supported interaction and I want to
start with some motivation kind of
driving motivations behind this work so
one was how can we interact conveniently
with large information spaces such as
geographic information spaces but it can
be kind of also in general zoomable
information spaces stack of pictures or
more extensive information spaces and
another driving motivation was how can
we interact seamlessly with large and
multiple distant display so there is a
increasing diversity of displays that
are surrounding us and how can we
seamlessly interact with them and one
modality that hasn't been kind of
investigated much so far is to use our I
gaze to support interaction with such
environments and there are several
benefits no kind of potential benefits
to use eye gaze for interaction so a
couple of very or rather obvious
advantages are that all I guess is very
fast it doesn't really require require a
lot of effort to look look at something
that interests us and it is also pretty
implicit on the other hand it also
provides the potential for a very
seamless interact remove this with our
verse and display setups and it's also a
very interesting alternative input
channel that can add to traditional ones
for example using mouse and keyboard and
adding gaze input to that not so much
trying to substitute mouse input which
has been tried a lot of times
in other research works but rather to
really think about how can I gays
support other interaction modalities and
also another potential benefit our
attentive system so that actually
digital systems or computer systems that
are surrounding you are more aware of
what you're looking at what you're
attending to on the other hand there are
a lot of challenges actually that make
the design of case-based interaction
pretty delicate on the one hand we have
the Midas touch problem which has been
coined by Robert Jacob in the early 90s
which simply describes the involuntary
issuing involuntarily issuing a command
why I guess so you're looking at
something and immediately something is
happening so this is very overwhelming
of course for the user and this is also
closely related to the double role of
our eye gaze if we want to use it as an
input modality because we use our gaze
constantly to observe our environment
and also want to use it as a control
input so we have to be aware of that for
designing gas-based interaction then
another challenge or at least an aspect
you have to be aware of is that our I
guess is not a very precise in pointing
modality so this is due on the one hand
due to tracking errors but also it is
simply due to the physiological nature
of our human vision so we do not have
laser precise vision so that's why we it
is pretty difficult to assign one pixel
that represents our I gaze and these
impressions can usually be kind of
categorized into jittery case movements
or representations of the I gaze and
offset so that the cursor would be
displayed at a point what you're
actually looking further to the left or
right another thing is that a lot of eye
tracking systems are visual tracking
system so they are reliable or
susceptible to changing light conditions
so it is pretty difficult to use eye
tracking in the
I was changing light conditions
especially direct sunlight is more or
less a no-go currently but there are
other eye tracking systems that use
other mechanisms for that and also for
the design of case-based interaction one
challenge is prejudice so a lot of
participants that I've worked with they
were kind of coming to our studies and
kind of already thinking this is a
natural I don't want to actually really
use it but they were often surprised
actually how good it worked so talking
about the technical challenge so the eye
tracking systems have very improved over
the last year's but it is sometimes
still common that you have to stabilize
the head to acquire high accuracy in the
gathered gaze data however there are
continuous development so here are just
two examples of wide range of the
different eye tracking systems one is a
table mounted I tracker from Toby which
is kind of distant from you so you're
not it is very enough true civ because
you're not you don't have to wear
additional equipment and whereas a
head-mounted I tracker on the right here
you have to wear of course and it may be
occluded some of your yeah what you're
looking at your visual field on although
it kind of looks you're very obtrusive
usually users really quickly adapt to
that and just look beyond the camera in
front of their eyes so these were kind
of the basics so my research has focused
on how can we now use gaze as a
supporting input modality for distant
interaction and since there hasn't been
too much research in this area so far we
have focused on supporting quite basic
interaction tasks such as navigating
through virtual information spaces for
example steering in a virtual 3d scene
and also selecting positioning and
manipulating distant graphical objects
for
we have investigated different
multimodal combinations of gays input
with other modalities input modalities
as you can see here a few examples using
mouse also kind of a way to confirm my
action also touching put from ubiquitous
smartphones or tilting your hand tells
your smartphone and we also considered
for input and with these multimodal gate
supported combinations we can address
several challenges that I've mentioned
to overcome or to counteract for example
the Midas touch problem or inaccurate
case data and also in contrast to a lot
of studies that use only the only gaze
and put to also confirm selections so
you would have to stare for example at a
practical button to then confirm it we
can counteract or overcome this time
consuming to Ellen using such multimodal
combinations also there are of course
different ways to use gaze for forum
applications I tracking application so
here kind of this is a very broad
overview so in general you can
distinguish diagnostic studies and
interactive so our focus was of course
on the interactive part diagnostic would
be that you just observe or want to
investigate how people look at the
website for example or how people
observe their everyday environment for
the interactive part we can roughly
distinguish three main areas where I a
tracking or I data has been used for
interaction so far a very prominent one
is gaze directly pointing of course
which we also have focused on and where
you can distinguish different levels of
precision for example do I really need
pics of precise case pointing or is it
enough that I know the rough area of
interest and also different levels of
abstraction for example am I only
interested in a certain target versus a
certain point I was clicking describes
that you have to find a way to confirm
an action that is what I've mentioned
earlier with the trailing but they have
also been works that try out different
clinking behaviors how you plink was
your eyes to confirm an action which of
course was assessed as pretty unnatural
and also there have been more and more
works recently on I gestures so that you
have to have to perform particular
gestures using either your eyes so just
moving your eyes quickly to the left to
the right to confirm a particular action
or more precisely to use your eye gaze
so you look at certain objects in a
certain order to then issue a certain
command however my work hasn't focused
on this area just to distinguish here to
make that clear we really focused on the
gaze directed pointing and to kind of
take benefit obviously implicit gaze
input rather than forcing the user to do
some explicit I gestures or something
like that so now I want to go into
detail about some of our projects I will
start with gays supported navigation so
we have investigated different areas
here for example how can users use their
eye gaze to navigate to steer in a
virtual 3d scene for example also this
is relevant for example for digital
games first-person shooters or something
like that we want to move through a 3d
scene and also we have explored how you
can use your eye gaze in combination
with a mobile handheld for example a
smartphone to explore large data
collection so that you could zoom in to
a certain picture that you can select a
picture and so on but now in the next
few slides I will actually talk about
our last project that we had done in
this area about more general gay
supported pan and zoom for which we have
oh sorry for which we have combined
different modalities with I gaze to
perform pan and zoom operations and
first I want to distinguish here or kind
of highlight the
different three different aspects you
have to be aware when kind of designing
for the pan and zoom or when to
distinguish also from other research in
this area and these are three conditions
you have to be aware of so one is the
input conditions of course so does the
user have to look continiously on a
certain at a certain region on the
screen to perform a movement for example
penning up or down or do I just start a
movement then it's moving I can freely
look around so I'm not this would
address a double role of the I guess so
you can still freely look around but you
have to find good way to stop the motion
to prevent over cheating on the other
side the speed can be constant or you
can gradually increase and decrease the
speed using your eye gaze and similarly
for the direction is it a fixed
direction for example I look at a button
a certain speed and directions
associated with this button or can I
gradually change the direction here just
two examples for this on here at the
right from Adams and colleagues they use
kind of active regions so you have to
look at a certain region to perform a
certain patting motion with a constant
speed and also down here Hans and
colleagues used an approach where this
penning speed would increase the further
you look away from the center part and
also you could gradually adjust the
direction so if you look to the T up
here this would then move to the center
and here are of course a couple of risks
for gays Bay steering so there's a high
risk for involuntarily issuing commands
because these this approach for example
was a gaze only approach so you head to
12 then a certain action would be issued
and it was sometimes yeah it would raise
involuntary movements also the lost
feeling of control often people run into
overshooting problems using gaze for
movement
control and it was described as
overwhelming and we wanted to address
this with different combinations of I
guess who is different modalities and
for this we used a gaze directed zoom so
you could simply zoom in towards what
you're currently looking at and could
adjust the speed and the input condition
whether it's discrete or continuous
using different modalities and just
picking one here for example the
attached input was really nice for that
so you could have as large screens so
the idea was you can just step up in
front of a large screen you look at a
target that interests you and you can
perform a very saddle and very low
effort touch gesture for example moving
your touch point up to zoom in or down
to zoom out and could also gradually
adjust the speed and here's an example
for that so the cursor is controlled
using your ideas which is up here you
look towards London and they are very
subtle where you you barely notice that
there are movements and you can zoom in
and out and explore for example Googlers
here for penning we also used a gauge a
gradient-based rate based and continues
approach so what you are looking at
would then move towards the center of
the screen and the speed would increase
the further you look away so that it
becomes faster the further this way and
then gradually gets slower towards the
screen center and it is a continuous
input so it would adjust all the time
however you can it is not active all the
time so when you are not touching your
smartphone for example to take the
previous example then gaze and put this
off you can freely look around and once
you touch the smartphone you indicate
okay now I'm ready I want to interact
and the penning would be active so as a
very very rough conclusion this approach
the gaze directed
ending worked pretty well for slow
patting motions so if you have for
example zoomed in towards London you you
want to follow a certain Street for
something like that it worked pretty
well however if you want to quickly go
from I know Berlin to New York and you
are already kind of zoomed in have to
perform very fast pending motions or
want to perform fast patting motions
then it gets really overwhelming so we
wanted to address or still have to
address this form or we have to keep
that in mind for the design of course it
for slow panning it is ok but first not
so much and the gas directed zoom was
all in all it says very positively we
have tried it out I mean I have demoed
at several conferences and usually got
really really good feedback so there was
assessed as very natural and oh I'm
looking at that and zooming in there and
once you don't want to interact anymore
you just lift the finger from your touch
pad and or touch phone and that's it
awesome we could we tried out different
combinations of these aspects that I've
mentioned earlier and for example also
disk read input so that you have to look
at a certain area and so on but the
continues rate based in gradient-based
combination was assessed as providing
the highest feeling of control and
people really liked that they could
still stop when they wanted to they
didn't feel so overwhelmed and this is
of course really important if you want
to design something using your eye gaze
also kind of work in progress we have
showed a demo at hide this year in Paris
using a combination of foot input
devices with I gaze also for panning and
zooming and here's the idea was actually
to leave the hands-free for additional
input for example our scenario was it
you have a geographic information
systems I want to annotate certain
things on your map and also of course
the one too yeah don't you want to
continue to draw
going and it's the same time one to do
some painting or zooming operations so
for this weird kind of the setup was at
OBT x 300 which would then capture your
eye gaze and had these different input
devices so we had one directional foot
pedals to just zoom in and then another
one to zoom out again and also to when
the foot pedal would be activated then
also the penny would be active using
your eye gaze and then we had here this
one is a two directional actually a kind
of a joystick kind of way that you could
also do pass panning motions just using
your foot input and then going forward
and backward you could adjust the
zooming in and zooming out speed and
similarly was this two directed foot
pedal so potential benefits of this gate
supported foot interaction is that of
course we can address several of the
challenges that I have mentioned so we
can compensate for unintentional
activations because we have this
explicit foot input and the user can
freely look around when taking the foot
of the pedal also it is it was described
as having a high comfort because we can
take advantage of very fast it
emphasized gaze input and also of course
we have this aspect of multi modality so
we still have the hands-free we can do
other tasks you can also imagine other
application areas for example kind of 3d
modeling where you want to move around
an object but at the same time want to
manipulate it for example so as I have
mentioned this is still work in progress
we have gathered first user feedback we
also showed it to a couple of not only a
couple of there were a lot of
participants at Chi which we asked so
actually actually X experts in the HCI
feel so that was really insightful and
are still working on more thorough
evaluation of course really on the
suitable
practicality in in a real working
environment do or would people really
use it would how can they handle the
primary and secondary input and so on so
this was the navigation part and now i
also want to talk about the gate
supported selection and positioning so
you can again imagine you're stepping in
front of a display want to select
certain graphical objects and maybe you
want to reposition them manipulate them
and for this we particularly focused on
a combination of i gaze was ubiquitous
smartphone so that you can simply use
touch input from your handheld and we
propose the principle gay suggests so
that we can really take advantage of
very hesitant precise gazing put and
then have touch to confirm the intention
of the user to select a target for
example or to refine the rough input and
also for example to add additional
actions for example manipulating
rotating an object so first we
concentrated on gay supported selection
kind of only gay supported selection
itself and for this we deliberately
wanted to support not only selecting
large targets and not only selecting
targets that are maybe far apart so that
you can do kind of a snapping technique
or something like that instead we tried
out very small targets also small target
that are closely positioned and even
overlapping targets and again the
scenario was that you have a smart phone
in your hand and you want to interact
with the distant display and for that we
considered that you should be able to
interact with your smartphone ice free
so that you wouldn't have to wouldn't
need to switch your visual attention
from the distant display towards your
smartphone all the time instead that you
can interact with your smartphone not
looking at it and maybe just holding it
in one hand so that you only use your
thumb
to interact also we wanted to address
several challenges of course for example
how can we prevent involuntary actions
that we can of course do similarly as
before with the pan and zoom project by
just touching the smartphone indicating
okay I'm already and lifting the singer
if not and also subtle gaze interaction
not to overwhelm the user so how can we
actually design a way to also not
visually clutter the view too much to
overwhelm the user and how can we
contact in precise case data and there
are several words on how to counteract
emphasized case data for example zooming
lenses of course it's not only in the
eye tracking field but also of course
for touch and put where you can visually
extend targets and motor space to more
yeah easily select tiny target then yes
okay and there was actually another
slide well I don't know where it went
about the magic technique so that you
could manually fine-tune your rough gaze
directed cursor position till then yeah
also select small targets you could use
your mouse for example or in our case we
use a touchpad to then refine the
position but I will talk about that on
the next slide so again here are the two
main approaches to counteract or to
counteract in precise gaze data the
first one actually just is a very
straightforward approach do you look at
the target and you can can confirm the
selection however there are no
compensation for in precise case input
or actually I have to rephrase it there
are no conversation for offsets so we
did counteract jittery gaze movements
using some filter techniques then the
gaze directed oh okay now here's a slide
and
gaze of manual input just to show you
again so this was a magic technique
where you would use your rough and test
case input to indicate a rough area of
interest then you move the mouse your
gaze your cursor would jump to the gates
location and then you can refine the
input using mouse input and there have
been other words for example kind of a
sensitive mouse which already deterred
detects when you're touching the mouse
similar to our approach where we touch
our smartphone to know okay now the user
wants to interact now the cursor would
jump to the gays location and then it
can be refined because here in the first
approach there were some problems is
over shooting because you would look at
a certain locations and the movement of
the mouse would initiate the
repositioning of the cursor so now to
our techniques the guys directed zoom
lenses we investigated different zoom
lens behaviors first the zoom lens
wouldn't be visible all the time to not
distract the user too much so once you
touch your smartphone the lens would
appear you can configure the zoom lens
for example zooming in further or
increasing the size of the zoom lens
using simple sliding gestures up and
down or for the size to the left and
right and then we had these different
behaviors one is i slaved so that the
lens would always follow the eye gaze
around so that essentially the gays
curve is always at the center of the
lens this of course can be a bit
overwhelming if you have a bit jittery
movement and although we filtered it out
you never really get rid totally or that
and on the other side we had a semi
fixed zoom lens behavior which would be
stabilized or fixed to a certain
locations as long as you would look into
that kind of proximity and once you look
outside the lands would move around with
your eye gaze
so just to show you an example so you
would activate the zoom lens and you ya
can then move the lens using your I
guess I mean you cannot really see it we
use the very very subtle cursor here to
indicate your current gaze position
because usually it is advised to not
show the case cursor at all but actually
I'm I would always show the user at
least a saddle feedback about the gays
location so then we also had magic
techniques one is a touch enhance gaze
cursor which takes benefit of past yet
Empress eyes gaze input and you use the
touch input to refine the cursor
position so one approach was that you
have kind of this reference circle so
the circle indicates the rough area of
your interest visual interest and then
you can then you have a reference circle
on your smartphone and you can just tap
quickly to then directly place the
cursor accordingly or you can do
relative cursor movements as similar as
on your on your laptop using your
touchpad and here's an example of a bed
so you touch a kind of a circle here
peers and you can quickly tap the cursor
to the location you want to have it or
you use the relative positioning on your
touch on your smartphone then we also
had the magic tap technique again with
this rough area of interest and you can
simply tap through go through a list of
target candidates which are ya assembled
based on which target on this or
intersect with this circle and then you
can just go through this list by
performing a simple horizontal sliding
gesture which turn out to be pretty
useful for going through
discrete targets and ah here's an
example so you perform a simple
horizontal siding just showing can go
through the list of targets I mean
here's the targets are pretty large you
wouldn't have needed it but just so that
you get the idea so we evaluated these
techniques who is 24 participants and
also evaluated that for very different
conditions so we had large targets we
had them in a grid we had overlapping
targets with really really really small
targets words difficult to the task
always was to select the yellow block
here and here you can hardly see it so
this was kind of the extreme how can we
still use gas and food to select
something like that and the third
tassinari was kind of a desktop mock-up
and in our evaluation we also assessed
task completion time and error rates but
really wanted to find out what works
well for users so where do they see
shortcomings and what about the general
usable usability how do they assess that
and of course you can find the details
at our paper from KY 2012 but just to
give you a very very rough summary of
our yeah conclusion here so the although
the gaze directed cursor as I have
mentioned earlier didn't have any
compensation compensations for offsets
and so where it was pretty much
impractical for selecting small targets
it was still assess very easy and
effortless for large targets and a lot
of times users tried to just look at a
target and confirm the selection a
couple of times instead of using maybe
zoom lens or to perform some manual fine
positioning on the other hand the gaze
directed zoom lenses were also or both
manual time positioning in guess erected
zoom lenses were found flexible simple
and fast so people could cope with them
however and the gas directed zoom lenses
once you zoomed in very much four very
tiny targets it showed to be only useful
kind of in a limited way because people
would kind of get lost they didn't
really know where they were and it was
partly distracting due to the visual
clutter whereas the menu fine
positioning was in general SS very
positively we we had kind of a problem
with the implementation there was kind
of an overshooting of the touch input
but this was simply due to a lag in the
communication with using the smartphone
as a computer and the the interesting
part is actually that little magic touch
was practical even for very tiny targets
and so if you only have the task of
selecting something kind of such a
simple way to go through the targets was
seemed very yeah useful however we
oughta thought about well usually you do
not just select targets right I mean you
would select and/or then usually you
would go on maybe reposition a target
maybe also manipulated resize rotate the
target so how do the techniques apply to
that how can we use for example magic
tap in such a way and here it is
actually not so practical because it is
very difficult to find position the
target on a white canvas using kind of
such a technique then so we investigated
this further elaborated a set of very
simple touch input again where you don't
need to actually look at the smartphone
to interact with the distant display and
for this we deliberately again wanted to
support not only selecting large targets
and not only positioned them roughly but
also what about small targets and fine
positioning them a very deliberate
outlining lay outing for example of
targets and also what about different
transitions so on the one hand I could
distinctly select the target maybe
resize it and later on reposition the
targets or what about kind of a trek and
drop how can I support this using gaze
input and for this we considered kind of
a trek and drop approach where we have
defined kind of this proximity around
the target so while you look within this
proximity that target would not be
repositioned just be selected so that
you can address in precise kind of
jittery gaze movements and that the
target wouldn't immediately jitter
around with your eye gaze and only when
you kind of leave this proximity you
would start this track and crop process
also for the zooming lens we notice that
only using your gaze to then reposition
something using zoom england's is we run
into problems there because of the
double role of our i guess users wanted
to observe is whether an object would
already be correctly aligned and then
accidentally reposition the target and
so to address this double role of the
eye gaze we use actually a hetero acted
zoom lens so we could still kind of take
benefit of the rough visual field of the
user but lift the problem with a double
role but on the other hand as a short
coming it is more explicit and what
sometimes Eden described as a bit
straining from the users so and here's
just an overview of the different
different interaction said that we have
developed where you can see or take this
one the distinct so you can just live
first you can adjust the zoom lens the
head director zoom lens using up and
down and left and right touch gestures
to increase the size of the zoom lens to
zoom in further
then you can just look at a target
double tab then you have selected the
target and look somewhere else and
reposition the target by double tapping
at the destination again whereas you can
also just look at the target touch your
phone look somewhere else initiate this
track and drop procedure and then just
release the the target at your desired
destination so so here to support the
precise selection and positioning you
would have to zoom in at the beginning
whereas with a touch enhanced gaze point
you can of course still refine using the
cursor using your touch input so he was
or he's an example from our setup where
the user actually had a head-mounted I
tracker and had to interact with the
distant display what for that we use the
Arrington research point view I tracker
we also had to help with track of course
the head so we use an opti direct system
for that and used samsung galaxy two
then yeah as a smartphone so here's an
example of that he is ahead directed
zoom lens which you can see up here is
bit difficult to see but you can just
look around and initiate different
rotations and translations than using
your smartphone by putting it to the
side and then resizing it for example
so with this if we could support with
the multimodal gate supported
combination we could support different
input conditions fine and coarse
selection and positioning and also the
different transitions between distinct
and seamless selection positioning the
combination of gays and touch for this
task and we assessed as practical a
faultless influent the yeah some some
kind of conclusions we had for the
touching hands gaze pointer was it it is
very fast but it took users time to get
used to because we had a well of course
a couple of participants who haven't
used I tracking before for interaction
and so it was kind of overwhelming for
them at first for the head directed zoom
lens this was less overwhelming or at
least users described it as less or less
overwhelming but also was described as a
bit more unnatural for precise
positioning using this lens I would have
preferred to do some manual
repositioning so summary and outlook so
in my work I developed and investigated
of course novel gates based interaction
techniques and we have carefully
designed new ways for convenient gates
interactions as it has been lots and
lots of work on using gates for
interaction but not so much for
convenient use rather to allow people
with disabilities of for example to
actually interact with human computers
with fork human-computer deccan sorry
then gate supported interaction we
really stressed this term gates
supported because we really think
there's a high potential for using gaze
as a supporting input modality and
that's why i put a particular emphasis
on this they use it in a more subtle way
and also to take advantage of tests
implicit yet in precise case and put
finally also the mod
the moral aspect so we thoroughly ex in
mind several different combinations not
all I couldn't discuss all now but if
you have any questions later you can ask
and for us however a very very
interesting combination was the
combination was gaze and put and
ubiquitous smartphones especially kind
of having this scenario mind you just
stop stepping up in front of a large
display a public display for example and
can start interacting with that and also
the proposed principle of case suggests
and touch confirms whereby the
additional modality in this case touch
not only confirms but also of course
refines rough input and also allows for
additional controls such as rotating a
target or resizing it so my work was
limited in the way that there are of
course a lot of different additional
investigations that you can pursue for
multimodal gate supported interaction so
we have focused mainly on basic
interaction task navigation and
selection and positioning but there are
a lot of interesting additional research
questions for example what about the
practicality of gaze interaction in
everyday context also in an office
scenario but kind of for prolonged use
yeah for prolonged use also what about
gates based interaction for multiple
users or for example for collaboration
scenarios and as a more passive means of
interaction so for attentive user
interfaces that as I've mentioned
earlier the devices are actually aware
that you're looking at them for example
this is pretty interesting also for
human-robot interaction then contact
sensitive information adaptations this
is interesting for multiple display
setups or also for very large screens
where you may have a different distance
to the screen and certain details may be
only reveal to you once you get closer
and kind of the fine details and finally
also always a very interesting area are
novel gay spaced
mechanics because you can play around
with a lot of very creative and new
ideas and you're just a few more
examples that of course you can also
consider further or additional
multimodal combinations so that it was
also kind of a limitation of my word
couldn't look at all of them so a very
interesting combination would be of
course to combine it with praying user
interfaces also with hand gestures in
front of a display of kind of proxy
makes location and this orientation of a
user towards the display and speech
input and also there are various
application domain that could benefit
from subtle and seamless gate supported
input one is of course our scenarios
where the users hands are already busy
so they have to find other ways to
interact with a system also for kind of
these very multi display set up where
you may want to attract the attention of
the user want to lead kind of subtle
gaze aids that you lead the user to
certain information and an area which I
also find very interesting are of course
augmented reality setups and how to
interact with them using for example
your eye gaze so thank you for your
attention lays in the other stones you
did did you always assume that you take
the user and you calibrate the eye
tracking for them beforehand or are you
able to avoid the whole calibration and
waits by using an additional device to
confirmation environment etc because I
tell voice I tracking that I
participated in it's the constant
calibration every time
down a new user or just walking you sit
down the chain you move the chair can
you recalibrate to get anything can you
can just get rid of that calibration I'm
not sure we can totally get rid of the
calibration for our scenarios we just
assumed that the calibrations would
improve and that for some setups you may
not require totally precise I tracking
so there have been studies where you
just use the very rough gaze estimation
and this would be sufficient for
interaction and also actually an idea
that we had was of course you have kind
of these intra personal characteristics
of your eye gaze so you could actually
with your iphone or your your smartphone
could authenticate yourself walking up
in front of the display then it just
grabs this information so that you can
interact more conveniently with it if
you don't want to share your information
then it's more roughly or something like
that so there are kind of ideas to to
address this but it wasn't focus of the
work yeah very thing I think yes mr.
fuel work is related to large displays
rather than just conventional
workstations yeah mention of
workstations frankly touch input is
probably going to work best so I was
wondering what you were using is the
control case that you are making the
comparisons again so when you test it
gazing what was the control system do
you have a view as to what the standard
method is for interacting with large
displays against which you were
comparing your techniques we actually
didn't compare it to techniques for
large displays because the eye tracker
so there was kind of the main scenario
we had to mind that at some point we
could interact with large displays but
the eye trackers that we mainly used
where in combination with some desktop
screens so what we their use was simply
mouse input but what we were actually
more interested in was not so much the
the performance in terms of speed or
something like that but
how do people like how do people like it
where can we improve the techniques
because we are not at a point where
there are already totally optimized and
there are of course certain issues even
with these works where we see
improvement or where we see things that
could be further improved and once we
get to that point I would compare it
kind of to a strong base line or
something like that but until then it
was rather really finding out okay what
do we have to consider for the design of
such techniques yeah for example what
immediately came to my mind after I'm
afraid and perhaps what you would
describe as prejudice but that's
episodes on the basis of experience like
I've tried to do this and fail the
number of times so I'm always impressed
but I wondered if you just tried taking
the touch input on the smartphone
without the eye gaze and use that as a
comparison and but how do you so no
pointing at all Oh what do you mean or
something on the smartphone even if the
person is holding a smartphone I just
have a sneaking suspicion that the eye
gaze in addition to the smartphone is
probably slower than just some cunningly
devised interface on the smartphone
alone showing the same image that's on
the large two spoke not necessarily know
just using it as a touch tablet of some
sort I see yeah so and we actually tried
that for the head director for the last
project that I have described so you
could actually don't use your gaze
pointing at all and immediately start
just using touch input if you don't want
it to use your eye gaze but the thing
was that if you have a large display I
mean you could do some some kind of kind
of speeding up so that you need some
clutching and sometimes people would
rather like to look at it and do a small
movement on the tablet instead of
clutching the curls until it's at the
right position so there was a difference
and actually worked pretty well so there
wasn't so much an issue
and using the exam is impossible yeah we
tried that but unfortunately this wasn't
assessed so well because there were a
lot of Midas touch was the smartphone
actually so that people would I mean
they they already tilled their phone and
then I'm involved in unintended Lee they
would they move something because we use
that for the depending and then people
so have and I didn't want to pan and
something like that so it's a bit tricky
just your size in the kids either we
found we build some what we've done this
is that the magic tab system which is
one thing to go that was it was quite
popular with your users was it was a it
was a it was comparable like to use it
assessment was comparable to the magic
touch but the performance was so much
better with a magic tab you did it once
the as soon as you started tabbing was
the gays still use like did you sort of
fix the gates point and then tab there
or was a sort of a sort of continual
updating of the tab region as you would
your eyes and it was fixed once you
touch the phone your gaze and put us off
and you can just continue have yeah yeah
exactly there's a moment you touched
this case position would be used and
then yeah although it seems another
application is you know is so choosing a
a so hyper haha honest own webpage that
the magic have been pretty good for that
to you yeah seguin did you look at that
application we didn't look at that
application but is it is a good scenario
where I would see definitely yeah that
it would be useful no but we didn't try
it out um so our eyes already have a
magic zoom in them right there's a pho
v8 in center region so if I look at the
word V in vielen dank I really really
can't read the
you I might think I can but I can't so
if you had perfect gaze tracking and
people in the 70s used to implement this
by gluing a needle to the eye and then
putting it a picture on the end of the
label you should not notice the magic
zoom at all you should you should be
able to just enhance foliation I think
but there are certain limitations to
that I mean if you have a really tiny
target let's say I don't know the eye of
the awesome picture here like that then
it's difficult I mean what do you
precisely want to select so it's similar
to the touch input I mean it is it was a
rough what is it rough because I guess
what I'm getting is it rough because our
technology is bad I mean I feel no no
it's not off the auto I really feel like
I'm focused on that I I don't think i'm
generating am I really judgment you're
jittering first so there are micro
seconds but forfor some diseases you
actually can notice the jitter more but
usually like normal healthy human being
you don't see that and actually the
fovea vision is still in precise so if
I'm stretching out my arm the focus area
it's actually the size of my thumbnail
so now imagine you're sitting there and
there's a display and it is kind of a
cone thing not as much there is a
certain distance so you cannot see
something totally precise at a very
large distance so there are a certain
aspects you have to consider for that
and it's actually similar to kind of
hand based pointing or something like
that if there's also kind of digital
small movements at a distance make a big
influence of course and it's similar
with the eyes so there are kind of so 2
2 aspects so that one of the
physiological aspects of your eyes and
another one is really the tracking
itself how big is the microscope age in
pixels on this screen say again I think
it's a micro circuit
I can I don't have numbers for that I
don't know I was just wondering if you
would you combine the gauge with the
touch like this is there still a place
for gays only control mechanism so you
know what to do I only activations so
for instance what's a sliding point
along a slider to activate it or
something like that um I'm just
wondering if there was a if you see
where we going as the mixture of gays
and touch but will there still be some
like buttons that you press with your
eyes would that you activate oh we
didn't use it and I mean there you could
imagine some combinations for example if
you look at certain information and
maybe there's some some additional
informations and kind of shown when you
look for prolonged time at it but we
didn't really use kind of this
combination of case only in multimodal
is there was a without does that when if
they were alternatives for the hover
interaction but what's it that's for
different papers hover interaction yeah
no we didn't I mean we did it in a way
that when you were looking around like
in the selection scenario things would
highlight if you look over them but
there was kind of it we really wanted to
leave it at a sabul interaction and that
is modeling gaze at the points rather
than has a code that can change at
radius i'm wondering if you use the
radius in any of the interactions Jamie
didn't I I was thinking about it also
kind of the distance to the screen and
so on but actually that we didn't do you
said but it would be interesting maybe
to look into that further</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>