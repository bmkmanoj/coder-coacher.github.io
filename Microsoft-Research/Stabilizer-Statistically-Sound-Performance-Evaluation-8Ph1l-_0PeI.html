<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Stabilizer: Statistically Sound Performance Evaluation | Coder Coacher - Coaching Coders</title><meta content="Stabilizer: Statistically Sound Performance Evaluation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Stabilizer: Statistically Sound Performance Evaluation</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8Ph1l-_0PeI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
all right well welcome everyone it's a
great pleasure to again introduce Emory
burger from the University of
Massachusetts Emery is here for a month
and he's still here for another week so
if you want to talk to him you're
welcome to Emory's done a lot of
interesting working systems we heard
last time about Otto man and this time
we're going to hear about stabilizer
well which is a very creative way to
slow your program down but make it
better thanks Henry thanks yeah it turns
out thanks man yeah it's that was great
uh yeah it's it's a lot easier to slow
down programs and to speed him up so so
I have a rich career ahead of me all
right so uh so this is some work on
performance evaluation that just
recently appeared at ass boss I actually
could not attend ass boss this year
because I was sadly in Rome so I was
Rome Houston um but this is joint work
with with yeah right exactly um so so my
student charlie is the the lead grad
student on this work the only grad
student on this work and he presented
for me so and he I here did a great job
so hopefully I'll do as well um so uh
you know we I think most of us in this
room actually care about performance
right a lot of people in general I think
when you write your program you think or
your optimization or your system you say
well we really would like to show that
in fact speeds things up not slowing
things down right so the problem is it
turns out that largely this isn't really
necessarily everybody although I include
myself here many of us have been doing
it wrong all right so we've been doing
performance evaluation for years and
years and years and we actually have
been making some crucial errors in
particular things that we think are best
practice like running things repeatedly
and getting error bars and showing all
these very pretty graphs that's not
actually enough and the reason is that
there are things that really are lying
under the covers that we're not actually
seeing that have a huge impact and they
have such a huge impact that it actually
makes the results that we're reporting
potentially meaningless
so so what do I mean so in particular
there's this problem on modern systems
which is that the layout of the objects
in memory the objects include your data
and your code dramatically affect
performance all right and even very very
slight things that you might do to your
program actually have a pretty serious
impact on layout right so if you change
some of your code it moves things around
if you change some allocations and so on
it moves things around and right now
when you run your program a bunch of
times there's really no way to isolate
this effect and this effect is super
super important so the goal of
stabilizer is to eliminate this effect
all right and by eliminating this effect
it's going to enable us to do we call
sound performance evaluation and I'm
going to explain what that means all
right at not only am I going to explain
what it means I'm going to show you some
case studies that demonstrate the value
of using stabilizer and taking this sort
of sound performance evaluation approach
okay this all right just one second
finish a finish up the slide so the case
study yeah really the case study i'm
going to present is an evaluation of lvm
s optimizations and so i'll show you
what the impact is of using stabilizer
and how it lets you understand things
differently catherine go how what
happens if your performance optimization
is to chase a layout in order to prove
it for ya so yeah that doesn't
randomizing layout like defeats the
purpose of what you're trying to do yeah
so I'm actually going to talk about that
a little bit later but it is true that
if your goal is to do something that
affects every single aspect of memory
layout not just the data placement but
also the code and also Global's and also
the stack frames then stabilizer would
be undoing those things however if it
only attacks one which would be normal
like typically data for example or
possibly code then stabilizer allows you
to isolate all the other effects have a
base so I'll derail you just
more short of a system that does code
layout which is trying to give you good
cash mccalley right but then you do with
take a layout optimization and so do you
want to screw up the finely tuned code
layout as well to see if your data
layout is independent of that like
that's the key thing so so here's the
thing so first I think it is you know
that there's there's this fear of
optimizations like the ones you're
describing right no no don't know I
understand it I'm not saying that
they're scary although you know then
there's many many other things that
people do with their code right so they
go and they manually try to optimize
their code or they're writing their code
they add features and the features make
the program go slower right so this is a
performance regression right so the
problem is that you run your program and
we'll get to this in a second where you
run your program and you see some
performance change and you draw a
conclusion and that conclusion may be
completely unfounded because of all of
these other confounding factors right so
these confounding factors are all of
these artifacts of running on real
systems today when it comes to very very
precise things that actually depend on
memory layout the question is does it
depend on every single aspect of the
memory layout or not things like stack
frames typically are not subject to
these kind of optimizations it depends
code placement definitely heap object
placement moon's iffy right so there are
a number of these factors that we can
you can sort of say all right I really
care about these factors whole don't
change those but change everything else
all right ok so I'm going to go ahead
and focus first on this issue of memory
layout and how this affects performance
ok so of course some of you are aware of
this but that's fine so suppose I've got
a program so here's some chunks of code
and let's call this program a and then
what I'm going to do is I'm going to
actually make some modifications right
so I move some stuff around I refactor
some things I modify some functions and
now I call this new program program
on alright so the question is so say I
meant to do this to make it faster all
right like I was like oh wait there's
something here I can really speed up I
can make this go way way faster okay so
what do I do so I run it right the green
one is is the new one the blue one is
the old one and it's a little hard to
see the numbers here this one says it
took 90 seconds and this took 80 7.5
seconds all right so which one's faster
okay so a prime is faster what's that no
I yeah you have no idea man see all
right so this is the problem right
everybody's like it's a trick question
much faster uh I'm scared all right so
this is really how we formulate this
right we say is a prime faster than a
and you look at the time and you look at
the bars you say oh this bar is to the
left of the other bar fantastic okay so
clearly a prime is faster and in fact in
this case the difference is two point
eight percent faster all right so you
say okay two-point-eight percent faster
all right but what's the first obvious
objection to this one run one run okay I
ran it one time so the question is what
about variants what if there's some sort
of noise in the system is
two-point-eight percent really something
that overcomes that noise so now I run
it 30 times okay 30 is kind of a magical
number it turns out for many of these
studies so we go ahead and we run 30
times and now we look at the variance
and the variance is pretty tight
everything looks good looks like a prime
is faster in fact the means not just the
means but the extremes the max of one
and the men of the other still
two-point-eight percent difference all
right where's awesome so here yeah so
this is just there's actually three bars
here and two bars here but this is just
the distribution empirical distribution
of running this code yeah that's right
just happen to be away it's really just
an artifact of binning okay so why is a
prime faster than a of course it's
because I'm a genius programmer and I
made a two point eight percent faster
after three weeks of effort right
terrific so it could be
either code change and that's what we
generally assume right like I made this
change and good things happen but it
could just as easily have been the new
layout right I made a bunch of changes
here moving around functions changing
the size of functions the head of
significant impact on where these things
end up in memory and one of your
colleagues Todd myth which back in 2009
presented a paper talking about how
layout biases measurement all right so
what kind of things can you do and this
is what Todd showed that can have
dramatic impacts on on the the
performance your program without
changing code so you can go into your
make file and change the link order so
just changing the dot OS like Fudo
bardot or bardot Fudo can have a
substantial impact on on layout right it
moves all of the functions from one
place to another environment variable
size this one is completely bizarre so
it turns out that when you run AC
program it actually goes and copies in
your entire environment and then starts
the program after that all right and
that means that changing your
environment and changing your
environment can mean changing your
current working directory the time of
day the date all of those things
actually move all of your code in memory
so you might think all right come on
we're moving the stock a little C
component the manager runtime has the
same problem not just the secret that's
true actually so catherine has a good
point it goes even further because not
only is Java or.net actually a sea
program it turns out that the the
runtimes actually do import the
environment and they import it into your
heap right and a lot of the code gets
compiled on to the heat which means that
your code placement and your data
placement are still affected by these
environmental factors all right so you
might think all right come on how much
of a big deal is this going to be well
it turns out add as Todd and armor
showed and Peter Sweeney was on this
paper as well that the impact of these
changes can be greater than the impact
of dash 0 3 and these are very very
coarse-grained changes right this is
moving all of the data
just on block or taking a whole bunch of
functions and moving them separately the
changes that you make is an individual
you're sort of very very fine grain
changes can have much more deep impact
so what's going on so the problem here
is the cash right so you have these
caches caches are awesome they make your
programs run faster right you have these
very very fast memory but it's
relatively small and in some cases you
can end up with some sort of
catastrophic collisions in the cash
right so you go and you have your code
here these two pieces of code are hot so
they end up unfortunately mapping to the
same column cash set and you get
conflicts so they don't all fit in cash
at the same time it runs slower when you
change your program by luck you could
end up actually getting an empty space
in that set right so the code is not
there anymore the hot code has moved
elsewhere and now we have no conflict
alright so this can happen because of
caches caches aren't the only source of
this sort of problem branch predictors
suffer from the same sort of phenomenon
branch predictors depend on program
counter program counters to sort of key
into hash tables then there's TLB right
translation lookaside buffer same thing
its address based the branch target
predictor I could fill up the slide
basically with all of these things that
use essentially addresses as hash
functions so really anything in your
hardware that has a hash function
suffers from this problem yes these are
free association so the TLB is fully
associative but it depends on where
things lie right so if you think about
it if I have a function that spans two
pages right then I would need two
entries but if it's all in one then I
would only need one entry right so all
you're right about the address not being
the issue but it's about sort of fixed
sized resources as well all right and
the placement of things in memory all
right so now that you've seen all this
if you hadn't seen before let's go back
and and think about this we're asking
this question is a prime faster than a
but now we know that they're all these
weird confounding factors so what do we
actually do in practice in practice we
essentially
go and we find the one layout right we
don't try all the environments we don't
try all the link orders we just ask one
person right so it's sort of like we ask
this guy all right and it's like hey it
seems to be faster right it's
two-point-eight percent faster we're
done right but here's something you
would never really do in practice right
you wouldn't go ask one person for their
opinion about something and say hey what
do you think is this faster you guess
yes yes yes yes yes yes right do it 30
times oh I have so much more confidence
that it's faster right so what you'd
really do you really want is you want a
sample you want to sample the space of
all of these things right all these
different layouts all these different
guys some are going to say it's slower
potentially some could say it's the same
but you really need to cover the space
to find out what reality is so there's
an objection that people often come up
with at this point which is hey I like
the faster guy okay the faster guy is my
friend I just want it to be faster now
this is not just like I wanted to be
faster because I want to publish a paper
right although that that there's that um
it's also you know look you ran it
faster it's like well I don't care about
these other layouts maybe I just want
the good layout I want the faster layout
so so what if we just sort of stick to
that layout we say you know this is a
damn good layout we like this layout
let's keep it all right so we're only
going to talk to Bob which means here
we're only going to use this layout okay
so that sounds good there are problems
with this approach suppose you upgrade
your C library you kind of have no
control over this by the way it's like
up you know time to update there's some
bug fix this happens all the time
Microsoft see runtime gets upgraded
whatever libraries that you depend on
get updated that changes your layout all
right if you change your username not
you changing your username but somebody
else uses it right that changes the size
of your environment right so I actually
have this experience myself with the
student where I proposed an optimization
and the student came back and said it
doesn't work it makes things slower and
said that can't be true
I ran the program and it ran faster he
ran the program it ran slower we ended
up running it on the same machine two
different windows and his was slower and
mine was faster and his username was
longer than mine that was the only
difference so if you changed the
environment variables then everything
worked out great so this is kind of
brittle let's say or everybody could
just canonicalize and have the same
length user name so we could do that
that's one possibility all right so so
yeah we should all exactly do a search
that's right for every program on earth
that's right I agree yeah well as you've
already heard it turns out my username
is optimal so yeah I have to make sure
that the program do does not so easy
right so so in fact that's not so hard
to do right so you can actually make it
so that it doesn't depend on that but
all of these other little factors like
you change one line of code in your
program you change a library right your
libraries are getting shifted out from
under you all the time there's all these
different versions of all these
different dll's and a slight change in
one it changes everybody's layout all
right the new directory is another
phenomenon again same thing with the
environment variables etc all right so
layout is really brittle and it's
brittle not just you know I'm running my
one program you know you've got the
whole execution environment you're going
to make modifications to your program
it's going to go through different
versions it's always changing so you
can't really stand on firm ground and
say I got a two point eight percent
performance improvement right if all
this stuff is happening all right so by
layout biases measurement great what do
we do about it so it's bad news can we
eliminate it and the answer is yes all
right and I'm going to show you how we
do it so stabilizers the system that we
built that directly addresses this
problem right memory layout affects
performance this makes it hard to
evaluate stabilizer eliminates the
effective layout and i'm going to show
you that by doing this you actually can
do what what is really sound performance
evaluation all right so what is
stabilizer
so as Catherine already spoiled the big
reveal here right so stabilizer
randomizes layout so what is it
randomized they randomizes pretty much
everything it can so it randomizes the
placement of functions randomizes stack
frames it randomizes heap allocations
and it actually doesn't do it just once
it repeatedly randomizes this layout
during execution and this turns out to
be crucial for reasons that will become
obvious later but basically when you do
this you eliminate the effect of layout
because you're sampling all of these
different spaces of layout randomly
right if it's completely random it can't
buy us the results okay so I'm going to
walk you through a few of the mechanisms
at a very high level you already know
that it's possible to randomize the heap
this is something that I've been doing
for for a little while now I'd be happy
to explain offline here's some of the
other stuff we do so with stack frames
we actually modify the compiler so that
instead of doing this usual thing where
you call function main then foo the
stack frames are always adjacent what
happens with stabilizer is it actually
adds these random pads and these random
pads mean that the stack frames start at
different positions okay um let's see
the when it comes to function placement
you have your basic function placement
that the compiler gives you where it
jams everything contiguously into memory
what we do in stabilizer is we install
trap instructions at the very top of
these functions and then we randomly
find places to put the functions we make
copies somewhere else randomly in memory
and we change the trap into a jump so if
you go to execute that particular piece
of code you'll actually end up executing
somewhere else we keep this little
relocation table off to the side because
all the functions that foo may reference
could also move so this will give us
their addresses so when we go and we
invoke bahs bahs gets moved somewhere
else and so on and so forth so when
you're running your program stabilizer
will actually go ahead and do re
randomization so re randomizing for the
stack is
pretty straightforward you just do
different random paths for functions
it's a little more subtle so the timer
goes off and it goes and it reinstalls
the traps in the original locations
these are the old randomized locations
these have to stay around they're sort
of potentially garbage but somebody
could be running in them right now they
could be on somebody stack so it goes
and it generates these new versions and
periodically after some actually during
this we randomized ation period
stabilizer walks the stack looking to
see if these things can be reclaimed yes
relocation table at these function why
do you need it because everyone
otherwise is pulling the original
location oh that's a good question so
I'm sorry I didn't make that clear so
when you're in this thing right here
initially it goes to the original
location and then it overwrites it and
jumps to the main location all the time
so it avoids an interaction it's just
for performance that's right yeah yeah I
need to change place from that anything
what can you do static meet once or
confirmation so that every time Yuri
come by so you have different versions
yep program and then yeah okay so um
there's one very sort of easy answer for
that and then there's the sort of deep
mathematically motivated answer the
doing it statically compile-time sort of
this is the weak answer experimentally
this is a pain in the ass right so if I
want to go ahead and test my
optimization I have to recompile all of
my code and every single one will
correspond to one sample so if I do it
this way then I actually get to sample a
large space but it turns out that
there's a statistical benefit that you
get which is really huge from doing this
rear and emission which I'm gonna get
you in just a couple slides okay so just
hold on hold that thought for a few
slides dan it's a one more hopefully
quick question yeah you'll get to it so
by again the relocation table you kind
of changed how food prime calls bar yep
because it won't be like this tyrant
call it will be now more indirect yeah
it really changes as we could
potentially affect branch prediction and
all kinds of things in the hardware
level you can change right right so um
the so when you do a jump the branch
predictor is already obviously if it's a
static jump then there's no branch
prediction necessary however if you do a
jump through an address then there is
actually a branch target predictor these
work very well so it's so I mean that
that part in terms of performance is not
really as much of a hit as you might
think and i'll get to spur portents
numbers very soon okay you do change
something yes we're changing something
yeah you're right miss you listen
initially but then if you're always
jumping to the same place for a while
Daniel hit right right right i think
that your objection is somehow that
we're changing the program overheads
that will swamp the other ones i think
that's a better well that's the real
problem that could happen but right so
the concern boy that's right that's
making everything slower has been right
so the concern that's it so the concern
that you're getting too is another
concern that i'm going to address which
is whether doing these changes actually
affects your analysis okay so obviously
i'm going to argue that it doesn't but
we'll get there alright so let's now
that we have this in place this whole
thing of and bear with me let's
everybody assume that this is all going
to work out great and it's going to be
reasonably fast and everything is going
to be fantastic i'm going to talk about
performance evaluation yes Mike
how do you know you address all the
things that create these problems right
right so so the question really is all
right so there's all of these different
confounding factors how do I know I've
addressed all of them so I haven't
addressed all of them so I can give you
an example of one that we have an
address which is that you could
randomize the census of branches for
example right we don't actually go into
functions and change anything about the
code inside a function right you could
imagine that there's some impact of
having things in relative locations
inside keep objects right inside stack
frames there's relative positions that
we aren't disrupted so there are things
we already know we aren't doing it turns
out that these you know I don't think
that we're covering the whole space we
understand the space we're covering and
we can address it up front which is a
huge advance over saying what over doing
nothing or saying here are a couple
selected factors that we account for
okay and one of the things that we
observe well you'll see what it does to
execution times and it gives us some
characteristic of execution times which
gives us very very strong reason to
believe we are covering things very well
okay all right okay so let's go back to
performance evaluation so we run our
things 30 times but now when we run them
because of randomization we have a bit
broader of a distribution okay so now is
a prime faster than a what do you think
how many people say yeah it's faster I
see a couple nods and smiles well bit
well this is oh you're saying you're
measuring this this chunk of the curve
here um it seems faster all right how
about now is it faster now people seem a
little more skeptical looks like it
about now all right now we're all like
I'm not so sure anymore so the problem
is is that this is not actually the way
that one should proceed to say these two
curves are different right it seems like
you know we all have kind of an eyeball
metric right we say if the curves are
really far apart then it's good and if
they're close
I'm a little less comfortable about it
but in fact this is not really the way
that people do things from a statistical
standpoint so so what people do it when
and this is what people the social
sciences do and people in biology and
all of these folks do they're faced with
these kinds of problems all the time
right so I have a drug does this drug
have a significant impact on this
disease or not right what they don't do
as I ball it ok they don't say seems
good right everybody use this drag right
so what they do is they adopt this
hypothesis testing approach alright so
the way a hypothesis test works is
actually quite different so we don't ask
the question are these two different
right or is this faster than the other
we say if we assume that they're the
same okay this is called the null
hypothesis assuming they are in fact
identical what is the likelihood that
you would actually observe this kind of
a speed-up due to random chance all
right that's the statistical approach so
it turns out that this sort of thing you
know often we make these assumptions of
normality in biology and in social
sciences and it's very easy to compute
what the likelihood is that you're going
to end up with this null hypothesis
appearing to be true by random chance
and you all have imagined seeing this
curve this is the classic normal
distribution and you say you know the
odds of being more than three standard
deviations away from the mean due to
random chance are less than point one
percent all right so by giving us this
situation so we do this this
randomization we do all the stabilizer
stuff now it's actually going to put us
into a position where you can ask the
question in exactly this way so what
we're going to ask is what is the
probability that we would observe this
much difference just randomly all right
and the argument that everybody makes
statistically is if this probability is
low enough for some definition of enough
right then we argue that the speed up is
real and because we've randomized the
layout we know that the speed
is in fact not due to memory layout okay
so there was this question before about
why not just use a static one-time
randomization right so what is re
randomization do for you so this is an
empirical result of execution times with
exactly one random layout / run and you
can see that it spans some space but
it's fairly uneven right so this is just
you know we start off we do a
randomization it's a one-time
randomization we run with it the whole
time okay here's what you get when you
do it many random layouts per run and
you can see that curve looks very very
different right it actually has you know
it's nice peak it has a tail it's it's
you know modal what's going on so
stabilizer do a stabilizer works it
generates a new random layout every half
second all right and if you think about
it what's happening you've got your
whole program and it's composed of a
bunch of these half second epochs your
total execution time is the sum of these
periods so it's the sum of a sufficient
number of independent identically
distributed random variables right those
are approximately normally distributed
this is the central limit theorem right
so doing this randomization repeatedly
actually means that you can use the
central limit theorem yes if we take
this program compile it and shoot it to
a customer right you're going to run on
some random configuration but that
remains
scene for the entire run of the program
right because what can generate is this
fact and strain runtime that is not
available to anybody else so I can't
ship with us is what you're saying I'm
assuming idiot yeah oh but you could I
think that's an independent question
okay is it identically distributed but
it you could you could design make
hardware that doesn't have are now
randomly you could leave you couldn't
yeah let's see if that that's not
happening okay um then then I would then
I would claim that I'm actually running
that is creating lots of static
configurations and then running each one
of them like 30 times and then doing the
sum of overall that if you do if you do
the sum over N I'm a large enough planet
configuration if you look you should
also get the same normal distribution
what is large enough for you I mean you
do it till you do you get this normal
distribution right ok I Center the
central limit theorem that's what we
feel so so what is your sign I think the
reason why you're seeing this is just
that you you're able to explore a lot
more random perfect great lot more
configurations if you change it
dynamically versus what you're doing
it's time that's right so I think you
could I think you're making an argument
for using stabilizer right so what
you're saying is boy you could get a lot
more experiments done a lot faster by
using stabilizer then by getting a
one-time randomization with a whole
program recompilation and doing that
over and over and over again no the
evaluation with i spend like 20 seconds
or 20 days that's not a
I think it's a question for a lot of get
the right okay Sherman right yeah so but
then what do you do with that result
it's I think that are you saying that
we're not going to use this to do the
hypothesis test or you are I'm just
saying that you know actually changing
having a standing confirmation as much
closer to what the customer is going to
be running so that's probably true man
but so so which one are we going to ship
but you can randomize I on the customer
but I his point is only that you can
eliminate some of this overhead because
you're doing the rear and manure and
that's closer to what the real the real
well so there's a question there's
overhead then there's randomization so I
thought when you were actually going to
argue when you first started talking was
it you're going to say I don't want a
curve right I don't want a curve I want
some point in the space right and I want
that space to be you know probably one
of these sort of extreme to the left
points in this face right my arguments
for the previous life the one with had
two bins and a scene nice normal curve
yeah yeah yes I didn't you seen this
just because you're not running and I'm
randomly oh yeah so let me be clear it's
not just the code that's being
randomized here right it's the code it's
the stack frames themselves and the heap
object like other heap allocations are
actually quite randomized it's not as
randomized as diehard so diehard has
this very very strong randomization
property we actually install what we
call a shuffling layer on top of a
conventional allocator and it generates
a lot of entropy so there's actually a
lot of randomization happening here so
at in the dataspace and in the code
space but would you get maybe a
different distribution if you did
mother's think that you did your son so
if you just randomize the code you won't
get as much of a sample of all of the
layouts right because you would be still
sticking with the same memory allocation
you'd still be sticking with the same
stack frame right so this does more
extensive randomization than just code
something you really good it is just
this is an empirical result how many
runs are in blue this is 30 for each
okay but maybe mod this point is just
that unless you should stabilize her
unless you sugarcoat the stabilizer it
may be that so many of those points are
coming from places where you're going to
have you're going to have layout that
didn't course that would never
correspond to any new layout because the
real layouts don't have these paddings
the really odd stone you know pack in
this it's somebody could run well of
course it's possible right because the
randomization can lead to anything
that's available through randomization
is also a possible real estate right
it's just maybe a low probability state
but it's randomly exploring all of the
state space right but I think that your
real estates always have some padding
between zinda staff well but the pattern
can be 0 but for example you put in new
libraries the linker changes the layout
so so at the at the deployment site like
you could try to explore and move
yourself into the right proportion of
the distribution space into something
that's unlikely above unlikely to get
randomly but good performance right so
you could try to do some optimizations
that push you over there right but if
you're just shipping something you have
to say oh I could be anyplace in this
curve rather than the place I I had with
my one testing because that's just one
thing right and remember so what the you
know I think that's the most interesting
thing I liked you we'd like to say we're
always over there on the yeah like that
we'd always like to be here or so okay
terms of performance and that's the
distribution over time right but but you
can't guarantee yeah and I yeah and it
turns out as as will show that in fact
there are many cases where throwing
stabilizer into the mix because of the
randomization actually turned out to
improve performance so it improved
performance because the actual compiler
and whatever the memory allocator and
whatever the libraries and whatever the
state of the world was that was actually
less than the mean right left less I
mean it was higher than the mean
execution time and stabilizer tends to
regress towards the mean yes
the layout as your animal in performance
of each okay yeah so uh so this is
something that my student charlie is
actually working on right now so charlie
is in fact doing more or less what
Katherine described and what you are
sort of suggesting which is what if we
could deploy something that did this
randomization and actually deliberately
targeted the left extreme right and so
it can observe these things essentially
it's doing online experimentation and
then steering the execution that way
right but this is this is unpublished
work that's not not out yet um okay is
it submitted it is not in submission
right now yes
so everybody jump on board just like you
download stabilizer and beat us to the
punch actually I will say that using
stabilizer to do this because it does
add some overhead turns out to be not
the best approach so Charlie naturally
wrote his own dynamic binary
instrumentation system yes so good luck
with that that's not easy okay yeah yeah
but it has to be anyway so yes I know I
know I know all right so um okay so let
me go ahead and show you what happens
when we use stabilizer alright so we
went and we decided to try stabilizer
with spec alright and lvm and in
particular we wanted to understand was
what are the effect of these different
optimization levels so I think that most
people have a sense that optimization
certainly the layman thinks 01 pretty
good pretty fast right doesn't take long
to compile 02 takes longer to compile
but produces better code 03 takes a long
long time to compile produces somewhat
better code right there's a sense it's
not a linear increase in speed and
certainly not a linear increase in
compile time but that it does something
good so we wanted to see if that was in
fact true so we ran this across the spec
benchmark suite we did it on on each
benchmark and then we also did it across
the entire suite okay so the first thing
we did is we built the benchmarks with
stabilizer stabilizers a plug-in for lvm
you can invoke it just like an ordinary
c compiler it's szc if you run it like
this then it randomizes everything
however you can optionally choose one
thing that you want to randomize at a
time so this addresses Katherine's
concern what if you care about a
particular thing and not all over the
possible randomizations the default is
all of them are on so that corresponds
to code and heap and stack frame
randomization so it turns out that there
are good reasons not to to randomize
Global's and it's a pain in the neck but
that's right so it's actually a lot
harder problem to randomize so now we
run the benchmarks ok so we run them as
usual as usual we run them 30 times each
but this time we drop the results into R
so R is this statistics language all the
graphs you see in this presentation were
actually generated by our it produces
lovely love of the graphs and it's it is
the tool that statisticians and many
many social scientists and biologists
and so on used to do perform like
analysis statistical analysis all right
so we get this result all right is a
prime faster than a obviously this is
the wrong way to do things so we do the
null hypothesis construction we say if a
prime equals a right then we're going to
measure the probability of detecting a
difference at least this large so what's
the way that we do this in our we use
something called the student's t-test so
the student's t-test this is how you
invoke it in our pretty simple and it
allows you to say well if the
probability is low enough and the
standard threshold in and everywhere is
this arbitrary point of 5% you can
choose whatever you like so if this
probability is below five percent then
we reject the null hypothesis the null
hypothesis is that they're the same
alright so that's the name of the game
the name of the game is we're going to
try to reject with very very high
probability right high confidence I
should say that the null hypothesis is
true okay and what that means is that in
this case that whatever we're observing
is not due to random chance all right
okay in other words the difference is
real so here are some runtimes for dash
0 2 vs vs dash of 1 these are speed ups
on the y axis the x axis is ordered
increase in speed of all of the spec
benchmarks and you can see that there so
all of these things are green these
these are the error bars these are I
think actually don't recall the the
percentage around the error bars I think
it's one standard deviation all right so
all of these are statistically
significant and so you can
see that in some cases you get a
statistically significant huge increase
in performance that's for a star in some
cases here actually I can't see if it's
red or not I think that might be the one
that's not statistically significant not
too surprising right it's a very very
small difference but here we actually
get statistically significant
performance degradation from using dash
0 2 all right and it just turns out that
the layouts that these guys ended up
with were you know it's like well you
know there's this huge space they end up
in some layout that layout turns out to
be maybe it was lucky back when the
person did the implementation of dash 0
2 but it turns out not so good right now
okay cuz you have you you have some set
of benchmarks and sometimes you slow
them down right and find on average you
have 0 0 cash bar all right so yeah yeah
all right you could still choose to turn
this optimization on even though cases
BFF I see your point right right so your
argument is so the argument is is that
well all right across a sweet it's not
going to have this much impact right
maybe it doesn't improve everything
right some things it degrades right um
so I think that for dash 0 2 vs dash 0 1
this is pretty surprising because dash 0
2 is pretty bread-and-butter
optimizations I think that if you
presented the optimizations for dash 0 2
to almost anybody versus Dashiell one
you'd think of these are going to
improve performance and they don't yes
this is all with stabilizer okay on the
ground without stabilizers so one yeah
so I we actually ran this this
experiment i think i'm trying to
remember so i think that these speed ups
are actually speed ups mmm gonna have to
try to remember i would have to look at
the paper i can't remember if the speed
ups are with respect to the stabilizer
build or with respect to the actual
original execution we definitely observe
cases where these optimizations slow
things down and when you throw
stabilizer at them it makes them run
faster so it's it's a bit of a weird
issue but yeah I would have to check
I thought was much more yeah it's not
yeah it's not a huge amount i agree but
alright well let's go to 0 3 right we
can crank it up not one one machine yes
yes that's right who knows absolutely
bro of these and what machines look like
yeah yeah no so that's an excellent
point and that's something that's
totally out of reach for stabilizer so
you could do this with a simulator in
conjunction with stabilizer or something
like that but you can't like stabilizers
still observing the execution on your
actual machine and it's having this
effect of disrupting you know the memory
layout but you know if you have a
machine that has a 1 megabyte cash and
then you go to a machine has a 16
megabyte Micahel 3 the performance gonna
change dramatically right so and there's
nothing no way to account for that yeah
bug dash of 0 is actually 0 that's 0
does not ok ok josho one does some very
very simple things da so too does more
advanced things especially register
allocation as well as in line did you
measure today shows here um yeah dash of
0 it turns out well it's so slow that
measuring it for the entire suite would
take months so so we didn't do it yeah
it's bad ok so here's dash 0 3 vs dash 0
2 so I this is the same axis as before
so I'm going to make the axis 10 times
larger so we can actually see what the
differences are yeah yeah so anyway you
can see that these performance
differences are quite small now one of
the things that's interesting is this is
a very very small difference right but
because we're using stabilizer we can
actually say this is statistically
significant and this one is not and the
eyeball test you'd be like you know
especially this eyeball test like
totally insignificant right because it's
so small but that's not really how these
things work right so we actually get
these statistically significant
improvements and again some
statistically significant degradation
Zhaan the left it's kind of a wash in
the middle it's interesting that there's
this one which is appears quite large
but is actually not statistically
significant
okay all right so what do these results
mean so right now I've presented results
on a per benchmark basis okay but that
doesn't actually tell you what the
difference is between dash 0 2 and dash
0 3 because this is a point wise
comparison all right so what we actually
do is we need to go ahead and run all
these things 30 different times right
LBMA star etc etc we get a sequence of
graphs right so this is when we
aggregate them before what the results I
just showed you we were actually looking
at here's one benchmark compare it to
the benchmark with and without this
treatment I want to know something about
Oh 3 &amp;amp; 0 2 in general what I showed you
just now is actually not the way to do
it right this is sort of we want to know
right is 0 3 faster than 0 2 and you
know looks like it's slower here faster
here and you're like oh sometimes it's
good sometimes it's not but this is not
actually the way to do this all right
it's again we have to go back to this
null hypothesis treatment approach right
we say if these two were equal what
would be the likelihood of measuring
these differences all right and to do
this there's a generalization of the
student's t-test for more than one thing
and it's called there's actually a whole
family of these tests its analysis of
variance all right so you can again
invoke it with our in the beautiful our
syntax as you see there and you get the
same sort of results you can say if the
p-value is below five percent then we
reject the null hypothesis okay so we're
going to go ahead and say you know that
these things are in fact different so
when we compare oh 3 and 0 2 we get a
p-value of twenty six point four percent
we wanted it to be below five that means
that we're only 73.6 percent confident
that these results were not due to
random chance in in other words one in
four experiments will show an effect
that doesn't exist okay so this is a
classic feeling of the null hypothesis
that is you can't reject the null
hypothesis that dash 0 3 and a show two
are the same that all the effects you
observe for due to randomness okay and
you know colloquially we say the effect
of
Oh 3 vs 02 is indistinguishable from
noise yes I push and you can Donnie
processes but I didn't see you a case
when how the cutoff point that actually
tells you what a sensitivity or a
specificity of the 11 test and water
bodies because I'm a talking about the
effect size is that beg the question
well is the Italian point this sounded
kind of fun when you have two graphs
idea where the meat is a cut-off point
for that colorful is very important it
tells you by moving the pariah that's
you it tells you if your test is really
sensitive enough you this nervous
I didn't see you addressing be one so
how I agree yeah you got the same but
the naughty x sub 2 because the P
you are choose me of course it's a
problem but I mean so so the choice of a
p-value is always I mean this is just
the way it works right you pick some p
value theoretically the way this whole
thing is supposed to proceed is you pick
a sample size in advance you pick a
p-value in advance and then you go when
you do your test right the in fact this
like presentation of the p value here of
twenty six point four percent you don't
use that value all you do is you say we
can't reject the null hypothesis right
so that that is the sort of standard
statistical methodology that I'm
employing yep great so you you know much
more about this than I do right you've
probably forgotten more about it than
I've ever learned I had just happy I be
I'm more than happy to talk to you about
this app does up okay okay so there is
this concern about this is not really a
probe effect but it's an effect of using
a tool is there something about
stabilizer that's hiding the effect
right so what is there some systematic
bias and stabilizer that's changing the
effect that we're observing and so it
turns out that you know we observe all
of these speed ups when we run it with
dash 0 302001 or dash 0 0 and stabilizer
actually independently if the
randomization it employs adds the same
sort of impact to all of them so it has
a fixed additive increase in the number
of cycles the number of management the
number of branches taken it's the the
length of the path etc etc all right it
clearly disrupts the memory layout but
that's the point but everywhere every
other point in this space where we're
counting cycles or accounting reads and
writes and so on it stays constant so we
end up getting a basically say the same
sort of additive increase claimants
the same additive increase so if one
optimization say is in one one of your
big points is about jumping to the you
know stack frame for that method we're
not doing inside of method so oh it was
the question right right so do we
actually prevent certain optimizations
on happening so we don't because this
actually happens after all the
optimizations it's an excellent question
so all of this stuff that happens to the
code is post optimizations it doesn't
change the in London's right all the
inline decisions all of the optimization
decisions have happened and then it goes
and it does this there's no method
called there's fewer if they've inline
the methods then they're not jumping to
the method so they're not experiencing
the randomization of you that's what
that part is true so then I'm just not
sure about that either constant across
the different so the question so the
issue really is if we take the code
that's been produced by llvm is
stabilizer doing something to it that's
disrupting the optimizations and the
answer is no because all of all of the
optimizations happen first and then it
goes and it does all this stuff to
instrument it with randomization the
point is valid in for inlining yeah you
but by performing in lighting you've now
reduce the potential insertion points
for steak that part is true so there's
less disruption that's right in with
each other that's right yep that's where
o.o with 03 agreed agreed one of the
good things is that if you go and you
actually observe the runtimes of all of
these executions I don't have these
grafts here but they're in the paper for
all but two of the cases if I recall
correctly we get normal distributions of
execution time so you can still do all
of your hypothesis testing the reason
that you might not get it is directly
related to this problem it's not
actually so much because of code so how
could you fail to get a normal
distribution well if you're not actually
getting any independent randomizations
then you'll get none so how can that
happen one giant mean that means no
randomization of any functions 11 well
there's a custom memory allocator that's
actually the big problem okay so some of
the spec benchmarks have custom memory
allocation many of them have customer
allocators and if all you do is spend
all of your time in one
int array allocated on the heap then you
actually can't randomize within it
that's the biggest problem luckily in
almost all the circumstances it doesn't
matter there's enough other allocation
enough other functions to obscure this
yeah but but ko ngayon opposition's like
hot code placement yes are going to be
totally disrupted by years that's right
i mean this goes back to after this
point right so in that case what you do
is you run this with without dash our
code so you say dash our data dash our
stacks any effect from her so else hello
to Rizzo tree because of me now don't
they don't actually do this so Microsoft
compilers I know Microsoft okay so yes
one of lvm is not right so so this is
just performance of stabilizer in some
cases it slows things down considerably
like I said it slows it down sort of
uniformly but it does slow things down
Thank You Catherine man pearl is a
disaster pearls it well the benchmark is
ridiculous in many ways but pearl is a
disaster because it has it's a giant
switch statement with function calls
that's all it does all right and so if
the functions are placed randomly then
they don't all fit in in the TLB that's
really what we see is we see TLB
pressure here so if we had a
hierarchical TLB we had a bigger TLB
these problems would go away but that's
the bulk of where that cost comes from
and most of them are low the average is
about ten percent
they lose them yeah so what happens is
is that all the functions are getting
laid out right there just check function
function function functions so they're
very compact right it is a purpose
already well no it's just an accident of
the way that the code gets laid out
right nobody randomizes code right so
the code is just there it might not be
an accident there fits in the glbtq
could learn oh there's okay so i highly
recommend you take a close look at the
pearl cup there's not that much
intelligence happening the same right
it's a classic interpreter design all
right that's right uh anytime you take
something that fits in a fixed size
stretcher and do anything to it you know
don't fit in the fix I'd structure and
TLPs haven't grown in years that's right
yeah what about cactus why I like okay I
totally expected pearl yeah and GCC to
be over there right so yeah so what's
happening here if I recall correctly
this one right here is actually the same
problem with the TLB the TLB is what
kills you but this is for the heap okay
alright so we're randomizing keith
locations all right i thought there was
an overhead pearl just for random die
hard yeah bunch of overhead into anyways
but again it's TLB right oh yeah so
every all of this overhead is basically
attributable TLP there's a second order
of fact which is like l3 mrs. but tell
because we need supervision so you
thought about that action but but we
decided not to to actually go that route
for mostly because linux makes it real
real real pain in the neck it's not like
you say I want to do super so you have
to do sort of if you boot into your
system and you boot into superpages
everything is fine if you want to
actually allocate chunks of memory in
superpages it's a mess it's really very
bad yeah so it looks like what you need
to do another automate design point we
can actually still have compact code
right randomized functions within that
yes so um so that's an interesting
observation so part of the problem here
is that we what we need to randomize so
we undoubtedly have too much
randomization okay and we can randomize
in a compact way it is possible I know
how to do it
we didn't do it one of the reasons is
what we actually need to randomize is a
certain range of addresses that are the
bits that are used in these hash
functions right and those are actually
not the low order bits right so they
leave off the low order bits they leave
off the very high order bits and they
grab some in the middle so it's very
important that you randomize those and
that is going to lead to things being on
different pages okay that's for the T of
you yes okay so um what does it mean it
just like you slow down forty percent
yeah I'm gonna measure 02 yep yep yeah I
think when people are being say that's
right yeah I think a regional London
it's really the one on each city this is
like such a huge effect so it has a huge
effect so what it allows you to so it's
important to understand that the effect
of stabilizer has that is the dilation
effect does not actually affect its
ability to discern very very small
differences all right it's just a
question of running it more times okay
so if you imagine like all I'm doing is
looking for signal to separate from the
noise so if there's a very very small
amount of signal I will be able to
discern it I just need to run more tests
so having the test itself be slower
doesn't actually alter the ability for
us to do statistically sound performance
evaluation now the result that you get I
mean obviously when you run stabilizer
you probably don't want to shift with
this right but that's a whole separate
question the question is we're trying to
understand whether these effects that we
see in performance differences are
affected or not and if both of them get
massively dilated that's okay even if
those those effects are very small if
they're consistent then will detect it
so your audience is the person who's
choose
Oh 203 and making the choice should I
spend the time you know to do 03 versus
a 2 is it worth it but it's not it's not
in some sense the end person who's
running so certainly not the end person
but I would say that the audience is
actually much broader than the people
who are running 0203 so my audiences are
the following one developers who ran
their code and their code seems to run a
little slower they think oh my god I got
a two percent performance regression
right well before you go chasing that
down right find out if that two percent
matters or it's going to go away
tomorrow when you modify some more code
all right so performance debugging route
and perform it certainly well both ways
are meaningful certainly if you're like
I want I have this crazy super
complicated way of doing something and
if i plug this into internet explorer
it'll make it run point five percent
faster but it's going to be a source of
bugs and maintenance nightmare and all
the sort of thing do I really want to do
this or not right is that meaningful you
can decide whether point five percent is
meaningful or not as an effect size but
you need to know if it's statistically
significant or not right then the other
audiences really researchers right so
researchers publish lots of papers on
performance optimizations of all stripes
not just compiler things right runtime
system stuff different algorithms
different lots of things and there's a
kind of you know well seems to work or
the number is large and you might say oh
if it's over twenty percent that's
clearly good enough but actually we see
larger than twenty percent swings just
by doing static like link time like
change the link order change the
environment variables right you can
easily get a 30-percent swing and
performance just with that right so it's
important that people when they go ahead
and produce these results that they know
that they're real but is it for Grandma
no it's not really for grandma right
grandma doesn't or grandpa I I
definitely happy to include both grandma
and grandpa I'm definitely ageism but
certainly my mother and my father have
no idea what's going on so uh so anyway
so so when the effect is so large which
effect of your optimization no no no
when when the effect of adding this tool
to your runtime is a large how do you
control for its variation like you of
course more runs right but but so it's
only source of variances its effect on
memory layout right it doesn't actually
do anything different
itself but it's the observer effect
right has has two parts to it right one
is because it's code and it's in the
runtime with your your program right
this is something that wouldn't have
been in the runtime so there's some
effects true Joe there's the null effect
where it gives you the same layout and
it has some performance impact right
have you tested that so if I stabilizer
and make it generate exactly when and do
before and then what's the what's the
cut there yeah so the overhead is again
the overhead is totally swamped by these
memory layout effects right there's
almost weight interest yet how much
overhead yes sir we we've measured that
I think that it's on the order of two
percent so this is exclusively from in
direction and actually Charlie has a
faster way of doing this that should
make it go away but taking the traps so
every time you take a trap ah then you
know you take some hit it's tied to the
frequency with which you do this this
relocation the longer your did your time
delay is the less you see it's there is
a small effect on straight line
execution of code without randomization
it's very very small that's see this we
let every okay I'll wrap it up well I
got to the punch line long time ago um
yeah tada the punch line is I know
you've been waiting and then the cat
says to the dog okay so anyway so memory
layout as I hope you all will agree
affect performance makes it difficult to
do performance evaluation stabilizer
controls for the effective layout right
it randomizes it this lets you do sound
performance evaluation and we did a case
study case study shows that in fact dash
03 does not have a statistically
significant impact across our benchmarks
and you can actually download this today
it's a stabilizer to lorg and I want to
thank the NSF and sorry guys only thank
Google for for helping to fund this
google also
his funding Charlie's PhD so he got a
PhD fellowship so anyway that's it
thanks for your attention I'll put up
one backup slide here so this is dash 02
vs dash of three yeah did you verify
that establishes of the scene for change
in my babyhood you mean the performance
you rather run it your student on sit in
his his name and under your name and you
get exactly the same distance so yeah so
so it does it does because the the start
and the position of everything really is
totally disrupted by all the
randomization have we verified it I
pretty damn sure we verified i should
say one of the things this is so this is
the the little secret about stabilizer
you know stabilizer gives you
performance predictability right now the
cost is too high but it gives you this
performance predictability and you can
argue look the chances of me being more
than three standard deviations away from
the mean are less than point one percent
right and so the fact that you get this
actually gives you very very predictable
performance so we'd be completely
shocking if you observed anything
different but it turns out that it's
immune so I'm pretty sure the paper
actually has an experiment to this event
yeah well I mean one thing that's
interesting is it goes to this question
which is now there there are changes in
the environment that and some will
affect the performance and some well so
the claim is that the environment very
will show the time to perform its life I
change a library maybe well maybe it
won't might make something faster right
but you can actually get right you can
you know evaluate that if I go to a
different piece of hardware you can look
at you know now is stabilized you can
say how do these compare and with
stabilizer you know compared to what
without Stabler so it seems like you
know you have a better way to understand
the real effect of the environment which
I know I be curious to see in terms of
like Catherine of this great work on
with me like 20 different harp
architecture things it's like about to
energy and things like that you want the
same thing with stabilizer to see how
much difference these are ya that's an
interesting point yeah i mean i would be
leery of doing this well it's an
interesting question i'm not sure
whether the energy impact of the memory
layouts i would have to imagine that
that would be substantial um and but you
could certainly detect whether there's a
difference I would have to think very
carefully though about what the meaning
would be of running your stabilizer
results on machine one and running it on
machine to write and comparing them
right you need to really have a pretty
clear null hypothesis so I guess the
null hypothesis would be that these
machines have no impact ah that's that
would be a very surprising normal for a
p3 p4 i mean you know i think the
pentium 4 wasn't clear that it was
really better right oh yeah sure so they
are I'm sure there are cases where you
could actually see ya you know
confirming right right I'll the trace
instruction cats are yeah go ahead tom
yeah um the other question I had was
you can adapt to your sampling are you
put your we where were you relay up yeah
and did you to reduce your overhead and
and if he's still got acceptable sort of
experimental results so have you looked
at that so I mean we deliberately chose
this number because we want to make sure
that we would guarantee that we got 30
samples for any reasonable executions
and so you do need to have some number
of samples to make using these
hypothesis test meaningful we are
looking at altering the the rate at
which we do these things based on
performance counters for this other tool
that charlie is working on yes so you
were you're randomizing layout related
stuff here and i can see because I
ashing that's a large but are there
other possible effects other than layout
that could affect you know performance
even with your small changes in your
head nod layout religious own on layout
related yeah um so I believe that there
are so this but it's it's tricky so I've
spoken to one of my colleagues Daniel
Jimenez who's an expert on branch
predictors and he says that in fact
there are other things that you might
want to do beyond just laying things out
they would actually alter the way that
the code behaves in the branch predictor
I'm sure there are many more very subtle
things but he he also thinks that those
things are second order that the caching
and the addresses themselves are going
to be where the big hit is the question
however of randomizing things within
objects so you have a function right or
you especially for a function and the
code right the relative position of them
because they all move together means
that there's a lot less randomization
happening so getting deeper inside would
be a way at getting you know add more
possible payouts
this topic you mentioned offhand Bailey
randomizing stuff with within a severely
which seems like it could be really
attractive look if you have an
optimization you want to do here code
and you think this is probably going to
speed things up because of instruction
scheduling maybe you'd like to factor
out cash geometry by randomizing that
Ryan sure linear opposition is giving
the speed up from the source thing to
thought it was sure sure so there's
there's actually some work I think it's
mark hill if i recall correctly it's
either mark healed or david would
somebody in wisconsin where they
actually do inject some randomization
into simulation to try to get some error
bars because otherwise everything is
exactly the same all the time but they
haven't done to my knowledge nobody has
done anything like this within a
simulator um you know it begs the other
question obviously about simulators yeah
right not on purpose that's right um
yeah it begs the question of you know
simile simulator fidelity to the real
platform but that's always a question
yes my official come on what
I said what you are an amazing girl did
you consider what other and how it you
did them in that
the highest but you need to get my cow
come from now so I can't say that we've
considered every factor right what we
did is we said you know we talked to
them good and how you give good choose
build Oh beyond layout so it turns out
that so we focused on layout actually
specifically because it has two very
direct consequences on very very
performance critical pieces of your
hardware one is the cash and one is the
branch predictor alright so the cash is
huge right if you miss in the cache you
go all the way out to ram it's a hundred
times slower than if it were in l0 right
so this is a gigantic gigantic impact
branch prediction has a much less
dramatic impact but it also has an
impact on performance right so that's
why we focus on my out ok so so within a
function the branch layout matters a lot
like that's an optimization that has a
lot of performance so how would you
generalize what you did you'd have to
change the compiler that because the
offsets are baked into the code because
you don't want to look up the offset of
the else branch yes yep nothing like so
so so Charlie and I've talked about this
we should I guess we should wrap this up
very quickly briefly the idea is that
you generate different variants of the
code you generate different functions
and then you sample those at execution
time so when you're doing reran demise a
shin you don't just take you know this
function move it somewhere else you take
function version k right it's ok is
fantastic thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>