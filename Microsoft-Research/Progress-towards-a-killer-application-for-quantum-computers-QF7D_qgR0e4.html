<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Progress towards a killer application for quantum computers | Coder Coacher - Coaching Coders</title><meta content="Progress towards a killer application for quantum computers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Progress towards a killer application for quantum computers</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QF7D_qgR0e4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay so welcome everyone and today we
have Nathan we've visiting the court
group he's a candidate for a researcher
position in quark he's currently a
postdoc at the Institute for quantum
computing at the University of Waterloo
and today he's here to talk to us about
progress towards a killer application
for quantum computation so Nathan thank
you for coming great thank you very much
for the introduction Krista
so as Krista said I'm gonna be talking
to you today about the quest to find a
killer application for quantum
computation so the idea for this talk
came from a question that was at
actually asked me by two different
colleagues at two different conferences
and this question is if I was given a
quantum computer with a hundred qubits
in it what would I use it for and this
question got me thinking and made me
wonder whether we've actually found
something that realistically is a killer
application for quantum computation yet
and if if not what such an application
would ultimately look like now the
notion of what a killer application is
is to some extent sociological it
depends on what problems you think are
interesting or what problems you think
other people find interesting so in
order to preface the the opinions that
are contained in this presentation I
really think it's necessary just to
spend a moment talking about the sorts
of research problems that I find
interesting so my research is touches a
whole bunch of different areas involving
quantum simulation adiabatic quantum
computation quantum algorithms numerical
analysis foundations of physics and
Bayesian inference
so it might come as a bit of a surprise
given this that to me my vision of a
killer application looks something like
this this is vizhi calc a program that
is widely regarded to be the first
killer application for home computers
this simple application single-handedly
justified the purchase for many people
of a very expensive home computer in
1982 so from my perspective something it
ought to
call itself a killer application for a
quantum computer should spark people's
imaginations at least as much as visi
calc did anything then I would not
consider that to be a killer application
so in this talk I'm really going to put
candidates forward for potential killer
applications and then discuss whether or
not I feel that these are adequate or
whether they're close to being killer
applications and these two candidates
that I'm putting forward are quantum
simulation and the quantum data fitting
the outline of the my presentation for
the simulation will involve an
introduction to quantum simulation and
then I'm going to present a new method
based on non non unitary simulation that
was developed by myself and Andrew
childs that improves upon existing
methods used in for example quantum
chemistry simulations and then I'm going
to discuss data fitting in particular
I'm going to give an introduction to the
linear systems algorithm on which the
data fitting work is based and then I'm
going to present the algorithm after
which I'll conclude and discuss whether
either of these actually is a killer
application and if not how close we are
so quantum computation really began with
thoughts from Richard Fineman and Yuri
manin that suggested that quantum
information could be stored and
manipulated in quantum systems
themselves now this actually doesn't
sound on the surface like all that deep
of an insight after all Moore's law has
caused us to miniaturize to get
components smaller and smaller so this
sounds like it may just be a replication
of the same theme just smaller but
actually it's a fundamentally different
idea the reason why is because of the
fact that the information has to be
stored in a manifestly quantum fashion
that is in a way that defies a classical
description so the way that this is
generally done in quantum computation is
that we have bits that then are
generalized into quantum bits or qubits
and unlike classical bits these quantum
bits can be a arbitrary superposition of
0 and 1 so the difference of the
difference between this made a full
analog computation
one of the differences at least is the
fact that when you measure this you get
a discrete outcome so every time you
measure the quantum bit it probabilistic
Li will either be 0 or 1 and the
probability is depend on the amplitudes
of a and B you can of course repeat the
same idea with more quantum bits so with
two quantum bits you your system can now
be in an arbitrary superposition of four
different values and obviously if you
imagined you had three qubits then it
could be in an arbitrary superposition
of any of eight different possible
combinations of 0 and 1 so that's how
this works and of course a quantum
computer also has to be able to
transform these quantum bits strings at
least in principle into any other
possible quantum state within the
quantum computer so the question is
where does quantum computing potentially
get its advantages over classical
computation and there's many different
candidates that one could use for this
but to some extent one of the biggest
things is this exponential growth of the
size of the amount of information that's
required to represent the quantum state
so to give you an idea about how these
Exponential's end up scaling I've given
the following diagram imagine that you
had a single quantum bit and you could
write down all the information you
needed in order to represent that
quantum bit within some fixed air
tolerance on a single grain of sand then
of course the relevant unit is well how
many qubits would you need in order to
make a pile of sand the size of a
Tyrannosaurus Rex and that turns out to
be 48 qubits if you want to look at a
quantum computer that has twice as many
qubits 96 then the pile of sand that
you'd need to represent that quantum
state would have a volume about a small
moon going up again a factor of 2 that
pile of sand would have to have this
size of a small planetary nebula so
obviously this problem that I began I
began with this question of what I would
do with a hundred cubic quantum computer
this is not a trivial computational
device this device would require a lot
of classical information in order to
specify what's going on inside the
system
so you ought to imagine that quantum
computation should be useful for a host
of different problems and a number of
algorithms are knowing for which speed
ups over the best known are best
possible classical algorithms exist and
of these I'm going to focus only on
those that offer super polynomial or
exponential speed ups the reasons what
is that obviously we really want
something that is much better than a
supercomputer so of these there's a
number of a number of examples and the
two that I consider most promising are
of course quantum simulation and linear
systems algorithms and I'm going to be
discussing those in detail later on so
let's start with quantum simulation and
the quantum simulation work that Andrew
and I have done so the idea of quantum
simulation is actually really
straightforward at its core what you
have is you have a quantum computer
which intrinsically is a quantum system
and you have the ability to perform
arbitrary transformations on the quantum
state inside that quantum system so in
principle other physical systems are
themselves quantum systems so you'd
imagine that the transformations that
naturally say a chemical system is
undergoing could be performed on a
quantum computer at a logical level
similarly the quantum transformations
that occur in condensed matter systems
ought to be emulated by a quantum
computer of an appropriate size and this
is the idea you basically use the fact
that a quantum computer is a universal
device to emulate the natural quantum
evolutions that exist in other systems
and you can actually do this efficiently
whereas on classical computers there's
no known way in general to simulate
arbitrary quantum systems efficiently so
some kind of you can do some kind of
amazing things if you were able to get a
hundred qubits scale quantum computer
you'd be able to simulate reaction
dynamics and learn spectral properties
of chemicals without ever having to
synthesize them you could use these
devices in order to investigate models
for exotic superconductivity or look for
quantum phase transitions
condensed matter systems there's many
different possible applications of hard
problems scientifically that quantum
simulation could be used to address
right now and important ones too so what
you need right right right right
like yeah I was referring to
spectroscopic properties obviously if
you synthesized it then you could just
throw it through whatever spectrometer
you wanted and look at that but you
couldn't get information say about
reaction dynamics or more maybe more
sensitive pieces of information from it
so one of the key things that you can do
with quantum simulation is that actually
if you do this on a quantum computer the
quantum computer can can perform error
correction and it can actually certify
that the answer that you get out of this
quantum simulation is actually valid so
that's one of the very neat things that
you can do with quantum simulation you
may not be able to do with other
approaches try to build analogous
experiments say to one of these systems
and also the number of logical qubits
that you need in order to equal or
exceed the power of a conventional
supercomputer isn't very big you only
need route something on the order of 40
ish qubits in order to start getting to
the regime of existing supercomputers
and also unlike many of these other many
of the other quantum algorithms that
have been proposed so far these this
family of quantum algorithm solves a
host of problems that scientists
actually care about deeply right now and
a lot of processor time is spent
currently on solving these sorts of
problems can you clarify
I got some maybe you know an example of
what do you mean you know what actually
at the next slide it probably is a gives
me a better prop for describing this so
yeah sure maybe this is what you're
going to talk about this in a bit
because we're gonna talk about series
but to two steps I was curious
first question
on a cloud computer we can see like time
to announce but we might be interested
also in having to cool the molecule to
its ground state before during the time
than ever so this might happen already
next your bath qubits well there's
several ways it technically actually
there's no overhead with bath qubits
that are required because you can always
in principle simulate an adiabatic
evolution to cool it into its ground
state you don't require any additional
qubits for that the bath for the
question about the bath worst-case
scenario you require one additional
qubit you can effectively do a Fourier
transformation algorithm and use that to
cool it
you have 100 cubits is sort of in terms
of a spin system that's great in terms
of a system of electrons I mean it
really depends on the way that you're
doing this and if you what you for
example if you're talking about
simulating a quantum system in you know
some first quantized form then the way
that you would have to probably encode
that is you would begin by saying okay
well the electron cloud is distributed
according to you know say an S orbital
at the beginning of that and you can
code your information efficiently with
that and then you would require a
quantum algorithm to prepare the initial
state as an S orbital and then evolve in
first quantized form that of course
isn't terribly efficient a more
efficient way of doing it is to if
possible map it to a second quantized
form and use creation and annihilation
operators to deal with that then you
only just have to abstract Li mention
the state at the at the beginning of the
algorithm that the electrons are in yeah
in terms of first one five second
quantized the hilbert space of two to
the hundred is it sounds huge but at the
same time when you ask you know how many
orbitals electrons can have
count the numbers possible space in a
fairly small markets it's already there
- even in the second month - porn you
only get 50 orbiters at best you still
need to loosen things off for phase
estimation exactly so that's the size we
can still be doing classified right so
sorry you're probably trouble limit on
the edge but you know we really you know
the number I tend to wind up using
business with 200 at least because now
you're beyond shen can do classically
share it anyway good time yeah yeah for
these these sorts of four for 40 to 100
ish cubits is certainly for spin systems
going to be where the sort of the
state-of-the-art is I think whereas yeah
there are certain certainly overheads
that come in with the chemistry
simulations but we can discuss that
later
so a going back one more on the stack to
the question about the certification and
why I was bringing that up one of the
big trends in experimental physics these
days is to look at so called analog
simulators and this is justified by the
fact that building a full quantum
computer seems to be a very difficult
thing to do so but quantum mechanics is
just something that happens naturally in
the lab so why not just construct a
gigantic lattice of hundreds of
interacting atoms and call that a
quantum simulator well something
actually it is quantum Eddy and it is
simulating something but the question is
is it simulating something other than
itself and can you actually if you want
to use this in order to answer a
computational question what do you need
to do in this case certifying this
system of several hundred qubits that
was taken from the ion trap group at
NIST what you need to do at least on a
naive level is you need to first
understand what the state is and that
would make require tomography which is
going to be exponentially expensive so
there's no known way of getting these
sorts of simulations which are already
very complicated and very sophisticated
to actually solve genuine problems so
that's one of the reasons why I
emphasize the certification issue
because you know this is false dichotomy
where you say this digital ease this
digital simulation experiment that
people have done is only six cubits why
are they bothering when you can do
a few hundred cubits well the reason is
is because some extent you can trust
what the output is of these sorts of
experiments whereas with this one who
knows because you can break quantum
computation up into very small parts and
then do an algorithm yeah other than
having a large system that you just say
that's something that well you just you
just pray that according to your
understanding of the laws of physics
that it ought to be the system that you
think it is tomography as a killer clock
man
that's possibility that's a circuit that
I'm going to get to that point with my
data fitting algorithm later so that's
no set so let's talk about let's talk
about basically how this emulation
process inside a quantum computer
actually works out so there's several
steps the first thing is what you have
is you have this quantum system and this
is the thing that you'd like to simulate
and you have your quantum computer which
is the device that you're using to
simulate it in general you're going to
require more qubits for your quantum
computer than the quantum system but for
spin systems it turns out they can
actually be the same size and often so
the way that the algorithms begin is you
you want to simulate the evolution of
some initial state forward in time to a
final state in the actual quantum system
you begin by taking that state and
encoding that as a qubit state inside
your quantum computer that's logically
equivalent to the initial state then
this continuous time evolution that you
have you would see in the physical
system you proximate via a series of
discrete gate operations which map the
system to a final state and at final
state ought to be logically equivalent
to the Evolve State up to some air
tolerance and that's how these these
algorithms fundamentally work and if you
design this right then you'll be able to
know beforehand what that map air can be
in a worst-case scenario so
unfortunately this isn't it because
getting the final state is the outcome
of this this thing is like getting a
fortune cookie that contains the answer
to all of your all of life's problems in
it it doesn't really mean too much until
you extract information from it so that
final quantum state that you get here it
doesn't solve any problems what you have
to do is you have to measure that which
destroys the state and then start the
simulation protocol all over again to
get more information and repeat this
potentially a large number of times in
order to learn the information that you
wanted about that quantum system yes
concerning certification since one
computer makes errors as well be careful
you know Apriori opposite this is better
than just testing
insisting that this think is available
you know if testing is available like
this let's say you actually have the
quantum system you're entirely right
because of the strong analogy between
the quantum computer and the other
system yeah why not if you've got the
actual system the very best simulator of
that system is the system itself if you
want to learn something about it just
experiment on it but the problem is the
problem is you don't often want to use
simulation in order to understand a
physical system what you want to
understand is you want to understand a
mathematical description of a physical
system and that's what a quantum
computer can do a quantum computer can
say that if you designed this simulation
algorithm proper properly then the
mathematics that describes this time
evolution should be simulated accurately
by the quantum system yes and that's the
secondary secondary benefit there are
certain experiments that would at least
be impractical to do in a physical
system so I'll just do a very brief
example of a spin system type problem
just to understand it just to give you
an idea about how the simplest variants
of these quantum simulations work so
this is a model known as the transverse
Ising model for two interacting spins
it's a model used in condensed matter
physics to describe quantum magnetism
and the basic basic matrix you could
just imagine it's a four by four matrix
that has an interaction between the Z
components of these two quantum spins
and also it has an interaction with an
external magnetic field that's pointing
in the X direction so if you want to
simulate that then it boils down to the
question of how do you end up taking
this quantum mechanical time evolution
operator which generates say this
transformation into a series of gate
operations on the quantum computer and
well in this case I guess it's a four by
four matrix so you probably could do it
directly but in general for higher
dimensional systems it's hard to
actually synthesize this in a
straightforward way so the way that this
is done is using a
Trotter decomposition so the idea is is
that this Hamiltonian is the sum of
three terms each of these it turns out
can be efficiently implemented on a
quantum computer but together it's not
clear how you would do it so what you do
is you say okay well this we break up
this time evolution into a series of
very short time steps and in each of
these time steps you will evolve only
according to one of these three terms
and by increasing a number of time steps
you can make this approximation
arbitrarily accurate so if you do that
you can actually find a quantum circuit
that's equivalent to these three
operations and this quantum circuit is
down here but unfortunately this isn't
done because there's there's actually
two halves of these sorts of simulation
algorithms the first half is what I've
described here it's this process of
trotter ization and breaking it into
elementary rotations and the second half
is deal is converting or compiling these
elementary rotations in two fundamental
gates of the quantum computer can
actually offer I can actually use and
these gates there's a number of
different gate sets that can be chosen
but a common common choices are PI by 8
gates in the Hadamard gate
there's many methods that are known to
synthesize these single qubit rotations
that appear in this sort of a circuit
and of course the group at Microsoft
Research is our world leaders on these
sorts of techniques now let's talk about
what the sort of state-of-the-art
methods are at present in quantum
simulation the most common approaches
that are often used are these product
formula approaches which are exactly the
same sort of thing that I showed you
with this example of the spin system
just more sophisticated they use high
often will use higher-order trotter
formulas than the one that I presented
they're very good for high accuracy
simulations of extremely sparse
Hamiltonians more recently methods have
based on quantum walks have been
developed largely by dominic barry and
andrew childs and these methods are
unfortunately they are not as good for
high accuracy simulate
but they are much better for non sparse
hamiltonians the methods that I'm going
to be discussing in us are multi product
based methods which actually are
superior to these product formula
approaches in almost every way and I'm
sure I'm sure the group will tell be
able to tell me very quickly with the
one way they're not superior is but I'll
leave that as a surprise
so with product formulas the best know
one result for prata for the scaling for
product formulas is is this here M is
the number of elementary Hamiltonians
that you have in your Hamiltonian again
the Hamiltonian is like your quantum
energy operator so it scales it scales
like quadratically with a number of
elementary terms that you have in your
Hamiltonian and it scales nearly
linearly with the norm of the
Hamiltonian times the evolution time
yeahit's is a maximum norm of all a
little H sub J's so M times that will
clearly be an upper bound at a norm of
the Hamiltonian
oh no they can think that they can
interact with each other but the point
is is that each of these individual
terms have to be a have to be
individually simulate able okay so they
they don't necessarily have to be non
commuting so the final thing over here
is the air tolerance and this these
algorithms scale sub polynomial II with
the error tolerance that you want out of
the the simulation but unfortunately not
poly logarithmically so that's that's
the basic intuition behind this and in
order to get this sort of performance
you can't just use the basic Trotter
formula that I showed previously you
really have to choose higher and higher
order Trotter formulas as the evolution
time and the the air tolerance to end up
becoming more stringent Oh Big O tilde
what I've done is I've dropped terms
that are logarithmic in here and not
really yeah just to make the expression
look nicer yes multiplicative locks
oh okay the reason why is because
obviously if I didn't have this this
square root here it would just be like e
to the log 1 over epsilon so that would
be you know log 1 over Epsilon
but when you have the square root on
here this actually makes it smaller than
any polynomial function so that's why
this is a sub polynomial so the basic
intuition for how you generate these
high order charter formulas is like this
saying you have your time evolution
operator and what you what you want to
do is you want to write this as a
product of elementary time evolution so
you can actually carry out and you want
to choose the times for these elementary
evolutions so that you reconstruct the
Taylor series of the actual time
evolution operator within some pre
prescribed air and so this is actually a
hard task to try to find all of these
different times that are necessary in
order to actually reconstruct the Taylor
series
so there's actually cottage industry in
numerical analysis for finding different
times in order to do this
however there's fortunately a very nice
recursive scheme that suzuki invented in
order to refine a low order
approximation into a higher order
approximation and this iterative scheme
basically works as follows you start
with a low order approximation and you
do two time steps forward with that low
low order approximation then you do one
time step back for a certain value of
time and then you do two more time steps
forward and with this two forward one
back to forward procedure by choosing
this single parameter rather than all of
these different parameters individually
you can actually find a neat way of
guaranteeing that you'll increase the
error by two orders by doing this at the
cost of increasing the number of terms
in your approximation by a factor of
five so the trade-off is you have five
times as many Exponential's but you get
two more orders of accuracy out of doing
this
sure this is it yes so here what I'm
doing is I'm just talking about
analyzing one of the short time slices
for the time evolution if it's a if
you're looking at a long time evolution
then imagine just taking our slices of
that and making each of them short and
then you use one of these high order
approximants
yeah it gets partitioned some more into
these smaller bits out here and so I
just like to give you guys a visual way
of understanding how these product
formulas work because our approach in
contrast is going to do something very
different so here these boxes what they
do this represents a Taylor series this
first box is a zeroth order term in a
Taylor series for the time evolution
operator this is the first order this is
the second order and then these two have
errors in them and what the Trotter
Suzuki formula does is it combines them
in an appropriate way such that when you
multiply these boxes together that these
errors over here this end up getting a
negative sign because of the fact you're
looking at a backwards and time
evolution the products between all of
these end up causing these terms to
interfere with each other and cancel out
and a symmetry consideration ends up
causing these higher-order terms to also
cancel out when you put it in this form
and so that's basically how this works
one of the big drawbacks though is if
you just take a look at what happens
with the errors whenever you multiply is
that every time you multiply new types
of errors are created by multiplying
terms that previously worked with error
terms so for example you know let's take
a look at the air terms that like the
second-order air terms here these can
actually be formed by by multiplying
this correct term or an air terms of
that scale can be formed by multiplying
this correct term by that incorrect term
and thus you'll generate a more complex
series of errors through multiplication
than you would otherwise
so a lot of the effort you can imagine
conceptually in the Charter Suzuki
formulas is to actually counteract these
errors that are introduced by
multiplication and deal with the fact
that multiplying number of poly no meals
isn't a very natural way to build a
Taylor series the natural way to build a
Taylor series is to add them and that's
exactly what we do so we we suggest
doing something very different don't
multiply start with your lower order
formulas come up with some weighted sum
of these lower order formulas and you
add them together in an appropriate way
to get to make the Taylor series you
want and this is very natural because of
course with Taylor's theorem
you just add the individual terms and
Taylor series expansion to construct it
anyways so this doesn't and this doesn't
create this problem of propagation of
errors this sort of an approach has
already been known in numerical analysis
community for quite some time the
Richardson extrapolation is the simplest
example of the these sorts of
approximation building methods and in
general we can we can construct multi
product expansions that work by adding
together many low order product formulas
together with different coefficients out
here and construct our approximations
that way and that's the method that we
use here so rather than these massive
product formula approximations we add
together a bunch of them with different
coefficients in order to approximate the
time evolution operator within some
accuracy and so there's some advantages
to doing this the first key advantage is
the number of Exponential's that you
need to create the formula using a
charter Suzuki you notice will grow
exponentially with the order and that's
exactly for the reason I mentioned every
time you increase the order recursively
you need five times more Exponential's
and it's the 5 to the K minus one
whereas with the multi product formulas
you only end up needing order K squared
terms which is fantastic but of course
you're something that's very unfair
about this as well and that's the fact
that although the Charter Suzuki formula
is unitary which means that it's a
ordinary quantum transformation that can
easily be done on a quantum computer
these formulas in general are not
unitary so you have to go through a
greater effort beyond a quantum computer
in order to try to synthesize them
however we end up discovering ways to do
so and actually find surprisingly that
you can use non unitary operations on a
quantum computer to simulate unitary
dynamics more efficiently than you could
by using the unitary dynamics by itself
so the way we do this is using a the
following circuit this circuit is
designed in order to create a linear
combination of two different unitary so
it just effectively sums to unitary with
an arbitrary coefficient
in front of it the way you do this is
you take this single qubit quantum
transformation here you apply it to the
input bed and perform these controlled
evolutions where here u 0 + u 1 you
could think of as just two different
operators splitting formulas this one
might take one step time step and this
one could take two time steps but
they're the same formula and if you
measure this and observe zero then
you'll have actually performed the
correct linear combination that you want
whereas if you measure one then you
won't and you might have to perform an
aircraft some error correction this can
also be generalized pretty
straightforwardly to adding more than
two terms just by using a larger unitary
on many qubits and that's effectively
what we do but if the purposes of this
presentation I'll focus on just the
adding two terms together because it
captures all the intuition you need
information
the business cubed website right so how
big does the loss of information
why do you think that the anything
information is well sure the actual
qubit it remains it remains in a pure
state actually the during the entire
evolution the reason the reason why is
because the when you take a look at this
linear combination of unit Ares that you
end up getting over here this is
actually just going to map a pure state
to a pure state so it's not like it's
not like you're going to end up getting
a mixed state out of this linear
combination for example you know let's
take the worst possible you know most
destructive linear combination that I
can imagine for these sorts of things
imagine you want to do a linear
combination of identity and Zed what you
end up getting is you end up getting it
just a projector onto the zero state
from that and that just ends up mapping
you to a pure state so these sorts of
combinations will always end up giving
you a pure state yeah sure
one more back okay in cases that you do
with zero okay
the case is a probability that I deal
with and it depends on the on the
operation and the reason why I'm gonna
get to this in a second but there's two
type sorts of errors that end up
happening
it's temp depends if you a is a prop you
one is approx you 0 or approximately
negative you 0 those are the two cases
that come up in the simulation algorithm
and in one of the two cases it turns out
if an error occurs it's entirely easily
correctable in the other case it's
catastrophic and you have to throw out
everything so much of the cost of this
algorithm is actually going to boil down
to making sure that the probabilities of
these catastrophic events occurring are
vanishingly small but the for the non
catastrophic once the probability of
success is about 0.5 is what we found
the optimal to be okay in general you 0
and u 1 are going to be two different
values of the same let me go back for a
second this is what it is
SX over here is an approximate
approximate for the time evolution
operators so it's going to be one of
these product formulas so in general
these are all going to be of the same
form but they're just going to use a
different number of steps so one of them
will be the formula that you two copies
of the formula with half the time and
the other one will be like one copy of
it with a full step so they're going to
be exactly the same form of
approximation it's just one of them will
involve many steps and one of them will
involve maybe a few for different values
of Q here
what did you get a few measures one okay
you know what yes you know what how did
I go to the slide all right so if I go
to the slide if I measure zero then what
I get is I get the linear combination of
the two unit areas that I want to
actually implement if I measure one then
I get the difference between those two
and so this is one of the reasons why I
mentioned it depends on whether or not
the the two are close to each other or
very far from each other because if
they're very close to each other then
this term will be approximately zero and
if it's approximately zero it's just
going to clobber your state and you're
going to get some garbage left over
whereas if you have this K if they're
opposite to each other then you'll have
approximately to you B which is a
unitary operation and furthermore it's a
simple unitary operation you've got an
operator splitting formula for so you
can actually invert that and so that's
the that's the basic intuition for this
so for some of the steps it'll be
catastrophic so I'm going to be good
typical probability is about 50% yeah
that's the best to shoot for yeah
you've lied that it's actually that's
rejecting in their lower dimensional
subspace um that yeah oh well I mean
your your what do you mean by that
you're really just reading a first Cuban
Institute yeah it's the second the
second is where all the information you
want is actually encoded this is just
used in order to allow you to do this
trick where you perform a weighted sum
of these two distillation
perhaps is certainly it certainly along
the same sort of theme so you make this
unitary transformation and it's been
okay just throw it out and discard it
but perhaps one thing to do then instead
of trying to apply Inglaterra
transformation to your actual state yeah
it's one of the things that I remember
we looked at we weren't able to get any
traction on that particular idea to use
you know some sort of a gate
teleportation type ID you have to do
this because I would clearly be ideal
but I'd be happy to talk about it later
all right so that's basically it and as
I said before you know but now it's a
belabored point some of these are bad
errors and some of these aren't not so
bad errors and the Justin so just to
give you a basic overview of how this
how this would work for the simplest
possible case imagine what you have is
you're just using this Richardson
extrapolation formula here in their
simulation you use this particular
formula here S one here could represent
the simplest simplest trotter formula
you can use the symmetric trotter
formula so you use that formula with
weight of four here and weight a minus
one there and you use those circuits in
order to implement this particular
linear combination and again in general
what will happen with this is that the
air correction can be done with high
probability in this case because s 1 is
has opposite sign to baddest one okay
so essentially Holley how it works is as
follows the flowchart is you attempt a
time step with one of those terms in
here if two options you use succeeds or
fails if it's seeds go under the next
time step if it fails well then you
attempt error correction and the error
correction that you attempt you can do
using this exact same method it turns
out there's a chance of a catastrophic
error when you attempt that error
correction step but that probability can
be made arbitrarily small so you attempt
that and again if it fails you abort the
simulation otherwise you try that time
step again and repeat this process until
you're done you're doing this at every
time step you could have them
could have a chance yeah the second last
time step could have it could be the one
that fails nonetheless when you consider
all of these possible things that can go
wrong and all the additional costs
involved in making these errors small
you notice that what ends up happening
is we end up getting an exponent up here
which is smaller than the exponent that
you end up getting for charter Suzuki
just because even despite all of this
these problems with error correction and
like the advantages of these
multi-product formulas are so great that
you end up you do end up getting a
benefit out of it this polynomial e ends
up reducing the scaling with the air
tolerance over the best known results
using just pure trader formulas yes
and so basically what this means is
we've given a method that is superior to
product formula simulation methods in
nearly every way furthermore this could
lead to improve quantum chemistry
algorithms and also most importantly I
think it gives a new way of thinking
about simulation that doesn't directly
involve a logical mapping between the
initial system in the final final system
it the dynamics effectively that are
going on in the quantum computer are
actually not the same sort of dynamics
that are going on in the physical system
because the non unitarity and it says
that this paradigm has some advantages
over the traditional paradigm but the
real question is well is quantum
simulation to kill their application and
I would I would save it for scientific
computation arguably you can say yes
however for general-purpose computation
probably not really I I can't I what the
golden standard of this talk is is
visicalc and quantum computation is not
something that is going to get people
who are you know say interested in
directly or sorry quantum simulation is
not going to get people who are
interested in say data analysis to
directly get out and be excited about a
quantum computation so I don't think
it's quite as compelling for
general-purpose computation but
undoubtedly it could be incredibly
useful for scientific computation
however the next thing you could think
of is well maybe actually because you
have such a compelling application for
scientific computation maybe it's
possible that some problems within the
scientific computation community or
simulation problems can be mapped to
other classes of problems that are more
relevant and this is exactly the same
sort of intuition that's used in the
harrowhouse Ataman Lloyd algorithm for
solving linear systems of equations so
this leads to the second part of my talk
which talks about linear systems and
data fitting so the linear systems
algorithm really actually is built out
of three three components and the first
component is quantum Singh
second is it uses a essentially ideas
from phase estimation and amplitude
amplification in order to make
everything work and basically the way
that this this problem works is imagine
you have some knowing matrix a
multiplied by some unknown vector it
gives some known data and what you want
to do is you want to find the unknown
vector so this is a standard matrix
inversion problem that you want to solve
something that people do all the time so
obviously you know if this actually
provided an exponential speed-up that
has no caveats this would certainly be a
killer application but there are some
pretty big caveats that we'll get to in
a second the way that this the the that
you approach using quantum computers in
order to solve this problem is you begin
with a basic problem and then you
quantize it you replace the input vector
by a quantum state and again this
quantum state and fortunate because of
normalization will only be proportional
to that that input state you can lose
constants of proportionality with the
way that they design this then what you
do is you invert a by diagonalization
and the way the diagonalization works is
basically it uses phase estimation to
store the eigenvalues of e to the minus
ia T in a register then you divide by
the eigenvalues in order to implement a
inverse and that's how that's how the
quantum algorithm essentially does its
job and a key point though is that at
the end of the algorithm what you end up
with is you end up with a quantum state
that encodes the answer to your problem
not the answer and that's a that's a big
drawback because in general the size of
this the problem is exponential so if
you wanted to read the output out of
this you'd have to sample the quantum
state an exponentially large number of
times and you would taut you totally
lose any advantage that you could
possibly get from this algorithm by
doing so so this that's one of the major
drawbacks of this particular algorithm
however you can if you're if you're
interested in this I don't know why you
would be but you could you what you can
get out of this is you can figure out
expectation values of your
solution to the system of equations
there may be some application where this
is useful but I don't know what it is
right now and furthermore this work can
be actually applied to solving systems
of differential equations but again you
don't get the solutions there you can
only find the expect expectation values
or other similar properties to systems
of differential equations which although
I've gotta admit that does have some
value there it doesn't give you quite
everything that you want one kaseman why
is
the sequence of zeros and ones yes so
then when you compute
kidnapped
I'm sorry so if lambda
so let's I think let's go back here I
think this one every problem don't you
get it right okay so if you get if you
get bye-bye yeah you you will you'll
always get a binary representation but
the way that actually this is encoded is
generally speaking it's going to be all
these real values for each of these
components will be encoded as amplitudes
of the particular values of this so for
example you know the the first the first
entry in that would be encoded as the
amplitude yeah
sufficiently working case when y
consists of zeros and ones
so don't be Jesus
right but the animal yeah the amplitudes
will be zero and one in that case and in
that case there may you may be able to
take advantage of the peculiarities of
that particular constraint to learn more
information than you would otherwise but
you certainly the general problem is
going to be hard
maybe this particularly problem that you
mentioned there could be a cunning way
to get around it but I don't I don't
know what that is for the moment so the
other Pro there's two basic problems I
guess you know for this we mentioned the
output problem of reading the output but
also the input could be exponentially
hard to generate so you have this really
odd situation in a way with this
algorithm right the actual hard part on
a class of computer now becomes trivial
but generating the input and reading the
output becomes extremely hard so so
clearly this isn't a killer application
at all the question is I mean is there
is there something that you can that you
can do with this can you generalize
these ideas to come up with a different
algorithm that doesn't satisfy I that
doesn't have these same drawbacks and
this is something that Daniel Bron and
Seth Lloyd and I thought about when we
were approaching this and the
application that we thought of was
quantum data fitting and the basic idea
behind the problem is as follows oh
sorry I think data fitting to me is such
a ubiquitous task in general-purpose
computing that if we did get a quantum
speed-up for this then certainly this
would satisfy the goal of this talk it
would be a killer application for
quantum computation and so the question
is well does it work out well let's
discuss the data fitting problem and how
that actually works imagine you've got
some function of Y of X and you what you
want to do is you want to represent this
as some combination
of fit functions FJ of X your goal is to
find weights for each of these fit
functions that minimize the square error
so that's the that's the idea and
there's many ways that you can do this
conjugate gradient methods are a great
way of doing this on a computer
classical computer but you can also use
a linear algebra approach where you just
apply an operator known as the more
penrose pseudoinverse and that operator
is given down here the key advantage of
our of our method even though this is
this is actually more complicated than
the inversion problem is that this
matrix f here no longer necessarily has
to be square so for example you could
try to fit an exponentially large data
set to a line right in that case your
output dimension would be you know you'd
have two parameters to fit a line so in
that case you could actually solve the
output problem you could efficiently
read all the information that you need
for sure in this one particular
application the input problem still
remains unfortunately but the output
problem could be resolved by this
application all you have to do is apply
this more penrose pseudoinverse I should
also mention that the pseudo inverse
operation in the case where the main
matrix is actually invertible reduces to
the previous problem considered by
harrowhouse Adam and Lloyd so the way we
do this we follow the exact same sort of
strategy that harrowhouse has been lloyd
employed you start with your vectors and
you encode them as quantum states with
each of the coefficients stored as
amplitudes of that particular value so
or I should say entries and then what we
do is we use these trick because quantum
quantum CompTIA to you leverage quantum
simulation we need to have a hermitian
matrix so we need to have something that
is self adjoint and that generally won't
won't happen for these fitting problems
so we use a dilation of the space we
introduce an additional qubit in order
to make the matrices hermitian in a
larger dimension and after using this
trick it turns out that the
F dagger F inverse is actually just F
inverse squared so after using this
trick this ends up becoming that and so
this can actually be employed or
executed by using the Harrow houses and
Lloyd ideas three times yeah yeah you
actually only need one in order to
extend it out so the idea is as follows
you generate the initial data then you
use a Harrow house cinnamon Lloyd to
implement F dagger I should say you
don't quite use it because you don't
need to divide by the energies in that
case in that case you multiply by the
energies but everything else is exactly
the same then you use the algorithm two
more times in order to implement these F
inverses and that's what you that's what
you do
the cost of the algorithm doing so is as
follows the key points take-home is that
it's efficient there's nothing it's
exponential in this problem s is the
sparseness of the matrices in question
capos the condition number and epsilon
is the air tolerance so that is o and
capital n is the dimension of the
largest dimension of the matrix F so you
can do a number of things that are
actually kind of interesting with this
so yes
power F dagger and F inverse related
okay they are not directly but if it was
units you're thinking unitary if it was
if they were unitary then F dagger and F
inverse would be you know the same but
in this case the it's it's not it's not
square so you know this is just the
conjugate transpose of a rectangular
matrix this whole thing just collapses
down to the
you know an F dagger cubed yeah will
that work in the square case does it
help you at all no the whole point was
to do waivers yeah exactly the whole
point was using more penrose
pseudoinverse in order to write this as
a series of matrix multiplications that
you can then carry out on a quantum
computer so that's the that's the basic
idea and there's actually a few things
that you can do with this
although the output state in general
could be exponentially exponentially
large and it could be for the fitting
problem hard for you to actually learn
to fit it turns out you can actually
learn the quality of the fit efficiently
regardless of the dimension which may
come as a surprise
also of course if there's a small number
of fit functions that are being used you
can directly learn to fit okay so
several I haven't mentioned to Oracle's
so there's two orc well actually no
effectively there's one Oracle sorry the
Oracle will provide you you know all the
matrix elements you need for the fit
functions in a particular basis or in
fact in general you can imagine in
several bases if you have different
natural bases for each of the fit
functions that are that are used there
it's also useful to imagine that the
input state is generated by an Oracle
but yes actually yeah these are these
costs that I mentioned are effectively
the query complexities so the way that
you would go about and learn the quality
of the fit is really straightforward
what you do is you'll note that F lambda
is your approximate is gives you the
approximation to the data set that you
have and Y is the precise data set you
have so if you want to compare the two
just use a swap test the swap test is
this as a quantum mechanical test that
allows you to efficiently determine
difference between two quantum states so
you repeat this process some number of
times and that you then you end up
finding the quality of this fit by
comparing so the one of the things
that's that's actually kind of
interesting is actually the cost of this
algorithm is less
than the algorithm I gave previously the
reason why is because the F here clobber
is one of the f inverses that was that
was used in order to construct lambda so
actually cuz making f lambda is cheaper
than making the previous date very good
question so there's only one step using
our approach that it actually pays to do
amplitude amplification let's go back
and this is where this is where the
capita the six comes from this is the
only step that we currently that we use
amplitude amplification and these two
steps over here it turns out if you try
to use amplitude amplification after
that you have to reflect around the
evolved state and and that's just it's
just too expensive it's the cheapest
thing to do is just use amplitude
amplification here and not use it on
those two
if it's not square
excellent question so f here sorry is
what I should be saying here is that
these are not the rectangular matrices
in the original problem these are the
hermitian extensions of the original
matrices okay so it from this
perspective
once you've dilated it to a hermitian
matrix f inverse of that dilation makes
sense and it is square F dagger is not F
inverse yeah and okay so the next the
next thing is if you also want to learn
the value of the fit function so well
one of the things you can do is you can
just use compressed sensing as a
tomographic technique in order to learn
what the amplitudes are for the the
output States and that cost ends up
coming in as this additive term here
where M prime is the number of fit
functions you actually use and epsilon
is the air tolerance well another thing
that I should mention that's really kind
of cool that you can do with this is
that you can actually find if you don't
know a priori a good set of the fit
functions actually by measuring the
output of the algorithm because the of
way that everything is encoded it
encodes the answers as amplitudes so the
functions that have the highest
amplitude are the ones that are most
significant to the fit and the ones that
you're most likely to measure when you
measure the state in the end so what you
can do actually with these algorithms is
something really kind of cool you can
say alright I don't have any idea what a
good set of fit functions would be to
represent this data I'll start with a
complete or near complete set of fit
functions then you measure the output
state and you sample from that you find
the ones that appear most frequently and
cut out all the rest of them and that
will give you in some cases a good
estimate a good guess for a set of fit
functions that you should use for the
problem even if you can't a priori know
which fit functions are best
but you but the thing is you only have
to do the tomograph actual tomography
later right once you found the fit
functions that you want to use in the
final tomography then you do the
tomography so this process over here you
can think of like a compression process
you start with a set of fit functions
and you use this to throw out the ones
that aren't useful so just to compare
comparative harrowhouse edemen lloyd
preparing the initial state is
inefficient with harrowhouse Simon Lodz
approach unfortunately with quantum data
fitting the same thing in general is
true learning the output isn't
necessarily a problem with our data
fitting algorithm in many cases and
furthermore we can actually estimate the
fit quality efficiently so which gives
you actually something that's useful
that you can do directly from this
particular application also going back
to something and I mentioned previously
a very natural application of this is
parameterizing quantum states because if
you have a quantum state that's yielded
to you by an algorithm or some other
device then you've already solved the
input problem you can just use this as a
alternative to tomography and get a
approximate reconstruction of the
quantum state that way and yes
unfortunately it also strongly depends
on the sparsity of the matrix in the
basis that you choose for the problem so
you really do have to be clever about
the way you choose it or use good
simulation methods so is is data fitting
a killer application well I honestly
have to say I think no the reason why is
because of the fact that it still has
many of these problems that the the
harrowhouse and Lloyd algorithm had in
order to be a killer application what
I'd love to be able to do is get some
random data set that somebody has and
just feed this in and get it to process
that random data set incredibly quickly
the problem is if you have to generate
that data set via a lookup table you're
not going to be able to prepare that
initial state efficiently so what that
means is that means that this can't be
used in order to solve many
general-purpose problems that people are
interested in
but it comes agonizingly close maybe
there are particular problems that
people are interested in maybe in a
cryptographic setting or something else
where the initial state actually can be
efficiently prepared and I think that
taking a look at studies along these
lines may actually end up leading to the
first true killer application for
quantum computation thank you very much
hey Mark I think it's let's think of a
time where data in the world will
restore this phantom space to begin
yeah if it's already at a point where
you know
where your resources are already quantum
then this the I agree with your remark
so do you have examples of problems
well yeah actually I do have problems
and this of course betrays my background
but imagine what you have is you have a
untrustworthy quantum simulator that
produces you know some particular output
data and you'd like to learn what that
data is well then you can use that
device to generate your input state run
it through this algorithm and then fit
it and learn to fit quality efficiently
but however yeah I think that there
still is more work that probably should
be done one of the things that I have
that we haven't done is we haven't
looked at particular sets of fit
functions that we care that can be
generated or cannot be generated
efficiently and that would probably be
sort of the next step into towards
making this much more practical by
identifying some problems that are
concretely useful for people to solve
and actually looking at the costs of in
particular implementing the oracles
needed in order to perform this
it was pretty obvious that it was a step
in the case of physical it's a step
people like Bill Gates who went try to
feel
personal computer or suitable
various applications it was obvious to
them that it will work in general
and here we still with the stage that
even if they said to kill or application
both of them still specialized device
she does few special tasks
far removed from
so your question is so I'm trying to
parts your questions this account a
reasonably comparable case yes well I
think that I think it I think it is a
reasonable the spirit of VisiCalc
certainly is a very reasonable thing to
compare against in that really
ultimately what I'd like out of a
general purpose killer application for
quantum computation is something really
gets a bunch of people who already have
substantial data processing needs to be
say wow I really could use a quantum
computer should it ever come out in
order to handle my daily problems and I
think for many of these people who
aren't already in the scientific
community one of these applications
hasn't come out yet for a general case
which is the move information it's not
that it's they're worrying more
especially services well I don't know
how specialized has to be if you
actually solved kilogram problem that's
the point it should be generally sort of
- are you familiar with a turret like
kenosis work on social in their ways I
know not late 90s design go extremely
efficient preparation of lots of data
into States yes
the other is plater Jacobs with spouses
I'm very familiar with that as I figured
so that that work in particular I feel
that some of the ideas can be used in
order to speed this up in particular the
their ability to avoid using amplitude
amplification and measurement in the
early parts of the procedure ought to be
able to be useful for DES for reducing
this dependence on the condition number
in particular I suspect that we might be
able to reduce this as low as Kappa to
the third by by adapting their
techniques but yeah that's that's
actually ongoing work right now that
we're looking at to optimize these
well again it's it really depends on the
fitting functions and the basis in which
you want to do this and I haven't
thought of of it in great detail I know
that this is actually a very real and
very serious problem but I think to some
extent you need to have an idea of what
you'd like to use this algorithm for in
the first place in order to in order to
do that so that's that's we're doing
right now we're trying to think about
what the actual best use cases are going
to be for our algorithm and then attack
those however natural candidates for
these sorts of fit functions would be a
for example of bounded polynomial
functions that only end up having
support over small areas or
trigonometric functions those are the
sorts of things that we are certainly
will be very easy to do either with a
quantum Fourier transform or directly
using a sparse representation because
she learning people only pick the
fitting function things because they're
easy to compute and are easy to separate
and do the same things to find things
that are quantumly easy to do it doesn't
matter the actual function is so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>