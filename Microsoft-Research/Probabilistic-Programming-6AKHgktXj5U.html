<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Probabilistic Programming | Coder Coacher - Coaching Coders</title><meta content="Probabilistic Programming - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Probabilistic Programming</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6AKHgktXj5U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
um this is where it mainly done by
Nicola season and Danny Taylor is going
to be talking next but I thought I'd
take the credit for it so what we're
going to be talking about today is I'm
really excited about public programming
I think it's a really powerful way of
doing machine learning of building very
rich and complex models and so what I'd
like to be able to is Bill massively
complex models prodigious models in
public programs and then the problem is
the minute such things we cannot
actually execute this probably brings we
kind of do inference and so the question
is how can we persuade prodigious
probabilistic programs to be somewhat
plausibly performant I got that off you
okay so um who knows about public
programming that's pretty much everyone
that's great okay so um so we have our
own in-house probabilistic programming
language we have our inference engine
compiler called inferred net we've been
working on for ten years this month and
and here's an example of the underlying
programming problem asleep programming
language of the users which we're
calling probabilistic C sharp though
it's just C sharp with the addition of
some random operators and some other
operators that I hadn't shown here when
this is this is just a base point
machine classifier influent it as a
probabilistic program perhaps a more
familiar publicity programming language
too many as a functional form is church
so here is a Duraflame process written
up in church the exact choice of program
here is random just to illustrate the
differences between the two so what we'd
like let's look you have these two
different languages which actually
express the model in two different ways
but they also have two different ways of
doing inference
on these programs let's look at that so
how does it inference work in internet
so it's based around deterministic
message passing and in order to me that
work needs to be aware of a large set of
built-in message passing operators
factors like plus minus multiply and so
forth and we support expectation
propagation and variational message
parsing to do that but in order to make
this message passing algorithms work we
need to have a very detailed
understanding of the probabilistic
program we need to know about every
single function that gets cold we need
to know about every single variable and
how it's connected to every other
variable we need to know about every if
statement and function call we're
extremely detailed understanding of the
program advantage of this is that we can
then pass messages up and down that
program in either direction let's
contrast this with with church and the
family of church languages so this here
inference is done by various sampling
approaches ins been a mass of them I
won't attempt to summarize on one side
but crucially the program is treated as
a black box it's essentially only run
forward likelihood free is term for that
um there's some more recent work that
uses a dependency structure but really
it's trying not to know and not to
understand too much about the program in
detail and this is great in some ways
which I'll get to in a second so the
program with anyone from work so let's
look at the the pros and cons of these
two approaches so let's start with a pro
infonet because it's home territory
envelop met is fast we've run out and
terabytes of data it's deploying on
large-scale server farms and it's this
is this is so seriously scalable stuff
and because it's able to essentially we
producers on hand coding machine
learning solutions that you're I won't
code up and the van
of church they're super exciting is that
you can use any of your programming
language elements you're the super rich
modeling language you cannot some
incredibly exciting things um but with
these huge advantages come disadvantage
so as I've sort of suggested in fellow
neck can only do inference on stuff that
it knows about so if you want to do
something new like say fitting the lips
to some point and just see if Annie's
awake yeah and you don't have the the
cosine function built in then you're
sunk I mean you can add it yourself but
it's a lot of work on the car in
contrast church will have no problem
running writing such a program but then
it's very slow to run because
essentially because you're only running
forward it is my crude understanding is
that because you're only running forward
it's very difficult to determine what
some parameters of your program are
likely to give rise particular data in a
highly mansions so the question is is
that some way we can get the best of
both worlds could we get really fun
funky rich probabilistic programs to run
on large scale and at high speed which I
think will be extremely exciting so um
here's an idea that I'm hoping like
going that direction so the earlier is
we mix the two things so what you see
here is some info on net factor graph
and then we're gonna have this red
factor in the middle but the red factor
is not going to be defined and it's not
going to have message operators defined
for it like all the other one so let's
assume all the black ones are things
that in phila net already knows about
and so we know how to pass messages and
the rest of this graph but here we don't
so rather than writing a program for the
whole
will write the program just for this one
factor so here we're going to have a
throw a ball factor and so what it's
going to do is going to take in the
velocity angle and height of someone
throwing a ball and return the distance
that the ball goes so it's a very simple
little function but it would be quite a
nightmare to code up as a factor graph
so what this one slide will tell you the
whole story and then i'll go into as
much detail as i had time for so step
one then is we define an arbitrary new
factor and to put into our into a factor
graph via a forward program what we're
going to do is we are going to use the
same sampling methods of the church
family of languages essentially to
compute the messages going through going
from the factor in all directions up and
down and this is going to take a while
so this is the slow part but what we can
then do is treat those messages as a
training set for some really nice fast
regresar like for example and your neck
or Madame forest okay so whilst this may
take you know well not quite Andrews
billions of years but this year you can
stick this off on a cluster a thousand
machines for a while what you'll get is
a training set that you can then train a
regression one long that will be
extremely fast and then you can use it
so then you can run inference in the
entire model um and it should be fast
and scalable zebra so you need the
regression model to be able to actually
compute the messages in all different
direction yes from all combinations of
the inputs yes the other yeah so
actually it's a regression model / /
outgoing message
okay so let's dive into a little bit
more detail we are looking particular
expectation propagation so here we have
our throw a ball faculty and we have
some messages coming into it and we need
to know the message is going out from it
in particular we're interested in this
message for this slide um so the
messages that are coming here in this
little distributions like here and then
EP tells us to compute the output going
message we need to compute this quantity
so for those of you not familiar with
expectation propagation we must be
multiplying together all the messages
that are coming in multiplying by the
factor function and then projecting this
onto the family of outgoing messages
that we want and then dividing by the
incoming message on that edge okay so
what we're going to actually do is train
on this top part before the division so
this quantity as I say for each of the
operators in internet somebody usually
Tom maker has gone and carefully done
the numerical integration to implement
that that equation as I say we're going
to learn to do it instead okay all right
so um so in this case where we're just
going to use the simplest possible form
of sampling important something but you
can plug in better samples and that all
the Ritz was sampling ideas coming from
from the church community could be
plugged in at this point the largest G
is important something and so all we're
going to do is we're going to take the
incoming message to the factor we're
going to sample from those messages and
then what we need is a proposal
distribution so sorry I don't jump step
so we're going to proposals distribution
the simplest proposed
you could think of is just a sample from
the messages and we'll show you that
that can work in some cases you might
want to change that Faison distribution
and then we're going to be able to
compute this top quantity as an
expectation of those sample messages I
proposed distribution x this which we
can't compute but we're only going to
sample from that clear I've got a
picture so this before i get to the
picture though so we're going to need
though is we're gonna need a whole
representative set of possible infamous
trees that could arrive at this factor
so that we can train on it so we need
something to kick that out so they were
hand-picked some broad distribution over
messages that's going to say what
messages arrive at that factor all right
let's have a picture of this so here are
the four incoming messages to this
factor and so step one is we sample from
messages going in so there are some
samples from the less just going in and
step two is we take those messages and
we pass them through the factor and so
we get samples from the output of the
factor and then we compute the important
sample weights based on the inverse
message and then we so what you can see
here is weary weighted the samples in
this case for this message then you can
rewrite the samples up at the top in
this case for this message then you can
take the resultant set of samples and
fit the appropriate family distribution
to okay now this thing exists already
it's called ABC EP and obviously it
works fine if you can make this step
work but the crucial additional step is
that you then train this very fast
procedure to do to replace this slow
sampling step okay so let's look at some
examples um so what we have here is a is
where we've taken the logistic factor
and replaced it by a short piece of code
that computes the logistic function and
we have a logistic factor already built
an internet so let's see how they come
back so what we have here is um we ran
it in the side justic regression model
and we have here is the KL divergence
the log of the KL divergence between the
true backwards message or lease the one
computed by the Infonet handcrafted
operator and the learned one and what
you'll see is it stimulates the log kale
is generally extremely small in fact you
look at these points that one two and
three that the two curves are
indistinguishable at the very worst
point they look like this so that
occasionally it goes wrong for the
forward message this is the very worst
approximation so on the whole it seems
to have captured automatically what it
took I like several months of a work to
cone up by hand and so we can then use
this in a model so let's put it into our
logistic regression till get some
artificial data just to test this out
and what you find is that the test
accuracies with the neural network um
are essentially indistinguishable from
the test accuracies with the hand coded
factor
life-saving so we're going back to your
previous slide the patient so here what
you've described as the important
sampling procedure that you would need
for one particular value of the incoming
messages yes and so you need to actually
repeat that yes for for a lovely
representative semester so this is what
this takes a while yeah so mapping is
from message parameters from actual
message miraculous actual misters to a
dramatic so that's how if I wasn't clear
so you might have like a million
examples of incoming messages and their
corresponding output outgoing message
and they need to learn that mapping from
message parameters to method parameters
for most you couldn't last better say
yes but often that's kind of a nice
well-behaved mapping the assumption is
that it's a well-behaved mapping that in
some sense even though you will have a
rich complex program that that mapping
is going to have it lower statistical
complexity that can be picked up by a
suitable function very abrupt things in
the mapping your smoothie over them by
having randomness in company the
messages are distributions and so I
think you still need to have abrupt
changes in the mapping not something
we're still investigating and actually
cut that bit but I can talk about that
offline but that's why I think we're in
forest might be more useful the neural
Nets because they're better able at
having some more abrupt sort of phase
changes in the mapping I can show you
some mapping pitch isn't okay um and
similarly on real data the test accuracy
is slightly lower but only very very
slightly could be within noise only one
of the classic you see I'd a set let's
look at another factor so this is the
compound gamma so another example
example of why this is useful is that in
infant net you have this a building
block plus minus x gaussian and
you've only have some factorized
variational approximation on these so if
you could somehow merge them together
you'd get a better approximation so here
the compound gamma is actually a small
structure in the graph it's a gamma
pointing at a gamma and this gives you a
nice heavy tailed distribution um and
sorry jump ahead and so this is a very
useful thing to have in your modeling
tool box and we can replace that whole
graph structure by a single factor that
does the same thing and so we this other
sort of potential which is as well as
automatically getting sort of new
factors we can take existing some sub
chunks of the graph and replace them by
one factor and hopefully improve our
inference accuracy so here's a here the
point I wanted to make about this factor
if we use the Q distribution that was to
propose a distribution for samples that
was just a product of the incoming
messages then we can sort of miss bit so
here this is the log of the sum of the
importance weights from the important
sample is an indication of the of the
quality of your of your samples and
you'll see that there are sort of gaps
if we what if we robusta phi that
proposal distribution a little bit by
broadening it out essentially a mixture
of the original with a broader version
then we can overcome that problem which
is shown down here and if we look at the
message surfaces here so what you
basically want to see is that in this
case the robust if there excuse me this
is the shape and rate coming from the
robust important sampling and then the
corresponding shape and rate as
predicted by the neural net and the
black dots so show where for a typical
experiment that we ran the values that
were actually used okay
I'm going to skip to the ball thrower
the final minute so we started with this
and so here is a nice example of putting
in a custom factor the bullfighting
factor inside a a model we also built a
regression version of this so here we
have a bunch of people m people each
thurible t times and we want to know
that's an underlying typical velocity
angle and height that characterize how
they throw balls from the three examples
and so what we have here are after 12 5
and 10 observations for two different
people how the posterior is for the
learn factor compared to important
something in the in the entire model
then you'll see that they're they're
matching up obviously this took much
longer if we didn't have that if you
penalize important something to have the
same compute budget it does much worse
okay so this is maybe not be prodigious
yet but we're heading in the right
direction so what we're very keen on
doing next is exploring random forests
and certainly on that's the other thing
is that the minute you have to sort of
say what the distributions are the
messages are but you can imagine doing
this in place so imagine you just create
this factor you actually want to use it
you stick it in your model the first few
iterations is incredibly slow mitosis is
the sampling and then it's got enough to
learn the fast regresar learns it and
suddenly everything goes fast it will go
fast from then on oh yeah so it's really
was human like like when you start doing
a task it's slow and then it gets faster
and as I say the goal n is to try it on
larger and more prodigious probabilistic
programs thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>