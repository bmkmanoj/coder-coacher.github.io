<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Inverting RANSAC: Global Model Detection via Inlier Rate Estimation | Coder Coacher - Coaching Coders</title><meta content="Inverting RANSAC: Global Model Detection via Inlier Rate Estimation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Inverting RANSAC: Global Model Detection via Inlier Rate Estimation</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YWRrlRVRAkk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
good morning it is my pleasure to invite
Simon Corman to give a talk here at
Microsoft Research Simon is finishing
his PhD at the tel aviv university under
the advisement of Professor shy of Eden
he was also doing an internship here at
Microsoft Research so Simon okay thanks
very much for having me this is the
topic of the talk today inverting
ransack this is a recent work that will
appear in cvpr joint work with oil it
man Alex Bronstein and shy of Eden so
before I get into details here I'd like
to give the context and just say
something about other things I've been
doing during my PhD so the problem i
first started looking into is of
computing an approximate nearest
neighbor field between the pair of
images so that is you need to find a
dense mapping between the entire set of
patches of one image and the entire set
of patches of the other image so this is
a problem of high complexity because the
number of patches in each image is like
the number of pixels so you need to do
some approximation here you probably
know of the patch match method that came
out in 2009 which really made this task
possible in in a couple of seconds I'd
say so we came with this approach
coherency sensitive hashing where we
incorporate a hashing scheme into this
problem and we managed to do so to find
a matching which is more accurate and
faster than what patch mature can do
later on and this is joint work with Al
here we looked into the same problem
but when we have the additional
information of a depth channel of the
image and we asked ourselves whether we
can do a better so again just in RGB
matching between patches if we're given
this additional depth information and it
turned out to be that the the answer is
yes we have a way of using the depth
information to kind of rectify in some
way the patches and reduce the problem
to a problem similar to the original one
and as an application we focus here on a
single image denoising and we show that
when we use patch based de-noising
methods which are fed with our patch
matches we get results that are
significantly better than
state-of-the-art in single image
denoising but again compared to methods
that do not use death so we really
managed to show that using diff makes a
difference here and now to this is the I
think the main line of my works so and
you could put them under this frame of
global model detection so these are four
different problems I'll just go through
them quickly so the first one is about
template matching under affine
transformations the second one is about
finding global rigid symmetries of 3d
shapes the next one is and this is the
topic of today's talk about registering
a pair of images given a set of matching
interest points which contain noise and
outliers and finally there's this work
here about depth extension and it is
done by designing a special kind of
template matching that is specific for
this task so this is still work in
progress and what is common to all these
problems all the way that we tackle
is that there is some very large search
space in which we want to find a model
and what we show in different ways here
is that if you have an understanding of
the error function the way it changes
across the search space you can do a
kind of an efficient exhaustive search
that gives you global guarantees about
the result you obtain so let me get into
into this work so I'd like to start with
this very simple canonical example in
robust estimation which is fitting a
line to a set of points so in this case
the the line is the big model you can
see typically what you have is a set of
in liar points these are the red points
which have some amount of noise around
the model and in addition you have
outlier points these are the blue ones
here and if we count the number of in
lies in this example we can say that the
in liar rate and we market by P star is
in this case 8 out of 17 there are 18
lies out of a total of 17 points now
what would be the standard way to find a
such a model behind a set of points so
you would use kind of solve the problem
of maximum consensus maximization and
the standard way to do that is ransacked
so you're looking for a model that has
the highest support of data so in this
case what is ransacked to it's an
iterative method at each iteration it
picks at random and minimal subset of
the data that can generate a model in
this case it picks at random two points
looks at the line that goes between them
and measures the amount of support and
how does it do that it's given an inline
threshold which we mat by our star and
it counts the number of points within
that amount of error okay so this is not
a very successful choice that gives only
two in
but repeating this process you can get
better and better lines like foreign
lives here for instance this line was so
the two points that were chosen were
actually in lies but still the line that
went through them is not perfect it's
close to the true model but not there
yet so this has gives five in liars and
eventually you might find this this line
that explains the complete set of in
lies in this example now our method
works in a very different way and we
call it gmb global model detection so
what we start with is as I said it also
suggested before is we find we look at a
sample of the complete space of solution
so in this case we were looking at a set
of lines so for instance we take this
sampling of horizontal lines we look at
different orientations and once we have
this sample of lines the second step is
to directly estimate the inline rate p
star okay now assume we've managed to do
that and we somehow know now that p star
is 8 out of 17 we can go to the third
step which is to find the best line for
this specific in high rate p star so we
can look at each of the lines you know
in L set for instance this line here we
look at the eight closest points so
sorry this would be the eighth closest
point and it gives some amount of error
and we can measure this over the entire
set and I will show that if we will turn
the best line that we found we have a
guarantee on the error that it gives
compared to the best line in the
continuous space so this is something we
can do but we go further and we can
employ here a branch-and-bound scheme
again using these exact guarantees and
actually get as good as an approximation
as we want
and we can actually find the optimal
line for this specific enli rate p star
so this is just an example of another
line and finally we will find this line
which has this error which is optimal
for for this p star equals eight of set
out of 17 so I will talk about the
guarantees that allow us to do this but
the main part of this work is actually
going back to step 2 which is how do we
figure out what the entire eight p star
is so getting back to step 2 and we can
we notice this very interesting
phenomenon here if we look at this best
line for the true p star what we notice
is that the error that it obtains is
very rare and what do i mean by that i
mean that if we look at different lines
in the space it turns out that there are
very few lines or the portion of lines
that have an error that is similar to
this one is extremely small okay so you
can imagine if you take different lines
different orientations or locations
there would be very few lines that gets
get this kind of error for the eighth
closest point but what if we had a wrong
estimate of P star so for instance if we
had an underestimate like five out of 17
so this is the actually the best
possible line for the fifth point but
here it turns out that there are many
other lines like these ones here that
have a very similar that have an error
that is very close to the optimal one
that i just showed and these lines are
close to be to the real model but
actually it might even happen that a
line like this one which is very far can
also have a very close error to the
optimal one and if you take an
overestimate like 11 in liars you have
the same behavior this is maybe the best
line for 11 but there are many lines for
11 points that have a similar error
so what we get overall and this is just
an illustration but this is the behavior
we we see is that if we look at the
x-axis at different possible enli rates
and for each one we count the number of
we call them good good lines or good
models so good lines are ones that have
an error that is not much worse than the
best one possible for the specific in
lie rate so we see this kind of behavior
so this this curve has a clear minimum
at the true in my rate p star so we call
this our IRA mejor I'll define it more
precisely and we basically compute it
for different percentiles and choose the
minimum take P star to be the one with a
minimum value the entire array given
line you find the end closest points
like you find the eight closest points
of the measure the some spray
differences or something so we actually
work with the distance of the the eighth
closest point not the average or
anything like that just we just take the
percentile of the specific percentile of
errors with respect to the model so in
this case this is just a simple case it
will be the the distance of the eight
closest point this is of the eighth
costs mover ik right okay yeah so you
basically forgiven a for a given
candidate line you can just tabulate all
of the distances it has its sort them if
you need to but maybe you know that the
sort and just basically compute what the
thing is okay right because that's
exactly what we did yes this is this
curve very sensitive to how how densely
your sampling your kind of eight lines
so I'll get to that so we havin a
theoretical analysis of of this
phenomena when it actually we give
conditions to when it actually we can
prove that it actually happens but we
see that in fact
it's so strong that it happens even with
very a course kind of sampling of the
space so show real examples and we saw
this behavior across different kinds of
data transformations it's it happens
it's a very strong thing so getting back
to the problem that we're really looking
into so we have a pair of images and
we're given we're looking our model is
some transformation that map's the first
image into the second one in this case
where we're going to work with kamagra
fees and we're given a set of point
matches so those matching interest
points between pairs of matching
interest points between the two images
so each match point M is the point Q 1
in image 1 q 2 in image 2 and in this
example so you can see the true
transformation that's the one that maps
image 12 this pink quadrilateral and we
colored the the matching points the blue
ones are in liars the red ones are
outliers and this is a real example that
we obtained by matching a shift points
okay and in this case like we had before
a point and a line and the distance was
the error in this case we define the
error of a match M with respect to a
transformation T this is given down here
below so if we have the match q1 q2 q1
and q2 this is the the match itself and
some transformation T so the error is
exactly this distance in the second
image and this is the standard thing
that also ran soccer would work with
okay so now if we look at the ground
truth transformation we can look at each
and one of the errors of the matches
that we have so say we have here two
hundred matching pairs and and say we
sought these errors from the smallest to
the largest it's very interesting to
look at the distribution of these
errors and this is what we do this is
just an illustration but we plot here a
curve which is the CDF of this
distribution so starting with match
errors that are close to 20 down here
and the higher you let the match error
ago you can accumulate up to one hundred
percent of e of of the matches and this
is up here okay so this is also a
typical shape of this CDF so there would
be very few perfect matches even they in
lies suffer from some noise and usually
you do see at a certain point this kind
of plateau in the CDF which kind of
tells you that even if you let the match
error grow significantly you won't be
finding any more matches and this is
usually the area of the true in my rate
so just to compare ransacked to our
approach looking at this illustration so
what ransack does is that first assumes
or is given usually and what an in liar
threshold so this is this point down
here and then it tries to find a
transformation that maximizes the in lie
rate with respect to this specific
threshold so trying to find a
transformation with as many as possible
matches with up to this error and if it
succeed succeeds it will find a point
that's at least at the height of this
CDF and you can run ransack with
different thresholds you if you're more
generous give it a higher threshold that
would find a higher amount of in lies
now our approach really starts as I said
with the estimating the percentile of
matches of interest and then we find a
transformation that minimizes the error
with respect to that specific percentile
of matches
okay so now back to this real result
this is what we get so down here at the
bottom this blue curve is exactly the
IRA mejor that I was talking about and
on the y-axis this is shown in log scale
so it's actually a very sharp minimum
here so in this example we find a
minimum at thirty percent thirty-seven
percent of the matches and once we have
that we can we look here this is a el
search scheme that's for thirty-seven
percent finds the transformation with
this with this amount of error and you
can see here also ransack results on
this example okay so this is basically
the idea so again to talk about some
prior work so most methods tackle the
consensus set maximization problem which
is given a definition of what it is to
be an in liar to find a model with
maximal consensus and ransack is the
main technique used here starting from
the original scheme there have been
these are only a few of the many
improvements and extensions they were
over the years which actually made it
much more robust made it run faster with
different very good heuristics and we
will be looking at this recent work from
Tammy to South 2013 which is called
Universal ransack which kind of
incorporates all the good extensions
that have been proposed over the years
it also comes with a very efficient
source code so we looked at that and I
should mention that there are four this
consensus that maximization problem
there are global optimization techniques
these some of them so these are methods
that actually guarantee that you find
the global best transformation the
one that finds the real maximum number
of in lies for a given threshold but
these methods are they're interesting
but as you go higher up with the search
stay sighs so like with homography
they're not very practical so now let me
get into our method so basically we have
these three set steps we sample the home
ography space then we using that we
estimate the in lie right p star and
finally having p star we can find the
best transformation for the specific p
star so how do we sample homography
space so i'll start with some
definitions so this we've seen already
the error of a match m with respect to a
transformation T as i showed before is
this distance in the target image
because we're now going to to sample the
space of transformations the space of
thermography so we need to define a
distance between the home agra fees or
transformation so what we use is this L
infinity kind of distance where if we
have transformations t1 and t2 we look
we go over all the points in image 1 and
the distance is the maximal difference
in the target locations in image 2 ok so
this is an l infinity kind of distance
between transformations and one more
thing this is the Sampson error it's the
standard way of so when you come up with
find some transformation you don't only
want to see how many in lies you have
you sometimes want to see how close it
is to the ground truth transformation so
the Sampson error is the standard way of
measuring this so what you do is you
take a large set of points or maybe the
entire set of points in image one and
you map them with both the ground truth
and your transformation T and you take
the average distance between the target
locations in the second image so this is
the the Sampson error ok so
justly so this this the the maximum were
kind of won't tell you the whole story
of what the difference is like the
maximum we use because we are going to
be using it to bound how much the error
can change its kind of a worst case to
give worst case balance on the change in
the error but in the end you'd like to
know over the whole image how close you
are to the ground truth so now how do we
sample the lomography space so we build
this sampling that we call s epsilon we
have this precision parameter epsilon
and we do this so if we're looking at
images i 1 and i 2 we impose this
two-dimensional grid of points over the
space of image 2 so the diagonal
distance here is epsilon and the
transformations that we allow are
exactly the homography is that map the
four corners of image i1 on 24 points on
this grid of points so for example you
can see this green quadrilateral is a
legal one because all the corners are on
grid points on the other hand these two
red ones or not because they each have
at least one corner that is not on a
grid point but on the other hand there
are in our sample transformations that
are very close to these ones that are
not in a sample okay so here too and
formally what we get is an epsilon cover
of the space of thermography that means
that any harm ography in a space has an
epsilon close sample in this in this set
that we constructed okay and the the
main point that we get from this
covering is that if we look at a single
match between the two images and we're
looking at some
donation t we ask ourselves what happens
when you change the transformation to
some neighboring transformation to some
other transformation whose corners are
at most epsilon far and what you easy
easily get is that the error of the
match when you move between the two
transformations cannot change by more
than Epsilon this is kind of trivial but
it's it's very important and furthermore
if we don't only look at one match but
we look at all the matches so we can
look at different statistics of the
matches like the average of all the
matches a certain percentile a mean of a
certain quantile all these measures will
also change by it most epsilon when you
move between these neighboring samples
so these facts are very important for us
to give the guarantees on the error and
to allow a branch-and-bound scheme that
is fully guaranteed again okay one other
thing is that may be a nice thing about
this sampling is that it's a rather
uniform in the space of homography sand
when I say uniform I mean uniform with
respect to this distance between
transformations that I defined so you
can measure hear that also the cover
radius and the packing radius of this
cover which is which are not very far
apart so and this is important for
instance when we so we have that I are a
measure that we would like to measure in
the continuous space since we do this
through this sampling we would like the
same thing to be a kind of uniform in
space to represent what happens in the
continuous space okay so that was the
sampling and now technically how do we
use this to estimate the in irate so
we're given a set a list of matches m
and we constructed this sampling of
transformations as epsilon you can
strive to complete oh if there are n by
n great points right so MN grid points
you construct the two to the N
potential holography s yes but again the
idea will be to start with a very coarse
sampling so okay yeah it does go to the
power of 8am Agra feels like you have to
choose the each of the four corners
writedowns it's eight to the well right
oh yeah yeah that's the number of the
points to not not really too I mean some
you want to make sure that it's a
homography and we kind of limit the
scale of the lomography but after that
it is those yeah okay yeah so we built
this two-dimensional matrix error matrix
e where the entry IJ is just the error
of the match mi under the transformation
TJ okay so you can think of maybe a
couple of hundred of matches typically
and many thousands or tens or hundreds
of thousands of transformations so we
compute this matrix again each row is a
corresponds to a match each column to a
transformation the first thing we do for
each column we sort the errors in an
ascending order and what we get as a
result is that if we look at the certain
percentile a row at a certain percent
ILP we get the peak match error for each
of the different transformations so
having that we can now for each such row
of errors we can find the minimum error
and we store this in this vector R min
so our mean holds for each percentile
the best possible error for that
percentile over all different
transformations and now we can count in
each such roll how many of the errors
are at most epsilon larger than the
smallest error we found so how many of
the entries are at most epsilon larger
than the minimal one we stored in our me
and this is the IRA mejor that I was
talking about that
look of that we saw and so we simply
find the minimum of this vector and the
corresponding row is the percentile p
star that we choose ok so it's pretty
simple now once we did this we can
continue to find the best transformation
for P star so starting from the same
point here so this is giving a little
more detail so this is the row that
we're working with say this was the best
error we found so we stored it here in
our men and saying this example that
there were three errors so 1 2 and 3
which were at most epsilon worse than
the best one and I'm at with the Red
Cross all the errors that were more than
epsilon higher than the than the optimal
one so what we can do at this stage and
again we have to keep in mind there the
construction of the the sampling that we
had so one thing is that if we want to
just return this transformation here
that gave the best error this
transformation we know that it's error
is that most epsilon higher than the the
optimal error in the continuous faith
but what we actually do is we we want to
employ here a branch-and-bound scheme so
what we can show here is that we would
like to improve the approximation to
move to a finer sampling s epsilon half
and what we can safely do is any of
these red transformations we can safely
discard in the search space because from
this sampling we know that even if we
open final resolutions around this
transformation the error can not change
by more than epsilon and therefore it
cannot be even as good as this error
that we already have so what we do is we
can focus on these three surviving
transformations and open the finest
sampling only around those and we can
repeat this in a branch and bound
okay so this is the idea and now I'd
like to relate back again to the this
enli right estimation measure and really
the two main insights were so first we
claim that this measure obtains attains
a minimum at the true in my right p star
but another very interesting and
practical fact is that as we just saw
that the branch and bound scheme works
best the most efficiently when it is
given the true enli rate p star and this
are just going back here so again as a
reminder we chose the road to work with
the one with the minimum value here and
this vector our IR a measure is actually
it's a count of the number of surviving
transformations for the branch and down
scheme so and if we look at this real
example again this is log scale so if we
chose the wrong didn't work with this
minimum point but within a different
point the number of transformations that
we had to open in a branch and bounce
game would be much worse so it's a kind
of a lucky situation here okay so a lot
of the paper is deals tries to deal with
understanding this the this minimum of
phenomenon here and we try to analyze
this behavior and give some kind of
analysis about why it happens so we did
this making several different
assumptions which are very I guess
there's a very large gap between what
the the setting that were able to prove
that this works compared to what we
actually seen our experiments so we
believe that it's a much more general
thing
at the moment the proof is quite
difficult even in a very limiting
limited setting so what we do is I'll
just tell you about the basic idea so we
move to continuous spaces of
transformations and matches and we use a
generative process for creating matches
under which we will analyze this measure
so what is this a generative model it's
a i think that the standard way of
thinking about this so how do we
generate a single match n which is a
pair of points in the background we're
given we have some the ground truth
transformation T star and we have some
in my rate p star and we're also working
with an amount of noise our style which
we allow the entire matches to suffer
from so the inline matches also have
some amount of noise which is has a
magnitude of our star so for the moment
assume we take just an arbitrary point
to q1 in the first image so we can look
at the target location second image this
would be where the perfect in liar point
Q 2 would be but what we actually do is
we open a radius of a star around it and
with probability P star we generate the
point Q to inside this circle as an in
liar and with probably probability 1
minus P star it will be in the rest of
the in the rest of the image so this is
the basic model and it's kind of the
standard way that they ransacked papers
generate synthetic examples so this is
what we have and what we get is this
kind of probability space in image to
regarding where the location of the
point Q 2 will be and then if we look at
any transformation T where it maps q 12
we can look at a radius R around that
point and what we get here is again that
if the point q2 is within this red
circle the meaning is is that the error
of the match is at most are and if it's
outside the era of the match is more
than our and we actually define we look
at the probabilities here we can look at
the probability of the error of the
match being at most r and this is
exactly the amount of this the
distribution in image I to that this red
circle captures so this is what we will
be doing and in our analysis what we
when we counter transformations what we
do here is we actually count such red
circles of a certain radius that capture
a certain amount of the distribution so
this is how we analyze the measure so
again the radiuses here correspond to
match error threshold and probability
here is corresponds to so if you
generate many matches what happens with
a certain probability will happen at a
certain percentile of the matches so the
limiting the assumptions we make here
are first of all regarding the noise of
in lies and both in lies and outliers so
an in liar is mapped into the green
circle with uniform distribution and an
outlier is met to the rest of the image
again uniformly we think we can
regarding the in lies we could use a
Gaussian distribution around the center
point which is maybe more realistic and
what this allows us is to when we look
at any kind of area in the second image
to catch to calculate the probability of
a point landing in that area we just
have to measure areas inside the circle
and outside the circle to get the the
probability and also we assume only
2d translations because this allows us
not to assume anything about the
distribution of the point q 1 but only
focus in what happens in the second
image and what we end up getting is
these equations that tie the iron I are
a measure that we call here V with the
different parameters are star p star
epsilon and NP and what we do is we
differentiate this function around p
star to show that that under certain
conditions a minimum does exist so we're
able to show so we can see here in this
this D shaped area in the space of the P
star and our style so am I rate and the
in liar noise in this whole d shaped
area we show that the minimum must exist
under these assumptions so this is the
theoretical analysis and there are many
details in a paper now I'd like to move
to some experiments some of the
experiments that we did so this is one
of the experiments that try to validate
the IRA mejor and what I'm showing here
is two extreme cases of this experiment
so it's a synthetic experiment we
generated point matches exactly
according to the model that we defined
and what you can see here is so images
i1 and i2 this is a very extreme case in
terms of the in light which is very low
its p star is only one percent of the
matches so only the red matches here are
in lies the rest are random outliers and
the true model is the one that takes I
one to this black parallelogram and here
are the IRA measures that we obtained
for this example and you can see that
for different values of epsilon the
minimum happens to be quite close to
one percent which is the true in lie
rate again this is another extreme
example and this one in the sense of the
the amount of noise that the in lies
have eight percent of the image size
which is quite large and you can see
here again that the different curves of
the IRA mejor obtain a minimum around
the true in light which is sixteen
percent in this example yeah clarify see
if I understand on the left side example
since P star is one percent you were
kind of bound to take a low r % because
of the V shape right haha yeah otherwise
just wouldn't work right right so so
these are two extreme cases this one in
terms of pieced out in this morning to
us and after if you take them both to be
extremely low we'll see real examples
that are in that direction but yeah okay
yeah and you kind of alluded to the fact
that you start with pretty coarse
epsilon and i can assume that you then
do kind of a heroic pickle or somehow
you know yeah use them sequential logic
epsilon which are lower in specific
areas of the solution space but i don't
see how you do it so again the branch
advanced game is only in the second
stage for for finding the transformation
basically the estimation of the in i
right p star is from me we usually use
the initial initial epsilon it
practically in some cases we we do maybe
if we see that there that there isn't a
clear minimum we can try again with a
one finer epsilon but we can't go much
refined epsilon too much because the
sample grows a lot so glad so long is a
gassy parameter so in this case in this
case epsilon is not a sampling parameter
but it's the threshold
of what we defined as a good
transformation it's the one that has the
optimal error plus epsilon so this is a
synthetic example we could still sample
it very finely because it's a 2d a fine
but I'll show now the real results on
homography Sui work with a very coarse
epsilon so this is now to move to the
real data we use these two data sets the
Mikolajczyk data set from Oxford and the
pairs of images that come with the USAC
method so to talk about the runtime it
really depends on how how well the
branch and bound scheme works here but
typically i say that it's around maybe
five up to ten seconds for an image pair
we haven't done much to to improve this
it's quite a basic implementation but
again the advantage is that it works on
the full range of possible enli rates
and amounts of noise that we in lies
suffer from unlike ransack and so this
is what we have and practically for
homography so we do use an initial
resolution of around epsilon of around
thirty percent of the size the dimension
of image to which is extremely coarse
but still we were able to find the the
in irate correctly in most cases even
with this very space a resolution
three-by-three grid so it's larger it's
a bit more than 3 by 3 because we also
have to take points outside the image
because of scale but say maybe five by
five six by six but it still gives a
huge number of transformations because
it's thermography but yeah and we did do
some very small changes to to try to
make things
so with when you have a branch-and-bound
scheme there are many ways to to do it
correctly efficiently some heuristics
that can actually help you manage better
the way that the mystery expands we
added here a very degenerate kind of DFS
search where we take the best current
transformation and we just drill down
locally hopefully it will increase the
bound a little when we move to the next
stage there are better ways of doing
this but this is what we did this stage
and also once we find the final
transformation we can do this ree
weighted least-squares optimization on
the the final set of in lies that we had
because even when you find a set of in
lies you would like to concentrate on
the in lies that have a smaller amount
of noise so this is something that like
low Rancic does so we have this as an
option and let's see some of the results
so these are real images from the USAC
data set we have three pairs of images
here for each in the middle column you
can see the in irate measure again the
y-axis is a log scale you see the
minimum the minimum that we found fine
here forty six percent in lies
thirty-six percent and in this case
seventeen percent and here you see the
results overlaid on the ground truth cdf
so you can see Rancic you suck here and
and our result how do you interpret
these green curse or prevent dashes when
you save ransack results these are
ransacked results for different in liar
right yeah so what's how you tell if the
algorithm is doing well I mean these are
just number of matches and the
presumably the the homography that's
estimated right could be very similar
between doing 25 matches and
37 matches right right so those what's
significant release might be quite
confusing because even when you run
Rancic here this is the standard
threshold of ransack assists two pixels
of era you find this another in lies if
you use your 30 pixels that there you
find this amount of in lies but actually
most of these searches will find maybe
the same transformation or a similar one
just you count things here in a
different way so this doesn't really
tell you how good the result is it's
just an interesting way of you get an
idea comparing to the ground truth zdf
but what I will show on the other data
set is what the Samson errors look like
so 41 times for you know the recommended
you check algorithm so you suck it also
deferred it's that there's no one run
time I could I could say because they
have these you know they have a prozac
which is one improvement in low sec so
in some cases so you know standard rents
that would have to evaluate say in for a
certain in irate 50,000 transformations
to kind of get a set of in lies but they
have these heuristics where in some
cases like prozac you rank the matches
by their quality so sometimes they
evaluate 10 transformations and find the
perfect one so it can take fractions
milliseconds to find it transformations
but in some cases when so especially
when the noise is higher and enli radius
is lower it can take much longer and
even fail so that's what we see so the
more interesting data set I think is the
mikolajczak one because you have these
five harm ography sequences each
sequence has frames 126 and you can kind
of see where when the difficulty
increases where the methods are still
work to what stage they still work so
this for example is from one of the
sequences frames one and two so these
sequences are mostly a view
going to change so from frame 1 to 2
there's a slight viewpoint change you
can see so there is the ground truth map
it's like a dashed thick green
quadrilateral and the pink quadrilateral
is our result of a leg on that so you
can see that it looks right in this
example but again this example you can
see that all the matches I think one
hundred percent of them are in liars
because sift just worked very well here
so this was very easy but as you go
forward in this sequence I hope you can
see that the number of the percent of in
lies drops drastically and what you
can't see that even the in lies that you
still have here also have a considerable
amount of noise so they're not localized
exactly and we were kind of surprised
because people think of sift as like you
could can control how much the
localization error can be but we found
that it's a much more than what people
usually work with so these are some
other examples of these are frames one
and four from three of the different
sequences and again the same kind of
results that i showed before and now if
i move from frames one and for two
frames one and five these are frames one
in five so you can see that the in lie
rates drop here in this example with
only 30 thirteen percent of in liars and
these are frames one and six actually
this is one of the only examples that we
totally failed and this is due to the
fact that there are only two correct
matching sif points in this example so
sifter totally broke down here we
couldn't do much so really looking at
the these are the Sampson errors of both
our method GMD and you sack so we're
looking at five different sequences and
in each sequence the five different
image pair of combinations so you can
see here the Sampson errors which are in
pixels and if you look carefully so in
most cases i think the the methods are
comparable they do quite well and i
think even the ground truth
transformation is maybe not doesn't have
sub pixel accuracy so being around one
pixel error is I think just fine but the
interesting thing I think is looking at
these two sequences where at a certain
stage yousuck fails earlier than we do
so in these two examples we still
managed to find a transformation which
is not too bad compared to their failure
and just to look into these two examples
so now these are the two sequences
graffiti and graffiti five and we
compare the two methods in terms of the
in light that I find and the amount of
noise that these in liars have so this
is just an example but the two cases
where they failed and we still succeeded
you can see that they're very extreme
both in terms of P star which is only
twelve percent and nine percent here and
also the in liar noise which is kind of
related to our star so this a star is
also very high here so 8.5 with standard
deviation of 14 and this example 2 and
down at the bottom here we can see for
these two sequences again the ground
truth for CDF's of the errors with our
result overlaid on each one of them so
you have frames the red one is frames 12
frames 13 and so on so these two
difficult examples are this one down
here and this one here where you see
that if you look at the CDF there are
very few errors that have very few
matches that have an error that
below savin 10 pixel so these examples
are really very challenging and we still
managed to get a result the use of
higher than 0 yeah we're a music with
like in different steps of thresholds up
to I think 40 or 50 but the problem with
use that so again it's and I think it's
a mistake in the basic analysis of
ransack where you assume there are in
liars a certain amount and your only
goal is to manage to randomly pick a set
of points that are purely in liars but
what happens if and I showed an example
before that the set of in lies that you
pick still has some amount of noise
you're not guaranteed using that to find
if the amount of noise in the in lies is
high you won't always find that the true
model so i think this is what happens in
these two examples here ah right first
you don't have to find a minimal set you
can just find the in liars and then do
it these squares or early waited yeah
yeah that's what we do okay so i think
i'm done with that I'd like to say just
a few words about the first match work
how different it is a compared to this
one so just in a couple of words because
I think it's interesting so so there
it's the problem looks very similar
because you're trying to search in
transformation space mapping the first
image into the second one so we worked
with affine transformations but you
could we actually generalized it to to
home Agra fees just like we have here so
it looks very similar but what is very
different is the error function that you
work with so this was a pure kind of
geometric problem because you you work
only with locations of these matches and
in template matching you work with the
pixels the pixel intensities themselves
so we work with the
sum of absolute differences of when you
met the template into the image you take
each pixel and look at the difference to
the target pixel it's a map to so again
we have this idea of a sampling of the
space but the and things are much
simpler because we don't have this we
didn't handle this outlier problem that
we had here so we consider all the
pixels in the template as in liars we're
working with a hole with everything so
in that sense it's much easier than this
example but on the other hand so when we
had this sampling we worked with here
and the main point was that when you
move between neighboring samples the
error does not change by more than
Epsilon that was very easy to was
trivial to get here but what happens
when you do that with the sum of
absolute differences it's not true so
even if you change your transformation x
1 pixel the error can go anywhere so we
don't have that guarantee but what we
did manage to do there is to show that
when you change your transformation the
error can actually not change
arbitrarily but it can change as a
function of how much the transformation
is different and also depending on the
smoothness of the template so it turns
out that the the smoother the template
is all the image is the change in the
error is much smaller and the more
texture the template is the era can
change in a faster manner so we we
managed to prove their exactly bounds on
this relation on how much the error can
change as a function of the changing
transformation and the smoothness of the
template and based on that we can
determine the exact density of the
the sample of transformations that we
use and we have the same kind of
guarantees day so we again we do the
same branch and bound scheme and we
managed to get results i'll just show
i'll skip these but sorry so these are
kind of results that we get this is a an
image we extract a random template and
then search for red back inside the
image and it works very well these are
examples with kind of a large template
with respect to the image but we looked
at different sized templates going down
to very small dimensions of templates
and what we show that so we compare our
method to a interest point based a
method like a sift base matching method
and it turns out that really the smaller
the templates are you can see that a
feature-based the methods just break
down so when you work with small
templates like these are twenty-five
percent of the image size here they just
shown larger but actually this is the
where it comes from so this is
twenty-five percent in these examples
fifteen percent and so when we go down
even to ten percent of the image so you
can see that when you take templates of
this size it's almost impossible to
think of trying to match feature points
between the two images so actually I
think I don't know of any other way to
to tackle this problem especially as the
templates get smaller so I think I think
it's a very effective method for doing
that at all scale window template and
the escape you assume the jury's scaling
in half or so again we we were in this
setting we worked with the
affine transformations and we we limit
the scale to say up to x 3 and 1 over 3
so yeah you have to limit your search
space in some way but still it manages
to do things are quite quickly here you
can see one more thing that special
thing that we use here that instead of
computing the when you map the template
into each of the thousands of locations
in the image because you're willing to
suffer this approximation of say epsilon
so you can only instead of mapping the
entire set of pixels you can only choose
a random can choose only a random set of
pixels and evaluate the error only on
those and get the same kind of guarantee
so we we look at actually a constant
number of pixels in the template sighs
it doesn't depend on the size of the
template and evaluate the error only on
those specific you can see these pixels
here so this really together with a
branch and bound enables to speed up the
whole thing and it works quite nicely so
just to summarize now so these are some
other examples so we believe that some
different large spices search spaces can
be exhaustively researched through if
you have an understanding on the
behavior of the error function and and
if you do so so so you get global
guarantees on an approximation and you
can then usually also use a branch and
bound skiing to to get a very good
approximation so really there are other
domains that we've we were already
looking at applying these this technique
and also the robust estimation with the
inlay rate error measure we think would
be interesting to look at other domains
like in even machine learning where you
specifically want to take into account
the fact that there are in lies and
outliers that you'd like to totally
reject and not just use some robust
robust measures that can limit how much
they can ruin your results but actually
work with the try to isolate the in laws
of the problem so we're thinking in
those directions too that's all I think
any questions do you have will there be
an indentation so there will be for all
my work so their ends up being and
implementation available for most of
them there is this latest one with the
in my rate is not available yet but
hopefully in a couple of months ago when
you're looking through the space of at
my transformations you said for instance
that you will eliminate the scales
beyond 3x + 1 over 3x yeah mother could
you have kind of saved probabilistic
priors on the affine transformations and
with that help at all of you which ones
were more likely to look at before
others so like cousin but you know let's
say like a a 1x scaling is twice as
likely as a 2x scaling for instance you
can imagine that distribution so I would
imagine how you could do things to get
your result faster but in terms of if
you still want to keep the guarantee on
the global search so probably not but
practically yeah I guess that you would
prefer the sample more densely to start
with in the areas that are more likely
estimating the Wraiths of in liars if
someone gave you if you started to check
with some guesses which are
close to the minimum you will find
faster right and then you could maybe
order the branch and bound according to
what expected expect to see so I'm not
sure I exactly get how that can be done
but I think that so one of the things is
that we we do the in my right
destination first only at the first
level and then continue with that there
should be a way of kind of running
things together so like maybe from the
first level getting a course idea of
what the in my rate is and then also
refining that moving to the next data so
possibly that could be done
right very nice thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>