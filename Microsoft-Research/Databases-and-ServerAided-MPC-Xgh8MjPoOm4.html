<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Databases and Server-Aided MPC | Coder Coacher - Coaching Coders</title><meta content="Databases and Server-Aided MPC - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Databases and Server-Aided MPC</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Xgh8MjPoOm4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so hello everybody welcome to the
session on the MPC for databases and
this will be certainly the best session
because now you are talking so we have
the first talk that will be given by a
while collision occur and dull marking
and that they will be talking about
blind seer scalable private database
query alright thanks so this is our work
of the past couple of years with pretty
large team mostly people from Columbia
solder than towel is Steve Bella wins
and go joy been fish Angeles keremeos
Fernando Krell vasilis past papas and
info and people who did summers or some
longer stays with us in Wesley George
and I by Kumho subramanian and of those
Ben and Fernando are here if you also
want to talk to them so this is this is
a project on private databases as part
of the work done by our plan so the both
towel and I will talk about the project
about what we did so this is roughly how
we assign what we're going to talk about
and just in a few words i'm going to say
what the project is about then that will
cover the basic algorithms you know the
system architecture and the basic
algorithms and i'll talk a little bit
more additional features such as
protection against malicious players so
we'll see i guess we expect people will
ask questions as we go so we'll see how
much material will be able to to cover
so i was wondering acquired as I r /
cares about this and you know this
project seems to be very probe privacy
and last year in the workshop those
workshop damage or chip on cryptography
in in AT&amp;amp;T and Tim Edgar who was given a
token turns out there is a guy who is
called deputy
for civil liberties for the Director of
National Intelligence so there is it
looks like there is some effort to you
know to to care about people's privacy
at every level of government so this
project and the stock what what we're
going to talk about so we're solving a
specific problem so it's kind of MPC but
so it's a specific problem but a general
MPC will be the main tool here so this
is kind of how this related so our work
was we recently notified that to appear
in Oakland so the mostly what towel will
be talking about that's the content of
the Oakland paper and this is what this
project is so this is so this slides
intends to give you the scale the
features roughly what what what is the
goal of the project so it's it's a
medium sized database for today about so
100 million records 10 terabyte total
size and we want to do a real life query
so very rich query set should be
supported so you know general boolean
formulas range queries all sorts of
tricky things and more this is just some
of the things that that we're dealing
with and the important requirement is
that it has to be really fast we were
allowed according to the contract
according to the BAA extremely small
overhead compared to the plaintext
evaluation of the data so 22 maybe
factor 10 and overhead compared to to
mysql so this is a kind of introductory
project setup and towel take over
all right I want to talk about the
basics of the project so first of all
these requirements like kind of send
them very quickly but they seem very
hard to achieve maybe even impossible
and I just wanted to comment a little
bit so for example when you die said
that maybe it's hard to do then he took
it back maybe to do massive data sets
indeed it seems very hard and there are
some theoretical impossibility for
example it everything in first secure
computation has to be at least linear in
the size of the of the data for
interesting computation and we can't
afford it for large data so it's clear
we must relax security guarantee somehow
and this is so what we did here we try
to find a good way to relax security
guarantees while achieving all the
requirements in the performance and
doing it in a good way and the question
is how and so for example one way to
break the linear bound is to do secure
computation with all Rams right like we
did we had a ccs 2012 paper on it and
there are many follow-up work and
marianna we'll talk on some other work
on that approach so that could be seen
as an example where what you leak is the
runtime you I'm willing to relieve the
runtime then I'm willing to do amortized
pre-processing it's no longer you no
longer have to be linear but even that
is way way way too inefficient for you
know doing twice as fast as you know the
database queries that are not secure so
I am NOT going to talk about all of this
this is the open area of research that
we haven't completed and very
interesting is how to do what does it
mean how to find the right trade-off ok
this is just a bunch of questions I
wrote what we want is to find something
it has to be meaningful and reasonable
security whatever it is has to be
provable and I think both David
mentioned something like that and it's
dark and it came up with other talks
it's very hard to tell it's a I wanted
to encourage you to try to work on it
even from an either a theoretical
perspective or a practical perspective
how to evaluate is this leakage how is
that ok here is what we leak is that too
much is that too little how do we
formalize it how do we say whatever it's
good or not
so we did not solve all of that what i
will do is I'll describe our system the
basic system and approach I'll touch
upon some of our trade offs okay and I
you know to and we don't have any form
of justification why these trade-offs
are okay I'll try to talk about it a
little bit and I'm really skipping most
of our system because given the time
there's no way to really describe
everything we did but i'll try to touch
on some interesting things i'll focus on
boolean queries only and i'll focus on
semi honest parties I've Vlad will touch
on extensions we did many other kinds of
queries we have policy which I think
Vlad you didn't mention we have a policy
or I will mention in a minute so I'm
skipping a lot of stuff in terms of the
technical part but i'll try to touch on
the interesting things let me just add
one thing about what you're allowed to
leak and what not it seems so our system
is searching a database with complex
queries it seems maybe it's okay and
necessary for performance to leak access
patterns right so maybe somehow you know
you want if you have if you have one
quarian you have another query if you
use it seems justifiable that i would be
willing to leak that my first query in
my second query had word the same for
example this is not acceptable in
standard crypto security but we will
leak it okay but we will look a little
more than that and I'll show what okay
so let me start with the system
architecture there are four parties here
let me start by saying that this
therefore logical parties we really need
three parties the query checker could be
the same as the server but the system
works like this so there's the client
and there's the server the server has
the data and the client wants to search
so already one compromise towards the
trade-off is adding a third party of the
index server this was already in the BAA
from the government that that helps a
lot to reduce the performance cost so
this index server is not trusted with
neither the data of the data of the
server nor with the queries of the
client is not allowed to learn either of
them we want to protect both privacy but
he's trusted to behave honestly and
trust
not to collaborate with the other
parties so the way the system works is
first there's a set up pre-processing
stage where the server and index server
construct an index of the index every
will have the encrypted database and an
index structure on top of it then when
the client comes online he has a search
with the index server while he searches
there is the policy the query checker to
check the policy if the query does not
satisfy the policy then the client will
receive an output that there are no
matches found okay he wouldn't know
whether the policy failure there are no
matches found and at the end if their
match is found he gets the encrypted
records from the index server and
finally has some correspondence when the
server to decrypt those records and get
the actual answers okay so that's the
general architecture yeah so the server
what does it do first it has the setup
which is said the index and the
encrypted data structure and then
encrypted the data at the index server
first and then at the end he interacts
with the user to let give the user keys
to decrypt the output because the user
gets the encrypted records from the
index server now we need to decrypt them
the client okay so I'll just describe a
few tools that we use so we'll use the
yao's garbled circuit and let me just
remind you how that works that was a
joke we've seen it ten million times but
we will use that the point of this is
very very fast for small problems right
for small circuit and the question is
how to scale so this is just some
approach that we did is you know we try
to identify privacy critical subroutines
implement them securely using yell
garbled circuit and then insecure
implementation of the rest and then
analyze what you get out of it okay so
good and it's a hard challenge to
understand and formalize what the
security guarantees are good so this is
what we did I want to know that of
course you cannot apply Yau on the whole
data okay the data is ten terabytes okay
so
the other the other tool we use is a
bloom filter data structure that's a
data structure that allows in a constant
loves constant insertion and constant
search so basically you have a bunch of
hash functions and for every element you
want to insert yeah you have you
get a bunch of you know 20 indices and
you send them to one and if you want to
search you check whether all of them are
one and you get it and it has a certain
false positive right no false negative
and you can choose the parameters of the
defaults positive rate is small as small
as you want and so we all talk about
encrypted BF will want to encrypt the
the terms that we put in so we will use
something called occluded VF which is
exactly the same as BF it's the same
data structure bloom filter which is I
of course needless to say a very useful
data structure that appears all over the
place the bloom filter we wanted to hide
it and let me try to say a little bit
about why so the point is the index
server will be holding a bloom filter
you will see how it works the index
server will be holding a bloom filter
and the client will try to find out some
matches on it and we want to hide from
whoever's holding the bloom filter
whether there was a match or not ok we
don't want him to know whether there was
a match or not so one way to do it is to
apply yow on the whole bloom filter but
we don't want to do it because the whole
bloom filter is too big so what we will
do is will apply yow only on the
relevant indices but the actual content
of the indices will be masked ok with a
one-time pad with a with a pseudo-random
function ok so the what the index
overhauled for each blue filter will be
the original bloom filter that we want X
sword with pseudo random function with a
string generated by sedo and a function
that the user has the client has the key
for ok and this is very amenable to
efficient yeah because it's XOR yeah ok
so let me so this way so I said yeah I
said loop filter let me just show you
the
all data structure of the that the index
server holds so first of all the index
overhauled encryption normal good
encryption of each record of the data
and in addition to that it has a tree of
bloom filters okay and the tree work
works like this this on the bottom level
which I guess right is the bottom on the
bottom level there's a bloom filter for
each record and that basically contains
all the key words in that record okay or
you know the key words and the column
name everything corresponding to that
record so we have a bloom filter for
each record then at each point of the
tree you insert all the terms in all the
records in the subtree under it okay so
that's the data structure that involved
and now when the client wants to search
for whether some some key word exists in
the which records match a certain
keyword or a certain more complex query
over keyword he will start by executing
by searching the topmost the root bloom
filter to just check whether it's there
and the way he searches to check whether
it's there this is a secure two-party
computation using yow okay so depending
on the query so he asks the question and
then if the answer is yes that means
somewhere in the hole in the leaves of
the tree there are records satisfying
those let me take a concrete example
let's say he's looking for something
like a and B okay so he wants all the
records that have both a and B in the
keyword so if he gets yes here it means
that there are some records with a and
some records would be somewhere that's
all he knows and then he checks the
children and so if this BF says no it
means none of its children have a and B
so it means there's either a doesn't
exist and that that sub tree or B
doesn't exist in that sub tree or both
but that's after we said yes so there's
something there and it goes on whenever
there's a yes you go on and at the end
when you get to the to the leaves if you
have a yes that means both a and B are
in this record and therefore you can
obtain that record okay
so in this in general you can search
keywords or any any complicated formula
this way I'm going very fast but if you
have a question please stop me to look
at the time hold on i'll start the timer
okay i just don't want to go over last
time okay yeah so this is how you search
good so what is leaked that's that's the
big question what is lik 30 the searches
I'll talk about efficiency later but
clearly doesn't require linear time
right to go over all the records unlike
secure computation full secure
computation but certainly a lot of
things are leaked here right so let me
try to talk a little bit about what is
leaked so one thing that is leaked is
the query pattern because if later the
let's say I come with the exact same
query they will do the exact same path
but even if I come with a query that's
similar so in the first square i asked a
and B and then later I'll come with a
query that says I don't know a or see
the indexer we might see something that
there's some similarity both because the
indices to the bloom filter will be
similar they'll have some intersection
and he will also learn the returned
record access pattern so he will see at
the end even if I ask completely
different disjoint queries they might
have the same matches and he will see
that we got there these things I think
are easier to argue although I don't
know how to formally do so but easier to
argue that are not a big deal and are
necessary if you want efficiency and
it's not a big deal to lick the patterns
we also like the tree search pattern
which I'll talk about a little more so
if this is the tree you know and let's
say my solutions my matches were in five
and six what the indexer versys he knows
what the path was that was taken okay
and that teaches him something about the
query recall that the index server
doesn't know what the data is but still
this gives some information i'll try to
qualify what it is a little bit ok so
what does it mean let me start with an
example for our queries so let me see if
here you have an arc worry you will
never have let me
go back to the previous thing yeah so
this example could never be or it
couldn't be that here I searched for or
because an or there would never be
something where you you said yes and
then you gotta know right no actually it
could be or so forget it sorry I
apologize but okay so but what is true
so for our queries when if I ask is
there a or b on the top if it says yes
then I know there is something there for
sure there is no pruned I thought that
example had a broom prune branch but it
didn't there's no pruned branch ever for
an or I would never say there would
never be a yes and then I go go go and
never get anywhere right it couldn't be
where's for an end you could have you
could have yes there's both a and B in
the leaves but at the end there's no
single record that has both so for or it
turns out that the tree pattern leaks
nothing more than the access pattern so
if you knew and in fact the first time
ever you set up the system you do an or
it leaks nothing other than the number
of results because the tree is organized
the order of the leaves is random and if
you know the number of results you just
choose a bunch of random leaves and you
do the path and that's all that's
revealed once you ask more queries
without reinitializing the tree some
information is leaked about the pattern
and the efficiency for or is
proportional to the number of result
right again as you know the it's just
for every results I mean matching
answers for every matching answers I
have every matching answer I have a path
ok now for end queries so this is
optimal in some way I mean we'll we'll
see real performance results but at
least theoretically it's optimal I must
have at least as the size of the output
right for end queries it's more
complicated both the efficiency and the
leakage are related to each other so for
end it's more complicated the tree
search pattern reveals more because you
can also see you know you see that you
went and went and went and then stopped
and so you know and if you stopped and
never get anywhere then you know that
the search for something where some
leaves satisfied some of the terms and
some leaves satisfy some other terms
with no leaf satisfies both and that
gives information both to the client
into the index server some information
and it's also related to the efficiency
so because the efficiencies how many
nodes we checked every time we check our
know this is a yao computation involving
you know that's that's the heavy
computation I mean it's not that heavy
but involving you know computing a small
a circuit depending the circuit is the
query okay um and some X or for the
bloom filter so the efficiency for end
queries are proportional to the number
of matches for the best term so if the
queries a and B and C and D and a has
one answer and B has seven answers etc
that will be proportional to the small
the term with the smallest number of
matches okay and this is you would you
would have liked something proportional
to the total number of matches but but
you can't and this seems asymptotically
optimal even without security and this
is in particular wet my sick well does
in terms of you know if you have a and B
and C unless you had specific index that
for every three terms it gives you the
efficiencies proportional to the number
of matches for each of them but this
abandoned pets both give you in famiglia
information and make the runtime bigger
okay and there's some sort with which we
have some work in progress proving that
some of this is inherent but in any case
this leaks something now if I go this
with or and end if i go to arbitrary
boolean formulas the efficiency is very
nice the efficiency for any arbitrary
formula no matter how you you do it you
could imagine that you look at the CNF
representation of the formula and you
look at the term with the smallest
number of matches and the performance is
proportional to that or less and what's
nice about it is you don't need to
translate it to CNF you don't need to
know which term has how many matches you
don't need to know anything this happens
automatically by the tree okay in the
leakage is what I said before access
pattern in the tree search pattern it's
hard to quantify exactly how much it is
but it's much less than giving an
information on on patterns of each
individual term but it does give some
information okay good so oh we have a
definition and a proof but their
definition is kind of the definition
exactly so our definition is the ideal
model leaks the the tree pattern I mean
this is what we leak but so we have a
proof that we don't leak more than that
but what the meaning of this is we don't
have any kind of proof what this means
yeah no I don't know anybody janessa oxt
construction yeah oh yes oh it sees the
IBM construction yes I can compare the
leakage I didn't know what oh XD is yes
so that's interesting so this this
project just to give the background a
yard pass bar was given to a bunch of
teams for phase one and for Phase two it
was given to us and to IBM and they have
a different kind of system it would
incomparable leakage so else they're
pros and cons you ask him at liquid
specifically or okay so they their
queries have to be of the form a and
something ok and then both the runtime
and the leakage depends on the number of
matches for a for the first term so this
means both for arbitrary formulas this
might be linear it's not sub linear okay
unless the so the client needs to guess
which term would be the best and put it
first and it also leaks how many results
a had separately which are as doesn't so
this is for their general form life
their formulas on the form a or b or c
they basically just do the search
separately for a separately for be
separately for see they leak a lot of
information on individual terms so we
better than them and that for sure in
terms of protecting individual terms
however they have they have other
advantages over us so they definitely
have advantage over what I'm talking
about and that they have malicious
client and I'm only describing on his
but curious but Vlad will talk about
malicious
okay that's one and also we have a very
interactive protocol at each step so
depending on the number of matches they
are sometimes better performance than us
yeah some noise search very friendly
wesam
from the client point of view what are
you talking about privacy for privacy so
that you could reduce this leakage first
yeah we did and I skipped so many things
so in fact it's kind of related to the
next thing I was going to say but but
let me answer your question more
generally you could do things like a
dummy things and the client searches
more but it's very hard to do it in a
you know you can do it in a way that
will reduce the leakage to negligible
with negligible security so it's a
little related to the next point that
I'll show you which is one specific
example we have many other points
there's anything did I miss something
about comparison with IBM okay compare
yeah it's very hard to compare because
for example the whole aspect that I'm
not talking about at all is policy right
we need to apply policy so there there's
there's you know who so there's do you
need to keep the policy secret you don't
want the client to know what the policy
is can the server know what it is can
whoever checks the net the server the
index server can whoever checks the
policy learn so more what can they learn
about the query is it okay to say the
query asks about this column in this
column in that column or is that not
okay it's very there are many trade-offs
to play and that's I think an
interesting area for research okay so
let me see the time okay I just need to
leave time for Vlad okay so let me just
say this mmm let me just hand wave this
instead of talking really about it but
one thing so we want one specific case
we did want to improve the leakage is to
hide the difference between zero matches
and one match for the index server okay
this was presented to us as a
requirement and we came up with a
motivating example I don't know what was
the real motivating example but the
motivating example we came up with is
let's say there's an airline and you
want to search you know whether the list
of the airline whether the list of
passenger contains some terrorist
and the thing is if you don't want the
so it's not likely that the answer will
be several terrorists right it's mostly
that and the answer will be zero almost
always right but sometimes it might be
one okay and you really don't want the
airline to know whether the answer was
01 okay for example one could cause
panic etc however if we wanted to
completely hide the number of answers
that doesn't seem possible with this
performance okay and you can't just hide
between zero and one with negligible
probability because then you could show
you could come hide any number of
answers so what can you do well okay we
showed that there is you know for any
epsilon that you want none negligible
for anyone of a polynomial you can say
you can tell the difference between zero
and one except for one over poly but
that's kind of not good enough because
what we wanted to add in addition to
that is that even with this small one
over poly probability that's not
negligible if the bad case happens the
news is not terrible it's not awful
catastrophic okay so we we had this
definition of indistinguishability
saying this so let's assume there's an a
priori distribution of what the
probability of one is okay let's say the
airline knows the probability of a match
is whatever Delta and Delta is small
0.01 some small constant what I want is
even in the worst case no matter what
randomness I choose at most they will
double if they knew the probability is
0.01 and then they see the if they see
what actually happened maybe they will
gain absolute confidence that the answer
was 0 I don't mind leaking that but this
the answer was actually one they would
never say okay now I know for sure it's
one it's at most will go up to twice as
much okay so that's I know I'm not
explaining it clearly but I just wanted
to give you an idea that's very similar
to your question because the technique
of how to do it is indeed by adding
extra dummy pads okay with a certain
distribution that for which we can prove
the claim okay so the client what does
it mean adding past the client decides
even if the answer is no the kind the
client decides not to stab but keep
going to a first certain number a random
number of times okay so um I'll give
some performance results
to the end and then Vlad will talk these
performance results are from a year ago
I guess from a long time ago from phase
1 we have brand new performance results
that are not yet in a slight format or
even in any other format for me but oh I
wanted to say um it's a good chance to
say this was a the performance was
evaluated by Lincoln labs and there's a
three people from Lincoln labs here who
did this so I guess it's your your slide
more or less so so I'll talk a bit about
performance from a year ago there's the
much better performance now with
optimizations that Vlad will mention so
this is comparison to the my sequel the
insecure thing this is for single so
i'll just give you approximately this is
for a single keyword search not a
complex query ok so the first all of
these the first four have a single
result is some keyword search where
there's only one matching record ok so
we are not much worse than mysql not
much at all and we have did you see we
have the same performance you know these
are different kinds of queries and we
have the same performance in all of them
because we said this int and star and so
there is never mind so we do the point
is we have for if there is a single
match we have very good performance and
similar to mysql the next one's have
more than one match so you see two to
ten matches in all of them and in that
case we do worse than my more a bigger
factor worse than mysql and we have a
large variance because it depends on how
you organize the tree what what your
lack is there ok this is um ok so in
general very general it when it's very
clear also why if you see the system if
you have few results we have very very
good result the more results there are
meaning the more matches there are for
the query the worst results we have
because we do more and more over the
tree and that involves interaction at
each layer
so when you have five thousand results
this is about 15 times worse than mysql
now we improved it significantly so okay
so and this i think is interesting this
is the boolean query which is what i was
talking about so the first three bars
are conjunctions of two terms a and B
where they were chosen such that a has a
single record that matches it and B has
either 1 or 100 or ten thousand records
that match it okay and as you see even
if B has 10,000 records that match it
and a has a single record that match it
the performance really goes similar to
the smallest term it doesn't matter how
many matches are in the other term okay
and this is to compare with IBM if they
had a and B that would have the same
results as ass but if they put B and a
if they would be first and the result
would be terrible there ok the next the
dnf is a more complex query it's not
just a and B it's a more complex query
in the NF form and it had more result
okay and when it has more results I'm
not sure how many results at hand maybe
the Lincoln life people remember but so
they designed the experiment they had
more results and then both mysql is
slower and we are slower and the last
one is interesting which I don't
understand we need to go look back at it
but the last one is an end of two things
a and B there's range query in there but
the point is they were designed
especially for the worst case of our
system so a has many answer matches and
B has many matches but a and B has few
matches okay and so I understand well
why our system doesn't do well this is
the worst case for our system but it was
supposed to be bad for mysql as well so
that i don't understand and i couldn't
figure it out maybe fernando and then no
um anyway so but this this was supposed
to be pretty bad for mysql as well okay
because they didn't do indices for 42
term
oh good and then so they've land so you
see least es para yeah the exit strategy
search pattern with the tree search
pattern is more information than
exercises so what means there there are
ways of testing of having residues of
destiny
that are not in their own destruction
so you version you yeah yeah so I will
talk about I mean his paper from 2003
right but this is a single keyword
search and we'll do genbook lets me out
to cover other stuff so all right so
don't mention that there's a bunch of
optimizations and the most efficient one
is the most obvious one is the
paralyzation which we didn't do for
phase one and we kind of proper or more
or less properly implemented it and now
it's about 15 times faster so whatever
the numbers you saw they consist of two
parts one is data transfer one search
data transfer didn't improve probably
from the paralyzation but the search
time improved so you can scale the
numbers that you know the bars from soda
for correspondingly using proper
algorithm so yeah we did better bloom
filter analysis a little bit better in a
number that you saw before we each you
know each search included bloom filter
that had ten 2-6 false positive rate but
that was a requirement for the final
output the probability that you return
the run record system 2-6 but internal
nodes you don't need that so by doing by
increasing false positive on internal
nodes we got two things simultaneously
the the we have a performance
improvement because you don't need to
touch on so many bit so the circuits
that will evaluate will be smaller and
second is that will be naturally
introduced noise into the tree search
pattern so now there is less information
leaked so there's the performance and
security improvement from that there are
some things that I guess I was going to
talk about now skip below we could have
tried we didn't implement several
variants of of garbled circuit we tried
and we did get some improvement but not
as much as I hoped or we still expect to
get from information to Radio garble
circuit
the benefits information to very garbled
circuit is that the data transfer is
about three times smaller than the best
state-of-the-art even with two garbled
row reduction okay so there is sort of
code optimization that we didn't much
you know a lot still to do a privacy
improvement so we do so surprisingly
this managing all of this data it takes
a long time so for example the setup
phase of our system takes a day or two
and same for MySQL so it's not it's not
surprising to building the syntheses etc
but so during the execution times you
run many executions and you accumulate
these three search patterns and we
thought what can you do if you can reset
it and it turns out that we can for
cheap we can reset or largely we said
the accumulated information by
rebuilding the this index the search
index structures and it turns out that
we can do it in about 20 minutes so into
to rebuild the entire database so ignore
the slides so a big feature of of the
database management is that the policy
compliance because the query is private
so the server does not see the query but
it wants to ensure that only authorized
queries are allowed to execute or return
data so this is a big requirement from
my our parent we meet it so we need to
make sure that the policy ok so the
security policy secure policy checking
says that the policy rejection should
look like ok the query that you return
the empty result set and it works for us
because we based on garbled circuit and
what you can do sort of the natural way
is that you have a policy checking
circuit that outputs some encoded value
and then you you have a search circuit
and then you just do a conjunction of
those that's so it's all great and then
finally well so there is a malicious
client protection that's a big
feature because you would think so you
know there's a lot of discussion that
malicious garbled circuit is much less
efficient than Simona's garbled circuit
and we deal with it with no overhead in
efficiency but a little bit of privacy
laws so that's that was our way of doing
it and sort of the intuition of just
just to say what we did just an
intuition is that the garble circuit the
good property that we use carbon circuit
is that it is secure against malicious
evaluator so the way the towel is
describing it is that the client
generates this garbled circuit formal
name is the so the client generates the
garble circuit because he knows the
query right and it's so what we did is
that we have now the index server
generate the garbled circuit right so
now the client when he evaluated he
cannot cheat but now the index server
does not know the query so what we do is
that the indexer it will generate the
universal circuit and then the clients
input will be plugged into there so
let's see how I'm going to skip this
part so in phase 1 the picture was that
so the client roughly you know the this
is the garble circuit evaluation the
checks the policy and then the query and
then there is a conjunction so now this
guy will be able to to send different
queries and win but by by doing the
right things we can prevent that so if
this guy sends a universal circuit here
right then the clients input will define
the function that is being computed and
now by properly synchronizing the key is
the client input you can make sure that
the same input is used in the policy
circuit and in this circuit so now we
have the guarantee that actually you
cannot cheat all right and I'm going to
skip I mean this is some
natural things of doing it and the
universal circuit is actually cheap
because we decided that we're not going
to hide the structure of the circuit so
now there's almost no overhead and then
the second thing is that implementing of
the universal gate actually costs you
one gate because it can be done like so
with the three XOR gates and the one non
XOR gate and you can check later in it
the fact that is true and sort of to
finish the talk I wanted to discuss know
what is the practical circuit for MPC
because and we think that this may be
relatively useful thing practice our
system so people always I mean starting
a couple of years ago I guess the p ssw
was the you know the door thinking like
what how do you benchmark those things
what is the right thing and people think
about what is how do you know what
circuit you should devaluate what
practical circuit you should say well my
system does it in one second and aes des
and all these things i guess they're
kind of practical circuits and they're
reasonable and the beats auction gives
an auction circuit that also you can say
this is a reasonable practical circuit
and so with this work and for this
audience will give you this power
circuit which is our basic evaluation
circuit at each step and that is a G
circuit it at the inputs you do X or
between the players inputs then you do a
conjunction that evaluates to the bloom
filter value and then you apply the
formula so this is a small circuit but
you can also think about it as many
times duplicated so the way we we do it
is that the inputs to the circuit so you
don't need to run OT on each circuit
circuit separately because the inputs on
each circuit will be related so you can
think of it this is actually a very
large circuit I don't have time to
explain it and this is where i'm
finishing i'll be happy to explain to
you i think this is a reasonable
tickle circuit I'm not sure how
interesting it is for benchmarking I
will see so it's a large and we think
useful circuit okay thank you if I can
start so the second talk in this reboot
Alice say he's going to talk about
database queries now for his approach
above doing the practical linking of
database queries in MPC it was thank you
for the introduction and yes i'm going
to show you how multi-party computation
can be used to do database linking and
do it in a privacy-preserving manner and
actually this is not all my work there's
a very large team behind it of like 18
people or something so I didn't boot
thought of their names even here yeah so
you can call it team share mine
basically let's go right to the problem
statement so let's assume that the state
is interested in making really informed
data-driven decisions but it has a lot
of databases and registries scattered
throughout different agencies and
institutions and sometimes it it's not
sufficient to actually analyze like
separate the distinct databases
sometimes what you actually want to do
is take several databases or registries
combine those together and actually get
more meaningful results from the gamma
combined database and of course you
should be careful when doing this
because so okay because instantly if you
start to copy your databases and put
them together then this kind of combined
super databases they really become an
attractive target for all kind of all
kinds of attackers so it's it's really
risky
business and furthermore sometimes you
actually you are really forbidden to
even combine databases win when they go
contain sensitive data of course you and
I all we know here that you actually in
some of the cases at least or in most of
the cases you actually do not need to
combine physically those databases
together you should use secure
multi-party computation and to your
analysis on distributed data sets of
course and so the torque is is it yes
it's about database linking but I'm also
giving a couple of practical application
scenarios because just we are we have
been doing a couple of them lately so
here's the first one last year we
created a practical MPC application to
analyze the income data of public sector
in Estonia so the data came from local
governments and ministries and basically
what made this application little bit
simpler is the fact that all of the
ministers had local governments there
datos basically they had the same data
structure so all of them had triples of
job title than the number of people
holding that position and then the their
salary and basically what they did the
just secret shared their input records
those triples and they input into this
one big table and actual statistical
analysis was done on this one big
distributed secret shared database yeah
since we had many input parties and it
wasn't really feasible to deploy special
purpose software for all of them then we
decided to make an wave
chaste MPC application for both the
input and rear like analysis part and
actually we are furthermore decided to
deploy it on a public cloud and actually
this is available as a public tumor and
I'd like to show it to you hope it gets
beer maybe I can use that okay so this
is the first page I'm not going to like
explain it in detail you can go and
check it out yourself the data input
part is not public but the reporting
parties so I don't know how much you see
probably something on the right side you
can see that there is there are they
started like statuses of those free
computing parties they are all under a
cybernetic control at the moment because
it's a demo but you can actually see
that they are hosted by different cloud
providers Amazon Microsoft Azure and
then the third one is Estonian cloud
provider so when all of them are ready
we can push the report button and it
says here that all of the secure
multi-core computation is done in real
time there is no caching or nope
recomputing so this gives you an idea
how long it actually it takes so since
it says that time the only client at the
moment it should take around 30 seconds
if
the inputs are basically like the number
a job title number of people in that
position and the count and the result is
basically all kinds of averages grouped
by like overall average salary for local
government ministries and overall then a
virtual salary by local government all
of them estonia then average salary by
mistress and average salary by position
basically so you can go and play around
with it total combined not problem much
it's like 1800 something like that so
but yeah it's it's just computing like
averages so it's not like that kind of
big deal but it's deployed on a like
three separate public clouds so the
computation was done in real time over
the Internet okay but like I said what
made this application a little bit
simpler actually is that we actually
didn't need any kind of like database
linking technology here there was like
one big table basically now we are
taking part in another project and this
really has a more complicated scenario
because input parties really have
different data structures to the project
is called privacy-preserving statistical
studies are linked databases and it
tries to give its input to an really
ongoing argument between universities
and companies and the problem is that
companies tend to hire students even
during their bachelor studies and
universities really are not pleased with
it because they think that then the
students will concentrate as much on
their studies and they will learn like
they won't graduate on the right time or
they will drop out and something like
that so it has been a I can
ongoing argument and I guess well this
is for Estonia but I guess it's all so
it holds for other countries so what
what we are doing in this project is
actually we are trying to answer the
question if this kind of early
employment has some negative effect on
Virgil studies or further care here and
in the first one we are going to answer
that question for ICT sector where they
she is most pressing and how are we
going to do this well it's pretty simple
actually we are going to take the income
data from the tax office that's highly
sensitive they test you can guess and
that then we are going to combine it
with education data from the national
education information system and we can
analyze the combined combined data set
then find out for example if you were
employed starting from the second year
of your bachelor studies then what's the
probability that you dropped out or
something like them all more meaningful
results and then actually the the
project has two parts so one of the
partner is a statistical analysis
company and they are solving this
problem using classical approach so they
are asking the data set for relevant
data set from the doc's office and
another data set from the education
information system then they are using
their everyday statistical analysis
tools to combine those and do some
statistical analysis on this combined
data set of course since we are dealing
with sensitive data this requires
approval from data protection the
protection agency that they already got
but even then the tax office is actually
forbidden to give out raw individual
records of people's income and they
actually they're going to apply k
anonymity on this
on this input their input data to
protect the privacy of individuals and
actually just last week this analysis
company got the data set from from the
tax office and Deidara creates a price
after applying k anonymity when grouping
with by education in for gender and age
the data loss was seventy six to ninety
eight percent which means that basically
more than three-quarters of the input
data is is just lost which is pretty sad
at the same time what who didn't even I
think it was like free but it's just
that the variance is so big for example
in the resulting data set we had like
only this ninety-eight percent data
losses for PhD students in ICT sector so
I did the resulting set pad only like
eight PhD students and they have to do
statistics on that so at the same time
we are actually are doing the same thing
green secure multi-party computation so
basically it looks the same way but we
are going to get like sea creature the
input data from the decks office or not
and another data set from the education
information system and we are actually
going to use privacy preserving database
linking to combine those data sets so
basically both of those data sets have a
personal ID codes you can think of them
as Social Security numbers and we use
them to actually combine the records in
privacy-preserving manner and of course
the output will be also like a bigger
database but the also secret shared and
when we have prostate the protection
agency with so we try to explain them
what secure multi-party
petition is and how this privacy
preserving database linking works then
they said that we actually do not
require any special approval because we
are not analyzing personal information
because we can't see that the
information so this gives us hope that
we actually are going to get the better
input from tax office so that they do
not have to use k anonymity so we can
get all of the relevant records directly
in secret shall form of course so we
could get actually better like more
accurate results from this so we can
like we can compare the classical
approach and tempest approach the NPC
platform we are going to use or we are
using a share my tap l'occasion server
and just to know just so you would know
what it is i'm going to show you briefly
what what it can do basically so Sherman
the application server is practical
implementation of secure multi-party
computation and what makes it a little
bit different is the fact that it uses
modular design with protection domains
that allows you to use different the
secure computation schemes and different
secret journey schemes so for example
additively sea creature to party with
active security would be one protection
domain and then using end parties with
some in secret sharing would be another
one and for historic reasons additively
secret shared free party is our most
evolved protection domain so it has
supports all kinds of signed and
unsigned integers balloons then I think
we are one of the few who actually have
32 and 64-bit floating-point numbers and
known and bounded link strings and of
course there
like many standard arithmetic operation
supported on all of them so besides that
there is oblivious sorting so we have
implemented sorting networks that are
oblivious by nature by the way so
oblivious erratic sorting and then the
oblivious quicksort proposed by Hamada
and his team so then there is oblivious
shuffle that actually this quick sort
and Reddick short use so this leads to
basically randomly shuffle the rows or
columns in your data matrix matrix and
then there is this main thing for this
project privacy-preserving database
linking and you can really think of this
as a privacy-preserving equivalent of
SQL join so you just have two tables
Yubikey columns and based on the quality
of values in those key columns you just
combine the records together yeah so and
what actually makes this chair mind the
implementation and application server
like you would have java application
service class fish and tables and so on
so it has data persistence layer so
basically you can use database to store
data secret in secret form or in or
public data and it's fully programmable
so you would actually just write
programs and then deploy them on on your
system yeah and the programs are written
in a special programming language called
secrecy that uses a hybrid model so
here's a simple example you first import
the protection to mankind module called
additive free PP that stands for
additively secret shared free party
passive then you create a new domain
private using this protection domain
kind so whenever you declare the value
to be private it
actually means that it's additively
secured between three parties and the
simple example here shows you that well
we first initial initialize two integers
they are actually secret shared it
doesn't make much sense here because you
we are using constant values and you
know the values but in real applications
you would actually load the shares from
the database so we are like multiplying
them together the result is also private
and now if you want for example print
the value out then then we can't do it
directly because he is just a share we
have to move it from this private domain
the public domain public domain is
something that you always have so and
for that we have to explicitly use the
Declassified operation so I had like
five minutes okay okay and then I'll
skip this slide that the show's
protection to mankind Oliver
polymorphism I can talk you two later
about it so based on this secrecy and
all of the light data types and the
stuff we have standard labor library and
the big part of it is actually statistic
suit that has stable filtering linking
and sorting and all kind of descriptive
statistics like mean variance and
standard deviation actually they are
using floating point numbers so you get
secret child floating point numbers out
of the system that you can use in
further computation and then five number
summary box plots histograms t-tests
chi-square test wilcoxon test and so on
and all of those algorithm algorithm to
port oblivious filters which means that
when you have a table of like hundred
rules then if you want to apply a filter
of like age greater than 18 then you
won't get the back like a smaller table
but you would actually get back 100
element secret child boo
vector indicating which rose mature
predicate yeah so our goal is to build
on our like statistics application and
you may be wondering why we chose the
districts well at the beginning of
another project called usable and
efficient secure multi-party computation
we performed 25 interviews and we
explained two different domain experts
what NPCs and asked if they had some
like problems that may be good be
resolved by using embassy and one of the
most popular rounds first or statistics
and I guess I have like two minutes I
hope so i'm going to give you a sneak
preview of what i don't have this are
like application yet set but we have
another web-based think because we are
getting really good at this so this is
not public let me see just the mean of Y
maybe does something let's see
it takes on time this is a local
computation at the moment it Claire runs
in a virtual machine on this computer
but the it what David switch right make
you crazy yeah yeah it's artificially
made slower I guess at the moment it has
only a couple of hundred rose but
there's some floating-point computation
that's really really really slow I have
to see if something Oh be crushed oh let
me try again yeah officially this is the
last thing so
just to be on the safe side
they should connect and the third one
hey
prices
maybe it works maybe it doesn't it it's
only a prototype and it's actually held
together by a lot of my chick at the
moment yeah data protection agency to
approve this yup did they ask you
anything about the queries that you
would allow yes yes we only like the
thing is that you deployed the statement
application server and then you deploy
your programs and there you define what
kind of curious you actually accept so
all of the pre-computation parties must
basically except qre to start the start
the computation so this is pretty fine
yes of course because in real time it's
really like difficult to like make it so
that you can't do anything stupid in a
sense yes here it is yep in stock so
there was one patient so did you ask him
properly you make a better than good can
on DVD but I find the same in a cane on
a video differential privacy and NPC to
be slightly heavy with MPC you are still
out putting the reason yeah they are
completely different like a an aluminium
differential priority they're like
privacy they are output privacy MPC
doesn't like doesn't give you any I put
out to privacy so that's a first are you
still doing the whole histogram so see
if I have half database you're there
half and I can determine many things
about the you other half when we are
linking to databases so although your
density but the final isn't is going to
publix Oh indeed I don't know I feel
like there is some privacy no effect
yeah sure sure that there could be so
actually what happened in this project
is touching that statistical honest
company they told what they would like
to do at what they are going to do and
this is life cam
this kind of capability is a little bit
hard coded into the system so and this
is accepted by this tax office and the
education information system so they
know what kind of queries are made but
since we we get like raw data we should
get more accurate results but this is an
ongoing project because we we do like
statistics on road ETA whether they the
statistical analysis company had to like
do statistics with pre aggregated data</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>