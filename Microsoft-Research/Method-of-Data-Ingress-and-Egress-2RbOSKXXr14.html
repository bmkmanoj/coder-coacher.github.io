<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Method of Data Ingress and Egress | Coder Coacher - Coaching Coders</title><meta content="Method of Data Ingress and Egress - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Method of Data Ingress and Egress</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2RbOSKXXr14" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so um if you can continue to stay so
let's get started by first you know
logging on to Azure with your live ID
and creating a machine learning
workspace so if everyone that's kind of
done that we can go ahead and get
started creating our first experiment so
the product supports different ways to
do both ingress and egress ingress we
support data from various different data
sources which includes a local hard disk
so you can basically then and we'll do
this in a few moments but if you have
data sources on your local hard drive
you can kind of pull them into your eyes
or m/l studio and then start working on
it from there an HTTP so if you give it
an HTTP URL to a data source you will
actually go ahead and utilize the data
from that from there um that's great
if for example you basically have you
want to have one source of truth for
example of data or it's kind of a data
set that that is kind of you just kind
of keep one keep that separate as your
storage so if your data exists for
example in a blob or table storage with
an azure we can also work with that the
other thing too is if it's in sequel
server hadoop or microsoft power query
so we can utilize a number of those
different sources for data for our
experiments and similarly when we're
actually running our experiments we can
also write back to different locations
as well again that includes local hard
disk Azure blob storage or table storage
sequel server or or Hadoop so the first
thing that we'll do is actually upload a
data set to the workspace and a lot of
the data that we'll be working with
comes from a variety of machine and
learning classes so this one happens to
be from the Irvine campus and it's their
zoo data set but and I should
we didn't do you know I'm just gonna
pause here if you what what I'm gonna do
is I'm going to put up a URL to a zip
file and that zip file contains both the
data it contains the PDF versions of all
the slide decks that have all the steps
but there's one document in there it's a
PDF file that actually has all the links
and it has our script in it that that
what we'll do is it makes it a little
bit easier to click on that rather than
typing in the URL and certainly you
don't want to be typing in all the our
script either so you can do a kind of a
cut and paste so let me just back off
real quick and give you that sorry if
this last
yeah so it's this last URL here at the
bottom so if you were to go ahead and
just type that in its to a zip file and
if you download that zip file it
contains what I had just described there
yes yeah and there's a full right and
you'll see one document in there that's
called the Asura mill workshop data
source references and you know what you
want to do is open that up and just kind
of keep that maybe cash on your desktop
or something like that because it has
all of the links that we'll be using so
you don't have to type them and you can
just click on them it also has like I
said the our script in there so we'll
cut and utilize that a little bit later
okay you guys got that okay
so what we'll do is I'm going to switch
back and forth more I can show you is I
could show you how we do it and then if
you like to follow along this one's
fairly straightforward and what we're
going to do is is grab that grab that
data set it's called zoo data and if you
click on that URL in your methods of
data ingress and egress shouldn't have
access to it just save it somewhere on
your local hard drive and then what
we'll do is we'll upload it together so
let me go ahead and switch over okay so
on this is the this is the workspace
that I had selected created earlier
Cambridge 44 and I'm gonna go ahead and
sign into the ML Studio I've already
kind of downloaded off the internet so
the first thing going to do is go to
experiments here select new data set
from your local file it's fairly
straightforward and browse to the
location where I had stored it let's see
I wish I could go
so they zoo data I might give it a
friendly name and it has a kind of a
description here you can go ahead and
select for example the type of data set
it is I know that this particular one is
a CSV file with no header so we'll go
ahead and click that and hit the check
mark and then in the background what
you'll do is you'll get some status down
here there on the bottom that describes
the progress of uploading that data set
it should go fairly quickly and right
now so one of the things that we put
into the product early on when in the
early previews was actually another pane
here that that actually had studio home
experiments and data sets so you could
actually see the list of data sets that
you've that you're working with in the
system
unfortunately that page went away
they're bringing it back I pointed out
because it's kind of funky the way that
you have to kind of find the data that
you uploaded so you have to kind of go
in and create kind of a new experiment
to see it so if you do new experiment
you'll see it basically see a pallet
right here of menu of menus and these
are all the UM basically the actions
that we can perform within the system so
you have saved data sets and if I was to
expand that you'd see a whole bunch of
data sets a lot of these data sets or
data sets that support the samples and
unfortunately right now there's no way
to kind of filter them out to see which
ones that you've uploaded so the ones
that you've uploaded will actually be
mixed in here so if I search down a
little bit zoo data that's the one that
I had just uploaded and we're going to
touch on each one of these throughout
the day the second one is basically data
format conversions so converting various
formats to a rff CSV a data set and so
on we're going to just next we're going
to do a data input and output so this is
a reader
writer we call these things modules we
have a number of various data
transformation services as well
filtering manipulation sampling and
splitting and scaling and reducing
feature selection machine learning that
support basically evaluation
initializing evaluation scoring and
training the models we have our language
support and we'll do something about
that later
statistical functions and some tests and
analytics and again a lot of these
functions especially the models and
statistical packages actually come from
either research or Bing or Xbox or
they're the best breathes read ones that
we've found externally so all we have
this experiment page open
let's now exercise how to read data from
ash or blob storage so I went ahead and
I have Azure blob storage and an account
setup and so for that what we're gonna
do is utilize what's called a reader so
let's go ahead and first let's name this
experiment so if I go up here you can
call this blob reader that's the name of
the experiment and in the data input
output there's two modules one's a
reader and one's a writer this sounds
pretty obvious what they do if I grab
dragged the reader over so really what
I'm doing is I'm clicking on reader and
just dragging and dropping now I have
that on like my canvas and if you take a
look over here on the left hand side you
see a number of different ways that
different data sources that we can read
from one is Azure blob storage HTTP
sequel Azure a table a hive query or
power query so we're gonna basically
read data from an azure blob storage
obviously in order for the system to
connect to that blob storage where that
data lives we have to provide it with
authentication information so
gonna hit we're gonna authenticate via
an account name there's two different
ways public or account the account name
if you go back to this sheet that you
downloaded this handy-dandy one you'll
see in here they also write the reading
and the writing so let's just do this so
we're going to select public because I
didn't put I basically I didn't put any
credentials for this time this for this
blob storage I made it public accessible
so it's public and I need to grab the
URL from it so let's just do this let me
bring up that sheet of paper real quick
so you guys can follow along
there's so it's this URL right here the
second one so what I'm going to do is
just copy that and what I'm doing is is
that there's a data set called adult
adult census data CSV and what I'm doing
actually is is I'm going to reference
that and read it into my as UML
workspace so I'm going to paste that
here that URL and then from a file
format I'm going to leave it as CS
because I know that file format CSV and
I'm gonna check this because I know that
for example that data does have a header
row everyone follow now once I've
configured this what I want to do is
select run and when I select run what's
happening is is that the ashram else is
posting this this job more or less on a
service on a queue and you see in the
upper right hand corner it was queued
now it's running and you get an
indication that it's running fixed to
the reader and when it's done running it
will say finished running and you'll get
the checkmark if there's any errors you
would basically kind of see like a red
bang or something like that next to it
so now what we can do is just kind of
visualize it to kind of make sure that
we actually got it so if I go here and I
click on the results so you know so
these are all modules and sometimes they
they take inputs and sometimes they also
produce outputs this particular module
just produces an output the input is
really configured through the properties
window on the side and later on we'll
take a look at some modules that have
both inputs and outputs so this one I
could actually take a look at the
results data set and visualize it so if
I go here and click on this I can
download it I could save it as a data
set a training set save it as a training
model and we'll talk about all these a
little bit later also visualize it so if
i click
utilize the product produces this really
nice summarization of the data and this
comes in handy especially when you're
starting to take think about your
strategy on how to clean and transfer me
transform the data to get it in a
position where you actually can do some
machine learning on it and we'll kind of
review some of these things fairly
quickly you know throughout the day but
for example here the results data set I
can take a look these are the column
names age work class education
occupation I can scan across those the
product automatically generates a mean
medium mid max and so on and so forth so
I can take a look and scan this data to
kind of at least say hey how does this
look do I see in any anomalies that I
need to go ahead and correct it also
indicates for example if there's any
missing values and we'll talk about that
in a few minutes but you know here for
example in work class it detected that
there were a hundred and I 1836 missing
values and that might be an indicator
that I need to do something with those
missing values a little bit later on and
I can quickly scan through the data so
that's that's an example of how to read
data from a blob storage and then the
next thing what we'll do is actually
read data from an HTTP source so this is
the case where you don't necessarily own
the data but you but the data might live
somewhere like at an NGO which is very
common or some institution that produces
for example data and makes it available
for the public then instead of copying
it locally and kind of bringing to the
system I can just reference it via a URL
and leave it there so what we'll do is
create a new experiment so select new
experiment and what we'll do is we'll
call this an HTTP leader all right that
makes sense because that's what we're
going to do is we're going to access
this from the web again we expand the
data input and output window here drag
and drop the reader and then we want to
go ahead and configure this so again we
want to specify our data source and in
this case it's HTTP you want to provide
with the URI and if I go back to my
cheat sheet here don't see it oh yeah
we'll grab this one we'll just use the
same one the zoo data one on the top
because it's referenced over at the
Irvine campus in California and what
I'll do is I'll cut and paste this here
and I know it's CSV format and I also
know that it does not have a header row
and I'll go ahead and run that
and what it's doing now is the services
going out and affectionate directly from
that URI and kind of pulling it into the
system
and that should work and you can always
kind of validate that it worked number
one not only you get the checkbox but
also you know visualizing the data as
well and in that case there it is and
you can see in this case and it created
column names but there were no column
name so it arbitrarily just put
something one through and ok so I think
we've covered read pretty well let's do
a writer so suppose maybe you're working
with an experiment and you want to
basically write and persist the
information or rather the output to
somewhere let's call it maybe blob
storage or a hive table or something
like that
so this is where the writer module comes
in handy and what we're going to do is
let me just save this off what we'll do
is we'll create a new experiment and
let's call this one
vlog writer so this time what we're
going to do is we're just going to grab
a data set that exists here already
today and let's use this one here this
is a iris information I can go ahead and
visualize this so this is an iris data
set so you know information about
flowers and so this this was already
it's probably supports one of the
samples that are in the product today
okay and then what the next thing I'm
going to do let's do something different
so this time let's convert it to a CSV
file so I know I just happen to know
that this iris to class data is an AR FF
format and really what I want to do
potentially is work with it in a CSV
format so what I want to show you here
is a data format converter and I'm going
to convert it to CSV connect the two so
really what I'm doing is I'm connecting
the output of the data into the input of
in this example the convert CSV module
and then let's go ahead and run it and
it's finished and what I could do is
visualize it and because it's CSV I just
happen to have Excel I double clicked on
it and it's just going to show it up in
Excel and you can see that it's it's
it's actually taking the RF format and
actually converted to a CSV format with
row headers our format doesn't typically
have row headers okay so we've done a
little bit of conversion and really kind
of what I'm trying to expose all of you
to is really functionality of the
product more so on how to do data
science it's really just kind of so some
of these scenarios are a little bit
arbitrary later in the afternoon we'll
actually build a classification model
which gets a little bit more into the
modeling aspect or the experimentation
of aspect of the product again these are
just more of the fundamentals so so now
let's come ahead and write this CSV file
to blob storage and let me just do one
thing here let me bring up a visual
studio and I'm going to connect up to my
Asscher blob storage from visual studio
so let me do that real quick
one minute here while I login
it's refreshing okay
so I created this storage account called
Agora ml workshop if you notice when you
were doing the reading exercises it was
pulling CSV files from there it's just a
blob storage account that I created to
support this class so on the left hand
side I've created the azure ml workshop
and a container called data and that's
where that CSV file exists that you had
moved in earlier I also created another
container called blob sample and that's
where we're gonna write this file too so
in a few minutes so we're going to
configure the studio to take that iris
data convert it into CSV and then write
that csv file directly to blob storage
so what I want the reason why I'm
opening it up here is because of the
exercise yesterday as you can see people
uploaded all this data so let me go
ahead and just clean out that that blob
storage real quick let me just delete
all these real quick so these are all
blobs that people had written in the
workshop yesterday okay so it's clean
now so let's go back and what we should
see is when we're done with this
exercise we should see our work show up
in this blob storage container so we
have this the next thing we want to do
is drag what's called a writer so we're
going to drag that and drop that there
and we'll connect the output of the
convert to CSV to the input of the
writer and I'm going to write to Azure
blob storage which is that account in
this time this case it is a secure blob
storage as opposed to earlier it was a
public so you have to provide
credentials and what you're going to do
is enter in the account name the count
key and the path to the blob so and I'll
show you how to get this information
from Azure if you want to do it and if
you
but for now let's go ahead and open up
that handy-dandy sheet here and you'll
see for example the account name is
azure Amell workshop so I'm just going
to do that and copy that and paste that
here and then the account key I'm gonna
open this up here and the account key is
a very large key that gets generated
automatically by Azure and it's good to
kind of it's a good practice to change
that every once in a while so I'm going
to cut and paste that here and then the
path to the blob storage if you remember
it's blob samples the name of the
container and think of a container in
blob parlance to - almost like a folder
in the folder and the Fulton folder in a
folder structure so in your file system
basically you have containers which are
basically folders so think of a blob is
kind of a folder so what I'm going to do
is here's the path blob sample and I'm
gonna go here and do that but I'm gonna
put in some because we were had a large
class yesterday you know just give it
your initials and some random number so
that you wouldn't collide when you're
writing it up - - the blob storage I
know that the format is CSV and I know
else and and I know that it also has a
blob header row so let me go ahead and
run this and hopefully each and work
nope got an error what I've been finding
lately is that a lot of times the error
is a result of me cutting and pasting it
incorrectly the information here and
sometimes I put like a get a space in
there or something yeah see that yeah
that's right again
there we go yeah so you just have to be
careful with the cut and spacing so when
you run the experiment how does it
affect you really mean each module yeah
yeah so the user experience actually is
is a little bit misleading so really for
example when I added the two class data
and the convert to CSV and I hit run
what it does and then I added that the
next one it caches basically the result
so what I mean is is that the convert to
CSV because I ran it previously and I
didn't change it
it doesn't actually rerun again from a
compute perspective unfortunately the UI
shows that it is but it's not really so
so what it really tries to do is kind of
as long as you're not changing things up
above it's not be computing each time
all right I get this thing working and
go real quick and I think I have a space
here yeah see that yeah I just have to
be careful when I'm cutting and pasting
here
Bagon so if I go back over here and do a
refresh I should see it and somebody
else did it too so that's now writing
back to the blob so let me just show you
real quick on how you get that
information that account information if
you want to do that with an azure if you
go into your azure portal here this is
my as your account you'll see on the
left-hand side all the various services
that are available in Azure and if I
click on for example storage here I'll
see you that storage account that a
dremel workshop all I have to do is
click on it and select manage access
keys and there's the information so the
storage account information that I
entered in and their properties window
on the right-hand side came from there
and then the primary and secondary key
you only need one of those you don't
need both but again it's it's kind of
good practice from an operational
perspective to change it you know every
once in a while just good good good
password practice yeah basically
provides you access to the site yeah
yeah well rather to the storage count
all right well you know it thank you
very much for first thing here and it's
actually nice to UM to do this with a
small group let me just go back to the
slides real quick we won't go through
all of them but what kind of so win
those handouts that you have that you
downloaded it zip file
it has all of these slides in them and
as you can see there's all step by step
things to do so if there's something
that you see or you know if you have to
leave or something like that you can
open this up and do the experiments on
your own free time so we caught up where
we did the right to the azure blob
storage
and again this is these are the
parameters example of the parameters
that for this particular experiment that
we completed this is how you get Azure
blob storage account information we
won't go through through these last two
here but this is how you would read data
from ash or sequel it's the same you get
the gist right it's the same kind of
process you pull the reader over and
then basically at the top you're
specifying that you want to reach from
sequel Azure and then you're giving it
credential information and this time in
this instance you're giving it on the
sequel credentials right so your
database server name the physical
database name the user account and then
the password and then you can also give
it a database query so instead of for
example selecting a whole table you can
be very specific about the data that you
want to get from the sequel database and
then lastly very similar if you want to
do it from Hadoop you can do that as
well in this case this is a hive query
so you're selecting again hive it's your
data source you're giving it a query and
again some more credential information
so it's that I sure AM L knows how to
connect up to that data source and then
pull in the data so pretty
straightforward I mean these are the
fundamentals and we're going to kind of
build upon this as we continue on with
the workshop now we got the data so then
next step will is what we'll do is we'll
start you know transforming data okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>