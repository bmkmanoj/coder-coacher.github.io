<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Intersection Workshop - title TBC | Coder Coacher - Coaching Coders</title><meta content="Intersection Workshop - title TBC - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Intersection Workshop - title TBC</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dAj0GZhN6rM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so then next and last speaker in this
session I actually before the coffee
break is Mark polyphase he's a professor
at ETH series and he will talking about
challenges and opportunities in modeling
urban environments okay thank you
Michael thank you for inviting me so yes
I wanted to talk a little bit about some
of the things we've been doing in the
last few years oops the
okay okay so this is what we could do
already about 10 years ago and so we are
very successful in essentially getting a
video getting some images and turning
goes into into 3d models and so we
decided to try to push that towards ok
to use that also to reconstruct 3d urban
models as more and more there's demand
to build at these these models of the
whole world and so starting from
essentially videos as you see up there
essentially trying to turn them into 3d
models and so we worked hard on you know
setting of a pipeline that we could do
this also actually with goals of doing
all of this in real time as we want to
model the whole world we better to
iteratively efficiently and so by
leveraging the GPU and so on were able
to to achieve this type of of course
however there's there's many challenges
that we encountered in this so here you
see the type of models that we obtained
when we do that so essentially we use
the same type of structure motion and
then and then in particular dense dense
multi-view stereo when we actually
analyze those models the median errors
are in this you know a few centimeter
range typically however you see many
artifacts windows are just missing
there's lots of holes because we're
missing some data or we have some
uniform regions where you don't quite
know what to have what happens we look
at things actually at the edge of the
field of view and and we have edges of
the line with the direction of motion
and so on and so we have lots of
ambiguities and things that like for
wrong and in particular discontinuities
also in the actual world it's not just a
single nice object we look at from
different directions there's actually a
very discontinuous world with many
objects of different types all
intermingled so so visually they're a
bit less pleasing that we had hoped for
it's also let me show you here a little
bit of what's actually going on under
the hood so here to video streams
watching a facade and then here is
essentially what we work with so this is
the the real-time stereo that we get it
actually works really nicely on most
walls and another
of you know trees other other structures
except that we get clearly some problems
when we have strong horizontal lines and
we drag horizontally past the facade
that's one issue and of course windows
they really give you strange results so
so we try to see what we could use on
the other hand of course we're looking
at the urban scenes we're mostly
interested in the building so that's
what we want to do well on and of course
that's that's not a generic object
there's actually quite some structure in
there so one of the things we try to
explore was to to try to to bring in
planar priors now there's been a lot of
work in the last few years to do a
Manhattan World War constructions and
apply this type of priors but typically
they've been applied indiscriminately to
the whole scene giving often unpleasant
results if the scene wasn't only a you
know a cubic building but actually was a
mix of trees and buildings and other
things so what we did here was to first
generate a an arbitrary number of
hypotheses planar hypothesis just from
the data and then at the same time also
identify actually learn both from the
image and from the the stereo volume
here learn from training data and then
and then infer here from this image what
is likely to come from something we want
to be as a plane so in particular what
is likely confirm wall or what is likely
come from window even if the window we
don't even really perceive it as a plane
in the original data here if it looks
like a window and it doesn't look like a
treat and we probably really want to
just fit whatever the closest plane is
to it and so essentially and vice versa
when we're looking at the tree or some
other organic structure even if
approximately it looks more or less like
a like a plane locally we just want to
avoid replacing it by plane okay so we
get this type of results then by writing
this in a marriage optimization that
combines all different terms we also get
much nice consistent planes actually
here shown in one depth map but it's
actually consistent over over the the
video sequence so we extend those planes
beyond a single frame
so here's then a video where you see
that at work in particular notice that
there's multiple times the same color so
actually the plane is extended for over
from from one frame to the next heir as
also here see how it can nicely handle
particularly problems with windows and
other things of course it doesn't quite
fill in things behind you know in place
so that we didn't see and also this was
rather expensive in terms of processing
with the Americanization
okay then then as we moved on we try to
find something that could both be fast
might not be as accurate in geometry but
but give us something that would give us
essentially close models that would give
us that we try to to somehow capture a
lot of the urban structure so in
particular in urban buildings and urban
structures mostly have vertical facades
are so vertical direction it's really a
preferred direction and so we
essentially try to look at if we could
use just a height map representation to
do that map fusion on so instead
essentially instead of doing that map
fusion in one of the original frames
order from the original viewpoints we
did essentially Deb my fusion on a
vertical representation so vertical
height map now of course if you just do
that then you get and you have some
overhanging structure you get this type
of problem right so you just you have to
choose if there is an overhanging
structure if you put the ground at the
top of the overhang structure on the
original ground and get something with a
single layer is on the left there
however what we can do is actually
decide to based on a model selection and
on a per pixel in the height map make a
decision on how many layers we need to
do a good representation now this
representation can both be calculated
very very fast this is actually all
running in real time but it has this
very strong organization because
essentially everything on the same
vertical structure will vote for the
same depth discontinuity essentially in
this height map representation and so we
get here results essentially you know in
real time rates on on a single PC so
here you see now the texture here you
see the texture is a bit washed out and
the geometries course this is because we
use a presumably coarse grid and then
essentially just project all of the
images we have on the same voxels and
that's why you get this washed out
effects after of the texture so we could
work on cleaning it up but but you get
an idea of you we get now almost
perfectly close models things like trees
and pillars and some come out quite
nicely clearly we have artifacts within
a texture there's you see it but look at
the car is actually
surface now you get here even the
basketball ring is actually present
there so you get you get this nice
structure to actually robustly come out
of it so it's not the only not perfect
or constructions but we actually get
very robust rec instructions here I
quite close to the original shape up to
some some nice so here also then using a
similar type of technique on indoor data
in this case we have forward facing
stereo camera which could be mounted on
robot is actually at the Mountain View
offices of Honda research these are the
dead maps we see very noisy lots of
missing data and so on and so then when
we do fusion it's really important to
have this strong regularization and so
for robot for example here to navigate
in an environment it's important that it
somehow can figure out the environment
even if the data is actually hard at
work so here we're in the previous work
we had every every point was calculated
fully independently and you could
already see the results were typically
quite good here we actually have
regularization across different across
different elements and then you also see
here that you get essentially a nice
flat surface as as as neighboring pixel
emmons talk to each other actually
related to that and and prior to that if
you want to do this just from a passive
camera walking for an environment or
practiced arrowhead it's important that
you actually also calculate of course
the motion of your vehicle or you robot
or your platform through for the
environment what you see here is a
real-time visual slam implementation the
the video on the top is accelerated five
times but essentially we we do a full
visual odometry and then they take loop
closures and then adjust everything it
runs essentially on a laptop in real
time we don't do a full bundle
adjustment which would be of course the
perfect thing to do but we keep
essentially a topological metric
representation that tries to locally be
metric and be globally always
topologically consistent and try to get
a speed as possible metric
but make sure to preserve the topology
for sure ok so then similar word beyond
working with with vehicles driving or
robots walking around we've done in the
last few years lots of effort on trying
to have fully autonomous flying
helicopters so our first generation here
was flying autonomously but with markers
so we engineered the environment but
then it was flying fiat enormously all
calculations on board when then you have
the algorithm crash it it's actually a
physical crash so this is this is
actually very challenging and then what
you see here is one of our most recent
results where we here you see the safety
device of course so where we fully fly
autonomously in this case again all
calculations on board and so on but
without a prepared environment so here
there's even you can set waypoints or
you can actually even tell the the
helicopter here it has a stereo head and
in this case it will actually detect the
wall and then it will follow the wall
and keep exploring the world to twist
and model the environment as it goes
here so here you see it fly autonomously
still with a safety rope attached to it
you see the depth maps here this is all
calculated on board so we have very
limited well rightly limited resources
on board and so or models of course
limited in that sense but we can
actually use it to do to do a real-time
online navigation fully autonomously
with everything on the device yes so the
so so it's the battery takes lasts about
15 minutes so
the helicopter itself takes about having
50 watts to Hoover and so our compute
platform is essentially the equivalent
of a laptop Accord to do oh and type
just add 20 or 20 or so wats so it's not
much of an issue ladies I mean in
general of course the applications are
limited by this currently both in those
helicopters as well as actually nowadays
in every you know every cell phone or so
is actually there's many interesting
opportunities in using these cameras
instead of just an arbitrary you know
photo camera is that you have a lot of
additional sensors in there of course
you have gps in here but also very
interesting is you for example have
financial sensors or gyro and gyroscopes
in here and so a lot of the calculations
that that are expensive and are
difficult to get robust can be
simplified by using those additional
sensors so here's just an example of
something we did was to use the fact
that you can if you take a picture then
you typically hold your camera still you
have essentially if you read out gravity
if you read out the acceleration you get
pure gravity vector and then you can use
that to really simplify your relative
pose estimation so for example here you
can go down from a tenth degree
polynomial to solve to a linear thing
with five points or if you go to the
minimal solutions three points and you
get a party to solve notwithstanding all
of this when we when we are closing
loops in work instructions often except
if we have them in the directional
images but otherwise we mostly can only
closed loops when we walk through the
same hallway in the same direction if we
happen to walk in a different direction
or cross our path we didn't closed loops
and I think most people have trouble
closing loops and in those conditions
and the problem is really that white
baseline matching is still it's still
really really hard and so one thing that
we propose is to really depending on the
scenario there's not many scenarios
where you might be able to do small
baseline matching and get several images
for them from your video sequence
locally to get a 3d model and what you
arguing is that then you should really
use that information build a local 3d
reconstruction and then use that to
essentially not have to pay a lot of you
know essentially pay a high price in
terms of trying
to get in variance in the image you know
and due to the invariant feature
extraction here but essentially go to
the 3d surface and extract sir features
directly on that surface so that's what
we call viewpoint environment so VIP and
essentially we take your favorite
feature detector but instead of doing it
in the image in 2d directly just extract
it straight on on the model on the
surface now of course nowadays with
Kinect sensors this becomes even more
relevant you have not the geometry
straightaway in many scenarios and then
you can extract also there directly for
the you take the image you warp it to
the surface and then you attract future
is in there so here's an example of how
it works two different models here on
the left and on on the right you see the
camera viewpoints are very different the
actual correct matches are this part
matches this part here the green are
just the original sleep descriptor is
matching the red is where we have a fit
of scales so what's interesting is
there's a lot of challenges in urban
scenes which repeated structures and I
come back to that but in particular here
you have bricks random Briggs matching
other random bricks well it's the wrong
grit but it's actually still the right
scale of brick ok and so actually you
get initial support for the right scale
then after that you get initial support
for the right orientation because
different bricks on the same wall will
vote for the same orientation etc so so
that way we can actually handle here
massive amount of outliers ninety
percent of the features initially are
outliers and still trivially essentially
get the result the correct result out if
you try five points on this just forget
about it so we've also tried to to use
that in scenarios that are a symmetric
where we have maybe a treaty database in
the background but we just get a single
picture as a query image and so in that
case what you do is try to rectify the
query picture so in urban environments
you have a lot of structure and so
typically it's not very hard to rectify
you could also use the potentially
sensors for that if you have them and
then extract features on rectified
facades and then of course it's the same
thing in the back in
database you actually also store those
features for rectified images and then
here also you can actually take
advantage of that and match between the
query image to rectify query image in
your database and get lots of matches
and he also actually explored the fact
that scale should be consistent x and y
translations we verify separately notice
that for example here there is a
repeating window here and that's why we
here in the x-direction get secondary
peaks here for the other windows
matching so the incorrect windows
matching so that way we can get I think
it's fifty to eighty percent retrieval
from a single picture where we are
anyway in San Francisco based on a
database of 1.2 million images and this
actually work together with Nokia and
navteq and stand for it and there's a
there's actually a test test available
online and in a benchmark that you can
participate so here's another
application of these kind of things of
capturing these representations here
just an image based representation on
the device where we align it with a
ground plan and then can dive in
essentially show you this is all running
on a android phone so then you can walk
around this course the idea would be
take you take a picture and you ask ok
I'm where am i and tell me where to get
to that auditorium or something like
that in a complicated building and so
we've done some work on that but it's
actually very challenging and and the
reason is the following is because in
such a building it's very symmetric a
lot of the spaces look roughly the same
different floors look the same etc
that's actually a problem we've also
encountered on the outside of buildings
this work here with Christopher so where
what you see here is images that
actually in correctly matched however we
did geomet we did everything we had to
do with the geometric verification so
matched features the geometric
purification and we get a lot of support
so I most searched for motion algorithms
from here we'll say ok I get two hundred
matches three hundred matches correct
this must be correct and then they don't
question this anyway and they just put
it in their sexual motion bundler other
techniques will just do that and
and somehow they fail then to give good
results coming out of it so what you get
is essentially between all those
pictures if you have this collection of
pictures you will get all the red dots
there are different pictures they denote
and then the edges essentially are
verified geometric relations between
them and so it so you get a lot of those
but some of those are wrong some of
those are matching this side of the
facade that side of the facade if you
let it in your structural motion
algorithm it just blows up and you don't
get results you get results that look
like this or so so what realize that you
could use is that if those are actually
meaningful geometric relations whenever
you walk along a loop you should
actually be able to close your loop also
geometrically so in other words if I
walk around the room and I get back here
my translation should accumulate 20 and
also my rotations should accumulate to a
multiple of 360 and so that's something
you can verify if I match this facade to
this facade here only hundred eighty
degree apart then I get a loop that only
does hanate 80 degree then I know
something's wrong okay so essentially
you get an x just rats trying out and
sampling random loops you can
essentially detect that there's a few
places where there's inconsistencies
accumulate and then you can start an
inference where you actually try to find
what's the smallest number of links that
have to cut to essentially get rid of
all my inconsistent loops and so by
doing so you can actually quite nicely
identify a lot of those problems get rid
of them and then get my circle
instructions coming out of it okay so
that's what we did two years ago with
Christopher and then after a while and
it's actually very close to what Michael
was talking about earlier we realized
well why are we throwing all those nice
links away knowing that this side of the
building is almost exactly the same as
that side of the building we should not
show that way we should actually make
use of that and so we started exploring
actually keeping track of all those
links because those are definitely
non-random occurring events and they can
essentially be be used to make a much
tighter reconstruction so in particular
if at the end to do a bundle adjustment
you can see all of those copies where
you detect that this site looks exactly
the same as that side of course now that
we actively look
for those relations we'll also look for
symmetries we never get confused by
symmetry because you know you'd have to
mirror all your descriptors or so to get
the wrong matches but of course once
we're looking for those relations we
also look for those and then essentially
we get all these type of links between
parts of the geometry that are known
local but essentially our virtual loop
closures and so they can really tighten
or 3d model a lot and also low for you
know things also as Michael was talking
about where you can do completion this
way you might only have one feature
detected here but it's reconstructing
treaty here so you can hypothesize it
and then verify it etc etc so you can
actually start doing nice things also
you do get all these symmetry planes and
other relations that give you some some
nice sometimes compact description of a
few models and they can also they also
tell you something about the dominant
orientations in your building etc etc so
so you get a lot lot of nice things
coming out of this so we had already
been interested longer by this type of
symmetries and and and repetitive
structures here in particular on facades
we've been looking at directly detecting
them on facades by detecting dominant
orientations of the facade rectify them
and then extract repeating structures
and detect actually the size of the
repeating element so again very similar
to what Michael was doing the directly
in in a 2d image and not on a 3d model
which has some additional challenges of
course but when you see is what we also
realized so you can extract those you
could use these for different things for
potentially extracting or or using an
Arctic architectural grammar to explain
those images or maybe even extract
grammars you can of course the shape
from symmetry or repetition so here if
we get essentially we detect that
something is five times repeated every
time from slightly different viewpoint
you can do our construction from that
something that that is really nice also
is you could see here these regions that
we extract here as we could see this
whole system as an invariant region
extractor a very fancy one but on urban
scenes actually very powerful one
and that's actually is traded here which
is a test on the civic building data set
this was suggested by one of the
reviewers of our paper which has
essentially 200 buildings and for each
of them five images and so what we did
was to use those features here to given
one image to try to retrieve the
corresponding images of the same
building in Zurich what you see here
those two curves here this is what
safety is doing with the 10 and hundred
largest features that you find on those
images here and then what you see here
is essentially what we're getting with
different settings or different choices
but in particular what we do is we take
one of those elements here and describe
it by histogram of gradient type of
representation similar to but but then
on an eight by eight grid instead of a
four by four good so it's like six but
on a bigger grid because it's larger
elements and a more descriptive and so
that gives us about 75% with a single
feature actually 75% retrieval for the
first and so on and it goes down for the
fifth the fourth corresponding one so
the last one we still at above fifty
percent correct with weasel so it's
actually very powerful feature that you
get in this case for urban seeds other
things you can do with symmetry also
just symmetric buildings get depth for
construction then of course something
important in in in urban scenes is that
they're not static depending on this
time scale you look at there's changes
happening and you could of course start
somehow rebuilding every time a new
model if you if you do that but that's
clearly not what you'd want to do you
want to know where some change took
place and so here we propose a very
simple approach where you're given a
reference model so here image-based one
but it could be laser-based also and
then you get some images and you want to
very five to omit the current images you
get are still compatible with the
geometry of the previous model in a way
what you use is something very similar
to what we do with plain sweep planes
with stereo where we hypothesize all the
different depths
here in this case well we don't have to
i pologize all kind of different models
we actually have a model you know we
have one model hypothesis so let's just
try that one out so we project from one
image on the model project back into the
reference viewpoint and then we can
compare images if everything is still
consistent we have no errors so dark
image if we have some updates then we
start seeing some differences and so
that gives us hints that something has
changed so here you see some results so
of course on top of that many changes
might be relevant so you can run if all
your favorite detectors for the features
that you you might not care about like
people cars vegetation in urban setting
those are not relevant of course more
generally you might just if you have a
vehicle like a trash car so that drives
by every week you might just look for
non transient changes so if you know
something is part here or there if it
changes every time it's probably not so
relevant but if ever something has
changed and then staged changed forever
probably rather ok so then while we're
doing all this structured capture others
were actually trying to we're trying to
do capture from unstructured data
collections like Alyosha also of course
Noah Snavely and Steve sides we're doing
here for taking a large photo
collections online and trying to work
with that so here Rome there were two
million images Steve effing pro 750,000
in 24 hours and 64 machines we decided
to give it a go and try to run
everything on one pc in 24 hours the
whole three million image data set and
so here's some results of that so
essentially we used so we used the GPU a
lot of GPU algorithms we actually had
our pc ok or pc was a
ten-thousand-dollar pc and had
essentially 4gp used in there and
neither 2,000 watts to good which was
also not so trivial so but anyway so we
first cluster images together to avoid
actually pairwise full pairwise
comparison between our three million
images without three million images so
we first cluster them based on
appearance then within cluster do
comparison
and then from cluster representative to
class the representative that allows us
to give a quite good set of fire of
matches we have a lot of 3 million
images to align to to align to each
other about 300,000 images we can
actually localize or combine attached to
clusters like this and then also as we
have still some time left we decided to
do also do dense modeling at the same
time and for this we actually use our
height map model and so here in this
case our model selection with select on
fines up to seven different height
levels here for the Coliseum for example
of course the approximate models I told
you they're washed out here particularly
they look essentially like in taking
overcast sky that's because we take the
average of all possibility nations we
get so yannis from Germany so he also
tried some I also tried on Berlin worked
also so here you see the model so this
is very blurred but as I said this is
just based on blurring on all those
images together with the course geometry
it might be alone
okay so anyways yes so I'll finish with
with this year so so something else we
explore is beyond just capturing 3d
models we also wanted to explore what if
you not have an event so this is
actually very very related to do a young
was talking about earlier what if we
have some interesting event people all
take out a cell phone start recording
from from their viewpoint how do you how
would you want to explore this this this
data right so here here's a system at
work essentially what what we wanted to
try to do is to let you transition from
one viewpoint to the other so you always
you always embedded in the video but
then as you go from one video to the
next we wanted you to preserve the
spatial context and so not have an
instant switch from one viewpoint to
another but really somehow naturally
transitioned as much as possible
naturally transition so this goes in two
steps no details no time for that but
essentially a lot of offline processing
to calculate as much as possible out of
the videos and enable then an online
interactive navigation system where we
do image based rendering or video based
rendering and recombining all of the
data we've calculated before in real
time then interactively so here's then
the system in action so you can try the
last results you can actually download
the player any data and then you can
actually interact you to navigate
yourself here we only have three video
sequences so it's very wide base line
and so we still can generate reasonable
transitions
we get especially localized songs for
free here
so for this many challenges here we need
to localize your video camera at every
point it's a handheld camera so you can
move around so we need to look like
every frame versus the general reference
frame we need to do segmentation of the
foreground character so here is how to
flip actually right but that's because
we were from two sides okay so that's
essentially okay so that's it so yeah so
okay conclusion there's clutter and
effective surface that challenges we're
actually currently working at really
capturing full reflective surfaces like
the normal maps of windows and stuff
like that but that's future work I mean
oh that's that you'll see at some other
point repetition symmetries dynamic
transient parts are all big challenges
but often they also correspond to
opportunities if you can explore them
properly also having cameras with
sensors is really nice and of course
interesting opportunities and also the
fact that there's cameras now everywhere
okay I'll leave it here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>