<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Deterministic Replay as a Platform for Understanding Dynamic Behaviors in Web Programs | Coder Coacher - Coaching Coders</title><meta content="Deterministic Replay as a Platform for Understanding Dynamic Behaviors in Web Programs - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Deterministic Replay as a Platform for Understanding Dynamic Behaviors in Web Programs</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GruERkA6Z0A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so let me go ahead and very happy to
introduce Brian Berg today he's visiting
us from the University of Washington
where he's just finishing up his PhD
he's been doing some really nifty work
on doing deterministic record and replay
in the browser he actually I believe a
lot of that has been mainlined into
WebKit okay main line into WebKit so
that's a great accomplishment and he's
also doing some nice work i think
building on top of that core
infrastructure to build some useful
developer tools so i'll let him take it
away and tell us all about that okay
thanks mark so this talk is summary of
the stuff I've been doing over the last
five years or so in my PhD work there
and a lot of its focusing on
understanding dynamic behavior and in
particular retro actively meaning we can
go back in time and look at past things
so i'll be talking about three different
projects and sort of motivation that
connects these together so i know lots
of developers I live in Seattle a lot of
times I'm like I'm the software
engineering researcher I'm trying to you
know make your life better what do you
hate about your job and the first thing
is you know they complain about whatever
language they use it doesn't defend what
language they complain about that so I
try to be more specific like why are you
stuck at work later than you need to be
and it's never like well I just wanted
to write one more test or I needed
lambdas in my language it's usually like
I got stuck debugging and I just
couldn't figure this thing out so a few
years ago Robert o'callaghan said to a
room full of academics you know we talk
a lot about finding these bugs but you
know our bottleneck is fixing them we've
got plenty of bugs and we don't have
enough throughput to fix them all so
allow my work is figuring out how we can
improve the throughput of fixing bugs
and it's been observed over and over
that when you're fixing a bug writing
the fix is not the hard part it's
usually understanding what the bug is
you know what the sequence of events
were that led you to something going
wrong so in that sense like I'm not
really interested in tools that
automatically right patches but things
help you understand the program and what
happened so if you're going to go try
and de bug a bug there's lots of
different things you could try and do to
improve your understanding of what
happened so you could add logging you
could capture a trace or a profile
accord on a lot of these things you can
only use in certain situations depending
on how you can interact with the program
and you know if you have limited access
to the program you can't rerun it then
you may only have a few of these
available to you so in a sort of offline
investigation you have a full access to
the source code and the program and you
can run it but instead you'll use a
static analysis tool to see facts about
the program no matter which way it
executes so a example of state of the
art work for debugging the use of static
analysis is something called Reacher and
Reacher is a reach ability tool so a
user can ask reach ability questions
like does this method potentially ever
call this other method using the
interface on the right here and what
Reacher will do is it'll do some reach
ability call graph analysis and on the
left side it will show the relevant
source code and on the bottom it will
show an interactive control flow graph
so these symbols mean things like there
was a guard in front of this branch or
in front of a call site and this other
thing can call this other thing so this
is an interactive tool but it's
completely static so that's great you
can see everything that could happen but
it's not relevant to any particular you
know interaction and you need to know
what code you should be looking at in
the first place you can't just
demonstrate something and then see what
could be called a net interaction so
another way of going about this
debugging is post-mortem investigation
and in this setting you don't have
source code necessarily but what you
have is stuff that came out of one
execution so traditionally this is like
a core dump that you get from a customer
somewhere and once you have this data
you don't have access to the original
source code or the execution you just
have whatever you don't
but this is actually really powerful
because you can dump lots of things that
would be useful so the state of the art
here is something called y line and the
idea of white line is that as a program
executes it collects enough information
about all the memory accesses and
darling commands that you can ask
questions about the output and they'll
be able to automatically answer their
questions so in this case the user might
ask you know why was this color black
even though it should be blue and
there's an interface where you
right-click on the output and ask your
question and what white line will do is
look through its huge trace of
everything that happened do some
analysis dynamic and Static to figure
out why this could have happened and
explain it in terms of a slice so on the
bottom you can see this happen then that
happened and you can ask follow-up
questions about what happened but the
thing with this is you need to record
everything that ever happens so it
doesn't really scale beyond paint
applications you know if you had to do
this and word or something it it
wouldn't work you know it would just
create terabytes of stuff so the most
common way of sort of going about this
debugging is iterative investigation
where you have the binary you can rerun
it over and over to get different sorts
of information as you think it's useful
but usually this information isn't
kool-aid together it's you run once and
you hit breakpoints you run want to get
logging this stuff is just sort of comes
and goes so state of the art here in
terms of investigating dynamic behavior
is a project called senseo and the idea
of this project is to show dynamic
runtime information as it happens in the
IDE so in the center pane here you can
see which methods are really hot because
they have more bars in the gutter and if
you hover on a specific call side or
method it can show runtime statistics
from the vm like for this call site
which methods actually got called the
most how many times did this branch get
taken and so forth so this is great
because you can get really detailed
information without making your own
logging or whatever but you still need
to know what code to look at and you
need to rerun over no
and if you're debugging something non
deterministic well this could be really
a problem so in this talk I'll be
explaining a new way of Investigation
called retroactive investigation and the
idea here is that you have one execution
and from that you can use tools to go
back and look at different aspects of it
without reproducing the behavior over
and over so if you need logging or you
need coverage or you need a profile you
can click a button and that data will go
and be collected for you without you
having to know re interact with the
program or anything like that and if you
change your mind and need some other
information well you just use a
different tool so what this gives you is
a more live system or interactive system
where you can quickly narrow down what's
going on so first I'll talk about dolos
which is a deterministic replay
infrastructure for web programs so this
was presented two years ago a twist and
in this part of the talk all talking
look at the infrastructure of record and
replay so some key points here
background for a deterministic replay
the main idea is that one execution is
determined by a program and in some non
determinism that was used by the
execution so if you keep the program and
non deterministic non determinism fix
then you'll get the same execution back
so to do this we assume that the program
is fixed anyway we don't need to control
for that we just need to find all the
non determinism that's used by the
program so in the browser these are
things like event loop work and non
deterministic aprs and later all sort of
go through the full scope of what needs
to be handled here and there's lots of
other strategies for replay and these
tend to be grouped into different levels
so there's tools that replay at the vm
level which sort of work like you know
they record all register stayed and make
sure x86 execute the same way they don't
know anything higher level there are
systems that replay at the operating
system level so they replay POSIX goals
determine
mystically is still pretty low level and
there's also application or managed
runtime levels so some of marks work and
mark shot was also done here those
operated the level of like the
JavaScript engine or the Java Virtual
Machine and that has lots of benefits
like there's just less to record and the
main nugget in this work is that you can
modify the browser engine itself and use
Virtual Machine replay techniques and
it's a you can capture and replay a
really low overhead that way so to sort
of figure out what the the main design
goals are here we'll look at video games
so for replay video games are sort of
like your worst case scenario video
games exercise lots of different parts
of the browser you download download
them over the network they're parsed and
evaluated they handle user input they
really timing sensitive if you slow down
then the game becomes in playable you
can't reproduce a bug they use the
animations stuff this could be
implemented using canvas API or the Dom
or both a lot of times these games are
implemented with frameworks in
JavaScript which make the code hard to
understand so I see this is sort of like
you know we need to design to video
games and you can view gmail or other
apps as video games that where you don't
shoot bullets for replay purposes so
going back to design goals the first is
that there should be really low overhead
when you're capturing if you miss a few
frames when you're playing a video game
that's noticeable and it may cause
different behavior so we want to make
sure that when we're capturing a
specific execution it doesn't really
slow down anything the second is that we
need to exactly replay the game
otherwise you know it could have a
different ending so we need to make sure
that we're handling all sources of
non-determinism as accurately as
possible
is your question okay the third is we
want this to be transparent and
composable and what I mean by this is
that replay is not like the one to
approach to debugging it needs to
integrate with logging and break points
and profiling and so forth so if turning
on replay disables all those other
things it's not going to be very useful
for debugging so this needs to be
implemented in a way that doesn't mess
up everything else the last thing is
zero configuration lots of prior work
assumes that you can install some tool
is root or that you can install a
reverse proxy or something like that and
most people either don't have root or
don't want to install a proxy and a lot
of websites that won't work either
because SSL so this needs to be
something that just works when you open
up your developer tools so next I'll
talk a little bit about what non
determinism exists in the browser so
there's sort of a few main categories
most obvious one is going to be user
input that's clicking mousing keyboard
it could be things like geolocation or
motion data or even navigations right
you click on a link that's a kind of
user input you're saying I want to
navigate to this page and you want to be
able to replay that action another thing
is network traffic so anything that
comes over the wire could change any
time so you need to capture all network
traffic and the headers and so forth the
scheduler is also pretty important so in
JavaScript it's single threaded but it's
highly asynchronous so you can in queue
new work to happen at some later time so
you need to make sure that those things
happen in the right order the fourth
thing is the environment so the computer
that you use your web app on depends or
can change the behavior lot so it could
change how big things are rendered it
could change the layout you could be
passing in the user agent and it gives
you a completely different program so
all those sorts of things need to be
captured and another thing is persistent
state browsers can use simple things
like cookies which is pretty much like a
big string that every
page on the same domain can look at or
you can have more complicated things
like a sequel database / website and you
know if the sequel database has
different state and you try and replay
and may do the wrong thing so you need
to make sure that that sort of thing is
addressed so where does this non
determinism show up during execution so
there's two sort of main ways that
you'll see this the first is that it
will be specific things in the event
loop so an event from user or from the
networking stack or from asynchronous
work these are sort of things that drive
execution and are also non-deterministic
the second category of things is
non-deterministic ap is so this is sort
of like this calls in a sense where you
know you can get the current time or the
random number or the size of the window
or whatever it is so those are the two
ways that this non determinism gets
surfaced and lastly we need some
strategies for handling these things
both to capture them and also to replay
them accurately so for things that are
sort of creating work like user and
networks what you do is you capture that
whole piece of work in the event loop so
you can capture the event and what it's
dispatched to or the network call back
or what time were fired and on replay
you just you know inject new work the
second thing is we can memorize the
results so for these syscall type things
like the current time or the cookie we
can save the value on capture and on
replay we don't actually call the the
built-in function we use the thing we
saved and third for things like
persistent state or the browsing history
at the beginning of replay we can just
sneak in the save state that we sell at
the beginning of capturing and this way
we don't need to memorize every call and
you know after that point because we
assume that everything else is
deterministic so for example you can
save the random seed and restore it once
or you can memorize every single call to
random number you know one of these is
going to be
a lot less space but in general this is
sort of a case-by-case determination so
you could you know save and restore all
the cookies for a page or you could just
replace one use of document.cookie in it
you know you can do both of those and
see which one uses less space there's no
sort of golden wheel there so lastly I'm
going to walk through sort of one
interaction with a game and how that
might be handled by the replay engine so
here on the top we see things in the
event loop on the bottom we see to clip
the execute on the right side this is
sort of what's in the recording so first
we load a game that will fire like a
resource received call back with some
network data this gets parsed into
script and run and this calls some
functions and eventually it will look
for document.cookie you know it wants to
restore the high scores or whatever so
in the recording what we're going to
save is this event loop thing resource
received all the network data and
headers and then later will also save
the memorized value of the cookie so the
next thing that might happen in the game
is we press the key to fire bullet or
whatever this can call a bunch of
JavaScript code eventually it will ask
for the current time because this is an
animated game so we need to know when
the bullet should be drawn again so look
at the current time and then set a time
out based on that so in the recording we
have the up arrow and we have the
current time and so similarly when it's
actually time to redraw a timer will
fire so we save that as an event loop
work and then inside all the drawing
routines it's going to look at the
current time in the size of the canvas
or the page determine where to render
the bullet so this will produce a
recording like this sort of the the
major index here is event loop work and
the things underneath each of those are
non deterministic and wise values there
might also be things at the very
beginning like restore the initial state
of cookies or history
so a short performance story is that
this is really fast in the overhead
because we're not really capturing a
whole lot on the left side there's a
graph kind of hard to read but we picked
up a number of small wheel benchmarks
and we recorded the speed with replay
without replay and capture versus replay
playing back at 1x feed trying to
simulate the original timings and just
replaying as fast as possible so in
general there's you know maybe like a
few percent overhead for capturing
depending on you know how image intents
of the pages you have to copy the
buffers over but for playing as fast as
possible it's really going to be much
faster because whilst web pages are not
bound by cpu or network it's bound by
the user deciding to do something so you
can elide all those weightings wait
times and replay you know a few times
faster but in cases where you are
cpu-bound like in the rate tracer you
know we can't really replay any faster
we're not doing any application specific
memorization of rays or something like
that so did you have a question oh sorry
I sign on speed okay oh yeah this is a
relative to 1x um and also these
recordings are quite small so what this
is showing and orange is the size of all
the images on the page and other content
and the other bars are showing different
representations of the other stuff we've
recorded in the recording so where the
user clicked and what time required and
so forth so if you see a recording as
this tiny red bar plus the orange bar
you know the size of recording is just
dominated by the stuff on the page so
you know sending a recording around is
not much worse than just sending the
page saved as HTML or whatever archive
format you have so that's sort of the
replay framework and now I'm going to
move on to time-lapse which is a
marketing term for several different
interfaces for navigating through
boarding it's great that we can record
and replay but unless you can move
through it well you just see the
gameplay again and that probably won't
help you too much so moving on we built
this replay framework and we're like
well lots of people said replay is
useful for debugging so like this should
be really obvious with the UI is going
to be right so our first thought was
well let's just visualize the recording
and show what's in it and show our place
in it over time so now I'll show a video
that has the initial UI for this and
sort of explains now what we did at
first cut for the UI and precisely
capture and reproduce entire program
executions to record a user opens the
time-lapse tour and starts and stops
recording by clicking a button instead
of writing reproduction steps users can
record an execution while they
demonstrate interactive behaviors of
interest recordings can be saved to or
loaded from file and share by email or
bug tracking software recordings can be
replayed on demand using time-lapse as
navigation affordances time-lapse
visualizes program inputs over time
using several linked views that provide
different levels of detail an overview
shows the entire recording in its inputs
using a stacked line graph each input
category is visualized with the heatmap
timeline users can adjust the displayed
interval by adjusting zoom settings with
link sliders or by scrolling
horizontally and vertically each circle
can be inspected to reveal the inputs it
represents lower level recording details
are also available the current replay
position can be changed by dragging the
red slider in any view or by
double-clicking an input circle
in this game the player should only have
one bullet on the screen at a time but a
glitch allows multiple bullets this is
hard to debug without time-lapse because
the developer must play the game while
setting breakpoints in reasoning about
the program's behavior with time-lapse
the developer reproduces the behavior
once and can then seek through the
recording to isolate the buggy behavior
to specific inputs breakpoints and other
debugging tools are usable while
replaying and recording a developer can
experiment with breakpoint placement
without having to reproduce behavior or
worry about losing execution state break
points can be set when execution is
paused when it continues the debugger
will pause normally once the developer
has isolated the bug using time-lapse
they can use existing tools to to fix
the bug get clip Tyler Lee so this is
sort of a version one of the UI it shows
the recording and it shows inputs over
time these inputs are the event loop
inputs like user Network timers and the
user can see a huge list of all of them
at the top or they can look at the
visualization and double-click to move
around through it so this was be one
we're wondering you know does this work
for debugging test it seems kind of
useful you don't have to manually
interact with the program but it doesn't
dump you directly to specific causes at
the bug safe so we did some user studies
to figure out you know how does this
work out in practice so the studies of
two rounds of 12 each and these were
researchers and web developers and
people with enough experience to ship a
webpage or application despite that you
know we interviewed people here at other
big companies there was a high variation
and prior debugging experience so some
people fix bugs all day some people
write new features some people do design
we wanted to sort of see how these
experience levels wouldn't impact how
they could pick up a tool like this and
use it so for the tasks in the studies
we had the multiple bullet bug and also
a bug in a Tetris tile game
and the order of these are randomized
and slope among two groups to see what
the differences would be with the tool
and without the tool so they had time
lapse for one tool and they had just
normal for one for one game and for the
other game they just had the the stock
tools and we were really interested in
how does replay affect one sort of like
the high level thing like are they more
productive but more specifically do they
spend more or less time reproducing
behaviors and how does it interact with
other tools so sort of the high level
results are that the you know regardless
of their control group people spent ten
to fifteen percent of their time in the
debugging task just getting back to
specific states in the program whether
this was manually interacting with the
program we're using time-lapse to sip
around by different inputs with time
lapse they didn't have to manually do
this they could just zip around so they
did it even more because it was easier
but it didn't necessarily make them more
productive and fixing the bug because
you know the buggers still further down
in the execution so they still had to
revert to logging and break points to
you know find more specific program
states that had to do with the bug when
they were replaying the users found it
really hard to use breakpoint at the
same time because we could halt replay
by just not feeding any more inputs in
or that the debugger could actually be
paused and that's a separate system and
so we tried really hard to make it clear
what was going on but users got really
confused as to what was being paused and
why and why do I need to care about the
difference also many users were just
really uncomfortable using breakpoints
even without time-lapse most people use
logging as far as they can go until it's
no longer feasible to use it to fix
their boat the other are set up our
users that will go straight into two
break points but in our study the vast
majority people we had to really like
coax into using break points at all so
our sort of insights from this round of
studies where that people really looking
at the output not the inputs to the
program so they're really focused on
what does the game look like at this
time
that time and they use that to sort of
dig into more interesting states and
they are trying to use logging as a way
to index into this you know vast
database of runtime information that we
have captured in the execution and the
debugger is sort of like that the needle
on the record player like that's what
they want to be at the right place they
don't care about being at the right
place with respect to time it's more
about is the debugger at the statement
that's buggy or that will get me some
useful information so with these in mind
we thought about ways to come up with
better interfaces for replay so in
particular we're looking at how can we
navigate to pass program states in this
multiple bullet bug it'd be really great
if we could you know add logging or go
to the statements when the bullets are
created or destroyed because in this
example we're creating more bullets than
we're supposed to so there must be some
you know missing statement or wrong
guard or something like that so what if
we could just add logging there without
replaying or anything so we try to
figure out how we could implement this
ability to retro actively go and add
logging into this execution and so this
is what we call it probes and the basic
idea is that it's like a break point you
could set it on any statement and add
some expression to be evaluated when you
get there and that you can press the
button and the replay infrastructure
will go and collect all possible outputs
that would be executed at this line so
it'll go replay regenerate all the
logging that you need for that probe
that you added and for any of these
these samples that you got from the
probe you can go back in time to that
statement that generated it and last
thing is that this is all integrated
into the developer tools so you could
log them into values you could go back
in time to one of them and then continue
with the debugger so this is sort of be
two of the UI we have less of a
visualization of overtime at the top and
instead we have actual runtime values
over here so on the left side this is
so you know you window with breakpoints
and probes on the right side are the
values that we collected from you to the
probes so in this case we have a probe
at line 107 at the bottom and we have
three probes installed at that point one
is corrects her why and the event so
then we can see over time this is like a
Tetris game we can see the coordinates
of the piece as it moves around and we
can use that as an index into the
execution to get to the point when the
piece was at the bottom or all the way
to the left or or whatever it is and we
can also get screenshots so you can
correlate this runtime state with you
know something that's visual so in this
case you can clearly see you know what
the coordinates mean in terms of the
output oh and lastly the the sort of
timeline at the top and the the actual
samples here are synced synchronized
together so things that are faded out
happened in the future meaning that you
know they're not live on the heap yet
because it hasn't been executed but we
can see a preview of it so if you want
to you know say if this was a more
complicated thing like an object we have
to replay up to here to have that thing
actually be created and inspected all
but the developer tools often make like
a preview and you can see the preview
from the last run so what's going on
here under the hood is that to replay
back to any of these things that we
logged we need to be able to uniquely
identify any statement that executed and
this takes advantage of the fact that my
recording is split up into a bunch of
event loops and then stuff that happens
underneath that so back in this diagram
if we scatter a bunch of statements
throughout the execution we can index to
see you know per event loop we can see
how many times each of those statements
is executed so say we want to get to the
very last purple one at the bottom well
we don't even need to install break
points or anything until we get to that
third event loop so we'll run up to
there with replay at that point we'll
say
this statement at tetris line 107 we
needed you mean we need to install a
counter there so we can do that with
break points or by rewriting the program
using byte codes and once the counter
gets to two right we can just you know
stop in the debugger and we've replayed
to that executed statement so this is a
way of you know having a time point for
any executed statement in the prototype
this was done with break points just
because it's easier to implement but in
the future would be great to do this
with rewriting the bytecode because
going into the debugger is really
expensive and WebKit if you turn on the
debugger and have any breakpoints it has
to rewrite all the code blocks and not
go into any of the Jets and it's just
like 10x slower so possible you want to
avoid that when you're navigating
through the recording so with problems
we have this retroactive workflow we can
add some logging to some interesting
statements we can get a bunch of states
we can find some weird discrepancy like
here the bullet dives more times than
its created so it seems sort of sketchy
maybe we want to replay up to that point
and investigate further and then we're
there and we see that either the logging
is wrong or there's really something
interesting going on so we can add even
more logging rescan the values and then
keep going from there so there's also
some other uses for this time points of
specific statements that we didn't look
at too much so the first is that you
could theoretically replay up to any
output producing statement so we just
did this for probes but anything output
on the console you could replay to the
statement that made that output you
could generalize this to replying to
specific lines in a profile that you got
so if some code is really hot you want
to know like why do we get here or why
aren't we busting out of a loop you can
replay up to some point in the profile
and start debugging that way the second
is that with this you can pretty
accurately represent any point in the
execution you could add annotations like
seems like an interesting part of the
recording and you can send that to
someone else and they could get back to
the exact same point and the last thing
is sort of extending that where you
could have a journal of your
investigation of a specific bug and you
can keep that on the bug tracker instead
of in your brain or in your personal
notebook so that someone else could pick
up your debugging session and just
continue or look back and see what
you've already tried so lastly I'll move
on to a tool called scribe so probes is
a way to navigate through an execution
that's still pretty statement and code
centric and you know you can navigate to
specific log statements but if it's
still really you know you it requires
just know where to look in the code so
we're thinking deeper like how would we
get a tool that lets you go from output
back to the relevant pieces of code so
sorry is a feature location tool for
visual changes on the webpage so I'm
going to play a short video which shows
the motivation and how the prototype
works fellow pers often Reeves existing
designs and interactive behaviors by
inspecting the code of third-party
websites to reuse an interactive
behavior a developer needs to understand
what it does what Dom and CSS it uses
and how interactivity is programmed
through JavaScript feature location the
task of working backwards from outputs
to internal states to source code is
time consuming and difficult with
existing tools first current tools do
not capture an interface elements output
history as it changes users can only
inspect the current state of the web
page second isolating and elements
internal states is difficult because
these states Dom elements and CSS styles
have hidden non-local interactions third
existing tools provide no link between
changes to the dominant CSS in the
JavaScript code responsible for these
changes
we present scry a feature location tool
that directly addresses these challenges
scry introduces a novel feature location
workflow first a user selects output
examples from an output history timeline
then scribe visualizes differences
between the Dom and CSS that was used to
render each example finally the user
selects a single difference to see the
operations and source code responsible
for the change we illustrate these
features and design contributions while
using scried to inspect several examples
this article web page as a search box
that expands and contracts when clicked
however it's difficult to see exactly
how the transition is implemented
because the interaction is brief and
intermediate states are lost to inspect
this element using scry the user first
finds the dominant limit using the web
inspectors inspect function then the
user tell scribe to start tracking the
element as the user demonstrates the
search bar interaction scribe populates
a time line with the output history of
the search bar as they tracked elements
visual appearance changes screen
captures output screenshots internal
states and a log of state changes using
this data scribe can show the rendering
engines inputs and outputs at any
intermediate state this used three panes
show the screenshot dom subtree and CSS
styles for the selected element when a
user selects two screenshots scribe
visualizes the differences between them
using familiar inline tiff annotations
here we see that the root elements width
is animated in the inner form elements
opacity property is also animated this
tetris games user interface is
implemented entirely with Dom elements
and CSS styles let's use scribe to find
out how the game board works pieces can
be translated rotated or dropped if we
compare two screenshots where a piece
was translated we can see that the same
Dom elements are used but a repositioned
using CSS if we compare two screenshots
where peace is rotated we see that
instead the Dom elements for the piece
have been removed and readded added
elements and attributes have green
highlights while removed elements and
attributes have red highlights by
clicking on a diff annotation we can see
the operations that caused the change
each operation is linked to JavaScript
source code which can be previewed on
the right pane or displayed in the main
content browser functions named rotate
piece and rebuild peace seemed like good
starting places for investigating the
relevant game logic we have demonstrated
scry a tool for locating the code
implements interactive behavior scribe
supports a new workflow for feature
location based on selecting output
examples comparing their internal states
and jumping to the javascript code
responsible for state and output changes
so that's sort of the demo of the UI so
and the rest of the talk I like to
explain what's going on under the hood
so that these changes can be tracked in
and link back to source code so first
thing to start with is well in a web
page what determines the appearance of
some element right and by some element
we're talking about something you click
on the UI which could also include its
sub tree in the Dom so the first thing
is well you know what's in the Dom you
know is it a heading is it a form
element what text is underneath it and
the tree that's going to influence how
it looks the second thing is CSS
properties so things like the color
whether it should be floated or
displayed like a table should be
underlined you know our children table
rows or they columns that sort of thing
so that can influence how something is
rendered so these things get fed into
the rendering engine and that will spit
out
you know some rendering of it it's a
bitmap so so that's how we get 11 visual
state how can we go between visual
states so we can change either the style
properties or the Dom tree so on the
left side there's some ways to mutate
the Dom tree there's you know structure
operations like I added or removed a
child or I changed my ordinal among my
siblings that could change of CSS rules
and on the right side there's lots of
different ways that properties for a
specific element could be changed so
style properties can come from an inline
style which is sort of a legacy way to
say this node has exactly these
properties attached to it they could
also come from style rules which are
declarative ways to say match all these
elements in the page and apply these
properties to them so you could have a
CSS rule like dot active and that will
match anything with the active class and
maybe you know make make the element
look more active and you know if a rule
start snatching or stops matching that
could change what properties get applied
to the element so and then the last
thing is animations the browser is able
to animate specific style properties
like the opacity of an element so you
can say take opacity very it from 0 to 1
and take one second and underneath the
hood what the browser will do is figure
out how many frames to draw interpolate
opacity for each of those and apply the
property to the element before it gets
rendered so all these things could
influence you know what an element looks
like on the page so if we need to track
all these things it would be really
expensive and invasive to record them
and figure out which would then actually
have any effect and you know the
browser's like the million lines of code
so we don't want to have to reason
specifically about what causes changes
so instead we can take a more output
based approach and so what's cry does is
it has a snapshot of the element is a
bitmap and what instruments are
notifications from the rendering system
about
rectangles are painted on the screen so
we have a snapshot and we have a bunch
of rectangle notifications and the first
thing we do is see if they intersect if
we repaint it and it doesn't intersect
with the target element then it couldn't
have changed and if it does intersect
then you know we could have possibly
repainted some part of it in a different
way so we use this as like the first
pass if these things do intersect then
we'll go and take an actual bitmap
snapshot of the element again and then
we'll do a fuzzy diff of the two images
to see if they're pretty similar and
right now scry uses 1% mean pixel
difference so what it does is it Lex
pixel by pixel and it computes how
different it is and if it's greater than
one percent over the whole image then we
say it's two different this is a new
visual state and this is really designed
to get around gotcha like sub-pixel
rendering or font smoothing or video
encoding there's actually lots of
different sources of non-determinism in
the rendering pipeline itself so our
experience 1% of pretty good level at
which to you know except do reject
changes so once that passes i can
capture a snapshot of this new visual
state and inside this visual state we
want to capture what it looks like so a
bitmap of the element and it's subtree
we also want to capture the subtree that
you know what what was in the tree and
its attributes and so forth and for each
of those elements we also want to
capture what its CSS properties were
applied to that and also like what are
the sources of the CSS properties right
it could come from a rule or if you come
from an inline style and we want to know
the source of that so we can link it
back to specific places in the source
code and lastly record the mutation
operations mainly on the Dom tree so a
pen and remove child and changing
attributes and so forth and these are
you Slater to figure out you know what
actually caused the change
so in the UI you saw that you know the
main thing is you can select snapshots
and compare them so there's lots of ways
to compare trees and we picked probably
the simplest thing you could do which is
look at each node on either side and
compare them note by note instead of
doing like tree differencing or edit
distance or anything like that and for
the small snapshot so we're taking that
works fine you walk over you know 100
nodes you look at them pairwise but you
know you can decide you know does it
have fewer children does it have more
children have attributes changed have
the style properties applied change as
their source has changed so you compute
a change summary of changes to the Dom
structure and the properties for each
node and then you visualize that using
green and red if markers and this
crucially relies on the fact that we can
instrument the browser and we have a
stable identity for a dom elements
across runs so we know for sure that
this element on the heap is still the
same element at this later snapshot so
we can use that to you know just index
into both of these snapshots and then
like exactly compare them instead of
trying to you know do some fuzzy
matching to figure out which node
corresponds to which node so and then
the last thing we need to do is figure
out what code actually caused the change
or changes so we see back here we have a
list of these mutation operations and
what we need to do is figure out or some
specific change that we've computed what
operations are responsible for it so
this is called slicing and so this is
pretty straightforward slicing approach
first you instrument all these
operations you build a dependency graft
so in the example we had this subtree
that was added and removed so you would
have individual events for we added this
element we edit its child we set the
attribute setting the attribute on some
child that was added since the last
snapshot is going to depend on the node
existing so there's a pretty
straightforward Dom tree shaped
dependency graph here and based on this
change summary
we look at a bunch of candidates for
what sort of operation could have caused
this change now if we see that the class
was changed to this string we look for
that that string being set in an
operation and from there we work
backwards in the dependency graph to
find what other operations were required
for this thing to happen and then in the
UI that's presented as just a subset of
the operations we captured and each of
those has a call stack but we can see
what you know what code contacts called
this so it sounds nice but there were a
lot of fun interesting challenges along
the way which I'll briefly cover here so
the first thing is that a rendering is
sort of weird on the web and that you
can have some target element like that
search box and if we ask the browser to
just render the search box it's not
going to render the black background
because that's from the the root node
the body so you know according to the
browser you know that that's what this
node looks like so someone else could
change the background color the user is
going to think that the search box has
changed but it if you just look at the
subtree it hasn't so you have to be
careful about how are you defining like
what what the element is and you know
what the visual aspects are the second
thing is software versus hardware
rendering we depend on these paint
notifications and those only happen if
you're rendering in hartland software
know if you're rendering on the GPU and
compositing and all this stuff it's
happening happening somewhere off there
on the GPU and you're never going to
figure out what exactly happens unless
you know you can reconstruct that from
your instructions to it so in some cases
we have to turn off hardware
acceleration for animations and so forth
so that we can get accurate paint
notifications and then capture the
intermediate states of the animation and
some animations you can't actually
accelerate on the GPU like changing font
size or something so in that case we're
fine but in other ones like transl or
transforming something you know you
really have to do this the next thing is
this relies on stable Dom element
identities both from the back end we
know that something on the heap here is
also the same element at some later
time but also in the application so in
that Tetris game things were pretty
stable from snapshot to snapshot you
know the red piece is still the red
piece and has the same elements but in
more complex applications a lot of times
Dom nodes or just uses like these
fungible building blocks that you need
to you know construct the UI so react is
a popular library these days and you
build up a virtual Dom using only
JavaScript and under the hood we act has
just like pool of Dom elements that just
like plugs in here and there to create
the proper visual effect so this really
confuse describe because it's expecting
these nodes have some sort of identity
in the user space and another thing is
that a lot of times people use jQuery or
other libraries to do all the UI work
for them and this can make the call
stacks pretty useless or hard to
interpret because there's like five
layers of dollar signs and gobbledygook
so may be great if you can filter out
some of that but that that's sort of a
orthogonal problem the last thing is
that sometimes a lot of style properties
change and they may be changed
intentionally but they still have no
effect on rendering so for example
sometimes people try to move text to be
left or right or centered and the
property that they're using doesn't
actually apply to the the layout context
of the element they're applying it to so
it's really a no-op and it changes based
on classes but really it has no effect
on what you see on the screen so it
would be straight forward to you know go
one by one and prune the styles that
don't really have any effect and just
not show them in the diff because you
know the user might look at that and
think it's significant but really
nothing changed because of it so last
challenge here is that to use this in a
replay context where you're at some post
date and you want to go back in time to
see the history of an element you need
to be able to identify that element on
successive replays of the execution so
we can't just rely on the heap address
we need to do something a little more
intelligent like a saw
and deterministic IDs or something like
that so for this you notice it didn't
use replay at all because for this we
just prototyped the the rendering stuff
but earlier version was integrated with
replays so we could use this to go back
but for time I you know just sort of
stopped working on that part but we know
like how to hook up together so this
sort of illustrates you know several
different ways of moving through a
recording once you've captured it using
dolos into a future work there's lots of
different ways you could take these ways
of capturing and navigating recording
and applying it to more no specific
domains so the one that get asked about
the most is can I use this for bug
reports and the answer is yes you could
once it's finished and serialization
works right now it doesn't quite work
because you know it's not important for
the video but um you know we can
serialize the whole recording to JSON
and load it back up and that works fine
but you know there's probably still some
determinism bugs so we're not saving
enough state the next thing is that you
can imagine more complicated analysis
that actually do some sort of dynamic
analysis or on the floor slicing or
something somewhat heavy weight right
you don't want to necessarily do that
while the user is staring at the program
executing but if you're able to take
this recording and then replay it on the
beefy computer or in the cloud then you
could you know press the button you send
a request to the cloud it goes and gets
all this data and it shows it to you on
your own computer so this would be great
because you know then we don't care so
much about the overhead of dynamic
analysis as long as it's you know going
to finish some time we can just go do
this in the cloud and we can't really do
this in a lot of context now something
from a more testing angle is using these
recordings as a way to sort of author
test cases so if you develop an
interactive application you've probably
been through the pain of trying to write
a test that simulates the user and you
know you have to get the coordinates and
like enter them in and so forth and it's
kind of a drag so what if you could just
take a recording and then
take a subset of that and spin it off as
a test so I haven't looked at this but
other people I'm working with of in
trying to do this and also minimizing
you know the stuff in the recording to
just stuff is necessary for the test
case another interesting Avenue is using
these recordings as performance
benchmarks for interactive web pages so
if you go to edge website or chrome or
whatever there's lots of benchmarks and
most of them are just JavaScript and
then there's a few interactive ones but
really it's like how fast can we load
this page and play around on the
Facebook gate page right you're not
logging into facebook and then like
messing around with it and you know
sharing photos and for a user's that's
sort of like the more important thing
not how fast can I run you know my
mandrill benchmarks so you could use
replay as a way to get deeper into those
applications to you know exactly replay
those interactive behaviors or you could
synthesize benchmark that simulates it
and lastly like there's just so many
ways you could plug in existing tools to
be retroactive in the sense that you can
use them as ways to index into the past
execution so I mentioned profilers as
one but there's also lots of other
opportunities so that's all I got right
now I'm happy to take questions so how
does the replay of capabilities work if
you have like web page with multiple
iframes of it for origins communicating
over post message really maybe on
multiple threads or something what's
that so um the unit of replay right now
is pretty much a tab on the web page so
if the page is gonna communicate anyway
they you know they'll be recorded
together so we record the mainframe and
all the iframes underneath it and post
message would be recorded as sort of
like a event loop piece of work on
replay for things like workers we are
not actually executing the workers we
just save the messages they send back
and we play that on the main thread
and like right now since you can't see
workers or what they're doing and a lot
of developer tools said you know it
doesn't matter so much but you could
also you know make us up recording for
those if you wanted to step into it can
you kind of talk a bit more about the
browser support they require it comes up
how much would how much of it wasn't
using well-established extension points
or how much of it was just change in
browser code it's all changing browser
coach okay um so there's the prior work
that tries to rewrite JavaScript to
capture this stuff like mug shot done by
James Pickens here but problem with that
is it doesn't work well developer tools
like you look at this rewritten code and
you try and step through it and you're
like what the heck so we were just like
well we're going to take off all the you
know niceness and just like change the
actual browser and I used to think is
possible through extension points but
now like there's just so many weird
internal sources of non-determinism in
the browser itself that it doesn't
really seem possible anymore in
particular ordering of a synchronous
work that can trigger JavaScript but
wasn't really caused by the user the
program itself the admission is future
work perhaps taking replays and
rerunning to this tests hmm I know that
selenium has that capability but it's
kind of like it's a starting point for
your tests and then you were supposed to
refactor do you can page out to call
would you try to but given that i guess
you can make this more deterministic
would you think this is an easier way to
write tests so one is you can just
replay the recording itself as a test
which is fairly straightforward the
other is like synthesizing a selenium
tests from it and you know we haven't
really looked at what's required there
but you know we have all the event data
so it's pretty much one to one we'd be
able to automatically go right the
selenium commands to do that whether
it's easier or not I don't know this
sort of depends on and which efforts
applied to it there's also issues like
do we package the resources or do we
want them to be live so a lot of it
seems to depend on like what the use
case is how well that would work maybe
somewhat difficult you guys the goldie
bowls which is a purpose of having a
desperate we difficult to keep the
knowledge a mystic playback in sync
right because they're a cookie if they
go make a different number of calls to a
not domesticate guy doing it during a
loop cycle right you not have them
memorized value for some of us right
right so for testing where you you want
different executions to happen you do
need to be able to have the replay go
like off the track so to speak and right
now like we haven't really investigated
that like what you need to do but some
people at Mozilla have been working on
this tool called RR which is essentially
replay a POSIX calls so you can replay
the entire browser together and they
have this ability to make a diversion
session which is essentially you you go
and make a different ending and then you
could go back so we've been looking at
ways to support that but the bigger
point is that like yeah you can make
different network requests or
different code could run and respect and
response to a click so you need to be
very explicit about what you want to be
the same every time is it the user is
that the network is it timers that just
seems like a you know what you want case
by case alright sounds good any other
questions your yeah I guess we're good</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>