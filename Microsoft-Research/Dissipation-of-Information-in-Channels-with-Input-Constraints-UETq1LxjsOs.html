<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Dissipation of Information in Channels with Input Constraints | Coder Coacher - Coaching Coders</title><meta content="Dissipation of Information in Channels with Input Constraints - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Dissipation of Information in Channels with Input Constraints</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UETq1LxjsOs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
ok so we're delighted to have your
poliansky tell us about dissipation of
informations in channels within put
constraints Thanks evolved yeah so this
is a joint work with my colleague from
urbana champaign and ok so i want to so
there is a basically in this talk there
is one technique and somewhere in the
middle and but i will start with the
problem so i mean i'm most excited about
the technique not the particular problem
that we solved using it but the problem
might be of some nice i mean it can be
easily interpreted ok so so we have some
original message and you want you are
allowed to process it to encode it into
some other real number X 1 the only
thing I want is that every time you
encode it that they expected square stay
bounded so you this is to prohibit you
to use two large constellation ok and
then after you encode the original
message into something its perturbed by
Gaussian noise and that's what gets fed
to the next encoder and then the next
encoder again looks maybe he will try to
first infer the original x0 which could
be for example a binary bit right and
then rien called it and so on but the
idea is that you are allowed to do
whatever you want as long as you don't
violate the power constraint and the
question is yes so you are a year is to
design this processors so that you can
still reconstruct even after great many
hopes you can still reconstruct the
original a the original message ok so
yeah so we call a chain of Gaussian
channels all right so initially when
when I started so when we started
looking at it was completely obvious to
us that I sympathetically I mean as n
grows large there must be some
asymptotic independence and I used to
start the stock by saying ok obviously
we conjecture that you know each stage
has finite energy budget so you cannot
know is completely so therefore after
aight many hopes basically you have
asymptotic decoupling so the Joint
Distribution cuisine initial and the
last message is approximately
independent how do you Dino is I mean is
the X you yeah baidu noise I mean yes
oppose x0 is plus minus one for example
right so then you can encode it into
plus minus trillion right then you
convolve with the noise you get some
real number so you threshold it at zero
and you again wrinkle it to post-
million oh no ya f can be anything but
that's a great at some point i will
return back ok so yeah and i used to say
that it's obvious so my talk is just to
prove this obvious fact but then I
started to sample opinions before I say
that turns out that some people actually
believe that it's possible and the more
advanced I'm an information theorist by
training so the more advanced people you
ask and then the more of them kind of
say because they know that okay oh yeah
and they used to also say that this
works everything I will say works for
arbitrary dimension so it doesn't have
to be scalar it can be arbitrary
dimension and so they would of course
but error correcting codes work right so
there must be some way to preserve
information so nonetheless yeah I'll
spare you the a this kind of thing I
will show that indeed we have asymptotic
decoupling and I will gauge it in one of
the three metrics so it's a total
variation distance or TV then the KL
divergence I hope so people here know
what KL divergence is so basically it's
a symmetric of some kind of distance
like function and then also we have a we
have asymptotic decorrelation each of
this has an equivalent interpretation so
for example conversions in KL distance
is equivalent is exactly cool just by
definition of mutual information the two
convergence of the mutual information is
in the first and the last guy to 0 and D
correlation which is important for one
of the applications is exactly
equivalent to saying that the best
minimum the minimum mean square error
estimate of the original message given
the output actually becomes
asymptotically trivial so just
estimating by the mean
gives you the optimal thing that's and
roughly speaking I mean if you if you
know some a little bit about information
theory or I mean even some general
probability it's easy to see that KL
implies both others so by those
directions by pins Karen quality this by
something called rate distortion theory
but so it was very natural to us to
immediately consider k on well let's
just try to do KL ok so let's see what
what kind of standard tools we have in
our arsenal so the first two is the
famous data processing inequality which
says that divergence is distance between
output distributions between
distributions perturbed by some
stochastic transformation may be
discreet maybe non discreet it just it
cannot expand it so it always shrinks ok
so so the equivalent statement is that
mutual information and Markov chain it
does not increase ok so what does it
tell us about our chain something of
course very obvious that basically can
apply it repeatedly and say that ok the
sequence of mutual informations cannot
increase but of course it doesn't tell
you that it has to decrease right so
that's the only thing we know ok but
this suggests that we what we need is we
need to some quantitative version of
data processing inequality right maybe
it doesn't decay maybe we can strengthen
the state cannot expand right and ok so
there is such a thing and it's called
strong data processing in quality so
actually from people who heard this talk
I mean for many people this is a big
surprise that there is such a thing so I
want to also spend some time on this
inequality so ok so what it says is that
actually for most channels you can
insert a multiplicative constant here
which is which I denote by etta KL the
contraction coefficient and so then not
only divergence decays multiplicatively
but actually I mean this is a cool to
say that mutual information decays
multiplicatively to ok so now if you
have this attic al which formula is just
defined by the soup of the ratio of
input output to in today
or mutual informations then I mean
there's this famous result by Aceveda
engage that for discreet in decomposable
channels but in the composable chance
basically you don't want something like
some zero error effects there so you
want every input to be to be spreading
to all possible outputs roughly speaking
but basically everything unless I mean
binary symmetric channel binary erasure
channel all the stuff is in decomposable
so okay so this coefficient at the kale
has a lot of funny connections I mean a
lot of connections to some cute topics
like this original also had a gash paper
the reason they were interested they
didn't care about this strong data
processing what they cared about is the
hyper contractivity which shall mention
a little bit later oh yeah and they
showed that the the worst possible hyper
contractility ratio is actually exactly
given by this coefficient at the kale
okay and then there is some connections
to log so belief in equality that's and
so on so let me just the focus on all
right I'm being a little sliver it what
exactly did this diagram mean what are
the two areas on the left mean and what
the two yeah exactly yeah sorry this is
this is a good question so I pull just
everybody who watches the video and
there are many of people like oh they
can see the board oh oh yeah yes okay so
I mean what I mean to say when I draw
something like this it's just that ok so
you have this stochastic colonel so this
is just this is this just means by
definition that py is just the integral
of d PX of sorry yeah d PX of p y given
x equals x right so that's what I mean
it's just the averaging of the input
distribution over this over this so for
example I mean the probability of a SAT
ii is just the averaging of this
function right I mean nothing fancy i
don't know it's just a marginal of the
joined you form the joint PX x p y given
x and then you compute the marginal on
why not exactly yes yes exactly exactly
so that's what I say so basically the
way different yeah it's like the way
information theorist feel pretty good to
think about channels just it's something
that morphs distributions right so
Sophie X is more than 2 py or pair is
most into another pair okay hey yeah so
this yeah all right so now of course if
this contraction coefficient is less
than 1 then this mutual information I
mean the gain by the same repeated
application you get exponential
convergence 20 right ok now the sad part
is that for my channel this contraction
coefficient is actually won a the good
the good news is that for a very slight
modification and the i'll mention i'll
spend some time on this so if you
replace this expected constraint with an
absolute constraint then it will be
actually strictly less than 1 then this
contraction coefficient will be less
than 1 okay so let me explain how to
derive this in the fancy way so this is
a photo of Roland the bruising who is my
personal hero and then I couldn't resist
inserting something here about him so
okay so now the portion to find this
coefficient which is the same as et al
but I denote that the TV this is the
contraction ratio of total variation
again as we pass from the channel and of
course what it's very easy decisions
this is just an l1 norm then the worst
the worst case inputs are just Delta
functions right at two inputs so it's
very easy to compute and the original
motivation for studying this coefficient
was in mixing of Markov chains at least
for the bruising a ok so yeah so how
does that arise this is a slide that
inserts because I knew that there are
many people who like Markov chains here
so a yes so I mean of course you all
know that Markov chains mix
exponentially I mean frequently right so
and then these coefficients which i will
use in the store
get the TV at the high square at the
kale they just correspond to mixing in
various different distances all right so
you know that this typically is obtained
from logs over this is what is called
spectral gap and so forth yes so then
the some people studied the general
relations between these contraction
coefficients for discrete and general
channels like for example for final
state mark change there is this funny
statement that Etta care was less than
one if and only if at the TV is less
than one and at the TV is easy to check
right because you just need to check on
Delta functions for ethical you don't
know yes so then there is this more
recent result about that contraction
coefficient in high square is exactly go
to KL there is some technical difference
between so this statement of course
doesn't mean an incorrect statement that
lok sabha love constant is also always
equal to spectral gap which we know it's
not true but it might appear like it so
if you're interested I can explain what
the differences but they yeah but let me
proceed for now okay so now I wanted
this is another slide which is tangent
to my talk in some sense but I wanted to
say that a little bit about this theorem
so this six people proved the result
which says that ethical which is tough
to compute it's actually always upper
bounded by a DTD strictly okay so why is
it interesting so here is the fun
implication all this so let's consider a
finite-state markov chain and t will be
its propagation operator then let me
denote by the independent markov chain
just something that averages a function
with respect to environment distribution
okay so now suppose that so then this is
just P P star right now suppose it rolls
of this stochastic matrix are all
basically they are all closer than
one-half to the P star then it implies
by triangle inequality that at the TV
will be strictly less than 1 right so
you can insert p star here all right and
then by a by this theorem which is
simple result it's kind of amplify
knowledge but the simple drag on quality
into something that at the kale is less
than 1 and n biosphere the GOC what you
know is that then this stochastic
operator will be hyper contractive right
so you can upper bound qs norm by
slightly less beef norm okay and what
this tells you and notice that the this
estimate I mean they give you an
explicit essence I say take P to be a
t'kyel times Q right so now for any I
mean if you replace one half here with
some something slightly smaller you get
an explicit estimate for which operators
are hyper contracted with constant one
yes again try to slow you down a little
so I decided ETA into total variation
that's the one step contraction yes yes
yes what is et al in terms of the logs
over that concept it's a it's
effectively equal to is the log so yes
is it I mean dependent because you seek
hey when there are many locks or believe
Constance right there is two looks over
the one log so modified logs over looks
over for continuous time for discrete
time what is it in terms of the logs are
constant well will you have the dalish
life form and in the end but ok so you
this is a bottom in continuous time
right yes so for continuous Emma group
PT right yes but then it's not connected
because i'm talking about discrete
groups so so the key so
a yes so there is a so if you define
something like this okay yes mm-hmm yo
if not if it said with Logan just on the
light I want okay you want this yes I
don't think it's related to this one so
it's related to this one to the modified
yes the model modified to the modified
there is a statement which says at the
KL is always upper bounded by 1 minus
alpha and lower bound by wireless some
absolute constant alpha but the of
course for similar groups frequently I
mean one modified looks over because the
usual logs over right not always yes yes
so in this sense yes so it's not the
universal connection Randall
prepositions there is no yes yes yes
yeah anyways but the I mean lok sabha
was not going to be mentioned here so
it's a what I wanted to talk about is a
yeah so you see I mean this this
connection was done without the I think
you're asking because hyper
contractility frequently is derived from
lok sabha right by integrating low so
girl in quotes but the else whether gosh
this was completely discreet statement
so they didn't they didn't talk about
logs over fat also was just the
different methods so anyways so okay so
now the punch line here is that
surrounding this so in the space of all
stochastic matrices right so there is
one matrix e who's rose are all p star
right and then there is a ball
surrounding this there is a no
non-vanishing ball such that for every
operator inside this ball it's LP 2 lq
norm is one so it's a little bit the
funny situation right so it's basically
you move away from pure independence but
you but for there is a planner face so
to speak in this ball of of matrices
whose
form is upper bounded by one and this
effect is actually a yes let's ignore
this and this effect is funny because it
was actually discovered in including
space first by a cigar and pfefferman in
70s and then generalize to arbitrary
operators in 88 so basically it's the
it's the usual effect that if you if you
stay inside an operator which integrates
with respect you sure if there is a ball
surrounding an operator integrating with
respect to probability measure such that
they LP 2 lq estimate stays constant so
it's but this is a completely is
basically one line proof for discrete
cases following from that upper bound
okay so a let me return to my talk and
now so we still talking about this chain
and we're talking about the amplitude
amplitude constraint right so all axes
now are required required to be bounded
in amplitude so now we apply that
estimate at the KL upper bound by at the
TV so here it's very easy to estimate TV
contraction right because you just want
to take two points at plus minus a so
i'm going to use so I'm an engineering
department so i'm going to use Q
function which is a complimentary CDF of
the Gaussian distribution so I apologize
to everybody who is who likes the feel
better but they okay so now what we
derive from this is that for every
amplitude constraint channel you have
exponential convergence of mutual
information to zero and surprisingly
this is I mean this is the entire proof
right to line so there is a sequence of
papers actually which deals with this
Gaussian also called Gaussian line
networks for exactly this question and
they derive a worse exponent than us
with this two-line so I'm just saying
that this this theorem about ethical
upper bound by DT is very very
overlooked and very few people action
law about it I think it was rediscovered
by my claw in particular hits at some
point McLuhan we do all right okay so
now what about the power average power
constraint okay so if you are analyst
right at this point you can say okay I
mean come on from case
right some sort of truncation should
work well it turns out no in this
particular case I know so that's what we
thought I mean basically after amplitude
constraints I didn't want to work on
this but then my friend actually he
found the counter example he just said
well wait a second what if we take what
if he puts some mass at zero right and
the other mass is traveling farther and
farther apart so then when you mix them
so this these parts the total variation
the only thing contributing to the total
variation distance is this part right
but when you convolve with the gaussian
so you get basically two very far apart
gaussians right and so as this constant
a thought operation between the original
distributions the case turns out that
you can I mean the etta TV converges to
1 so say so basically the same binary
type distribution with two masses show
that the divergence and mooch
information all do not contract so there
is no contraction yeah very key to 0 yes
yes so key is the distance between PX
and QX TV distance right because this
just cancels when you subtract or so you
just get t yeah so as T goes to 0 and
yes then I mean so this basically proves
there is no hope of exponential
convergence right but maybe there is a
sub 0 exponential convergence oh okay so
this is our final result that for this
particular chain you have the super
trooper slow convergence right so I mean
we have a super trooper slow estimates
for now right so basically it's 1 over
log n where n is the number of steps ah
120 students as you're going along the
two the two signals you have there only
so you're trying their only difference
would be
translation by different calcium in a
run and and in this when you're looking
at these ratios you consider in turn to
general right yes translations are very
special yes so you say that maybe you
can exploit the fact that after a
certain number of iterations right you
are not considering arbitrary
distribution but now notice that there
is this f2 right so why so for example
why one is a convolution so it's a very
smooth it's a very smooth distribution
right because something convolved with
Gaussian but then you apply some
arbitrary function right it can be some
crazy stuff so then it makes
distribution of X to something crazy
again so a yeah but there is obviously a
workaround so as this as this result
shows so and again I mean these
estimates were so slow that it was just
painful so we also make made sure that
actually our estimates are more or less
tight so up to this log log n factors
there is a lower bound to so you can't
improve it by much okay so what I was
going to talk next is how how to prove
it what what's the idea to prove this
thing all right so the idea is the
following so a so that strong data
processing in quality says how much the
input divergence contracts right so more
precise characterization would be what
if we just compute the full curve right
we can we do the following thing so we
take two dimensional plane and then for
every pair of distributions we compute
the input divergence and the output
divergence right and put it here and
then just iterate over all pairs of
distributions so data processing
inequality tells us that well all the
points will be strictly below the
diagonal if there is strong data
processing them to tell us that all
distributions are here we know it's not
true right but maybe the situation is
like this and why would it be good
because well then what we can do is we
start from somewhere we don't know where
but from some point right and now and
then we proceed with the situation's
whoops so
and if this thing stays strictly below
the diagonal that we will converge to 0
right and so this was our hope so
basically the goal is to find now this
was very exciting to us because we take
a Gaussian channel is something which
was studied from 1948 the very first
paper of Shannon the most famous thing
is one half log want was pee right so
Gaussian channels beaten to death here
we want to associate the girls and
channel some curve right so something
new to say about Dawson channel so we
spent some time trying to make sure
nobody actually do since know and it was
surprising so we were excited at this
point so that okay hey let's do this
let's calculate this thing yeah so
basically once this curve is curved a
little bit then we are done now then
there was a sad news at this moment we
try to compute F care and we realized
why nobody computer before it was
actually it's exactly the straight line
so a so there is nothing good and I
don't know why we didn't give up at this
point but because I mean every I mean
for everybody pretty discouraging right
you trying to prove a result which
obviously holds right and nobody cares
about and you actually write and you try
it for so many I mean by this time we
already tried a few other things which I
don't mention that they it doesn't work
but here at this time for some reason we
also decided to try ftv and magically
turned out for ftv there is actually
contraction and to color our hero de
Bruin we decided to call ftv the
diversion curve of of a channel okay so
let me be formal alright so first of all
this is the strategy for the proof which
i'm sure i'm not going to have time to
go over but yeah so the idea is to first
work with TV the start operation to show
you cannot transmit one bit then
upgraded to general in general input
general message x 0 then there is a
trick how to go from the coupling entity
to decoupling in KL in our case it only
we have we actually use some special
property of the Gaussian noise here then
there is a general trick to go from
decoupling in TV 2d correlation anyway
so we will see if we can get there okay
so let's start with the transmitting one
bit
a so suppose X 0 is plus minus 1 like we
probably right and you're trying to show
that there it does not exist the test
which distinguishes won 4-1 right and
the only thing you need to do is you
need to propagate you need to introduce
two measures P and Q right one is
conditioned on +1 not conditional minus
1 and just look how these two measures
morph as they go along this chain right
okay so right so then here's the formal
definition of ftv ftv is just the
supremo overall p y minus Qi over all TV
total variations at the output given
that the total variation at the input is
less than T and that there is a power
constraint so the average power under p
and q is less than P okay yeah and of
course the bruising coefficient is just
the maximum slope of the bruin curve
which is always at 0 okay a right so now
here is another surprise eh at least one
not only we were able to prove that ftv
is slightly below we actually found it
exactly so this is not an estimate this
is exact value so now the interesting
thing another interesting thing and at
this point of course we did know the
speed of convergence but then once we
found ftv we realize that it's actually
a funny curve because of this expression
it's a a it's smooth but not analytic so
all the derivatives so the first
derivative is 1 and all the other
derivatives are 0 so that's why your
convergence is so slow that's why it's 1
over log n these iterations so basically
it kisses the the straight line very
very a yes too high to very high a
degree okay so how do we watch the idea
okay so the idea is the following so you
give me to eat two distributions PX and
QX whose total variation is T right and
we are trying to bound how does the
total ration shrink under convolving
with Gaussian noise right that's our job
and the idea was the following so now
suppose we couple xpx 2qx so under this
coupling p right so then a basically
under the good situation x minus x
I will be very we actually equal to 0
right and then you can also couple the
noise so that y will be approximately
equal to y prime right but but you don't
know what to do when X minus X prime is
is not equal to each other right but
then it turns out that this cannot
happen too often because of the power
constrain now if I describe this to you
you'd say okay obviously can convert it
into some bound why does it the why do
you get exactly exact estimate so there
is another thing where we got lucky and
so this is the full proof I don't know
if you want to it's very easy i mean
it's a very easy exercise after being so
lucky for so many times that it was just
very easy to do this yes yeah okay so
let's go through it actually I said I
don't need laser pointer but it would be
useful now but nobody has say find it
okay so uh yeah okay thanks to do more
coffee just dropped from the ceiling
yeah okay so a solar so one thing one
function we need to introduce associated
to the channel is how does the total
variation distance interplay with the
client distance in the input space so
it's just this thing so it's very easy
to compute it it's just something like
something involving the Q function okay
now so here's where we will lock the
first time so TV total variation is the
only f divergence which is also vas
search time distance so if you don't
know whatever divergences doesn't matter
but so okay so then as any of I searched
and this is its convex in the pair so
then what we do is we just we can
average overall Delta input
distributions so that this just gives
you this estimates expectation of this
data X minus X prime so now theta for
arbitrary dimension right so even if
this is not scalar total variation of
the god of the isotropic gaussians only
depends on the distance between X and X
prime because you can always rotate so
you can put absolute value inside then
you can apply Jensen right you condition
X not equal to X Prime
you apply Jensen and you estimate this
distance so you have the distance of
first moments but you can estimate it by
the second moments right and that's it
yes so basically I mean after we
obtained all this we asked ourselves why
we were so lucky and then turns out that
yeah so one lock illness was that this
is TV not the arbitrary F divergence so
we can do the coupling trick and the
second where we were lucky is that this
is a this works this works for any
unimodal symmetric noise symmetric
meaning X noise on it depends on the
absolute value on the distance 20 okay
yeah and for the lower bound you just
take the same distribution as I
mentioned before so they attained lower
bound exactly okay so a I don't know is
should I it seems like yes basically
pretty trivial one say okay there are
many more generalizations and you want
to know about a soldier's you can
generalize to are the most important
thing is that you can generalize to
arbitrary dimension for Gaussian at
least for Gaussian noise okay so a all
right so I these are the two slides I
inserted here so there are two so if P
is is something that each is you and
then so you can interpret this result
exactly as a statement about pdes so
let's take the heat equation right so
you start your sewing heat equation on
RN with the initial condition f of X
then you know that because of just by
integration by parts l1 norm so L 2 norm
is of course preserved but l1 norm
decays so of the solution at time T
decays to the average volume right so
the initial average value of the
question is how fast and so we gave a
best possible estimate he of the speed
of this decay which is given here I mean
it's just a reinterpretation right a
convolution with Gaussian is exactly a
solution to this PDE and the funny thing
is that i don't know I've another PD
person so I don't know if they study
this but what is lets you do is it lets
you trade the speed of convergence the
speed of decay in l1 more let's say
integral of F at 0 so it gives you the
estimate of 1 over square root tea
estimate of l1 norm at what price of
course doesn't come for free so the
price is that you there is a tail bound
on on the on the initial solution so we
trade the knowledge of the tales of the
initial a function initial condition yes
a and then you extract from it so that I
thought it's funny so another another
punch line yes or another corollary we
observed is that you can also derive
something about CLT so this is a CLT in
what we call smooth total raishin so let
me go over this so remember that the
intermediate step was to introduce this
coupling between a convolve versions of
P and P prime estimated like this right
then we apply Jensen at this to get this
thing and then in our proof we just
estimate the expectation of X minus X
prime through the second moment right
distance between l1 distance while to
but actually why didn't why don't you
take the best coupling between P and P
Prime that minimizes that one distance
so then you get the an estimate in terms
of a search time distance right and of
course why certain distance that we want
distance is something we all love
because there is a steins method to
derive I mean doty quick estimates for
convergence and what searched and one
distance so you get the estimate like
this so that give me any AJ's id0 mean
and unit variance add tiny little bit of
Gaussian to this normalized sounds right
then it will be closed in total
variation to the Gaussian was one plus
Sigma square and here's an explicit
estimate of of the closest so this is
why we call it smoothed over Asian
because of course if you said well I
don't care about the smooth version what
about just the original one of course
there is no converters in TV because if
AJ is discrete then you will always have
a distance one actually prokhorov showed
that to have convergence in non-smooth
than the usual TV the only thing you
need is that after a few convolutions
you
have some non-trivial absolutely
continuous component but what is more
fun is that there is an exactly there is
the best estimate if from 62 I don't
know if you all know or anybody here
knows about this I was surprised that
there is an exact estimate of the
asymptotics of the speed of convergence
and total variation which I've never
heard about it smoothing the variax
sealed tight well various and estimates
corner of distance right it's here I'm
talking about TV but exactly yeah the
estimate heroes of the same order in the
last one are you assuming is absolutely
continue component of course yeah it's
under the same condition yes and
moreover a there is a mistake here it's
not absolute it's not the absolute third
moment to stay in well it just without
the absolute value so if it's zero if
you have zero skewness then the first
order term is less than 1 over square
root n but our estimate is order type
right typically unless you have a zero
skewness ok that's just again some
tangent which I thought you could maybe
like all right so now we're done with
the with the convergence to 0 if X 0 is
binary so let me mention a few things so
when I'm supposed to finish at four
thirty or something like this okay okay
so then let me just quickly mention how
to get these arrows so first of all to
get from binary to general case it's
convenient to introduce this this T
information with basically the distance
to product in total variation and then
just buy some convexity you get the
correct estimate that this total
variation distance actually the case
with the same speed satisfies the same
data processing inequality under ftv e
okay so now you can upgrade T
information to Shannon information by
some pretty easy coupling arguments and
for discrete I'm sorry if the if the x0
was finite then you can do some coupling
trick and if it's general then you need
to use
some discretization and I'm sure you
don't need to know about this with
anyway sits there is some analysis here
now for the correlation so how to get
correlation from TV again there is some
rate as I mentioned in the beginning if
you have convergence of mutual
information then there is convergence of
correlation just by rate distortion but
if you want the speed of convergence if
you want explicit essence you need to
get some you need to bounce on higher
moments for example if you get sub Gauss
if you have some grounds and x0 and just
by applying some again some coupling
plus holder and quality you get again
the log log and over a log an estimate
for the correlation okay now this is
definitely a you can skip well one punch
line here is that if you cared about
remember i showed you that fkl of T was
actually the straight line so it doesn't
contract at all turns out that already
divergences of order 1 and above so que
el divers if you need ever to further
one so a then they also do not contract
but everything below one is actually
does contract so I don't know something
which all right so I have about 15
minutes let me mention some applications
and implications of this stuff so here
is a one version of the main result just
to recap so we have a arbitrary chain of
processors which satisfy potentially I
mean these links could be infinite
dimensional right this could be buses
connecting computers together right as
long as the total l2 energy expended in
the communication is bounded you will
have mutual information converging to 0
as 1 over log n other convex functions
but so you still get the same rate if
you had texted a 1.1 instead of and no
rate changes yes yes I think rate
changes the it's a different functions
but it's still log in but it's different
power is something like minus P over
tours yeah
if you just have a first moment
condition there is nothing I don't
remember I need to I mean for TV there's
a yeah I think it's actually still Logan
but I need to double check the paper
yeah Hey ok so anyway so here's the sort
of philosophical implication right so
you store something on the hard drive
right every once in a while you decide
to copy it while the cop here only has
finite energy right and then what this
means is that well after great many
iterations you can't tell one bit even a
single bit written here under arbitrary
error correction right so this is yeah
and of course the point sign yes I
understand it everybody oh yes very good
very good exactly it's 1 over log n
which which basically killed us but now
here is a more fun example right so if
you have if you talk about memory
controller well memory controller does
exactly this right it reads a memory
cells with rice memory cells read the
rights we didn't write and that happens
pretty often it happens 16 times a
second so in one year you get to to the
29 reads and writes so you know once you
plug the plug things into here and then
the annoys you know noises nonzero
always right there is always some
terrible noise so I don't know I didn't
try to estimate what you get but they
comes to board because those constants
are actually hard to extract ok yeah if
you allow the power to go slowly yeah of
course no then of course you're golden
but how well I depends e I think square
root Logan is enough or login something
like this yes I mean that's kind of
interesting philosophically because if
you want people to be able to read your
paper in a million years and you're
compared to assume that yeah more energy
available to the road yeah I mean you
can't exceed the memory
yeah yeah hey anyway so this is one
implication so a another implication is
this section was motivation of my friend
my colleague who we derived this
originally with so a so there is this
question that these two guys leaps and
Martin's in control theory ask
themselves i said okay so suppose I have
an Gaussian random variable right and my
job is to preserve it intact but I have
to read and write it from time to time
and when i read that there is some
exogenous noise which corrupts me right
so then what can i do so what is the
optimal way to read and write what are
this processing funk so this is what
they call it memory less controller
because it only has one real number to
store right and when it reads it it gets
converted with gaussian noise so it's
very easy to show that if you want to
this operation to work once for one read
then f1 linear is up to it's much less
easy in fact i didn't read the paper but
it looked intimidating enough that for n
equals to steal the linear controller is
option why this is intimidating well
because you see now when you try to
optimize the minimum square minimum mean
square error right mean square error you
get composition of f2 and f one so it's
pretty tough to optimize over
compositions anyway so and then this
same guys show that actually for but for
anything greater than for when you have
greater than five convolutions than a
there is a nonlinear scheme which
actually outperforms any linear scheme
and a what what our results show that
first of all asymptotically you always d
correlate so you can't store I mean you
are as good as storing zero and the
second thing is that actually is the
linear fjs correspond to I mean special
restriction on the type of distributions
exactly as we won't mention they all are
Gaussian and then it's easy to show that
kale actually contracts so therefore you
actually decay exponentially fast so and
yeah so just from this you can
immediately see that non
nonlinear controllers are up to so
another application we discussed in the
paper is so I don't know how many people
here know about Gibbs matters but there
is the game which is played in certain
corners of science where what they do is
a Malaysian yeah is maybe here it's
played often I don't know about that ah
corner yeah okay all right hey yeah
anyway so given some conditional
distributions you try to guess if there
is a margin if there is a joint which
solves this problem and yes I'm sure
most of you know the idea is that you
try to see if there are multiple
solutions it means that there is some
phase transition happening and it's
funny and of course two dimensional
Isaac models the famous example but the
rule of thumb is that if links if this
dependence is weak right and there then
if there is a high temperature then
basically the nodes cannot communicate
too much so there is just one optimal
there is what just one solution so no
phase transition right that's why I've
been yeah it's like curie temperature
for ferromagnets that's the typical
example all right so yeah so one way to
show uniqueness is the bruins method
right so the version came up with this
ingenious idea he said well let's do the
contrapositive suppose there are two
solutions let's run them side by side
right and let's couple all the neighbors
using some coupling right if this
channels which go from neighbors to the
to the guy are contracting total
variation then you can refine your
coupling at this point right you can
refine it and keep refining it until
until the total variation of any finite
dimensional margin on this lesson
arbitrary number so there has to be the
same so what can we do with our method
because basically the I mean what you're
trying to do is we're trying to upgrade
this TV contraction to some general
thing so for example we could prove some
a estimate like this but basically if
you have some lower bondage
I mean ugly condition because yeah we
tried to be as general as possible if
there is some lower bound on all the
potentials then there cannot exist more
than one Gibbs measure which does not
explode to infinity so this is
additional qualifier right so the
bruising method gives you the statement
for all measures we only tell you that
well there is no some kind of schlossman
staircase kind of trick right where you
something explodes to infinity ok now
the final application is for the
circuits of noisy gates that's another
area where contraction coefficients were
used very successfully to a to prove
certain results so okay so a what is the
game here the game here is that suppose
you have a circuit of gates with K
inputs which computes some binary
function for example so then we say that
F essentially depends on all arguments
meaning every young argument influences
the output at least sometimes so then
it's easy to show that well the depth
should be lower bound by log N and then
ok and then the next step is to say ok
now suppose it and our gates instead of
being ideal they have some noise added
after each output right so for example a
the standard think that people study
starting from 1 Norman is the noise
Bernoulli right so there is a Bernoulli
Delta noise other than the question is
can you actually build an arbitrary
because the length of each path for
complicated function becomes log n so it
grows right so it kind of annoys
accumulates the question is can you
actually a construct any circuit such
that it out put its probability of error
is bounded away from one half and of
course I told this whole field started
when Van Orman show that actually yes so
here is an architecture by just using
three input majority voters which
requires constant depth expansion and
logarithmic gate gate expansion and
produces a circuit whose probability of
error does not it stays bounded away
from one half
a the depth yes well the depth basically
if you start with depth D then your
depth becomes rd now for some circuits
you need logan depth yes so then the
depth becomes our log n yeah so this for
von neumann's method and the roughly
speaking I mean it was shown that both
of this both constant depth expansion
logarithmic gate expansion is actually
required in both cases here the function
was the sort of everything so sort of
all the inputs yeah so now what is what
what what is our contribution here well
we can replace noise with Gaussian noise
right and bound the energy of each gate
right so now we don't care I mean maybe
use some funny operational amplifiers
right and I don't know how they work but
maybe they extract so it's actually one
of the critiques and for Norman's work
that day you can I mean heat himself so
that maybe I should have considered
analog analog wires as opposed to
digital wires now yeah but the so one
thing we can prove is here we can prove
a lower bound on the probability of
error a asymptotically is n grows to
infinity so it approaches some some
number which is a fixed point of certain
function yeah and then you can evaluate
it so you get certain so as SNR of which
gate changes say I mean I know Here I
plo's from minus twenty to twenty you
get some some lower bound on the
probability of error and the funny thing
here and this is the main difference
with the with the Bernoulli noise of
phenomenon pip injure is that here my
lower mount always stays I mean it
always stays bounded from one half okay
first of all maybe this lower bound is
not tight right but actually there is a
reason I don't know the answer but I I
think you should yeah and the four for
the blue line is actually what happens
is that for when Delta when the noise is
strong enough it is shown that K input
gate let's say three input gates cannot
can
not have probability of error different
from one half if epson if the noise is
greater than one-sixth for example
that's under some circumstances this is
optimal so here I don't expect anything
like this roughly speaking because of
that very very slow contraction log n
right so one when you have exponential
contraction like for the discrete
channels you can oh I mean a one way to
overcome it is too I mean have a lot of
paths right if you have three with ya
like you all has this fantastic paper
and information flow on trees right so
when the degree of the tree is large
enough you basically can overcome the
contraction of TV but here there is
nothing to overcome right because it
doesn't contract so that's why roughly
for all snrs I expect you can produce
but I didn't try to compute to produce
and at your ability bound alright so
that's it that's everything I wanted to
say and then it's four-thirty so I'm
yeah I'm on time yeah but take away
message is that yeah you don't have
linear process linear contraction maybe
look at something like this sometimes it
works yeah so in your mainly so can you
do something if the noise is bad movie
noises Bernoulli yeah Bernoulli noise
it's very easy because then there is
construction so it's just that the
contraction coefficient is 1 minus 2
delta squared yeah so you just every
time you have some basically mutual
information contracts exponentially fast
decays exponentially faster 0 yeah
so did you tell us the functions that
achieve the battle in the matter yes
it's City strange yeah I did but it's
exactly the same thing that so pair of
distributions is this 1 minus T Delta 0
plus T both the minus square p / t &amp;amp; Q's
1 minus T belt 0 I mean there is nothing
funny function oh the processors you
mean yet yeah I forgot what it is it's
some kind of quantizers yeah you mean
you mean like a special yeah it's some
kind of threshold that is that you most
of the time you output 0 began but you
save your energy budget to sometimes
flash a huge input that's roughly the
idea icons i forgot yeah you don't just
rush hold in all know you you also have
to have a dead zone where you mapped
everything 20 yeah that's what I mean
yeah and outside is just a binary
threshold yes yes inside no yeah
something like this yeah I mean we tried
the load so I just forgot it was it one
of the first things the other comments
not thank you again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>