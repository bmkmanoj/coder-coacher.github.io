<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Can you convince me why your software works? | Coder Coacher - Coaching Coders</title><meta content="Can you convince me why your software works? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Can you convince me why your software works?</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1TeKbeedV9M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I'm very happy that we have on Bashir
to give us a talk this morning and to
run through the gauntlet of the whole
Pro Stock interview process so he's
about to finish up his PhD at CMU
working with it Clark he's been doing
some stuff all over the well not all
over the place but in model checking
mostly compositional I in bioinformatics
and I mean a bit longer ago and and also
some stuff with probabilistic systems
and statistics so I'm really curious to
see what happens now because this
question is entertaining take it away
Amish thank you very much for the
introduction and thank you all for
coming and thank you very much for
inviting me over here it's true that
I've worked on several things but I will
be focusing on just a couple of things
on the slides today so I want to talk
about the question which is on the slide
can you convince me why your software
works first of all why bother for one
thing software development is not about
me and my software anymore it's v and
our software it's becoming a team effort
and it's always been but it's becoming
more and more nowadays and we all know
how hard that can be especially when the
documentation is available is only
informal which is most of the times not
even available and it's great when
things work out but when they don't it's
the same cycle all over again and this
is not that for an exaggeration from
reality on the other hand software can
be very critical with pieces of our
lives parts of our lives depending on it
and the sad part is unwanted things only
happen when we don't expect them that
makes it very hard to detect and fix
bugs and here is a bug discovered in
April which which is part of a widely
used open source network software
forbes magazine says it's the worst one
er ability since the internet so what
was the fix according to wikipedia the
fix is to ignore requests messages that
ask for more data than their payroll
needs in simple terms it's a buffer
overflow error apparently the part of
the code that contained the bug wasn't
reviewed enough but what this says is
even seemingly simple bugs are hard to
catch and when social processes a week
the mechanized skeptic is the more
valuable as Donald McKenzie says in his
book about mechanizing proof in other
words one way to attract this problem is
to use the machine itself to try to find
bugs or prove their absence and that's
been the focus of my research in the
last few years and in particular I have
been working on automatic verification
techniques so today I will first talk
about analyzing program correctness in
an automatic fashion and in particular
how we can use the state-of-the-art
solvers we have today for satisfiability
both for propositional logic as well as
for first-order logical theories also
known as SAT modulo theories or smt I
will then discuss some exciting future
directions for program verification and
finally engineering a piece of software
isn't just about getting it right and I
will describe some of my interests
beyond program correctness as well
mechanized verification has a very rich
history beginning with manual deductive
verification and automatic techniques
and techniques for trusting the results
of verification and beginning with the
90s we have seen a rise in automatic
techniques scaling them beyond just a
simple type programs and systems and in
particular in the last decade we have
seen significant rise and breakthroughs
in solvers for satisfiability both of
propositional and first-order logic and
today I will be talking about some
techniques for making efficient use of
this one
solvers but first of all what is the
intended output of verification I am
sure you all know but let me just
clarify what is that I'm trying to do
here proof of correctness this goes back
all the way to cheering and 191 and has
later been formalized by Floyd and whore
and here is a very simple flowchart
program from Turing's paper in 49 and
here is a specification which is a
relationship between variables you want
to check holds whenever the program
terminates approve consists of a bunch
of assertions at various program
locations such that they can be locally
checked to hold for example if you
assume this assertion before this box of
statements then we can show that these
assertions for after the statements and
the tricky part here is to come up with
what is known as a loop invariant which
is something that holds at the head of
the loop no matter how many iterations
you take through the loop and here's a
very simple concrete example here is a
program and here is a assertion which
says X is non-negative and that holds
for all iterations no matter how many
creations you take and the important
thing is we only need to come up with a
sufficient over approximation that helps
us prove the assertion at hand and when
we have procedures we also need to say
how the inputs are related to the
outputs using which we can again locally
check that the assertion holds and again
the idea is to come up with a sufficient
over approximation that helps us prove
the assertion so with that in mind here
is the concrete problem i will be
talking about today suppose you have a
sequential program p and some safety
assertions the goal is to automatically
verify for assertion failures either
output a proof of safety or a counter
example but because of undecidability we
also have a third possibility where we
say the run out of resources but here
are some useful facts now what do I mean
by useful facts becomes clear as we move
along
now one way to address this problem is
to I mean you'd like you could have a
lot to get anything then we say to your
security or and it's big space right
look I'll see you do so i'll be i mean
the algorithms i'm talking about our
general but the implementations we have
done or specifically for arithmetic
properties and we are currently working
on memory as well so i'll be talking so
i won't be focusing on memory in this
talk but these are generated technician
space the wrong properties be my left
right these are generic techniques
modulo the underlying theory right so
one way to address this problem is to
incrementally consider reachable states
for example start with the initial
states check that they are safe and look
at states reachable in one step two
steps and so on but even though we bound
the number of steps the reachable States
can be infinite right or practically
infinite if you look at if you consider
the machine limitations into
consideration but fortunately we have
efficient solvers today for
satisfiability and we can use them by
using a technique called bounded model
checking which i'll be described in
shortly but how are we to find a proof
of safety if you are always within the
reach of the states right the goal is to
find a safety proof that separates the
reachable states from the bad states and
that brings to the idea of
generalizations where after we have
proven safety for a particular value of
the bound we generalize what we have
seen right and this can be done using
techniques based on interpolation so if
you generalize after checking safety for
every value of the bound the hope is we
converge to a safety proof right that
includes all the reachable states and
separates it from the bad states if you
have reached we can check that we have
reached assuming you have an oracle for
smt again
so here is the high level picture of
what we've seen for smt based
verification each iteration you fix a
bound on the reachable States you check
safety using boundary model checking and
you either get a counter example or a
bounded proof which is a proof that
proves safety of the particular subset
of reach of the states you've seen so
far and then you check if you have
converged is the proof really dependent
on the bound or independent of the bound
if it is not independent then you
continue the next iteration consider the
next subset of reachable stage and so on
it comes out in this proof so bounder
model checking does here I'm assuming
that it does safety and also generalizes
it's inside this box okay yes this is
great but existing techniques for
bounded model checking are inefficient
in the sense they create large formulas
that can grow exponentially for programs
and I'll be describing shortly why and
because of undecidability of the of the
problem this loop doesn't always
converge so how can we deal with
divergence in practice so we try to
address these problems using composition
reasoning which at a high level says
break down the question of the entire
program into questions about individual
pieces that can be analyzed locally in
particular I will describe two
techniques the first one is a
compositional approach for bounded model
checking and I will be focusing on
procedural programs possibly recursive
ones and the second technique brings in
ideas from abstraction refinement into
the context of smt based model checking
and I will be focusing on loopy programs
we have a single procedure but we can
have all sorts of loops so let me begin
with the first technique to understand
the importance of compositionality let's
first look at how traditional bonded
model checking works so here is a simple
example I've shown you before
now the idea of bounded model checking
is to bound the number of iterations of
the loop ste b number of iterations and
create a formula that symbolically
represents unrolling the program on the
left 4b iterations so here X 0 is the
value denotes the value of x initially
and each X I denotes the value of x
after I iterations and after be
iterations we exit the loop we flip the
sign according to the this statement and
then we negate the assertion for the
final value of x and the idea here is
this formula is satisfiable if and only
if there exists a counter example of B
iterations now for procedural programs
the bound is on the recursion depth or
more generally stacked up if you have
multiple procedures and here is an
example program where you have a main
procedure and a bunch of level
procedures indexed by a number I main
makes two calls to level one each level
I makes two calls to level I plus 1 and
let's say level n doesn't make any calls
to other procedures and here is how an
unrolling might look like for this
program for n number of level procedures
so main makes two calls to level one and
each level I mix too costly for I plus 1
so we get this pre structure so here i
am using b10 and be 11 to denote the
input value in the output value of the
parameter B now the thing to note here
is the size of the unrolling grows
exponentially in the number of
procedures n which happens to be the
size of the program the execution time
is a satisfiability of this formula
which can be exponential model checking
but if I run the program it would take
exponential time as well no no oh man
missing something moving tips every time
so you only do one of the food
cause haha does it not nothing I called
level I plus 1 and B didn't clear
anything looks exponential to me this
the execution time exponential is what
I'm an exponential so that in person
arguably you might not be so bad if it
takes exponential time to verify an
exponential run track program right
right it is surprising because we know
of polynomial time algorithms right
especially when the program variables
are boolean and now these algorithms are
not sad based for example i'm referring
to be Bob for example here and they are
not readily extendable tuned on boolean
programs so what can we do so this
brings to our key ideas where we say
that instead of creating this large
formulas for the entire program break
down the question and analyze procedures
individually so we create what are known
as reachability queries for individual
procedures so here are each ability
query denotes a formula describing a set
of states that we don't want to reach
and to do that efficiently we need to
maintain approximations of procedure
behavior and in particular we maintain
two kinds of them under approximation
and over approximation and I will
describe shortly what i mean by these
but these two ideas are enough to
improve the complexity to polynomial
time when you have boolean programs and
in general for arbitrary first order
theory if you have an assembly Oracle we
show that this approach terminates for a
given value of the bound despite the
non-trivial compositionality of the
algorithm and the over approximations
here are essentially the generalizations
I was showing in the Venn diagram now
these ideas utilize or create formulas
that involve auxiliary variables and in
order to be efficient we need to avoid
depending on these variables and I will
be describing
model-based projection for practically
dealing with eliminating these auxiliary
but first of all let me show you the
high level picture of our approach so
suppose you have a bounded reachability
query for a procedure p in other words
you have a formula describing a set of
bad states you don't want to reach and
you're asking can I reach those bad
states and you also have a bound on in
this case the stack depth we first try
to locally answer this query by
utilizing known approximations under and
over of the procedures we are calling
and if they are not sufficient we need
to update the approximations and we do
that by creating compositionally new
queries for the procedures we are
calling and update their approximations
and iterate this process so let me
describe each of these steps in the next
few slides first of all what do I mean
by these approximations so suppose I
have a procedure foo with an input X
output Y and the procedure has two paths
right based on a condition whether X is
positive or not I say that this formula
is an under approximation of foo because
every sat assignment of this formula
corresponds to an actual execution in
this case this corresponds to the then
branch so it says if X is positive I
increment it twice and assigned to Y so
that means Y is X plus 2 so that's what
this is on the other hand this formula
is an over approximation because every
execution of the procedure satisfies the
formula right in this case the else
branch can be summarized by simply
negating the if condition and that is
the first is junk and then branch says
if X is positive Y has to be positive as
well so that's the second district now
how do we find these approximations
automatically so I will be describing
using an example so suppose you have
just struggling with hunger and Fox
Nation over confirmation come on cutie
backstitch right so X begins here that's
so the accident put on yden happen so
it's kind of like if the pre can do if
it was in fact if I think of actuator 0
it's a precondition then y equals x plus
2 is a post condition right right so ah
the second one doesn't look quite the
same as it has an O in it right so here
these things are thinking terms of
preconditions and postconditions so
these so our approximations can be
expressed as a pre and post so you can
saying so these these are essentially
relations we are expressing these
relations so if you want to say if you
want express it as pre and post
conditions the way to do it is we are
treating this as so what we do here is
we have so if you can i can write so
what I'm saying is we are essentially
looking at describing these
approximations as relations between
inputs and outputs so as a precondition
and post condition it becomes
essentially you introduce specific of
auxiliary variables denoting the input
values and the postcondition denotes how
the inputs are modified or how inputs
are a letter to the outputs true for
input and user is enough so truthful
friggin idea all right then you're
saying is more helpful just think this
is a relation and the other
approximation is smaller than the actual
exam what's message exactly ok and we
can express this as pre and post
conditions using auxiliary variables
because more generally for if the
variable is modified we can say here is
the initial value let's say at x0 and
how does the final value layer to the
initial value
so let me explain how to infer this
approximation using this example suppose
you have three procedures and suppose we
know already some approximations which
I've labeled here so let's say for four
we really don't know any approximations
under proximities false over is false or
is true and four bar let's say we know
this under approximation corresponding
to then branch of this if condition
which says x if x is greater than equal
to 5 and y is x plus 1 right I know that
and I don't know anything about the
other branch let's say and the over
approximation says no matter what brand
should take the absolute difference is
at most 1 and similarly we know some
approximation support baths as well so
suppose we know this already and now
let's say we have this reachability
query for foo which asks is there a path
info such that the output Z is greater
than the input X now we can answer this
readily using approximations and say oh
I know these approximations about bar in
particular I know that there is a
feasible execution in bar that satisfies
this under approximation I however here
I can just instantiate this in terms of
the actual parameters here and similarly
I can instantiate this with y and z
right so I know that there are
executions in bar that are feasible
satisfying these conditions so in
particular I can use this to say or if X
is 5 I can go to six and then I can go
to seven and seven is greater than five
satisfying my query so note that I
didn't have to look inside bar I just
had to use the approximations I already
know so now i can update the under
approximation of foo and say oh I've got
a new point five and seven right but
this is very specific and this is not
going to scale if you just look at
concrete values of the variables but
fortunately we can do better than this
maintained is a sect or something as a
relation other for a 200 approximation
so it's just one formula just always one
for me keep conjoining things to it that
could also blow operate inside explain
 it's going to improve it right now
saying hey it's going to improve right
now this way of adding funds it couldn't
explore it initiating precise can grow
but whether it's exponential or not I
mean if for example if I just compute
the symbolic and strange one for every
but then the end of approximation will
be exponential rates if you look at the
number of parts but if I'm looking at
right right i mean exponentially in war
exponential in the number of variables
or declaration in the so if you look at
this for boolean programs for example
its linear in the state space the state
space is exponential in the number of
variables this formula will explore I
mean unless use a simplification or
something great Jace Navy just right but
if I simply look at simply add points
that are reachable then I can i can only
be as bad as the number of states number
of actual values of the input output
line so there are be variables be
parameters then I can only be I can only
have 2 to the B points11 is little form
exponentially say it for me right but
this is polynomial in the number of
states right yep yeah the formulas ice
cream explanation what I'm saying it's
only as big as the number of states I
mean yeah but that's still better than
what we have today but but in general
what I'm saying is that what happens if
you maintain them explicitly but we
maintain them symbolically so what I'm
saying is in particular you don't have
to look at the specific concrete points
but you can look at the path condition
and in particular if you look at the
path that goes to the then branch then
we can simply look at the under proxima
shins we have
for bar because if click on join them
and say any pair of values of x ends
either satisfy this is a feasible it's
feasible inside bar inside foo and in
particular this includes five and seven
and possibly many other points as well
right but now we have added some
auxiliary variable Y which is not it's
neither an input nor an output so we
need to quantify it out and typical
approaches which use somebody's your
refuge here at least what you described
over here doesn't seem to use the
calling context rate to so the volume
harness comes in where comes into play
when we look at how queries are created
so let me show you using another example
so right so how we deal with this
quantifiers I'll be describing shortly
but suppose we have a different way now
which size Z is less than X minus 5 so
now under approximations are not enough
to show reach ability but we can use
over approximations to show that it is
not possible at all in no matter what
you do so here I'm instantiating so
scary which may seek where we go
previously we were checking soleus you
were checking z is greater than X right
we're always checking reach ability okay
so now we are significant about this
query it's a different query it's a
different query and I'm showing that the
under approximations are not sufficient
we cannot what's the difference why
was it okay before but no matter what
was our approximation not before but no
for this new query I mean we can check
whether we can show each ability using
this apart I'm saying good you say do
the algebra doesn't work it doesn't work
yeah right okay was it did before okay I
think it's just giving young
approximations you have you could ensure
yeah
this is this example but also more any
reach really query that isn't actually
true yeah that is a good answer so we're
using under proclamations to prove that
the Richmond seeker is definitely true
and over approximations prove that it's
definitely false right okay now you're
cooking with gas okay right but all we
can say using under proxim agency is
here we don't know right but using over
approximations we can say this cannot be
reached so in this case here are the
over approximations we already know
which says the absolute difference is at
most one we are just instantiate it and
here we instantiate in this or
approximation and we say that no matter
what brand should take if you take the
then branch z is at least X minus two so
it cannot be less than X minus 5 if we
take the else branch these at least X it
cannot be greater than X minus 5 so
using over approximations we can show
that this cannot be reached and now we
can update our over approximation by
simply say negating to reach ability
query but we can do better than this by
using interpolation techniques and I
won't be talking about this in detail
but what if you have another query where
existing approximations are not
sufficient and again I'm not going to
show why that we can do the math but
let's say the approximations are not
sufficient but we do have reach ability
using over approximations in this case
so we know that no matter what bass does
Z is greater than or equal to X and if
this is all we know then we can go from
one to two and two is one plus one but
the question is is it really feasible in
bass so we create a new reachability
query for bears looking at what's
happening along this execution assuming
this or approximation and ask bars can
you really have an execution where your
output is input plus 1 right so this
this is where the calling contest comes
in we look at the apps abstract
counterexample to see what is that we
need to know in order to show
feasibility of the counter example and
we can analyze bars in a similar way
using approximations are the proof that
is definite sure definitely false then
we need to have innately apologies about
whether we try to improve the under
approximations about coolies or the
overclock summations anomalies how does
how do you make that choice so we make
queries so we know so we know that days
there is at least one procedure call
which whose approximation needs to be
updated and we essentially ask what is
that it needs to be consistent with so
we look at the calling context all the
other procedures or other constraints
along the path and say are you
consistent with these other things such
that i can reach this query so i just
ship off a query and when the query gets
answered it gets answered either
positively or negatively and then we
know what's happening so it's all done I
don't have to ask you questions yes so
I'm asking are you reachable can you
reach this query and it comes back
either yes with an under approximation
or no with an overall Crocs mission
again we're not in this a lot in the
stock yeah but I can explain it offline
the composition may must analysis for
example in yogi right so there so I
don't think that is complete in in the
sense I don't think it has guarantees of
termination because the under
approximations there are based on
testing and then it's a must yes it's
it's similar but they don't deal with
quantifiers the way we are doing in a
complete way
right but you you pick a test case and
you run and you look at specific values
right or you do exact wonderful
domination which can be expensive and
the summaries are different in the sense
you also have ghost variables in that
procedure okay we only have parameters
or homeless or parameters defined
parameters you right right but but if a
parameter is getting modified for
example you don't have a relation
between input and output you say if this
is the precondition this is the post
condition right and that can blow up for
example if right if if I available as
not and we can discuss it offline so so
here is the picture try to describe each
of the steps in the last few slides
we're suppose you have reachability
query we first try to use existing under
approximation the procedures for calling
and see if you can answer reach ability
positively or use over approximations if
you can answer negatively and if neither
happens then we need to update the
approximations by compositionally
creating queries now can let me come to
the question of how to deal with this
auxiliary variables so suppose you have
this quantifier formula very your
interest rate in acts but you also have
some auxiliary cell which you need to
eliminate so how can you do this well if
we have algorithms for corner for
elimination which computer equivalent
formulas which don't have the auxiliary
then we can directly use them and for
example here is an example which says
there is a why between X and Z is the
same as X is less than C right but
unfortunately these algorithms have very
high complexity especially if you have
integers and moreover if we always
eliminate auxiliary sin an equivalent
way
amounts to exact reachable very analysis
and that's not going to scale in
practice so how can we eliminate
quantifies approximately and let me
consider the specific case of obtaining
under approximations so suppose you have
a path condition using some under
approximations over the pre and post
parameters and suppose you have
auxiliary cell that is consistent with a
reach ability query you suppose you have
the situation now the goal is to find an
under approximation of the quantified
formula which doesn't depend on
auxiliary which is also consistent with
the reachability query right now we can
simply pick a specific point like if X
is 5 then input is 5 output is 7 but can
we do better than this right so that
brings you the idea of model-based
projection where the high-level idea is
is following suppose you have this
circle which represents the space of all
values of the variables X that makes
this quantified formula true now instead
of generating one formula that covers
the entire space the idea is to come up
with a formula that covers a subset of
the space and if we can have a finite
decomposition then we are not losing on
completeness either we will eventually
generate all the subsets and the idea is
to generate the subsets using a model so
suppose you have a model which is an
assignment to all the variables X and L
that makes feed true and suppose here is
where the projection onto the X variable
slice the idea is to pick that subset
that covers this particular model number
n is when you say something about the
projection right so you're looking at
the X variables right so Emma signs
values to all X and L yes so let us look
at what the assignment to the axis ok
let's say this is where it lies
this is where the point lies in the
space space is all assignments 2x yes
and this is where that particular point
is so you get the rest of freedom from
so so let us say m assigns extra five
and l2 70 in the purple space all the
assignments to X are the same no no so
I'm saying that suppose this is a space
of all values of x that makes this
quantified formula true and here i know
that say x is 5 and l is seven makes fee
true so that is what Emma signs and let
us say X equal to five like somewhere
here let's forget let's forget about L
ok that's the black dot and and the
other dots in this subset can be can be
other value say maybe one is also here
three is also here what I am saying that
but I'm saying that find some subset
which is more than the point which
comfort which consists of other points
as well that includes this point is that
any subset word I'm saying that if we
can do this using a finite decomposition
if you can only manage to get finite
subsets then we are not losing on
completeness either you will eventually
cover the entire space and how we do
that i'm going to describe that shortly
in fact i'll just do that using an
example so suppose you have this
quantified formula in let us say linear
rational arithmetic just to keep things
simple so all the variables of rationals
for reals and we only have linear
arithmetic here and we want to eliminate
this auxiliary L now we can show this
equivalent formula which does not have l
and this can be obtained in particular
using the quantifier elimination
algorithm by los weyes pfennig so let me
just illustrate what each of these dis
jumps is very very high level so the
first extent corresponds to the case
where the equality is true like L is
equal to e then we can simply substitute
efore L in the form
and the second disjunct corresponds to
the case where the lower bound is true
and third disjunct when neither is true
I'm not explaining how these are
obtained but there is an algorithm to
obtain these dis jumps right so for
reals it is exponential it is not
doubling but for integers is double
exponential okay now the idea is instead
of generating this entire disjunction
the idea is to use a model to pick one
of these disjunction right so if the
model satisfies equality pick the first
one if the model satisfies the lower
bound pick the second one and so on so
that is the idea I am NOT describing the
entire algorithm here but the idea is to
look at the model and pick the disjoint
with the model corresponds to a
satisfying son a mogul you find a more
and then you either look far to start
with and then you identify which
disjunct covers that particular model
and pick that discharge Miss Jones sorry
so you found all the distance we don't
find all the districts is there is an
algorithm to directly find the district
right we don't need to generate all of
them at the same time and in future if
you get a different model then you get a
different district and we make sure that
we don't find the same disjunct twice so
we don't have to dunnedin sees so this
is all I want to say about model-based
projection and the complexity degrees
due to this area and design technologies
I don't explain the complex change yet
riously uh-huh help reduce the
complexity this is the complex this is
just a practical heuristic we explore
the complexity you might through minute
before actually generating right but in
the first case I might generate all of
the decisions so this is not for
reducing complexity this is just for
practically dealing with more effects on
fire okay so coming back to this picture
we can now substitute the composite
oh how this works and yes yes how's it
compare with xiii for exam yes i'll show
you shortly right so now we can
substitute the compositional approach in
place of traditional bonner model
checking and we can show that this
terminates as long as you have an smt
oracle and this whole loop terminates
for boolean programs and this is
polynomial time and a high-level idea is
there are only as many approximations as
there are reachable states and each
iteration refines at least one of the
approximation so that's the high-level
argument which was this even pavilion
programs yes I'm saying polynomial in
the reachable states it says infeasible
in practice well depends on what him
when feasible that's larger than the
number of protons in the universe is not
going to be helpful to me is it but
they're still better than existing
approaches I'm saying okay boolean
program is potentially very small
compared to the number of states in an
actual program that's how many videos
yeah yeah yeah but we don't say it
better hopefully you're going to
justified with some empirical results
rate comparison bball right here Oh next
slide actually so here is the vm
implemented these algorithms in our tone
call spacer so we have a sea program and
there's a front-end based on compiler
framework llvm we generate that into we
compile that into intermediate form a
simply lib and yes people here know the
horn class format so this is the horn
clause format and then we feed that into
spacer which implements these algorithms
and here is a comparison with xiii on
the boolean program i showed in the very
beginning which has exponential
unwinding sorry please PDR yeah
and we see that as we increase the
number of procedures spacer can handle
the increasing complexity more
gracefully and we have also done
controlled experiments to see the
individual advantage of the techniques
specifically model this projection here
where we see significant improvement as
well as addition of under approximations
so these I believe or the yeah these are
the control flow integers device drivers
and currently there is no in the five
repetition how do you compare well the
point of these these is to show the
individual advantage of these techniques
I'm not saying spacer is the best but
I'm saying these techniques really add
value and here is very pretty different
totally into PR so you're not comparing
just one little technique no actually
the key difference is actually the key
difference between spacer and p dr oz
III PDR all these two techniques MVP and
honor approximations if you strip them
off it looks very similar to PDR and we
can discuss our offline so what we have
seen so far is a compositional approach
for a simply based model checking we're
approximating both the feasible and the
infeasible is is important and we have
also seen model-based projection for
obtaining cheap approximations and the
idea is full quantifier elimination may
not be necessary in practice and it's
interesting to look at other theories
like nonlinear automatic or
uninterpreted functions for example with
other program verification tools that
participated in the stl right we haven't
done others comparisons yet
okay so let me quickly explain the
second technique which tries to bring in
ideas from abstraction refinement into
the context of smt based verification so
let me check do I have like 10 minutes
or 10 minutes okay all right so so let's
consider this program which has this
assertion and we need to check this
assertion and we try this program on
xiii the PDR engine and cannot verify it
in an hour and here is a loop invariant
which is useful to show the assertion
right so so let's see so if you look at
what z3 is trying to do in terms of the
bounded proofs it never really connects
the variable Z and W the way we wanted
here right to show the assertion in fact
it produces things like after one
iteration w is 1 z is 10 after two
iterations w is 2 z's 20 and so on so
even though there is an implicit
connection it doesn't really find it so
on the other hand here is a very simple
abstraction of the program it says let's
say T gets an arbitrary value right the
same tool can verify the program in less
than a second so it's it's apparently
finding connections because now t is not
a concrete value so it says W is TZ is
20 and apparently it's doing some
resolution or tea and so being symbolic
helps seems to help and and as we all
know abstractions only add behavior so
it's conservative and the question is
how can we automatically find
instructions and in the interest of time
I will skip some of the details and
let me explain it a very high level
what we are doing here so the very high
level the picture is the 4 line so smt
based verification explores under
approximations like different subsets of
reachable states what we are doing with
or the idea we have here is to come up
with abstractions and add a new
dimension so let's say we prove initial
state safe then instead of looking at
stays reachable in one step we look at
an abstraction of the reachable states
of up to one on one step and iteratively
refine it until we come up with a
suitable abstraction using which we can
show bounded safety for that value of
the bound and continue this process
until we get this so the idea is to use
the proofs to obtain abstractions so
that you can use the abstractions for
checking safety of more reachable states
so instead of looking at the original
program you want to look at the
abstractions going symbolic UT is that
instruction that's an example but we
have a different kind of abstraction as
well I do not have the time to explain
that the cr-v so Seager only looks at
abstractions and try to refine it and
the idea there is or the main assumption
there is you can get an abstraction
which you can decide those correctness
you can decide but for example it is
predicate abstraction to get financial
abstractions and you can use any BDD
based model checker or any other model
checker which is known to terminate and
give a value but here we are not
necessarily confined to financial
obstructions so you cannot get an
abstraction and ask another model
checker because that's not guaranteed to
terminate so we have this finder
integration between abstraction and smt
based verification and I can describe
that offline
yeah so let me just say that okay so
showing the results probably doesn't
really help without explaining the
technique so in the short time I have
let me just describe some of the lessons
we have learned I've not here the first
is composition allottees key to
scalability both in theory and practice
and it improves the complexity as well
as we see the advantage with realistic
examples and heuristics continue to be
important to get practical
implementations and we have seen that
with eliminating quantifiers and they're
also questions like when is the right
time to define an abstraction and you
know that we have seen a hierarchy of
automatic decision procedures so you
know smt has been influenced greatly by
heuristics from the sad community and
today I've described one technique one
and a half technique for inferring
invariants using SMT solvers and these
come under the category of corner for
free invariants and it's interesting to
look at how we can extend these two more
expressive invariants and in particular
how can we find corner fighting variants
and moving on there are many interesting
directions for future exploration and
here are some of the ones that interest
me the most excite me the most for
example for for one thing today's fire
fires and software model checkers they
are primarily single core
implementations and I think it's time to
move distributor for example we can
exploit compositionality as I've been
describing we can analyze different
pieces on different machines we can
exploit non determinism in algorithms
and today's algorithms have many
different features and they make
specific choices in the implementation
but we don't have to add a high-level
proof finding and bug finding any
different techniques and this also helps
with better memory management to scale
to
larger and larger programs another
direction is to augment verification
with synthesis techniques synthesis is
the flipside of verification which given
a specification tries to synthesize the
code fragment that is guaranteed to
satisfy specification and we can use it
for example to synthesize terms are
useful in invariance and this is
especially important for more expressive
invariance by quantified invariance but
you need to find appropriate terms to
make problem more feasible synthesizing
ghost code which is a piece of code
whose main purpose is to assist
verification and not interfere with the
execution of the original program and we
see that done often in manual deductive
verification but we don't see that in
automatic techniques and combining with
domain-specific synthesis techniques and
we are seeing rise of those techniques
for example synthesizing regular
expressions from examples or data
structures from relation representations
and so on so how can we leverage the
advantages of both synthesis and
verification in in a seemingly
integrated fashion and finally when
Xactware occasion is hard to scale how
can we deal with that using approximate
techniques and abdom some work on
probabilistic correctness guarantees on
finite state systems but it's more
interesting to look at realistic
infinite state programs might really
have integer variables and so on and I
think we need new deductive systems and
logics and this is something
significantly under developed in my
opening tell you you given example what
kind of thing you have in mind like for
example an approximate variant of whole
logic right so what is invariant which
corresponds to a probabilistic guarantee
and how can we fit things in a
compositional manner I have invariants
probabilistic invariance for one piece
another piece what does it mean to
compose them
how what's a good proof something that I
say it's probably true the dance is
bigger than buying something is some
assertion is true with probability
ninety percent right and some one
solution is for one component and have
different component with such a
guarantee and how can i combine them
into a problem is to guarantee about the
composed system for example so what's a
good logic for that sentence with the
pump you starting with which is that we
really want to know there's some
programs we care that they're absolutely
correct you started off with heart bleed
right I'm saying that when it's hard to
scale exactly fication is hard to scale
can we at least get probabilistic
guarantees and person suppose it was
high enough like you know there's only
one shots into the fifty millions of all
that that I'd be with you yeah I mean
there are some very safety critical
applications for like security and
hardly might be one of them and they're
perhaps you don't want to take the risk
of probabilistic guarantees but examples
of the probabilistic guarantee my love
life when I should exist you sites and
papers here I don't have an example on
the slides but but I mean existing
systems where you feel them n useful
purpose so the examples I have concerned
considering these newspapers or when the
program's themselves are all ballistic
so you have randomized protocols oh good
story yes right but but i'm here i'm
generally talking about the case where
you have like not necessarily
randomization in your program but you
still won't have progress to guarantees
I don't have examples that oh is this
something that you just think it's
hybrid systems community and so on go to
work in Syria yeah right I guess it's
okay right so let me just say that yeah
let me just take one more minute and so
moving on beyond correctness here is the
high-level problem I'm interested in how
can we raise the level of abstraction of
programming to make it more intuitive
and more easy to understand and I think
there are several interesting exciting
challenges here for example the moment
will raise the abstraction it it becomes
less formal and we need to deal with a
lot of non-determinism these things and
I think I'm imagining some interactive
system with interactive programming
environment where you are dealing with
this high-level intuitive environment
and you have a back end that is trying
to synthesize a a formal precise program
which can be run and there are
interesting challenges like what is the
right HCI human-computer interaction for
programmers themselves and for this i
have this approach in mind which is what
i call compositional development of
programs where you're dealing with this
high-low interfaces and today we have
this lots of repositories of code up
there some of them can be even produced
by synthesis techniques so how can we
leverage all of this and this is what i
call crowdsource programming and we are
getting there this is not too far in the
future and so with that I want to
conclude my presentation where we have
seen some
efficient techniques for smt
verification and I see an exciting path
ahead towards being able to convince me
why your programs work thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>