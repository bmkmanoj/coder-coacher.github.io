<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Session one, Artificial Intelligence and Machine Learning in Cambridge 2016 | Coder Coacher - Coaching Coders</title><meta content="Session one, Artificial Intelligence and Machine Learning in Cambridge 2016 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Session one, Artificial Intelligence and Machine Learning in Cambridge 2016</b></h2><h5 class="post__date">2016-07-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mNWmFaNQfH8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay let's get started hello good
morning everyone let's get started so
thank you all for coming
we actually had 125 registrations we had
to take off the registration form but I
don't see on our 25 because I'm not 25
has a capacity of this theater so there
is some some nos here maybe people come
up they come here a bit later so and
thank you for coming this is a cert
iteration of this workshop so sirki this
happens first of all I want to
acknowledge the help of the other
organizers which is Matt Hoffman who
cannot be here today and zubeen and we
put together a really exciting program
in particular also the other help I want
to acknowledge of course is you have met
the - Jessica Watkins an M&amp;amp;E food it
splendid job at organizing everything
organizing the catering and and all the
coffee breaks and so on and the
registration and printing the badges and
so on so I really appreciate that help
and also thanks to the head of the
machine intelligence and perception
group here at Microsoft Research Jaime
chaton for providing the funding for all
the catering and speaking of the
catering I am aware that last year we
had delicious food but it was a bit too
few and so this year we over-provisioned
a bit and we also changed that for tasty
curry instead of the Italian small
snacks so I hope you will appreciate
that okay so in general we also renamed
the workshop - bit to piggyback on the
hype of AI in a way so we said instead
of machine learning in K which is now
artificial intelligence and machine
learning in Cambridge and indeed I I
can't do the counterfactual reasoning
but the number of registration reflected
that increased attention so that's good
and also another thing this event have
always been 5050 talk slots between the
University of Cambridge and Microsoft
Research and I think we should use the
opportunity of the workshop in
particular the coffee breaks the plenty
coffee breaks to mingle a bit and to
increase maybe the intersection and
discussion of how maybe we can
collaborate or find exciting topics to
collaborate on between minds of research
and the university that would be really
good
we have been resident here for a long
time in Cambridge and we have had some
collaborations but I think and there
could be much more exciting
collaborations coming up so finally one
change which we started to introduce
last year already is to include startups
in in one of the sessions and that is
really because the our field is driven
largely by progress and technology as
well and to acknowledge that we may need
to look to the startup community to find
really exciting applications of machine
learning and therefore we actually
invited three startups and unfortunately
a few weeks ago I received an email that
one of the Sun up stopped out because
they made a few trip to Nepal or
something so they couldn't be here today
so we fill that slot with another
speaker but in essence I'm really
excited to have two talks here by
startups so please stay until these
talks happen so we have four sessions in
general there's coffee breaks half an
hour each I may ask all the speakers to
stick to that time so I will I will make
sure that actually it's enforced so the
time is general the speaker's been
instructed to be 15-minute talk last
five-minute question but there is a hard
time limit of 20 minutes so how you
divide that talk time up is basically up
to you all right so then let's get
started the first session is speakers
from the University of Cambridge and the
first speaker is Karl Rasmussen so I'm
really excited to hear so I want to
speak about inference in nonlinear time
series models so modeling nonlinear time
series or model unit nonlinear dynamics
is actually quite a quite a difficult
task and a lot of the methods that are
out there are very focused on linear
dynamics and so I'm I'm very keen on
trying to push some of our nonlinear
modeling technology to to these to this
particular problem
so this
sort of a an idea that's that sort of
come together over quite a while so I've
listed a number of people here who have
been involved in this there might have
been all those two and we'll hear later
on the afternoon that'll actually be
other talks that'll that will use some
of the same basic ideas of our
variational inputs all right so the late
memory model that I'm interested in is
is the the usual one right so we have
some observed variables here Y and then
we think that we our model says that
there's some unobserved process that
gives rise to these Y's and there's some
there's some unobserved dynamics here
and we have to figure out you know how
how does that how to do inference over
over these models and of course we
probably all know that if the system is
linear on Gaussian then we can just
write down what the what the what the
answer is but the problem here is that
basically that linear dynamics are
boring right Linna dynamics can only do
exponential decays of things and maybe
some oscillations right but you can't
really they can't they can't capture
anything anything else than that right
so and that seems to be neat to me to be
a devastating limitation right so so I
really worry about using linear models
for to try and in model you know lots of
things that they've been they've been
used for okay so so the essence of the
talk is basically comes down to trying
to deal with this irritating fact so the
fact is that we have lots of good tools
to do to do nonlinear regression
problems but it's very hard to use them
in a model like this and I'll try and
explain why it's hard so it's hard
because we don't because because we
basically we have to model things that
transition from uncertain states to
uncertain states right and we're not
very good at doing regression where
there's uncertainty in the inputs and
uncertainty in the outputs right so we
have to sort of deal with that with that
problem okay so I'll start off with a
with a with a with a simplification so
you could think generally about doing
non linear transition functions and
nonlinear observation models
but actually it turns out that without
loss of generality you can think about
just doing nonlinear dynamics models you
have to have nonlinearities here because
otherwise you can't capture dynamics
that are nonlinear but you can get away
with having a linear observation model
possibly at the costs of having to
extend the state a little bit right and
you can do that you can see that by
Jacek aliy you can just copy you could
just make a copy of the observations in
in this state basically right and and
and and so it's and so you can you can
if there's a if there was a nonlinear
bit here you can kind of fold that into
this into the state when we haven't said
anything about what the state is
representing right so from now on I just
talked about nonlinear dynamics models
and Lena observation models okay and
some of you might have guessed already
that I'm gonna use Gaussian processes so
a Gaussian process is basically an non
parametric model that is a distribution
over function so what we need to take
from this is basically given a set of
inputs the Joint Distribution of the
function values here take on a Gaussian
distribution and the nice thing about
that is that that we can we can then
compute very easily with that but also
we see that the that the inputs here
which are technically the index set into
the into the stochastic process the
inputs here are given right and that's
what's going to cause the problem that
in our application these things are also
going to be random variables right and
then it's sort of unclear what the what
the Gaussian process will will look like
okay so let's try and write down a model
so this is going to be I'm going to have
a function here which is going to be
given a Gaussian process prior so I have
my function f here and my inputs X and
then I'm going to have then I'm going to
have so this is a this is a
straightforward Gaussian process but
then I'm going to have this this extra
little bit here which says that well the
input at the next time step has to be
the same basically as the output of the
previous time step right well that's
what makes the whole thing
into a time series right and I allow a
little bit of a process noise
essentially to be to be added to that
right so now so this thing here says
well that the that the input at the at
the next time step has to be related to
the output at the previous time state
okay and then I have the same
observation model as usual in this case
I have a Gaussian observation model it
turns out that that doesn't really
matter very much and I've just plugged
in a linear a linear model here okay so
I have my parameters here or high
parameters I have the parameters of the
of the Gaussian process I have maybe the
the process noise here and the
parameters of the of the observation
model okay so now I can write down by my
joint probability of all the things that
I'm interested in here and the joint
probability okay we start over some
distribution over over the X's and then
I have this the my Gaussian process here
on the on the FS and the Gaussian
process basically says well you know you
can do it you can you can draw then the
the next time step F of T it has to be
consistent with what you drew from the
Gaussian process in previous time steps
and it's also conditioned on the inputs
here and then and the and whatever
parameters are of the Gaussian process
and then I have my my my process noise
here that says that the that the input
at the next time step should be should
be related to the to the output at the
previous time step and then the and then
the observation model right so this
thing here the the time index so this is
saying that so f of T is predicted at X
of T minus 1 right so that's so time
moves forward in this step and not in
this step you could arbitrarily we label
that the time indices okay now so what
do we what do we need to do here well we
need to we to find the marginal
likelihood for example if you want to
train the model so the marginal
likelihood is basically just the
probability of the data given the the
parameters say and then integrated over
these over the distribution of the FS
and the X's now but this is this is
really this is really hard to do this
integral why is it hard
well it's hard because this distribution
here if you look at the F's for example
these aren't even these aren't even
Gaussian distributions anymore right
because of this nasty linking of the
inputs and the outputs right so if we
look at what the Gaussian process is the
Gaussian process says that for any
particular choice of inputs here they
the outputs are going to be jointly
Gaussian right but now I'm imposing on
the model I'm imposing this relationship
between the input at one time step and
the output of the previous time step
right so that says in in in this sort of
plot it's saying well if the output at
some time point has some distribution
then the input at the next time step
will have the same distribution right so
if I start moving around that point then
I move around the input to this point
and then that then that'll have
consequences for all the other outputs
right so this is going to be in general
a very complicated distribution and
certainly a non Gaussian distribution
right even having one input that was
uncertain then renders the Joint
Distribution of the outputs non Gaussian
right so this distribution is a tricky
opt object to deal with okay so so what
are we going to do okay we can start by
trying to lower bound in a variational
way we can lower bound the the object
that we're interested in so I'll just
walk you through this so this is
basically saying that the the log margin
likelihood it's bounded by and now
introduce this this this Q distribution
distribution over the things that I that
I don't know the random variables here X
and F and the and so what I've got here
is my my my likelihood and and prior and
I can basically multiply and divide by
the EQ distribution and then take the
log inside the the integral or the
average using yet using Jensen's
inequality and you can see that you can
quite easily see that the well the
likelihood times the prior is equal to
the marginal likelihood times the
posterior so I can substitute that in
instead and by looking at this equation
you can see that the
that the that the gap in the bound here
is basically just the KL divergence
between the posterior distribution and
this in this cutest region okay but the
lower bound is always a lower bound
regardless of what which way we choose q
so the idea here is to say well maybe we
could choose a Q in some restricted
family somehow such that we could
actually compute this bound okay now
what
what would we choose Q to be well if we
chose Q to be of this form basically
right so I'm replicating from the from
the prior term here I'm replicating this
structure that the FS are have this
particular form and then I just multiply
by by some distribution over over the
over the X's here if I chose the
distribution to be of this form then
okay I won't show you on this slide but
it turns out that then you basically get
some very convenient cancellation here
and then you can actually compute the
bound okay and of course it is a bound
because we don't have any restrictions
on q unfortunately it's a terrible bound
right so the gap is really wide now why
is this why is the gap really wide well
it's really wide because what we were
supposed to well to keep the gap tight
we would have to choose the Q
distribution to be very close to the
posterior distribution right but now
we're choosing it to be something that
is unrelated to the data right so that's
not really going to in general be close
to the posterior distribution right or
we would have to be incredibly lucky for
that to happen or our data would have to
be pretty vacuous for that to happen
okay so so okay this is true that this
generates a pound but it's actually not
gonna be a useful bound okay so what are
we going to do about that well here's an
idea we can augment our our Gaussian
process with an extra set of variables
okay so that's what I'm going to do here
so okay there's a lot of symbols in play
here now so I'm now going to augment the
Gaussian process with extra variables so
so what what happens graphically is
basically I'm going to introduce some
new locations along the x-axis here
and some some some new outputs so the
inputs are called said in this case and
some extra outputs which are called you
and then and I'm allowed to do that
right I can add extra variables to the
Gaussian process and if I marginalize
out those variables then I just get back
to where I came from okay so let's try
and do that so now I'm going to
introduce these new new inputs you and
our input said and output you and now I
can write the Joint Distribution that
I'm interested in and now in this case I
used to have P of X and you appear of X
and F here now I just write the
conditional of P and X and F given the U
and then times the marginal distribution
over the use right so this is certainly
this is certainly true I can add those
variables if I want to and but if I do
that then of course I've just I've just
made my problem bigger right now it's a
it's a nasty problem that includes more
variables okay
so I can write down my bound again just
as I did before now I've got the so the
use our random variables and this the
inputs to those corresponding random
variables are some fixed extra
parameters of the of the problem and now
I can just write down my distribution
the same way as I did before and now my
Gaussian process here will be first
conditioned on you and then then I have
to add the the multiplied by the bottom
by the module of the you variables okay
so now I can I can do the same trick as
before and if I want tractability then I
have to choose the mike-mike you
distribution to be of this form again
having this factor here which will
conveniently cancel out this stuff down
here and the nice thing about this is
that now I can actually choose a Q
distribution that does depend on the
data right because because they can
depend on the data through the you
variables right so I can get rid of the
objection that I had before right if I
choose a nice set of of inducing Marable
that can basically capture what is the
data telling me about the function then
this can become this can become a much
tighter bound right so I can write down
a lower bound now which is which is the
bound on the on the on the margin
likelihood and it depends on these three
objects here Q of X Q of U and Q of F
given X and u right and the parameters
so my algorithm is basically going to be
the following so it turns out that for
certain choices of covariance function
you can actually be the the posterior
distribution over the over the the
optional distribution over the FCR is
you can actually integrate that out
right and the covariance function for
which you can do that are basically
covariance functions where you can do
the integral of a Gaussian against the
the the functional form of the coherence
right so there quite a lot of those
covariance functions so I can basically
marginalize out the the F so now I have
a lower bound which doesn't depend on F
anymore it also turns out that the that
you can you can do a free-form
optimization of the cutest the Q hue
distribution so you can basically just
optimize and find the best q of you it
turns out to be gaussian and so you can
just maximize out you so now you have a
lower bound that only depends on Q of X
and the parameters and the final step
you can show that the Q of X
distribution has Markovian structure
after all that's not too surprising like
the original assumption on the time
series was that you had a Markovian
structure so and I'm going to make one
further assumption namely that I'm just
going to consider Gaussian distributions
on X all right so the optimal Q of X
isn't Gaussian but I'm just gonna assume
that I'll choose something in the
Gaussian family so now the parameters of
my my function is basically the mean of
the Q's at each time point the variance
of that thing in the covariance between
pairs of of neighboring points right so
my algorithm is basically looking at
this bound and found it finding the Q of
X distribution of the Gaussian Q of X
this
Markovian queuing that maximizes this
lower balance
okay so let's try and and do that just a
brief comment here that it turns out
that one way of looking at this is that
to see that sparse approximations
actually built into the framework
because when you're making predictions
about the the FS or when you're when
you're doing the inference then it turns
out that the it's only they use that are
basically carrying the information about
the FS and that means that the effective
training set size is only the size of
you right so choosing a small number of
yous will will give you a fast algorithm
right of course approximate there's a
trade-off there but you have a sparse
approximation already built into the
framework okay let's look at an example
so an example here I have a time series
here so I have time out here and I have
so in this case it's just a
one-dimensional example because
otherwise it gets a little bit hard to
display what's going on but there's
nothing in this framework that restricts
it to be one-dimensional and I've drawn
some this is a noisy time series from
some nonlinear dynamics and we should
try to infer what are the dynamics here
so a better way of looking at this data
is plotting X of T plus 1 as a function
of X of T and you can see that there's
actually a structure in the data and it
looks reasonable to assume that this is
a Markovian structure right you can tell
a lot about X of T by looking at looking
at it if a t minus 1 ok it also looks
quite noisy and in particular it's quite
nonlinear so trying to do inference in
this under the assumption that this is a
linear system would probably not be a
good idea okay here I just drew this
with the transitions so here I've shown
you what do they what is the Q
distributions look like so in this case
to display things I I've just basically
said that I've restricted the likelihood
function to just be a noisy copy of the
of the latent state right so I can look
at the at the distribution over the
latent parameters then they play the
same role as they as the observations
and
basically in red here I have the
observations and in in green I have the
posterior distribution of the of the of
the X corresponding to that observation
and here I have the the underlying
dynamics the Gaussian process that tells
us what do we know about the about the
dynamics there and and and these are
then connected so you can see how they
how the inference which finds how much
noise was in there what's the what's the
uncertainty of me in the in the
posterior distribution and it's able to
clean up than the data points quite
nicely so conclusions well basically I
think this is a nice example of a
principle approximate inference method
that works on on general nonlinear
dynamical systems and I think this is
cut this is quite difficult to do in
other ways right so there are lots of
examples in literature or things that
are restricted in one way of another you
can have piecewise linear dynamics or
you can have you know mixtures of mix
just models or but they're all quite
limited and this is quite a general
framework for for doing inference in
those cases alright so I think I'll stop
there to have a few minutes for
questions yeah I think you can so the
only thing the only the only crucial
property that we're using on the
Gaussian process is that you have to be
able to compute the the if you have a
Gaussian input you have to in closed
forum say what's the what's the
distribution of the output yeah yeah
yeah yeah yeah so I think you could you
could use this framework in exactly the
same yeah
raishin incidentally using additional
yes this is something that's been
pursued quite a bit by the sheffield
group for example so they you know
pointed in this direction alright so I
guess the special thing about a time
series models is that you have this it's
extremely layered the model right so it
so it depends like each layer depends on
it on every or layer so there's a lot of
dependencies in there but you could
certainly think about using the same
framework with just a few layered model
without the sort of extreme linear
dependency yeah no it's not
for M equals one but it would be if you
used if you use the a covariance
function corresponding to linear
transition models right so you could so
we didn't say any much about the
covariance function for F but you could
just you just stick in the covariance
function for linear functions yeah okay
so I'm personally interested in using as
a picture but I think recognition is is
certainly also possible right and it's
it's you can see in the example that is
that it's able to quite nicely you know
clean up a data set for example alright
so yeah yeah I'm afraid we have to move
on but let's thank Carl again for a
great talk
okay next up is ink gently
speaking on variational inference using
Rennie divergences okay so my name is
Jen and I'm a PhD student working with
rich so today I'm going to tell you my
recent work on applying version or
inference techniques to any divergence
so I think I have already given us a
great introduction we got International
inference and I'm just going to repeat a
set up up front optimization point of
view so we are interested in doing
Bayesian inference and to do basic
inference we need to compare the
posterior distribution of the model
parameter given the observations however
in most of the cases the posterior
distribution can be very complicated
because we need to compute the
denominator which is the marginal
probability and computing them marginal
probability requires integrating out all
the parameters in the model so this is
very difficult for saying your networks
Gaussian process state space muscle that
cultures talk about that so to solve
this problem we sort out approximate
Bayesian inference that means we will
find a simpler distribution Q to
approximate the true posterior
dispersion and then we use the to
distribution in a future for inference
so I think the main idea to a computer
to diffusion is pick a divergence
minimize the divergence from the
approximate posterior to the actual
posterior distribution so this is
exactly the idea of version inference
which takes the KL divergence and I will
show you why stats
I think there are two main cool things
for version inference that help us to
sidestep the difficulty of computing the
true posterior fusions so first version
inference proposed an equivalent
optimization problem to KL divergence
organization by first flipping this side
of the divergence value and then adding
a constant which is the lock marginal
probability so you can work out the math
is pretty simple and you can see that
the new objective only involves the
Joint Distribution POF theta and D
instead of the true posterior
distribution so that means we no longer
need to compute the difficult term the
marginal probability so we call this the
objective as the virtual lower bound and
indeed this is a lower bound the log
much of ability so this means we can use
this lower bound and say on an
alternative to do hyper parameter
optimization or maximum likelihood
because the increase of the lower bound
will guarantee the increase of the log
Martineau so we are kind of safe to have
a problem the optimization using that so
these are the good things for version
inference but there are some problems
with that and I will show you some toy
samples to give you an intuition so the
first example considers approximating a
correlated Gaussian with very low mean
field so that means our cute expression
will be factor s Kelson so in this case
version inference get'em incorrect but
it underestimated the variance of the
pediatrician here so this can be
problematic for some cases where you
need a well calibrated uncertainty
estimates
here's another related example in this
case we want to approximate a truncated
gaussian with virtually inference so you
can see a truncated Gaussian
by restricting a Gaussian distribution
shown in red dashed lines in the box
that is in the black dash lines so if we
want use
a virtual inference to approximately
strand again truncated Gaussian with a
Gaussian distribution then the zero
forcing behavior of the virtual
inference will force the Q diffusion to
have zero value whenever the key
description has zero value Tara so that
means at convergence it will fit a delta
function at a mode so which has no
uncertainty estimate here the final
example considers hyper from the
optimization using aversion lower bound
and here I'm showing you the log
marginal probability in red of a linear
regression of a linear regression model
as a function of the hyper parameter the
output noise variance Sigma I'm also
showing you in blue the better lower
bound as a function of Sigma and this
bound is kind of tight because I have
already optimized the Qt tribution for
every single Sigma setting so now we
want to do having from the optimization
using vegetable instead of the truth log
marginal probability and we can see that
at convergence the solution we get is
severely biased from the true ottoman so
this is because when we use better lower
bound
it contains the K autumn and that Carol
term will bias the da solution towards a
place where the bow is tight so I think
for more complex models both the low
majority and austere version of a bug
can be non convex and we have very
little idea about whether this solution
we get by optimizing the brush lower
bound will be close to the true autumn
so now I've shown you some kind of toy
examples about why clear divergence is
not the best choice and here we want to
consider some alternatives to KL
divergence minimization and in this work
I will show you how to apply the veteran
inference techniques to a family of
divergence called Rainey divergence so
why we can do that I think in general or
a divergent minimization
incredible but nowadays the models we
use for version inference it's very
complicated for it's like neural
networks deep generative models and for
those models the lower bound is larger
interval but we can still do that
because we have now we have Maputo tools
like multicolor methods either simple
money color on em cnc and kind of
recognition models to parameterize the
key distribution so here i will also
show you how to apply these advanced
techniques to any divergent immunization
in our framework so let's see the
definition of rainy diverges where I
shown in the first equation I should say
that the drain diverter is one of the
definition for alpha divergence and
there are a lot of definitions for that
most of them include this integral term
here but I will show you in the minutes
why we use this specific form so I also
listed some kind of special case that
includes the divergence we use in other
problems and one important example ears
here that if we limit the alpha number
to 1 then we basically recover the KL
divergence so that basically means clear
leverage this minimization can be a
special case of our framework before I
go to the this house I just want to say
the two important properties that we
will use in the next durations
so first Raney divergence if you fix the
number of P and Q it's non decreasing in
alpha and second we have this skew
symmetry it relates no the divergence by
flipping the argument so this can be
easily seen by the definition you just
need to replace this constant here and
these two properties are also true even
for negative alpha values I mean in the
next few slides I will instead of frame
when to negative alpha although for
negative alpha this definition is no
longer a divergence measure
but it can still be very useful
okay so now let's say we want to do an
alpha level this minimization to get the
post-it how do you get the approximate
posterior then I'll just use this thing
idea of veteran inference to form the
equivalent automated and problem that I
take the negative value of the
divergence and then add in the constant
the same constant as before so you can
substitute in the definition of alpha
divergence and work on the math but I
will show you the final results where we
get this new objective function that is
also a function of the joint probability
rather than the posterior fusion so we
are kind of happy that we all also kind
of sidestep the difficulty of computing
this lot Martinho top I call this
boundless version or any event kind of
the closet ever just is Jenny divergence
and we can see that aversion Aloha BAM
is a special case in this framework by
limiting alpha to 1 so we are kind of
reached a kind of modern offering or
diversion inference and also when alpha
is greater than zero the diverges value
is always non-negative so we arrive a
lower bound to the log marginal
distribution yeah before I go to the
properties of the bound let's see how it
was for Tito exam host so here I'm
plotting you the kind of this result by
taking alpha equals 1/2
so by using alpha between 0 &amp;amp; 1 we can
compensate for the underestimation of
the variance in the first problem we can
fit a better approximation to the
truncated Gaussian comparing to rational
inference and we can be less biased
I'll have a from the organization when
we use the mere objective as a
surrogates
so in general I would say that by
picking alpha between 0 &amp;amp; 1 you can
change your emphasis between they are
all avoiding and
and the zero forcing so basically when
you want to flee the mode you may want
to pick alpha that is close to one
that's close to where any inference and
when you want to cover the house
probability mass you may want to pay
alpha that is close to zero so for
previous simple problems the divergence
minimization is credible but in general
you will be very difficult to compute so
here we use the money color estimate and
this kind of softens problem so I just
do Simoni color by taking examples from
the Qt fusion and then replace the
expectation with the empirical mean here
so this estimation
unlike the monocle estimation of the
original bound is biased because the log
now is outside of the expectation but we
have some nice property of this
multicolor estimation where I show you
in this kind of little figure here so
first remember these two properties I
mentioned the non increasing property of
divergence and also the US skills mitri
so that basically means we can lower
bound or upper bound the locknut already
by picking alpha valued picking positive
or negative valid values then if we use
Monica estimation to approximate the
assets bound we basically achieve a
lower bound to the insert value when
alpha is less than 1 so this is shown in
this blue dotted lines you can see that
this estimate is the lower bound the is
set value and especially if you just
used one example so then you can kind of
get rid of this average so you basically
get back to the Monte Carlo estimation
of the variable
finally you can improve the tightness of
this lower bound of this row bumper
increase the number of sample
so this is shown by this blue - arrow
here so combining all these properties
we can see that if you are limited to
use a finite number of sample k then
basically you can pick a negative value
alpha 2 here to actually achieve the
best approximation to the lot of
marginal probability and I think this
can be very useful I have a frame the
optimization or much more likelihood so
now I will show you an example and here
I use the better auto encoder as in
illustrating case so I think now the
deep general model is very important and
very popular in for research so the now
we consider fitting a generative model
shown back here to the data distribution
and because doing mathematical is in
general here that means we need to
integrate our all the hidden variables
that's incredible so we will use the
veterinary bank as a alternative
objective for approximate map much more
likelihood and here I use the Monte
Carlo estimates because for these models
it is hard to actually compute the
expectation in that way I think the idea
of rational or another is that we will
penalize the key distribution using say
a neural network and the neural network
we will be penalized by some parameter
Phi also they will try to optimize both
the Batchelor parameter and the model
parameter theta jointly using this lower
bound so if you are kind of familiar
with bed-wetter autoencoder
then I will show you a trick to help you
implement our world in less than 3
minutes so the trick is the repair
insertion trick also used in original
paper so I think the information tree it
tries to separate the nonlinear
transformation transformation frontier
and eni's so for example if your
positive use is a Gaussian with mean and
variance defined by a neural network
then you can basically reap Emma tries
your hidden variable by a link function
that separates deterministic turns here
and also the noise so then basically
means you can replace the estimate ation
of overkill by the expectation over the
noise variable so if you do that and use
a finite sample to estimate a lower
bound then you can push the gradients
inside the empirical average and you can
work out the math that the gradients of
the money hello estimate will be the
weighted average of this log ratio so
this love ratio also appears in the
version altering the training algorithm
and here the main leaf the only
difference from that is we have the
weight here which is proportional to the
ratio so that means basically we have
this term so that means we can get these
weights for free by just picking it up
and brace it with some power we can see
that if we take alpha equals to 1 then
basically this term vanishes so we will
get back to the original version or
going to the framework also very
recently further at all proposed the
important weighted auto-encoder or IV
they call it and that is exactly
equivalent to our framework by picking
alpha equal to 0 and as I said before
for finally some holes we actually use
negative alphas to improve the estimates
of the log Martino's and in this case I
don't know how to pick alpha so I just
put it push it to the string I pick
alpha which tends to minus infinity so
that means basically this means this
ratio goes to positive infinity so that
means we will just pick the sample with
the maximum importance weights and back
prop
it's ratio as the error signal for a
gradient descent updates so let's see
some results I tested this idea against
the original impersonal also with a
friend one and also the arisen idea I we
so I would say I we works much better
than virtually inference basically peace
hers their bias tighter and but our
method was basically very similar to I
we I think one kind of composition gang
is that we compute these things in a
much faster way than I we because
remember the batprobe step which we only
pick the sample with methanol weights
however I wish as they for I will they
will pepra all the samples so that
basically explains disk out of training
time just to finish just to finish for a
few more points so last just I just show
you the example of doing much more
likelihood with version right now but if
we go back to the posterior
approximation problem we clearly see
that the new bound needs to be computed
on the whole you know set so that can be
a problematic for you know many bad
training so we have considered to
approach to to this one is to say okay I
derive the gradient of the fixed point
conditions on the accept found and then
I will use a mini mini batch beta 2
approximately community so this is the
idea of Scott has an EP I finally really
realize that and which is published in
last year snips and another idea is to
approximate the lower bound using mini
batch data and then the arrival crater
on that so we call this idea
black post alpha so which appears in
past
nice workshop for Carol evidence when
you're paying alpha equals to 1 so these
two methods are equal and and this is
store has a very inference so
just to conclude so in this work we have
derived a kind of rich family of
variables that includes lot of is it in
case and we also kind of proposed a new
special case in this framework so we
instance the techniques like Monte Carlo
methods and returning models to the
rainy reverse immunization and I've
shown you some kind results so I think
for the future work the first question
is definitely how to pay the divergence
in use and currently I do not happen I
do not have a clear idea about that and
secondly how to apply other tricks for
example like control variance that is
like the prize money kind of messes for
version inference to the better range
bound and finally can we quantitative
analyze the bias no no I give you kind
of like a intuitive idea about the Monte
Carlo estimate bias but we don't
actually know at the current point how
to quantify model it so thanks for your
attention and I would see some questions
we do have time for one or two questions
yes yes there will become some variants
here I don't have to figure here I can
show you a kind of offline but actually
we have evaluated the kind of a bias and
variance of this example and it seems
that for different alphas the variance
seems will be a constant so that's it's
not not to be a problem Alex in this
case so you mentioned the bias problem
so there's kind of classic techniques so
try to reduce the bias for example like
the logarithm function to do an
expansion they write a polynomial
expansion and then do a bias correction
there are these applicable or what is
their fundamental difficulty in trying
to apply this classic bias reduction
techniques I mean I think that should be
applicable in fact I think lots of
methods that has been used for virtually
inference will be applicable to the new
version of framework I haven't tried
that I think I would try that in a
future great if there are more questions
then let's thing in turn again
so the next talk is a tongue buoy
talking about deep Gaussian processes
for regression using approximate expect
so today I'm going to talk about our
recent work on a postman inference for
deep Gaussian processes and this is a
child work with Jose Miguel Hernandez
Liberto
and Daniel Roberto sings an Li and my
supervisor Richard Turner and he's our
the links to the archive paper that we
just put up a few weeks ago and the
motivation of our works is to do
nonlinear equations ie if you are given
a set of observations X Y you want to
make prediction at some location X and
we are interested in the meaning of the
predictions as well at the uncertainties
of the predictions so you just and I
think here the few points that we are
with things that are important for
prediction machines first its need to be
deep and wide because real data often
contain hierarchical structures in it so
we have to be models to reflect that
structures and also if we want to be it
will build that hierarchical structures
is arguably is easier to learn because
it's often easy to learn
a series of weak non-linearity than
having to learn just the strong
non-linearity at one because the model
parameters are the bottleneck between
the training data and the test data so
ideally we want the model to be non
Prometric or like the
number of parameters well with the
number of training points and let the
data prune those parameters the third
poise and the most important things is
we need to be probable effects ie we
need to get uncertainty estimates of
output for our predictions so you you
heard Karl talked earlier about Gaussian
process through questions so this is
just a refresher slice a cheap piece is
a collection of random variables any
finite number which have a choice gasps
in the traditions and the map you can a
multivariate Gaussian distribution has
the mean mu and Sigma and and you can
look at the GP and it is fully suppose
specified by a mean function MX and the
covariance functions K xx prime how we
use Chi pay for nonlinear equations in
nonlinear equation settings we often
assume that our observations is is
formed by taking the value of the fun of
the functions and adding some noise and
we use GPS at the prior over the
nonlinear functions FX so we talked
about the three important things that we
need for prediction which is its turn
out that GP satisfies two points here
with non parametric and probabilistic
but we need deep and Y as well so GPS
request doesn doesn't have this this
point here
and and two other important things that
we have to deal with GP regressions is
it's very hard to pick a covariant
functions and predictions Mac by
different covariance function can be
drastically different and we often use
Parcel Post Nations and those reports
emotions are quite a match
damaging for the predictions so here's
one examples where GP regressions that
in perform really well so in this
example here I tried to fit the value
functions of the mountain car problems
so this is the phallic valley functions
with input x1 and x2 and you can see
that there are sharp change around this
region here and you you GP then it just
smooth out those certain or annuities
and it doesn't captures the change that
happens in this region here but if you
use deep Gaussian process for example it
will the typical overall functions if
you look like this so it's handled these
are the changing the regions this
regions very well and it's much better
compared to the GP fit ok so so this is
the graphical models for deep Gaussian
processes
so you can imagine you have the input as
before and you have the the cheapy down
here but instead of taking the input
directly to the final GP you map that
input through a series of Gaussian
processes and mathematically you draw
the the the functions each layer from a
GP and you walk those the outputs and
the inputs through those functions to
form the hidden variables and I defied
the overall functions as G and I defy
the hidden variables in the intermediate
layers as the functions of the previous
hidden variables so deep cheapies
multi-layer generalizations of Gaussian
processes and is mathematically
equivalent to deep neural Nets with
infinitely wide hidden layers it will
interview by Domino and Lawrence in
2013 in this unsupervised learning
context but here we want to step back a
bit and look at supervised learning
tasks and see whether we can get those
models to work on those tasks so he is
that mouthing car example again I use a
two layers deep cheapy where I have the
input mapping through two functions in
the first layer so at 1 1 &amp;amp; 8 1 2 and
then I take the output of those two
functions
I want that true now the cheapy and some
adding some noise that's in my
observations so the elbow functions
again at I'll show you before
yeah it's look like this this doesn't
look anything like these functions so
it's a function in the first layers are
different and this is walk through this
highly nonlinear functions I want to
make some connections to the GP States
by models that car top earlier so the
deep Gaussian process is a stack Haraki
of GPS where you observe where you have
some inputs and you observe some outputs
at the bottom layer here whereas the
Gaussian process dies by models for time
series you have possibly inputs coming
in and some observations at each time
steps and you have so the same GPAs each
time steps taking the input from the
previous time step to come the output
for the next so you can you can look at
you can turn this around and look at
compare that to the deep GPS and the
same inference technique that Carl
talked earlier can be applied for the
deep Gaussian processes but I'm going to
talk about different techniques based on
expectation propagation so why what's
what are the the advantage and the
advantages of BGP first they are deep
nonparametric so they can be very useful
they can discover useful input whopping
and compressions and and expansions
because you can imagine you have the GPS
before and these upper layers just
perform feature learning for the kernels
down here the overall mapping G from the
input to output is no longer now non
gaussians as in Gaussian process also if
you use sparse approximation for each
layer that the damage done by the
summation can be repaired by deep GPS
and importantly this hierarchy here
supports a postman Bayesian inference
which is what we're interested in a few
drawbacks of bgp the models is very
flexible and at the moment we don't know
how to to be in prior knowledge for
example in variance in the in the input
or there's a problem with
identifiability
so if you have highly nonlinear
functions here it's very hard to
identify them ok so we want to find out
the what are the distribution over the
functions the lighting functions given
some observations and this quantity here
can be can be found by using Bayes rule
which which has the cheapy prior the
livelihood functions and the
marginalized hood at the bottom so we
can use the marginal Idaho to do hyper
parameter tuning but because of the
non-linearity in the models the
posterior of the license functions and
the marginalized hoot are intractable
so here is the diagrams of the inference
technique that we can use the exact
solutions are of course intractable in
our case so we can use various inference
techniques for example sampling based
technique MCMC or the tabulation upon
summation techniques allows EP or
variational influence or we can go to
the extremes and just get the point
estimate and the x-axis is the
complexities of the method and the y
axis is the accuracies for our
predictions and these guys are what we
are interested in in particular we are
interested in EP and variational infants
and a general class called power EP
which has EP and variational inference a
special case okay so so I'm going to
skip the introduction to expectation
propagations
but I think this this is the
introductions slide from EP to power EP
so the goal is to approximate this
posterior this difficult posterior here
by some simpler oppose approximate
posterior that tact this form so this
this has the prior over the blyton
functions and some approximate
livelihood factors that are
possibilities the true lining hood
factors so EP performed this KL
minimization problems and then we try to
minimize that by finding out what's the
best approximate livelihood factors
where we fact the we divide the via this
approximate linear factors and then put
it back to the decay out here and this
is the correct value factors
power ep's replace this KL diversions by
general alpha diversions and the the
this fashion in size the the diversions
are the same as before
so but why do we use power EP again
instead when alpha a quant skip back EP
solutions and it's offered as alpha
tends to 0 you get back to the
variational solutions so it's really
flexible that you can tune alpha between
0 &amp;amp; 1 and you can get solution between
EP and variational solutions so we can
perform the the the procedures that I
tell you before and at the end you get
the the apostolate Martian O'Reilly hurt
given by stock procedures this has some
interesting terms that effectively asked
whether the posterior the approximate
posterior is it's like the prior or
whether the posteriors explained the
data well and for our case we use the
form of the approximate for serious is
the products of Gaussian processes where
we use the ideas of inducing poise or
pseudo data points to parameterize HTTP
and computing the apportionment marginal
area hood involves computing these
difficult terms here and that requires
an additional approximations where we
have to propagate the Gaussian through
the networks and then compute the the
gradient backwards so it's very similar
to the back propagation algorithm that
are used in veneto neural net it was
used in the Bayesian neural net contact
by Nadella bottles and Adams last year
so
we can see DGP training in actions so it
starts the again the valley function
example that I showed you before we have
the function at the beginnings they look
very simples
this look like identity mappings but you
tried it bit is quickly realized that
you can fit the data better by using the
full powers of the GPS in its layer and
having very different non-linearity and
this is no longer like identity mappings
as you see at the beginning and again
this is what I show you before each
keeping fit our is much better than the
cheapy fit what interesting experiments
we did was detect the or Liberty dataset
which has 15,000 files images and 5000
test images and crop and rotate the
image and set up a prediction task so
that we want to ask what is the angles
of rotations given some training image
and and the inputs to our networks are
permutation invariants we have now no
ways to be into the structures at the
moment yet so if you give a cheapie the
arrows is about 16 degrees if you do
each DGP you can approximately heart
that arrow right
if you use a convolutional neural nets
on top of a GP you can get down to six
or seven degree so it's a significant
improvement if you use deep CP and you
get it you can get better results by
getting some structures into the model
an additional experiments we did was to
compare the GPS GPS and basic neural
nets with one or two hidden layers using
these approximate inference techniques
so variational influence with without
the realization trick starting then
discussed before ppb's which is idea
assumed density filtering with the
probabilistic by propagation method drop
hours which is to come by drop a
prediction at test Tom's and to sampling
based approach SEO D stochastic gradient
95 dynamics and hey CMC Hamiltonian
Monte Carlos but this only works for
small dataset and small networks for now
I think you need to pay attention to the
colors the blues and the blacks are deep
CP Greens GP and Bret Bayesian neural
Nets we compute the average rank across
all dated sets across all the splits for
all the data sets so the lower the
batteries in our average the the the lip
GPS often performs a perform GPS and
base in neural nets we can have some
interesting discussion on this if your
interest again here's the same we had
ten data sets of various stars the the
biggest data sizes has five thousand
data points with 90 dimensional inputs
this is these are the best results for
each model and inference techniques if
you use Bayesian neural nets with
deterministic approximation techniques
this is this is what you can get and the
higher the better
so you sampling is you get slightly
better in some cases and you cheap hey
you can get a little bit better but not
these for this data set
you leave cheap paid on average you get
even better so I think Dixie peas are
great because we have
we have showed that is confirm well on
various regression tasks it satisfies
all these three important points and our
contribution lburrows is we revisit the
GPS and use them for regression tasks we
propose a powerful postman inference
techniques based on power EP and yeah
that's it thanks time for questions but
can I ask the next speaker to set up
from my experiments I think we can get
like results with very only two or three
layers
did you pee of course there's a problem
with the gradients when you have more
and more layers so I think we need to be
structured like linear plus nonlinear
line the residue left in order to get it
to work or deep architectures but I
think we we have kept tools to do that
that's a you part the answer that
question I was gonna ask so I asked it
related question which is that release
been a problem of very deep neural
networks and also you mentioned that
it's similar to a dynamic system neural
network people have come up with
different hacks to make this vanishing
gradient problem go away so for example
a test EMS for the dynamic systems is
there anything like that that you could
you think you could apply to the EP
based approaches
I don't have a good answer to honest
so the I think we are working on the
linear personal linear bet which is sort
of similar to to what people with the
tricks that people have been useful for
the deep neural Nets to encounter the
problems of the gradients furnishings
but yeah I yeah I don't have the answers
well I don't know but there's the
systematic way a principled way to do
that the 1080p or variational a final
question rbfn right with one layer now
basically if I look at your deep TP
looks a bit like a multi-layer rbfn deep
are be fed if you considered and it
sounds like okay when you understand
them to be like a more constrained
rbfn then you're kind of imposing
additional structure in order to Train
the weights first of all the static
equation correct and see have you ever
considered comparing yourself against
the deep rbfn to answers the questions
we now we haven't compared to RBF
networks at all but that's an
interesting model to compare to I think
I I'm not entirely familiar with deep
RBF networks but I think the the RBF
networks often assume a certain number
of basis functions and it's often very
hard to trend deeper architectures so
even even shallow idea is often and
often so I think but for this network I
found that this is very easy to train
there's no special trick in term of
initialization at all I'm afraid we have
to move on but let's same time again
so that I think we've got the coffee
breakneck so don't worry this is joint
work with Tony Jabara and with Justin
Dom key and for those who are interested
these slides and also the papers that go
with them are available on this website
here so I'm going to speaking about
undirected graphical models they'll be
familiar to many of you but I'll give a
quick background these are powerful way
to represent relationships across
variables with many applications
including computer vision social network
analysis deep belief networks protein
folding and many others in this talk
will focus primarily on binary pairwise
models so it's called easing models
although some of the results I'm going
to describe do apply more generally so
ask me if interested
Winery pairwise models still have many
applications so as an example here's a
typical grid from a computer vision
application here each variable will be
associated with a pixel and we might be
doing foreground/background analysis
where we have local information about AB
at each pixel that comes from the color
and intensity of that pixel but also
because objects in the real world tend
to be contiguous we know that adjacent
pixels tend to have the same
characteristics so we have edges between
these pixels which are attractive and
tend to pull those variables towards the
same value that don't have the same
character of being either attractive of
being either foreground or background
and because all the edges in this model
are attractive we call this an
attractive model and I should say that
for a binary power-wise model we can
always characterized each edge as I have
been attractive or been repulsive in
which case we tend to push the variables
to different values so here's another
example part of a social network here
each dot would be a variable associated
with a particular individual and might
represent their preference for a
controversial topic so maybe do we think
that the food is better at Microsoft or
in the engineering department although
maybe that's not such a controversial
topic actually and we're very happy to
be here at Microsoft so here we have we
have blue edges which are attractive the
idea would be the people who are friends
with each other might tend to pull each
other towards the same preference and if
they don't like each other they may tend
to push each other apart
from preferences indicated in red so
because we have both attractive and
repulsive edges we call this a mixed
model here on the top we have a very
small restricted Boltzmann machine at
the bottom we have observed variables at
the top we have unobserved variables and
these are connected by edges which could
be attractive or repulsive so again it's
an example of a mixed model and a
fundamental problem for all of these is
what we what we know is marginal
inference where you want to estimate the
marginal probability distribution of
just one or a small set of variables
which requires us to to sum over or
marginalize out a whole lot of other
variables it's closely related to
computing the partition function which
will define carefully soon both of these
problems require summing over an
exponential number of states so they're
both computationally intractable and
hence has a lot of attention on
approximate methods and the theme in
this talk is that by combining
approximate inference with clamping we
can we can use that as very powerful
proof technique and it can be very
helpful in practice so first let me give
some background of binary pairwise
models we naturally have binary
variables and we have singleton impaired
eyes potentials which we're going to
concatenate together into one vector
theta we'll write theta dot X for the
total score of a complete configuration
which includes the singleton and edge
potentials and the probability
distribution as usual is given by this
expression where the probability of
configuration is proportional to e to
the score and we need this normalizing
constant Z which ensures that it's a
valid probability tribution which sums
to 1 z is called the partition function
it's a fundamental quantity which we'd
like to compute or approximate and we've
heard earlier today that variation
variation methods can be very helpful so
let's take a variational perspective
here it's well known that the exact law
partition function can be written in
this way at the top here so we've got
the maximum over me mu is going to be a
vector of marginal probability
distributions and we optimize over this
space M which is the space of marginals
that are at the correspond to a globally
consistent probability distribution over
all configurations and given this this
new we want to optimize theta dot mu
which is the
affected score given that set of
marginals plus s of mu which is the
entropy of the global distribution which
corresponds to that mu this is exact
inference from a variation perspective
we're going to consider a few different
variational way various forms of
approximation first we'll look at the
beta approximation and there's a picture
of an Spade row at the top right looking
down here so the base approximation
makes two pairwise approximations we're
changing the red things for blue things
so instead of optimizing over m the
space of globally consistent
distributions we optimize over a relaxed
space that's easy to describe this is
the space of pairwise consistent
marginals and we replace the mature
entropy with this beta pairwise entropy
approximation even for those of you who
aren't familiar with this beta
expression probably most of you had seen
loopy belief propagation and it was
actually shown by a diddy Freeman and
Weiss that fixed points of belief
propagation correspond one-to-one with
stationary points of this beta
expression in here so it's in very
widespread use and it's well known that
models that have no cycles the Speicher
approximation is exact so we have Z be
equal Z if there are no cycles and it
turns out that even if there are cycles
the major approximation is still often
very accurate but it's been very
difficult to to guarantee its
performance and one of the things we're
gonna do here is we're going to show in
many cases you can we can show both
upper and lower bands on Z B relative to
Z so we've said that Z B equals Z it's
an exact approximation if there are no
cycles when else does that perform well
so intuitively performs well when you
have models which are tree-like so if we
have only very long cycles or if we have
edges which are very weak then it's like
a tree and it performs typically very
well it also it turns out performs well
if we have a tract of models so remember
the first example from computer vision
there were a lot of models which which
have attractive edges all the edges are
attractive we have an attractive model
and empirically it turns out the beige
approximation there performs very well
also turns out empirically
Erik saw that Martin Wainwright and
Allan will ski observed that it tends it
will that they in all their observations
had turned out the ZB was a lower bound
for Z and they proved that that was the
case for certain models they conjectured
it would be true for all attractive
binary pairwise more
that conjecture was open for a while in
2012 Nick grazie proved it using very
interesting method of graph covers which
we won't talk about here there was a
fascinating method introduced by Pascal
von Tobel
but it was a little tricky for some of
the people in our community to follow it
somewhat technical here we're going to
provide a separate proof building from
first principles which also allows us to
derive an upper bound for Z in terms of
Z B I'm going to use the idea of
clamping variables so what is clamping
so here at the top left we have an
example model and to compute the
partition function just a brute force
way to do that would be to enumerate all
of the states we've shown here for and
for each state we can write down the
score we can write down each of the
score and we can add them all up to
compute that total partition function at
the bottom alternatively we can think
about splitting that total into two
pieces
so think about clamping the variable x1
to each of its two possible values and
then adding the two sub partition
functions so you see in gold we've got
all of the configurations where x1 is 0
and pink we've got all configurations
where x1 is 1 and obviously add those
two pieces together you get the same
original total but notice that when
we're computing one of those conditional
sub partition functions we've held x1 to
one of its values and when you do that
it doesn't influence the model so you
can consider removing that from the
graph now after you removed it there
were if the remaining models were
acyclic which is not quite the case here
because you'll see we've still got one
little cycle left but if it were a
cyclic then you could find those sub
partition functions efficiently because
as we've said before the Pedro
proximation
is exact on trees if it's not a cyclic
well we think we have two options we
could keep repeating me to keep clamping
and removing variables until we get an
acyclic model or we could settle for
approximate inference on the sub models
so I'm going to introduce this idea zpi
in red by analogy to the two the red
expression above for the exact partition
function so we'll define zbi new and a
new thing we're introducing it's going
to be the approximation you get if you
clamp variable X I to each of its
possible values and you sum the
approximate sub partition functions this
is a new approximation
and at least to a question will this
lead to a better estimate than
approximate inference on the original
model so any guesses this message says
yes will it always do so Sebastian says
yes so it's very good intuition it turns
out the answer is often but not always
so close close and we're gonna we're
gonna prove in some cases that it will
and will also show a case where it
doesn't so let's see where happens it
and just another good piece of intuition
which would meet with one of the things
that was helping specially think about
it's clearly if we kept clamping enough
variables until the remaining other
words acyclic then it would be exact but
you don't necessarily always improve
monotonically although you will in
certain cases which we'll see so let's
take a variational view of clamping so
at the top we've got our original
expression for log CB observe that when
we clamp X I we can think about that as
the same optimization but over a subsets
we're gonna take the slice through this
space L where we're going to hold the
marginal Qi to being zero so because
we're optimizing the same thing over a
over a constrained space clearly our
we're going to get a null form which is
going to be upper bounded by the
original optimum and that'll also be
true when we constrain x i2 be held to
one so both of these conditional
approximate partition functions are hapa
bounded by the original approximate
partition function let me just recap
notation and please try to follow this
we're going to keep using these
expressions we have Z for the true
partition function we have ZB for the
beta optimum partition function and we
introduced z bi which is what you get if
you clamp xi2 each of its values and you
sum the approximate sub partition
functions and we observe from this
simple variational perspective on
clamping here that z bi is upper bounded
by 2 ZB will see that simple observation
gives us a powerful result so here we
start with with what we had before at
the top we have the simple observation
and then like we suggested doing before
we're going to clamp and remove
variables until the remaining model is
acyclic where beta is exact let's see
what happens if we do that for a model
which requires two variables to be
removed this is our original original
model we started with if we clamp x one
in the next to the remaining
is acyclic so if you have a model like
that you clamp and delete these two
variables we're going to get this
expression we get Z bij so this is the
approximation you get if you hold xixj
to all of their possible values that's
for possible settings and you sum up
those approximate sub partition
functions and because each of those is
optimizing over a subset each of these
conditional zeebee is upper bounded by Z
B so this an in total is upper bounded
by for Z B the insight is that now we've
got an acyclic model here so each of
these conditional zeebee is actually
equal to the conditional true z because
we have accepted a cyclic sub models
here and when you add together the
conditional true Z's it's like when we
had the pink and the gold before and
they add up to the exact true z so this
expression is exactly equal to z and we
get Z is upper bounded by for Z B we can
use that idea anymore the way we're
going to get a bound which depends just
on the number of variables you have to
remove to get to an acyclic model so a
such a set of variables which when you
remove them leave the remaining model a
cycle is called a feedback vertex set
and we get this general bound that Z is
upper bounded by 2 to the K if K is the
minimum size of this time Z B this is a
band which will hold irrespective of
potentials it depends only on the
topology and if people are interested we
can talk more about its ties in a
certain sense but actually we of course
you think you might be able to get a
better band if we could also use the
potentials so let's just notice what we
did we came up with an upper bound for Z
bi and and by using this idea of
repeatedly clamping or removing
variables until we reach an acyclic
model that gave us an upper bound on the
true z so can we go the other way can we
come up with a lower bound on z bi and
get a lower bound on z and it says we
can for the particular case of
attractive models so we have a theory
I'm going to skip over the details here
but the details are on the website or
asked me if interested we actually have
a stronger result so it's it's quite
it's quite nice but we show that for an
attractive binary pair wise model and
any variable X I when you clamp and sum
the approximate sub partition functions
you can only increase the estimate so
then if you if you repeat as before you
get this series of inequalities
which ends up with the exact true
partition function and so clearly we
we've reproved this result the ZB is up
abandoned by Z for this class of models
and in addition we have this result that
each time you camp and some because you
have these this series here you're
moving closer to the true value so for
these models you do always improve the
approximation
just to recap we've used clamping so far
as a proof technique deriving both lower
and upper bounds on Z for attractive
models and we also have a one-sided
bound for any type of models we also
prove for attractive models the clamping
and summing can only improve the
estimate but what about for mixed models
so interesting here's a small example of
a mixed model blue edges are attractive
the red edges are repulsive and for this
model it turns out that if you clamp any
variable and you sum the approximate
side partition functions you actually
get a slightly worse estimate um it's
interesting it's it's it's this is quite
unusual case the S moves only slightly
worse and typically if you if you look
at a model if we pick a good variable to
camp you will you will usually improve
the approximation quite significantly
but there are examples like this it's
interesting okay so I know when it turns
to you work where we examine the same
sort of thing and what the effect is for
the mean field and that and the TRW
approximations so the mean field
approximation probably you all familiar
with this this assumes that the
variables are independent and always
yields a lower bound the tree we waste
approximation is a pairwise
approximation a bit similar to beta but
it it allows a convex optimization
optimization and always yields an upper
bound so we have this nice nested nice
sandwich relationship that the true
partition function is always between
these two and it turns out it's actually
also not hard to show that the beta
approximation
there's also naturally between these two
so be very nice if it turned out that we
were able to clamp in some situations
and improve the bounds we get from these
because it would give us better bounds
on both the true and the beta
approximate partition functions so what
do people think if we clamp in some with
mean feelings urw what happens maybe
I'll turn to Sebastian since he's oh do
anyone else
sometimes it works hard for that to be
wrong any other guesses sometimes it
works is right but actually Sebastian
would be right this time so it turns out
may be surprising and given the previous
result me feelin TRW we can show that
irrespective of the potentials and also
actually for models where you have
variables than any number of labels you
always can only improve the bounds and
improve the approximation with clamping
we haven't got much time to look at
empirical results please look at the
paper if you're interested but I'm just
going to show you a few quick ones so
this is looking at grid models where
actually that the method works
reasonably well but there are other
situations where it works better so this
is not a horrible event it's not an
unfair example each each of these four
panels is showing a different setting
let's just focus maybe on the top left
panel to start with what we show I hope
to make it easier to compare is in the
in the middle we're showing the beta
approximation results at the top was
showing TRW and at the bottom was
showing mean field which is justified
because as we said before the beige
approximation always comes in the middle
on the x-axis we're increasing the
number of variables which were clamping
so as we clamp more variables the
approximations are improving and on the
y-axis we're showing the error in the
log partition function the signed error
so TRW is always so it mean feels always
an underestimate TRW is always an over
estimate and you can see some things
well actually me be sure to point out
I'm showing results for different
heuristics which we introduced in the
paper for how to pick a good variable to
clamp and best means if you pick the
best of our heuristics worst is the
worst of our heuristics greedy where we
can do it mean to actually try every
single possible variable to clamp and
see what happens there but it gets
tricky if we're going down to five
levels of plan things when you show two
three and pseudo greedy means just take
the best of our basket of heuristics so
some what you can immediately see is
that beta typically performs very well
but actually you do get some good
improvements with uh with with clamping
and our heuristics work quite well so
I'm just gonna end with some conclusions
for practitioners typically beta
performs very well clamping can be very
helpful it's more helpful when you have
denser models with strong edge weights
which is a setting where inference is
typically high
so this is useful and you'll see in the
paper we provide some fast methods to
select a good variable to clamp the
results would mean film TRW are very
helpful because that gives you
guaranteed bounds both on the true
partition function and on the beta
partition function thanks very much we
do have time for questions so I have a
question to start so there are other
operations you could do to the graph for
example just practitioners sometimes
just drop edges or for example you could
merge two variables right like leading
to higher order approximation so
obviously you have only looked at binary
models in this talk but you said some of
the techniques extent do you think it
would also allow you to for these
operations to provide bounds or to say
statements like it will always improve
or things like that or it is restricted
to clamping only it's very interesting
question and the shorter answer is I'm
not sure and it's it's it's an
interesting area da which has looked at
some things in this direction but hasn't
really looked at this exact way of
thinking about it
I don't really know we have been looking
at some of those ideas from math
inference and we very happy to talk
about those afterwards if you interested
to the previous tops can we extend the
style of analysis to general affordances
that would be very nice don't know I
mean it's clearly initial explore there
are certainly attempts to sometimes know
about qpb oh I'm not so familiar with
them clamping the key PPO is usually as
you said for map inference yes I'm not
sure it's called PPO - oh okay I'll take
a look thank you well one thing I would
say which is just may be interesting so
I've talked about attractive models q
PPO is guaranteed to work beyond
attractive models it
it's guaranteed to work for balance
models but balance balls are I think of
them as being essentially attracted they
sort of attractive up to a flipping of
variables and the same thing would apply
here so everything I've said for
attractor models also generalizes to
balance models if there are no more
questions we can go to the coffee break
but let's first think a drienne again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>