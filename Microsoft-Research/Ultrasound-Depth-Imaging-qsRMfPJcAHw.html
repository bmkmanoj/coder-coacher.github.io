<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Ultrasound Depth Imaging | Coder Coacher - Coaching Coders</title><meta content="Ultrasound Depth Imaging - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Ultrasound Depth Imaging</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qsRMfPJcAHw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by Microsoft
Corporation may be used for internal
review analysis or research only any
editing reproduction publication
reproduction internet or public display
is forbidden and may violate copyright
law
I think we can start good morning thanks
for coming here good morning those who
watch us online today we have even
docked my niche from audio visual
communications laboratory in EPFL in
Switzerland with his advisor professor
Moulton bitterly and today he is going
to present the results of his
three-month long internship in Microsoft
Research here without further ado even
you have the floor thanks for the
introduction
yeah so I'm even and I'm going to tell
you that's right so yeah I'm going to
tell you a bit about what I was doing in
the in the last 12 weeks actually so let
me start with first things first
just a small thanks to - or a big thanks
to to everyone that sort of you know
helped me out at various technical
difficulties while I was doing my
project so the audio group or mark I
couldn't find high-resolution photo
yen's and anyone right and and also I'd
like to thank I'd like to thank the hard
work team
so Jason Goldstein and Tom blank
specifically Jason who Jason I'm just
mentioning you who helped me a lot with
with building you know and debugging
hardware ok so let's start with a short
overview basically gonna start with
trying to motivate why you'd use
ultrasound for something that's
typically done with light and it's done
very well with light and go through some
basic principles really fast of
ultrasonic imaging then I'm going to
talk about our hardware design and you
know beamforming how to get the
reactivity some calibration issues some
annoying things that happened and so on
and then I'm going to move to algorithms
I'm going to describe the naive
algorithm so the first thing that you
try to do imitate the way you do it with
light and then why this doesn't work
really well and and the things that we
sort of tried out and some of the things
that turned out to work much better some
things that we proposed specifically if
creasing the resolution and for for
increasing the frame rate so for
obtaining and it's like a usable frame
rate okay and then I'm going to go over
some experimental results and suggest
things that could be done in a future to
get even better results okay so okay
let's start with why ultrasound depth
imaging you know is typically done by
light either by by using syrup you know
the disparity between two images two
photos of the same scene or by using
blazers for example in connection and in
lighter so this is good and this works
fine but for example if you want to use
cameras you actually need you need the
camera you need you need advice that
does it which can be quite large quite
expensive can use a lot of power with
lasers you this potential always to use
a lot of power and these are all you
know very fast signals light has a very
high frequency so you have to take
special care in the design in order to
you know design the circuits that are
good for RF frequencies and and so on
right so what are the ways that
ultrasound can serve you know provide
something else well you know some of the
advantages are following right first the
frequency range this the frequency of
the ultrasonic sound sound that we use
is 40 kilohertz and 40 kilohertz is
fairly low in comparison with the
frequency of light right so this means
that you can design very simple hardware
you don't have to pay special attention
to PCB design you know to RF design and
so on so the design is very simple on
the other hand the propagation of the
sound is fairly slow in comparison with
light right so it means that your
wavelength is not gonna be too large
which is good because for imaging you
don't want your railing to be large so
this is somehow a fortunate coincidence
right other good thing is power
consumption so I I snipped here two
images from from the datasheet of the or
the transducer that we're using and you
can see here that you know at
frequencies of frequencies of interest
the impedance of of our transducer is
something like 500 say firearms and we
want to drive it to get maybe 100 DB of
SPL of what maybe 105 so we want to
drive it maybe 5 volts RMS which results
in 10 milliwatts of power consumption if
you use it full duty cycle right but we
don't use it full full duty cycle we use
it only only
part of the time we when we emit pulses
so do your math I mean we can really go
low in power consumption senses are
cheap they're quite available off the
shelf shelf the ones that we have are
you know fairly cheap ones and in some
ways it's complimentary to light so
light might fail you know if you have
for example a thin piece of fabric in
front of you or if the space is filled
with smoke or there's a lot of mirrors
you know it could be complimentary in in
some regards right so of course there's
a lot of challenges with ultrasound
perhaps much more than 10 advantages
when you look at it for the first time
so first challenge is the frequency is
still low so there's no way I mean to
get the level of detail ultimately that
you will get with with light because the
wavelength is for example of this
ultrasound that you use a bit less than
one centimeter
another issue is directive et if you
have lasers how her lasers are quite
directive by themselves right there
isn't no no mention of of trying to make
them more directive but which sound
sound sources are typically not so
directive and so you know we need to
find a way to serve imitate what we have
with lasers which sound okay and an
hour-ish you slow propagation that's a
very knowing one so if you want to
imitate light so if you want to rush to
scan the scene music using ultrasound
you know something's maybe three meters
away for a pulse of sound to travel
there and back it takes maybe 15
milliseconds which would amount to to
maybe 60 points sixty pixels if you can
scan in a scene or if you want more
pixels you'll have a really lousy frame
rate this is not good right this is
completely unacceptable so we need to
deal with that and the most annoying
thing probably is the specularity of
sound reflections so sound always
reflects specularly very little a very
small part of this reflection is a
diffuse reflection which means
essentially that if I'm standing here
and I'm shooting directly to the wall
I'm gonna hear a strong reflection right
so if I have a source of sound here and
a receiver there then a strong
reflection is gonna happen somewhere
there but if I'm trying to see what's
happening there with a source and
receiver standing here I'm you know I'm
out of luck essentially because the
sound is not with like here there is a
small diffuse component but it's
completely impossible to see it with
what we currently have so we also need
to somehow deal with this
okay another challenge is of course
attenuation of ultrasound in a sound
always gets attenuated when it travels
through air you know there is this four
point sources we have one over our law
just because if you put some energy in a
point source then it travels and this
energy is spread over large and larger
spheres so if you're listening in a
certain point on this sphere you hear
some sound that's with lower intensity
but ultrasound gets attenuated in air
because of the compression and expansion
affair much more so than the ordinary
sound in fact above frequencies of
around 100 kilohertz you get the
attenuation of more than three decibels
per meter which is quite substantial so
for something to three meters away you
would be attenuated to something like 20
decibels okay which is which is quite a
lot so with all these challenges you
know a real legitimate question is why
bother at all what I try to do it and we
have a good example in nature of you
know this beast that can actually do
quite well with ultrasound so somehow
you know we hope that you know the limit
is far away which we still don't do as
good as they do these are quite
fascinating little critters and they use
frequencies that are most mostly are
ultrasonic so so all the way up to to
200 kilohertz and they have fascinating
resolution resolving ability for example
the big brown bat can detect spheres of
roughly 2 centimeters across at 5 meters
away ok and more perhaps the more
fascinating thing is that they can
resolve detail which is one fifth of the
wavelength that they're using so beyond
the diffraction limit okay using some
some heavy special processing you know
and they're bright brain we're not quite
there yet but but you know sort of we
can hope for it right in the future ok
so let me go quickly over some related
uses of ultrasound and you know
ultrasound is mostly used the vast
majority of ultrasonic applications are
in in biomedical imaging and for a good
reason right so not so much ultrasound
was actually used in air and typical
image that you see that you see in
biomedical applications is image like
this one okay and this is this is called
a B mode scan so what is the B mode scan
how do you obtain the B mode scan
there's a simplest possible way of
obtaining and
images but just inserting sound so
emetics out into the body and then
recording reflections as a function of
time and plotting them as a function of
time if you do this in two dimensions
and encode the amplitude of the
reflection as the intensity of the pixel
you get an image like this okay so in
body you can do that because always some
part of the sound will get retransmitted
okay so something will refract on here
but some sound will go on okay then it
will reflect further and the medium is
like this random scattering medium so
every point of your tissue acts like a
small scatterer and reflects some sound
back okay so in air perhaps you know in
the late 70s that's one of the first
uses a polaroid was using using using
ultrasonic sensors as range sensors in
their cameras okay and then there were
people like Liam in the early 80s they
try to image the spine deformation okay
they're trying to use ultrasound in air
to image spine deformations so they were
scanning mechanically an ultrasonic
device along the spine okay obtaining
they were obtaining images like this one
then in the early 90s they were trying
to use ultrasound to measure the skin
contour so with slightly higher
frequencies and this sensors were always
very close to you okay so this is not
our scenario where we want to image
something is further apart
okay for your way so more close closer
to what we were doing we have some some
more recent developments by mobile
sensor built for example from 2007 where
they use for hard with microphones in a
synthetic aperture to try to image
objects that are you know a meter or two
meters away from from from the imaging
device and you know we also have some
some attempts to build aids for for
visually impaired people for example
these guys is 64 electric microphones to
image the scene and you know these guys
here also use ultrasound ultrasound in
air but it was different I'll come back
back to this later a bit they use
different kind of transducers different
than us something very small very very
very efficient and something is probably
the future so that's why I mentioned it
mentioning it here and what I'm showing
here are two images from from the
provide by my bus in subir so they were
trying to image Hill they were imaging a
PVC pol I'm not sure exactly at what
distance but I think it was five
centimeters across okay with four
hundred microphones so this is this is
the image that they obtained okay and
here they're trying to image a cubicle
object also also the same distance so we
understand that the specularity is a big
problem right I mean you see now what I
was talking about when I was telling a
talking about challenges of ultrasound
there is no way to get a reflection from
from this part this part of the pole
here because this doesn't reflect back
to the device so keep these images in
mind for later
okay so let's let's talk a bit about
hardware and and beamforming and
everything we have to do with this so
first thing we have to you know first
thing that i had to do is i had to
choose the proper transducers and proper
sensors and and this was quite a
challenge because you know you sort of
need to understand what will play what
will play correctly with the hardware
that we have here motor units how to
interface these things to PC and so on
and so very common very common design of
the transistors is yes electric okay so
just a piece of crystal that vibrates
when when you know you apply a AC
voltage across it and these are
something these are some typical designs
so this is a closed one if you have to
expose it to the elements this is the
one that actually we use devices here
the whole the whole thing is here I
could send it around later and then you
know you have you have these things all
the way so these are really small ones
these are really big ones they spent
something like hundreds of watts okay
and I found this this black one on this
guy here I found on eBay and the seller
lists a couple of Intendant functions he
says one medical treatment two beauty
device three kill algae okay so there we
go I mean yeah here you have another
example this thing here is 350 Watts
okay and it goes up to 35 kilohertz I'm
just showing it to understand but there
is a huge diversity of these things and
you know this the guy whose webpage I
had took this from says the sweeping
ultrasounds can cause certain adverse
effects like paranoia severe headaches
these are
datian osseous cranial pain upset
stomach or just plain irritating
discomfort right for animal control so
yeah a lot of a lot of interesting
things out there right so this is our
device and once again thanks Jason for
helping email Sui immensely with this
thing okay we're sort of debugging this
infinitely and still it has a lot of
problems but you know we sort of did our
best right and so let me talk a bit
about microphones okay so I explained
what kind of transducers what kind of
sources we can have this piezo electric
devices they can also act in the
opposite direction
when you the crystal vibrates they
generate AC voltage so you can use them
as microphones but we decided to do
different things so we're using here
MEMS microphones typically intended for
for using cell phones okay and these
microphones are typically intended to go
up to maybe 10 kilohertz but it turns
out that they also hear 40 kilohertz
their polar diagram should be only
directional which at 40 kilohertz and
with our design apparently it's not I'll
talk about it later
this is kind of a challenging point by
the nice thing about this particular
model by nulls is that it has a pre
amplified the differential output so
even if i drive it to the preamplifiers
i'll show on the next slide actually I
could use this directly to drive motors
line inputs it is very convenient and it
has differential output okay and these
are these are the transducers that I was
talking about
we interfaced everything to to a PC
using you know first two mode to using
db25 snakes and and so on this is the
battery to power the microphones and
this thing uses a separate power supply
because we need to drive it to negative
olds RMS so we can't use a battery for
us I could do it okay so so this is the
overall design of our system right we
have we have here the microphone array
and the speaker array microphones go to
microphone preamps and then two motor
unit and everything is driven by MATLAB
you know from from a PC using firewire
is actually USB a cool thing as I'm
saying is that it could actually drive
these guys directly to moto this is
something that we've learned later but
but this is just
in case we wanted to preamplifier them a
bit okay
so let me talk a bit about about
beamforming for for those of you who
perhaps don't know what beamforming is
which is none so you know we have to
find a way to direct sound to certain
directions and also to listen
directionally so it turns out that if
you have more than one microphone or
more than one loudspeaker even if this
microphone is for example
omnidirectional with more than one you
can achieve so that the the overall
characteristic the overall directed ET
of the system is selective so by
properly playing with the signals that
you record from these microphones so we
can use this to create a beam of sound
or to listen in a beam of sound and try
to steer this beam electronically which
is another cool thing because you can do
it electronically by changing the way
you play with these signals and to scan
the scene ok so there's many ways to do
beam forming but somehow two opposing
extremes are something we call minimum
variance distortion less response beam
former it's not important what it really
is and delay and some and they're
different in the following way
MV dr is great if your system is
perfectly calibrated and then it gives
you the best possible beam okay but if
your calibration is is imperfect your
calibration is imperfect then actually
it could perform worse than a very
simple beam form which is delay in some
and which turns out to be the most
robust due to manufacturing tolerances
so we sort of lean towards the delay and
some beam former because well
unfortunately we don't have excellent
calibration of our device even if we
wanted to have it which right so so this
is a polar diagram of a three element
microphone of a three element microphone
just through to understand how these
beams look like okay so if we for
example listening microphone array right
so if we're listening to
towards the direction of 0 degrees we
get you know sound intensity of 1 and if
we're listening for example to 30
degrees then we get less than a third
but still quite substantial right it's
you know it's still a lot of a lot of
sound coming from from this direction ok
something cool that we can do using this
forming theories we can think about how
to properly design the geometry of our
microphone and loudspeaker arrays
because obviously not all geometries are
equally good right some geometries will
provide us with better beams and
somewhat worse beams we need to find a
way to measure this and a classical good
a reasonable way to measure it is using
something called the directivity index
which is just if B here is the
directivity pattern directivity index
just says you know what is the ratio of
the directivity towards the direction
that I want to be looking at with
respect to the average reactivity in all
directions and this is a completely
reasonable measure and so what we're
trying to do what I was doing is I was
think I took a couple of candidate
geometries so across geometry a square
geometry and something like a circular
geometry and I was varying for each of
them the spacing the pitch between
microphones and I was evaluating the
directivity index for the corresponding
geometry and I was trying to find the
best one it turns out that for
microphone array somehow you know the
best overall geometry was this square
one and the spacing between the
microphones that we use is six and a
half millimeters which is slightly more
than lambda half and then half of the
wavelength okay so this is the finalized
geometry for loudspeakers four buzzers
four sources unfortunately we couldn't
we couldn't optimize it properly and the
reason is the physical size of the thing
is it's just too large right so we have
a mechanical mechanical reason and we
can not to be able to put it as close as
we would like to so their pitch is 1111
millimeters which is the smallest
mechanically allowed allows pitch okay
and what you can see here is that we
angle the transducers a bit out it still
we tilt them out again this 20 degrees
here is not an accident it's an
optimized number somehow the best one
and the reason is that the transducers
themselves are a bit directional so in
order to cover a wider field of view
it's a good idea to just tell them a bit
out it turns out that the beams in in
extreme directions will be better if you
do so okay so this is just a simulated
beam from from our microphone array you
know this is just to understand that
that even if you want to have a point
here a pixel we don't have it
unfortunately so
this is what you would get if you would
be looking at the beam from the front
and just flatten it so if you have one
scatterer a point scatterer then instead
of seeing the point scatter you see
something like this okay so this is the
meaning of this image so you know if you
remember how it looks like it's three
microphones essentially laterally and
three microphones vertically so with a
twin microphone array in some direction
you can't use much we have eight but if
you want to extrapolate the performance
you have in a horizontal plane you need
to square the number of microphones okay
so for example the performance that you
get with eight microphones horizontally
you'd get with sixty-four microphones in
in the whole space okay okay
so let's go on write calibration
calibration is important of course it's
important we want to have perfectly
calibrated things for example here we
have we have the frequency response of
our transducers actually of some other
transducers but but all of them are
roughly the same so you see that at 40
kilohertz they're the most efficient
okay and as you move away from 40
kilohertz they are less and less
efficient so we want to equalize this we
want to because we assume in beamforming
that they have a flat response right so
we need to push the frequencies away
from 40 kilohertz means to push them up
okay so things to calibrate our face
amplitude and polar response so we
should also measure the directivity
patterns of this of this these small
devices if we want to do a proper
complete job
so the way I did this is in the anechoic
chamber here and you know I was sort of
pointing I have a laser here I'm sure
he's sorry I stuck a laser here I was
pointing this laser at the microphone to
sort of ensure that I'm really looking
at the main response axis of this arrays
I was doing it many times unfortunately
you know I was only able to properly
calibrate the loudspeaker array there is
an issue of repeatability and I couldn't
figure out why but especially with
microphone arrays there directivity
pattern is not at all only directional
so small small tilts of these things
resulting in completely different gains
so we had to use some sort of overall
good gain okay now enough enough of that
so let me just show you the the actual
correction filter
how they look like for the loudspeaker
race just to show you that they make
sense that they look like something that
we expected so on the left hand side you
see um you see the correction to the
amplitudes that we have to make so you
see at 40 kilohertz at 40 kilohertz
they're very efficient so we don't need
to do anything away from 40 kilohertz we
have to amplify them okay and you see
that phases are fairly reasonable here
you also see another another thing you
see the two of the transducers are
worried silent for whatever reason so we
really didn't have time to debug this
piece of hardware anymore so we just
said okay we're gonna amplify them
digitally so you see this these two
transducers we have to push much harder
than the other ones in order to perform
the same okay so to highlight the
importance of proper calibration and
just show you an example of a
loudspeaker beam with and without
calibration and just this morning I
realized what I'm showing here is
actually the microphone beam with a
loudspeaker correction filters message
is the same don't worry so on the left
you have you have the calibrated beam
which I already had shown you and on the
right you have an uncalibrated beam
perhaps this this image shows it better
right so the uncalibrated beam looks
weird and even if it's more beautiful
than the one on the left hand side we
actually want to have the one on the
left-hand side right and you can see
actually this these side lobes are most
likely the microphones that we have to
push harder
okay have to do we have to do something
with them okay
so now that I'm done start off with
hardware let's go to algorithms okay so
so okay first I want to explain how in
general you create ultrasonic images
right what do you do so I already was
talking a bit about B mode imaging where
you serve emit sound and then plot
reflection intensity across time along
all the time axis so in every mode of
ultrasonic images you emit some pulse
okay and then you measure some
parameters of the returned pulse and
depending on which parameters of the
returned pulse you measure you get
different sort of modes of imaging so B
mode I already discussed okay so in air
when you use ultrasound B mode imaging
makes limited sense because nothing is
getting retransmitted if I get a
reflection from that wall I should not
hope to get the reflection
from the wall in other room the way it
works in your body okay so it makes much
more sense to do something that we call
intensity image so just find the biggest
the returned pulse from each direction
and plot its intensity against angle so
this will be some somehow and intensity
image and then in depth imaging we're
not interested in the amplitude of the
pulse we're actually interested in in
the timing of the pulse or some temporal
temporal property of the pulse so when
the pulse comes back okay that's how we
create depth images okay and so these
are somehow different approaches I'm
going to quickly go over them and some
problems and and then you know I'm going
to tell you how how we solve them so
naive approach is just just a raster
scan so to do exactly what water lidar
does for example a raster scan pixel by
pixel find the time of flight in this
direction finder time of flight this
direction plot the times of flights and
you get your image right so the problem
with this is you have seen the beam and
in reality it's worse so this is not
gonna work very well right I'm not gonna
be getting time of flight from this
direction I'm actually going to be
getting something that's coming from the
side lobe most of the time okay I'm
going to come come back to this later so
we sort of proposed some some some
techniques Hoffer deconvolution I
explained how this is different from the
economic and intensity image and and you
know how we can deal with this so then
then I'll propose another method based
on sound source localization so how we
can couple sound source localization
with beamforming to get to get slightly
better images to gets actually usable
depth call unquote images and I'll tell
you how to improve the frame rate so
frame rate is one of the most annoying
issues so I'll explain you how to
improve the frame rate from the naive
approach where the frame rate is
completely unacceptable to something
that's completely accepted like 30
frames per second while exploiting the
whole you know every choice you so that
you actually have okay so before moving
on I'll just briefly go over the choice
of the pulse that we have this is a
topic in its own let's use chirps we
don't use chirps bets are much smarter
so they use chirps we use something
that's quite picky and it's like a
filtered Dirac pulse okay so it's it's
just a complete husband pulse between 38
kilohertz and 42 kilohertz
Levy use I could have plotted
the correlation of this thing so that
you understand how our image when we you
know when we try to estimate the time
delay it looks like but the
autocorrelation of this thing is
actually this thing so it was not
necessary right because you see you see
how the spectrum looks like so this is
just a filter Dirac and here's the
formula it's a difference of those
things okay so so this pulse turned out
to be very good for detecting
reflections when I come back
but perhaps you know you'd want to take
some more time to actually design the
perfect pulse okay so let me explain a
bit about the convolution and why we
have to think of different ways to do
the convolution than conventionally is
done in in ultrasonic imaging so
typically the convolution is done for
for intensity images and so now imagine
that you have you know that wall over
there is the source of noise is it no
it's noise noise generator and different
parts of this wall have generally
different different amounts of noise and
so imagine that the wall is created by
very small pixels of noise something
small is that you can detect well you
know then I direct my beam former
towards certain part of that wall and
I'm not able to listen to to individual
pixels what i belissa nning to when i
direct the beam former in some direction
I'll be listening to weighted sum of all
these pixels I was trying to represent
this here mildly mildly successfully so
you get to weighted sum of this
intensity across the beam shape okay so
this is actually convolution some
convolution either you know spatially
variant especially in variant with the
beam shape and this is a linear equation
so the convolution here amounts to
inverting a linear system then you can
talk about conditioning you can talk
about do you really know the beam shape
and things like that but basically it's
inverting a linear system that's that's
the convolution in intensity images
right for us we sort of these small
beams are supposed to represent this
individual pixels so how what is the
image creation model that we have in
depth imaging well you know imagine that
you have that you have you know that
each small reflector here each pixel
emits something when you you know
reflects a part of your sound back and
this sound with the reflected sound I
call X of data so for different
directions
and tea so this is a function of time
then again this gets convolved with the
beam shape because you're not listening
just a particular pixel you're actually
listening to a weighted sum of pixels so
there's a convolution with beam shape
then what we do is we cross correlate
this with our pulse template in order to
find Peaks right so you know I just
wrote it as a convolution with some
pulse templates you and then we actually
create the pixel by finding the maximum
by finding the largest peak okay so this
is the time okay the time of the largest
peak so this is this is our depth right
so obviously this is not a linear model
because there's a maximization inside
okay we cannot write this out as a
matrix multiplication of some true depth
image with some beam shape our
measurements are not linear okay and so
there's no hope to just do very simple
the convolution as is the case in the
linear you know in the intensity images
hope this is you know partially clear so
because the signal I mean because you
will not actually get you know so what
if you have what you have a continuous
surface so not not discrete discrete
distances in your then you'll get some
sort of a smeared pulse right so so
that's a good idea and actually I'm
using this idea and something else with
source localization but the thing is
that you you know you're not really
getting the correct if you have a very
wide beam you'll get some weighted sum
of these pulses that will overlap in
time okay so it's the question of how to
how to and disentangle these pulses you
know this answer your question
you'll not get discreet you know this
most of the time you will not get
discreet separated pulses there will be
overlapping if the surface is continuous
Samsung somehow trying to you know give
it the complete creation oh okay and
well you know for for for what it is
first part of the image creation model
in this case is it's definitely it's
definitely convolution so if you at
certain time you know the output of the
beam former at a certain time is
actually some combination of the outputs
of this perfect beam formers or you're
you're you know the reflections that you
would be getting from the smallest
pixels so by X here I denote these
smallest pixels and you get actually
some for every time you get some
combination of them partial I here
represents all all the pixels that
contribute to to the ice beam as just a
shorthand notation for that so this
again is some kind of a convolution it's
a combo it's it is the convolution right
it's a two-dimensional convolution and
you can write it out is a matrix
multiplication with some toe plates like
matrix a here so seen this since this is
correct for for every time instant since
this is correct for every time instant
we can sample the left hand side on the
right hand side and we obtain a matrix
equation like this one here now observe
that this matrix equation has some
vertical structure determined by the
convolution so vertically things on the
left are some convolutions of things on
the right with the beam but what we
don't exploit here quiet yet is the fact
that this matrix X has horizontal
structure okay and this horizontal
structure is the structure of the pulse
so let me explain this if you meet the
pulse and you have just one reflector in
your scene one point reflecting what
you'll be getting back is the same pulse
again at least in shape you know forget
now filtering but roughly what what will
come back is the same waveform
attenuated and delayed right so this
means that really every row of X here
has to be an attenuated and delayed copy
of the pulse okay
I'll be positive emitted so this is we
can enforce this in our modeling by
representing sorry this should be D here
the by representing X as a
multiplication of a selection matrix of
a sort of delay selection matrix and the
pulse dictionary so D here is a pulse
dictionary every row of these is a
different shift of our pulse and then
matrix B just X so as to select the
proper shift for each purge individual
pixel these ones are just representing
non zeros so these obviously very sparse
and and and we know we know exactly how
sparse it should be ideally it should be
having only one nonzero per row because
each smallest each infinitesimal pixel
only only gives one single reflection
and so the way to obtain the true B
which is then representation of our
depth image is by trying to minimize the
vectorized you know it's like compressed
sensing like trying to minimize the
vectorized one norm of our B matrix just
trying to minimize the number of
non-zeros while ensuring that what it
yields is not too far away from what we
measured okay and then you can have some
non negativity constraints so so one
thing that you could you could try doing
is you could try to eliminate the
dictionary from this matrix by
multiplying everything from the right by
the pseudo inverse of the dictionary
this will generally perform worse
because the conditioning of this
dictionary is not very good and so this
will amplify the noise and you get the
worst equation but an advantage of this
thing is that it's now separable here B
is in the in the middle of the two
matrices here B is sort of alone and you
can solve this column by column so so it
has potential to work faster so it's
just briefly some simulation results so
for this I only have simulation results
the reason is we haven't been able to
really measure so far the exact shape of
the beam and this requires it not
perfectly but at least you know to some
extent so this is some artificial
definitely that created low resolution
because this thing is for now slow and
this is the mass that I was using so I
convolve this thing with this mask and I
generate temporal signals using some
pulse that I generated okay and I get a
lot of noise a lot of noise to this
pulse and then I run the algorithm and
this is what you get so you can get
second the convolution performance like
this one even with sort of you know 10
dB
SNR's so with a lot of noise which is
much more than you would get be getting
in the break in practice from ultrasonic
images so this is just sort of to show
you that this thing really works works
surprisingly good but unfortunately slow
right and and for real problem sizes
it's still too slow for anything
real-time okay so next problem is of
course these side lobes and the thing
that I was talking talking about before
so say you think you're looking there
okay but your side lobe is looking
towards that wall and you're taking
getting a reflection from there but if
your reflection from there is a thousand
times weaker and your side lobe is only
100 times weaker in this direction
you'll be hearing this wall okay and
it's it's pictorial here so in practice
you know we don't have one
two hundred ratio between the main lobe
and the side lobes so this essentially
means that you don't know what you're
listening to because of the side lobes
there may be nothing there but you'll
get a reflection from here okay hope
this is you know the illustration helps
to understand okay so our proposed
solution is to couple this with source
localization and and to do it in the
following way so what I'll do is I'll
listen to their direction and I'll be
informed in order to get to get sort of
as directional thing as I can in this
direction then I'll find peaks in the
beam form signal and I will go back I'll
show this again I'll go back to two row
signals recorded my microphones and then
I'll feed them into the source
localization algorithm and I'll try to
find out where these reflections are
really coming from okay and if where
these reflections are really coming from
doesn't agree with where I'm looking at
I will not put the pixel there okay so
this is a block diagram of the whole
system I have microphone signals here I
feed them into the beam former but also
into this generalization of music that I
implemented which is just a
generalization of it's like both for fur
as maternal innovation music is a very
well-known source localization algorithm
so you know for example I concentrate on
some frame okay this is one frame this
here highlighted in red is one frame so
it's one direction and I feed this into
the beam former this is the output of
the beam former
okay and in this output I find Peaks
here
one peak but actually I find several
peaks every time okay I find for example
the largest three peaks okay now what i
do is i zoom in to this peak and go back
to two row signals and i feel these row
signals into the source localizer okay
and I found out and I should try to see
if the direction that this refraction is
coming from is agrees with with where I
think I'm looking at okay and I do this
for for many pics inside every inside
every be informed signal this is an
output of the source localizer this is
just sort of the score for each
direction okay for each possible
direction and you can see that for
example in this particular frame there
were two sources active for example I
could have been standing like this and
imaging myself and I got a reflection
from this hand and this hand together
okay this makes sense because they
arrive at the same time but the robot is
deriving from from from different
directions okay what I really do in
practice is for every direction I make a
list of distances so I'm not just saying
okay this is the distance in this
direction I actually make a list of
distances so I have a large list of
distances for every candidate distances
for every direction and in the end I
pick the smallest one because the
smallest distance makes somehow physical
sense you know and it turns out to give
good results okay these are just for fun
some some possible outputs some possible
score maps of this sound source
localization algorithm remember we don't
have so many microphones right we have
eight microphones two further for the
full azimuth or full elevation so that's
why this is not so picky but you can see
that sometimes we have a clear source
clearly one source active sometimes we
have multiple sources in the same frame
which completely makes sense right so
okay I'll show you results for this
later so now I'll move to the next
problem and the next problem is frame
rate so what people do with frame rate
is you know because frame e suffers
because if you wanted to raster scanning
so we want to point the beam at every
possible location then it's just too
slow you know sound shows too slow as I
said and it's going to take too much
time so what people typically do is they
say okay we can do microphone
beamforming
in in the computer we can do it offline
so we'll just use one source
just flesh everything with sound and
recorded microphones did a lot of
microphones and then do microphone
beamforming offline but if the limit on
the framerate is then quite high because
well you just need to send one chirp
sick and then only only do the receive
beam forming so now there is benefit in
using using multiple multiple sources of
sound also I give here an example by my
persons a beer who I mean who do a great
job in imaging with for hairy
microphones but they use this argument
right they say okay we're not going to
do transmitted beam forming because
that's too slow okay so we just use one
source of sound so what we proposed to
solve this problem is something really
simple it's just basic basic properties
of LTI systems I'm going to tell you a
bit about probably why probably people
you know don't do it too too much so so
let's go over over over again over what
microphones here so this is the signal
that microphones what Microsoft here so
let's say that s I here is the signal
emitted by by the ight source okay and
the H I J is a is the channel from the
ight source to the Jade microphone then
for all the microphones of her for all
the sources the Jade microphone just
hears the sum of this of these
convolutions okay that's very simple so
for us every every signal emitted by a
certain source is is a filtered version
of some template pulse that you're
emitting so we can write out this si as
some WI which is the beamforming filter
for this particular beam and this
particular microphone and U is the
template pulse that we're using okay so
we can rewrite this in the frequency
domain just just as multiplication so
now look at this right I mean it has two
distinct parts first part is something
that we know for sure we know the
beamforming filter because we designed
them okay so this is something that we
have in our device store and there is a
part that we have to they have to you
know measure that we have to put
physically into space and then measure
and it's this part here so it's how jate
so light source excites jate microphone
okay but now if we knew this part here
which I denote with our superscripted I
down there if you knew this red part we
could actually create this some offline
you agree there's no reason why
we'll be able to create this some
offline okay so what we can do actually
is we can learn the red parts easily by
firing every transducer individually and
recording ours and then just do the
transmitting forming offline you agree
with me it's simple linearity and
commutativity there's there's nothing
special to it but you know it allows us
to to to only do eight chirps eight
chirps if you have eight eight sources
and we create every beam computationally
and instead of you know having a frame
rate of one twelfth of a frame per
second we can actually have thirty
frames per second for you know for a
certain resolution so so just why
probably people I mean of course people
know this but why probably they don't
use it too much is that well two reasons
first for lasers there's no thinking
about it
lasers are themselves very directional
and utilizes raster scanning in a
loudspeaker beamforming usually people
want to listen to sound okay so they
reproduce sound for for people to listen
to and then you can't use this you can't
have a loudspeaker area of 2020
loudspeakers and then play each one of
them individually and then say to
someone okay now do the beam falling
mentally in your head
this is this is not good you want to
create a sound picture okay but here we
don't care we're creating an image which
doesn't have anything to do with
listening to the actual sound so we can
we can do this okay this is just the
block diagram so what we do is we play a
delayed version of the pulse so every
loudspeaker filtered by corresponding
correction filters store whatever is
recorded by by microphones in our case
this is 64 signals and then we create
everything from this 64 signals okay
this actually came out from a discussion
within a who suggested this for
deconvolution that is connection layers
also be used for for 4:40 convolution
I'll talk about it in future work so
these are some results brace yourself
and then some future work that I think
you know could be done in order to
really really improve what I'm having
so this is the experimental setup at
some time of the night in the atrium of
our building here you know cleaning
ladies were not very happy that I was
preventing them from from coming here I
was
the experiment in the ATM because with
the current design the beam is still too
wide and the reverberation sort of
doesn't doesn't do doesn't do me well
when I'm in a room so when I have a lot
of reflecting objects in the room
the images are worse so so that's why I
went to the HCM in order to have a
smaller number of reflections I mean the
reflectors are are coming much later
okay so results
I was imaging myself okay and because
this thing I didn't mention something
some somehow the underlining of all of
this is skeletal tracking okay what
we're trying to do with this is we're
trying to see whether we can design it's
an exploratory project whether we can
design a skeletal tracker using
ultrasound instead of light okay and so
I was standing like this in the atrium
with my arms folded I mean next to my
body and this is the image that you get
so first thing remember what you know
some people get with 400 microphones and
I mean they do a really good job right
but you understand that this is actually
not bad it's not bad because you can
actually see that there is a body here
beyond all question of how much my body
reflects ultrasound beyond the fact that
you know not every reflection here is
specular but you can actually see
something okay
so now this is what I'm showing you this
is an intensity image which means that
in every direction you just calculate
the intensity what we want to have our
our depth images so naive way to create
a depth image is to just as I said point
the beam there and measure the time
delay okay and I said you know this is
not going to work because of the side
lobes and indeed this is the depth image
that you get with this naive approach
I'm showing this to you just just for
the drama right so you see everything is
at two meters almost everything because
every time I'm standing there right and
every time some part of the beam some
part of the side lobes will catch my
body and and will create a reflection so
i'll thing that everything is at two
meters after applying the sound source
localizer to this we got a much more
reasonable thing so you see that now
only the pixels that I can trust are
sort of shown and these pixels are at
correct distances and they're sort of
aligned with my body of course this can
be improved further
for work but this is a pretty large
improvement improvement from this okay
so now let's see a more interesting
thing okay and and this this is kind of
cool so what happens if I spread my arms
okay so arms are quite small whatever
and and you know they're not very good
reflectors and the specularity whatnot
so if I spread my arms you can actually
see it and in the in the intensity image
we're just kind of you know I say I had
say like maybe four weeks ago III lost
hopefully is right but now instead of
really happy that I got this image this
means that you can see where the arms
are okay so imagine having much more
microphones imagine having better
algorithm secret you could probably you
know do very well so you can actually
see where the arms are you know again
just for comparison this is without
darkness fold it and this is your
darkness spread okay so you see ours
here and here
depth image again naive approach
everything is you know it's around two
meters not very good if you apply sound
source localization now it's good we got
something like this so you got this
alien-like creature with some
imagination is me
in that spectrum but you know this
actually shows something somebody and
some arms spread so so it's much better
than this it's actually pretty good
because the distances are correct and
the arms are in correct positions
reasons why this works is that you know
on the body you have many kinks
Bartz small small detail that actually
does reflect sound sort of in different
direction so you can hope to find
something around here that will reflect
sound back back to your device okay and
what I'm showing here is the same thing
but with this fast frame sort of
single-shot acquisition with linearity
used against again with my arms folded
this was a different experiment so this
is not the correct the correct photo I
should have put in a different photo
from that experiment but there was no
people in the atrium that could take a
photo of me at that time so I had to I
had to give up but you see that the
image is somewhat worse I would expect
the image to be exactly the same as the
other one unfortunately for some reasons
that they haven't been able to debug yet
the image is slightly worse but still
it's sort of correct right and again
everything is that I was standing a bit
closer at the bit
two meters Indian a naive approach where
I sort of recreate these beams if I
apply sound source localization and you
sort of see sort of see the head which
is from reflector and you see arms this
requires further work requires you know
it's not quite clear to me why why you
know this didn't work as expected but in
theory it should so so this is probably
just a bug okay so these were the
results and I'll go over some some of
the conclusions so what do I have to do
had to design is hardware with a lot of
help from Jason I had to you know this
involved a lot of things like what to
pick how to interface it to a PC how you
know definitely how to how to design the
geometry I still explored various image
creation algorithms and the images that
we have they have three degree angular
resolution you could create finer images
but with current beam shape it doesn't
make much sense right I mean they
wouldn't have much physical meaning
unless you were able to to properly
deconvolve them so three degree angular
resolution is somehow a qualitative good
number four for good number of pixels
okay so I proposed some convex
optimization based and some source
localization based algorithms to improve
to improve the depth images to improve
the actual depth estimation and you know
so one of the major problems fear of
sound they proposed a solution
experiment it still needs some work but
definitely the solution is to to use
multiple transducers but to use these
simple properties of LTI systems that I
can do things offline even in
transmitting okay and so this takes us
from from 1/12 of a frame per second for
the naive approach to 30 frames per
second for eight strands users for fir
this approach with beam for transmitted
beam forming online and the good thing
about is a great thing about this is
that this doesn't scale right the time
if you fire only eight strands users and
get more sophisticated algorithms you'll
still need exactly a one thirtieth of a
second to acquire the frame because you
only need to fire rate transducers so it
doesn't scale see what I'm trying to say
so you can have very sophisticated
algorithms after this but the image
acquiring is going to be very very fast
ok so the limit is sky major advantage
of this thing is low power
sumption again you know lays things
operating lasers they use a lot of power
okay it does depend on the number of
pixels but typically they use a lot of
power so in this regard our prototype
uses three watts but this is this is a
lot and this is because of the nature of
the operational amplifiers that we're
using the quiz and currents are quite
high but if you do the design
specifically for these transducers you
could achieve something like 448
transducers at once like one what in
pulse okay and perhaps 10% of duty cycle
we're not using them all the time this
is also maybe 100 milliwatts of the
power consumption then if you go back
here and you do a single shot
acquisition when only one transducer is
active at the time you could actually
you know perhaps divide this by 5 so get
something like 20 milli watts which is
really I mean which is a huge difference
if you compare it to it with what what
you need for lasers microphones
virtually don't don't need any any power
ok answer the question somehow you know
that I forgot I didn't highlight enough
in in the beginning of the talk is it's
like ultrasound for for skeletal
tracking well I think that probably the
answer is yes if use higher frequency
and and you play with all groups you
have proper calibration I'm pretty sure
that you know you can extract enough
meaningful features about positions of
limbs in order to create to create a
skeletal tracker and so let me go just
quickly you know to some possible future
directions so definitely there is value
in B mode or intensity images so perhaps
you know we could use these images to
maybe detect connected regions or
something like that right and then use
them to enhance the depth images okay so
there would be value in in combining the
two information to build like a
reasonable skeletal tracker then for
free actual skeletal tracking well what
what is the information that we use the
information that we use is 64 signals
that you recorded okay so perhaps you
know the important features are times
and intensities in every temporal bin of
these 64 signals that's what matters so
what you could do is you could divide
these 64 signals into bins compute the
average intensity over every bin and
feed these as features into
training of the skeletal tracker right
you see what I'm trying to say so you
could not create the depth image at all
you could completely bypass the depth
image and just get the skeletal tracking
from these features which directly
encode actually the positions of objects
in the scene okay we could if we can
formulate the convolution that I was
proposing directly in this in the in the
signal domain never creating beams okay
so it can also form you this the
convolution starting from from 8x8
signals that we have similarly for the
other thing and of course that didn't
you know the most mundane idea that that
we have is increase the number of
microphones okay so nowadays the guys
pretty villa the paper one of the papers
that have shown in one of the first
slides they use something called
capacitive micromachined transducers and
these things you can fit like for
example they fit 37 microphones on on a
PCB 6.5 by six point five millimeters
it's quite fascinating problem currently
is that they work at slightly higher
frequencies so their range is limited
but I think in very foreseeable future
these are this is going to be the
technology of choice for ultrasonic
imaging in air okay
so just just as an example like what
what could you do with it what could we
do if you had 64 microphones okay so
this would be the beam shape right and
this is the beam shape that you kept
that we have currently okay so obviously
virtually no side lobes it's still quiet
I mean it's still not the point right
it's still some parts to some piece of a
beam but it's much better than what we
currently have so many of the problems
will be mitigated me to get it with
having more microphones okay uh well
that's roughly everything that I had to
say thanks for for listening if you had
any questions please go ahead
you're trying to
see yes I mean no it's obviously wrong
answer right so it can't get worse so it
will it would get better because you'd
have some smoothing over time so so you
could probably use this also to to get
to get higher resolution here's a band
from 38 kilo Hertz support to do do you
think it's possible to shift that I make
that being a little smaller
all worthless on the first frame is to
from $38 239th works on the second frame
which at 39 to 40 something looks like
me but it's not exactly sure but it's I
mean we were thinking about things like
that so one one thing that you could do
for sure is you know use multiple
frequencies not not like 38 39 but try
to use 40 kilohertz and maybe 80 kilo
Hertz or 48 and 73 I don't know and then
this would be really useful to remove
for example coherent speckle or some
effects that that are the artifact of
the actual frequency that you're using
the thing that you're suggesting would
be great also for example for for
improving the framerate further in a
following sense if you could use this
kind of OFDM you know idea if you could
use separate bands you could actually
emit you know one pulse so one beam in
one part of the spectrum other beam in
other part of the spectrum and you know
other beam in the third part of the
spectrum all at the same time okay so
that's a good idea but it requires a lot
a lot more design and you need a
slightly more wide band so you know had
a lot of trouble with releasing with
repeatability of of these transducers
okay sometimes you know they don't work
as expected so you'll need a more
reliable source of ultrasound that you
can and that's more wideband okay
because these things are very narrow
band it's it's 40 kilohertz
and a bit a bit aside so we need to push
things a lot right maybe you'd like to
have something that has a much much
wider bandwidth and these things exist
no questions depression section is first
and in terms of what exploit absolute
most interested in a dynamic interesting
loosen question
so can you ex white so you can you
explain the difference for friend
so you should have just one friend
answer or I don't even care what's here
the mag's about what's different
is a little part and then if you know
what which reports different that's
their purpose you see yeah I think
that's a good idea it's actually now
that you're saying this this kind of
reminds me of a you know maybe someone
is familiar with Facebook older so you
know or something like I see you trying
to determine a real frequency you have
an f50 of something and you're trying to
deter determine the real frequency right
so the way to do it is to take two
consecutive frames and see the phase
change and interpolate the frequency
right so and then you you know you're
not limited to the bin anymore user of
can can get the real frequency so yeah I
think I think this could be really
useful because maybe something is also
intensity image could be austere that's
right because some intensities would
slightly go down some intensity would
slightly go up so we could probably
interpolate you know play on these two
to two to get much better image
so but due to which have that's actually
not true
but you know it's well if you really
have said that you really have a really
narrow beam you know that you can really
listen to this particular pixel then
what would have to happen is that you
know you have a multipath component that
comes from the exact same direction and
this is very unlikely to happen well
because you know it's probability zero
I'm you know so in the limit case where
you have infinitesimal pixel if you're
riesling exactly to this direction that
we have also a multipath component
coming from exactly this direction is
probability zero a lot of a lot of like
random things should align in order to
have you know a couple of reflections
and then suddenly reflection again from
from from there right see the time you
know like if any decimal pixels for me
are
haha okay first I didn't understand your
question and uh yeah I see I see what
you say okay so for example if I have a
mirror how will I know that it's that
it's a mirror but it's not something
over there that's a good question and
the answer is there's you know without
further information there's no way to
tell it right so if you're just using
this information there's no way to tell
you should be able to estimate the
positions of such big reflectors one way
to do it is you know it's probably too
slow but since this is all modeled by
image sources actually one way to do to
do it would be to try to combine pairs
of pairs of pixels to get the third
pixel pairs up how to say this okay so
this reflection from that wall will also
have a corresponding object there right
so they have they have a clear
geometrical relationship so the object
that creates the reflection will also be
hopefully inside the scene okay so since
this object you have seen this object
and you have seen its reflection but
maybe you know that there is a wall here
there is a second this geometrical
relationship between the reflection and
the real object and you can detect the
reflection okay by exploiting this image
source relationships you can say okay ah
this guy is actually a reflection of
this guy I'll just burn it out let that
would work but it's probably too slow
because you need to combine pairs of
let's one one one idea on how to remove
this thing
more questions reflections did you
consider treating him to filter design
problem whereby at the expense of
increasing the size of the main lobes
you could suppress the side the side
lobes reduce that reduce the probability
of reflection for tesota but at the same
time reduce the resolution yeah I
actually have it I mean that was
initially one of the ideas serve design
the beam beam pattern then later we
didn't really do it
reasons are that we hope that the side
lobes are not gonna be so loud and then
later I mean I just - just forgot about
it then you know the domain may lobe is
still quite quite wide and if we did it
like that then it would be even wider
now considerably wider probably so it
would be beneficial but okay so so the
main problem with with side lobes is
with specular reflection right so I'm
trying to get something from there and
say that I'm hoping I don't know why but
I'm hoping for a small diffuse
reflection from something over there
okay and here I'm getting a strong
refraction so this diffuse reflection
before example one thousand times weaker
than a strong specular reflection from
there okay and I doubt that especially
with eight microphones you can create a
beam former that will suppress
everything but the main lobe more than a
thousand times and I'm sure that you
can't so you see what I'm saying like
the problem is that if there is a strong
reflector in the scene and even if
you're shooting just just a bit of sound
or listening just slightly to this
direction you'll hear everything coming
from this strong reflector you know it's
like that one reflector in the scene has
a reflectivity that's a thousand times
stronger than the other thing is in the
scene so maybe some fabric and then you
know a piece of wood I'm just talking
gibberish but then you know everything
would seem to be coming from this piece
of wood I'm not sure if I'm clear but
practically displace a in a living room
whereas there are reflected to expect
for it to be they expect to have tables
for two ceilings and things did you have
a calibration procedure that there's a
preservative where they're likely to be
make sure that the nulls of the Cyclones
are pointing in the directions we have
seven in the series yeah I didn't think
of that and and this is simply seems
like a like a really practical and good
idea yeah why not so yeah you just sort
of move out of the way calibrate today
and then go in and use it yeah this is
actually good especially with more
microphones and more margin sisters you
could certainly play on that team MVP on
says if you if you walk out then your
your noise is your I want to see which
one your honor reflecting mm-hm you
first try to minimize that and you can
such a way such that when you actually
come back in then the only thing that's
in the room is the one to signal that
that's gonna be you yeah I like this it
yeah I think this could be really good
course that would be something that you
would do so there is benefit in having
there is benefit in having more than one
transducer and the benefit is the
following you can you can serve
partially isolate regions of space which
is good for some source localization I
can elaborate on that later but
definitely what you would do is you
would not increase the number of
transistors too much but you would
increase a lot the number of microphones
more questions okay let's give a tax</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>