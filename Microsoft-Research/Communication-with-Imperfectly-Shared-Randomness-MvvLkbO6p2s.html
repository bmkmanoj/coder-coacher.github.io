<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Communication with Imperfectly Shared Randomness | Coder Coacher - Coaching Coders</title><meta content="Communication with Imperfectly Shared Randomness - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Communication with Imperfectly Shared Randomness</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MvvLkbO6p2s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I'm very happy to be here last time I
visited a Microsoft Research Lab other
than New England I didn't have such a
nice experience and you'll find out a
little bit more shortly so this is
imperfectly shared randomness I'll tell
you what it means in communication this
is joint work with Clement cannon who
was a intern at New England this summer
and it's one of these really amazingly
compact projects we never had an idea
that there was a question out there till
he came started thinking about this
thing then we started thinking about
some question and sing like a nice
question but we weren't very sure then
we started talking to venkat guru Swami
who was a visitor in the lab for the
full semester and Raghu Mecca who's now
unaffiliated unfortunately but it was
from this who was at the Silicon Valley
lab in those days and he suddenly sort
of triggered some sequence of Cotts and
it led to something which i consider
quite pretty mathematically this will be
a pretty mathematically intensive talk
where there's a few slides of general
motivation but after that it's a pure
core theory talk ok and so of course i
do want to dedicate the stock towards
svz colleagues so in addition to the
fact that this was joint work with Raghu
who was who was at the Silicon Valley
lab the first time I was planning to
present this talk was to them and this
was on the day they were all told that
they were all laid off and in a really
shockingly sad move and furthermore I
mean you know when I was the dis entire
talk by the way I mean after I heard the
news I didn't have I just didn't want to
look at the slides I looked at it today
in the plane I just desire to add this
one slide otherwise it's really aimed at
that audience I was thinking of members
in that group and saying you know what
would I want to say to this person or
that person that's what this stock was
constructed so I'm sorry this fit ends
up being inappropriately targeted today
but I didn't really have the heart to
change it since then ok so that's that
being said let me move on to the subject
of the talk the
underlying the broader picture issue
that I want to address I'll be
addressing one particular very specific
aspect of it but in general what I want
to talk about its context in
communication so context is a central
element of communication this is what is
assumed to be shared between sender and
receiver before any of the communication
questions come along and it's usually
implicit you never mention it you never
talk about how large is it how important
is that how you know how much of it is
perfectly shared or not and it's in
particular when we talk about complexity
of some communication task it doesn't
participate in the input parameter n
okay and you would just say look let's
ignore all of that talk about the real
new input which is really what one of
the two players has and the other
doesn't and see how much time it takes
to get this part of the message across
or some functional aspect of it across
but it is always sitting in the
background okay so when so let me start
with a few examples so when you say you
know I'm talking to you with a sequence
of letters over the English alphabet or
some such thing there's always an
Associated meaning to this thing but we
never talked about how do you
communicate that meaning we only talked
about how do you communicate the bits
that we want to communicate in the
theory of Shannon for example when you
say look you know here is how you do
source coding you know I have sender is
getting messages from some distribution
wants to compress it so that the
receiver can figure it out you don't
usually talk about the distribution
itself you just assume the sender the
receiver know it or work around it the
behavior of the channel should it be
specified explicitly as part of the
input or not we usually don't we just
say let's assume it's known that we
build the codes the codes that are used
either for source coding or error
correcting codes we never discuss it
it's already in the background somewhere
now that they've been fixed how do we do
the next step is usually what we talked
about but these are the implied context
in communication complexity in the model
of Yao where I have some input X in my
head you have some input Y in your head
and we want to compute some joint fun
we could ask you know do I really know
want to compute the same function as you
and do we know about it each other okay
so that's you know these functions that
are being computed are they really known
and if not how are they specified should
it they play a role in the communication
complexity the randomness that's shared
between the communicating players is it
you know we just say oh there's plenty
of lot of distance available but you
know it's really part of the context in
human communication going back to this
place bullet in the sense issues like
the language that we use the grammar
that this that these are all part of the
context they give you know if the
context did not exist in a sense we
wouldn't be doing any communication it's
2x it's to really evolve this context if
you really do the communication but we
never talked about it explicitly and it
the one of the sort of things about the
context that makes it important that we
should stress it explicitly is the fact
that actually context tends to be large
so in our communication today when I'm
speaking to you our shared context is
the knowledge of English all the
knowledge of mathematics that I rely on
maybe words like Shannon and Yahoo
communication complexity etc all of
these things buy shares of meaning and
I'm counting on all of that in order to
be able to deliver a message but the
amount of context that I really count on
I cannot even sort of start to write it
down in some finite amount of space I
mean that's just huge material you say
what do you mean by shadows and I'll
start writing pages of stuff and then
when inside that page you point to some
word and you say what do you mean by
that then there is a further elaboration
this amount of context just keeps
exploding honest it's large and at the
end by the way I don't need you to know
this precisely and I suspect when you
read about divergence you called it this
and I called it that it by the way you
always measure the divergence of P with
respect to Q and I venture measure
divergence of Q with respect to P etc so
all these things so many things could
change but really nothing will change
because of all of these things changing
social context is large about but it's
shared in some very imperfect way
and yet we try to build up the shared
context we communicate so this is a very
interesting phenomenon in natural
communication and we want to talk about
it in a thing so I mean you know when
you go to any of these online forms
there's always some shared contacts
there please I take a pin love which one
of pins you gave me three different pins
I'm not going to start entering them
each one at a time when I usual
violation of security baby but you know
and you go to these forms there will be
so many things and say please enter the
image that you you know the numbers that
you see in the image but there's a
number here there's a number there
should i put this one first or that one
first no no it's like you know we never
tell you the rules if I see you know
tell me what's the sequence of letters
that you see we somehow we managed to
figure out what they might have st asked
for us far and we estimated that we give
an answer according to that we never
laid out in words if we'd had to it will
take you know each one of these
instructions will be longer than the 34
screen folds so in compression this is
an example that we will come to again
when I want to compress information just
think about one short communication I
have a message and i want to send it to
you and I know something about the
underlying distribution we can compress
it if the distribution is known to be a
do but how do we know that we are
working with exactly the same
distribution so this is a very you know
so do they always have to agree
perfectly on the prior distribution or
can compression work if I estimate that
the distribution of messages is
something you estimated it as something
else and we never agreed on the
disagreement on how much we disagree and
we're at what good thing so you would
work there so I did some work with
Brendan Juba Adam kalai and said you can
learn this in few years back and then
later with her amethi and I'll tell you
a little bit more about what we learnt
about this shortly and then let's talk
what I want to focus on is another one
of these things so whenever you're
talking when you you're trying to
execute some protocol this communication
complexity model and talk about it pre
next the things can become much
quicker if you are allowed to share some
randomness between the sender and the
receiver and fairly natural question is
you know should this randomness you know
be shared perfectly or not and today's
talk is really going to be about that
particular question for the most part
okay all right so shit I will tell you
about communication complexity I didn't
draw a very detailed description formal
description of what communication
complexity is for the most part we won't
need it I'll sort of skim over the issue
but we'll do it by an example here is
the most sort of the first question that
anyone is probably exposed to in
communication complexity is the
following Alice has some n bit string X
in her head ba BOM has some string Y in
his head and they want to exchange
messages till they can determine whether
x equals y or not privacy is not a
concern here they don't mind if Alice
doesn't mind if Bob finds out X and Bob
doesn't mind if Alice finds out why all
they want to know is is X equal to Y and
they want to exchange as many bits of
information as I needed to be able to
determine this but of course they want
to minimize the communication
deterministically it's well known that
you must communicate at least n bits
this is really actually n plus 1 I think
is the correct number that can be
achieved trivially Alice sends X to Bob
but Bob sets back a bit saying yes it's
equal or no it's not that's n plus 1
bits of communication that's the best
you can do if Alice and Bob try to be
deterministic if on the other hand they
are willing to be randomized Alice may
be able to toss some random coins and
then communicate something to Bob and
Bob may be able to again toss some coins
in the communicate back etc etc and they
get a log in bits determined with fairly
good accuracy whether x equals y or not
if X were equal to y they would probably
come back saying yes and if X was not
equal to y they'll probably come back
saying no the probability is purely over
there coins that they are tossing this
is what is called private right of this
in case you haven't seen this this is a
very simple protocol which does this
Alice takes her input X and encodes in
some air
pincode some code where any two code
words will be back to strings which are
different in ten percent of the
coordinates or something like that and
then what she does is picks a random
coordinate I let's say capital L is the
length of this encoding of X she picks a
random coordinate I and says on the i'th
coordinate my encoding has zero or on
the i'th coordinate my encoding has one
now by the very definition of an error
correcting code for a randomly chosen
coordinate encoding affects an encoding
of why would be different with
probability 1 10th or whatever and this
is going to get constant error
probability and basically just checks to
see if at the if' location encoding of y
equals the encoding of X that's
determines this verdict this is correct
it has ninety percent error probability
maybe right now you can repeat this a
few times it gets out it's very well the
amount of communication is logarithm of
this capital n but because we have very
good error correcting codes that's the
same as the logarithm of little n plus a
constant in fact ok so this is a very
very good communication protocol so this
is what happens with private randomness
blow we can ask what happens if you if
they share some random string Alice has
some random string in her head and it's
equal to the string that bob has in his
head this random independent of x and y
what could they do well then you can
interpret that random element that they
have as this index I and now Alice
doesn't have to communicate the index I
she just communicates the encoding of X
of the location I that's one bit of
information and this is giving you
constant confidence in the answer ok so
with a constant number of bits with
shared randomness they can actually
determine if x equals y so the model of
this story is if you have shared right
of this communication complexity can
drop dramatically with without any
randomness you have n bits with
randomness which is tossed privately you
have login with in width randomness that
is
you know shared between sender and
receiver its constant number of bits and
this is the best trade-offs that are
possible since the gap between this and
this is never more than a relative login
and this is already achieved over here
at the most dramatic of ways without
private random this login sorry with
private randomness login bits but with
shared randomness constant bits okay so
this is an interesting example we'll
come back to it later this is one
example where shade randomness helps a
lot in communication complexity but this
was not our motivating example our
motivating example was really the
example of compression that I talked
about earlier and that example let me
try to tell you what was the
mathematical question there so we want
to do compression of information when
the priors are not fixed and agreed upon
so how do you model that problem where
you model the problem is alice is going
to get a message m from a universe of
size capital n and it's drawn according
to some distribution P and let's say
this distribution P is known to Alice ok
so p sub i is the probability with which
M will equal I alright and so she gets
the distribution explicitly now putting
this this used to be typical compression
context you would never venture the
distribution is input to the problem but
here it's an input to the problem this
is the context and it's an input bob has
a distribution q which is close to the
distribution P okay what do we mean by
close there are many measures of it
distance that could make sense here the
one that we chose because it was the
easiest to work with was on every
coordinate p i and q ir off by a
multiplicative factor of 2 to the Delta
where Delta is some number and that
measures the distance between P and Q
okay so P I / qi is there were less than
2 to the minus Delta it's never more
than two to the Delta that's the measure
of distance so in particular if Delta
we're equal to 0 then p i equals Q I
this is the classical communication
problem what can you do well
p equals Q the expected communication
that you get is the entropy of this
distribution P this is what is achieved
by Huffman scoring algorithm okay the
very simple Huffman coding thing that
you might have seen achieves entropy of
PMI number of bits between so Alice will
send entropy of P many bits in
expectation meaning when M is drawn from
this distribution and after that many
bits of communication Bob will be able
to guess m he'll reconstruct it from the
message ok since now what we were able
to show the paper with Juba hello in
July is that when P is approximately
equal to Q then by the way the standard
compression records do not work Huffman
doesn't work or if my decoding doesn't
work etcetera etcetera etcetera and
quite often people ask me you know isn't
that somehow or the other implicit in
the literature that you should have been
able to do something reasonable in p
equals is approximately equal to Q well
the only way I know to do something
reasonable is when there is shared
randomness between sender and receiver
then we can achieve entropy of P plus 2
Delta so if the distributions are very
close then you communicate almost the
entropy of information and as the
distributions work further and further
and further apart you have to
communicate a bit more but still it's a
very manageable tolerable price okay so
you can get this but you'd eat or he's
this solution needed shared randomness
we try to explore this question can we
you know it's a this whole motivation
for this question was excused Alice and
Bob do not share their context perfectly
so p is not equal to Q and they are
trying to overcome this lack of shared
context and to say that they can solve
this by having shared perfect randomness
seems to be like the really wrong
solution for the problem right why solve
it with you know zooming X can be shared
perfectly when you know what was your
bike it's not shit is sure perfect
excuse me so we try to say well maybe we
could try to do this get this resulted
to realistically it's so far the best
answer we have is okay entropy of P plus
Delta but then there's another log log
in coming in that's the mrn is the size
of the universe just to see how bad is
this number if i put in a log in over
here you should have been scoffing at
this result why because I have a message
from a universe of size n I can just
ignore the distribution and send you a
login bit string that that would just
tell you what the messages right so
getting log in here would have been a
triviality okay so we get sort of a
logarithmic last compared to the trivial
solution but really when you talk about
enterpise of distributions we really are
thinking of distributions of messages
which may not even be on a finite
support okay so a log log n when n is
infinite it's not a very good answer you
really I mean the nice thing about
entropy is it was a clean answered said
measure the uncertainty the random
variable and that tells you how much you
have to communicate is this still the
right answer for compression if you are
really compressing with uncertain priors
in compression with uncertain price is
really much more natural when people are
talking to each other so for example it
shattered one of channels original
papers he sort of measures the entropy
of English or some such thing okay so
you can apply this in natural context
you can certainly measure the amount of
information in the natural communication
but really is the compression length
measuring the same thing as a the
uncertainty is compression rings really
bounded by the uncertainty it's not
clear in this natural context not if
there is a log log n I wouldn't
you had a digitally of words history of
the alphabet and there are some
differences some decals are missing or
some parsley or right exactly so okay
think of Delta as given a word you say
you know what is the meaning that I
would associate with it so sometimes it
could mean some you know words which
have multiple meanings okay most words
tend to have you know several different
meanings depending on the context or
what is the context in which you are
planning to apply it might be another
thing you have many different you have
sort of a choice of what it could have
meant and you could say you know but you
know suppose I say yes to you send me
some email I said you in response say
yes but you sent me three emails in the
last day and i sent a yes which email
did i respond to you sort of make a
guess about it and i might make an
estimate saying look obviously you know
I ignored your last two emails and i
responded to this one within three
seconds of it you would assume that i am
responding to the latest email but on
the other hand you know from your point
of view you might have seen the message
after you know my response three hours
later and you might say well you know i
send that message yesterday this one
today and I got the spawns day after
tomorrow so could have applied equally
well to the two messages so the this
sort of where you give over a particular
phrase and English or some such thing
and the meaning that you associate to it
in the context in the context of context
there's a certain amount if there's a
probability you will associate with it
saying this is what I think you meant
this is what do you think you meant and
these probabilities may be different for
the sender of the receiver does that
make some sense
the sender of the sea would have a shape
you study right okay good huh let's
check it because right so that the
dictionary was a solution to the as far
as I know that you cannot tolerate even
the slightest missing the static so for
instance you know you might have said
look we've been communicating for many
days I'll be in applying level serve and
is building a dictionary online for what
should be the best compression between
the two of us it may be that your
dictionary missed out one email that I
said but my dictionary kept it in the
thing our dictionary has evolved in two
different ways from this point on our
communication could be completely bad
okay so as far as I know there's no
soundness associated with this kind of a
process so you don't usually associate
it with the most natural compression
schemes because they just do not
tolerate p is different from q you can't
associate it with more of human
communication and especially human
communications the sense of what did I
mean to apply that particular word to
their there is a lot of probabilities
and probability batching is going on the
video question all right so brave urban
route talked about a very similar
setting I don't think their goal was to
so they had P and Q is way different
their goal was to sample from
distributions p and q with probability
sort of roughly matching p and q i think
something along those lines the one
thing that I recall over there is it's
an interactive setting so they in this
case we are looking at a one-way setting
for short Alison's a message so we don't
have the ability to say well we look at
the divergent bnq and try to attain that
so here this is more or less the right
measure of distance that we had and as
far as I know there was no thing saying
alice is going to deliver a message to
Bob but rather this is how they would
sample from some distribution together I
think
it's only from Alice to Bob yeah we
really want to think of it as one way
communication okay so this was another
one of the motivating example sorry I'm
getting skin of a slut I'm sorry cap
yeah good good question so so one of the
things that as far as I can see you know
the amount of communication is so little
that you're barely able to get the
message a little bit extra maybe that's
been tossed in but as far as we can see
there's no simple way of going from here
to saying we've learned something about
the difference between Alice and Bob at
their distribution and at least is the
knowledge tract ascetic nothing really
happens so there was a subsequent work I
don't know if it was published ever by
Jumba and comments who looked at this as
they said well you know what happens if
they're communicating a series of
messages at some point you know Bob evil
has enough information to learn Alice's
distribution and so should not even need
to lose this to Delta but the time that
happens is so far into the future like
after almost n messages or some such
thing so you don't really get much
benefit till then it's not like the
second message already you're getting
something so you might be learning
something about the distributions but
not enough really
after a sequence right exactly after so
if you were sampling from this
distribution repeatedly and then sending
this thing you should be able to
advertise out this to Delta after a
while you would not see anything but for
the first 100 messages any constant
number of messages for each one you will
be sending to Delta extra missing bits
okay all right so let me March ahead so
uncertain compression we can say look is
it the right measure of compressibility
with uncertainty it may not be well at
least we don't know how to get this
deterministically randomized solutions
if you are willing to buy that it's
saying that entropy is the right measure
but randomized it this very weird sense
of perfect sharing doesn't look nice so
this really motivated this imperfectly
shade right of this question for us so
what happens if Alice and Bob just like
they don't match on the distribution
also don't bash on the run of this could
they have done the same thing or
something reasonable this is what
motivated our question for us then we
started looking at the literature at
clement went to a conference I kalp in
2014 and he found a paper by bavarian
government ski Anita which actually
studies exactly the same question
imperfectly shared randomness in
communication complexity and what's very
embarrassing why could put independently
in courses that Bavarian is a current
student of mine so it would be the
student for three years so all right so
our I'll mostly stress on the our model
but I tell you a couple of things that
they did and tell you what's different
so we want to study general
communication complexity with
imperfectly shared randomness if you can
see the colors this is and are are
highlighted and i'll be using this
abbreviation in the future so this is
the model of randomness sharing alice
has some string are arbitrarily long
which is a uniform random string ok
every bit as uniform independent of the
previous one bomb has a string s where
in each coordinate our I and si are you
know s is also in the marginal
distribution is uniform and it's just a
noisy version of RI so if
para is plus or minus 1 then si will be
equal to arrive with probability
something so i think we prefer the plus
minus one rotation and here correlations
are measured by just looking at the
expected value of the product of rin SI
and we will just say that that's row if
row is 1 it means RI is always equal to
s I if row is minus 1 RI is always equal
to minus SI and if RI if row 0 it means
these are independent random strings
okay so so we will talk about roco
related pairs and Alice and Bob have an
infinite supply of row correlated pairs
what can you do with them in
communication complexity with this thing
ok this is the question that we are
going to be looking at so for some
function communication complexity
function to be computed F we will say is
our sub row is the communication
complexity with row correlated bits ok
and PSR is what we will be comparing it
with which is the amount of
communication needed for perfectly
shared randomness this is what used to
be also called public randomness or
shade right of this in the past this is
what we will be comparing against and
another benchmark to compare against
this private randomness what if alice is
only allowed to use randomness that she
has and bob is only allowed to use
random is that he has and then how do
things work and just to put all of these
things in a single thing privately
perfectly shared random this is the best
model I mean it's the one which will
give you the middle communication
complexity if you have it and then
imperfectly share none of this should be
somewhere between that and privately
shared out of this you know if if I have
n imperfectly shared random bits with
you I can look at the first n over 2
bits you can look at the second and over
to bits that spread over to privately
private random this so certainly this
inequality holes and same there in fact
the imperfect shade randomness this is
imperfectly shared randomness Withrow
equal to 1 this is it perfectly shared
out of this with rui quill 20 and indeed
it between row 0 at 1 this is a
monotone function okay going the right
direction so it's consistent this
inequalities and finally private right
of this is never a private communication
complexity is never more than the public
of education complexity plus log n okay
this is a a old theorem in the
communication complexity so this
difference will never be more than log
in so this by the way was the reason why
a Bavarian at all actually never really
studied this question they said well
this is not interesting i'm only going
to see a logarithmic additive gap but
from our point of view we were thinking
about say compression login was
something that we were going to be
dismissive wave off trivially anyway so
a gap of log in is a huge gap by our
sense and in general i think whenever
you talk about the shared context the
inputs to problems tend to be large
communication complexity tends to be
small i mean input to my our
communication problem today is all my
life experience and all your life
experience that we have an hour barely
to talk about things okay so
communication complexity is tiny for
whatever task considering the context is
massive so in such settings saying that
look okay i'm going to add another log
of n where n is the size of the context
is huge so you can't refer to do that so
the scaling really has to change the
perspective has to change so we think
this is a big difference so you can't
just dismiss this and say so we really
want to say how much better thing then
this upper bound can you do on
imperfectly shared randomness so here
are some results so bavarian at all they
what they focused on instead because
this thing was so bad was they said okay
look we won't talk about communication
in any of these classical models instead
we will talk about the simultaneous
message model where Alice and Bob have
inputs they compute some sketches of
these inputs and set it to a referee and
the referee outputs the value of the
function in such cases this gap of log n
doesn't hold and so then you know you
get still interesting questions that
they focused on it here our focus is on
one-way communication typically actually
no longer one way we could pretty much
everything that I say holds about to
wake
education but i'll only state the
results for the one-way case maybe i
might say some other things later so
typically i'll focus on the case where
alice ascetics a message to Bob like in
the compression problem that's it and
Bob outputs the value of the function
and we are going to focus on problems
and by the way when we started looking
at this question we came up with some
problems which are not so standard they
were interesting problems as
communication complexity problems of
their own where the difference of log in
is going to be significant so this is so
what are the results so first thing we
went back to our familiar problem
uncertain compression what happens to it
turns out it actually works very nicely
even when you have imperfectly shared
right of this between sender and
receiver and the solution turns out is
not very hard it you know it but it said
it's an interesting solution i'll skip
the slide which talks about it just try
to say a line or two about it now the
usual solution for oh baby let me just
come to that slide will it come I'll
need the help of the slide another very
interesting thing is this was already
discovered by bavarian at all we
discovered it independently and actually
with two different protocols but
equality testing the problem that I
described Alice and Bob have strings x
and y they want to know if x equals y
can be done with constant amount of
causal number of randomness as long as
Rho is strictly greater than zero so you
Alice and Bob do share correlation there
are sharing perfect randomness this is a
protocol that is actually non-trivial to
guess ok you have to think a little bit
about it at least I would say it's not a
direct translation of anything that we
do bar and in fact it took a little
while after we saw this say oh baby
everything you know everything that has
small communication complexity in the
standard the perfect shading randomness
model also has small communication
complexity with imperfectly shared
randomness
and it also turns out to be true but not
in a simple way in general you have
perfect shade under this complexity of
some function is K there is one way
imperfectly shared Adam this
communication complexity is at most 2 to
the K so of course if this is a constant
that's a constant that's what we saw
here but the translation was not trivial
it was not going from k to k or k to
order of k it was k 2 2 to the K that
was what worked it out yes oh no be
resuming Rose a fixed constant between
zero at one that all of these constants
could depend in some ways and road
typically it's not a very bad dependence
but there is a dependence on R 0
everywhere and yeah n is not in the
picture but Rho should not depend on it
so now we when we looked at this result
we were sort of very curious cat we can
we really improve this K or not and the
converse that we found was surprising to
us initially in fact you know between
these two things there was a time when
we were convinced that perfectly shared
randomness and imperfectly shared out of
the survey that causal factors of each
other which was not true and at other
times we were also convinced that
actually there are protocols which just
require unbounded like log in
communication and that was also not true
so and depending on which things we were
thinking about we thought the upper bod
was already giving us the stronger
result or the lower part was already
given to the straw the result but turned
out the two of the bus there is a
function more like a promise problem
whose one-way communication complexity
with perfect randomly shared randomness
is K but with imperfectly shade
randomness it's at least 2 to the K so
you do need this 2 to the K growths you
cannot get away without that this has
been i hope i will not sure the stage
but i think we are almost sure about
this that we now have this result where
there is a function whose one-way
communication complexity with perfectly
shade randomness is K but two-way
communication complexity with
imperfectly shared randomness is 2 to
the K so you really cannot hope to
improve on this result in any
significant way so these are the results
that i'll try to talk a little bit about
so i will briefly go over all of these
results and so i should stop it for we
have the room so maybe I will and if
people are okay with it maybe we'll try
to take up to about 10 minutes extra and
then not go beyond 10 past four but else
yeah yeah sure sure yeah I know passing
that it's the same position great
randomness that shit is just a long
stream on either side which can be they
go right the correlation is the
correlation is Rho exactly yeah
correlation of room but it's but I'm
surprised at the link so the randomest
is there to help you you can of course
ignore portions of it that you don't
need to use right so so as a result the
yeah a bit exactly and so there are
times when you would be using large
portions of the shared right of this
when you have a very unlikely message
you will use large portions of the shade
right of this when you have a very
likely message you'll use small portions
but no matter how much you use the
amount of number of bits i will send to
you will scale in the length
proportional to a the likelihood of the
message that I'm sending you and with
the the strength of the correlation if
the weaker the correlation the longer
the amount of message but it is a fixed
constant factor on top of whatever I
would have normally sent you maybe I
think the next slide we're actually talk
about this so let me see what happens
right so ok actually I did do some color
coding which is not showing up to
clearly thats dark and these are
which piece that's what I'm going to
talk about next so the compression let
me just briefly tell you what the
original solution in the paper with Juba
at all was we said let's lick take this
common randomness between Alice and Bob
and think of it as if it's a dictionary
really now we're back to this thing so
what does it be so for each possible
message m the randomness I have a
infinitely long string associated with
it orbital elongated string associated
with it and Alice and Bob share this
perfectly that was the original the
perfectly shared solution when Alice
gets the message m what she will do is
she will start sending some prefix of
our sub m to Bob ok and the main
question is where should she stop and
what will she use in determining her how
many bits she should send she will look
at her probability of the message m and
she'll tell said you logarithm of the
inverse of the probability that many
bits if she had stopped right there the
expected length of the communication
would be the entropy of P this it's but
she'll send to Delta extra bits just to
make sure that your probability since
it's not exactly equal to my cetera et
cetera it she'll send that much extra
and so if she thinks if the Delta gets
larger she'll send more so she estimates
the Delta also right I mean she says
look I think I'm talking to a person
who's this far or this audience
collectively is this far from me so I
will say said this many extra bits per
message and I sent that much extra for
this message and sorry let me yeah and
what does Bob do to decode he just he
just knows his distribution in the same
dictionary he looks at all the strings
which have this as a prefix ok what
Alice said has a prefix in this
dictionary and he says let me look at
the most probable string under my
distribution from this set ok that's
what he D coached you and then it once
you have this protocol it's fairly
trivial it easy to analyze the nice
thing about this is this Tuscan spawned
a lot
to natural communication when you don't
do prefixes but in national
communications i say i have to send this
message across a message beacon message
in my head i say well if i could say
this but this is the likelihood with
which the audience would understand that
will hear is you know experts in
communication complexity i just need to
say the following i just need to say
cc's of row and then they'll understand
what it is well here is a slightly
broader audience for them by would have
a longer string associated with the same
message and here even brought an order
shall have it even longer string and i
choose the length of the string to send
based on my estimate of how far the
audiences and that's exactly what goes
on in this protocol now what happens in
our imperfectly shade right of the
solution we do pretty much the same
thing but Bob does an extra level of
error correction he says let me look at
all the strings which have this string
as a prefix or even some string close to
this as a prefix and amongst those pick
the most probable what so you're doing
some spell check before you do the
decoding and this is also fairly natural
as a mechanism this is the level of
error correction that we tend to do it
day to day use of communication not some
sophisticated error correcting coded
thing we do spelling matching so on and
that actually turns out that it works
perfectly sends a longer prefix and Bob
does maximum likelihood decoding after
are amongst messages that are close to
the received word not just exactly equal
so as long as you're within the distance
of row of the received word you're happy
with it okay that's roughly what happens
so this is it I won't do any analysis
here this works it's a fairly easy
exercise to work everything out once you
do this yeah oh okay this was Infinite
randomness and this is all right this
the it's a fixed constant multiple of
the amount of randomness and the fixed
random this is just you know the the
kind of the the expansion that you need
here is the kind of expansion that you
would ask for if you had
if you are doing communication of a
binary symmetric channel width parameter
row up an eye with a corresponding to
row okay so whatever it would tell you
this is how much you should expand
messages by that's how much we expand
messages fine so somehow the lack of
correlation between Alice and Bob
behaves like it's a noisy channel with
the same lack of correlation p dollar
symbol right so you take the large
corpus of shared randomness between us
to build this dictionary and so it's not
as if you choose it every time but it's
as if you have a it hard wired inside
your head it's a huge dictionary yeah
this is exactly based on the you know
some sort of expansion which is a
function of Rho ok so it's a constant
multiple which is a function of Rho yeah
and each time it's the same Castle
multiple right but right but we are not
trying to communicate the randomness
right we are trying to communicate the
message which has nothing to do it'll
the randomness is coming from the sort
of left hand side of the picture the
messages from the right hand side of the
picture but this is the site that we
want to focus on that is just an 82 our
communication but the aid the the error
in the aid seems to determine how much
expansion we were here
and decode means i can get back sumption
right making for the errors okay great
question okay good good good so I don't
know if I talk about it explicitly here
I don't so let me actually say one thing
a lot of times we have attempted to say
look you know we have imperfectly shared
randomness why don't we first distill
the rather this get a perfectly shared
right on the side of it and then work
with it and it turns out that that
doesn't work well ok so maybe on an
instance my instance basis over here you
could do this so you get but you really
you can only get a sort of a constant
factor saving over the communication
that you could think of without sharing
randomness that way and here we are
asking for you know we want to get rid
of that log log n and we don't have that
we can't pay for it with by just
multiplying it by a small constant
factor so here somehow it manages to get
rid of it in all the cases though maybe
this is not the most sophisticated use
of imperfectly shade right of this this
is maybe corresponding to roughly what
you're suggesting so that it's not quite
that I think yeah all right so let me
jump to the next example equality
testing and oops okay so the whole idea
over here turns out to be a bit it
really needs a different protocol to do
equality testing but the first step in
the protocol is really the standard step
which is encode the messages in an error
correct accorded think about these
encoded strings and try to see if they
are equal or not but if you thought
about it the usual way terms of having
distance or some such thing feels a
little difference what we really want to
think of these as as if they are really
vectors in plus or minus 1 to the N so
these are unit vectors are scaled with a
slightly different also vectors are the
same length in some n dimensional space
and if they are equal their inner
product between x and y is n
if they are unequal there is a product
is small so let's for simplicity I just
said n over 2 but you know could have
been something else okay so these are
the vectors of these inner products are
what we are trying to measure and this
is where imperfection randomness turns
out to be very good it measures in a
product pretty reasonably not perfectly
but inaccurately but but not whenever
the controlled in accuracy and this sort
of goes back to ideas from low
distortion embeddings or some such thing
these are just vectors in n dimensional
space we don't want to send n bits we
want to project onto something and it
turns out you can actually project onto
constant number of dimensions to
preserve you know this distance either
the distance between the two points is
zero or it's some constant so clearly
you can project onto constant number of
divisions this should work and by the
way the projections do not need to be on
the same direction they can be a
slightly correlated directions and this
would still be good enough so whereas
working with you know typical binary
strings etc etc this imperfect sharing
of randomness was killing when you're
working with real numbers imperfect
sharing of random is just sort of thing
alters things very slightly and you can
obviously go from standard collection of
you know row correlated bits to row
Kalitta garcias by just taking a
collection of pits and averaging them
take a collection of bits average them
now the two of us have correlated
gaussians okay this works for any
positive Road the correlation will be
that exact same rope okay so this will
be gone since the same so what does
Alice do she picks a random Gaussian
vector in n dimensional space and sends
the inner product of the Gaussian with
this X truncated to a finite number of
bits the number of bits depends
basically on some calculations that we
have to do Bob compares this quantity
that is received with his own Gaussian
which is not the same as that but it's a
correlated Gaussian with y if this works
great ok this is somehow in
communication complexity is a very new
kind of a twist to it so modulus of
analysis it turns out some castle number
of bits of ice and this is the
setting that we had first we thought
first this is a constant factor blow up
in the communication complexity then we
realized at cetera et cetera and by the
way I mean by varying at all give a
different protocol our protocol ends up
saying Allison Bob roughly need to have
polynomial in n share randomness and
bavarian at all gave a natural protocol
also which had two to the n shade
randomness and you don't have a generic
result which says if you have lots of
shade randomness you can use much less
if they are imperfectly shape so if
there's no general run so one protocol
doesn't follow from the other body black
box means that i can see alright so that
was the Equality testing the with some
level amount of work it turned out that
this was really a general upper bound
any product problem which has perfectly
shared out of this protocol with Cabot's
also has a one-way complexity which is
suppose two to the K bits and how do we
get this it may flee the idea is any
communication really reduces to inner
products okay so if you want to think
about one-way communication alice is
going to send a cab it message to Bob
and Bob is going to jail look at this
game it message and decide whether you
should accept or reject so you can
represent fix the randomness that they
are going to use Alice's message can be
represented by an element of a universe
of size 2 to the K okay by single index
in there and Bob what is he going to do
with it is represented by a function
which maps this universe 201 but if you
do it slightly differently I mean so in
F sub RFI sub R is the right answer over
our but you know if you fix the are this
is the answer that they're going to
compute okay no oops yeah I think okay
this is correct I just let slip that
thing over a bit now I'm just going to
go for a vector representation Alice's
message and Bob's output Bob's output
function f sub R can be represented by a
vector of the truth table of that
function so 2 to the K bit string or a
vector of size to the K Alice's message
can also be viewed as a vector
of size 2 to the K which is a coordinate
vector it's one in one coordinated zero
everywhere else and what is the inner
product that there what is the function
value that they are computing f sub R of
I sub R it's the same as the inner
product of these two vectors right the
minis real inner product of these two
vectors because it's this guy is one
that one coordinated zero everywhere
else that's the message on that message
should you accept or reject is
determined by why that coordinate the
inner product tells you exactly that the
inner product has a contribution of zero
from all of these things and one
possibly one from the one coordinate
where X sub R is one okay so this is
trivial but now we will back into inner
product regime and so which we know we
can do nicely with imperfectly share
randomness and so if you do girls in
protocol over here it will estimate the
error the same protocol that I used for
equality testing it will estimate these
are the products efficiently it will get
a relative error of epsilon with someone
over epsilon squared communication and
hear what I really need to do is get a
relative error of something like 1 over
2 to the K because these are long
vectors especially why is a vector of
knob about norm square 2 to the K but I
want to measure the inner product with
the plus or minus 1 so so this gives me
some 2 to the K bits of communication
suffice and this works out pretty neatly
so in the rest of the talk I will tell
you a little bit about the converse this
is probably the technical meat of the
paper and it says that there is a
promise problem with this one-way
communication complexity small but the
imperfect communication complexity is at
least 2 to the K this will happen
provided n is very very large that
cannot happen will n is small because
then i also have a k plus log n solution
that could be so this is not work with
k's and over two or something k is very
very very small we need to put measure
how small but there's there are such
functions that do exist all right so
what's the problem oh that we use this
is the result that we want to prove the
problem is probably interesting i'll try
to tell you a little bit about it that
maybe give you some idea
about what goes into the proof here the
problem is again going to be an inner
product problem alice is going to have a
vector X Bob is going to have a vector Y
and they want to compute the inner
product and they are happy to compute
this in the product to visit an error of
plus or minus epsilon and the inner
product is supposed to be normalized
between zero and one they want to
measure it with an epsilon but the neat
thing is we will work with strings where
alice is going to be guaranteed a sparse
string so very few coordinates are zeros
and most sorry very few coordinates are
once in most are zeros very few is like
a constant fraction of a tiny constant
fraction is going to be once and
everything else is 0 so 2 to the minus K
fraction of the coordinates are ones and
1-2 the minus K is zeros and Bob gets a
vector in plus or minus 1 they want to
compute the inner product now what
happens over here and the gap comes from
the fact that there is a promise either
than the product is high or its load
it's the right level of thing now why is
sparse in the product easy to compute
with perfectly shade right of this so
sparsity gives Alice a lot of clues you
know which index should she said she
should send you know she should try to
send Bob the value of x of i or some
coordinate where X sub I is not 0 if Bob
looks at y sub I'd that coordinate that
gives them a pretty good clue about
whether the inner product is going to be
positive or not ok if so in particular
so if you if you think about picking y
uniformly at random from this thing and
X to be a vector which is chosen
randomly but supported on the
coordinates where y is plus 1 that would
satisfy this condition on the other hand
if X is just chosen uniformly it's
sparse if we satisfy that condition but
now if you know Alice communicates to
Bob some coordinate where X is not 0
then the Bob can easily estimate the
inner product now how does Alice
communicate this coordinate
well they share a sequence of indices
okay so what is their perfectly shared
randomness they go to share a disease i1
i2 i3 i-4 and so on which are uniformly
random and Alice looks for the first
index which is where X I sub J is not
zero ok so now since 2 to the minus K
fraction of these indices are nonzero
you expect J to be about 2 to the K and
to communicate j you need k bits so you
can do a compression rather the two to
the K bits of communications just gave
it some communication and this works out
quite nicely perfectly shared random
this is nice now if these indices I are
not shared perfectly but imperfectly
this protocol breaks down and you really
don't have much to do except to go back
to the standard original protocol that's
what we end up proving works so I have I
guess 20 or 30 in slides are not going
to get to them so let me just see if I
can say something about the lower bound
which would be useful okay so is not to
pardon let me just sorry i'll just take
a look at this online
okay so one interesting thing in order
to show the lower but we sort of reduce
it to the following interesting problem
this is a problem that came up in the in
the Jays question as well there is a
very interesting problem that arises in
the setting of imperfectly shade
randomness which is how much can you do
by just a reduction to perfectly shared
right of this and so you could say look
maybe you can just do some shared
randomness distillation Alice has Bob
have lots of correlation but they don't
have perfectly shared Adam this why
don't they just somehow you know reduce
their input to something a little bit
smaller which they share exactly equally
so you can model that by a very nice
communication question which is problem
where Alice sorry Charlie and data we
just change the names because we reduced
Allison Bob's problem to Charlie and
Dana's problem Charlie has some string
our dana has some stray guess they are
correlated and the goal is for Charlie
to output some variable view random
variable data cannot put some variable
be both of these should be high entropy
random variables so you're not allowed
to you know output zero both of us them
cannot put 0 and get agreement that's
not good so no string should be output
with two higher probability and we would
like them to output the same value with
noticeably high probability love gamma
what can you do and we proved a simple
lever which says that with zero
communication the best value of gamma
you can get is really 2 to the minus
entropy some constant entropy so
constant times the entropy I mean
entropy very bits you can get trivially
Alice Charlie just sends the first
Cabot's of our to dinner they like doors
s it gets this you can get some constant
multiple smaller than K also by a slight
compression mechanism and that
compression mechanism will do very well
with Rose very close to 1 do very poorly
with raw vasilich very close to 0 but
it's kate is sort of linearly between or
linearly in some entropy of roar
subsisting
but but that's really the best you can
do if you want K bits of entropy and
Rose not Zeno not one that you have to
you know sort of communicate roughly
Omega of K bits or equivalently if they
don't want to communicate the
probability that will get it right it's
just due to the bile escape and this
already is very deep I just bought this
book by o'donnell on analysis of boolean
functions in perfectly enough you know
all the answers we were looking for was
inside that book so great so in fact
O'Donnell was very nice and I said look
you know Ryan i bought this book when I
really need to know what's the statement
of this theorem and how it's proved they
said okay here's the PDF so I have a
nice large PDF of its world book with
thee and this is a there's an easy
corollary for the zeros of communication
game to the sea communication game where
Allison so if you had a sea
communication protocol you can reduce it
to zero communication protocol but Alice
and Bob just guess what was communicated
they don't actually communicate they
just guess and then they look at their
thing and say suppose they were so with
2 to the minus C probability their
guesses are right and if they are right
you just multiply that it the error
probability you know if you have such
good bounds of the error probability you
could use it quite easily the rest of
the proof I will say it in words the
neat thing about this protocol which
used perfectly shared randomness was
that you know you could use the fact
that you know some coordinates alice is
the Alice's X is non zero and they
really heavily bias Bob's answer ok or
rather sorry the first you know I 1 X so
my sub 1 has huge influence on the
outcome of the communication protocol
once you fix the right of this x mi sub
2 has next largest and extra 3 has
something these kind of communication
protocols were some bits of Alice or
some bits of Bob have huge influence on
the outcome work well with perfectly
shared right of this and they do not
work with imperfectly share randomness
and the question is what's the tool
which allows us to use separate you know
very very you know operations which have
hugely influential variables and
operations which don't and so we went
through the literature there's this very
nice body of work which is emerging
again covered nicely in Wright's book on
in the invariance principle it says that
there are very settings where if you're
doing something and you're not letting
any one of several variable influence
the outcome too heavily you can actually
place vectors by gaussians okay so
instead of having zeros and ones you
could X could be a Gaussian vector and
why could be a Gaussian vector now if
it's Gaussian there's no such thing as
sparse okay and so the sparsity goes out
of the window and one sparsity goes out
of the window there's really a bit 2 to
the K bits I mean if you want to measure
inner product difference of plus or
minus 2 to the minus K you have to
communicate 2 to the K bits you could
sort of work this into the calculation
somewhere and so on it be managed to do
this we also got a kind of a nice
express explanation of what invariance
principle should look like when you try
to apply it in the sense of
communication complexity we say that if
the paper and so on but i'll stop here
i'll jump to the conclusion slide and
see if I had anything interesting to say
there okay so I just want to go back to
the high level thing no this imperfect
agreement of the context is a very
important theme I think really even you
know so far we've been sort of looking
at it saying it's an important question
to the pension this time we also solved
it has mathematical depth to it so it
was very nice to see this question it's
a new layer of uncertainty and it
exposes us to do questions and bottom of
the mathematical size it also brings up
this issue of scale what should you
really think of as it puts eyes we
always try to throw the context out to
say well input size is really
proportional to how much we should
communicate if you bring that back in
it's actually the communication is
always a tiny component of what the
input size
is and in this setting you really have
to measure communication complexity with
a much finer lengths you can't just
throw a log n say oh that's okay this is
it's really important the many
interesting questions some coming from
this question some specific questions
related to perfectly share randomness
and so on but then I think even more
interestingly it okay you just sort of
broaden this layer of you know like for
example this complexity problem many
questions could probably consider it an
ascetic where Alice has input X and a
function f bob has an input wire and a
functor gee Bob wants to compute G of X
Y Alice wants to compute F of XY how do
they start exchanging information and in
particular if f @ g are actually somehow
close by functions with could be fairly
interesting questions I have not really
been able to look at it carefully but if
you you know depending on how you
instantiate the question there may be
something interesting there ok stop
yeah oops maybe yes it does inflation
never healthy natural language with that
information right it's a good question
okay if it does help that I'd love to
hear what it does because usually if it
if it doesn't help in our case it does
help with natural language business an
interesting question that remains to be
modeled from our point of view we've not
been able to come up with so the
assuming the two theorems which I didn't
put down on the slides but I'm quite
optimistic are true if you have
perfectly shade right of this between
you and me then interaction is very
useful but imperfectly shared randomness
immediately we seem to give up and we
say look either there was no need for
randomness in which case an interaction
could still be helpful or we really you
know I might as well send your message
and let you think about it for a long
time okay so that's the non interactive
version of it that's what we are seeing
I don't know whether that's the right
message or not here all right okay
that's a good question so so so right so
in compression also bid one could ask
the question so I said to you for
example I could send you f of X sorry
this run you know i said if this word
from the dictionary a certain length
prefix and the other guy could set back
a thing saying look by the way that was
really to delta bitch to log x by bears
the measure right and then next time you
know that oh look i could have sent a
smaller thing our distributions are
getting closer to each other so yes if
you do so quite often what we're doing
with interactions is sort of holding
down the context to make it a little bit
closer to each other that's a very
interesting phenomenon but we'd have to
define the right kind of questions I
cook very curious too I mean it's a very
nice question by the way
right now but most of communication what
is communication trying to do is to take
your large context in your head and you
listen to what others I say you send out
some output and your context has changed
in the bead why so all of communications
is just a random walk or a walk or the
state space which is your context and
quite often you know our communication
the goal of the communication is to get
closer agreement on the context so that
we can carry out some other conversation
nicely yes that makes a lot of sense but
it also makes sense at the end we need
both players to be able to understand
the answer so I said something you said
something I should know what was the you
know after all the uncertainty I should
get some message which I can really
believe and they like a big progress
with it so there's a big there's some
interesting modeling questions over here
and what phenomena do we want to model
and what how to extract a lesson
mathematically but could be very tres
what is the difference poison right okay
good so so far you know telling road
finished writing the paper with never
thought about that question just kind of
a shame would be you should have been
thinking about at this area noise all
this time would they but I never thought
about it and at the moment i'm not sure
what we could do with adversarial noise
I mean if there was a genetic method
which would take two strings which are
close to each other and reduce them to
you know sort of something like
coordinated Gosselin's or some such
thing that would be nice but I don't
know for clean scheme so I don't know
actually so what might happen is it's I
don't to conjecture anything right now
but yeah it's it's a bit surprised but
we never thought about it and then the
thing that I'd really like okay that
being said i should say i mean then the
national model would be Alice gets a
random string from plus or minus 1 to
the N an adversary pics am asking
pattern and so that and then adds the
two so that Bob also marginally is at
least getting random string if Bob is
not getting rid of this that it's a
little I mean I I'd be less comfortable
with the model but if Alice and Bob are
marginally getting uniform rate of this
that would be clean and then one should
ask the pressure
right so the problem with seeded
extractors is now we have to say where's
the seed coming from it that they go to
have it exactly you have to communicate
it right so unless you could get away
with sort of cost I mean many of these
things where the weed can't control the
length of the rod of this and so if that
turns are to scale with the length of
the input then we would like the seed to
be like at most acosta number of bits
which is almost nothing can be done so
right i mean it's not really hard to do
anything but sometimes you can sort of
make do so by the way many of these
cases it's there's an easy solution to
dealing with uncertainty which is you
get rid of it you just ask the other
person who are you to get the answer but
most of these cases that will be
communication in efficient solution so
you really are looking for cases where
you can do better than the naive thing
and in this case we even think of
agreement distillation there's a naive
solution saying if using we want to have
K bits of randomness if there's a
solution which takes our amount of
randomness it reduces it to K perfectly
share data bits and works with it then
it's a reduction to a known problem but
this is the naive solution we are always
looking for things where you can improve
of this and all the solutions if we see
improve on this is very nice so it's
quite remarkable that there's so much
more power within perfectly share
randomness all right so thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>