<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Human-centric machine learning. | Coder Coacher - Coaching Coders</title><meta content="Human-centric machine learning. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Human-centric machine learning.</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eCH3EJ48ZMI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you on oh and there thanks to Gary and
who's been working hard all day behind
the glass doors they're recording and
they're switching all the presentations
is very efficient and also thanks
Sebastian and the other organizers for
putting me on the agenda and I know s
some of us like songs that end on a big
bang and some of us like songs that kind
of fade out in a slightly melancholy
kind of tone so you can judge which
which what what the analogy will be my
talk today where the fact you've been
here all day Lee may be a bit tired and
so on and so on so my talk is I'm going
to say very little about algorithms and
technology and or anything like that and
talk more about some I guess feelings
about how we might be able to expand the
reach of machine learning into society
so outside the kind of people like most
those in the room who are experts at one
kind of it and to help machine learning
have the impact it might have more
broadly so bringing a lot more people
that currently don't use machine
learning as part of their day-to-day
lives but might be interested in doing
so if only we could somehow make that
easier and more useful for them and
these thoughts have emerged after eight
years of work here at Microsoft Research
in the group that I had which is called
computational ecology and environmental
science we're driven by we're not driven
by an inherent interest in machine
learning our inherent interest really is
in the way that the natural world works
but we have a particular flavor of our
group which is to try to build
predictive models of different aspects
of the natural world and so in doing so
we've naturally ended up doing a lot of
machine learning from a particular
flavor and so there are some examples up
here I think I can get her I guess one
of these is going to give me a laser
right here we go so this one on the left
for example is some work that I did with
my post mark van de Ville and trying to
understand why it is that different
species of trees adopt different
distributions against the climate you
find some in the north and some in the
south and so on how can we understand
why that is and I've got a little demo
to show you in a second of that to some
work looking at ecological food webs so
can we can we somehow understand and
predict which species will eat which
other species so is it just a completely
idiosyncratic free-for-all or in fact in
our model we claim that if you
you can place the species on a line and
so each species sit somewhere on the
line and eats from somewhere else on the
line and that that very simple kind of
dimensional collapse down onto that low
dimensional system enables you to have a
high predictive power of which species
will eat other species this is an
example of a model that looks at how
plants grow and develop so each each day
the plant fixes certain amount of carbon
and invest carbon in leaves and roots
and one day it decides to flower and so
on and this eventually gives you a
massive seeds at the end and we're
looking at using that to try to
understand the potential response of
global agriculture to climate change and
and some similar model to do with
conservation this is a model describes
the response of different kinds of birds
to land use change this is a simulation
of a human process of taxonomic
discovery this is so this is looking at
the rate of discovery of new species
around the world we do know that most
species have not yet been discovered and
this model describes the species
discovery process in terms of a
collision between the number of active
taxon amidst and the number of remaining
species to be discovered and then a
trend in efficiency through time and
then this is a very ambitious model that
my colleague Matthew Smith did which is
a model of the global carbon cycle and
and so these models are they vary a lot
between different cases but there but
they in almost no cases do they
correspond to any predefined model
structure these are things that we've
tailored for the system of interest but
nonetheless what we've learned to do
over the years is fit these models to
lots of data where we rely on MCMC
sampling I'm sure some of you familiar
with that it's a very broad robust
technique that allows you to pretty much
fit any model to any data in fact we
have our own algorithm that we're very
proud of called Phil's back which we've
tested very heavily and stand so we just
kind of throw these problems that fills
back in its and it's able to cope with
these things so we do kinda I just want
to point that out that you know we do
actually develop new algorithms for
machine learning and we enjoy doing that
and we're proud of them but some but the
topic of my talk today is really after
doing all of this thinking you know you
can stress when you do these sorts of
problems that what you're trying to do
is is get to the prediction at the end
that you're trying to go through the
data get the data push it through the
model
and get to a prediction maybe though
before we move on I might just quickly
show you this demo it depends on
silverlight being installed on the
presentation pc and it looks like it
it's which is good so I just show you
one more because as though it's late in
the day and it'll be some colorful
pictures and so on so this is a
visualization that we built of the
forest modeling results and so what's
happening here is the term in these in
these figures what we'll see is how
different rates so things like growth
and mortality and reproduction in
different kinds of trees in this case
northern temperate hardwoods varies in
climate space and this was this was a
model fit against I think I wonder half
million individual measurements of the
growth and and death and reproduction of
individual trees across the United
States and then if you then project that
onto the geography of the United States
you can see how that varies it varies
for different groups not we're able to
do then is start a simulation where we
spread all the individual trees so
millions of virtual trees everywhere in
the United States and then we're able to
run and we let them compete it out over
about five hundred years in this case
and we can see eventually where they
settle down too and we can compare the
predictions of the model there to what
we see in reality and this is an
independent prediction because the model
was just fit against fine grained
short-term observations of growth and
mortality but what emerges over about
over hundreds of years in the simulation
is this distribution which we compared
with reality and then we did some more
experiment to show for instance that it
was the increase in mortality as you
went north that was the primary
determine determinants of the northern
limits of these southern species so we
get that kind of ecological insight out
of the out of the modeling one of the
reasons I want wanted to mention that is
because when we look back then although
we've we are now totally say we've done
this in our group a lot we tell you to
stress that end point of the machine
learning in our particular domain which
is the ability to make these kinds of
predictions because for instance we can
then change the climate of the United
States and predict how that might affect
this biodiversity and the carbon in the
forest but real but when we really look
back at what we as individuals and I
think the contributions of our papers
have come from machine learning
is really much richer than that we've
got we're taking a whole load of
different things away in terms of
insight in terms of understanding of
different the value of different data
sources and kind of your rica moments
and etc so for example so i think if
we're in danger sometimes and i'm not
everyone thinks this way of presenting
things a little bit like this which is
we just kind of throw data through a
computer it could be it this is a
Commodore 64 still my idea of the ideal
computer we throw data through a
computer and we get numbers and that
somehow makes the world a better place
and this is kind of pipeline sort of
mentality really that you know the end
we get some kind of and of course it's
not just numbers but it might just be
you know predictions or something which
we then roll out is this kind of develop
the model train and then deploy that and
in some way so it's a very kind of
engineer ii kind of situation of course
sometimes I exactly what you want to do
if it's something that's controlling a a
production plant or automatically
displaying adverts on the web for
example but in reality we're usually
after a lot more than that from the
process expanding on that little bit of
course we're not quite as naive so we
could think of having these predict and
response data and we train and them are
able to then reapply that model to a
much larger group of prediction predict
predict data and that again somehow
makes people happy we have this idea
that alternatively an unsupervised
machine learning we're just sort of
discovering patterns like like clumps or
groups or a PCA or some analogy of that
and again this somehow leads to two
people the world being a better place
and happy people we expand that a little
bit more we can think about and this
this i'm putting this partly and to show
that my group has definitely been guilty
of exactly this view as well which is
this again this pipeline idea of a data
and hypotheses bayesian inference
because that's our chosen method this
gives us a refined set of hypotheses
parameter distributions for any
remaining models which you can then go
on to make predictions with uncertainty
but we can at least complicate this a
little bit by saying actually this model
structure and parameter distribution
here and then feed back and say what
kind of data will be like to take next
so it can give us an idea of the value
of different forms of data is really
crucial because for example at the
moment we put up satellites for three
hundred million dollars a time and by
and large
and there is no kind of predefined idea
exactly about how that leads to
predictions of what we have what we need
there is a kind of there's a reasonable
instinct saying this is useful data kind
of but there isn't really a precise idea
the same is true with the predictions
here of course and then and then in some
general way these predictions will
somehow in our case interact with
society often thinking about things like
food security and climate change I could
be governments could be companies
individuals somehow will will make use
of these predictions and again I think
that much as this is my diagram from
previous talks I think it's even this is
just much too much to try people are
going to you know what what is really
the potential impact of machine learning
in in society and I think often it might
be the kind of for example here you have
at least the idea that the number of
hypotheses is reduced and so that's to
some extent we've made a scientific
discovery when we do that we've ruled in
and ruled out some hypotheses and it's
that kind of insight that that idea of
getting insight from the process and
maybe in in reality is it's those
moments they're going to have the
largest impact up there in the real
world I think the the idea of this
pipeline for me that the the two key
ways that it's misleading are worn is
that actually good machine learning
doesn't really look like this it's a
much more as you guys know it's a much
more interactive creative process where
you take data visualize the data think
about the models develop the models look
at train the models against the data you
look at the results you might switch
your data sources non and on it goes
right and actually it's enjoyable that's
why we and we like to do it and and so
this idea that is misleading them to
tell society that you can just throw
data through prepackaged algorithms to
get something useful out most of the
useful examples we have in reality don't
work that way and secondly I think it's
equally important it's a really
disempowering way of presenting machine
learning to the world it's actually
saying you the non machine learning
person you're an expert in your domain
you worked really hard to get where you
where you are you know about the real
world you've got instincts and ideas but
you know what we don't need you anymore
because we've got the computer and it's
so it's not really going to draw in
those people in my mind that don't do
machine learning yet if we present
present it in this way so now there's a
little bit of a long introduction so
what I want to quickly go through now is
what i see is six key barriers to the
adoption of machine learning in society
that follow from us adopting this this
this pipeline view and don't get me
wrong there may be many people in the
room that feel they don't adopt that
view so I say we as they mean perhaps me
and my colleagues more than anything so
the first book the first barrier I see
is actually getting data in the first
place so it's funny you have this I and
TI t and we concentrate so much on the t
but what about the I where are people
supposed to get the information from
usually we just assume that that's your
job to go and get the data and transform
that data into useful information so we
felt this in our group wielding a lot of
work we would have some kind of response
data like tree growth we wanted to link
it to climate and soils but actually
getting hold of that climate and soil
information is really hard every time
such a huge amount of wasted effort but
it just doesn't seem a very interesting
problem to a lot of people to supply
lots of data and information it's much
more interesting to think about their
the algorithms and the technology so
it's as if we're all of us left in the
dark rediscovering the same data sets
and working away making the same
transformations on those data sets as
different people and this is something
that we've tried to do something about
in our in our group it just tells the
six barriers I think this is I don't in
any way want to suggest that we've
overcome these six in our group
absolutely not this is one that we have
thought a little bit about and so we
built a service which sits on top of
multiple data sets in this case and if
you want to know something like I like
the African monsoon example and then we
built a web explorer which is what this
is on top of that back-end service and
so we're saying in this case I'd like a
grid over this part of the world I'd
like to know about precipitation if I
want to know what the date what data
sets are underneath era can have a look
and if I like them or don't like them I
can turn them on or off but if I'm a
normal person I'm just saying I just
want your best guess at precipitation
thank you very much i can tell you
something about time so in this case we
could for instance go through 12 monthly
time steps during the year and this
service then is going to go off find all
the data sets that could potentially
fulfill my query it's going to do all
the interpolation and sub setting etc
off those data sets it's going to
calculate uncertainty for each of those
and then it's going to take the least
uncertain answer and stitch that back
together and
send it back to me so I get my answer so
I've just sidestepped enormous amounts
of boring work just by sending off a
single query to this to this service
it's always a bit nervous doing
something live off their web service say
usually works in reasonable time it's a
look if not I will oh here we are and
then so here's the result in this case
and and then we got a little Explorer
there so you can go back and forth and
just see how the monsoon rains change
throughout the year in Africa typically
and so and it just shows you that how
much easier we could make it to get
information and then be able to visually
explore to make adjustments that data
look good is it the right resolution etc
etc so that's kind of barrier number one
and the weave that we felt a little bit
about bar your number two I think is
enabling people to insert their human
knowledge into the model so that if I
load for instance a neural network is
very difficult for me to somehow adapt
that neural network to represent what I
think I already know about how the world
works or how this particular area worked
and so in this is in Matthews carbon
model for example this is stock and flow
model and each step here is based on our
reading about biology and physiology and
ecology and so on so that's a very
complicated example this is something
that very much reflects our our domain
knowledge and it's that becomes the
model that we use for the machine
learning not some predefined structure
that was designed with computational
efficiency in mind but I think
addressing this issue could be much
simpler even just helping people through
the steps of choosing the right
predictor variables and maybe loading
priors in some way to say i believe that
this has a positive effect on this and
this other thing has a negative effect
on that or saying you know i think that
it's these two things together that
somehow affect this I don't know how but
I just really think it's both of those
and either either alone wouldn't make
any sense so these kind of human
statements which is what you might get
out of a song with some domain-specific
knowledge how at the moment can i as a
normal person not machine learning
person insert those into the model that
I'm going to use in my in my machine
learning it'll be really interesting
kind of problem in in interaction design
and user experiences I think we could
think about another one is as we all
know there is there a lot of pitfalls
in machine learning there's a lot of bet
aspects of best practice in general
though you know people like like the
people in this room understand what the
best practice is you know you understand
the idea of overfitting of correlated
predictor variables of extrapolating
outside your training domain all these
other things right the need for
independent testing and making sure that
you know your various search algorithms
of converge there are loads of things
like that and so having those things
kind of baked into again what whatever
into the systems that we use for machine
learning and in a way that normal people
can understand maybe if you through very
human human language and it will be
careful you know whatever you do don't
don't use this model to predict outside
this range because you could get crazy
results and make a terrible decision and
so on and I mean in our agree that a
little bit about this in the particular
domains that we work in but there's
nothing here that I think is
particularly surprising about the idea
of splitting data and training and
testing and so on this is quite orthodox
what's more surprising is that things
like that splitting datasets and doing
independent testing or n fold
cross-validation or what have you
typically doesn't come naturally baked
into the tools that we use is something
that most people program somehow
themselves they sometimes do but it
really should be something that's hard
to avoid in the systems that we're that
we're using and that's and that's a very
small technical thing more broadly we
might be able to help people just to do
reasonable things and to use their
models in reasonable ways and use the
data in reasonable ways if we thought
about it enough okay great that's great
thanks so another one is how can we how
can we let us assume that we've gone and
spun up some algorithm we fit a model to
data in some way so how can we now
enable human beings to extract
understanding from that as well as we
all know if when we train one of these
models depending on the model some
models are easier and hard of it those
things have now in a sense are some kind
of summary of that of the data or
summary of the the influences and
effects that are going on in that data
so how can we present that back to more
normal human beings in a way that can
can really help their understanding like
you know this is an example of the plant
growth model and we you know we altered
this model lots of times fit against
different data of invent individual
plants and we eventually had this
realization that that the optimal time
to flower as a plant was up as its
so called peak nitrogen so we start with
a high nitrogen level and the plant
sucks it all up and it goes to zero
initially it doesn't suck it up very
quickly because the plant is too small
and at the end it doesn't suck up the
nitrogen very quick because there's none
left and in between there's a time
that's where you call peak n which is
analogous to peak oil in humans and that
turns out to be the optimal time to
flower so after all that kind of effort
of fitting models to data actually you
end up with this kind of our ha moment
and I think for example in the world of
business I would imagine it's those aha
moments that might because they there
that the ones that often then lead you
to a more radical solution and it's kind
of more I get more a bit more empowering
right you know it's like wow I
understand things a bit more I'm doing
some work at the moment on extending our
food web models to protein-protein
interactions and we're looking at
fitting different geometries to
protein-protein interaction data sets
and so this is a spherical proximity
model the closer you are on the sphere
you're more likely to interact this is a
lock and key model if my lock matches
your key they're more likely to interact
and what we're hoping is that being able
to conclude between which of these
geometries and that's one there weren't
one of about twelve fit the data better
and more consistent with the data and
then looking at how the proteins are
arranged in that space might give us
some understanding about how it is that
signaling networks in biology are able
to do such complex computation but I
think again for for other people it
could be much simpler how we report to
them this is a tool called MaxEnt which
is used in ecology a lot so i think is
really interesting because the
underlying statistics are relatively
simple but what's really interesting
about the tool is the way it reports
lots of useful information back to the
user so it gives them a ranked it gives
them a list of predictor variables with
a useful graphic here saying that allows
you to assess how important different
variables are and it even looks at the
functional form so you can see the
effect of each variable on the output so
this is all kind of stuff that you know
the people that use this tool really
value and actually it's once you've done
the hard bit actually fitting the model
two day trip so it's pretty easy to
generate this stuff it's just kind of
legwork but it's really useful and then
I've only got a minute left i'm gonna be
very quick a couple of other points so
one is to think about realistically how
machine learning might be used in
society they're going to be a lot of
cost-benefit trade-offs
so that rather than looking at it a
little bit dryly like that what has the
most predictive power in my model I
might be interested in both that but
also how much money does it cost to take
that data how risky is it how difficult
is it legally so could we help users to
generate a kind of cost effective
optimal machine learning model which may
be very different from sort of
statistical best model might ever model
it's over fit not in a technical sense
was simply because it requires data that
costs too much money it's over fit in
that sense and and this is just to point
out the the huge annual expansion in the
cost of computing worldwide and data
storage etc and a lot of us are quite
excited about things like the cloud but
if we keep going at this rate then we're
in danger of being in a world like Isaac
Asimov imagined when people were
building this ever more sophisticated
computer to answer this unknown great
question and at the end it sucked up all
the all the energy in the universe and
just said let there be light and it was
a singularity kind of movin and that it
is quite humbling to think that none of
our analyses are even close to a human
brain yet but the human brain runs on 15
watts but anyway so the point is that
that money and an energy and etc other
considerations could we naturally build
that into the system to help people
build cost effective sustainable machine
learning and then finally this is very
very quick point just to say that even
at the end of that if you end up with
this information presented bats user in
the form of predictions for example it's
far from simple how you can make
reasonable decisions given the uncertain
predictions of models so for a couple of
headlines from recently where they
practically shut down New York and then
the storm didn't quite hit New York and
like a lot of flack for that and here's
another one which is every year and the
CDC and other people have to judge which
flu vaccine to develop which strain flus
most likely and this year they got it
largely wrong so the the flu vaccine is
pretty much ineffective this year and so
could we help people I won't explain
this but develop solutions for instance
that are robust to those sorts of
uncertainties and to help people with
their thinking and there we are it's my
final point just to say i guess the
overall point of this going back is to
get away from that pipeline into think
about humans as we can see in this
unbiased sample of humanity that was
returned to me from an image search for
humans
earlier this afternoon so let's just
remember this lot when we're thinking
about our algorithms thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>