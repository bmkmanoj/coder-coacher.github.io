<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Build It, Break It, Fix It: Contesting Secure Development | Coder Coacher - Coaching Coders</title><meta content="Build It, Break It, Fix It: Contesting Secure Development - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Build It, Break It, Fix It: Contesting Secure Development</b></h2><h5 class="post__date">2016-07-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KYMHL845D_Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
I'm very pleased to welcome mike hicks
from the university of maryland mike has
had a long-standing interest in building
secure and reliable programs dating back
to his thesis work on cyclone today Mike
Mike is now a professor at the
University of Maryland and today mike is
going to talk about his build it break
it and fix it contest and what is novel
about this work is that he's really
interested in understanding some of the
factors that help people build more
secure software as opposed just breaking
software so with that I'm pleased to
turn it over to Mike thank you very much
very happy to be here so for those of
you who this top topic might seem
familiar i gave a small a talk about it
last summer when i was here visiting and
the difference between that talk and
this talk is now we have results okay so
um in the world of cyber security we
have white hat so you know build stuff
and try to make it secure and we have
black hat which is about breaking stuff
so we want to find out Valle nur
abilities and exploit them and right now
the emphasis in the security community
is pretty much about black hat there's
some how some feeling that if you're
good at breaking stuff that you're just
so smart that you'll know how to build
it so you can't break it and as a result
there's a lot of emphasis on breaking
and not very much on on building and
this goes all the way down to education
like in contests for students for
example so I don't think this is good
ideas somebody who is interested in
building software that secure i think
that the skill set is not necessarily
the same and therefore we ought to be
looking at how to build software better
and we ought to be setting an example
for students that building is just as
important as breaking so my student
Andrew and I on Andrews happens to be an
intern this summer and is here co
conceived this contest called build a
break and fix it which is intended to
emphasize not just breaking but also
building so the basic idea of the
contest is to break it into three phases
so the first
is the build it phase which you have
about two weeks to build software
according to a specification and that
software will have some security
properties but you will be graded on
correctness and performance as well
because you know no way anybody can
build secure software that doesn't do
anything useful the hard part is to
build software that's useful which is
why people want to buy it oh yeah
absolutely the software is called exit 0
right very secure unuseful software
right but we want to have software
that's useful and it has good
performance so they're your graded on
that then in round 2 break at teams
which may be the same as the build at
teams get access to the source code of
all of the submissions from the build it
phase and they get to whack on those and
try to find vulnerabilities and other
bugs and if they do they gain points and
the build of teams will lose points
finally there's a fix-it phase where the
builded teams have an opportunity to fix
some of the bugs that are found and to
sort of improve their score as a result
and then finally we tally the final
results and have winners so the scoring
system is important for making the
outcomes of the contest meaningful so
there's both a build at score and a
break at score these are two independent
scores on the build its side you will
gain points for good performance and for
implementing optional features but
you'll lose points for bugs that are
found by break of teams and we try to
distinguish bugs from test cases so
break of teams will submit test cases to
show that you have some error in your
software many test cases will expose the
same underlying defect and so the fix-it
phase will be used to show that multiple
test cases effectively are in the same
bug and so your point your score is
scaled that way on the break it side you
gain points to your break at score for
finding bugs and again these are for
unique bugs so if you find the same bug
as many other teams then you don't get
as many points for it yep facing book
well no we don't have an automated way
of doing it I'll talk more about it
that's one of the few places where we
have to do stuff that's manual and we've
been thinking for like two years about
how we could possibly do it in an
automated way so if you know the answer
to that question I'll buy you a case of
beer
right now not just Avior I'll buy you a
bar so the part of the goal of the
contest is to make it a fun contest and
to encourage students to build it and
break but also part of it is to be a
scientific experiment so we have a semi
it's basically a quasi controlled
experiment and actually that is a
technical term you can look it up on
Wikipedia that is there are many factors
that are controlled but many factors
that are not controlled and there's an
opportunity then to study what works
right so there are many ideas out there
many people in this room or researching
those ideas about what might improve the
security of software or might make you a
more effective breaker and we can put
those to the test by seeing well who
wins the contest and when they win what
do they do right so what choices did you
make about programming language what
testing strategies did you use what
process did you employ what background
do you have right all of these things
come into play for an analysis of
outcomes so I'll give you it so this
talk will will go in I will go more into
the details of the contest but I'll also
tell you about some results that we
found and so here's a teaser of the
results so we have run three contests in
the last year with 116 teams
participating most of these teams came
from a Coursera specialization on
cybersecurity so the participants took a
four course sequence and then the
capstone for the class was this contest
we also had a few open graduate and
undergraduate student teams participate
as well so on the bill that side the
trends were as follows if you programmed
in c and c++ you gained some benefit to
your performance score but if you used a
statically type safe language like java
or go or something like that you reduce
your chances of a security vulnerability
as compared to using c and c++ on the
other hand if we ruled out memory errors
as being counted as a security bug if we
looked only at sort of semantic security
errors and you'll see more what I mean
about that a little later then actually
the benefit from static typing became
statistically insignificant so
essentially what is what is the
statically typed language do well no
memory
by definition you don't get that in C
and C++ but it turns out with your
statically typed language you also don't
get benefits for other more semantic
properties sort of out of the box and so
then C and C++ we're not really any
worse smaller code size does better and
we found that that correlated with
library use so basically the less code
that you wrote and the more work code
you use from someone else the better
that you did on the breakage side you
did better if you had a larger team
breaking is a more parallelizable
activity than building and it also
turned out that you had a pretty big
advantage if you participated in the
build it round when and we're also a
break a team so maybe ten or fifteen
percent of our team break up teams did
not participate in the build it round
and they did not do nearly as well as
the teams that did and so you can
imagine here that if you've built the
software that you're trying to hack
you've thought much more carefully about
the design space and you can think of
ways that people might screw it up
better yeah some teams felt like they
wanted to I mean it's the contest is a
five-week contest and if you do just a
break at phase it's only two weeks so
it's possible people just didn't want to
spend the entire time working on the
contest there are independent prizes for
building and breaking so you don't lose
anything I suppose you still have an
opportunity for a prize on the breakage
side sometimes breakin teams didn't
qualify for the build it phase they
didn't pass all of the correctness tests
so they could only participate in the
breakup face and it's possible then that
that correlated with their their
outcomes ok so I'll tell you more about
the design of the contest and about the
contest problems we ran in our
particular these three contests and then
I'll dig more into these statistical
results ok so the build it round correct
feature poll efficient secure software
so these are these are what you're
aiming for and we assess those according
to test cases so basically all of the
teams were given a specification and
they had to build software that met that
specification and we said that you sort
of meted enough if you passed all the
test cases that we provided to you and
people could run these test cases on
their own some test cases also assess
performance
for example how fast could you do a
particular operation or could you do it
in a more space efficient way and your
score then was scaled according to how
performing it was so for many elements
of this yeah Oh almost assess its pros
we in earlier versions of the contest we
did a bad job of being unambiguous in
the spec and we paid for that later so
so basically we solved that problem by
trying to be very precise in the
specification but also we have an Oracle
implementation that we use for various
auto judging things and we basically
made the Oracle implementation available
via web service to participants so if
they were confused by the spec they
could always run some tests against the
Oracle to try to clarify and so that
that way if the spec was a little fuzzy
the the Oracle clarified so the idea of
this was to leave many things open to
interpretation so programming language
being the obvious one but other things
too so we tried to make it so that even
the underlying design even while
implementing the high level spec you had
a lot of freedom in terms of the
underlying design and then the hope
there was that some people would do a
good job with the design and some people
would do a bad job and then we would be
able to see okay well what correlated
with with the outcome so in order to
make it so that both the build it and
break it side was tractable in terms of
scaling this contest up to many many
teams we set the development platform to
be Anna blunt to Linux VM so that way
everybody could be sure they're running
exactly the same configuration on their
local machines as would be on our
testing machines and then likewise the
break at teams when they got access to
the source code knew they could always
just build the source code and when they
found bugs in the vm they'd know they
could submit test cases that would
reproduce on our VMs and that would all
work out so we used vmware images for
that some people could convert them to
virtual box and that all seemed to work
fine we also required that everyone use
get to do their development so this way
we could have access to the full develop
in history so basically every time they
pushed a commit to either github or
bitbucket we would download build and
test their software and update the score
on the scoreboard so if they now pass
more tests or at their performance
improve then the scoreboard would be
updated with every push to the commits
the git repo and then we made copies of
every commit that they made so that
afterwards we could look at their
development history and maybe it would
tell us something interesting we
required that they not obfuscate the
code security by obscurity probably does
work at some level but we didn't want
that to be a consideration so we just
said to people you can't obfuscate the
code and we will manually check and
actually we didn't even manually check
we just made this a rule and we told the
break it teams look if you start looking
at code that looks like someone has used
an automated tool to obfuscate it or it
just looks like people named the
variable names purposely so that you
wouldn't understand them tell us and
we'll look and we might disqualify
people but in the end nobody tried to
test this rule okay so scoring I
mentioned this before so basically every
correctness test you pass you get some
constant number of points em which we
just fixed to be the number of
qualifying submissions and then for
every performance test we scaled the
score based on where you placed in a
linear way amongst everybody else so the
very best submission would get em points
for that correctness test and the very
worst would get zero points and then
every score in between is scaled by how
close you are to the best and the worst
right we didn't just do a rank order if
your score was just a little bit slower
than the best score then you got a
scaled score that was very close to that
forever sorry oh uh we did like 11 runs
and took the median or something like
that an earlier contest we rank ordered
the scores and this was a huge mistake
because lots of scores would end up
getting clustered but using this
strategy to doesn't matter if you're
only like slightly worse or slightly
better than somebody else for every
correctness bug that you had afterwards
that the break at team
found right they report some bug that
where they claim that you didn't pass
the spec but it wasn't covered in our
tests you loot you lost em over two
points if you crashed according to a
memory safety violation like a segfault
which we would then manually check then
you lost em points and for a security
related bug and I'll tell you more about
that in a second you lost to two
endpoints and this was a provable
violation of privacy or integrity
according to the specifications
definition yeah so have you used the
same test cases every time or do you
take some of participants come up with
new ones then add it to you so we we
have not used the same contest problem
multiple times at least not when money
was on the line so we're developed we're
gonna have another contest this fall and
we're developing a new problem for it
because the prizes for the open contest
are on the order of thousands of dollars
and when that much money is on the line
people will just you know find code on
the internet from prior runs of the
contest and then they'll cheat and that
would be bad how come you chose to rank
the scores this way you could make an
argument that a cratons focused worse
than a crash um yeah maybe so I guess
it's not it's it's not arbitrary crash
it's not like I throw an exception or
something like that its memory error
related crash really the crash that is
not secured be exploitable we felt like
it would be too much of a distraction
and would take too long for them to
exploit these bugs and demonstrated
Floyd ability so what we said was you
can get half the points of a security
bug because very often memory errors are
exploitable and we'll just if you if you
want go ahead and exploit it and then
you'll get two end points for it but if
you don't want to exploit it then you
can get end points whereas a correctness
bug I mean part of the contest here is
to emphasize security at least a little
bit more than correctness so we wanted
to make it worth more than a correctness
boat so um
types of hip safe languages get like an
advantage from spartacus yeah they get
an adventure from service that's right
okay so the break it round the goal is
the team's submit bugs and
vulnerabilities in the submitted code
they have access to to the source code
and again how they go about it is is up
to them so most teams used normal sort
of manual testing most in fact while all
of them use manual testing all of them
used code inspection manually code code
review some used dynamic static analysis
maybe twenty percent tried to use
fuzzing or some static analysis tools or
things like that so we wanted to
automate we didn't we wanted to again an
earlier version of the contest revealed
to us that manual judging for bugs is a
bad idea and so we wanted to make
everything is automated as possible so
you had to submit a test case that
showed that you had a correctness bug
but we needed a way to automatically
adjudicate your test case because it
could be you have a broken test case so
where is that used to be a manual
process we then started using this
Oracle implementation that the test case
has to pass on the Oracle and then fail
on whatever target system your your yeah
system that you're targeting right so
then you could also submit bugs against
the Oracle and you could do that at any
time during the contest so this was
another advantage of making the Oracle
available during the build at phase
because people would submit test cases
to the Oracle and go wait that's not
what i thought the spec was saying and
then we would give them points basically
break it points for finding bugs in the
oracle for our most recent contest i
think that only happened maybe once
there was only what one bug in the
oracle whereas for this the first
contest the one in the spring there were
more bugs in the oracle in that case but
not too many you know wasn't wasn't too
bad a different problem she's trying
different problems from this frame of
the fall so we used like a json format
and that sort of thing for security bugs
there were a specific test format for
privacy and integrity bugs I realized
this is all abstract because I haven't
told you the problems yet
but basically they were a way that you
could prove that you've had you've
generated an exploit against this system
because you know something you shouldn't
know right that basically you'll have
you'll be able to observe the execution
of something involving secrets or
involving information that we organizers
generate and the execution at the end of
it you'll be able to demonstrate that
you learned a secret or that you
corrupted some outcome that you wouldn't
have been able to unless you violated
unless the specification sorry the
implementation had some bug in it that
you exploited Oh to follow up one more
time on that Oracle do you think that
you're getting better at writing secure
software and that's why the Oracles have
your bugs now no I think this process is
actually improving you all know I think
that this process teacher taught us to
write unambiguous specs better that
basically when there were too many sort
of knobs on the specification too many
options or ways that in the spec
manifested so you can think of this as
like a library with a complicated API
rather than a library with a complicated
internal design but a simple API
complicated api's are bad you just you
didn't implement it the way you thought
you did so simple api's are better and
you're just a less likely to make
mistakes so yeah so the most again the
most recent one the API you'll see is
way simpler than the one that we ate the
first one this was also important we
limited people to five correctness bugs
so this this is important for two
reasons one is it forces people to
diversify their attention in the break
it faze they can't just focus on one
steaming pile of crappy software and
just submit 10,000 bug reports against
it and you know say hooray they get five
so that way they are going to move on to
other submissions to try to maximize
their score and also because they only
get five because of this unification
process that I mentioned if somebody
fixes one bug and multiple test cases
pass you only get points for one of
those tests not for all of them so if
you wasted your five test cases on
something that you know you just threw a
fuzz tester at it you generated five
distinct test cases that we're all
basically the same crash you're only
gonna get points for one of those so
this is going to discourage people from
acting that way and they're going to try
harder to find into
and it bugs which is good for us because
we want at the end of this to have every
piece of software really carefully
scrutinized see I think so he'll be part
of the designing secure software is I
think a lot of intellectual effort goes
into crafting nicd eyes and you just
take that problem away from students is
there some variant of the context that
you can run that actually you know has
an even more abstract spec that leaves
the students the task of trust of
crafting security I think that well that
would be worth thinking about I'd be
happy to talk to you or anyone else
about that I think that that approach
will not scale while still allowing you
to do an interesting controlled and
quasi controlled experiment because
basically then if you're now altering
the API it's hard to compare one student
against the other and it becomes much
more subjective in the judging of who's
the winner and who isn't so a lot of
like hackathons that students
participate in now this is the way it is
they have they get to sort of come up
with a design themselves and then these
judges go that's the best system or it
isn't and we wanted to stay away from
that kind of manual intervention and
have it be you know they're implementing
exactly the same thing and their varying
a few variables in how they implemented
it so we can just really compare one
versus the other which process was
better but you're right we're leaving
some stuff on the table like did they
come up with a better design at the
level of the API that's true okay so
then the fix-it round I've hinted that
this already many test cases might
identify the same underlying bug so for
example the buffer overflow that's here
right I could exploit it by providing
lots of strings that are more than 10
characters to overflow buffer wistar
copy and if I was an enterprising break
a team I would just make a thousand test
cases that all exploit the same bug and
go hey I get points for all these test
cases so we don't want that we want to
make it so that you only lose points
once for any test case that exploits one
bug so the way that we do that is we say
in the last round to fix it round a team
can fix the bug right so they can
convert stir copy to stir l copy and
and eliminate that bug and then of
course all of these failing test cases
will now start to pass right the the
program will reject whatever buffer
overflow is happening from before so
that creates an incentive well to fix it
round it there's an incentive for the
build a team to fix a bug that causes
multiple test cases to pass and there's
an incentive for the break a team not to
test the build it teams because then
they only have five test cases they
might lose some points that way so now
what we've done is we've changed the
problem of automatically figuring out or
manually trying to decide our multiple
test case is the same to now us judges
judging whether or not a fix is morally
fixing one bug or many bugs yep
incentives just submit one bad fix will
actually fix several bugs right so
that's what this is saying so we have to
judge whether that fix is morally one
fix or many fixes this is manually done
so I don't have it on a slide but for
the most recent contest we had sorry not
the most recent one the one in the
spring we had 24,000 test cases
submitted 15,000 of them were auto
rejected by the Oracle so that the
Oracle said no these are bogus test
cases so of the 9000 that are left about
2,500 of them were fixed at some level
and of those 2400 only there were 375
fixes so there was a 6x reduction in
number of test cases to number of fixes
and we could have basically inspect
these fixes in a couple of minutes and
you could tell straight away whether
someone was doing too many or too few
and 30 fixes were rejected out of the
375 okay well what was rejected would
like pound to find string copy de
stirring el copy so that your 30 cases
would that be rejected or accepted so I
didn't see that i know i'm not sure what
i would do in that case that's an
interesting question most of the time it
was like there there might have been
several bugs involving like argument
processing or something like that and
people might have fixed an argument
processing sorry there might have been
like an argument processing bug and then
bug in formatting like people didn't
handle zero size logs or something like
that two parts of the system that were
clearly very unrelated that people
checked in one fix for so then that was
rejected but something that's I mean we
had to actually think hard about what is
morally the same bug anyway right is
like you said is are all buffer overruns
morally the same bug because it's a
class of bug even if they occur in
different pieces of functionality or is
it sort of functionally there's some
unit here and there's some unit over
here and maybe it's the same error but
it's two different units same kind of
error but it's two different units so
mostly we went for the is it two
different units in judging but most of
the time you even have to make that
determination lots of these bugs are
very they're like two or three line
fixes and so it wasn't very complicated
yeah did the teams all get access to the
test cases against there yes yes I had
access to all the test cases and then
again because they were on a vm they
could run them themselves and see what
happened at the end of the break of
phase you got started fixing face
Jonathan's your son no you're that's
what I question ok ok so then for break
at scoring every bug is worth p points
so it's basically the same points that I
said for the build its side basically
you get em over two points for a
correctness related bug endpoints for a
crash and to m4a privacy or integrity
bug but then you divide whatever you got
by n where n is the number of teams that
found the bug so if you had 100 teams
and four teams found the same bug then
you'd get a hundred divided by 2 100 is
M and it's divided by two because its
correctness bug and then four teams
found it and so you get twelve and a
half points
so again what do these incentives try to
cause to happen you should look at
unexamined submissions because if nobody
else has looked at them it's more likely
you'll get more of the points you should
look for hard to find bugs if you just
take the easy ones everybody else is
going to find the easy ones too and
you'll have to share the points with all
of those people so you want to dig
deeper and not just in breadth you
should look for security relevant bugs
because you're going to get more points
for those and then finally you you don't
want to tie your own hands behind your
back by not submitting distinct bug
reports because again those will get
unified the participants look at what
submissions other people had already
examined or so they basically choose
randomly is the only way to fix
something that's unlikely to have been
examined I suppose so yeah I guess the
point is you choose randomly is which is
to say look at as many submissions as
possible right if you if there's so many
submissions and you leave all if you
look at the first five it could be that
the steaming pile of crap is somewhere
in the other 55 and you really ought to
be looking at all of them which again I
think is why it makes sense that in the
results the more people you had to do
this work the better off you are going
to be so with your scoring system it
worked two teams and we're thinking of
collaborating and I'll sure we'll give
you our bugs if you give us yours we our
world will trade an equal number of them
in the case where were the only only one
of us found a bug we both found a unique
bug whereas two teams combined we're
still getting as much currency out of
the system ah but if anyone else has
found any of these bugs we are strictly
increasing the amount our share of the
currency so this has this unfortunate
property that they extremely good to
cheat it never hurts users in the
mission so so you're saying I'm going to
plant some bugs no no no true teams
collaborating along a bug in terms of we
can sure what we found with each other
so what what encourages us not to share
well hopefully this scheme if we share
you're going to be giving out less and
so in multiple teams report bugs they
should be worth less but they're
actually worth the same you're just
dividing things up more so all right so
you're saying to break it teams
collaborating true sure they both find
each find some bugs they share the bugs
so they they they are now going to get
half the points that they both report
the same boat no no they're very nice
collectively they're gonna get the same
number of points in the worst case which
is they're the only teams that oh sure
sure so in any other case they're going
to get more than a greater share of
points than they would have otherwise
God collectively because because whereas
before let's say there were originally
it's just I were the two teams
collaborating and David here is a third
team yeah and I I david and i found one
bug in you and david found a bug so
before we reach getting the two of us
before we collaborate we reach getting
half now we collaborate we reach getting
two thirds and david only getting a
third of it mature you're also
potentially leaving points on the table
because you're sharing them with
somebody else so if you had been by
yourself maybe nobody would have found
the bugs that you found and now you've
given away half of that points from that
bug because you collaborated with
somebody else because now they know
about it and but because we're trading
were only if you can find more than five
bugs right there's a you're assuming
that there's there's so many bugs that
they're not a limited commodity it's
just assuming the tombs collaborating
find unique bugs that they that the
other 11 you wouldn't have found yeah
right great which maybe that doesn't
happen but I think as long as you
believe that you're better than the
others you're like hey like I break with
anyone that's gonna I'm gonna it's gonna
go my score it is to the reason is that
you increase your score no and also
you're talking about you're talking
about combines boys you're strictly
increasing the combined scores sure it's
right but now you have to show the prize
money i mean if if you are cutting your
individual score down you are now
basically why your bid now basic cutting
your individual score down because when
i when i share right let's just say
let's talk about
at the end progress because I I'm sure
there's something there but I don't okay
so here's the infrastructure that we
have for running the whole contest so
basically it just shows you all the
elements that I I highlighted before
maybe the most interesting thing is that
the front end application is written in
Haskell and it uses the Li oh security
checking framework the goal here was to
make it to the contest infrastructure is
not as easily hacked the code is not
available so people can't look at the
code and then try to find bugs that
might be there and maybe maybe using
Haskell and maybe using li OU will make
it harder for people to whack our
infrastructure and cheat to win the
contest that way ok so so the first
programming problem was to build a
secure log so this is one that we did in
spring 2015 and the the the idea was to
support two operations appending events
to Anna law to the log and querying the
long and appending events the the events
were for a hypothetical security system
in a museum where guests and employees
would enter rooms at various times day
and leave the rooms and the queries
would ask like well where is this
employee been all day or what people
have been in this room things like that
you had to use an authentication token
to add or query at events to or query
events from the log and it was up to the
participants to interpret what
authentication token meant but they knew
that the adversary would potentially
have access to the law without access to
the authentication token so they needed
to keep keep in mind that threat model
we didn't say you should use encryption
we just said there is this
authentication token these are the rules
this is this this is a threat model and
go for it so a canonical implementation
would be used authenticated encryption
so you can enforce both privacy and
integrity and you need to have some way
of effectively encrypting the whole log
that encrypting individual records would
be a bad idea because you can duplicate
records and therefore violate integrity
that way so the interesting question was
how we would auto test security
bye right to show that you violated
integrity or confidentiality and the way
that we did that was we generated for
each implementation logs using the log
append functionality for that
implementation and we provided a
transcript of all of the records that we
generated but without the authentication
token and the break it teams then got
these say 50 logs and a whole bunch of
transcripts for those logs and then what
they could do is they could submit a
test case that showed if I modify this
log I can make a query that would have
produced this result produced this
result instead that a correct
implementation should have rejected the
modified log entirely rather than
accepting the modified log and then
having a bogus answer for
confidentiality we did the same thing
except that we didn't provide the
transcript so now you just have a log
and no authentication token and now your
job is to submit a query that succeeds
on this log and if you do then you've
shown that despite not having the
authentication token you were able to
learn some information I mean each
submission had to tell you how to find
the operation different reason no the
submission just said here's the log
that's a modified version of this log
and here's a query and we know from our
infrastructure which log that is what
the transcript was that generated the
law so we can use the Oracle
implementation with the unmodified log
to then figure out what the query answer
should have been then we run it on the
modified log and see what the query
output is and if the two are different
then that's an integrity much the
information was given to the brave teams
she said it was a transcript of commands
and the log itself generated by that
particular implementation she had the
same Tim queries in payson
you're generating yes yes so for every
every build it submission we generated
50 random logs with random usernames and
everything else in the middle hopefully
those weren't predictable they were and
then people would have found lots of
bugs against lots of implementations and
then they use that the log so there were
50 submissions and each of those
submissions was distributed to break up
teams with 50 logs generated without
submission right so the second problem
was a client server implementation so
the idea was a client and a server
represented an ATM in a bank there was a
so called off an auth file for mutual
authentication this was distributed to
the client and the server in advance by
the testing infrastructure and then
there's also when you created in a bank
account you got back so called card file
that again you can implement it however
you like that is the Authenticator for
that client sorry for that bank account
so a canonical implementation would use
authenticated and encrypted
communications where the off files like
the banks publicly public key so that
you can mutually authenticate the card
file should be a suitably large random
number so that they're not predictable
and you want to protect against replay
attacks using nonces oh I should say
that the the the security model is that
you can have a man-in-the-middle so the
we're assuming it's an unsecured Channel
and so you need this is why you need to
encrypt and the man in the middle during
the break at phase the break of teams
watch it right this man in the middle
and they'll stick it in there and the
man in the middle can then wage attacks
that these defenses will will protect
against so the way that we yep are you
allowed to so you said that all the
break it stuff has to be submitted in
test cases hmm but have you ever
considered allowing some sort of like
social engineering attacks like phishing
scams and stuff because it reminds me so
I took Dave's class and that was a
problem and we had sent a fish
out to everyone and have gotten all
their keys and everything and I'm just
curious because that's a very real you
know attack that can help your time so
that's an interesting question I haven't
really thought about it I guess I didn't
think about it because I'm focused on
the how do you build software correctly
rather than how you build software that
use humans can use correctly yeah it
would be interesting to think about how
to work that in I mean the principle
that I do so far is to try to automate
as much as possible and so to the extent
that you can't automate it's hard to
make it scale but yeah it's worth
thinking about think about that right so
so basically the man in the middle
provides an interesting twist on on
these attacks so for an integrity attack
what happens is the man in the middle is
able to script a command server that
indicates commands that should be given
to the ATM and then the man in the
middle will be able to watch the
messages that go by modify them have its
own messages and everything else and so
what can happen is it can say okay
create an account the ATM will give you
back say the card file now deposit some
money now withdraw some money right that
will be the commands that go to the
Command server which will then use the
submissions ATM client to send those
commands but then the man in the middle
could corrupt those messages to attempt
to get the client and server to accept
whatever those corrupted messages are
but to get a different outcome so for
example if you weren't protecting
against replay attacks then what you
could do is you could replay the
withdraw command and there as a result
when you play just the script of the
create account deposit withdraw wit
without the man in the middle you'll get
a different outcome at the end right
because now you'll have withdrawn the
money twice so we use the Oracle and the
script to define what correct behavior
is and if the man in the middle doesn't
produce you know basically a hey wait a
minute there's something wrong with this
protocol kind of behavior then that's
evidence of a bug and for
confidentiality it's it's similar except
the test case format is you are able to
learn a secret so basically we generate
a couple of secrets at the beginning you
can refer to those secrets Simba
le in your test cases for example to
create an account using one of the
secrets or to store a balance with one
of the secrets and then if you can learn
what the secrets actual value is by
watching the messages go by then then
you've breaking broken confidentiality
okay so here are the results oakshire so
aaj do you allow them to rely on trusted
today so that's they just use HTTPS too
sure yet yeah you could use many people
did so you could use a I mean the
simplest thing to do is you could use
something like s tunnel to just send
your communications across yeah yeah
okay right so so then we analyze the
data we did linear and logistic multiple
regression to just get a basic idea of
which parameters correlated with
different outcomes so we looked at these
as the dependent variables we looked at
ship's core final score and presence of
security bugs so ship score is the score
at the end of the build around like the
score you'd have if you shift your
software so how well does it perform how
many optional features does it have
final score was the result at the end of
the whole contest how well did you do
and then presence of security bugs was
did you have a security bug or not in
your system the break inside we looked
at break at score prior to fix just sort
of how good were you at finding bugs
regardless of whether you had to share
them with somebody else and then we also
looked at number of security bugs you
found as opposed to number of
correctness or other bones so these are
the demographics of the participants
broken down into the three contests so
the two fall contests both did the ATM
problem and so they ran concurrently one
was with whatever it is I guess it was a
nine sorry seven open teams and then
this other one was with a Coursera thing
we ended up merging the contents because
we had so few participants here right
and you can see the different
demographics so be the interesting thing
is that Coursera these are more mature
students right so the average age of
Coursera student was 35 they had 10
years of programming experience so
sort of different than your typical
undergrad population I guess and you
could see the different team sizes and
so these are the programming languages
people use they're grouped according to
statically typed safe dynamically type
safe or the black sheep C and C++ so
yeah I was kind of surprised that Python
was so popular but in retrospect maybe I
shouldn't have been if you you know
python is a good quick and dirty
language there's lots of libraries and
such and Python it's a language that a
lot of people know so maybe not so
surprising and I was sad that so there
were so few functional or related
programming languages so basically F
sharp o camel Scala those are all one
implementation I love that PHP was used
successfully to build a client-server
application so they use PHP for the ATM
bank thing that's pretty cool right
there were plenty of CNC postpones sorry
yeah I suppose that's pretty pretty cool
too somebody tried to implement started
a late jack implementation they didn't
get very far good somebody tried rust
and didn't qualify they didn't manage to
finish what is using javascript in fall
2015 which you seem to be surprising
given yeah how knowledge is hahaha no
Jas that's true that's true and yeah
we'll see for the next one when I'm so
in order to make the data analysis and
reliable given the amount of data we had
we picked our independent variables in
advance based on hypothesis we had about
what would correlate with success or not
so there are lots of questions you could
imagine asking but we were sort of
limited to like nine or ten variables
before we felt like you know a p-value
of point 05 is supposed to mean one in
20 that it's attributable to chance but
the more sort of fishing you do
for independent variables the more
likely it was actually chance and even
if it was within point 05 so the way to
defend against that is to just not look
at the data and then decide what the
variables are but do the variables in
advance and try to pick them in a way
that you hope that the outcomes are
meaningful so this is very scary when
you put like two years into building
this infrastructure and you have to pick
these variables and hope that you get
something useful in the end but anyway
that's what we do yeah we'll set a lower
threshold right you could and then you
could have your paper continuously
rejected because people are like wait
why is it not point 05 and also there is
only a limited amount of theta here so
you pick too many variables basically
you torture today that you'll get
something out of that's right that's
exactly there's you can't just this is
already quite a lot yeah so we did a
standard power analysis to find out how
many degrees of freedom we had and the
fact that we have many boolean or
categorical variables and not numeric
variables so we did all the standard
stuff to come up with this number and we
ended up using this process for the
linear regression where you sort of
systematically remove variables that
seem to have less influence so that the
final models have even fewer variables
than this okay so anyway you can see
you've been reading while I've been
talking yeah you can imagine why these
variables might might matter okay so if
we're looking at ship score a lot of the
factors you'll see on these tables have
to do with their just differences
between the fall and spring contest just
because it's a very different problem so
measuring performance and so on just
which contest you are in made a big
difference in this case it didn't happen
to but you'll see that in other cases so
I won't spend too much time talking
about it here what this is saying is so
the coefficients over here are linear
coefficients so basically this is saying
that if I sum up all of these variables
with those coefficients I'll yield the
ship's core when I get to the end so the
biggest benefit that you could have had
is by being someone who was on the
Coursera side of things rather than not
Coursera that Coursera people just did a
lot better for some reason if we go
beyond that then what we see here for
dynamic vs
statically typed C and C++ were made as
the baseline and the way that we set up
the variable and so you took a penalty
basically if you wrote in a statically
or dynamically typed language in terms
of your your performance score so
basically we were you were able to look
and see what was that difference
attributable to and it was not to
tribute also optional features it was to
you just one performance more often and
I'll show you a chart in the middle
minute if you had fewer lines of code
you took a penalty to ship score I think
that this just correlates with C and C++
that you tend to write more code in a CN
C++ program and if you wrote a C and C++
program you're likely to have a higher a
better ship score because you have
better performance so you can see here
in this chart here is lines of code at
the bottom and here are the three
language categories i showed you before
and then the y axis is ships for so you
can see they're all of the blue dots
they're all kind of clustered along the
top so in general if you program in c
and c++ your your score was going to be
higher at the same time you can see that
all language categories had high scoring
implementations that were as high as the
highest C and C++ one so it's not a
foregone conclusion you know they
weren't separated that C and C++ was
always a way better it's just was more
likely to be better in terms of lines of
code fewer lines of code ments a
dynamically typed language and so you
can see that that again correlates i
mean the the dynamic type and we did the
the regression does factor out the
difference between dynamic typing and
lines of code but not entirely so there
is a correlation there but it's you know
it's not a terrible convolution improve
what are you becoming lines of code oh
we use slow count so we just which you
know is a knows what language so it's
written in and if filters out all the
comments and
okay so we also looked at log likelihood
of a security bug so how if you had many
bugs submitted against your
implementation how likely wants it to be
a security bug so it turned out in the
fall contest there were just many many
more security bugs in our paper that
it's quite stark many many
implementations have a security bug most
everyone felt played as some kind of
replay attack interestingly you reduce
your chances of having a security bug if
the language is known on your team was
greater so if you on average your team
do 10 languages and you know you
according to the ones you listed as
opposed to five you were less likely to
have a security bug in my case and I I
attribute that to your problem this is
probably a proxy for being a better
programmer you know that if you know
more languages you just know how to
program in different ways and you
probably thought more carefully yeah
experience here that's your own nose
experience was considered and so it
wasn't a factor in mom so we know in a
list of independent variables I put up
before experience was considered but
when we went through this get rid of
process of throwing away variables that
seemed not to can be considered that
they were basically correlated with
other variables experience disappeared
right and you got a benefit in terms of
static typing so what this is saying is
that you this is about an 8 X less
likelihood so basically it's 1 over
point 118 because this is log likelihood
a less chance of having security bug if
you use the statically typed language on
a dynamic typing you would appear to
have some benefit but not statistically
city that the lines of code was not
coordinates
oh sorry you're right it was so this is
saying that you were more likely to have
a security bug if you had more lines of
code looks very very small very small so
if you in some ways this is again it's
going along with its indirectly pointing
to the risk of using C and C++ your
programs tended to be longer and so you
had a slightly greater chance of having
a security bug we're again seeing that
here seen C++ woman had this
disadvantage compared to static typing
but on the other hand you got this
benefit for performance right so did you
have enough it's a differentially C++
because you've been very pretty sick
sick C++ you know what you're doing we
we didn't do that you know we didn't we
didn't have that variables to
distinction so anyway I find C++ can be
pretty 68 this is there's some studies
showing that I the code expect to find X
number of bugs in every letter code is
this come to anything that are mmm so
this is about security bugs it's not
about bugs in total and it's about the
likelihood of having a security bug not
the total count so i'll tell you why we
didn't try to answer those questions in
a second so I don't know how that might
have changed things that people do that
coefficient for lines of code is point
zero zero one per line of her line of
current yes that's right sorry yeah sure
if you've got like two thousand lines of
code on average with that saying is that
it right if you if you add another
thousand lines of code you're going to
see an effect of the same size
dynamically typed wrist right right
that's right so this is why we didn't
consider a final score and so what you
notice what I talked about was ship its
core and whether there was a security
bug I didn't say well what you know what
was your score at the end of the contest
how well did you do and that's because
we really relied on this fix it phase
two not penalize teams that had lots of
test cases submitted against them but
they all turned out to be for the same
bug right they just forgetting unfairly
penalized there but unfortunately what
happened was despite the incentive to
increase your score by fixing many teams
just didn't bother so all of the open
triangles there are
are people that didn't fix any bugs even
though they were submitted against them
and you can see there's a clear trend
that is you did better according to
score so resilient score is basically
how many points did you lose during the
build it round sorry during the break it
round right so your ship's core plus
your resilience score is your final
score and so the best case you your
score would be zero and you can see that
all the teams with the best scores fixed
bugs but then as we start to move our
way down to really terrible resilient
scores people just gave up they just
feel like I'm so far away from winning
any prize that I'm just not going to
bother fixing anything or you know I'm
either clearly going to pass or not pass
this Coursera class and so I shouldn't
fix so we didn't know that this was
happening until we did this analysis and
so I have ideas about how to fix the
incentives to make it so one way to fix
the incentives is to make a lottery for
people that aren't going to win a prize
otherwise right so at the moment we have
the top two teams winning a prize so
what we want to do is we want to make it
so that basically there'll be three
lotteries with different categories with
far less points and you'll get more
tickets entered into the lottery the
better your score is so then everybody
no matter where they are will have
incentive to fix all the bugs and then
we can have more accurate scores but we
didn't we didn't do that in these
contents question ah these people know
this course of other teams before the
physic faith through or doing yeah they
did they did know because this you might
have just hide that yeah so um because
it looks like people were fairly good at
guessing whether there's anyone so there
is it was the case that during the break
it phase we were having live updates for
scores so that we wanted to encourage
break at teams to maybe this contradicts
an answer I gave earlier unintentionally
we want to break a teams to sort of know
which systems were being attacked so
that they would know to attack other
systems and then diversify covered so if
we didn't do that we would run the risk
that a lot of people focused that they
would have to do what Matt said and just
try to hit everything rather than notice
in real time what people were going on
it's also would prevent collaboration
yeah I suppose that's sure okay so we
should think about that yeah well could
you fix the incentive problem by having
an incentive to submit a fix along with
a bug that way you could automate the
process of for that beer purchase right
if you have to submit the fix and you're
going to get scored on minimum edit
distance over a number of bugs fixed you
can have the people who are submitting
the bugs provide the bug fix that lets
you determine how many bugs would would
have bug fix fix the bug fix quickly
that's oh that's an interesting idea
okay yeah I'll think about that um okay
so what what those submissions at the
top there what do they do so they did
stuff that you guys have been saying
already they used existing
infrastructure they used sort of good
practices and many of the Coursera
people saw this I mean they took a
crypto class and a software security
class before doing this contest and so
you know everybody should have known to
do this kind of thing but not everybody
did it which again sort of reflects real
life another thing that I thought people
that did well was they were careful
about internal API design so that they
could sort of write code that did
something simple and then added security
at the API layer by calling out to other
people's libraries or why not some
people didn't do any security that was
bad so they some people thought hey if I
just serialize the log records you know
I have some internal data structure and
I'll just serialize it to disk if I look
at that file with my editor who I can't
figure out what that's doing must be
secure maybe not so there's I don't have
it in this presentation but I have
another presentation where you can see
people score in real time so there's
this sort of process y which as they
make their performance better and better
it goes up and then the break it round
hits and then it's like you like
everybody quickly discovers the teams
that aren't doing any security and they
just hit the crap out of them and then
there's scores go all the way to the
basement of this thing that I mentioned
before about encrypting individual
records rather than the whole log or
using
you know recursive hashes or something
like that so that you covered the entire
thing otherwise there are ways that you
could easily violate integrity so that
was common mistake and then for the ATM
problem oftentimes insufficient
randomness was an issue that they didn't
make sort of big enough pin numbers or
they use predictable keys or they didn't
bother with the mutual authentication
phase so that a man in the middle could
be a man in the middle and pretend to be
the server stuff like that this was fun
the deterministic crypto that basically
every you know the same plain text
always encrypts to the same ciphertext
so then they could basically do probe
attacks to figure out what this
encrypted men during a particular run
they could just do this was one thing
that was really nice I think I think
this is why there were more breaks
against the the client server because
the man in the middle could do
interactive attacks whereas in the
secure log you couldn't do that you had
to sort of learn the answer offline and
then just submit the report ok so for
break it many of the same variables we
also asked where you also build a team
and did you use an advanced technique to
use fuzzing or static analysis or
dynamic analysis or something like that
so this was the distribution of breakage
scores so higher is better and they're
sorted that way so you can see in the
fall sorry in the spring which is the
long thing scores were generally higher
people are finding more bugs but this is
not too surprising because there were
more submissions or like 65 teams as
opposed to 40 teams in this case but
here the number of correctness bugs that
people found was much higher whereas
down here most of the bugs were security
bugs and this chart also shows the
impact of fixing so all the circles
there are what the score would have been
if we accounted for fixing so this is
the score prior to the fix-it phase so
you can see in many cases like this guy
whatever attacks he did nobody fix those
bugs and so they stayed up here where's
this guy who had a very similar score is
score fell down here because the teams
that he attacked actually fix their
votes
what did well lots of things showed up
in the model but we're not statistically
significant coding experience on a break
a team was pretty close to being at the
point 05 p value but the most
significant thing was team size and this
makes sense because it's very easy to
parallel eyes whacking stuff right you
have 50 submissions five team members
you take ten I take ten and so on much
harder to parallel eyes effectively
building because there's design and
coordination and everything else so
that's not so surprising and then in
terms of number of security bugs found
same thing team number of team members
is really helpful also being a bill that
participant was really helpful so those
teams that didn't participate in the
build around just were less likely to
find a security bug no I've had examples
of the static analysis their news yeah
so people used fine bugs for java
programs they used various fuzzer
several teams use AFL for fuzzing and
remember what else and i'ma Royals I
mean mostly to notice that advanced
technique didn't show up in these models
and so I'm not I'm frankly not surprised
because the C and C++ teams in general
didn't were good and they didn't have
many memory errors that fuzz errs found
and these sort of shallow static
analysis tools did not give you any
advantage to finding these sort of
semantic security bugs that I was
talking about so I've again when you
when you hear me sit when I hear myself
say it this is not so surprising but I
think I've been parroting sort of Oh
static analysis we should all use it
because it's going to make our stuff
more secure I mean at some level yeah
but it's really low hanging fruit secure
that stuff that it's the design level
that's really serious issues it's not
going to find that stuff and so manual
code review and testing and stuff was
more useful right and this is just
showing the breakdown the top line is
people who were built it participants
the bottom line is people who aren't
until you can just see people found more
security bugs that way okay so these are
the summaries best performance C and C++
lower chance of the security flaw using
a statically typed safe language
and your higher overall score would come
with diverse programming background and
shorter code team size and knowledge of
C was not a factor for build it and then
break it increased chances of finding a
security bug if you also took part and
build it and you have higher overall
score if you are a larger team and
interesting the advanced techniques
didn't help so Brian raised this
question of minimal fix how do we could
we auto test could we automatically
figure out the multiple test cases with
the same bug I would love to be able to
do that it's just very hard to do it
when programming language is not fixed
but even if it was fixed it still I
think very hard to do it may be a more
tractable thing would be to sort of
somehow automatically figure out if it's
a minimal fix but I like your idea of
submitting the fix with it so yeah then
then your basic yeah maybe that's sort
of the best case but it would be cool if
we could also may be automatically
figure out whether or not we thought
there were minimal fixes but again I
don't really have any good ideas for
this because what we really want is a
way to deduplicate the bugs okay so our
next contest is going to be in September
we're gonna again we'll have it with the
Coursera class but we're going to try to
have it open to worldwide participation
in the past it's been for us
participation because we were funded by
the NSF but now that funding has ended
and we have a bunch of corporate money
so we're going to hopefully open it up
to anybody in the world and it'll be a
new problem and yeah hopefully we'll
we'll get more interesting data and see
what it shows so the prizes were well if
we've had at least ten teams the prize
would have been twenty five hundred
dollars for the winning teams and $1,500
for the second-place teams both at the
building and break it level but we
scaled those prizes by the number of
participating teams it was scaled down
to seven so I don't remember exactly
with the money and that was can you
describe the week things will put out
there at brawl
what would they come from so um I have
to go look but off the top of my head
the winning teams were I mean they were
people that were professional experience
software developers I've been the course
Arab side who are good programmers who
maybe were just learning about security
now but a lot of their sort of
industrial-strength code development
practices serve them well on this side
the let's see what else yeah and people
who my rough recollection although we
didn't try to statistically measure it
was that people who did well in the
Coursera classes that scored higher in
the software security and crypto classes
did well in the contest also but I think
in some ways that just correlates with
seriousness you know that people who are
kind of taking it all seriously did well
the people on the break outside that did
well figured out how to parallel eyes
things a lot so they were they were
there were just a lot of them and they
were good at generating lots of st.
manual tests just throw at every
implementation and then retargeting
breaks if they found for one
implementation like once you find a
replay attack against one team you could
then try to you use that attack but
modify in some way to work against the
other team so I think they teams were
that were adaptable did better as well
yeah did anyone submit there was was
there ever any submission that no bugs
or were found oh uh I don't think so I
think that everyone no security bugs
there were some with no security bugs
but i don't think there were any with no
bugs like if he looked at that resilient
score chart there was nobody had a zero
score that it was it was always at least
something I might have missed this did
you have min max limits on Team sizes um
yes for Coursera we maxed it at six and
but very few teams went that high I
think that I think it said the average
team says was 34 Coursera it was no in
fact I think that Simon right i think it
was like two and a half four horse arrow
name is 34 the 0
I think this makes sense this is a
four-course era because a lot of people
didn't know each other so we had sort of
this dating phase at the beginning where
you could try to find people that spoke
your same language or in the same time
zone or had complementary skills but I
think people were were worried about
working with people they couldn't trust
and so in general once you've found one
or good teammates you just stop there
you didn't want to push it yeah are you
find it continue running this definite
player jo a plan to handle um I don't
want to run it indefinitely it's a lot
of work to run these contests and then
coming up with a good problem
specification is super hard and yeah so
I'm plant we're planning to do to the
fall contest and then we'll see I would
like to open source the the
infrastructure but to do that I think it
won't get used if people have to program
in Haskell to use it so I want to make a
framework for developing problems we
have to look at any of the
infrastructure code but that has yet to
be done and it would be nice to find
some collaborators or helpers to do that
ok thank you for all</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>