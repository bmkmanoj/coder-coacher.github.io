<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Failures and Other Challenges of Big-Data Analytics | Coder Coacher - Coaching Coders</title><meta content="Failures and Other Challenges of Big-Data Analytics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Failures and Other Challenges of Big-Data Analytics</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nuarD2rXECY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
it's great honor to introduce professor
verizon skur from university of
washington seattle most of you know her
she did a PhD at MIT she works in the
area of distributed systems and
databases has a number of hours it's
hard to name all but he's a she's a she
won the microsoft new faculty award
carrier award and the ten-year most
influential paper in addition she has a
number of best paper award she works in
the area also of scientific databases
which i forgot to mention so today she's
going to talk about the big data
analytics so without further thank you
thank you sir Ajit and thank you
everyone for coming so before I start
I'd just like to acknowledge that the
core technical pieces that I'll be
talking about or actually the hard work
of two of my students prasang and young
tool and also that this work is
sponsored actually interesting about
both the NSF and action Microsoft so
thank you so what is the motivation the
motivation of course is that everyone
nowadays has a lot of data being on a
university campus we talk a lot more to
a domain or natural scientist more than
you know industry and all these natural
scientists or generating data is just an
amazing scale and rate for example
astronomers or building this
increasingly large telescope that will
simply you know serving the sky and
accumulating very large collections of
images and they want to analyze these
images and here for the upcoming LSST
which is kind of the new survey we're
talking about you know petabytes of data
similarly in other domains such as ashen
ography there are you know both
collecting data from sensors and also
simulating you know the environment
simulating the oceans producing large
amounts of data and in other areas such
as biology we have you know new
sequencing new lab automation techniques
so the bottom line is that scientists
are just producing a lot more data and
therefore they need tools they need help
to analyze all this data and of course
as you perfectly know better than I do
that this need does extend beyond
science so if someone has a large amount
of data what can they do with this data
so there are several solutions that
already exist I could use a parallel
relational database management system
perhaps greenplum I could use a
MapReduce type system such as you know
how duper scope or one of those systems
maybe I can use one of those you know
new prototype parallel data processing
system such as side DB and there are
many many challenges many interesting
challenges and the one challenge I would
like to talk about today is really fault
tolerance so what it is about fault
tolerance so if I'm going to analyze a
large amount of data clearly failures
are going to occur while I'm processing
that data in a recent publication Google
reports that they've see approximately
five worker deaths which means kind of
task deaths for each MapReduce job so
definitely failures are occurring and
the question then is what should we do
about these failures and in the
community really have kind of two
standard techniques that are pretty
extreme in terms of comparison so one
approach which is characteristic of
traditional parallel database management
system is to use kind of a pipelined or
if you want streaming execution so here
let's say we're going to start with a
query that is going to start with the
data set maybe select some values drawn
with another data set drawn with yet a
third dataset perform some aggregation
at the end if I run this in a parallel
database I will typically start reading
the data from disk and then I'm going to
stream the data directly from one
operator to the next as i do my
processing i'm not going to write any
data to disk I'll just kind of stream
the data right through all these
operators which can be spread across a
kind of several machines in the cluster
so this is great but if something
crashes during the execution of my query
well I don't really have a choice I have
to restart the whole query so there are
several advantages one advantage is that
i don't actually pay any overhead of
materializing checkpointing etc I just
stream the data right through the second
advantage is if my operators are not
blocking I do what is called for example
online aggregation online query
processing well I have the possibility
to produce results incrementally which
can be very nice if I'm going to analyze
large
data set to see the results
incrementally and to be able to maybe
stop the computation if I have seen
enough and I'm not happier or happy on
the other hand on the negative side
clearly failures are going to be costly
which at small scale is not a problem
but if you're going to scale this up
then this might become a problem so this
is one strategy commonly used today on
the opposite side we have MapReduce
types of systems where the idea is to
use blocking to be extremely cautious if
you want an to use blocking query
execution well what we're going to do
start from the data is again in our
cluster on disk and we're going to read
the data process it and write the
intermediate results back to disk then
we're done with the first operator we're
going to schedule the next operator that
will read data from disk process it
right into disk and so on the beauty of
this technique is that if something
crushes we don't have to redo all the
work we can only read the data from disk
again and just reprocess the operator
that failed so the advantage of this
technique is that recovering from
failures is much less expensive than in
the previous case because we only
reprocess the one failed operator on the
other hand there are kind of two
negatives one is that I would do have to
pay the overhead of writing all this
data to disk but the second negative
aspect is that I can't see any results
incrementally because I'm fully
processing each operator and blocking I
really have to wait for the whole query
to be done before i can see any any
results so the question that we answered
that we ask yourself is what would be
kind of a nice compromise between these
two techniques what we would like is
preserve the ability to produce results
incrementally we don't want to add any
extra blocking just for the purpose of
fault tolerance and second what we would
like is to have a fast execution time
assuming that failures are going to
occur so how can we do this so actually
can we achieve this and actually the
answer is that yes we can achieve this
because there exist several fault
tolerance techniques and you can use
them in a non-blocking fashion in fact
there are many ways in which we can
achieve that so there are many
techniques I'm kind of listing three
here
I'm going to describe them in details
later but the key question that we asked
is okay great so I can take a parallel
data processing system I can make it
fault-tolerant using one of several
techniques so which one should i use and
does it actually matter so as the first
set of experiments to kind of drive the
this research we actually started with a
small scale claas through kind of 17
machines and we're going to run some
simple queries and actually try to
observe what happens when failures occur
and we use these different techniques
for fault tolerance so in these
experiments we're going to run simple
queries we're going to inject exactly
one failure about halfway through the
execution and we're going to fail each
of the operators separately in different
executions and we will average out the
total run time and we're going to try to
use different fault tolerant strategies
so the first fault tolerant strategy
would be simply to do almost nothing and
if I have a stateful operator and the
operator crashes we're going to restart
the operator from from the beginning but
if I have some stateless operators what
I can do let's say I have a selection
operator in my query plan that operator
crashes well if i restart the operator I
don't have to reprocess everything I can
simply skip over all the data that
already processed and I'm going to
continue processing so that's very
similar to doing nothing restarting the
whole query plan except we are going to
restart only one operator at a time the
operator that that crashed so now small
scale cluster we're going to compare so
here we're going to have the different
fault tolerant strategies and on the y
axis we have the wrong time the blue is
the run time without any failures and
then the red part is kind of the extra
time we spent due to failure and
recovery so if I just run the query
without any failure this is the wrong
time that we get if i run the query and
i inject a failure of roughly in the
middle of the executions you can kind of
see the expected wasted time because of
the failure is going to be the red part
so in the case of something like a
parallel database if I just restart that
i'm going to add fifty percent because
in expectation i have
we do half of half of the work so what
if instead I use this kind of technique
of skipping over the input data whenever
just as possible so here indeed what
will happen is the wrong time without
failures kind of you know the basic run
time remains the same but in terms of
recovery I can speed up the recovery of
some of my operators I'm already going
to do a little bit better yes yes so
it's all the operators but the stateful
operators can skip over anything so they
have to go back and actually reprocess
everything because they have to rebuild
their state and you can see this that I
have kind of a little bit of improvement
only for that operator but that's still
one strategy that we can use yes this is
people in this case this is a we were
not releasing indexes in this in this
case the main thing here is Cano I'll
get to the details later of how we
actually implemented in in our context
because what you want is even operator
crashes it needs the ability to tell
whatever is upstream of that operator to
just resend a certain amount of the data
not resent the whole data in order to be
able to skip over some of the input
there's no state
also in this case to how much to skip
over so i'll get into those deals a
little bit later because i'll see well
basically show you how we implemented in
our framework so at this one let's just
keep it kind of a little bit vague just
to get a sense of does it even matter
that we that we check the right
strategies so this is just kind of one
simple strategy what if we also use the
MapReduce type of strategy what I'm
going to do is I'm going to write data
to disk at the output of each operator
the way MapReduce does it but I don't
actually have to do it in a blocking way
what I can do instead of writing all of
the data to disk and then sending it
downstream i can write a little bit of
data to disk and then send that data
downstream right a little bit more data
to disk sound the data then send the
data downstream if an operator crashes
the advantages all the data that the
operator consumed is available somewhere
on disk so you can simply replay that
data if the operator wants to recover
instead of a kind of propagating back
the failure and having to regenerate any
of the any of the missing data so does
this how well does this perform so if
you were going to go back we have again
our Korean actually forgot to say this
is a simple query that selects some data
joins with another data set joins with
another data set aggregates and this is
actually synthetic this is not a real
query just a synthetic query and it's
about i think this crew was about 160
million tuples that we were processing
so we had our restart strategy so in
restart what we're doing is simply if
any operator crashes we are restarting
the whole query in the skip strategy if
let's say my select crashes well I can
we start to select and skip over the
input data we're not using indexes we're
just going to skip over wherever the
operator started if the drawing crashes
well then the drone will ask the select
reprocess the select will kind of have
to crush itself three process but it can
skip over again all the data that we
want although the drone will have to
rebuild all of the state from the
beginning same thing for aggregate so
skip I guess if select crashes we can
skip over some input data if any of the
other operators crashes we pretty much
we start from the beginning in the case
of materialize we're going to write the
output of
like two discs right the output of
drawing if now thats a disjoint crashes
well we don't have to reprocess this
joint but the journey will just resend
the data from disk and that joint will
be able to rebuild its state from the
beginning and continuous kind of
MapReduce style how helpful it is well
we actually execute it is in this case
for this query it's pretty bad because
the fact that we have to write the
intermediate data to disk really
increases our run time in the absence of
failures of course once a failure occurs
we can you know we can recover much
faster because we only have to reprocess
the operators that failed but overall
for this specific query this is not the
best strategy there's one last very
well-known strategies and all these are
kind of common strategies we do not
invent any of these fault tolerance
techniques the last well-known strategy
from the communities checkpointing so
this is great for stateful operators
where we run the operator and every so
often we are going to write a copy of
the operator stage 2 disc and if the
operator crashes we can restart from
that last checkpoint which will include
the state of the operator and exactly
how far along the operator was yes so
bad in the previous slide yeah well it
is Crede you have to seek to that block
and wait for rotational latency I'm
gonna dis project and in principle to
find any blank spot on the disk right
there and make a note where you wrote it
so you should have to air force that
this movie that we made of that you had
that option sure I need to could also be
that you know we didn't optimize all
these techniques you know as you could
optimally maybe produce that right so
this is also something solid and you
just just start writing where your car
right sure sure that's next time around
sorry that's right okay in this case we
don't do these types of optimizations I
mean we don't we just kind of have an
open file the one thing we did have is
in this experiment is that each operator
partition has its own disk so we
definitely do sequential read sequential
writes but that's as much as we as we do
all right so and even operator fails we
restart the operator from scratch read
the state from disk and then process so
how well does this do so we're going
back to go to our query with the three
techniques so far and in this case a
chief we check point we
actually don't add that much overhead
because the state is actually not that
large and now recovering is extremely
fast because we only have to reprocess
from the latest checkpoint what is
interesting and this is kind of the
bottom line and our motivation is that
given a simple parallel query and even a
single failure we can get something as
much as seventy percent difference in
execution time if we choose kind of the
wrong if you want fault tolerant
strategy and the right for tolerant
strategy so the second question might be
so okay so is it just that checkpointing
is always the strategy that we should
use is it always the case that I should
simply checkpoint all the operators yes
so when you do the skill let's give to
be efficient you need to hang
efficient about it also use an index for
after all you're getting this
improvement are you rescan and just not
now you want to be able to seek to the
right location you don't want to scan
the whole input data that's right kind
of us is the offset exactly so here
we're kind of scanning from disk at the
beginning and we remember the offset and
we just restart from that yes remember
you said into it you could so this is
actually the experiments so it doesn't
decide but I forgot to mention it we
were using kind of our own we're using a
skeleton processing engine that all it
has is operators and fault-tolerant
strategies producer resumen user-defined
functions as system deco music
what's that started to make checkpoint I
need an additional contact with the
programmer to say what is it what is
even a right notion of the Martians
exactly and actually I want to get to
this that's right so what we did is we
initially let's just assume relational
operators we know they're inside and at
the end I'll show you that if we have
user-defined operations then we can make
assumptions that they cannot do much in
terms of fault tolerance and you can
still use our technique and you can
still get improvement so I'll get back
to the user-defined operations at the
end so this is very good question yes
music when I sing every little Jake
wondering yes it was a certain operators
would prefer to a checkpoint that's
another operator which has much more
state money would be much more expensive
and that's exactly where we are headed
so what we'd the question that we asked
and actually this is a purely academic
question what I'm going to present is
not something you could directly apply
but what we were just curious about from
you know just a science perspective was
all right so do I always check point is
there one strategy that's always the
best and the answer is no and actually
this is kind of the next experiment what
we did you simply replace the last
exactly the same query processes also
you know in the same number of input
tuples but we have a drawn at the end
and here the bars are completely
different in this case the strategy of
everyone just using you know skipping
over the input data when they can no
track pointing no materialization gives
you the best run time so the observation
here is that if i have a parallel query
the choice of fault tolerance technique
can have a significant of visible impact
on runtime even in the presence of a
single failure so imagine with more
failures the differences will amplify at
the same time there isn't like a single
well-known technique that I can just
apply uniformly so what you want is
exactly what you suggested we said can I
should I pick the right fault tolerance
technique at the granularity of each
operator and we actually went further
and the question we asked was can we
build an optimizer that will take my
query plan and my resources as input and
will automatically tell me what fault
tolerant strategy to use at each point
in my career plan and does it actually
matter would such a faltering optimizer
make a difference so that's the question
we kind of set off to answer any
the rest of the talk will will kind of
see how how we went about answering this
question does that make sense so far yes
photo acquisition is it just one RT
surveillance
so I'll get to this in a second kind of
exactly because that's something we can
just use the standard cost functions and
one of the challenges is here we really
try to get as close as possible to
estimating something equivalent to run
time and because fault tolerance the
difference between different fault
tolerance plans is not orders of
magnitude it's in the order of you know
with one failure maybe fifty percent
forty percent seventy percent and I have
to be a lot more accurate in order to
select kind of a good plan and one
challenge that these operators interact
with each other when they're running in
a pipeline so if an operator is like a
symmetric hash join initially it's going
to be the bottleneck will be half as the
input data is arriving but soon it's
going to produce so much tuples the
barrel length becomes the CP of the
operator so we try to so we had kind of
challenges in dealing with these
techniques and I'll come back to this in
our initial slides so the bottom line so
far is that we want to see if we can
choose fault tolerance at the
granularity of operators and if you can
make this choice automatically so this
is what we did in our optimizer that we
called FG opt and the reason why it's so
short is because the paper was kind of
over the limit so they need kind of
short name to squeeze the paper and this
is actually our in a last year Sigma
conference so the goal here was kind of
really two things first of all I need
the ability to mix and match fault
tolerant strategies so I'm going to have
a single query plan with one Operator
using one strategy another operate using
a different strategy so the first thing
we had to develop is some sort of
protocol that will hide all the details
of the fault tolerance technique and
have some sort of high level agreement
between operators such that if one
Operator crashes we only restart that
one operator and we don't care what
anyone else is doing for fault tolerance
so that was the first the first
contribution and the second one was
ready to have this cost-based optimizer
that we call it fault parents optimizer
the input to the optimizer is the query
plan so we first run the regular
optimizer get a query plan and then we
pick the right fault tolerant strategy
for for the query
and of course the question then is also
kind of how much do we gain so kind of
once we actually start to execute it in
parallel the way this is going to work
is we're going to kind of encapsulate
these operators we're going to have an
optimizer on the side and we'll have
this encapsulation with the protocol
between operators and the optimizer will
select the strategies and then different
operators will end up using different
strategies so the granularity will be
really kind of one operator in the
pipeline so let's first look at the
actual protocol that we use so the goal
is that we're going to have pipelining
we don't want to add any kind of
blocking I want the data to flow
continuously because if at all possible
we would like to be able to show results
incrementally we don't want to for false
tolerance to add any kind of blocking
stats the first goal that will be kind
of the goal of the protocol and at the
same time we need disability kind of you
people raised questions about figuring
that word where we start from what do
every process so the protocol has to
capture this so here's kind of a kind of
this is best illustrated with an example
let's have three operators or one or two
and 0 three our protocol really requires
that each operator or base for different
rules the first rule that we require is
that all tuples that we sent or all
records if you want between operators
must have some sort of unique identifier
like a primary key or it could be a
hidden record ID and the reason is
because we will when we restart an
operator we will want to tell the
upstream operator where to resend data
from so we need the ability to identify
these tuples uniquely so we need those
unique identifiers second we will
request that if an operator at any point
in time or two is allowed to go back and
ask I want to resend any kind of suffix
of its output data so i can say go back
and say please restart and send me
everything from the beginning please
restart and send me everything since
double number 225 and this operator has
to support this feature the easiest way
to implement this is this operator says
recent
everything from topple 225 this operator
can simply kill itself restart reprocess
and kind of send all the data just
delete all the output data until it is
the right couple and then continue and
of course this does assume that we have
determinism in the operator so we do not
support operators that have some sort of
randomization inside of them because we
will assume that it can restart an
operator and it will produce the same
output yes that's right so if you have
some sort of random seed and I can just
give you the suit again as long as
there's some way to restart and
regenerate the same sequence we're fine
and we discussed this in the paper i'm
not going to go into the details of of
that aspect all right in many cases if I
have if I require that an operator
remembers all its output that has the
ability to reproduce its output that's
fine but it can be actually quite costly
and in many cases operators once they
know they have made a certain amount of
progress let's say maybe this operator
chose to check point then this operator
knows that it will actually never asked
for some old tuples again well as an
optimization this operator can actually
acknowledge to the upstream operator and
say by the way I will never ask you to
replay anything before topple number 153
because you know it's my problem how but
I know that I will never ask you for
this and then if this operator uses
something like you know maybe metalizing
the output to disk then you can truncate
its logs it can do whatever options
ations it wants and finally the last
rule we're going to ask all the
operators to remember the last double
that they received so if this operator
crushes it can actually take a score
three and says okay I just crashed tell
me what is the last item that i sent you
and that you received and then this
operator will then ensure that it
eliminates duplicates and only sends the
new data yes
why don't you have the other optional
this operator itself to put the input
into the
or it can do whatever it wants so here
all I'm doing is I have four rules but I
don't tell the operators how to
implement them I just say that the
operator needs the ability and the
easiest way would be this operator could
do nothing except you someone requests
to replate could just crash itself
restart and regenerate it so we don't
put constraints on what the operator how
the operator implements those rules does
that make sense or was the question
different it is harsh it uh-huh if you
are you using
yes oh yes yes yes and you'll see just
on the next slide exactly so what kind
of the main observation here is that we
have these four rules but they don't say
anything about checkpointing
materializing the output they don't say
anything about how to implement it which
means as long as the operators can play
by these rules they can do whatever they
want and here's one example actually
with let's say checkpointing so lets you
go through this this example and
operator or to the middle one crashes
and restarts what the operator will do
is first it can actually ask the
downstream operator or three to say what
is the last couple that i sent you and
that 03 should be able to reply by kind
of the rules that we have about
identifying tuples uniquely and
remembering the last double d received
so now or to nose cut off when it
crashed with respect to the downstream
operator at that point or two can do the
following if it happened to save data
from disk on disk it can recover that
checkpoint from disk if you didn't have
a checkpoint while it restarts from the
beginning but it's going to basically
either recover state or start from from
cry from scratch and depending on which
of these alternatives the operator used
it's going to ask or to throw one to
replay a certain amount of data if or to
checkpoint it well maybe it will ask
only for the most recent few tuples and
then maybe it will be efficient because
maybe these few tuples are still in
memory at the preceding operator but if
it didn't and it has to restart from the
beginning well maybe it will ask to
replay all the data from the beginning
and finally one so two gets the replay
data from on one it processes the data
and waits until it catches up to the
last couple and then send only the new
data to the operator so what is nice is
that we have several nice properties
first the data can flow continuously in
the absence of failures we don't put any
constraints on when data has to be
written to disk there's no blocking the
data can just flow continuously at the
same time if an operator crashes we can
restart only that operator individually
and finally we don't put any constraints
as long as I can I have come up with my
own fault
strategy if it can be made to work
within this framework then I can use
that strategy for any operator so the
kind of late in many benefits and what
actually we did is that we show in the
paper how to implement the standard
strategy like checkpointing
metallization and skipping over data
within this framework yes yes good one
Rica Cory colors one has two parts of
food somewhere yes so actually what
happens in our fingers when a failure
occurs then we do block then we actually
block the whole credit plan yes without
a bit here I think about it without a
bigger than this exactly but when a
failure occurs we in GD block the whole
parade plan which is why we often have
this grass we have the wrong time
without failures and then the recovery
time we just added up because no one is
doing anything during that time mm-hmm
so one example you read the operator
could simply hold it in memory so if if
let's say 02 crushes but a 3 didn't
crushed and 03 could just have that
information in memory if or three
crashes yes so if it check wanted the
work so it depends what you did if Otto
starts checkpoints nothing crashes it
pretty much has to restart on the
beginning unless it's a stateless
operator if 02 is a selection then it
can just ask 03
this one so
I see so if I act a double at that point
I need to do something that will persist
yes exactly exactly and here it depends
on a failure model if you assume that
only like in this paper just for the
purposes of exploring the idea fault
tolerance optimization we restricted
ourselves to process failures we just
did not assume that disks will fail just
as a way to explore this idea without
too much complexity but if you assume
that discs can fail then perhaps or two
before it can acknowledge not only has
to write it locally but maybe has to
replicated somewhere else and then it
can acknowledge it but that would add
overhead your fault tolerant strategy
and then maybe twist this kind of you
know push the scale towards not using
any fault tolerance because you might as
well restart or as it's going to be too
much overhead at runtime system
it's these kinds of considerations
that's killed them to answer very very
low for tolerance kind of more the
overheads of introducing even moderate
granularity recovery become hot
they become complicated that's true and
here that was kind of a first step
toward saying is this even worth it
because if it doesn't work at that level
then there's no point going forward that
time say I'm saying it's not exactly
practical directly it's kind of just a
interesting scientific question on that
path all right other questions so this
is for that yes the databases for
suspending and receiving queries people
yes quality of operators establishing
contracts with upstream operator so that
again it will choose to check when yet
state and start at that point or in both
the contracting essentially along the
obscene opt to replace certain parts of
the state to be visually people in
operator so how does this fit in
comparison to also the case so the main
difference compared to a lot of the work
including this type of work is that this
is really meant towards enabling the
heterogeneity so there's a lot of
protocols that are kind of similar in
spirit but assume sort of homogeneity
everyone is checked punching everyone is
skipping over versus here we really
thought what are the minimum what is the
minimal set of primitives that we need
in order to decouple the choices of the
strategies but a lot of the principles
are acknowledging the double replaying
tuples they will kind of come up in
different protocols but typically
everyone is doing some sort of uniform
strategy or you might have all the
contract will be specific to a specific
pair of strategies forces here the
contract is independent of the pair of
strategies and that was kind of the main
goal other questions
alright so we've seen kind of most of
them we seen the protocol so the second
part which goes back to the other
question was with respect to actually
the cost model so here we want to make
the decision on the fault on and
strategy to use in an automated fashion
therefore we need some way to compare
the cost of these different strategies
so the cost function that we choose to
optimize for and you could pick other
cost functions what's the execution time
with failures so the cost formula is
going to be we're going to try to
minimize the time that we spend in
regular processing which is blue which
means the time for the first couple to
propagate through our whole pipeline
remember that we assume kind of we think
about non blocking although if an
operator if an AA traitor if an operator
blocks it's simply that it delays the
first Apple until it's done processing
so we have kind of time for the first of
all to propagate and then the actual
processing time then for the remaining
couples to go through plus because as
has been mentioned we block drink
failure recovery we're going to add the
expected kind of number of failures
times the time we're going to spend
recovering from these failures and this
is kind of reasonably standard cost
function to optimize for the challenge
though is when you started with standard
cost formulas we were getting highly
inaccurate results and like I mentioned
in a query optimizer the differences
between different query plans can often
be in an orders of magnitude huge so
even if the cost functions are not
perfectly accurate that's fine
sufficiently good to just pick you know
reasonably good plans here we are
talking about you know fifty percent
forty percent differences in runtimes
between the different fault tolerance
plans so we needed to be more accurate
in terms of the cost models for the
operators and the key thing that we
actually observed when executing our
pipelines is that there's kind of a lot
of dynamic between the operators in
particular an operator will not
necessarily produce like a selection
operator will produce it's a filter
right so i get an input double apply
some filter output the topple this is
going to be a kind of linear operator
that's kind of producing data at a
steady state but many operators start
empty and then they start to accumulate
state and they have this kind of
nonlinear behavior that when we were not
tring we had just kind of too much of a
difference in terms of the predicted and
the actual runtime so kind of what the
way we chose to solve this is by using
kind of a convex optimization framework
and to this model the operators with
constraints of the form if I put two
operators together while the input rate
for this operator cannot be any higher
than the output rate of the preceding
operator and the rate at which this
operator is producing tuples cannot be
any higher than let's say what how much
the CPU allows me to produce so we're
going to cut off model operators with
all these kinds of bounds and here's one
example for the symmetric hash drone
operator so the idea behind this
operator this is one example is that
we're going to read data as the data
appears for when we have an input double
we put it in an in-memory hash table
when I have a double from the other
relation I probed my hash table and I
also store this topple in my in-memory
hash table next up right next double
comes i probe the the other relation and
I store the tuples so at the beginning
when the operator starts running action
also if I assume that all the data is
available to me there's a certain
maximum rate at which we can produce
tuples which is how much CPU we have
available to us yes yes mr. clarify so
you have an optimal glass yes for each
operator or not what are you trying to
find it
yeah also this is yeah this is a good
question actually more clear with your
laser nose exactly so what we want to do
is for each operator we will decide
which of our end available fault
tolerant strategies to use social just
operators skip in our experiments we
looked at skipping track punching or
materializing and if i pick track
watching and have frequently should i
will check one check so when you might
realize your new metra is the output
when you check when you check on your
internal state so the overall
optimization is going to be that I have
my cost function and each operator has a
variety of fault tolerant strategies
available to it for each fault tolerant
strategies we need a model that will
help us assess what should be the cost
of that photo strategy we don't do that
which is either checkpoint or not and if
a checkpoint the only decision will be
the frequency yes are you give an
example thing difference between two
strategies could be as much as seven
Uranus yes so I would say that one
doesn't motivate a country doctor
perhaps exist one strategy then comes
within twenty percent of all yes so in
which case you don't probably need the
center I don't know maybe such as I
agree and actually just a kind of I
totally agree so in this case we really
wanted to explore the idea of the whole
optimization but I agree that in
practice you might want to take one of
those simpler strategies because you
might get with you know twenty thirty
percent of optimal the I to agree yes
this may be off subject but have a quota
John Massey saying some people don't
speak of a thing for ten percent my boss
Bob Davis would you say we do
unspeakable thing for three percent I
see ya so you can't something I did it
for product there is a big number pick a
number that's right but I agree that
this is this is kind of her thats I say
it's kind of more of a scientific
interest that we wanted to see how how
well does it work in theory mr. Edison
percent inside reasonable they may be
one that's always in the middle but uh
there's one that's consistently in the
other room is
if you need that any given strategy is a
lousy for some inputs which case this is
completely awful that's right so this is
at least we want to eliminate the bad
plants and we actually did a lot of
sensitivity analysis to see how
sensitive we were two different bad
inputs but the bottom and just kind of
going back to the cost functions is
simply that if I take an operator such
as a symmetric hash join if all the
input data is available then the output
rate is kind of limited by how fast
again process this input but typically
the input is coming at a certain pace
and as it goes I'm accumulating more
states i'm finding more drawings so
actually initially the output will be
limited by how fast the input is
arriving so when we combine these what
eventually happens to the operator is
that initially if processes as fast as
the input is arriving but eventually it
becomes CPU bound and then it kind of
cannot process any any faster once it
accumulated a certain amount of state
and what we found is that we actually
because this is something like twenty
five percent and we're talking about
differences in the order of forty
percent we had to account for this type
of effect in order for the optimizer
tracks you give us select good photo
lens plans yes look at all the brands of
each oh I agree yes yes and that's
possible we just didn't do it but
that'll be possible to kind of expand
the search space and look at kind of did
with the optimizer that might be
interesting mm-hmm yes your biggest
natural points they make perfect sense
the minimum state mm-hmm point so
example in a block-based nested loops
join you know that when you computed one
outer page and then your see your hips
it is essentially negligible these are
optimal points for doing a checkpoint
strategy so yes so take John is that how
this no specific choices were delayed to
the general framework so that will be
maybe one way to when we actually look
at the optimizer here we said just what
is the frequency and saying that if I
just pick the frequency without the
specificity of the operators that I'm
going to kind of get an average size
with a certain frequency but it is in
fact the case that for some operators
are certain points that make a lot of
sense we could restrict the option
measure to only look at these points and
say for these operators the frequency is
limited to being only one of the
following and then only get that those
best points indeed exactly so how well
does this model work so this is actually
you know this is actually our of a good
case where when we have different
operators what we did is we actually
looked at different pairs of operators
with different combinations of fault
tolerant strategies so no skipping
materializing skipping skipping and this
is the comparison of the real runtime
with the one that we predicted and as
you can see we're not always completely
on spot there are still differences you
know that may be in the order of ten
percent but at least we can distinguish
between all the main domain plans so
this is how as far as we wanted to get
so how well does this are so how will
the soft kind of the final question is
so now that we have this optimizer so we
have the ability to use different fault
tolerant strategies and we have the
ability to pick these different
strategies so the question is can we are
we able to find these good plans and how
much faster these plants actually
execute so this is one example from our
paper where we have a query kind of
similar you know selector onto an
aggregate kind of standard query and
this is a query that was actually
processing 160 million input tuples
producing 8 million I think the first
drawing one of the select outputs eight
million then we add another eight and
another 40 and then the aggregate has a
state of 8,000
couples so that's kind of the career
plan and as you can see we see different
strategies restarting skipping
materialization checkpointing and for
this operator this is one of the
examples were really a hybrid strategy
was helpful and the hybrid strategy and
the dog being to materialize the output
of the select do nothing for the
drawings and then check point the state
of the aggregate which is a bit kind of
what we would we could anticipate and
the difference was significant he
doesn't look as you know impressive from
the graph because we're not talking
about orders of magnitude but still the
hybrid plan was twenty percent better
than any of the uniform strategies and
it was definitely better than
thirty-three percent better than just
restarting and if you look at the choice
between kind of the best and the worst
possible plans in this case it was also
in the order of thirty-three percent
which is significant that's like a third
of your query runtime which is nice so
let me just kind of quickly show a
couple more results from the paper this
is a kind of another example career that
shows how many million couples of your
processing none actually is the same as
skipping this is the same as skipping
and this is another example where we
show what we predict what we observe and
at least it shows that versus restarting
that in this case the best strategy was
to uniformly do nothing and our
optimizer was able to select that
strategy another example where we have
this is kind of also important this is
select row and drawing this is the same
query select join join join with
slightly lower selectivities and
suddenly the best strategies to actually
materialize so given same operators but
we just change the input data now the
best strategy changes and again our
optimizer is able to find that strategy
although in this case because there was
a little bit more of a kind of network
higher natural consumption then we
actually had actually not this one we
actually our estimates were not as
accurate but still accurate enough to
find the right plan we change the query
plan a little bit again we change and
have an aggregate operator and this is
where again kind of the best strategy
here was to check point and we were able
to find it but this is the one where we
actually audit right this is one will
reproduce
quite a lot of tuples some operators who
network bound an hour actually our cost
model was not as accurate for natural
bound operators more accurate for you
know CPU mount operators so with our
estimator are as accurate but we still
found the right plan and finally kind of
against wall twist on the query this is
where we have a hybrid plan actually
this is kind of the same as I shown
earlier this is the hybrid plan where we
might realize as keep skip and an
aggregate at the end but this one who
shows also what we our estimates yes
this is very important so how we model
it is that typically I mean fat is from
so from what you read in papers is that
often people know that in their specific
cluster there's a certain number of kind
of average number of failures per job or
average kind of mean time to failure or
some of those statistics so what we do
is we say given that I have a query I
can kind of estimate roughly how long it
will run for and how many nodes I want
to use for it and then I will estimate
so how many failures that expects I
expect this correct run with 5 failures
6 failures and then we optimize for the
wrong time with that number of failures
actually no so we found that so we did a
sense TV analysis to have that later so
we found that we were actually and we
have the exact result in the paper we
were said we were not sensitive to small
wrong estimating the number of failures
and also we were not sensitive to our
performance estimate like reading
writing data from disk the one point
that we were sensitive to of course
record analogy estimation errors within
ten percent we can do quite well but
when we were off by more than ten
percent or more then we started to not
choose the right plants because we were
thinking like hey it's great to
materialize and then discover there's
actually so much data so that's that was
the sensitivity and finally to go back
to the impact of user-defined operators
so this is one example plan what we do
this will replace the final aggregate
with the user-defined operator so we
assume that the best plan in this case
the hybrid one had the last operator
checkpoint and that was the wrong time
we would get but here we replace the
operator with the user-defined operator
what we do with those is resume they are
unable to do anything in terms of fault
tolerance so now we have a query plan we
can pick fault tolerant strategies for
all operators but some of them don't
know anything about fault tolerance and
actually we show that it's still useful
even if you have some of those operators
we pick different plants and you can
still do better than some of the uniform
strategies such as for example
materializing on behalf of the operator
so just to go back to the user-defined
operator a question so how much time do
we have 130-230 or we have time okay so
this is just kind of ongoing work so
like I said this was interesting so what
we notice is that it is possible to use
different fault tolerant strategies in
the same query plan we can pick them
automatically and it does make a
difference that's visible on the actual
runtime the key challenge though is we
need these specific models for the
operators and that's really painful
because it's hard to get accurate models
like in one example where it was really
network bound our models were not that
accurate and it's hard if i want to
write new operators to develop these
models so the where we are headed right
nice to say let me forget about these
models what I'm going to do is I'm going
to start and run a query in parallel and
i'm going to use may be some sort of
alternate strategy like materializing
all the output and as i run it i'll
simply observe what is going on and if I
find that I really have some bottlenecks
I will try to remove maybe some fault
tolerant symmetrization points
dynamically at runtime and that will
allow me to to do better so that's kind
of what we're exploring right now so
kind of in terms of summary of the of
the health of torrents work is that we
looked at we looked at the problem of
running queries in parallel where it's
worth to materialise checkpoint and do
these different strategies some of our
contributions was to say look if I have
a parallel query I can act
run the query and make it false tolerant
without blocking I can pick and choose
the technique and I can do this
automatically so that was kind of the
main at the main point of the of the
whole fall trends work and where we're
headed like i said is more towards the
dynamic diamond dynamic scenarios so any
questions at this point yes learning
about model type analogies be to the
plan and we start with the new model
cardinality do you have a chance to
change the plan shake there that's a
good point we actually do not change the
kind of the shape at that point that's
right but we could this would be
something also interesting especially
that what happens is if we crash and one
node is restarting from a failure no one
else is doing any work so we could take
that opportunity to make some changes at
that time since the other nodes could be
doing some other work while we are we
playing
friend so yes if you're working at space
that I'm interesting yes after you find
any patterns and we make some strategies
are good for some operators respective
take for example the materialization is
good for aggregate operator so what we
found is we actually found some patterns
in particular if I have stateful
operators then it's often the case that
the operators right before if they met
they frequently the best strategies to
materialize before that operator or stay
operators that have small states they
can definitely check point but we often
found that it's not for one specific
operator but it's for a combination of
pairs of operators or sequences of
operators and also the input data sizes
but in that case yes we did kind of find
some patterns that if I have a sequence
of operators with stateless operators
and then big stateful operators it's
often worth to materialize the output of
those stateless operators but it's hard
to just make that general these are
still kind of things we kind of see but
it's the cost based model kind of allows
you to formalize that intuition which is
the case other questions yes you will be
fined operate or was in a recovery
strategy of none that's right you put it
around his google job that have fought
an average of five crashes what does it
don't ever get done well it does because
we still restart that user-defined
operator from the beginning but we only
restart that operator because the
operators around it can provide fault
tolerance and things like replay which
is the video don't want to really
process the whole crane and actually if
you have a few minutes I want to just
give I know how many minutes we have as
I'm asking do you have to have to go
over to 30 yeah all right so what I want
to do is kind of deterrence what fault
tolerance that's why I'm pausing here
and I can't remember how much time we
allocated that I'm hesitating but I
wanted to just give you kind of a very
broad overview of the kind of research
that we're doing in our group besides
fault tolerance and really and this is
something I'd love to talk to people
offline so we really are doing work
along three broad aspects so we are on
campus we have there a lot of scientists
we talk to them all of them have a lot
of data and we find that there are
really challenges wrong three lines of
work first how can I get someone like a
scientist who's technically skilled and
capable
but wants to run complex machine
learning and complex user-defined
operations and other analysis how do we
get them to do this efficiently and of
course we can leverage cloud computing
environments and this is our new ash
project and what I talked about is an
example project from within this
umbrella the second is to look at not
only making things go fast but also
making them easier to use and we're not
avi people but we're still looking at
things such as how can I help someone
articulate a sequel query can I use past
executions past usage history from the
cluster to learn what kind of course
people ask and help newcomers articulate
their own queries and finally the last
one that we're looking at which is
actually related to the you know windows
azure data market is to figure out how
can i price now a lot of people have
lots of data they all want to buy and
sell this data online what kind of
database support do we provide to make
it easier for them to articulate prices
reasonable prices price on you a couple
of views and then automatically have
them sell arbitrary queries and so on
and just to kind of give you quickly
some examples of projects in these
different categories in terms of let's
let me give you kind of a couple of
products in the efficiency categories so
we worked with a lot of scientists and
one thing we discovered time after time
is that often when jobs execute there's
a huge skew so this is one example where
it's a debug runs its smaller scale on
the x-axis I have time on the y-axis I
have all the tasks that I'm running in
my cluster and I have different color
tasks they don't really matter here but
you can see the duration of all the
tasks is very very small except for this
one crazy task at the top which takes
forever so what we build you actually
have a couple of systems that allows us
to smooth this kind of skill
automatically so the top line is 1.5
hours versus more of the other you know
lines in this industry beauty actually
or just in order of a couple of minutes
so build a system called scurried juice
and the details are in a sock 2010 paper
that is also actually optimizer takes as
input a sample of the input data cost
functions about the actual processing
that we want to do information about the
cluster and figures are how to partition
allocate the data in a way that
automatically kind of gets rid of this
type of us to
and what is interesting is on the real
scientific workloads we find runtime
improvements of a factor of six to eight
compared to a you know standard
MapReduce implementation so this is
really significant but that was specific
to one type of applications kind of that
we find in a science domain we also have
a more recent scoochie work where we
said I don't want these like here i have
discussed functions so in the new work
we said I don't want any information
from the user and what we do is we
simply observe what is going on in a
Hadoop cluster not is when stew is
occurring and then find a good way to
kind of reallocate the data in a manner
that introduces low overhead and kind of
looks ahead to try to optimize for our
total run time and without knowing
anything about the application we can
reduce run times by a factor of four so
this is significant these are you know
large factors and those are real systems
that are actually built on top of Hadoop
and publicly available now and actually
my student who worked on this is going
to join Microsoft in September in the
being search team so you'll guys get
those skills now we also looked at
iterative processing so a lot of people
want to use Hadoop clusters but they
need to run machine learning pagerank
kind of algorithms that iterate by
simply being smart about caching and
scheduling we can count run times in
half and this is also a system that is
not publicly available that we built at
u-dub we're also collaborating with the
side gb team which the goal here is to
build a parallel system that is
different because it's going to operate
a multi-dimensional arrays so scientists
often have they simulate you know the
universe they simulate the atmosphere
they operate on this multi-dimensional
arrays and this is a new system but
already has some 500 downloads and this
is not our work this is joint work with
MIT Brown University Portland State
University and so on all this is done so
this is kind of examples from this
category in terms of making big data
analytics easier I mentioned that we do
different things so we looked at sequel
autocomplete and the idea is that is as
a user kind of types of sequel query the
user can ask for recommendations and
what we are going to do is to say okay
so the user has put certain tables
certain predicates in their query we
look at that query we look at
usage of the cluster and see who asked
similar types of queries and what other
predicates tables did they use in their
queries and then you recommend the most
popular from those and we actually had
good data we tested this on the Sloan
Digital Sky Survey query log and we had
kind of very good results in terms of of
the type of recommendations we also
build kind of another thing that allows
people to browse through queries I'm not
going to mention this we looked at other
things that try to help people
understand their cluster better so once
they actually articulate their query do
you run it in a cluster we looked at
things such as better progress estimates
that allow people to have a better sense
of how long their cores are going to run
for and one of our key contributions was
to say well failures will occur sku will
occur so instead of giving one best
guess we're going to actually give
people ranges where we try to kind of
make the range as small as possible such
that it is useful and say this is kind
of the best case run time estimate but
assuming kind of a single worst case
failure or you know ask you at the
following operator is quite likely then
we're going to also give users a range
so they can have a better sense of when
how long their courage will take most
recently we actually build the system
that actually I'm pretty proud of so
what we can do is we can run a Hadoop
job and then we can ask questions such
as why did my Hadoop job just run slower
than this other job even though it
processed less data and it was running
using the same number of instances and
our system uses machine learning to then
say well because between these two runs
someone for example change the posture
configuration and increase the block
size and the Fourier actually not using
all those machines perhaps so this is
something that will appear at the P vldb
so this is kind of a few examples from
this area and again I'm really happy to
talk to you about this offline and
finding the pricing this is the most
recent project and this is actually
quite fun so we're looking at this from
several perspectives so the first idea
is that people buy and sell their data
online currently if I use something like
the windows azure data market I can't
price my data but i can only price it
using kind of very simple selection
queries so i'm going to define kind of a
view and people can enter parameters in
this view and then they're going to pay
based on the number of output records
that they get and you kind of pay
monthly based on kind of the total
amount of data you will be seeing what
we actually asked was to say well if I
produce a certain number of views than
my customers when they come they will
have to pick among one of those views to
satisfy their needs but if none of the
views is good well they will have to buy
some super set and if they're not
willing to pay for the super said well
they might not really be happy so what
we did is we actually build so this is
more in the theoretical side we
developed theory that allows us to
allows the seller to only specify some
views and we use these price points to
automatically drive the price of any
arbitrary query that the seller wants to
purchase we basically automatically
figure out given the query that the user
is interested in what is the minimum
number of views that they have to
purchase and in what combination to get
the best price and we can do this
automatically for a pretty large
selection of of queries and this kind of
pricing idea goes even much further than
just pricing data we're looking at
things such as if i'm going to buy and
sell data i also want to protect the
data i will sell the data and say you're
allowed to use this data but you're not
allowed to maybe join the data with some
other data sets so we are now looking at
building databases that can help us and
force these digital rights on the data
without too much overhead or if i'm
going to use cloud computing resources
many people share the same cloud I need
a way to figure out what optimizations
to implement how to price these
optimizations how to allocate the cost
how to give it to the users and we also
have automated techniques to do this so
this is kind of you know in conclusion
kind of a broad scope of the kind of
work that we do in the database group at
u-dub on the system side are related to
kind of big data analysis cloud
computing and data prices so okay at
this point yeah that's it thank you very
much good all right thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>