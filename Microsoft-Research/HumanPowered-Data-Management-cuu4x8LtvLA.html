<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Human-Powered Data Management | Coder Coacher - Coaching Coders</title><meta content="Human-Powered Data Management - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Human-Powered Data Management</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cuu4x8LtvLA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
hi it's my pleasure to introduce a
latere permethrin who's been a former
intern with us from Stanford University
to come here for a talk he has a lot of
publications in databases dblp count is
more than 25 I think actually some
pretty large number and I'm sorry a
crowd of applications I know my secret
and in fact office papers have been
among the best paper best papers of the
respective conferences so he has a very
distinguished record for a graduating
PhD student and he is here to tell us
about human power data management so
what do you write Thank You Raghav and
thank you for inviting me it's a
pleasure to be here pleasure to be back
actually all right so I'm Aditya
permission from Stanford University I'm
going to be talking about human power
data management right so we are now in
the midst of the big data age every
minute we have 48 hours of video
uploaded on YouTube hundred thousand
tweets and so on understanding this data
would help us power a whole range of
data driven applications unfortunately
an estimated eighty percent of this data
is unstructured so this images videos
and text fully automated processing of
unstructured data is not yet a salt
problem humans on the other hand are
very good at understanding unstructured
data they are very good at understanding
abstract concepts they're very good at
understanding images videos in text so
incorporating humans doing small tasks
into computation can significantly
improve gathering processing and
understanding of data so the question is
how do we combine human and traditional
computation for data-driven applications
so let me illustrate the challenges
using a simple example the task that I
actually wanted to do for this
presentation so I want five clip art
images of a student studying of college
age there must not be a watermark and it
must be suitable for presentation okay
so it's a simple enough task so the
first option is i do it myself right so
but this could take very long
I I need to figure out which queries to
issue to google for bing images for pie
right at the start so I need to figure
out which queries to issue to bing
images and for each of those queries I
need to go through hundreds and hundreds
of results so it's really painful the
second option is I asked a friend once
again could take very long the person my
dog to go might not do a good job right
the third option is to orchestrate many
humans by orchestrating many humans I
could get results faster because I'm
paralyzing and because i'm using many
humans i may have low error all right of
course orchestrating many humans gives
rise to many different challenges first
how should this task be broken up
presumably I mean I need to gather
images from an image search engine which
query should I issue how many images
should I gather how should I check if
these images or Bay properties right to
guarantee correctness I may want to
check one property at a time in what
order should I check these properties
since humans make mistakes I may want to
ask multiple humans how many human
should I ask how do I rank these images
how do i optimize a workflow how do I
guarantee correctness so these are the
kinds of challenges that one needs to
grapple with when orchestrating humans
for human power data management all
right so these challenges boiled down to
a fundamental three way trade off that
holds in the scenario trade-off between
latency how long can I wait cost how
much am I willing to pay and quality
what is my desired quality so recall
that in traditional database query
optimization the focus is on lakin
latency in traditional parallel
computation the focus on latency and
cost and traditional uncertain databases
the focus on latency and quality in this
case we have a three-way trade off so if
there's one thing I'd like you to take
away from the stock it's our focus on
this three-way trade off that permeates
all of my research on human powered data
management right so to get access to
humans we need to crowdsource so here is
a diagram of the landscape of
crowdsourcing in industry by a
organization called crowdsourcing
org as you can see it's a very active
area each of these tiny icons refers to
a separate company crowdsourcing means a
lot of different things to different
people it could mean generating funding
using the crowd generating designs using
the crowd solving really hard problems
using the crowd and so on in my work I
focused on crowd labor or paid crowd
sourcing to get access to crowd labor or
paid crowd sourcing one uses
marketplaces so all of the icons in this
figure refers to a marketplace
marketplaces allow users to post tasks
why a low-level API calls people who are
people who are online can pick up these
tasks and solve them and the canonical
example of a marketplace which I'm sure
a lot of you have heard of is amazon's
mechanical turk these marketplaces are
growing rapidly the size quadrupled in
2010 and 2011 and revenue the total
revenue reached 400 million dollars in
2011 so here's an example of a task that
i could post one of these marketplaces
asking people is this an image of a
student studying people who are online
can pick up this task and solve it and
we'll get the five-cent reward in this
case right ok so now let me draw a
diagram of the landscape of research in
crowd labor or paid crowdsourcing and
tell you how my work fits in so there
are lots of humans there are lots of
marketplaces like I described earlier to
reach humans and once again the
canonical example of a marketplace is
Mechanical Turk then there has been work
on platforms making it easier to post
stars to these marketplaces dealing with
issues like how should you interact with
humans what kind of human in our eyes
what kind of interfaces should one use
and so on then there is work on
algorithms that leverage these platforms
having humans do the unit data
processing operations operations like
comparisons filtering ranking rating and
so on then there are systems that call
these algorithms asking these algorithms
to sort cluster and clean data of course
these systems can also directly leverage
the platform's by having humans get or
verify data my focus has been on
designing al
rhythms and systems right so the focus
of my thesis has been on designing
algorithms and systems and efficient
algorithms and systems for a
human-powered data management there are
four aspects that I have studied data
processing data gathering data
extraction and data quality I've also
worked on other research that doesn't
fit under the umbrella of human powered
data management if there is time I will
tell you about that towards the end of
the talk alright so here's the outline
of the rest of the doc I'm going to tell
you about to human powered systems or
applications that both motivate and are
influenced by my research I'll tell you
about one of them immediately and the
other one interspersed with the second
topic which is filtering it filtering is
a critical data processing algorithm
that applies to both of the systems that
I will talk about I'll tell you about
the other research I have done in
crowdsourcing and in other topics and
then conclude with future research or
open problems alright so the first
application or system that I'm going to
be talking about is the data sift
toolkit so the data safe toolkit is a
toolkit for efficiently executing a
specific kind of query the gather filter
rank query on aerie any corpus so the
idea is you got gather items from the
corpus you filter them you rank them and
then you produce the result and humans
may be involved in all three steps the
gather step the filter step as well as
the wrong step and what we built is a
general purpose tool kit that can be
efficiently deployed on any corpus right
so let me dive into a quick demo so a
user of data sift will see a screen like
this they will select what they're
looking for in this case Google Images
sorry I don't have one for being images
but they select the corpus that they're
looking for type in what they are
interested in the conditions the items
must satisfy these are filtering
predicates and how the items must be
ranked so this is a ranking predicate
you can also specify how many results
they want and how much they are willing
to spend right now I'm going to play a
video of how i would use data safe to
ask my query so currently data sift is
implemented over for copper corpora
google images youtube amazon products
and shutterstock but
could be over any corpus for my clip art
of student studying example I would type
in give me a clip art of students
studying must be one student of college
age and so on and so forth right unlike
a traditional search engine notice that
i can use as many words as i want to
describe each of these predicates let's
say i want 10 results and i say that my
budget is let's say five dollars okay so
this is how a user would post a query to
data sift data sift will translate this
specification into questions that are
asked to the cloud so there are three
steps the gather filter and rank step so
it will actually gather items in this
case images by issuing keyword search
queries to the corpus in this case
google images so in this case it will
gather items by issuing the keyword
search query clipart of student study so
the crowd is not involved in this gather
step but the crowd could also be
involved in the gather step and I will
show you an example of that next then
DataSift checks if the image is
retrieved actually obey the filters
using humans then data sift ranks the
items using humans and then presents the
results to the to the user right now i'm
going to show you results from previous
runs for this query so this is one such
result there's just a portion of the
result there are lots more below so the
first column here refers to the rank
given by data sift for this query the
second column refers to the rank given
by google for this query for a clip art
of student studying okay as you can see
the first four results are all fairly
good this is one student of college age
sitting with books there is no watermark
and so on another interesting thing to
note is that the item with rank for was
actually ranked 78th in google so data
set managed to pull it up now i'm going
to play a video of me scrolling down so
that we can look at the rest of the
results okay so then you have items with
data sift rank 5 6 and 7 which are also
fairly good then you get to items with
data sift rank minus 1 so these are
items that data set discarded during
processing because it felt that it did
not satisfy
one or more of the filtering predicates
these items were fairly high up in the
Google search results for clipart of
student studying right so they were
ranked fifth three all the way till 16
if you go and man you manually inspect
each of these images you will indeed
find that they do not satisfy one or
more of the filtering criticals the
typical one that is not satisfied is the
no watermark restriction and sometimes
it's not even a clip art of student
studying right so the results are fairly
good for this example now let me try to
give you an even more compelling example
alright so in this case i'm looking for
a type of a cable that connects to a
socket that i took a photo of so notice
that i can add a photo as part of my
query and in this case i am searching
over the amazon products catalog alright
so in this case data sep does not even
yes to understand how would that photo
be leveraged during the gather phase
right I'm getting to that good yeah so
in this case a data set does not even
know what keyword queries to issue to
the Amazon products corpus as you
rightly pointed out so it asks a crowd
for keyword queries suggestions in the
gather gather step right and then it
retrieves items corresponding to those
keyword query suggestions all in the
gather step then in the filter step for
those items retrieved it checks whether
it satisfies this query or not right so
the items and there is no rank step in
this case right because you're not
ranking based on any predicate so the
items are fairly good all of these are
indeed cables that would satisfy my
query now I'm going to scroll down so
that we can look at the results that
were discarded by data sift so here you
can see item an item with data sift rank
minus 1 this is in fact a mini B cable
it is not a printer cable this is a
scanner pc interface cable which i don't
know what that is then there's a micro
cable then there's a printer which is
not even a cable and the further down
you go the more strange the results get
right there not even cables beyond the
point alright so once again data safe
does a fairly good job even for this
query
to summarize data sift is is a toolkit
for efficiently executing gather filter
rank workflows on any corpus there are
lots and lots of applications of data
sift how do you help school teachers
find appropriate articles to assign to
students howdy help shoppers find
desired products how do you have
journalists find supporting data or
supporting images right there are lots
of challenges in building data sift how
do you make it flexible and extensible
how do you optimize the individual
operators of gather filter and rank and
how do you optimize across the entire
workflow which is something we haven't
yet addressed so I won't have the time
to get into the detail design of data
sift I'm going to talk to you about just
the optimization of just one of the
operators specifically the filter
operator all right at this point are
there any questions yes yeah just just
the two examples you just mentioned
right so for example how do I sign how
do I find the right reading materials
for students with image recognition that
someone anyone something anyone can do
so people can look at the image say
that's a student finding out what is an
appropriate book for a classroom is
something that almost no one can do if
you pick up sort of random people up to
the Internet so is that really your
adequate task here too so I'm okay so I
I it is true that in in some cases if
the material is too specialized it might
not be appropriate to use a
general-purpose crowd but the thing that
I had in mind that the use case that I
had in mind was a little more simple
let's say I want I want to assign
articles on global warming and I want
people I want articles that are
well-written that have let's say neither
a liberal bias nor a conservative bias
it's sort of considers I don't know it
sort of has a very neutral bias it
considers the pros and cons of both
arguments and it is from a reputable
source right so these are things that
anyone can check I think anyone who has
English background of course if it's a
very detailed technical icon for
instance use data sift to ask for
related work to my publication right
that's some
thing that I can't use it as of now but
I suspect once we have a better sense
for skilled workers or skill sets of
workers I think we can get there
eventually but not right now any other
questions yeah why did you divide your
asking together to describe and systems
has more to cover squalene just choosing
this particular division called gathers
with the right covers your examples
right so um Susan I will talk about
another system briefly which does
general purpose computation but this was
a specific enough task even though this
very specific there are lots and lots of
applications that fit under this model
so it merited like detailed
investigation and the so one one one
aspect that is different although the
other system does have filter and grand
components it does not have a gather
component the gather component is very
new to this system so i don't think
there are corresponding components of
the other system but I'll get to that it
yeah question is look at databases and a
relational operators yes these look sort
of analogous to those relations you
precisely you have a language or you
because typically the interface no
database is a language and guess
everything is implemented using
composing operators so these are the
operators used to come for some is that
a language you know in this case these
this is this is the interface it's only
gather a sequence of filters and a tank
step that is the restricted language
that I can handle but our toolkit is
flexible enough that you can plug and
play these operators we really want to
but we are not supporting queries it's
almost like the relationship and 0 but
only for filtering and ranking that's it
no like complex operations like this
yeah these are the set of operators the
other is like from Klaus yes in
innocence innocence
hey um very interesting about all of
this we do you sort of envision a kind
of a marketplace I mean in some sense
but gather and filter depending on how
specialized the question is you might be
willing to pay more than to get people
with different level it's almost like a
matchmaking service between people who
have special specialized kinds of
knowledge and people who have questions
that need answering absolutely yeah so I
see this as just the first steps towards
a very specialized market place where
most of the people who are currently
sort of going to day jobs are and this
is certainly happening a lot more people
are looking for employment online and
this is certainly transition to skilled
labor of the kinds like a programming
design virtual assistants all of this
has already moved a lot to the 22 these
crowdsourcing marketplace not enough to
replace existing companies but people
really like this service right i mean
they can it's a flexi time a flexi cost
they can choose which our project they
are interested in it's great and I think
there will be a need in the future to
optimize the use of these humans even
for skilled labor and that's precisely
the outsourcing yeah yeah
yes scary we've giving opportunities to
everyone right anyway that's the
perspective I think all right so let me
move on to filtering filtering is an
algorithm that forms part of the core of
the applications that i just mentioned
as well as the other application that
we'll talk about later it's also one of
the fundamental data processing
algorithms so in filtering you have a
data set of items I don't need to tell
you this but you have a data set of
items you have a predicate and you want
to find all the items that satisfy the
predicate right so in our case items
could be images the boolean predicate
could be is this image a cat and I may
want to find all the cat images in this
data set right yeah it's in English
sentence much like the predicates that I
had earlier yeah so this is not
something I can automatically evaluate
that's a tricky so since I can't
automatically evaluate I need to ask
humans right does an item satisfy this
predicate or not and since humans may
make mistakes I may need to ask multiple
humans so the question is how many human
should I ask when should I ask them how
should I ask them these are the kinds of
questions that come up in this part of
the talk yes with the leadership is it
something the data set can be the better
sure but in in mice in my scenario i
have a restricted data set so the way to
think about this is let's say i did an
initial gather step i have a set of
images that i consider as you may be
able to optimize the Galveston yeah is
this image a cactus you could just do a
Google or Bing image search on cap yeah
you'll already get a bunch of so some
parts of predicates in your task you can
probably push to the gather face if
you're working of life field in the
search engines so so are you so are you
suggesting that I move so one so I think
you're what you're mentioning is the
option of moving some of the predicates
from the filter step to the gather step
so is that is that so so you're doing
trousers and yes and give probably the
questions you are interested addressing
a bradley general right so what so forum
try and understand is it always in a set
of precomputed items or the most general
sources the web itself right so if it's
okay if you think of the sources of em
yeah and you have some predicates in
mind let's say you're five predicate see
some of those could be pushed to simple
search predicates which would already
filter the images from the web okay and
it's sort of optimizing that we are in
landing some of the filtering in the gap
I think what we say in some sense i mean
even the other already has it filtering
operation earlier right so gather filter
friend sure that is not computed
disjointed something I agree I agree and
that is that is one of the reasons why
we haven't been able to optimize the
entire workflow yet I am just talking
about this one individual operator and
trying to optimize that there are very
complex interactions between gathering
and filtering and ranking in the sense
that one of the versions of this system
that we are building involves gathering
keyword query suggestions from the crowd
retrieving a few items for each of those
query suggestions then filtering them
and then going back to the gather step
to gather even more for the keyboard
query suggestions that did well as a as
a user of the system I use that micro
reset data set of item go through all
these images on the web i want to know
what are these reciprocity yeah i can
think of it to do is write the data I
just post this query to the haters of
system it as a system breaks it down
into a gather face yeah cause it can't
handle millions of images account click
comes out thousands of images ok so it
extracts something from I filter and
forces the greater the web and you get
yeah this is a first set yeah the
candidate second issues the queries
lunes right or i could have different
your system where the one is not me to
count with that who is you the user of
data sets ok the user of DataSift known
as another user of data set to competent
gather predicate ok so how is it down
in does not automatically extract our
galleries it could be so that the
toolkit is general enough that it could
have both options so one option is okay
so let me go back to data separate
mention nothing in gather and just say
so choosing google images and just put
filter predicates what happens see that
again I my gather fit search the gather
face there isn't that the topic is empty
empty search all Google you just
electricity right and then I say image
of a character right what will happen so
as a filtering predicate so in this case
so there are different versions of the
system one version of the system takes
the entire query and ask that fork you
ask the ask the crowd for keyboard very
suggestions okay but very suggestion so
that will be used in the gather step to
retrieve initial items then you will
filter those items based on the of
course if if I use the current the the
version that uses the the topic to ask
keyword queries suggestions that's
obviously not going to work in this case
right so just just to satisfy the
product with this so in a perspective
that I can understand one way one way to
think about the gather step is then you
give something crude as a way to the
specified what the set is then the
filtering predicates are actually a
validation stage exactly exactly yes
okay so gather step also another ranking
right for example I want cap yes now in
the google image yes a 10 home is this
repeal support only one count right yes
so how do you even decide like how many
to start in the galaxy great question so
um one so while we've not yet done
anything sophisticated in that step all
that we do is take the top 10 results
multiply it by a factor k and then
retrieve those many results and then
process it that's all that we've done so
far one so there are many ways of
thinking about this question one is that
the the search results are somewhat
correlated with the final results right
so beyond a point going down the search
query result the search results is not a
good idea right if you are searching for
instance for let's say
I don't know clipart of student studying
beyond the thousandth image you're not
going to get students studying at all
you're going to get very noisy images
some assumption about the data source
that google image good job I i am making
some assumption about the data source I
but shouldn't an initial set the size
already nation set and you say okay so
you should bury the google images you
get three million of them yes you have a
budget or five dollars so i shouldn't
you use that budget is on our guide well
so how big should my initial set p
second strike that 15 million down
second 27 that's all exec board dress
yeah so that is something we haven't yet
done right the optimum the entire
workflow optimization is something we
haven't yet done so i don't so right now
i have some ad hoc rules that govern how
many items how i use my budget on on I
mean I have a rule that says gather so
many items for how many items I actually
need but that is a great point yes
overall that's what i need to do i need
to think about how much i'm spending in
the gather step how much i'm going to
spend on the filter step and there are
the set of items constantly shrinks as
you go from the gather step to the
filter step to the right right so i need
to think about how much i'm spending in
each of these steps the very complex
problem and hopefully by just talking
about filtering itself welcome convince
you that it's complex enough alright so
should i get in to fill trick for all
great questions not please keep asking
all right so and in this part of the
talk i'll focus on the trade-off between
quality and cost i will not consider
latency although we also have results
for that case and for now i will assume
that all humans have the same error e
this is another assumption this is an
assumption i will get rid of later on in
the talk right so how do we filter well
we use a strategy so this is how we
visualize strategies in a
two-dimensional grid the number of no
answers gotten so far for an item along
the y-axis the number of yes answers
gotten so far for an item along the
x-axis
at all yellow points we continue asking
questions at all blue points we stop and
return that the item has passed the
filter at all the red points we stop and
return the item has failed the filter
okay so this is just one example of a
strategy let me emphasize that an item
will begin at the origin let's say we
ask a question to a human we get a no
answer the item moves up we ask an
additional question we get a yes answer
item moves to the right we ask an
additional question we get a no answer
item moves up and let's say I get a
sequence of vs answers we stop and
return that the item has passed the
filter alright so the key insight here
is that since I am making the assumption
that all workers are alike the way I get
to a point is not as important as the
fact as I am there right so these
strategies are Markovian and for those
of you who are familiar with stochastic
control this is in fact an instance of a
Markov decision process so this might be
familiar to some of you right so this is
just one example of a strategy here are
other strategies always ask five
questions and then take the majority
right wait until you have three es
answers or three no answers and until
then keep asking questions let's show
other examples of strategies now let me
move on to the optimization problem so
in the optimization problems this is one
of many variants I'm given or I estimate
why are sampling using a gold standard
if I have one or I can approximately
estimate it if I don't the per question
human error probability so this is a
probability that an I answers yes given
that an item does not satisfy the filter
and the probability that a human
answer's no given that an item satisfies
the filter and I also know the a priori
probability of an item satisfying or not
shattering the filter right so I know
these quantities huh
so if I have a gold standard then it's
easy in the sense that assuming the gold
standard is a sample of the actual did
actual data set then it would be an
accurate estimation the fraction of two
yeses versus true knows would be the
estimate of the a priori project is a
probability that any arbitrary wedge is
exact so in the data set case i estimate
these quantities approximately as part
of the processing so since it is a
completely unsupervised system i
actually estimate these quantities while
doing processing so i do a little bit of
sampling a approximate sampling to
estimate these quantities
I haven't really checked so I don't know
how how sensitive it is but my my
understanding is that these these
strategies are fairly robust so even if
the estimates are off and we've done
this using synthetic experiments even if
the strategies are slightly off you
still get fairly good results yes you
done anything wrong sorry have you done
anything I'm going to filter out what
should I call sloppy useless or
malicious users somebody just clicks no
on every major yes on every image yeah
so that is yeah or you could also see
something see the input set the things
that you know are correct true yes
perfect so in the data set case it's
completely unsupervised so I cannot have
I can't do this apart from apart from
sort of using the other workers other
humans estimates to check if a given
human is good or not right that is
disagreement Bay scheme we also have a
we have work on on dealing with data
quality but it's not integrated into
this current system so right now the way
I think about it in this part of the
talk I'm assuming that that all humans
are alike and since since Mechanical
Turk is such a I mean rapidly varying
pool of people that's an accurate
assumption to make because i don't have
the the reliable error rate estimates
for people over time because the the
pool of people that i have access to
rapidly changes so I in at any given
time I won't have a worker who I've seen
before doesn't matter in your chart
maintains the accuracy of those users no
no so all that they maintain is a number
of tasks that these users have attempted
in the past and their approval rate and
the approval rate is is not of any good
because if you do not approve their work
then all the workers will boycott you so
you just approve their work typically
that's just a factor that's just
something you do
right so mechanical talk does not have a
good reputation system all right so
quality is an important aspect right now
we are stepping quality by assuming that
all workers are alike that's one the
other we do take into account if we have
estimates of worker quality that can be
taken into account while filtering in by
suitably downvoting in some sense the
the bad workers and I will tell you
about the lid later on we will take the
users to the workers to some tests to
make sure that you know they're great
yes that is an option that is often used
in practice unfortunately in data sift
because the task is new every time a
user uses my system I'm getting a
completely new task so testing the user
on something I have I have information
about earlier is not going to help you
know the result from previous stuff and
give it to your system to sort of charge
the quality of previous waters what but
the deal with a new search but but I
mean suppose you are doing this task for
you know thousands of things yes someone
can just do 10 of those and those 10
could be a test that would be good guys
really I wish I can generate lots and
lots of ideas yeah all good ideas but
yeah I don't we let you see is what you
actually did all right thank you so yeah
and my goal is to find the strategy with
minimum possible expected cost in this
case since I'm paying the same amount
for every question the expected costs
nothing but the expected number of
questions and I want my expected error
to be less than a threshold so this is
the second objective the last constraint
is that I want my strategies to be
bounded so I don't want to spend too
much money on any single item so what
the last constraint means is that the
strategies fit within the two axes and X
plus y is equal to M ok so i SPECT don't
spend more than say to any questions on
any single item which is reasonable
alright so how do i estimate expected
costs and error so given a strategy the
overall expected cost of a strategy is
nothing but summed over the red and blue
points X comma Y X plus y which is a
proxy for the cost times the probability
of reaching X comma Y all right and
overall expected error is the
probability of reaching a red point and
the item satisfying the filter plus the
probability of reaching a blue point and
the item not satisfying the filter so
these are the two ways you can go wrong
right and how do i compute these
probabilities well I can compute it
iteratively right so the probability of
reaching this point is a probability of
reaching this point and getting a yes
answer plus the probability of reaching
this point and getting a no answer right
so i can compute these probabilities
iteratively so i now have a way of
computing expected cost and error of any
strategy so here's a naive approach to
compute the best strategy for all
strategies evaluate cost and error and
yes this is fallen identical to the
bottom of estimating advice
the last early days debating the bias
you know so i assume that i have my
estimates already know each picture is a
car ok this picture has a probability of
satisfy your requirements along now in
my case I know the true I know that each
picture is either a 0 or a 1 I don't I'm
not given that each picture is a
probability it's not a bias each picture
is a 0 or a 1 given that an item is a 0
or a 1 I have probabilities of getting
wrong answers ok so naive approach for
all strategies that fit within the two
axes and X plus y is equal to M evaluate
expected cost and error and return the
best one how do i compute all strategies
that's easy for each grid point you can
assign it one of three colors red yellow
and blue run through all possible
strategies and give the best one right
of course this is exponential in the
number of grid points if you have 20
grid points or anyway so if you have
twenty good points to the order of 3 to
the 20 and as in in other cases it gets
even worse so this is exponential in the
number of grid points so they're not an
approach we would like to take it so i
have given you a naive approach to find
the best strategy and i'll call these
deterministic strategies for reasons
that will become clear shortly computing
the best strategy is simply not feasible
it takes too long but the resulting
strategy is fairly good it has low
monetary costs I have another algorithm
that also gives me a deterministic
strategy once again this is exponential
but it's feasible I'm able to execute it
for fairly large M the resulting
strategy is slightly worse it slightly
has slightly higher monetary cost but
I'm not going to talk about this
algorithm either I'm going to tell you
about a different algorithm in order to
do that I need to introduce a new kind
of strategy as some of you may have
guessed the new strategy is a
probabilistic strategy so in addition to
having yellow blue and red points I have
point set up probabilistic so with a
like this point so with probability 0
point to you continue asking questions
with probability point 8
you stop and return that the item has
passed the filter the probability zero
you return that the item has failed the
filter all right so these are
probabilistic strategies we have an
algorithm that gives us the best
probabilistic strategy in polynomial
time and since probabilistic strategies
are a generalization of deterministic
strategies this is in fact the best
strategy period okay and we can get that
in polynomial time since this is the
best strategy it has the lowest possible
monetary cost so over the next four
slides I'm going to give you the key
insight behind this algorithm and then
tell you about the algorithm okay so the
key inside necessary is the insight of
path conservation so you have for every
point a fractional number of paths
reaching that point and what that point
does is to split the paths some of the
parts continue onward some of the paths
you stop and return that the item either
pass or filter filter so pictorially
let's say there are two parts coming
into this point this point decides to
split the path 5050 so one path
continues onward to ask one path you
decide to stop for the path that
continues onward to ask asking an
additional question this path moves to
the point above as well as to the point
on the right okay so this is how path
conservation works for a single point
now how does path conservation work for
strategies you have one path coming into
the origin since is a continued point it
lets the paths continue onward so one
path goes to this point and to this
point once again since this is a
continued point it lets a path flow
onward while this is a probabilistic
point let's say the probability is 50-50
so half a path flows on word from here
so overall you have one path ending here
one-and-a-half ending here and half
apart ending there right so this how
path conservation works in strategies so
now finding the optimal strategy is easy
we simply use linear programming on the
number of paths and so you have a number
of parts coming into each point those
are the variables the only decision that
needs to be made at each point is
how these variables are split everything
else is a constant multiple so the
probability of reaching a point is a
constant times the number of paths
reaching that point the probability of
reaching a point and the item satisfying
the filter is a different constant times
the number of paths and returning pass
or fail at a point does not depend on
the number of paths all right so finding
the optimal strategy for this scenario
is easy you just use linear programming
now I'm sure you thought of many issues
with the current simple model we have
generalizations that can hopefully
handle all the issues that you've
thought of all right so let me pick a
few of them to explain further alright
so the first generalization is that of
multiple answers so instead of having a
boolean predicate yes no what if we want
to categorize an image as either being a
dog image a pig image or a cat image or
you want to rate an item as being either
0 to 5 1 out of 5 all the way until five
out of five in this case we simply
record the state as the number of
answers of each category that I have
gotten so far once again I can use a
path conservation and linear programming
to find the best strategy second
generalization is multiple filters so so
far we considered a single filter what
if we have a boolean combination of
multiple independent filters like in my
data sifting in this case we simply
record the state as the number of yes
and no answers for each of those filters
and at any point you can choose to ask
any one of those filters or you can stop
and return the boolean predicate is
either satisfied or not satisfied then
the last generalization is that of
difficulty so far we assumed that all
items are equally easy or equally
difficult so they all have the same
error rates what if they are not right
what if there is a hidden element of
difficulty we capture that using a
latent difficulty variable and the error
rate of each item is dependent on that
latent difficulty variable once again we
can capture that in our in our current
setup now let me move on to a harder
generalization so
is a generalization of worker abilities
so let's say I have three items whose
actual scores are 0 1 and 0 worker one
who's a very good worker decides to
enter 0 1 and 0 for for these three
items worker to decides to answer one
one and one for each of the three items
so he's a fairly poor worker worker 3 is
adversarial so he flips a bit for each
of the items right so he's a fairly bad
worker but he gives us a lot of useful
information we can just flip this bit
anyway so how do we handle such a case
we are losing a lot of key information
by assuming that all workers are alike
so we can reuse the trick from multiple
filters we can certainly record the
number of yes and no answers
corresponding to each of the workers and
they certainly works unfortunately if we
have many workers with varying abilities
we have an exponential number of grid
points right and therefore our approach
does not scale all right so over the
next three or four slides I'm going to
tell you about a new representation that
helps us solve this problem any
questions at this point all right so
instead of recording the number of yes
answers in the number of no answers
gotten so far we record the posterior
probability of an item satisfying the
filter given the answers that you've
seen so far along the y axis and the
cost that you have spent so far along
the x-axis okay so now you make make it
clearer I'm going to map the points from
the previous representation to the new
representation so the point at the
origin maps precisely to the a priori
probability of an item satisfying the
filter and cost is equal to 0 all right
these two points map two points above
and below that point at cost is equal to
one right and the remaining points would
map to their respective points in the
new representation now as an
approximation i am going to discretize
the posterior probability of an item
satisfying the filter given the
answers into one of a small number of
buckets and as a result multiple points
in the old representation may map to the
same point in the new representation
right notice that I can discretize it as
finely as I want right as finely as my
application needs so if we have many
workers with varying abilities we can
once again map this to n dimensional
representation to that two-dimensional
representation and as an interesting
property as we reduce the size of the
discretization make it smaller and
smaller the optimal strategy in the new
representation tends to the optimal
strategy in the old more expensive
representation all right so what changes
in the new representation well instead
of starting at the origin you now start
at the a priori probability of an item
satisfying the filter with one path
entering the strategy at that point if
you have all workers having the same
error rates you have two possible
transitions one above and one below all
on spending one one unit of course so
you always transition to the right if
you have n workers with varying
abilities you have order of n
transitions so the size of each linear
equation scales up by order of n and
once again everything else works you can
use the probability the path
conservation property and linear
programming to find the optimal strategy
so let me quickly tell you about one
more generalization that works well in
the new representation then I'll move on
to experiments so the other
generalization that works well in the
new representation is the generalization
of a priori scores so what if I have a
machine learning algorithm that provides
for every item a probability estimate of
that item satisfying the filter for
instance I may have a dog classifier and
the probability estimate may be
proportional to the distance from the
classifier as well as which side the
classifier the item lies this is easy to
use as part of my strategy computation
say I have fifty percent of items with
probability point six and fifty percent
of items with probability point for I
simply start half a path that point six
and half a path that point for and the
strategy computation proceeds as before
when running a strategy on an item the
item will begin at its a priori score
and this a priori scores sort of would
capture the the intuition when we have
already have the input data set having a
ranked list of results right that could
you could certainly help in that case as
well all right so told you about a
number of generalizations we have other
generalizations that I did not have time
to cover yes yeah so I have like one
question at the problem populations so
in this case you studied the figure
operator yep but in practice I would ask
you for your application much more
natural operators I have some one
thousand faces and I want to take 50
matches against right if my ultimate
goal is to pick 50 meters from that may
not be optimal for me to answer each
image and get it graded right it's much
better for me to consider an image and
and if it and if an image is Mark yes I
want to prefer those images but if an
image is starting to have some radiation
like variance in the markings so it's
probably not useful because I want to
focus on images shall move it so it
seems that the nature of the problem
will fundamentally change you
incorporate certainly certainly so
that's another problem you've studied
and the key insight in that scenario
when you want a fixed number of items
from a data set that satisfies the
predicate is that as soon as an item
falls below the average item in the data
set you would rather pick the average
item in the data set that's the
intuition that you had as well right and
we have a separate paper on the ramen
I'm not going to be focusing on that in
this talk this in addition to systems
like data set also appears in lots of
natural scenarios things like companies
do this all the time so things like
content moderation right a lot of
companies
have have content moderation phase
before they upload their the image the
user uploaded images go on the live site
they have a content moderation phase
where they use crowdsourcing services in
that case you need to go and manually
check every single image right and the
second application that I'm going to
tell you about also you need to go and
manually inspect every single item all
right so but yeah so the finding finding
k items that satisfy the predicate
that's a natural algorithm that we've
studied yeah okay so I'm going to now
tell you about experiments I'll use that
as an excuse to tell you about the
second application that we've been
studying that is MOOCs so I'm sure
you've heard of MOOCs massive open
online courses they are very trendy this
in fact even a Photoshop poster of the
movie The Blob just been photoshopped to
read the MOOC they thought was quite
cute MOOCs are revolutionising the world
of Education there are hundreds and
hundreds of courses each being taken by
thousands and thousands of students
right there are lots of courses that
require subjective evaluation courses
like psychology sociology literature HCI
and so on and there's no way TAS can go
and evaluate all the assignments in all
of these courses so what we need
therefore is peer evaluation so peer
evaluation is crowdsourcing but with an
important twist the important twist is
that the evaluators are also the people
being evaluated right okay so now the
key question is how do you assign
evaluations for submissions so that you
can accurately determine the actual
grade of each submission and notice
these were the images that data civ gave
me for my initial example okay so
deciding whether or not to get
additional evaluations for each
submission is a generalization of
filtering that I considered where I want
to rate an item as being either 0 or 25
1 out of 5 all the way until five out of
five so we are very lucky to have the
data set from one of the
early MOOCs offered at Sanford this is
the Sanford eti course in this case you
have 1500 students with five assignments
each having five parts these are graded
by random peers whose error rates we
know because we've had them go and
evaluate assignments for for which we
know the true grade okay so we know that
error rates and our goal is to study how
much we can reduce error for fixed cost
or vice versa so here is one sample
result I am plotting the average error
in this case the average error is the
average distance from the actual grade
and remember actual grades are between 0
to 5 and along the x-axis I have the
cost index in this case the cost is the
average number of evaluations for each
submission and I'm plotting three
separate algorithms the first is the
median algorithm that requests a fixed
number of evaluations for each
submission and then takes a median the
one class algorithm that assumes that
all workers are alike have the same
error rates sorry and uses the old
representation and the two class
algorithm that puts workers into two
buckets way based on their variance high
variance and low variance workers and
uses a new representation right so far
now let me focus on the the median
algorithm and evaluations equals 5 so in
that case the median algorithm has an
error of point 3 so this is in fact the
heuristic that is currently being used
in the Coursera system for a range of
courses like psychology sociology and so
on so we can get to the same error using
just sixty percent of the cost using the
one class algorithm and just forty
percent of the cost using the two class
algorithm from the perspective of error
if I fix the cost at three I can reduce
the cost of the error by forty percent
if I use a one class algorithm and by
sixty percent if I use the two class
algorithm so either way I can
significantly reduce both cost and error
using our
geez alright so here is at this point
I'm happy to take questions because this
is all right moving on so let me tell
you about other work in the crowd
sourcing space that I've worked on other
research that I've done and then
conclude by talking about open problems
yes so don't fill drink so if you have
multiple filters let you adjust the curl
example you show like four or five right
how do you handle them do you ask the
big questions for each of the votes yeah
so currently the way we handle them in
the fridge ring operator is by having a
separate question for each of those
filters but you could also consider kind
of combinations and true true the reason
why we decided to go with separate
questions for each of the filters is
because it's not very clear humans are
more likely to make mistakes with a
combined question because it's not clear
what question they are answering if it
is a single unit question it's much more
clear what they are answering if it is a
it's like if does it satisfy this and
this and this and this if they might say
no even though if it does satisfy or the
other way around but yeah good point
ok so we studied other aspects of data
processing in addition to filtering
finding the best item out of a set of
items categorizing and item into a
taxonomy of concepts identifying a good
classifier for imbalance data sets also
the search problem which is the one that
you mentioned the finding key items that
satisfy a predicate determining the
optimal set of questions to ask humans
in a lot of these cases is np-hard even
for very simple error models therefore
we need to resort to approximation
algorithms and recently we've started
looking into some of the data quality
issues as well which are common to all
of these algorithms let me move on to
dekho so DataSift is is in my mind an
information retrieval like system
powered by the crowd dekho on the other
hand is a database system that's powered
by the crowd right so I don't need to
tell you this but database systems are
very good at answering declarative
queries over stored relational data but
what if the data is missing what if I
don't have the data so sodeko can can
actually tap into the tiny databases
that exist in people's heads so it can
answer declare the queries over store
relational data as well as data computed
on the fly by the cloud so if you ask a
query like this asking for the cuisine
of bites cafe at stanford deco will
gather the fact that the cuisine of
bytes is French and return that as a
result for the query it so yeah this is
a muhfucka clearance something
not the gathered step
I thought this is something about having
it is gathering missing data it's not
the the keyword keyword query
suggestions I mean it would take a lot
of sort of mangling data sift mangling
deco to fit under it is it but it is
true that is this is a much more general
purpose system than data set alright so
here are key elements of decos design it
has a principle and general data model
and query language you have user
configurable fetch rules for gathering
data from the crowd this is sort of like
access methods if you will use of
configurable resolution rules for
removing mistakes or resolving
inconsistencies from data gathered by
the crowd one such resolution rule could
be the filter operator right we need to
do to the three way trade off between
latency cost and quality we need to
completely revisit query processing and
optimization in this scenario so we have
a working prototype which was developed
by a colleague and myself at Stanford
you can pose and this also web interface
so deco supports the prototype supports
decos query language a data model query
processing as well as optimization
there's a web interface where you can
post queries visualize query plans and
get results right so let me move on to
data extraction let's say I have a
website like Amazon and I want to
extract an attribute like price from all
the web pages on Amazon I can ask humans
to provide pointers to where the
attribute is present on a given web page
then I can extract from all the pages on
that site right but what if the web page
is modified so my pointers are no longer
valid and I may end up extracting
incorrect information like in this case
I may end up extracting the fact that
the kindle costs ten dollars right so
what do I do in such a case we built a
robust rapper toolkit that can reverse
engineer
the pointers have have gone in the
modified versions of the website so you
can continue to extract from modified
pages accurately right so you can
significantly reduce the cost of having
humans provide pointers once again so a
robust rapper toolkit had some very nice
theoretical guarantees and over
internship I deployed this in yahoo's
internal information extraction pipeline
okay so there's lots of related work
that we build on in the crowd sourcing
space work on workflows games and apps
whenever I get I give talks I typically
get questions on the first four topics
although my work is more similar to the
last two topics deco and data sift are
similar to the recent work happening
around the same time on crowd DB and
quark and parallely there's been a
development of a number of other
algorithms sorts and joins clustering
and so on okay now let me tell you about
some of the other research I have done
I've also worked on course
recommendations for a course
recommendation site called course sank
course recommendations post a number of
interesting and challenging aspects how
so you need to deal with things like
temporality because courses are
typically taken in sequence you need to
deal with requirements because courses
need to be recommended that are not just
interesting but also help the student
meet graduation requirements right my
cost recommendation engine had some nice
theoretical guarantees and this was
deployed in intercourse tank corps shank
was spun off as a start-up by these for
undergrads and deployed at about 500
universities and I think a year ago it
was purchased by a company called
chegg.com all right so
this is t-shirt right that says all I
got was a my friends had a start up at
all i got was a lousy research paper
switch yeah so I worked on human power
data management and recommendation
systems in addition I've also worked on
information extraction and search but I
won't have time to cover that in this
talk so in all of my work I have
followed an sort of end-to-end approach
to research I model scenarios
conceptually starting from simple error
models and then generalizing like in the
filtering case i formulate optimization
questions that make sense in the real
world I find optimized solutions using
techniques from optimization inference
approximation algorithms and so on and I
build systems with these solutions right
systems like data safe dekho the robust
rapper toolkit so on of course in
research it's never really a linear path
there are lots and lots of iterations
but I intend to continue using this
end-to-end approaching research right so
I think human power data management is
only going to get more and more
important in the future there are more
and more people looking for employment
online and so there's a need to to
manage and optimize the interaction of
this giant pool of people who are
interacting online in a scene in a
seamless manner it and of course more
and more data is being accumulated
however many fundamental issues in
crowdsourcing remain issues like it
takes too long sometimes the work is
badly specified sometimes workers are
prone sometimes humans don't like the
tasks that we give them and sometimes it
costs too much okay so I have initial
angles of attack for all of these issues
let me pick a few of them to explain
further so latency can be addressed by
having systems produce partial results
as they do their computation but this
requires rivas
setting the computation models of
systems and algorithms given to
algorithms how do we pick the one that
produces interesting partial results
faster right so to deal with poly
specified work how can we use a crowd to
decompose so one more point about the
ego computation this is related to some
work prior work in the database
community on online aggregation for
instance to deal with poly specified
work how can we use a crowd to decompose
a query to a workflow what should the
intermediate representation be how can
we verify the correctness of this
intermediate representation to deal with
error prone workers how can we monitor
their performance to see that their
performance doesn't start to drop how
can we be sure that our estimates of
their performance is correct and how a
man should we provide feedback to
workers that they're doing a good job or
a bad job right so the steps that I'm
arguing for go beyond looking at systems
and algorithms to the other steps in the
pipeline the interaction with humans as
well as the interaction with platforms
so once we solve some of the fundamental
issues in crowdsourcing I think there's
no end to what we can do there are many
many hard data management challenges
that could benefit by plugging in humans
as a component and of course designing
some of these systems would bring about
a whole range of additional challenges
as well for instance how can we impact
interactive analytics using humans right
for Ken humans helped formulate queries
can humans help visualize query results
how can we build better consumer facing
applications powered by humans by
combining human and computer expertise
can I build a newspaper a personalized
newspaper that beats google news for
instance right can I build human powered
recommendation systems how can I impact
data integration a problem database
people have been working on for decades
now using humans overall I think there
are lots of interesting problems in
redesigning data management systems by
combining the best of system best
of humans and algorithms at this point
I'd like to mention that a lot of the
work that I have done in my my PhD is in
collaboration with the number of
collaborators both at Sanford and
outside Sanford in particular I'd like
to call out my advisor hector
garcia-molina my unofficial co-advisor
Jennifer Widom as well as frequent
collaborator al al case police orders
right at this point I'm happy to take
questions you guys are there you guys
said that Jeff huh we saw collaborative
innovation Jeff oh gosh and that is ming
han I couldn't find a photo of him yeah
I should have mentioned that the crowd
of Mechanical Turk workers there is a
mistake yeah any other questions all
right thank you so much for attending</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>