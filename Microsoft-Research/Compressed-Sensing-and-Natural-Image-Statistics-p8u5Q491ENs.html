<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Compressed Sensing and Natural Image Statistics | Coder Coacher - Coaching Coders</title><meta content="Compressed Sensing and Natural Image Statistics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Compressed Sensing and Natural Image Statistics</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/p8u5Q491ENs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so let's get started
welcome to Microsoft Research colloquium
and today we're thrilled to have a year
wise here to talk about compressed
sensing and natural image statistics
there's so much to say about yeah year
in his early career he worked on layers
layered reputation for videos which
really created a new era for vision -
there's a bunch of problems to work on
and then later he he worked on leaf
propagation and really created he made
very big impact in this area to make VP
algorithms theoretically a lot of
theoretical contributions but also
pushed applications in vision so much
the circus of a week operate on papers
we worked together I really enjoyed
working with him so if one thing I want
to say I talked about him is you know
when I super my students asked him to
look at model papers when they are you
know working for the airlines and to me
that the first paper I could think about
in computer vision is yeah years 2001 I
Cecilia prepare on deriving intrinsic
images from videos in questions that
that says a lot about his working vision
and yet
much rather yes but something of
interest um so I was asked to prepare a
talk that would be accessible to their
in broad audience and I thought I'd talk
a little bit about a compress sensing
which is certainly the most dramatic
topic that I'm working on so I'll spend
just the first least 1/3 of the talk but
we may not be about my own work but just
explaining what fencing is and this
picture over here the broadness of the
audience hasn't showed up so the picture
that you're seeing here is from the l1
magic website which I highly recommend
so the compressed sensing I'll talk
about in a little bit what it is not
it's a very interesting set of
mathematical results but also it should
be said in their support that Donoho can
des the people behind this are strong
believers in your producible research
and so they set up a very nice website
where all the software is available and
this is the picture from that website
one thing I'd like you to I'd like to
point out is I mean this is a set of
mathematical results compressed sensing
but even though it's about math really
you can see that natural images are
front and center on this webpage and
that's really what the focus of my talk
the last two thirds of the talk are
gonna be is whether really natural
images are a good fit to this beautiful
mathematics and I'll show as several
others have shown that there's really a
disconnect between what the maths
assumes and what natural images are
really like and the last half for a
third of the talk depending on the time
it's about it our work actually the work
of a young sunchang who just recently
graduated here at MIT many people you're
probably seen him walk around the lab
he's sort of a quiet Korean student who
comes to talk to me where we've tried to
propose an alternative cut to compress
sensing
that could actually be relevant to
natural images okay what better
scientific resource and Wired magazine
so what is compress anything I'll just
read it's a paradigm busting field in
mathematics that's reshaping the way
people work with large datasets only six
years old compressed sensing has already
inspired more than a thousand papers and
pulled in millions of dollars in federal
grants in 2006 can Dez's work on the
topic was rewarded with a five hundred
thousand dollar Waterman prize the
highest honor bestowed by the National
Science Foundation it's not hard to see
why imagine MRI machines that take
seconds to produce images that used to
take up to an hour military software
that is vastly better intercepting an
adversary's communications and sensors
that can analyze distinct interstellar
radio waves suddenly data becomes easier
to gather manipulate and interpret I
like how money this is America right so
it's all about the money
five hundred thousand millions of
dollars but really I guess we had a
Waterman Award winner speak here two
weeks ago
it's got Aronson it really is a
tremendous honor and it really I'm very
serious people also not just Wired
magazine will agree that there is really
very significant mathematical results so
let me just explain these two in two
slides what compressed sensing is and so
it really deals with a very simple
problem so I give you a signal X and I
multiply it by a matrix a which gives
you the vector Y and the question is can
you recover X from Y and if we know
anything from linear algebra then we
know that if a is a square matrix and
it's invertible and of course we can
invert it but what if it's a rectangular
matrix so the dimension of Y is much
smaller than the dimension of X so sort
of most people will tell you you cannot
because you have a null space there's an
infinite number of possible X's that
could have caused the same way and
really the main result of this field is
that if you also have an additional
assumption namely that X is case sparse
so that's what this stem by stem plot is
supposed to show you here is a signal it
has some 512
elements but most of them are zero and
only a small number are nonzero so if K
is the amount of nonzero elements you
can still exactly recover X and ay is a
random matrix meaning its elements are
sampled iid so it's really just a random
matrix as long as the number of
measurements grows faster than K log n
then you can exactly recover it by
solving a linear program so not only is
it that is the information there that
allows you to recover it you can even do
it very efficiently
you could you could imagine doing this
by some combinatorial search algorithm
right you can search over all ends
choose K possibilities for the nonzero
coefficients but these theorems so you
don't even need to do that you can just
solve a convex optimization problem and
get X exactly okay so that so that was
the result in two slides and again
because they have this beautiful website
you can actually download the code and
if you don't trust the mathematics you
can actually try this for yourself so
here's a proof by MATLAB I take the
signal over here it has I think 512
elements in it but only some fraction of
them are nonzero I multiplied by this
random matrix and you can see that it's
a rectangular matrix so there's only one
it's a 100 by 500 matrix so I've lost a
lot of information I solve this linear
program it takes point 3-1 seconds on my
laptop and voila the original X's on top
we estimate X is on in the bottom and
everything's fine
so it really works not only is it is
there math behind it but it really does
work
you know it not only works it's even
useful to capture people in the audience
who are not paying attention okay so
there are a lot of interesting method
this is certainly a very deep and
interesting mathematical result for a
lot of mathematical results what is it
about this particular mathematical
result that made Wired magazine call it
paradigm changing and so forth and
millions of dollars in federal grants
really the one of the main motivating
examples has always been natural images
because if you look taking natural image
like this famous image over here and you
do something called a wavelet transform
on it doesn't really matter for the sake
of this talk what it is but there's
somewhat change-of-basis that you can
change from the sort of a trivial basis
for images is you think of it as a
matrix where every every pixel is an
element and the matrix and but you can
also think of it as a vector of
coefficients in some basis and if you
look in that basis called the wavelet
basis and you plot a stem plot like I
plotted earlier you can see that indeed
only a small fraction of the
coefficients are significantly nonzero
so this seems like a great match to the
theory and that it would allow us to now
capture images with a small number of
random measurements and people have
picked up on this idea this was one of
the top five inventions of the year
according to technology review in 2006
it's called the single pixel camera it's
a very clever idea that shows you how
you can capture images rather than with
an ordinary camera you just directly
capture random projections of a scene so
the way it works is there is a lens over
here and then they use these micro
mirrors such that such as existent
projectors modern projectors and each of
these projectors can be either oriented
towards the sensor or away from the
sensor
and this in a random fashion and this
implements around the projection
optically before you actually do an
analog-to-digital before you ever have
the image in digital form you can
already do a random projection this now
you can ask if you have a camera a cell
phone you know that you can get a five
megapixel camera for maybe ten dollars
so why do you really want to build this
compressed sensing camera and this was
pointed out by several people that maybe
a better application of this theory to
natural images is not so much in
minimizing the number of pixels but in
minimizing the power consumption so this
is a very nice recent paper by Boyken
algal mall where they show that
basically by using random projections
you can get image cameras whose power
consumptions two orders of magnitude
less than standard cameras and
essentially the power consumption in
these cameras if designed correctly goes
with us with the number of projections
and here's a quote from a recent I
Triple E spectrum article on their work
this is not from their a paper but from
an electrical spectrum it says you
merely pick random combinations of
pixels and some their intensities
amazingly enough for the sufficient
number of such measurements but far
fewer than the number of pixels you'll
have enough data to reproduce the
original image okay and again this is an
example of the pop the more popular
press really being very excited about
this but if you actually look at the
papers and see the images that they get
out they don't look very good so this is
from the single pixel camera paper and
there's the original on the left and the
40% compression I want them emphasized
there's not a factor of 40 compression
but rather 40 percent compression so
really the number of measurements here
is about half the number of pixels it's
not at all a dramatic savings and the
result really looks pretty bad
and you if you remember the math you
think Oh shouldn't the reconstruction
really be perfect and it's far from
perfect so what's going on here and I
think the problem is the popular press
didn't read the fine print and so it's
important to read the fine print and I
get this is again from the l1 magic
website no just read here is an example
the 1 megapixel image below has a sparse
perfectly sparse wavelet expansion it is
a superposition of 25,000 go p-- she
eight wavelets instead of observing the
image directly saying we took a hundred
thousand random measurements ie were
given the projection onto a ninety six
thousand subspace I don't know what
happened to four thousand but nevermind
then we look then we look for the image
whose wavelet transform has the smallest
l1 norm that has the same projection
onto the subspace the measurements agree
with what we have observed the result is
below as we can see the image has been
reconstructed perfectly so the good news
is here we have perfectly construction
with a factor of ten so this is really
dramatic this is what we'd like but
where's the fine print so we went from
what one megapixel four hundred thousand
or ninety six thousand depending on
where's the fine print this is not a
real image what was done here was they
took the image found its wavelet
representation found a twenty five
thousand most strong elements and zero
without all the rest and that's the
image that were showing the fear it's
not the original image it's actually an
image that's been reprocessed in a way
so it's exactly sparse and in fact if
you look I didn't want to belabor this
point but if you look at almost all the
papers written by mathematicians on
compressed fencing that have images in
them it's always this is what they do
they take the original image we throw
away all but a small number of wavelet
coefficients create a new synthetic
image and work with that no it looks
very good so in fact this image what
you're seeing here is after the
compression after all but twenty-five
thousand having zero it out and it is
in fact the viewers have the noise in
the in the remaining things that aren't
zero that what these will interfere with
your compression and what you read if
you didn't know you were these things
ever
he was already compressed before there's
rican spec you know being a friend of
been saying if she didn't confess it
before you ran this thing then you might
you then you might get enough garbage
right that's that's what I'm gonna show
yes today I mean this is could still
improve the compression if he was doing
these things out all right all right yes
will improve it by a lot the pushing if
that's valid if you're allowed to do
that because in the real world you're
not gonna be dealing with these
synthetic images you want to design a
camera that will take an image of a real
scene ok the best part is you are all
right you mean if you want the power say
to dry me in like yeah if you want to
really build storage this would it would
be fine to strip out the oh yes this is
a very important point I'll say this now
the whole point about compressed sensing
is you want to do the compression before
it's in the computer Sean maybe this
answers your question if what you want
is just to take an image and compress it
it's already digitally in your computer
just use JPEG none of this theory is
really relevant the whole point here is
you do this before you acquire the
signal and that's what allows you to
save power and and the point is that
this is not something you can really do
before you have it in the computer maybe
it could be very difficult I think
that's actually a we're an interesting
direction to go no I mean part of the
beauty of compressed sensing really was
always that they use linear projections
and they say linear is very easy to do
in physics a lot of physical processes
are linear something like this that is
nonlinear is much harder to do but yeah
or maybe you have an idea know lowest
pass filter it's easy to construct
circuits which actually specifically do
that no no anything linear is not gonna
do it you need
let me keep going hopefully answer this
question okay so actually again I just
want to emphasize that there is nothing
against this talk is publicly available
so there is nothing wrong or misleading
about what's written in the other one
magic website they have this caveat
which I'm going to read now it says in
the real world signals and images are
not perfectly sparse and there is always
some degree of uncertainty in the
measurements
fortunately the recovery procedure can
be made robust for these types of
perturbations Teva stability and
statistical estimation papers for
details and again this is what I think
Henry is referring to that there are
additional mathematical results that
show that even if the signal is not
perfectly case sparse then things do not
break catastrophic ly however there's a
difference between not breaking
catastrophically and actually being
useful from an engineering standpoint
and just to illustrate that let's just
take an image and not do this
pre-processing trick just run the
compressed sensing as is directly on the
images without zeroing out anything so
this is the image and this is when you
do a compression to 15% so approximately
a factor of 10 so again nothing breaks
catastrophic ly you're getting something
it looks like the original image it has
a lot of artifacts I doubt that anyone
would pay money for this particular kind
of camera but it's not breaking
catastrophic oh here's another one
this is 25% random measurements so it's
getting much better
again it's not brain Kennex to
catastrophic Lee you can see the
original image so I think again it's
important to say that it's not the case
that once you had a little bit of noise
all hell breaks loose but it is
important to go back and compare this to
much more trivial things you could do so
here's something very trivial you can
take this image and I just want to
retain 10% of the information
how can I do that in the easiest
possible way even easier I'm just gonna
take every tenth pixel
let's take every test pixel and then I'm
going to reconstruct by linear
interpolation and I'm not gonna do any
of this l1 blah blah but it's linear
interpolation between the samples so
this does much better than the random
projections followed by l1 minimization
and was 25% again the effect is even
more pronounced so just to this interim
summary the dramatic results of
compressed sensing are for ideal sparse
signals it there is some extension that
shows that things do not break
catastrophically for things that are
approximately sparse but that results
are no longer nearly as dramatic that is
for many signals what you can get with a
random projection followed by this l1
minimization is far worse than what you
could get with much simpler techniques
and I should again add that this is it's
more of if you read the original papers
not only do they have these caveats some
of the authors of compressed sensing
theoretical papers like Justin Romberg
this loose to get all papers co-authored
by Dave Donahoe have pointed this out
themselves that when you actually want
to run something like compress sensing
on more realistic signals you should not
use random projections no one remains
Asian but I think somehow this message
has not made it all the way to the
popular press okay right processing
rather than just one minimizer yes so
that is something they do and that
improves that can prove a little bit
that you can basically take this this
result on the right and run denoising on
it and depending on how good your
denoiser is you can get very good
results but of course you can do but you
can also do it on any other technique
you could it is possible if you have a
very good e noising algorithm to improve
these results but that has a lot less to
do with the compress anything as much as
it's a lot more to do with it you know
okay so what's wrong really again I
think I was this Greg alluded to this
and I think I I did this on purpose by
going fast I said wavelets of natural
images tend to be sparse I was sort of
it's they're not really sparse in the
mathematical sense there's sparse in the
statistical sense meaning if you look at
wavelet coefficients that you have a
small number of coefficients that are
significantly nonzero but basically all
the coefficients are nonzero you don't
have any unlike this idea of sparse
signal on the left or some are exactly 0
and some are strong and natural images
all the coefficients are nonzero it's
just you have a small number of
significantly nonzero and that really
breaks a lot of the a lot of them
beautiful dramatic mathematical results
so what do we know about real natural
images there's six again this is a very
well studied topic and if you look at
wavelet statistics there I guess two
things that people will say one is that
if you look at the distribution of
coefficients you have what's known as a
statistically sparse distribution
meaning these are heavy tail
distributions that are peaked at zero
but they're very different from these
idealize this is a histogram so we know
in a sparse distribution you have two
spikes one at zero and one elsewhere
that's not what real signals look like
they have this protonic distribution but
it's it's peaked at zero but there's not
there's not a spike at zero that's one
thing the other thing that's different
between these idealized signals and
image wavelets is that there's a way to
order the wavelet coefficients according
to spatial frequency and they're in the
variance decreases so it's not it's not
only that you have a small number of
coefficients that are significantly
different from zero you know in advance
which ones these are they don't occur
anywhere along the signal there's a way
to sort them so that the strong
coefficients will always occur on the
left with high probability right you saw
it you saw it on the spatial frequency
so every one of these wavelets has a
frequency associated with it if you sort
according to that not according to the
value of the wavelets you're right
otherwise it wouldn't be very very
surprising yes
okay so what do we just uh are we doing
on time good so I don't want to spend
all my time just talking about some
positive no I'll talk about this in
that's a good question it's really about
the measurements losing information not
about the reconstruct not the decoding
algorithm in fact part of the nice
results they show is that for certain
class of problems L 0 and L 1 are the
same again that's very beautiful
unexpected mathematical result so they
can actually show that they're not
losing and there's a small regime in the
middle where the L 0 L 1 or different
but for a lot of these signals L 0 can
be shown to be L 1 ok yes that's true
because you're arguing about the
statistical level of sparsity and that's
more like L 0 I mean yeah it's l0 would
be I mean of course you can't really do
it but if you could do it even if you
could do it that's what I'll argue it's
not gonna work well you've lost a lot of
information the projections basically oh
I see yes but you simply cannot recover
even and for information theoretic
setting okay so some positive results
are at least so what can we do so this
got us thinking so if you abstract a bit
what is compressed sensing it says I'm
gonna take my signal multiplied by a
matrix and I'm gonna reconstruct using
some additional knowledge on the signal
so let's abstract and specifically they
say let's use a random matrix a and
we're gonna use l1 Immunization for the
reconstruction algorithm but let's
abstract that out and assume that you're
allowed to multiply by any matrix that
you want and your reconstruction
algorithm it can be as complex as you
want in fact you're going to do the
Bayesian optimal reconstruction given
the measurements what's the best what's
the best matrix that you could use under
these settings
so here's a it's a very easy to describe
problem I give you a signal a signal
that I tell you the distribution over
signals and I'm giving you some budget
say you have number of measurements that
you're allowed design the matrix a so
that when I multiply 8 times X the
mutant I get why the mutual information
between x and y will be as large as
possible there's a technical restriction
here is that you can make the mutual
information the arbitrary large by
multiplying a by a constant so we
disallow that by basically the trace of
a transpose a that's like the total
energy in your matrix the sum of the row
of the norms of the rows so every
measurement you think you pay according
to how much according the norm of the
rows the son-in-law in the rows and
we're going to constrain you to have a
limited power budget what's the best
matrix you could say you could design
why can't you just make it of course not
today then sorry
it's dimension constraint - yes it's a
pension constraint as well yes a SP by P
because you could take any matrix light
you give me I'll multiply it by a
hundred the mutual information is gonna
go up it's a fact about mutual
information that it's unbounded as you
uh so we we don't want to look we don't
want to allow that and also in a sort of
a more practical setting if you think of
the matrix a is being something like
like radiation or something it makes
sense to constrain that anyway so I
think what's interesting about this X is
coming from the distribution individual
high dimensional distribution that you
know I'm gonna give it to you for free
and you just need to design the matrix a
that's optimal for that distribution and
what's interesting about this is you
would think this is very easy criterion
to write down you'd think it would would
have been solved in the 50s or something
everything's linear here it's very easy
to write down I just multiply a I want
to find the matrix a so that when I
multiply a by X the mutual information
between x and y is as large as possible
but I want to convince you that it's
actually much harder than it seems so
let's do some examples okay here are
some examples of signals I'm gonna test
the audience to see if you can design a
matrix so here's some samples there's
more samples more samples or samples and
I'm gonna give you a budget of two
measurements okay so you can either
measure individual coordinates if you
want or you can measure averages of
coordinates
but you don't know if you measure an
average they have to be a mint l1 normal
l2 normalized so you can decide if you
want to measure say a random projection
or if you want to random projections or
you can take the average of all the
coordinates the difference between the
first quarter in the second coordinate I
give you two of these measurements
and this is what the class of signals
looks like what would you do well so one
thing I said I hope you can see is that
most of the action in the signal is on
the left hand side of the signal so in
fact these are samples from a Gaussian
distribution that has decreasing
variance so it has more variance on the
left then on the right and so the
optimal thing to do in this case was
solved more than 50 years ago it's
called principal component analysis
which in this particular case would tell
you to measure the left two coordinates
of this thing okay here's another set of
this another set of samples oops okay
what do you think so one thing I hope
you can see is again the variance
decreases so when this is a very sparse
distribution
unlike the Gaussian when there is a
spike it's much large tends to be much
larger on the left and on the right so
again if you were to do principal
components we would get the same answer
as before it would say measure the first
two in fact the best two projections for
this case are let's say all of it on the
second yes that much are a terrible
match you can do this well to sum the
squared error in your reconstruction is
what we're you take the vector in RN and
measure a squared error doesn't make any
difference should have been of the first
in this in this particular case no it's
just the norm of the vector is the
including over of the vectors so you
have to get it correct
in fact what I'm gonna show you now that
doesn't this metric doesn't really
matter because I can design two
projections that will get it exactly
right this particularly couldn't be
could be that measuring the last bit of
the best you see in the picture but
there's a slight amount yes that's right
when you look if you but like in the
real in the real world I'm gonna give
you the district the full distribution
so if that's the case you would know it
I'm gonna give you the full distribution
so in this case I'm telling you the way
I generated this distribution is I first
choose a random coordinate and if it to
turn on and if I turn it on I give it
either plus 1 or minus 1 times some
variance term that decreases so we see
there plus one or minus one that's a
very left or plus point one or minus
point one on the very right that's the
way this that's the true distribution
sorry no Gaussian in this case it's
either plus 1 or minus 1
yeah so basically with two random
projections doesn't really matter you
can also do it in other ways but well
okay there's no noise you ready could
you even do it with one projection but
suppose I also I make a Gaussian
distributed unlike what I do here then
even then two random projections will
give you a perfect reconstruction
doesn't matter if you're using a 1
normal twin norm to measure you'll just
get it perfectly and what's the worst
thing you can do under this case is
measure the first two coordinates so
here PCA which was optimal for the
previous thing is actually the worst
possible thing you could do you look
because what happens is your new PCA
you'll get it right only if the spike
happened to be in the first twin you're
totally blind everywhere else it's
actually the worst possible thing you
could so I hope I've convinced you but
this is a non-trivial problem it's not
totally clear what the right thing is to
do ok
so in his thesis soon almost well made a
lot of efforts in trying to solve this
problem but we in particularly
considered the following set of signal
models
where every coefficient is drawn
independently from some sparse
distribution from some distribution
which could be either sparse with dense
so it's been very sparse like here over
left on the left or less sparse like
here in the middle and on the right so
it's a statistical sparsity that is you
you every coefficient is independently
sample from this distribution and the
variance could either be the the same
along all coordinates or change as you
go from left to right so this is a class
of signal model that includes the two
things I showed you earlier the first
one I showed you was a Gaussian with
decreasing variance should be a non
sparse distribution with the particulars
it sparsity pattern and this one is not
exactly
well one sparse signal you can
approximate by sampling every coat every
coordinate independently from a very
very sparse distribution and for this
class of signals we want to find the
matrix a that maximizes the mutual
information between x and y i should
forget to say that this problem was
actually formula the first formal
definition of this problem is from at
1990 my Linh skirt it's called he called
it info max and he showed that the
solution for the case of the gaussians
which is indeed the principal component
analysis the principal component
analysis can be derived in all sorts of
ways but among other things it's also it
solves this info max problem if I give
you K measurements to sit for a high
dimensional Gaussian distribution the
best you can do in mutual information
sense is to measure the first K
principal components and that's it's in
question it's about it's in
computational neuroscience so his claim
was that sensory systems in the real
world should have representations that
maximize the mutual information from
what's going on the outside world in the
brain and he that was that's where he
for my info max and he was trying to
solve it analytically and and he solved
it for this particular case
but until huge thesis a few years ago
there has been no solution for anything
other than the Gaussian to show you how
how difficult this is it really seems
misleadingly simple because you can
write down the problem very easily but
it's very hard yeah and so most into men
it can be a multi-dimensional Gaussian
it doesn't have to be IAD yes but you
can that's not hard to find the
coordinates yeah yeah if it's if it's
along their standard axis and the best
thing to do is to measure the standard
axis but if it's along other dimensions
then you measure those cajon hey
directions ok so here are some results
that you managed to get and these are
all our subsonic results but they are
globally optimal so one is in the case
where the variance is the same along all
dimensions and turns out for a broad
range it doesn't matter what the
distribution is as long as the variance
is the same in all directions these are
called white signals in signal
processing random projections are
optimal in fact you can also think of
deterministic matrices that I mean it's
not if and only if they're they may also
be deterministic for certain K and n
certain particular dimensions he's also
managed to work out the true mystic
instructions that are as good as the
random ones but at least asymptotically
the random ones are always optimal you
make sure that kind of sure your second
moments all do what you want but again
the assumption is that there is some
basis along which every coordinate
sampled iid so it includes this very
sparse distribution but yet I believe it
now yeah I
I guess since this was not supposed to
be technical talk this is uh this is
related to the fact that people like the
scooped and others have shown that
random projections make things Gaussian
so if you take any high dimensional
distribution you project it down with a
random projection it becomes Gaussian
multi-dimensional Gaussian and then the
Gaussian is the maximum entropy
distribution and that's really where it
comes from but anyway we had to work a
lot harder than that because a lot of
those results only work when the number
of projections is much much smaller
growth sub linearly with dimension and
this result actually holds in a much
broader setting okay the other thing
that he could show is that if you have a
very skewed variance doesn't have to be
Gaussian anymore but if all of the
energy is say on the left
you know the predominant energy is on
the left hand side you might as well
measure the left hand side and again we
can show this so PCA projections are
optimal even when the signal is not
necessarily Gaussian but if the variance
is very skewed okay but the problem is
that images are neither of those two
cases
so again images when sorted accordingly
do have a skewed variance distribution
but they're not that skewed they're
certainly not Gaussian on the other hand
yes it depends on them okay this is this
is the sparsity level if if if if it's a
Gaussian and doesn't even if it's not
skewed then it's still off right even if
it's even very very narrowly you'd so
the point is the sparser it is the more
you need it needs to be more skewed so
we have this general theory on the one
hand that doesn't directly hold for
natural images we have a different
result that holds for what he calls
multiracial multi-resolution signals
these are signals that natural images do
fall into that category that
the natural bandstructure so if you sort
these coefficients in a particular way
they will divide be divided into the
separate bands and every band has the
same variance and that ends up allowing
us to Vanessa metonic result which shows
that the optimal thing are these band
wise random projections so for you
basically have some set of random
projections that only look at the first
two coefficients another set of random
projections that only look at the let
next two and so on and so forth and the
number of random projections per band
will depend on this how sparse the
signals are and so they actually have to
solve a linear program just to figure
out how many projections per band but we
can do all that and we do have something
that is asymptotically the optimal way
to sense natural images so now we can
look at some examples and I think one
important thing to realize again so I
talked about all the ways people can
choose play a little bit with the
examples in order to get good results
with compressing my natural images one
way is by choosing the image so you'll
often see this cameraman image on the
left in compress sensing papers which of
course was taken if you can see the I
might it's on the MIT campus it's taken
I think on the soccer field where they
just had the memorial service that is an
extremely sparse image so this JX number
is what's known as the mega entropy it's
the distance the KL divergence between
the distribution of coefficients and the
Gaussian so 0 would be Gaussian the
higher that number is the more non
Gaussian it is so the cameraman is a
very non Gaussian image if you look at
another more standard image perhaps like
this one over here which by the way the
copyright of this image is owned by my
university the Hebrew University of
Jerusalem so these are more typical
images they have a negative entropy that
is smaller that is if you look at the
cameraman it really has this big
piecewise constant areas partly because
it's a very old image and so a lot of
discretization effects are there but
real images are less sparse
if you look at this then you see very
different results so in the cameraman
so these show the psnr so high values
are good in the reconstruction this is
the fraction of pixels of measurements
over pixels so this is very high
compression ratios here and this is a
factor of two compression and the red is
CCA and the green is random so for the
cameraman image at least after some
compression ratio actually random
projections do work better than PCA
because it's a very sparse image it's
close enough to this regime of idea of
sparse signals for the Einstein image
that's never the case PCA is always
better to do so of course you in theory
which optimizes truth the particular
distribution is better than both well
the very close to PCA in the eyes so he
uses the j x equals this particular he
optimizes his matrix for a distribution
over coefficients that has he does this
distribution of that distribution in the
two cases so you're writing it's a form
of cheating because you assume that you
know for every image the distribution of
coefficients a more interesting example
oh let me just the other thing although
it's better these are not huge effects
so if you look here these are the
reconstructions and then I guess you
were asking about the post process and
this is doing this what's known as total
variation denoising afterwards so all
the algorithms are doing a little bit
better when simple l1 would have
predicted
so in PCA the results look very good in
a sense that there's no artifact but
there are no high-frequency so again
just remember the MIT campus there
should be columns here but you don't see
them because they've been blurred out in
this band wise random projections you
see the high contrast columns here much
better and the random has more high
frequencies but also has a lot more
artifacts here so
you like the same pressure in the game
the game you get a fixed point I don't
know you know I mean I'd like to you
that this cameraman image already went
through a PCA at someone because
somebody might have yes it certainly
went through all sorts of cleaning our
effect before they digitized it yeah my
guess is that you would converge to
something that's very sparse that
wouldn't look like the original image
but would be a fixed point yes five
thousand in this case I mean because we
can visually look at these things but if
you're talking about quantitative
reconstruction which is what you're
optimizing how are different so okay
then these are the numbers down here
these are these numbers the the psnr
numbers the higher the better
so paci is better than random and these
band rise random is even better
kearson are is the log of the mean
squared error negative okay okay so it's
it's like the means great error but in
the logarithm coordinates and high is
good just you could even yes you should
say that PCA is not necessarily optimal
when you do nonlinear reconstruction
even in the mean squared error sense
right if you remember the previous
example with this very sparse signal you
could get zero means great error with
random projections PCA is optimal in
terms of mean squared error when you do
linear decoding non linear yes and it's
the same for all the algorithm okay but
in general images have a range that can
be highly sparse like over here or a lot
less sparse like over here and on
average the this negentropy is around
0.5 or 0.6 so the cameraman is reading
that liar and if you look at Lena which
is what we started with it has again
something like 0.5 because of all this
hair over here
which really kills the or the feather
sorry
which really kills the sparsity
assumption so let me just finish with
showing the results on this since we
started so this is compare sentencing
was 25% ran the measurements and this is
reconstruction from PCA with same number
of measurements and it's really much
much better both in peace in our n terms
of visual quality to implement this in
hardware requires very clever
engineering to do all these micro
mirrors to get the random projections
its implement this in hardware we just
defocus the lens and you have a low-pass
filter because no so pick up pca for
natural images it's always going to be
equivalent to just blurring the image
you measure the low spatial frequencies
this has to do with the fact that the
distribution is stationary so you don't
really need to know anything about the
images for any stationary distribution
the principal components are sinusoids
so this is very easy to do in
engineering you don't get to be labeled
the invention of the year but it does
work a lot better you you could the but
the gains in this case in the power
consumption but gangs that you tend to
get are the same as the factor of the
compression ratio so getting this factor
of 4 I don't know how many people would
be would be willing to do it but I think
if we could do a lot better than that
yes this is also true by the way in this
megapixel battle the old cameras had
longer battery life if you're just
willing to live with a 1 megapixel
camera on an 8 megapixel camera you have
8 times more battery life but for some
reason consumers don't haven't figured
this out and
but yes you could I think there is a if
you're just gonna mail them in email
this later it's the one that could fix
this okay
people have written about this megapixel
arms race whether it really makes sense
letter anyway I think there is room for
some of these ideas to make it into low
power consumption batteries we're not
quite there yet okay let me end I think
one take-home message in this case it's
the same again because if the images are
the number of bands allocated to the low
spatial frequencies are all of them
basically it again depends on this J X
number if the negative it's very close
together if it's close enough to
Gaussian then that's skewed enough that
DJ is the optimal thing to do yeah I
should show that so it's much worse than
JPEG this again is at this point the
Chum was that Chen was asking about
earlier JPEG is a nonlinear compression
algorithm so you can basically get it
for this image you can get similar
quality with say a factor of ten
compression artifacts before if you're
willing to do non in your compression
they don't know what the image is just
the same thing there is typically take
it looks at the image no nominee oh
really means long linear in the sense
that for example if you were if you
could take the coefficients a very good
compression algorithm is to project it
threshold and then do run length
encoding on that that would be nonlinear
because of thresholding operation and I
think in that sense there is a great
opportunity from an engineering
standpoint to do interesting things if
you really want to get power consumption
down you should think about doing
nonlinear he could do not only your
thing before the digital art in the
analog domain
you could do very interesting things
much better than in our approach because
we're constrained only to linear but for
some physical standpoint linear make
sense so light 10 style operate linearly
or waves okay so one take-home message I
hope that everyone here who leaves this
room will then go on and spread is that
it's important to realize that the very
dramatic theorems of compressed sensing
really holds for ideal sparse signals
not natural images there are additional
theorems that hold in the in the
non-ideal case but they're not a lot
less dramatic they don't give you
anything the same sort of magical feel
that you get with the ideal sparse no
one would have a website called l1 magic
for the results with the for the
non-ideal sparse signals we've proposed
an alternative formulation which you
call informative sensing which I think
it's a very fun problem to think about
still open how do you design the optimal
measurement matrix given knowledge of
signal statistics and I think we've
solved it for a subset of signal models
but there's still a lot of work a lot of
other signal models that one could look
at for natural images we've shown that
the optimal measurements depend on
sparsity and for most natural images
measuring mostly low frequencies which
is what PCA will tell you is optimal
however again that I should say this in
defense of the comparison some people
for highly sparse natural images random
projections are actually optimal also
according to our theory and this is
certainly surprising I'm sure nobody
before 2006 would have thought to apply
random projections natural images and
there are actually settings for very
sparse images like the cameraman even
according to our theory this is actually
the best thing to do and maybe a final
point of negativity is that because
we're restricting ourselves to linear
codes even with optimal projections you
can only expect modest compression gains
thank you
right yes you can do much that's called
okay I should say one place where this
informative something just like
informative sensing has been studied is
known as Bayesian experimental design
which is exactly this you have a set of
measurements you want every experiment
in this sense is to multiply by a matrix
so you want to test some hypotheses
about the distribution and the
hypothesis and you're allowed to do your
measurements and they're sequential
experimental design something that's
very well studied and you can do much
better certainly doing them one by one
and deciding which projection to do
based on the previous ones improves
performance by a lot for the one pixel
camera I didn't say but the one pick of
camera is actually a sequential thing
that's another problem it has it can't
capture moving scenes it captures one
pixel at a time it keeps moving these
micro or a micro mirrors you know maybe
for other applications like medical
imaging
maybe it's it's really an engineering
issue of whether you can afford to
measure one thing and then change the
imaging setup before the second one
comes along I think both how are
interesting application domains to look
at for a standard camera it's that you
want to capture a lot of measures at the
same time no you have to do something in
advance
do that a really low resolution and then
take a normal 2d one at a medium
resolution and use 3d information to
form the basis that you use two entities
- that'd be better than using a wavelet
basis I don't ya get massively yeah it's
it's something similar to what I do it's
a special case in something what Adam
was saying because you'd have some
sequential algorithm where you do one
thing and then you change the camera so
I'm sure it would be better well there
would be significantly better - because
you have this problem of and you'd also
need to be able to do everything fast
enough yeah so I think that's in the
same classes what Adam was proposing
with doing sequential I think for for
most cameras you probably can't cannot
afford to do that because you want a
picture take a picture immediately and
you don't want to wait but I think for
some medical imaging settings or the
patients in the MRI machine frighten
them for an hour maybe you can do
something but on the other hand the
hardware there is very difficult to move
so maybe if you figure out the optimal
predictions it it's hard yeah I don't
know power consumption sure surely
there's a market for like super
high-resolution picture it's like say I
want to take a picture and I want like
you know I want like a hundred gigabyte
picture of you know something to print a
wall-sized portrait that's like a
beautiful okay
I'm sure there are people you know or
like like old school photography
everyone had these big cameras with huge
things really unwieldy yes blow their
picture up to be wall-sized yeah there
to look like you're looking at a window
and I haven't heard anyone like you're
like oh actually you know no we're not
trying to say the battery or anything
we're just trying to somehow make it
possible to get ridiculously high
resolution picture
out of the lowly you know ten megapixel
CCD chip on the camera like it's the
same issue I mean whether you going from
when they get picked up to a hundred
thousand they're going from no but it's
more saying like either were either you
can get these compression ratios or you
can right I mean it's not no no like why
do you think they'll be sort of spin me
I mean you're not gonna find a thousand
megapixel CCD chip for less then you
know five million dollars question is
compression the kind of compression
ratios that you're gonna get using
natural images are about a factor of two
I mean that it's the same assuming that
the statistics are the same at these
high-resolution there's a question like
is the behavior different from the Caleb
versus kind of the head because if
you're looking at global reconstruction
you kind of see this a lot when doing
PCA is like you know everything you know
you get a ton by just measuring the
image intensity or something like that
but in a sense where this is I can I
look at the image statistics when I've
already got most of the image already
and now how gives it to reconstruct kind
of the question anyway actually there
was an interesting conference if you
were interested there was a conference
that just ended at Harvard over the
weekend called the ICC P international
conference on computational photography
and there are people so computational
photography this general field that
combines computers with cameras and uses
algorithms in order to generate so what
they're they were actually demonstrating
a camera for for taking very
high-resolution pictures of paintings if
you seen a static there are certainly
Hardware things you could do that would
take an ordinary CCD thing and shift
thing and build it back together again
yes we'd be forced to do you know we'd
only had one pixel cameras be forced to
do something like this right yes I think
the point is even in that setting you
would not want to do random projections
you'd be better off doing something else
sure no I don't think it yeah and it's
not gonna give you the unfortunately
this factor of a million if you want
higher it really
high-quality you're not gonna get or
acceptable quality you're not gonna get
a factor of a million your factor of two
well because what we can what we
assuming our signal model is correct
then even under the optimal projection
if you want acceptable quality you can
afford about a factor of two and I want
a nice image so what you do you do a lot
of these projections you somehow recover
something rather yes no factor of two
like the number of projections you end
up having to do is equivalent to the
resolution you want in the end yeah
which is if you're okay with that that's
fine I'm just saying like you know if
you could get a camera that says oh it's
a hundred megapixel camera although it's
only built on a 5 megapixel CCD chip
then cell phone companies would be
knocking down your door so hard to do
right it's not so hard and I'm wondering
why people are I think people do build
you again you people have built these
things for like digitizing our cultural
artifacts actually this is a Microsoft
Research Asia project but I think it
really works well when the world is
static most consumers want to that's not
the that's not the killer app for most
consumer cameras right we want to also
be able to photograph things that are
moving and so we can't afford to take
lots and lots of pictures yeah so the
blur really kills the sparsity
unfortunately right but does it change
what makes DCA even more it would make
well I should say one one really
depressing thing for us so we have this
theory and we're really looking for an
application we can do much better than
pca using info max and these and it's
very difficult to find them for it you
know we've gone through application
after application pca is just very very
good
maybe we'll find something in the end
all right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>