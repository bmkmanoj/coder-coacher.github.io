<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Video-based In Situ Tagging for Mobile Augmented Reality | Coder Coacher - Coaching Coders</title><meta content="Video-based In Situ Tagging for Mobile Augmented Reality - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Video-based In Situ Tagging for Mobile Augmented Reality</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/O-kSXETELmo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so it's a great pleasure for me to
introduce 10 league he's from the
ubiquitous virtual reality lab at the
gwangju institute of science and
technology in Gwangju Korea he will be
talking about his work in mobile
augmented reality one will received his
bachelor's degree in mechanical
engineering from Hanyang University and
soul and then in in his master's degree
in information and communication from
from gist and he's currently about to
get his PhD at gist on this topic and
he's here visiting as a gratis visitor
for three days yesterday today and
tomorrow along with his advisor
professor moontak whoo-hoo is a
consulting researcher here for this
weekend next week so if you like what
you're hearing today and want to talk
with either of them would say this
afternoon or tomorrow afternoon or some
other time please see me in person right
after this talk or you can send me an
email Phil cow to set up a meeting time
with them so one was current interests
are in silhouette segmentation for 3d
reconstruction real-time object
detection for augmented reality and GP
GP use for high-performance computing
and we'll let him talk about his work in
mobile augmented reality thank you here
good afternoon I think it will be a
little bit sleepy after your lunch but
please focus my presentation so today
I'm going to talk about video-based the
Institute aching for mobile instability
itchy I have worked on doing my PhD
studies so the online of this talk is as
showing me the slightest I person show
some research overview why hot I have
done and I give you some introduction
about Oh
to the reality and then I go through
some details over my work and I conclude
the presentation with the future so one
of my researches which is ongoing is
texturally subjected detection using
depth information so using depth
information we can do many things as you
know so providing depth information can
help us detect some complex 3d objects
which does not have much textures so we
use LGBT we combine the lgb and depths
information for our target dictation and
then estimate the each port using
siddity shape of a target object this
video shows the detection wizards by
overlaying the name of the target
objects on top of the on top of them and
this part shows the advantages of using
depth information when we use RGB image
only by it really affected by lighting
conditions in this case it's fun light
but we combined both information with
depths you can do some local state
detection and also I think condition
problem happens when there are serious
shadows on the object like this so we
can do the same thing by combining LGV
and depth information so this is my one
of my ongoing research it and this is a
little bit apart from the topic today I
present and things interesting thing is
using depth information we can't know
the size of the target object and then
even though there are two object which
has the same textures but have different
scales using depth information we can
identify it
one from the other so this is
interesting plateau using types
information okay so during my PhD course
I focused on some computer vision
techniques that is poor the construction
of 3d objects and then the reconstructed
is really contents augmented used for
augmenting real scenes by poacher pot
pie augmented reality applications so
basically I assume that the users takes
photos their mobile phones and then
there we have collected the photos from
users and then it can be construct from
3d objects using the photos and then 3d
model and M it from its texture we can
create a realistic 3d content and then
the realistic 3d content is over rated
or mojeed in the real scenes oriented
relative techniques and then we will get
the some kind of a more realistic
peeling from the AR application so from
this viewpoint the system's works like
this the user captures photos from
mobile phones and then the photos are
transmitted to the modeling server and
in the modern example we have to do some
3d reconstruction but before that we
have to identify the target object of
prom there or set of photos which is
taken from multiple viewpoints so we
pushed to multiple schilder segmentation
and here we using some color and spatial
consistency measured in 3d and to
identify the program's object from the
geez so this video shows how the work
how they measure the works so here there
are photos taken from multiple
viewpoints and then we first initialize
the silhouettes of target object by
intoxicating to in volumes of each
cameras and then we and then we
iteratively optimize the target objects
children in all the views at
simultaneously and after doing some
iterations you can see that the Poland
object is a region sir will define and
then finally we get the silhouettes of
the year foreground object so here okay
and after finding a shoelace of target
object will build a 3d objects from
Charlotte and color images so next part
in the end is 3d reconstruction and here
i worked on building a smooth visual
horror of peter hall from shallots and
color images so in this case we have
images and shallots and then reconstruct
visual her and but the video here is
quite simple reconstruction method but
the we aim to build all smooth surfaces
by intersecting or silhouette to use it
and in in augmented reality applications
so we iteratively defined this vide
model to get a smooth message and why we
have to do some textures
then we have a 3d model over the one
object in the portal and then those 3d
reconstruction is transmitted to again
mobile users mobile phone again and then
is then used for augmenting their sins
through mobile phones so this demo shows
how we do it you just take a shot from
target objects and then we do some
automatic learning on mobile phones and
then the target is instantly detect over
from video sequences taken from mobile
phones camera so I be considered model
you construct a 3d model from the year
and ponce and then and then you can see
that the reconstructed is the model is
then overrated in the lira scenes with a
proper or six degree-of-freedom pose
so this was my research introduction and
then I will today I will talk about the
the part of the rest part of my walk I
showed a previous slide so the yeah
so people I get into details I
personally mention about the concept of
ubiquitous virtual reality which is
which aims for virtual reality
everywhere so in ubiquitous virtual
reality concept the real and virtual
world is molded in all elemental the
reality space where the real entities
are mirrored to put your entities so the
connection of the real and virtual
entities importantly this concept to
making interaction between the poacher
and real world so to connect the real
and virtual entities we have to
recognize the 3d objects or any way
target objects from images sequences and
my work is about them
this figure shows one some kind of a
pigeon or mobile augmented reality which
can be regarded as a subset of Olivia
concept in this figure the real object
in connected to virtual entities usually
we call error notations so so the
poacher information is operated to the
real scenes through camera and then the
poacher entities is geometrically
registered with the pod oh dear objects
but the problem arises when we choose
these things in outdoor scenes where the
scene is unprepared I'm prefer the same
means we have not much information about
the scene I mean in like a 3d geometry
of the scenes so in this case even if we
want to do some annotations on a
specific object we have no way to do it
usually using contribution techniques so
because we don't know the target object
at that moment so we need is some kind
of online learning and detection or
target object institute to make a user
to be able to interact with the target
object without prior knowledge so we
propose an augmentation method with
minimalist the user interaction which is
very simple pointing should approach
this video shows the overall overview oh
my method when you take a shot with a
mobile phone camera as you see that the
target object is can be detected from
without any difficulty interaction and
then users can add a virtual
augmentation on target objects so the
advantage of this method is it is very
simple and it doesn't do any complex 3d
reconstruction of the scene and also the
users can detect target object from a
different viewpoint order the input is
not as many from there's cameras
so yeah here so my message they have
some assumptions so the the input to the
algorithm is image of the target object
usually I assume here that the target
object is planar and the output is the
patch data and its associated camera
poses to retrieve with the six
degree-of-freedom pose and the
Assumption here we have known camera
parameters like a focal length and
principal points and also the reassuring
I assume that the target object is
either horizontal or vertical features
which are very common in the real world
so from these assumptions target
learning procedure loans in as shown in
this slide from the input image I post
the computer pronto para review which is
the normal view point two features of
the image of a target object taken from
normal view point I will explain that in
detail later and the target object is
learned from is it's learned from the
front of peer review by warping the
input patches and you applying some
blurring and I do some post processing
so the front of peer review generation
step is whopping the source seemed easy
to our pronto pero review and the
template learning step is for building a
template data from the input images so
the first the process step or will
learning the target is the pronto pillar
of your generation and you some of you
may wonder what the front of peer review
means so the front of peer review is
figures like this so it is taken from
the normal view point the the objects
and the camera has no orientations
orientation changes so usually in
computer vision based type object
detection methods it's a pronto pero
review is required to learn the planar
surfaces so the situation is the users
camera is at the same height of the
target object however in real world this
is not always happen so this situation
is more common the target object is
lower than the users viewpoint or higher
than usual point in this case the images
acquired by a camera is have a
prospective distortion because of the
cameras characteristics so not always
the frontal views are available in the
real in practical situation especially
for horizontal surfaces you will not get
the you you may not don't you you may
not want to get a picture of an object
from this viewpoint
just usually see from Heather have some
angles relative to the target object so
from these photos it is not possible to
retrieve the correct template data which
can applied for detection and project
nation so the objective oval pronto pero
review generation is whopping those
source images to make it a sin from
frontal view so the approach here is
exploiting mobile phone's built-in
sensors especially the accelerometer
which provides the direction of gravity
so we combine adding some computer
vision techniques and ponds answers and
we do this very easily and pests so let
me talk about a little bit more about
accelerometer sensor it provides the
direction of gravity in points local
coordinates and the gravity is normal to
horizontal surfaces and parallel to the
particle surfaces then so the direction
of gravity provides very strong good
information about the horizontal and
vertical surfaces which I want to
interact with
so let me talk about first the
horizontal so pissed cases in horizontal
surface case we can assume that there is
only one degree of freedom relative in
in the orientation relative to
orientation between the camera and the
target surface well usually it is not
usually generally it is not it does not
have it has more than a one degree of
freedom but I made a sumption that there
is only peachy rotation so from the non
camera matrix we can set the frontal
view camera which is both 442 our
identity pose and let me define the the
capture the camera has some rotation and
translation we can compute the rotation
from the accelerometer bill you directly
and then it's translational parameter
you can also be computed from the
rotation and some distance which he was
pretty find as a focal length and from
the known rotation and translation
parameters we can compute the homo
repeated what the input image e to the
pronto pero review and the th is the
homo gap here so doing that by simply
from the input image e we can rectify
the image that but to to make the result
have a right the pronto pero help you as
shown in this slide so you can see that
where it the rectification is quite good
even though we do we don't do any image
processing here
so horizontal case is very simple but
how about the particle surfaces okay we
we can wake on also make an assumption
of one degree of freedom rotation to
particle surfaces like a the images
shown in the top but in general what
complex case is happening in real world
is shown in this here so users have some
more or in orientation tip maker some
orientations using his hand and also the
particle surfaces can have a orientation
relative to the user so what about these
cases so we the sensors can not solve
this problem so now we add some computer
vision techniques to make this problem
easier so in particle so pissed cases
our fridge is using the painting point
which is very a straight port to find
the orientation of a particle shop is
the relative to the camera so here the
accelerometer again helps vanishing
point estimation so by estimating the
painting point from Rhine segments the
orientation is retrieved from them and
then the rectification is done by the
same as we do in the horizontal surface
case
so let me explain how the accelerometer
can have this procedure so here the
banishing points in protocol direction
can be expressive as the projection of a
pointed infinity to the camera and then
the the projection procedures
multiplying just cameras internship
parameters and extrinsic parameters then
stop applying the rotation and
translation to the Tour de Janeiro point
at infinity is giving us the point at
infinity in the camera coordinate system
and this is the this coordinate so the
point at infinity that is in camera
coordinate system is the same as the
gravity Direction measured by the phone
accelerometer because the protocol
duration is the same and its direction
is measured by phone exelon retain its
local coordinate system so by just
projecting the gravity values two
cameras coordinated system gives us a
lot estimation over pension point in
particle directions so
this is a this this helps of estimating
the protocol vanishing point because
this estimate this logo estimation is
quite good actually so too from line
segments we have some line segments in
the image and then we also have the logo
estimation of protocol vanishing point
by projecting the accelerometer values
to the camera and then using that the
lobby estimation we have to do some
refinement with the Lancer optimization
approaches we find the identify some
political lines using a distance
function from the particle vanishing
point because every particle line should
pass the pension point in the image
image coordinate system and we do some
refinements iteratively and then we will
get the good estimation of a protocol
pension point here another finishing
point I have to find is the patient
point in horizontal direction the
horizontal direction based in the
pension point in horizontal direction
can be found from using the pension
point we previously estimated in
protocol Teresa some also analytic
constraint between vanishing points here
like this equation shows that and we
made us some n hypothesis using this
also analytic constraint and then tues
Ryan clustering using charcoal distance
which is stupendous it is something like
this but I will not explain this in
detail you can leap or the reference
here
and then to some rank on the string and
merging the cluster sets and to some
iterative estimation we will get the
horizontal vanishing points from the
pest cluster so now I get the tool
pension points from to pension points in
horizontal and vertical and then from
this the orientation of the surface
planar surface can be retrieved and this
is nothing much difficulty part so the
advantage of using accelerometer here is
the speed and robustness in case of the
speed if we do the vanishing point
estimation in conventional way which is
just most of them are used to some line
clusterings it takes so much time on
mobile phones but the mobile you know
that mobile phone says still have a less
computational power compared to the
pieces so it is where it becomes very
slow mobile phones however using a
pension point as I explained you can
correctly estimate the protocol pension
points in quite good accuracy so it
makes the problem easier so as you can
see this pig early using the
accelerometer the speed of vanishing
point estimation becomes by pests and
the other is global stage when there is
a very complex when the scene is very
complex some finishing point estimation
using run close studying sometimes pace
because there are not much or horizontal
or vertical lines sometimes so however
again we know where the protocol pension
point is with a good accuracy so that
you can find the pension point even in
very complex cases and in this in this
case there are not many horizontal and
vertical lines but we still can to the
job Barry we're using the accelerometer
okay until I explain the how we get the
pronto pero review of the target object
using accelerometer sensors and then now
we are ready to acquire some template
data from the target for detection so
next part is templates the running using
upload fetches so the objective all with
this template based learning is to
acquire some data of the from the
textures of the the pronto pero review
we made it in previous step and we here
we adopt the approach of petty running
which was proposed in 2009 in CBPR which
is poor or pest petty running by
linearizing whopping procedure and it
uses a min patches as a petty descriptor
however it is the problem with this
method can be applying this to mobile
ponies the memory requirements of the
method the original method requires all
about the 90 megabytes here to Road
precomputed data for faster running
and the performance also problem on
mobile phone cpu at a time so instead of
using a bean patch itself we try to
mimic till the original algorithm by
applying some blurring method so how to
compute mean patchy is from the input
pesci the input petty is warped from
several people on the viewpoints and
these patches are every seed and then it
gives a min patch but our method is just
apply some blurring to original patches
and then get the similar resulting
patches and I call it plot a bitch
so how let's see how we do it so
applying this applying the some set of
clothes to the image ii requires some
time on mobile phone cpu so we try to
exploit mobile phones GPU from a
capacitor so our blood patch is computed
through a multiplayer rendering scheme
shown in this figure let me explain the
step each step in detail so the first
step is from the input pesci which is
what the true pronto pero review we woke
the input project to some another
viewpoint to make a detection in varying
viewpoints and this whopping is replaced
by rendering in a frame buffer on GPU
because it is much faster than the
working on the cpu and then the loading
party i come here so in the third piece
the Gaussian I know in the second phase
that we apply radio running to the
Warped Apache and this lady loading
allow the blood Apache covers a range of
opposes close to the exact pose which
means the original vintage algorithm
does several walk to the several times
to warping several times and then
everything but we skip the the whopping
procedure and we replaced it to later
blurring then we apply a Gaussian plot
to make the blood blood patchy low Busta
to image noises
and then the post pace we accumulate
bloated pitches in our texture unit and
the reason why is it readings assetto
plotter pitches from GPU is early it
reduce the number of lead back and the
number of times require the pole copying
data from GPU to cpu and finally we do
some post processing like a down
sampling and normalization in and we
finally get upset over below the patches
and its associated or six
degree-of-freedom pose and then now we
are ready to detect the target object so
from until oh we have a sine theta and
we want to detect the target object from
incoming video streams of mobile phone
camera and again we use the gravity
information for template matching here
the problem I thought the template
detection method is very good project
detection regardless of the textures and
shapes however the thing is that if we
use more templates we can detect the
target off to target object from razi or
different Roger set of different
viewpoints but if we use more templates
it makes the detection slower because we
have to compare more and more templates
with the input image sequences so so if
we have too many templates the
performance on smart mobile phones
becomes very well
so to to address this problem we again
use we use again the gravity information
and yeah so let me explain how the
gravity works here so we assume that the
near world objects are aligned with
gravity bong gravity attraction for
example like this the horizontal and
vertical surfaces I mentioned you know
that there are the crepe the direction
is either nomer or parallel to those
surfaces and photos video object you can
assume that the on rejection of the
object is usually parallel to the
gravity traction so
so here I introduced the term cravat
Earline the image which is where the
particle vanishing point is either 0 1 0
0 0 minus 10 it means that it is a it up
and work down direction so this means
that to make the Omnitrix one of the
target object in emmy gee parallel to
the gravity shown here so let me show
let me explain the more about this so
the in the in the original image which
are taken from a normal viewpoint is the
gravity and norm nitration is parallel
here and then when a user makes on some
orientation changes in on the pond
camera the captured in the captured
image e the target object is on relation
is no more parallel to the clave
titillation and then the gravity align
the imaging means we wap the capture the
image to make this on right direction
parallel to the original gravity
directions and how so the advantage of
this gravity aligned the images in
template detection is we can reduce the
number of orientations to consider when
building templates which means as shown
in this figure using a single template
to detect the target objects
okay we we just appeal the single
temperature to detect the object in
deeper into orientations like this if
you need to not use grape to your land
imaging we have to fill the templates in
all these cases and it increases the
number of templates here so the okay
they let me explain how the crepey
orlandi VG is computed it's quite easy
then let's assume that the images
captured by a camera there and this is
the crepe tea and what you wanted to do
is making the blue and later let arrows
parallel like this and this this can be
done by a simple rotation transformation
and if we know the angle theta R and the
transformation can be easily computed
the problem is how we know the angle
theta are here and the thing is the blue
line is the line connecting the particle
pension point and the center of imaging
so from this pact if we know the
protocol vanishing point we can easily
compute the angle theta and we can walk
tour is not limited to the gravity of
land imaging
for as i mentioned the conventional
vanishing point estimation methods are
very slow in case especially in this
case we have to do template matching in
real time so even though the vanishing
point estimation takes option or 100
milliseconds it is very slowly in
temporal matching process so our our
approaches using accelerometer to hear
and then I explained that the
accelerometer can give us good
estimation of particle vantage point
before through so I theta can be
directly obtained without any image
processing because we know the pension
the look the position of vanishing point
from accelerometer directly and applying
this rotational transformation is very
simple and we also to it on mobile
phones GPU for pastorate whopping
process
so this video shows gives you a very
clear idea about how the grim tideline
Dimitri works you can see that the only
in original algorithm the target objects
on right direction changes as the user
rotates the camera bag in crepe to align
the image you can see that it keeps on
right direction always here and another
video gives you a more clear idea about
this you can see that here it is always
kept in a bit some nitrogen in the
Warped imaging yeah and after using
template matching for target detection
we do some tracking using esm blow
algorithm which will be introduced by my
another colleague who will come next
week and then so we retrieved our six
degree-of-freedom pose from the detected
tag of the detectives targets office and
here we use some their own instructions
which is like a cindy instruction like a
sse in on intel cpu it is the pole of
mobile cpus
so I explained all the theories about my
work and I'll give you some experimental
lizards delivery or some parameters here
and here the always of this requires the
data of target object is just 900 k
about 900 chloride which is very pure
for target detection relative to do or
is in an algorithm 202 in five years yes
so you take picture of the reviews for
you know which often
ah actually the user of those been here
the TSL 200 225 of user generated from
the input pronto para reviews yeah see
this video shows how to work with the
horizontal target is a similar to what
you see at the start of this same now
learning difference template just takes
a few seconds here then user can start a
detection of target object by selecting
not selecting
so the target of the food is oriented on
the object and again user can select our
three object which is related to the
contents of the box and then render
about your object
and this one is interacting with the
particle surfaces again user takes a
photo of a particle surface from
arbitrary viewpoints and then it is
rectified using pension point the inter
template data is a generative on mobile
phones at the moment then users can
start the detection and from different
viewpoints here this one shows another
experimental results detecting detection
in different viewpoints for vertical and
horizontal targets and this one is for
different scales because we peel the
temple is considering scale we can't
detect the target object in different
scales this one shows the targets with
frontal view so unavailable usually of
particle particle surfaces on a building
and this one is horizontal surfaces
which is very far from a user and
definitely though it's front of further
review is not available to users in this
situation and by estimating six
degree-of-freedom pose the poacher
content is already with the right
orientation what did you say about
skills
the scary is that it distance from the
target object and the camera the input
images that the with just a single scare
then we walk the input images to
different distances I mean we don't you
can't recover this don't really know
it's given an input energy don't really
know yeah kabir kabir distance is not
known this is the relative distance ok
then if you want to put a sign on a
building of us that's the size of the
doors yeah the society of virtual
contents should be controlled it is
should have been known in prior or the
user have to give some input to
determine the Scarab portrayal content
ok so once you've labeled see the
annotated there's a sign of the right
size on the door and somebody comes up
later it looks at the same door maybe
from a much different distance
the sign it'd be the right scale yeah
yeah it is because the it just that the
metal rendering what side o what size
over the rectangle yeah you may be me in
the sign case yeah or that person who is
standing on the ground yet obviously he
was very big huh yeah
ah you mean me this case okay if we mean
if I see this region like okay this is a
loophole building and if I see this this
region maybe like a second stories will
it becomes therapy because because the
scale is maintained between two
applications and then this is a the
targets in a building which is very much
higher than you just few points some
useful which we use to
yeah we have three pictures yeah which
so total from your side the leftmost
remedy is the input image captured by
your user and then the other two is just
showing detection videos in this life
it's been trained by some other women
are learned the target has been learned
on some other know that the leftmost
image you used as an input and then the
temperature built from that image and
then the digit is there another
screenshot of detection these are
different energies yeah they look
similar but different we correct for it
was like roofing
that's that's not very common
why will they hold my something like
that
yeah but it happens America we usually
we use phone from mobile phone like this
and then do not see content like this
but when you use the mobile air
application there is always some
orientation changes because you just
want to see the contents in different
directions and it makes some orientation
changes on the phone it well it depends
on still I'm justify you to imagine so I
assumed that could be the image is a
test image yeah try two of you use a
picture of the same image and then the
system is going to detect trying to
determine
picture to the detection of course
there's no prior knowledge where these
are individuals yeah right scary sky
yeah me our party is doing some kind of
a scanning opportunity yeah destitute to
two possibilities using some corner
detection to make a let me say that the
target object there's a corner we try
the corner detection and post also
scanning approaches and then in case of
corner detection it's affected by the
textures you know if the corner is not
detected the template is not comparable
to the region even though the object is
there and so we adopted the scanning
approach so we really scary you know no
science to scale yeah it is a pretty
talented I mean yeah if there is a
pretty term in the window side you can
be like a sliding window we boobs the
patches the window to such a large
so you say the size of the window is
pretty self from yeah I mean capecchi
learn this rectangle at the center of
the first thing is it and only that I
get that part is used for running not
you don't use the entire easy but it
okay so suppose do you know he threw the
learning me to the left hmm suppose the
high sides the sides of that picture
expose it to 550 hmm but under the image
in the middle insanely the same a
picture the sight of it could have been
added a hundred you may let the inn in
the input immedi I mean not used for
detection in that image the target can
be smaller or rather yet so that we feel
there's some templates by applying some
scale factors there when we walking the
images and miss synthesizing the image
is used for template building so the
thing is they if the input images here I
know the target of chips here and when
user cap is the front of parallel view
camera is here if we oppose like if we
move the camera to the front now the
immediate object it becomes rather and
then we feel templates from the ETA in
reseda midget any cables templates over
those messengers cover the term
discovers of different sizes what is the
funniest
real scan your window you see window
when the way I really cover a portion of
the image yeah right so they so they
even though you tell that already has
different scales but does not help this
case yeah in that case to watch scaredy
preserve X a problem in detection okay I
give some more huge Empress you capture
the polio but I can attach some
spider-man to these words so what you
can do this method is instant the city
documentation which was similar to the
previous video and this one has some
tips on is interacting with the ceiling
and another video is just interacting
with this shaded region i made this
video for fun get yourself
and here comes
on Xtreme here
okay then another is this video shows
the sharing augmentations between two
mobile phones right in here the ruble
the target is acquired by point a and it
is transmitted to the pony through
bluetooth connection then then point B
started detection of target object
seeing the the poacher messages on the
target
it was very hard to control to mobile
phones in on him at the time so we do
some Bluetooth connection so make your
connection in sir idiot as native to /
from be
hmm
and I give you some videos about the
proponents and timing this work it was
the timing was measured on pc and iphone
flip homes so this is it shows that the
crepe of learning speed how much time
each step takes so it the time increases
as the number of abusive we consider
increases because we have more whopping
and blurring for more more few points
and on a pc is very pest just a few
hundred milliseconds and on I porn on
iphone four it takes severus a few of
your seconds but the too many viewpoints
make it slow here the most
time-consuming step is the radio below
step which is a it is a slow because it
accesses the textures in on in GPU in
random would not random but it is it
accesses not not horizontally not
vertically so in mobile phone TPU case
those kind of excess is very slow so
that the array the ruler takes much time
and but it can be it can be implemented
a pastor if we optimize my shade of code
well you do teach you to the denotation
radio poor yeah and then you
successfully
yeah he did them to try to get some
sample person
always course
the thing is what we wanted to know is
the great let me say great value over
picture at the same position which is in
the original patches my EP subsample the
first image and then we plug in whether
the top loader pixels are different in
that case yeah so that that's what I
didn't want to it so so I didn't try
and this slide shows the comparison with
the original algorithm in detection
performance and you can see that ours is
a little bit has a lower performance but
it still comparable shows good detection
performance but except that these rest
three cases these are his images are
very limited have eliminated textures
like a grass or bang mini pod or another
some food of electrical past there are
so many similar texture so if you are
applying blurring to that it shows poor
performance and you can see that also
the original algorithm shows a little
bit lower detection performance
and this is a comparison with in-memory
ucg so theology as I told you the
original algorithm requires large amount
of pre computed the data but we don't
want to do that so we reduced it to be
take another approach and you can see
our the memory consumption is much less
than the original one so we lose some
dictation performance but we can have
large amount of Emrys Abe here so let me
conclude my presentation here so the we
propose some computer vision based
approaches pull in siege they are taking
in real-world environments and we
exploit the sensor information which is
very popular on modern smart smart
phones for twin comforter vision walks
and then finding poor finding
orientations or finishing points a
template matchings so using our
approaches you just can do some easy to
interaction with the real world by
learning and detection the target object
so what I explained until I was about
interaction with real and butcher for
personally but augmented reality is in
not okay let me say in ubiquitous
popularity it is not limited to the
personal interaction so the users to
users can interact with the using
augmented reality applications by
connecting by sharing their environment
in pill by building augmented reality
spaces in there one locations so AR will
be can be used for interaction between
users as well as to a poacher and veer
words so this is the my future work
concept of mine so ok this is what i
prepared for this presentation thank you
for reasoning my watch we will in the
life regarding the scenario were in situ
attorney if some slightly scenario is
that you take a picture and then and uh
period and then you're gonna recognize
the same page again yeah right so it
doesn't seem to be very useful what this
is you already just take a future where
would you want you to rithmatic same
pitch in the same place well this is
about the ok for personally it is
nothing watching meaningful because yeah
I don't want to see see it again but
considering some kind of a social
applications but if I wake on a notation
about on object let me say a picture in
the museum I can make some annotation
about the my my impression about the
picture can the others maybe who visit
the museum first time may want to know
how the other piece about these pictures
and then it can be seen by their mobile
phone so this is not for just personal
usage the the application
is for sharing the augmentations among
the other people with other people's
like like we be jews Twitter's on
smartphones yes the accumulation step I
just want verification on that what
exactly was I look like I mean so I will
be say cumulation sounds like you're
doing you're creating like a minute or
something but the graphic you have ah
yes that's your answer the term the term
accumulation is can a little bit it is a
little bit misleading so my mind my
intention was that just putting the
desert in textures like okay let me say
packing it so yeah
let's thank you stay here again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>