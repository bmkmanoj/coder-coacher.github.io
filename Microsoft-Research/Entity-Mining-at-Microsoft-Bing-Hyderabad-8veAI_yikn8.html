<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Entity Mining at Microsoft Bing Hyderabad | Coder Coacher - Coaching Coders</title><meta content="Entity Mining at Microsoft Bing Hyderabad - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Entity Mining at Microsoft Bing Hyderabad</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8veAI_yikn8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so after seven days of what has been
mostly talks raised a kind of an
interesting day in the agenda as you
must have all seen we're gonna do a
hackathon for most of the day based on
Azure ml but before that we are very
fortunate to have Manish with us Manish
Gupta from Microsoft Hyderabad he's part
of the Development Center their money
stood as masters from IIT Bombay and
then he was with two years at Yahoo
after which he did his PhD from UIUC
he's also an adjunct faculty at I triple
idea Hyderabad and he's been with he's
in a senior applied scientist at
Microsoft Bing team in Hyderabad so
without much further ado I'll let money
take over the stage and begin his talk
thank you - thanks at this so let's
start today's day so everyone is
energized even after seven days of
torture okay so I'm going to do a little
bit more of torture but hopefully you
will enjoy it okay so I will be talking
today about entity mining and the kind
of work that we do at Microsoft in
Hyderabad okay so how many of you have
heard about entity mining or you know or
done some sort of research in entity
mining working ok ok so good so uh and
so all of you are master students
bachelor students how many bachelor
students ok master students Oh majority
of you I mean in fact our masters or PhD
is right so so that's good so so
hopefully if you are doing your PhD in
say text mining or information retrieval
or web mining this area would be of
interest to you and even otherwise I
think the last part is going to be of
interest to most Indians because it's
about cricket so anyway so let's start
so entities are essentially everywhere
so I mean that's a new way of looking at
objects entities are objects that have
an independent existence like people
locations books and so on ok so that's
because that's what I call as entities
and entities are represented typically I
mean you buy a name and ID so usually
for example in any No
bass which is basically a collection of
entities you represent them using some
ID okay so for example Microsoft Satori
has a particular ID Google's knowledge
graph has a particular ID freebase of
the particular ID and so on and it has a
particular name a type so for example
some entities are books some are
location somewhere people and so on
attributes and descriptions so on
Wikipedia a place like a Bangalore would
have different attributes area number of
people per population and so on
relationships to other entities too so a
knowledge graph or a knowledge base
typically contains a collection of
entities and also representation in
terms of connections with other entities
so knowledge bases you I think most of
you would have heard about most of them
Wikipedia Google knowledge graph
Microsoft Satori yeah go yeah go how
many people have heard about Yahoo some
of you right so those of you who have
not heard just just look it up yeah go
right so there are lots of such
knowledge bases available
so today's agenda is about doing
something about these entities ok and
mainly I will be talking about three
different problems roughly 20 minutes
each okay entity linking the other is
dominant entity identification and the
third one is cricket linking okay so let
me start with the first one entity
linking so here is what I mean by entity
linking a reader I mean a user is
actually reading this news page and then
say he hovers over this particular
phrase in the page okay so forgetting
sarah marshall and then what happens is
that is that being shows automatically a
particular task pane or a sort of an
entity description for this entity okay
so it is about identifying phrases in
which can be sort of linked to entities
into in the knowledge base so
identifying those phrases and then
finally linking them so essentially
disambiguating various other entities
which could actually be described by the
same phrase and then finding the right
entity in this context and then showing
it up right here so that's that's one
example the other example is anyway just
another example I mean user sort of
course over this and then automatically
the the user has shown some such entity
pain in some other application on say
Microsoft tablets okay so
the problem is as follows fine there are
two parts mainly I mean essentially
finding mentions of interests
essentially the entire problem can be
summarized as finding mentions of
entities in free text so the free text
could be anything it could be blog pages
it could be news pages it could be it
could be news articles and so on so it
could be any webpage for that matter and
you need to find out mentions of
entities where entities come from a
known knowledge base okay so entities
could come from any of these oil bases
and pretext could be from any of those
places
so not only find the mentioned entities
but also find the actual mention in the
text okay so the actual mention is
usually a key phrase that is deemed to
refer to the entity so so oftentimes
it's called as the mention and the
entity to which it is linked is
basically the linked entity so so first
so as you can now see this problem is
not really a one-step thing it could be
sort of divided into two main steps one
being finding all such phrases that can
be data that are eligible to be linked
to some entity and then linking them
okay so for example in this particular
text somebody has found out these these
these phrases which can be sort of
linked to some entities and the brackets
you basically Square packet you see the
entities to which they can be linked so
so that's that so so span for the
mention of this particular guy and then
the the link page is basically here so
essentially entity linking problem has
given a text document and linking system
takes in inputs from the knowledge base
and outputs at this particular offset 10
to 19 you could really link this
particular phrase to an entity Phillip
use or Australia and so on so that's
that so now I mean essentially now
clarifying the problem digging a little
deeper into the problem there are
basically these three phases that one
can think of one is mention detection
identify potential mentions so for
example words or phrases or or of of
entities in the documents right mentions
of the entities second is kinda drink
regeneration so given that phrase now
you could link it to a certain set of
entries so for example if the phrase was
just Apple now there are two obvious
choices that you could link to so for
each mention identify kind identities to
which the mention could be linked and
then of course the third phase is and
linking or really rankings so
essentially saying that well you have
the candidates now and you want to
select the top one the best one that
could be linked in this particular
context so if the context is talking
about Facebook and Apple doing something
together so then you need to really link
it to the Apple company and not to the
Apple Flute right so depending on the
context you need to really figure out
which is the best ranking candidate that
that should be selected from these lists
of candidates right so now let's let's
look into details of each of those tips
so one by one so if the first place is
mentioned detection right so mentioning
detection as you could sort of think of
it is more like I mean there many many
techniques but the most prevalent one is
name dictionary based technique where
since we are interested only in mentions
that could relate to entries in the clip
video we could build a dictionary of key
value pairs okay so essentially saying
on Wikipedia you have all the entities
you build key value pairs which
basically say that well this is the
value values the entity and the key is
how it is referred to okay so for
example such in Tendulkar could be
referred as master-blaster
okay so the key could be a
master-blaster that a value could be
essentially such a kind Oracle or
essentially I mean the value really
should be the ID unique ID of that
entity and the key is really is the
various ways in which that particular
entity ID could be referred to sometimes
it is also called as surface forms okay
so the different ways in which the
entity could be referred to so key
refers to a potential mention value
refers to the entity that the mentioned
could link to so for example Sania Mirza
could oftentimes be just called as Sania
in various articles right so now how do
you get these this kind of map so as you
can see we can sort of build a key value
map and then whenever you get a piece of
text say a news article you would try to
search for all the keys that are there
in your key value map and then we'll say
that all these matching keys are really
places which we can call as mentions
right things which we can call as
mentions so now how do you build this
map so you could build it from Wikipedia
entity titles so for example if the page
title is IBM and it is talking about IBM
you could just do this you could build
it from redirect pages so basically
Microsoft is redacted from Microsoft
Corporation so you basically can say
that well Microsoft is also called as
Microsoft Corporation okay and then you
could build it from disambiguation pages
so essentially saying that if somebody
is searching for Michael Jordan or
something of that sort
it should also really link to Michael
Jordan and so on okay so so that's that
I mean essentially Michael Jordan is
popularly searched and then it could
possibly mean any of those two right so
or any of these five anyway right
similarly you could get it from bold
phrases in the first paragraph so for
example oftentimes Wikipedia the first
paragraph contains bold face of this
kind and from this you could really
infer that you let the card company is
actually you let pickle right so the
links to your pic are even H P basically
means the same thing from anchor texts
in Wikipedia so for example this anchor
text right this is actually linking to
this guy service which basically can
help you derive this kind of
relationship okay so okay sure
okay yeah so essentially you could
really you could really build this map
using various examples from Wikipedia so
various clues that are available on
Wikipedia you could basically just build
a snap
also if Wikipedia query search logs are
available or in fact I mean for any
search engine company who has query
search logs you could just use the most
frequent queries to a particular page to
really identify identify relationships
of this kind so if somebody's searching
for spelling with a spelling mistake
right and most of the times he really
clicks on Maria Sharapova the actual
correct spellings Wikipedia page that's
the one that can be identified as the
most frequent spelling mistake that
people do right so so that's that now so
how do you use this for mention
detection it's sort of very
straightforward so from the text to be
linked find the matches to the keys in
the dictionary so essentially you have a
news document coming in you basically
just find it just find matches for those
keys in the dictionary or in the mention
map so to say and all such matches may
be treated as mentions okay all of them
you can call as mentions yeah
right right so so that basically comes
in the later phase so essentially first
you are trying to do just nation
detection so you are just identifying
Washington is something that can be
linked okay so now the next part is what
can it be linked to so now based on
these these linkages that you extracted
offline from say Wikipedia you would say
that well Washington could link to the
person Washington with this with the
CERN in Washington or it could link to
Washington city or it could link to
Washington museum or any other things
right so now I mean in the third phase
so these are all kind of generation that
you generated now the third phase you
would actually do the entity ranking and
then you would actually use the context
information you would say yes yes so
basically I take the text so usually in
verbs and adjectives won't come because
they won't be there in your mentioned
map so so essentially I mean if you see
from the text to be linked find matches
to the keys in the dictionary and in the
dictionary the way that you have formed
usually the dictionary does not contain
verbs and noun phrase verbs and
adjectives of kind I mean usually
entities are nouns in general noun
phrases so so even the candidates are
going to be pretty much nouns or noun
phrases so any other questions before we
move ahead
it would mostly be arbitrary engrams
because I mean because queries could be
arbitrary ngrams anchor text could be
arbitrary engrams so so basically what
you are doing is deriving insights from
what people have used to refer to an
entity so now if people have used to
refer to such in Tendulkar as the guard
now the guard would also refer to such
in Tendulkar and in fact I mean for all
we know if there is an article on
theology and somebody refers to the
guard we might in fact consider such
England Euler is also a candidate but
finally when ranking we would remove him
based on the context so the context
maybe does not talk anything about
cricket and then we would just remove
him so it could be any people any
arbitrary Engram as such but mostly most
prevalently it would be noun phrases in
general yeah any other questions before
we move ahead No okay so so it is
respect here I mean mentioned detection
the the thing is very simple you first
do an offline processing generate that
kind of a mention map which basically is
the usual ways in which entity is
referred to linking to the actual entity
so and then at whenever a news page or
any random page comes to you you
basically find mentions as those
particular phrases which appear as keys
in your mention map okay okay so that's
that so for example in this case these
things appear in the mention map and
therefore they are sort of highlighted
in this piece of text okay so now
scouring I mentioned so now the an
interesting part
yeah that's interesting so basically
what should be the granularity of things
because of course India would also be
there in the mentioned map I mean
because India refers to India clearly
right so so the point is there various
ways of doing these things I'm just
giving broad framework as such there are
more details as to you could also
consider I mean now different people
have different beliefs so people
consider overlapping mentions also some
things and then while linking we're
doing the linking they try to take the
longest mention that makes sense so for
example in this case I could really link
this India to the country India webpage
right but in if you look at the context
it just makes sense to consider to link
it to India Australia test series
Wikipedia page if there exists one right
so rather than linking it to Australia's
Wikipedia page or India's Wikipedia page
yes yes yeah yeah yeah yeah yeah yeah
yeah it will be inserted so when
inserting because the the insertion
really happens when you are doing
offline processing of the entire
Wikipedia so in the entire Wikipedia
they would surely be pages for India
Australia separately right so they would
get inserted in fact even this gets
inserted because in fact there is a
separate Wikipedia page for India
Australia test CDs also okay so now for
mentioned detection the point is you
could detect India and Australia and
India Australia test series all as
separate mentions but in general it
makes sense to consider the longest
possible mention without any overlaps to
be detected as as potential mentions
it's it's a it's a one-to-many map in
general well it could be in fact a
many-to-many map I mean because you know
a source of both ways so basically
searching Tendulkar can be referred by
many particular things the guard
master-blaster Tingley of sachin
tendulkar itself such in itself or as
heart &amp;amp; Deluca and so on right so all of
these could link to Sachin Tendulkar and
also the same thing could also link to
many other entities also so for example
Apple could link to the Apple company or
it could link to Apple the fruit right
so essentially it's a many-to-many map
and that's why the problem becomes
difficult if it were just one to one map
then life would have been easier I mean
once you just detect the mention there's
no ranking required really I mean that
really directly links to the entity
right but but but the way we name things
I mean humans like confusion right so
essentially it's many to many and that's
why I entity ranking the the the next
two phases kinda generation and ranking
become important any other questions
before we move no okay so so that's the
mentioned detection phase so now the one
important thing to note is that not all
mentions are equally important or
relevant okay
so although we don't read we won't
discuss filtering of mentions today okay
but important thing is not all mentions
are important for example candidates
opting for IPS need to clear the fitness
test here okay so this is a particular
particular piece of text so now tests
could link to Test cricket I mean in
fact your dictionary would say that well
in my dictionary test links to discreet
IPS links to our Indian Police Service
but all of us know I mean in this
context test to test cricket does not
really make any sense right I mean it's
basically about some fitness test and
not really about cricket at all so not
all mentions are really important and
therefore one also needs to do some sort
of cleanup so basically after you have
detected these mentions using the
mention map dictionary or essentially
such kinds of dictionary you need to do
certain kinds of clean up you need to
really be able to say that well this
mentioned does not look good at all I
mean and what entities it is linked to
is a separate thing but it just doesn't
look good in terms of this particular
piece of text itself and that's what
actually I will talk in the second part
of this thing where I try to find the
dominant mention so essentially I try to
find what is the most important thing
that should really be linked to in this
sentence so so so that's this just a
side comment in some sense that not all
mentions are really important and a one
needs to do some sort of filtering so we
will not go into details of what kind of
filtering you can do but this is the
thing so now the second phase of entity
linking essentially kind identity
generation so you have this mentioned
map and you have basically found out
certain phrases that could be linked to
some entities in the knowledge base and
now you need to do this thing so a
candidate generation okay so for example
you
identified Apple and yeah now you need
to really say well Apple could link to
Apple the fruit or Apple could link to
Apple the company okay so in case of
dictionary based approaches the
identification of entities to tag
dimensions will is very straightforward
why is it straightforward because well
you identify it Apple as I mentioned
because Apple was there in your
dictionary in your many-to-many
dictionary right so now if Apple was
there in a dictionary of course there
was some value and you could just take
that value and say that well those are
the kind identities yeah so essentially
consider the mentioned Sanja so find all
values from the dictionary where the key
is Sonia so this could include Sania
Mirza Sania Khan or you know Sonia
rimmel Airport and so on
so all of those could really be referred
by the same surface form all by the same
phrase or the same mention right
so now use those values as the entity
set to be associated with the mention
Sonia so those are the candidate
entities now that you have okay so
basically I mean when you are reading a
piece of text at Sonia won the game or
something of that sort well there could
be three different things say that you
extracted from this mentioned entity map
that could really be linked to this to
this particular word so however you may
want to do a better job and to do that
we may want to not completely rely on
the dictionary we may want to expand the
entity site to include other entities
too okay so now sometimes what happens
is that the dictionary that you have may
be biased and so to expand the
dictionary what you could do is to
really say search with the entity
mentioned so basically take the
information with the context so
essentially say Sonia won the game right
the entire thing use any search engine
with the with the filter site as
wikipedia.org and then get a list of
Wikipedia entities that match so this is
another way of sort of expanding the
thing so expanding the set of entities
that can potentially be linked to this
one to this particular key phrase okay
so that's fine I mean it's okay to just
understand that really I mean given the
dictionary all the mentioned map you
could really get a list of entities to
be linked to okay so now the problem the
third problem is I mean the third part
of entity linking is to do the ranking
so now you have identified that but for
this particular phase of all this
mentioned you really need to you you
have this candidate set of candidates
now you want to find out which one is
the most which one is the most fitting
one right which one is the best one
given this context okay so essentially I
mean and so how complicated this problem
is sort of shown here so people have
done this task or various data sets and
the number of candidates that they have
come up with is like this I mean
essentially say for one mentioned on an
average they could come up with 13
different candidate entities okay so I
mean the Apple case is a very simple
case where I told you this two different
possibilities Apple the company or Apple
the fruit right and or Sonya's case that
we saw we considered three different
cases right so but on an average people
have come up with like if so for example
in this data set people came up with
like 73 different candidates for a
particular mention so the problem is
really complicated
out of these 73 they have to really
choose the one the the most fitting
entity which can really be linked to
this particular mention or the phrase in
the in the piece of text right so now
entity designer equation is really an
important step and and the way that it
is done usually depends on multiple
things so one what kind of granularity
you want to do that disambiguation so
it's basically a design regression
problem you want to really figure out
what does the user really mean when he
is referring to sunny I here I mean does
he mean the airport does he mean the
person or which person and so on so the
descent equation can be done at the
level of individual mentions itself so
you might want to dissemble but just
based on this mention okay and its
surrounding words okay or you could do
it at the level of a document so
basically say that well this particular
document contains contains the five
different mentions so one is about say
cricket one is about Philip one is about
such in and there is about say the India
Australia Test series right so now you
know that all of them really I mean
since the entire document is talking
about one topic all of them really
should have to do something with each
other
so essentially somewhere in the
knowledge graph such an should really be
linked to India Australia Test series
and and he should also be linked to
cricket in some form so basically by by
using this information of linkage across
mentions you could do better at
disambiguating so maybe I mean is one of
the candidates for Sachin is also
system professor with the name such
entry so now if you really know that the
topic of the entire page is really about
cricket you will very easily check that
professor out because he has to do
nothing with cricket okay
so essentially saying that you could
disambiguated the level of our document
so ensure that mentions within the
document are linked to entities in such
a way that the document is linked to a
coherent set of entities so so so the
problem I mean if you if you can sort of
visualize the problem is that you have a
page and on the page of detected say
five mentions right and for each of the
mentions say you have ten kind
identities right and you want to select
the best entity for each other mention
so and of course one good thing I mean
if you want to do it at the level of a
document is that whichever files top top
one entity you would select for each
mention should have to do something with
each other so there must be a coherent
set of top five entities in some sense
okay so that's the design equation at
the level of a document you can also do
it at the level of a corpus so you could
basically say that well I have a
collection of cricket articles so now so
for example say reports of the 2015 ICC
World Cup cricket match right so now if
it contains somewhere money right now
you should not really link it to me
because clearly I'd i don't play cricket
right so and you could establish that
this entire collection is about cricket
because you see a lot of different terms
in a tree little it to cricket right so
basically this is the same equation at
the level of a corpus so consider
mentions in other documents with similar
surface forms and then use that
information to perform disambiguation so
maybe the context of a particular
mention in this sentence is not enough
but if you see that same mention across
in the corpus and which has very rich
very rich context so then you can
basically say that well I can borrow the
context from there and still make an
inference about what this particular
mention should be disambiguated to which
entity it should be linked to so so we
will see this in this in action so I
will basically talk about label for
document so so basically so basically
this thing is often treated as a ranking
problem in the sense that given I
mentioned you have multiple candidates
and you want to really rank those
candidates such that you chose choose
the one at the top
okay so and then to do this ranking many
times people do a super wise ranking so
so does everyone understand supervision
supervised methods right essentially you
you train using a particular data set
and then you try to I mean and using
certain set of features and then at the
test time whenever you get this nation
and certain entities kind identities you
rank them based using that ranking model
right so many times people use this
supervision kind of approach or
sometimes people use an unsupervised
approach so similarity assessment using
vector space models and so on so both of
the approaches are quite common but but
I think a bit but majority of the people
usually rely on supervised approaches
because it's just very easy to put plug
in more and more features and and
training data is very easily available
in general so can anyone think how could
you really get training data for doing
this kind of supervised learning I mean
any any ideas if you want to do
supervised learning for this entity
linking how can you get training data X
skill hmm yeah so what do you want you
basically want that so what is what are
the training data look like the trainer
should look like given this particular
mention and given this entity is it the
right connection or not okay or maybe
give a relevant score so basically
saying that say Apple right in the
context of say Facebook and Apple and
now some joint agreement or something of
that sorry
so Apple there in that context and then
say Apple the company it is linked to an
entity ID called Apple the company okay
and then the label should be rebel out
of five I think this is a very good
labeling so I give it a five I mean this
is a very good match
while another instance would be the same
Apple in the same paragraph and say
Apple the fruit and then I would give a
score of zero basically saying that well
I don't think this is a really good
match right so the kind of labeling I
want I mean the labeling which sort of I
mean something that gives a pair of
mention and identity a label how good
that particular mention entity payer is
crowd so you could crowdsource but that
that cost money right can you do it in a
free way and a very scalable way
yeah I mean not the true value but it
could be dimension and entity I mean so
any entity and the score that's true
that's true yeah right so so basically
yeah I mean you could also have other
mentions or context also in the picture
that's true agree yeah yeah so so where
can you get such data huh no no but you
are right I mean Wikipedia itself
because Wikipedia actually contains
those links right so basically people
when they are writing so for example
consider Bangalore's page right so when
people are writing about bangalore
somewhere they will write that well
bangalore has this institute called tata
institute or institute Indian Institute
of Science right and with the tata
institute for example they would link it
to Indian Institute of Science page
right so basically somebody in on
Wikipedia the editor in Wikipedia has
made a human judgment that Tata
Institute as I mentioned along with all
the context on that on the on the
Bangalore's Wikipedia page right has
linked it to to to Indian stood of
science right so they have actually
given us the positive label they
basically said that this really means
this right so that's the positive label
and negative labels you could just
generate based on everything that has
the same name tata institute but it's
not linked not linked directly by any
any Wikipedia editor right so that gives
you sort of train data at scale and
since you have train data at scale
supervisor algorithms are usually much
easier okay so so that's that so now of
course the sepoys algorithms need some
features right so what kind of features
you could use so of course the first
feature is entity popularity so
essentially saying that if William
Clinton appears as a phrase somewhere in
some document the with a high
probability I should link it to the the
famous Bill Clinton and with some
probability maybe link it to the guys
who are not known much right so entity
popularity is of course one important
feature so how popular these things are
of course that doesn't apply in case of
Apple because apple the fruit and the
company both are almost equally popular
but in case
where there are very easy things right
so for example nobody would link such
into some random guy right I mean most
of that most of the times people are
going to talk about such intent doulica
another thing so entity popularity then
similarity so essentially saying how
similar is this mentioned text with the
with the with the entity name so for
example this thing is very similar to
this thing and then an extra artist the
six character thing extra comes in so
therefore you would want to really link
it to the one which is highly similar
versus to the one which is which has a
larger edit distance okay so that's that
and then finally entity coherence so
basically saying that you are a document
if you have say these three mentions you
would really want them to be very close
to each other you would really want them
to be very close to each other in terms
of whatever notion of semantic
similarity you can think of okay so now
I mean this is very open what could be
the semantic similarity measure it could
really be how close they are in a graph
so if you have a graph of entities how
closely connected they are it could
really be so there are various ways of
computing similarity between entities
but what you would really want to do is
to really link these mentions to a
coherent set of entities a set of
entities which are very close to each
other in general okay so so essentially
this is what the linkages would be would
would then be so for each mentioned am i
find an entity I such that the following
is maximized so basically the paper I
really just tried to maximize the three
criteria the three criteria that I
talked about the popularity the
similarity similarity between the entity
names as well as in terms of the context
so the context of the mention and the
context in which the entities appear and
the coherence of all the entities
appearing in the document okay so so the
popularity similarity and at a coherence
rate so that's that and then you would
just use a machine learning algorithm a
supervised algorithm as I mentioned take
the mentioned entity pair and then try
to learn from this data as to what is a
good mention entity pair and what is the
bad mention entity pair and then after
you have learnt a model just apply the
model on on test data and then be able
to say well I mean learn a binary
classifier and then whenever a new
mention comes in just be able to say
well this is a good mention entity pair
or a bad one okay so that's that so any
questions before we move on to the other
part
that is right so often time so I mean as
I mentioned I have I have not put out
many details here so many times the
context is districted only to like
hundred words before this and hundred
words after this rather than the entire
document because it is observed that
when people are writing long documents
oftentimes the context changes very
rapidly so I mean many times it's just
the neighboring mentions so essentially
many features could be what is the type
of the mention just before this and just
after this forget about even hundred
words I mean it's basically a very local
context so you could really do a local
context learning-based thing also so
there are various kinds of features but
these are like the broad intuitions that
people use so so there are also features
like there should be of the entity
should be of matching pipes so when I
talked about coherence really I mean I
did not define really what I mean by
this similarity notion right I just
showed that these these three are
together I didn't really say what
similarity means it could be similarity
in terms of the attributes they share it
could be similarity in terms of I mean
the connectedness on the graph or it
could be similarity just based on type
so maybe the entire paragraph is talking
about movies and if American comes in I
should really not linked it to Aamir
Khan the boxer but really link it to
Aamir Khan's actor so what does the
dancer
mm-hmm yeah yeah so there are cases
where things can break agree yeah so
yeah I mean if he likes to eat apples
and break first I mean yeah I mean short
text it's always difficult to do entity
linking but maybe for break first it can
really link it to Apple to the right
Apple and maybe it cannot so it's not
really a 100% accurate method in such
cases things can break yes that again
depends on the context if you have the
dates a fourth for the news article you
would basically be able to extract 15
from there if you don't have the date
many times it's just the recent one so
you Kapena usually has recenter ones
right I mean yeah I mean the whole point
is that yes I mean it depends on what is
your knowledge base so if your knowledge
base does not have an entity you cannot
just do anything I mean entity linking
is about taking this piece of text and
trying to link it to a knowledge base if
the knowledge base does not have an
entity so that that there is a different
area of research is called as nil entity
diction so basically saying that detect
that this is a mentioned but not linked
it to anything detect that it this
particular entity is not really present
in the knowledge base but for today's
lecture let's just suppose that the
universe is defined by increasing your
knowledge base and if there is no I mean
it will just match to the best matching
entity yeah
I didn't get yeah yeah but the problem
so you mean the training data generation
right yes the Train data generation the
problem is that you don't know so for
example anchor links of course exist
everywhere right so you could really use
news sites also but those news sites
should ideally link to the knowledge
base entity if they don't then you don't
know which entity is referring to right
so for example you might have say
cricinfo page okay so cricinfo I mean
it's about a match report and they have
linked such in Tendulkar or the
master-blaster this phrase to Sachin
Tendulkar's profile on frickin
okay but we don't know I mean the
knowledgebase does not know that such in
Tendulkar's profile is really talking
about such in Tendulkar the entity you
know there needs to be some link to say
that well this is the entity entity ID
search engine has an entity ID in say
Wikipedia so the algorithm needs to know
that when it is linking to such engine
lucas cricinfo page that clicking for
page needs to be stamped with that I
need to be able to say that well
master-blaster means such intent so
essentially you could consider links
within Wikipedia or also links from non
wiki pages to wiki pages as as the
things that could supply at the training
data or I mean well to be more
complicated you can actually go a step
further and try to sort of find the
dominant entity for any random page on
the web and then you could also use non
wiki to non wiki links if you know that
non that particular target non wiki link
is really related to an entity on wiki
where where Vicky is very general so I
mean it wikipedia could really be
replaced by any knowledgebase for that
matter it could be Microsoft Satori it
could be ya go knowledge graph Google
knowledge graph and so on okay that does
not answer your question okay
that's true absolutely true so this is a
biased way and as I discussed you could
really rely a part of it on non
Wikipedia things also so I mean you
could tree I try to sort of generate the
training data from non wiki sources to
be able to have a less biased emission
agreed yeah yeah yeah yeah so so exactly
so essentially one of the features as I
mentioned is popularity
where popularity you could really
measure based on so I didn't mention how
do you figure out popularity but
popularity really means of all the
anchor text that you saw how many times
it was actually linked to the fruit
right
so that sort of captures I mean the
exact intuition is sort of captured in
the first feature that I sort of talked
about
yeah yeah I mean so this is very easy to
do usually so from I mean in general
what people do they consider each
Wikipedia page as an entity okay so
basically saying everything each
Wikipedia page is an entity okay if it
has an infobox so you know info boxes on
Wikipedia pages I mean structured takes
a structure tables at the right top
right so those are structure attributes
of the entity and then I mean
considering the first paragraph on that
page usually the first paragraph on that
entity page whatever entities are linked
from that paragraph form the linkages so
that's a usual way of really creating a
knowledge graph or you know entity graph
out of Wikipedia I mean and usually
people consider first paragraph mainly
because it is more clean I mean for the
remaining paragraphs editors oftentimes
link to pages which are not really link
cable but I mean they just link it to
enhanced linkages within Wikipedia
although that wikipedia guidelines say
that the linkage should be performed
only for the first time that that
particular mention is really that
particular mention ochres but anyway so
that's the usual way of creating a
entity graphically
oh that's a good question so we are not
doing co-reference resolution in this
particular case explicitly so in the
natural language processing kind of way
so natural language processing kind of
way has various ways of doing this
co-reference resolution the problem that
you mentioned right so that the crews
really links to Tom Cruise in the
previous sentence okay but this thing is
sort of a data-driven way to do the same
thing so essentially saying that Tom
Cruise is I mean whatever is the first
sentence right so with the context it
should be able to link Tom Cruise to the
right Tom Cruise in the Wikipedia in the
knowledge base and the same thing it
should be able to do with the second
cruise also i mean ii mention of the
cruise also okay but this thing is done
indirectly in the sense each of them
could either be done independently or it
could be done by considering this kind
of coherency based relationship but not
linguistic parsing so coreference a
solution is a way of doing it by doing
syntactic linguistic parsing of English
grammar and then understanding that well
this cruise relates to Tom Cruise but
here the same thing can be done in a
data-driven way
yes yeah exactly so as I mentioned I
mean just the way you generate training
data right I mean you could just hold 90
percent of your training data for
training and ten percent for testing so
and just test on that ten percent data
yeah are there are lots of benchmarks so
I mean in fact entity mining I mean in
this entity linking is a really popular
area last four or five years people have
been working on them there's lots of
benchmarks lots of data sets publicly
available to verify how good the systems
are doing and and various kinds of
difficulties of linkages so I mean
different difficulties of data sets we
are linking it to a entity okay so I
mean we are linking any phrase to an
entity now how do you represent entity
in in fact I mean we are linking it to
an entity ID so now how do you represent
the entity ID is up to you I mean the
knowledge base may represent it as a
collection of name of the entity
attributes of the entity linkage of the
entity with other pages are all other
pages on the web which relate to this
entity and so on I mean the
representation could be anything we are
linking to an entity ID in the knowledge
base yeah so we assume that each
Wikipedia page is really about an entity
ID so essentially every Wikipedia page
has a unique identifier okay so that
that linkage is sort of trivial in in
our case but that problem is a very
interesting problem if you want to do it
in general on the web so and that's what
I really call as the dominant entity
identification problem so essentially
saying that given a page really are
given an entity find all the pages on
the web about this entity only so in
case of Wikipedia it's a very trivial
problem because you know I mean II n dot
wikipedia.org slash Sachin underscore
Tendulkar really links to Sachin
Tendulkar as the entity ID
but click enforce such in Tendulkar page
is also about the same entity ID
identifying that is not trivial so and
that's what the second part of the talk
is which I would mostly skip because I
like the last part and given the time I
will not talk about the second part
maybe I'll talk about the third one but
any question any more questions before
that yes yes so so basically there could
be features like similarity between the
context of the mention so essentially if
it says master-blaster way and there are
various other things around it so the
Master Blaster played as usual now
played is actually a very interesting
word because player actually could be
related to player which might appear in
that info box
so basically called similarity between
the context of the mention and the
attributes and attribute values of the
of the Wikipedia entity page is a good
feature I mean so if it is like
absolutely another copy of such in I
mean doing cricket playing politician I
mean being a politician also and from
the same city and same year of birth and
so on it's difficult even for humans to
disambiguate right I mean the point is
that there is no binary features here I
mean it's really about the degree of
match so if humans can figure out based
on some degree of match the same thing
the algorithm would be able to do if
humans cannot figure out well I mean
it's it's it's difficult
there are various distance missions
they're just too many distance measures
I mean basically it's just a it's
basically a set of features I mean I
just told the spirit of what kind of
features you could define but you could
really have large number of distance
measures you could have worded base
similarity for all like all we can say
right or you could have Yago base
similarity saying that well on ya go I
would like to see these two entities and
how similar they are so that they are
just in number of measures a
hierarchical structure of what since I
mean yes yes so actually that's what I
was talking in fact so I mean so to
basically identify a similarity between
entities one could really look at their
pre distance measure on wordnet which
were not is as you know is a
hierarchical organization of entities
right so if the article is talking about
Mumbai and Calcutta alright and then
both of them are cities in India so
basically in wordnet somewhere Mumbai
Calcutta would appear and then they
they're both cities in India and so on
right somewhere they will have a least
common ancestor which is very close to
both of them so essentially saying tree
distance is a good measure is a good
feature also to compute the coherence
between linked entities in us in a
single document yeah yeah yeah
so in fact Microsoft so so so on
Microsoft Windows 10 we'll have a
browser called Microsoft edge
so you will probably know I mean
Internet Explorer is being replace a
Microsoft edge and Microsoft edge will
have this feature so in fact yeah I
think even for India they are going to
launch the feature or probably have
already launched I don't know but I
think yeah the public release will
happen sometime soon I think so there
when you say when you basically select
some text you could really select I mean
and then say show insights or some other
thing maybe ask Cortana or something of
that sort of ask Bing I don't know the
exact text but you could say show
insights in some sense and then will
open up a pane at the right hand side
showing more insights about it I mean
the way I showed in the beginning
in the first few figures refers to that
the difference is that this is going to
be power user I mean this is going to be
easily usable so for example you know
about those systems but your parents may
not know for example I mean what I meant
is common users may not know but this
since it is going to integrate with the
browser it's going to be accessible to
more people more easily accessible and
and then of course I mean the accuracy
also matters so I mean yeah I mean since
the system that we are training will be
based on line number of benchmarks a
large number of large amount of data I
mean I don't really have comparisons
with Dexter but yeah it's going to be or
any other systems for that matter there
are many other systems there are there's
a popular system called Ida by Max
Planck University that's a very good one
so there are many such systems available
publicly it's just that this is going to
be a commercially available thing and
easily accessible essentially so any
other questions no okay so I have a
couple of minutes and I will talk about
ticket and cricket linking because I
like that problem okay so I'll quickly
just go there and and it also relates to
the entity linking thing very nicely so
that will help us a little help us okay
so okay so the problem is like this I
mean how many of you I mean read
cricinfo reports I mean I assume very
few people or any cricket match reports
man I mean any report for that matter
Times of India opinion express place
someone would be reading some reports
right no very few people people are so
how many of you are cricket and OCR so I
mean at least like to know about
cricketer score and so on most of you
right I mean I was doing anyway so well
I am so I hope most of you are so anyway
so the point is this I want to do the
similar kind of entity linking but this
time it's a little complicated and the
complication comes because of the new
problem setting so let's look at the
problem setting so here is a cricket
article okay just like the news article
that I was talking about in the previous
thing but rather than really selecting
say Mohit Sharma or are a shoo-in right
I mean these things which can be linked
to good entities I mean
they keep identity exist me my linkage
has to be done at the level of say
phrases long phrases or sentences which
describe events so for example the loss
of a matches are followed biased so here
muxo in the space of three balls from
ooh mesh okay so this is basically a
phrase and this event has to be linked
to appropriate commentary balls so
imagine now my entities are not from
Wikipedia they are really commentary
balls okay so given a 50 overs match
with two innings there are total 600
balls that can be linked so but then the
point is that for this phrase I don't
want to link to one ball okay I in fact
want to identify how many balls should I
linked to because in this case really it
is in the space of three balls so I have
to link it to three different balls okay
so it's basically similar to even an
entity linking but here the entities are
really emails and the point is that
those entities do not exist in the
knowledge base as such okay so I mean in
fact the knowledge base here really is
just about 600 balls of commentary and I
want to link it link this particular
phrase to a set of those balls so how do
I do it is a question okay so so so any
I mean we don't really have much time
but any ideas how would one go about
doing this I mean a few balls exactly
how do you decide what is the right
number exactly that's a challenge by
itself okay so and here let's assume
that nation detection is done okay let's
assume that here we have the user
proactively selecting I mean you user
actually doing an interactive thing as
in the user has selected this phrase and
then told us get me the best commentary
balls can be linked and why is this
important this is actually important
because say I am reading a commentary
report and I don't have time to go over
the entire 300 balls commentary I just
want to drill down from this summary
into details of a particular event only
right so that's why it is interesting
and important okay so now how do I do it
is the thing right so so the way I mean
essentially provide an ability to the
user to zoom in on a particular event
mention from a match report and read the
ball commentaries most relevant to the
event
okay so okay right right so I think the
first step is basically to get a more
structured knowledge base out of those
right so I mean I'll just go over a few
things so basically examples are like
this so this guy produced a
scintillating 119 from 1 1 0 balls so
this face should link to the 1 1 0 balls
I mean theoretically so ideally I mean
well practically you cannot show all the
1 1 9 balls here are 1 1 0 balls here
but theoretically it should really
linked to all the 1 1 0 balls in which
he batted similarly brilliant bowling
figures of to 447 in 10 overs so now
this should this players should link to
the 60 balls to the 1000s said that the
player bowled Sparky Kenny off 29 from
25 all this players should really link
to the 25 balls in which the guy
actually did the batting right Munaf
Patel actually put together a spell of
19 balls for just 8 runs so this should
link to the 19 balls I mean essentially
it's like people balling in tandem break
together and what was the overlap
essentially and then I mean as generic
as this the India innings if somebody
selects it should really link to all the
balls in the Indian ink really right so
it's it's at a different granularity so
some balls to some some of these phases
could link to just one ball if say for
example it is describing a wicked wicked
getting dismissed and so on ok so and I
mean well the problem definition is
quite generic you could actually
basically compare it with the football
itself so you could try to link mentions
in football match reports to
minute-by-minute commentaries in
football or you could also try to match
you could also try to do the same thing
for chess where basically you have a
description of the chess match and then
you could our chess game and then you
could link
phrases to different moves in chess
right and your commentary by move and so
on so so the way I mean the the the the
basic system diagram is like this take
the match comment we do a structured
parsing of the match comment tree to be
able to extract as much information as
possible
I mean what event happened as in was it
a 4 or a 6 or a dismissal
you know all kinds of events was the
thing referred to the Empire and Empire
and so on so essentially lots and lots
of such structured parsing so who was
the baller who was the batsman who was
the which is the country he was playing
for I mean all kinds of things that
could potentially appear in match
reports so and then also derive some
entities on top of them so essentially
since we know cricket we know that one
could not refer to a random collection
of balls there needs to be some
collection of balls which can be defined
by all balls in which a particular
player batted or all balls in which a
particular player bowled
or all partnerships between people are
all balls in which a particular player
hit fours and so on so basically you
could really define a collection of
balls and extract derived entities okay
so I mean if you think of it in the
general framework of entity linking
people usually try to link phrases to
entities which exist in knowledge base
and people don't care about collections
of entities as such so for example I
mean now this is a specific problem but
in general say if some page is really
talking about fruits are good for health
so now usually nobody cares about
linking fruits to a collection of
entities which could really be called as
fruits right so but this this kind of
works off tries to bring in that
intuition that there is interesting
stuff which can be done if you can link
collection of entities so so where
collection really can be derived based
on the domain knowledge so derived
entities computation and then further
mentions from a match report so when a
new match report comes in you basically
do a lot of lot of parsing so
essentially since this domain is about
people right I mean people like people
playing coreference resolution makes a
lot of sense
then then doing post tracking NER
sentiment analysis a little bit and then
then there are the usual things I mean
the mentioned type detection so here by
doing mention time protection I try to
sort of first understand whether this is
a single ball mention or a multi ball
mentioned because single balls are
usually very easy to link while multiple
cases are usually difficult especially
because in the multi ball case you have
to first scientific balls you should
link to or what particular derived
entity you should link to in general
while in a single ball case it's usually
very easy if you can understand what
particular entities are playing the role
in that event and and you have done
sufficient parsing of the commentaries
and then mention sub class detection so
in both the cases the single ball case
as well as in the multi ball case so we
do a lot of sub class detection as in if
it's a single ball case what kind of
ball is it I mean is rated dismissal is
it is it a four or a six that is being
described because certain match reports
talk about say all six is by such in
right so and then and then they're
describing details about how the shot
was awesome and so on so so and then
again in the multi ball case that could
be subclasses like it's talking about a
partnership or a spell together or it's
about somebody's backing or somebody's
bowling and so on and then identifying
kind eight identities so here kind
identities for the single ball case
would be just a set of balls so what set
of balls I mean could potentially be
linked to this particular mention or in
the multi ball case it could be a set of
derived entities so what kind of
different entities it could be linked to
so essentially can it be linked to
batting by several balls in which right
or or can it be linked to partnership
between so against such in and so on so
those are the different kind identities
and then finally there is this candidate
ranking thing so the same thing that we
discussed in the interlinking thing
candidate ranking and linking so here
here we actually do it in multiple steps
so of course the coherent I mean all the
features that we discussed are important
but the way that we have implemented for
this domain is what really matters okay
so for example we surely do similarity
match between that mention text and the
commentary associated with each ball we
surely do that but the way we can do
that either we can do it in the
unstructured similarity winds
essentially we can say that number of
matching words so maybe the commentary
would say that well this was referred to
the third umpire and so on and then even
the mention is going to say the similar
things use the similar words right so
unstructured similarity helps but
oftentimes slot match I mean structured
similarity also helps where structured
similarity really means things like well
if
if this is mentioned where the
subclasses about somebody's dismissal
then this must have the bowler who
dismissed the guy the guy who was
dismissed and potentially the score at
which it was dismissed okay so if you
really can sort of identify the slots
that need to be filled and then just a
sort of map particular balls with this
mention based on those slots it sort of
gives an added benefit so this this
particular thing really maps to the
similarity part of the candidate linking
this part is really about coherency so
remember we said that the set of
entities to be link should be coherent
in nature so here the way you could
define coherence is by sequential
proximity so when somebody is writing a
report at least in the same paragraph
they mean they maintain a flow saying
that well the first week it felt like
there is the second one felt like that
and third one felt like that and so on
so there is some sort of a sequential
flow in in in that sense so the same
thing you could sort of exploit and then
rank candidates realign candidates based
on what is the I mean the sequential
proximity in terms of the the the ball
which is linked from that mention so and
then finally I mean in this particular
case you also need to do some iterator
handling because for the multi ball case
you could derive entities for example I
mean the derived entities could look
like wedding by sawako wedding by such
in right where the mention could really
be talking only about a part of the
entity so it could be talking about say
the in the first six balls have equated
really well gets the first six balls
really should map not to the entire
batting edge network but only to the
first six balls okay and then I mean so
one needs to find such iterators extract
iterators where by traitor I mean
iteration start iteration end in the
unit of iteration wait for example in
this case it is with zero to five and
the unit is really balls
okay so many times it could be wicked so
the last three wickets fell very quickly
so it would really be an iterator
basically saying wickets eight to ten
and and the iteration unit is wickets
so essentially saying the iterator
handling plays a role in the multi ball
case when doing the final candidate if I
find the linking yeah so okay so so so I
think yeah I mean there are more details
but I think more or less I will just
stop here
here there are various details how we
did that so this case is an example of
iterator so he started off in a frenzy
scoring twelve out of his first six
balls so there you don't really want to
link it to this entity backing by Sehwag
but really want to link it only to the
first six balls of that particular
derived entity right so so that's that
and yeah and then then we did it over I
mean over a large number of articles
from from 2011 Cricket World Cup and I
mean overall I think we have like I
believe we have like around 69 percent
accuracy at at being able to link single
ball mentions and about like 56% for
multi ball mention multiple dimensions
are usually difficult so in f1 score of
56 percent which is not very bad I mean
if you look at the difficulty of the
task overall okay so anyway so I think I
will just conclude here yeah so so the
takeaways are as follows I mean we
discussed not three but two interesting
problems in entity mining one is entity
linking and it's sort of application in
cricket and then entity mining is a hot
area of research so lots of people have
been doing lots of interesting things
and people have started to think about
words and queries is more of real-world
objects as entities so things are moving
towards entities and entity semantics
and lastly Microsoft being especially
Hyderabad is an awesome place to work
okay so that's one of the takeaways -
okay so that's it so any questions that
you have now
yeah yeah yeah so I mean there are more
details so to 2008 28 label dimensions
so basically 2008 28 phrases were linked
to some single ball or set of balls so
that's a reasonably large data set when
it comes to manually labeling things I
mean here we don't have auto label data
sets like Wikipedia so here we needed to
do manual labeling really okay any other
questions that you have huh from which
database I mean so so you could pretty
pretty much use any languages I mean it
really depends on what kind of formats
it is available and so on so I mean well
I use c-sharp for doing things but and I
probably I mean for in this case cricket
case I didn't even use a database I mean
it was basically just a very simple
thing because the number of entities
were very small I just use text file but
I mean yeah that's basic okay so your
question mainly is about about a
scalable implementation of such things
so I think in that case at Microsoft
well we have our own architectures in
place and we use our own specific query
languages yeah I mean we'll there is a
there is a very SQL ish kind of language
called scope but this is the point I
mean you knowing that because that's not
externally available anyway so yeah
people use Sparkle for XML each kind of
databases that's true yeah I mean it
really depends on how the how the
knowledge graph is available so the most
of knowledge graphs are available as RDF
triples so yes you could use particle
for for those kind of things and in fact
I mean everyone really tries to have
that benchmark in in mind that they just
expose their knowledge graphs as RDF
triples yeah
that's a good question so there has been
a lot of talk about having Satori
Microsoft Satori as an API so I don't
have a definite answer as to is it
publicly available right now or not or
if not may need to be publicly available
but if you want to have a definite
answer just email me I will try to get
an answer for you any other questions
yeah I mean I think so so I I sort of
get your point so there is a lot of work
in semantic understanding but I mean so
my personal opinion is that most of that
work is towards trying to standardize
things from xml ish perspective so
defining more concrete schemas to
represent the world and really talking
about knowledge representation and
things of that sort right so yeah I mean
in some sense it is a step towards that
because it also helps to understand the
world better but not by say real
understanding so not by a language
understanding or not by yeah yeah not by
Lavvy our understanding but by
understanding through data so this is
more I mean this field is really
data-driven so if you have large amounts
of data
your systems would usually do good
versus say old linguistics or aih kind
of systems and not old well I mean a
linguist would really kill me but I mean
systems which are from that perspective
they don't really need much of data but
they depend more on grammatical analysis
and stuff of that kind so this is more
of a data-driven Sciences what I can
best say yeah okay any other questions I
think money is gonna be around you can
ask questions so what we'll do is before
he switches his hats and where's the
hackathon hat like you've done for all
our speakers on behalf of the summer
school organizing team let's thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>