<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning from Rational (*) Behavior | Coder Coacher - Coaching Coders</title><meta content="Learning from Rational (*) Behavior - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning from Rational (*) Behavior</b></h2><h5 class="post__date">2016-07-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SoVOciI8vaQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let's welcome
thoughts and your kin thank you very
much
so what's about 20 years ago that my
that was well before i arrived at
cornell that my faculty colleagues
started to realize that the world of
computing was changing computers used to
be something that happened in the office
or happen in a lab but all of a sudden
it was moving into our living rooms we
had these these systems now where
computers and humans were interacting
and becoming computational and human
systems and if you think about it what
we have today we have all of these
social networks we have search engine
recommendation systems and it's really
about the interactions of human and
system that makes these systems work
this is how the Department of
Information science was born at Cornell
and the story that i'm going to tell
today during my talk is really one of
the stories of information science it's
about humans and machines have
interacting in a productive way so the
types of systems that i want to think
about are these learning systems that
use machine learning and interact with
users and learn from their users
recommendation system search engines
smart homes and many many other systems
that fall into this category and really
if you think about it these systems get
their knowledge from their users like a
recommendation system like the movie
recommendation system netflix has not
seen any of the movies but it knows a
lot about movies by using machine
learning based on the star ratings that
many many people give to the movies to
actually acquire a lot of knowledge
about the world or think about recommend
data or a search engine like you know
Google such research and it would you
know constantly like to monitor how well
it's doing based on the clicks that
people make and would like to update its
ranking function and improve and of
course there's personalization right you
always want to personalize to the users
make the system smarter as I'd say
actually if you if you want to have a
shot of creating more intelligent
systems it's about really gathering as
much intelligence from the
from as many people as possible so this
is what this talk is going to be about
these types of systems and on a very
abstract level if these systems which
kind of follow in this loop right you
have the user giving a command acts like
a query in a search engine the search
engine response with ranking y and the
user then gives some feedback in forms
of clicks and pre reformulations that
tells the search and how well it's doing
and overall in this loop everybody has
the interest of increasing the utility
that the system provides to the user now
the argument that I want to make here is
to make these systems work it's equally
important to not just think about the
machine learning algorithm but to
actually think about where your data
comes from I meaning much of the big
data that we collect today that we've
heard about today really comes from this
type of interaction loop in our systems
right but if you don't if you're not
careful about thinking about how we
gather this data will gather a lot of
junk but what you really want if you
want to have good data that actually
answers the question against us the
knowledge that we want to have so what
this talk is about is mostly really
about the user part of machine learning
right how do we get good data from the
user in particular how do we get
meaningful data how do we add unbiased
data and how to get more data and there
are many situations where we really need
more data let me start with the first
question getting meaningful data here's
the simplest problem that you may have
when you're a search engine you have two
retrieval functions which given function
1 receiver function to and you want to
find out which one is better so there's
a distribution of users for example me
coming to the search engine issuing my
favorite previous vm and ranking
function one gives me that ranking as a
certain utility to me rank a function to
gives me that ranking has a certain
utility to me now I want to find out
which of the two ranking function is
provides higher retriever quality on
average
so the first thought that you may have
is well let's do an a/b test half of my
users get this retrieval function half
get that your TV will function and then
we measure some statistics that are
hopefully correlated with the thing that
we want to improve retrieval quality so
you know people have thought about
things like abandonment rate like the
number of previous that they'll gain 80
clicks the number of clicks per query
where the ranking people click how long
it takes people to click and ideally we
want to find out whether any of these
statistics is actually indicative of
retrieval quality right anybody have a
guess is any of these statistics related
to return 0 quality who thinks yes who
things know a lot of people don't know
which is exactly the point where we were
as well so we wanted to find out whether
any of these statistics actually does
work so what we need it is we needed to
run an experiment and so we implemented
our own search engine which is the
archive full text search archive is this
repository of scientific papers that you
may have used as well and so we built
the search engine and one of my PhD
students Philip rozinsky he hand-tuned
to retrieve a function that gave
different weights to like author matches
versus title matches vs abstract matches
and that is our original retriever
function here and at some point Philip
didn't know how to make it any better
but he knew how to make it worse that's
pretty easy right so what he did was he
took two results from the top of the
ranking and swapped him was to results
from the bottom of the first page and
under very mild assumptions that makes
retriever quality worse and he did this
in more of his for swaps so now we have
by construction a retrieval function
original that is better than swap to
that is better than swap for we did this
again took the original retriever
function threw away all of the field
weights that's
lab that makes it worse and then we took
the results from flat and randomized
their order of the top ten so we have
another triple where we know by
construction original is better than
flat is better than randomized and now
we feel that these retrieval functions
in a in a user study and collected all
of these statistics and here are the
results so down here we have the
different statistics and here's the
value of the statistic and the blue bars
here are the first triple of retrieval
functions where we know the order of
retriever quality and the red bar is the
second triple again of known retrieval
quality and these little whiskers on top
indicate how you may have thought that
these statistics changed if you decrease
retriever quality well if you look at
that plot they don't right there wasn't
actually a single statistic that
reliably reflected the known order of
retriever quality that we knew by
construction that's a bit of a bummer
but maybe we just didn't have enough
data right we just had a few thousand
queries and small search engine maybe if
we had not thousands of queries but
tenth of millions of queries maybe then
it would work right chappell and others
actually did a very similar study on the
yahoo search engine with tens of
millions of queries but essentially came
to the same conclusion there wasn't a
single statistic there very kind of in a
satisfactory way reflected retriever
quality I mean some of these statistics
are useful for other purposes but they
don't reflect retriever quality by
themselves okay so back to the drawing
board so what should we actually do if
you want to decide whether one retrieval
function is better than another I'd
argue that well the first thing that we
should really think about is if you want
to measure something about clicks we
should actually really think about how
do people make clicks like what's the
connection between
utility that we want to measure or
retrieve the quality and the actions
that we can observe the clicks and that
let gets us into models of choice and
then to microeconomics so the basic
model of choice from microeconomics is
that of rational choice so rational
choice says if you have a set of
alternatives then you can model people's
behavior as if they were maximizing some
utility function that's on the line so
basically they would look at all the
options and then they pick the one that
has the highest value to them that
utility now for web search that's a
little far fetched people are making the
decision to click at a time constraints
and they you know you're just looking at
the abstracts there so really you only
have an approximate notion of utility so
if you have kind of constraints on this
rational choice model you would call it
a bounded rational choice model and that
probably fits more better for its better
what what we actually have in in in web
search here and then there are a lot of
behavioral issues that also bias the
selection that people make like framing
fairness loss of there's lots more but
let's ignore those for now because these
bounded rationality models are actually
going to be quite useful as well see
okay so what is a reasonable bounded
rationality model for web search how
people click and search so here's a very
simple model people read the results
from top to bottom and we've actually
done I tracking studies that's not a bad
approximation of how people scan the
results and then they at some point they
stop and we don't know where they stop
and it's hard to predict where they stop
but once they observe the top K results
whatever that k maybe they actually make
a kind of approximate rational choice
among the options that they've observed
so if this is our model of how people
may clicks things like number of clicks
done actually have a meaning right
so how should we then design these
statistics or even better I mean think
about it we are the search engine we can
actually intervene and we have complete
control over what the user sees so we
can adapt the interface to gather the
data that we want so it's a it's about
ten years ago or more 14 years ago that
introduced this technique of
interleaving interleaving as the
following you want to decide whether to
one retrieval function is better than
another what you do is you don't do a
baby test but you give everybody an
interleaved ranking so this interleaf
ranking is constructed by taking the top
k results from each of the two rankings
and merging them together and then
presenting the interleaved ranking to
the user the user doesn't know where the
results came from and so in this sense
it's a blind test then you can check you
can see whether user clicks on this
ranking and you can trace this back to
where these clicks came from so here the
user clicked on 13 and 7 in the
interleaved ranking and you can see in
the foreign key function one there was
only one of the results in the top four
and for ranking to ranking function too
there were three results in the top four
so regular function to produced more
relevant results and rank function one
and you could say it won these pairwise
comparison and now we can do this over a
whole distribution of queries and get a
noisy kind of a binary preference and
that tells you then kind of an aggregate
which ranking function is better does
that actually work so use the same
retrieval system same experiment setup
but now we dip this into leaving
comparison and that's the conditions
that people were assigned to and here
results here are the different pairs
that are being interleaved and I've
organized and this is the value it's the
percentage winds of the retrieval
function in
is interleave comparison and I've always
organized it so that the number of wins
of the red one red bar here corresponds
to the better retriever function that we
know it's better by construction and the
blue is the inferior variable function
so all the red bars are actually
significantly higher than all the blue
bars so interleaving actually perfectly
reproduces the order of retrieval
quality that we knew is correct by
construct now you may say well that this
was archive search this was scientists
very rational people searching for
papers to read right does that actually
work in a different environment as well
so at this point there are published
studies both on bing as well as on yahoo
that this also works for regular web
search so now we're in a position where
we can actually start thinking about the
machine learning algorithm right we
actually have a reliable way of getting
meaningful data and this was precisely
the point where we were six years ago
and we designed what we call the dueling
bandit setting of how do we do online
learning from these paired comparisons
right if I have now k retrieval
functions and I want to find the best
one how do i schedule these pairwise
comparisons to most efficiently find the
best one and at this point there's
actually a whole bunch of algorithm for
solving this problem but that's not what
I want to focus on in this talk let me
skip these algorithms because the key
point that I want to make here is that a
lot of machine learning which you're
thinking about this human interactive
systems happens well before you run the
machine learning algorithm right it's
actually about gathering the right data
and once you have the right data the
rest kind of becomes automatic so this
was kind of an example of how I think we
should be designing these systems right
we start with a model of how people
behave how the actions that we can
observe are connected to the underlying
that we want to extract the knowledge
that we are going to extract utility and
once we have a plausible model of how
that's happening and an accurate model
of how that's happening then we have to
design the interface and we can make
interventions in designing this
interface like interleaving together the
data that we want and then we can design
the machine learning algorithm it's not
just about big data it's about getting
the right data and getting meaningful
there so this kind of dueling bandit
setup is great if you have a few options
that you want to kind of window down to
the best but there's a lot more settings
in which we want to use kind of
interactive learning this users for
example in personalization and in
recommendation and you know this dueling
Bennett setup doesn't really work there
so let me kind of do another pass
basically through the same argument but
form a recommendation perspective that's
more suitable to recommendation so this
is my Netflix page and what Netflix does
is it gives me these recommendations of
things that I would like to watch so let
fix things that I most like to watch
Lewis Black now the way that I pick a
movie to watch or a show to watch and
any particular night is well look at
these recommendations but I don't just
blindly follow the recommendation of the
of the system but I browse around I'd go
like oh lie to me man that kind of looks
interesting and I click on that and at
the bottom of the page there's more
shows like lie to me and I would browse
around for five minutes and at the end
of these five minutes I would pick the
show to watch that I like best right and
let's say in this case it was awake so
the way you can think about this as a
boundary rational choice model is that
given the set of all movies that netflix
has the algorithm started me out at a
particular point that's the
recommendation it made lewis black then
i explored part of the collection in a
self-directed way and then I made a you
too
maximizing choice of picking the Y bar
that I like best so the way they're keen
to prove that you interpret that as
feedback is that y bar has at least
slightly higher utility than the Y that
the system the recommendation that the
system gave to me to start out with but
note that this Y bar is not the optimal
prediction the optimal prediction may be
something that I haven't seen doing my
session right so I can't assume that
this y star is you know that we actually
get this Y star which is the optimal
protection so we call this the cool
active feedback model always get a
slightly slight improvement to the
prediction that the system made you can
think about in the same way also about
other kind of settings so for example
let's take web search again let's say we
see the user clicking on this link then
it's fair to assume that if he
constructed a ranking where this link
was promoted by one position the user
would have liked this ranking better
than the ranking that we actually did
present right so we again have a
pairwise preference but the user gives
us a y bar a slightly improved ranking
over the ranking that the system
presented or think about machine
translation so my parents our I'm from
Germany my parents speak only German and
so I often have pieces of English text
that I want to translate for them into
German so the easiest way to do this is
to copy paste this into google translate
and here's the the the german
translation that google translate gifts
anybody reads German it's not a good
translation it actually doesn't get the
same meaning across at this piece of
text so whatever typically do is I would
fix it up right I would fix it up just
enough so that it becomes understandable
this so this is by no means the optimal
translation it's not beautiful but it's
a better translation than what the
system will read it
right so we again have this corrective
feedback setting where this Y bar that
the system that the user constructed for
me is slightly better than the
prediction that the system made so can
we design algorithms that work with this
feedback that's different from
supervised vtec supervised learning
feedback in the traditional setting
assumes that we get the Y star the
optimal label right can they also design
algorithms that make do with this much
weaker labels Y bar that are just a
slight improvement over the prediction
that the system hasn't made and the
answer is yes there are very simple
algorithms so the simplest one we could
come up with is a perceptron but you can
do the same trick for you know if you
want to do a deep neural left you can do
that as well so basically you just do a
perception update with this Y bar not
the optimal label but just the slight
improvement and so that's the key
difference right the feedback is
different it's not the correct class
label here and also the way that we're
going to evaluate the system's
performance is different we can't talk
about Miss classification rate anymore
what we have to now talk about some kind
of measure of how much in terms of
utility are we presented predicting in a
sub optimal way and so we define this
notion of regret regret is a is a kind
of way of measuring over time how sub
optimal a system actually is and regret
is something that you want to decay 20
or to some small value over time as the
system learns that it approaches optimal
predictions so the regret is defined as
take the utility of the optimal thing
that the system could have done and
subtract the utility of the action that
the system actually took right and
hopefully they're the difference isn't
too large so that's going to be our
notion of quality of the system
seems like a strong notion of quality
because an hour set up you know the
Cardinal utilities utility values we
never actually observe nevertheless we
kind of value in our system by it and we
also typically don't get the Y star here
that we're benchmarking ourselves
against well it turns out it's actually
pretty easy to derive theorems that
characterize the regret they expected
regret of this very simple perceptron
algorithm and so this is the bound and
you know if you have seen the perceptron
and if you have seen perceptron bounds
you will recognize this if you don't
haven't seen this doesn't matter that
much you have this second term here
which is a margin term R times W which
you always have in these perceptron
style bounds it's the norm of the weight
optimal weight vector and that decays to
0 and 1 over square root of the number
of training examples that you have
number of iterations that you learn
that's very much like a regular
perceptron and then up here you have a
term that characterizes how good your
feedback is and that's what it
asymptotes to so let me explain to you
how you measure the quality of the
feedback so we have this definition of
tsaia proximately alpha and formative
feedback that was a mouthful of the name
so we sort of come up with something
better but instead of you know telling
you what this formula actually means
here that's the formal definition let me
tell you in terms of our picture what
this definition of feedback quality
meats so here we have the utility scale
and at the top end we have the optimal
protection that's the thing that gets
you the maximum utility the best
prediction the system's prediction is
somewhere below they are typically right
and then what we wanted in our
cooperative learning setup is that the
users y bar the users feedback is you
know somewhere to the right here right
somewhere between the system's
prediction and the optimal protection
it could be stochastic right so
sometimes the user gives actually gives
us feedback that's worse than the
prediction that the system made right
but what we require is that an
expectation the users feedback is better
than what the system presenters and to
measure how much it is better we use
this factor alpha indicated by this
green region here showing you kind of a
relative improvement over shrinking this
gap between system prediction and
optimum now sometimes will also be in
situation where even an expectation the
feedback is worse than what the system
presented and in those cases are this
kind of loss in utility in expected
utility that's exactly the sigh that
goes into this bouncer so as long as the
size are small the co active preference
perceptron is going to converge to a
very good solution okay so we have a
very simple nice algorithm let's try it
out so we're going back to our archive
full-text search engine and so we came
up with a retrieval function that had
about a thousand features and about was
linear waiting so a thousand parameters
a weight factor to tune and that's what
we use this coactive preference
perceptron for and the way that we
constructed feedback was exactly the
same as we had on the on the previous
slide so if somebody clicked the link in
position in this position we constructed
the feedback ranking as the ranking that
you get if you promote that link by one
position we have to do a valuation right
we actually wanted to know whether the
learning system is actually improving so
the way that we did that is that in
every art iteration we did Co active
learning that the perceptron but then in
every even iteration we took the current
learned system and interleaved its
prediction or its ranking with the base
line with the baseline rancor this
original rank or that we had before
that's hand-tuned rancor so that in
every second iteration we could basic
check whether we're doing better or how
much better or how much worse we're
doing than the baseline and ideally we
would then see that over time you start
beating the baseline more and more and
more right that was our idea here's what
happened this is the number of feedback
iterations of the co active preference
perceptron this is the win ratio win
ratio of one is losing as often as you
win against the baseline so it doesn't
work right it's just you know it hovers
around the baseline doesn't actually
improve over the baseline and in
hindsight we should have seen this
coming we had the theory to actually
realize that we may be in for a problem
and let me explain to you what the
problem is this is the this alpha and
formative feedback and you know here we
have a system prediction that's pretty
far from the optimal prediction and now
it's pretty easy for the user to give us
a feedback ranking that's actually an
improvement but if you move closer to
optimal with our predictions it gets
harder and harder to actually come up
with an improved ranking and if you move
really close then we are starting to
incur these slacks and the reason why we
incur these slacks is the following I
mean think about it if this is the close
to optimal ranking here and users are
just a little bit noisy in how they
click then there are a lot of links that
they can click on that make this ranking
worse kind of point the craft of
preference perceptron in the wrong
direction and only a few links that they
can click on to give you an improved
ranking so if the noise level and your
clicks are sufficiently high which it
probably is in web search right then you
get this long tail here of all of the
rankings that are actually worse than
what the system today predicted and we
get the shift where the expected
feedback actually is quite bad
now what would we really want you would
like to shape this feedback so that the
distribution looks more like this that'd
be cut off this kind of nasty tail of
all of the bad rankings and all of the
noise that this is that that the users
give back to us in the ideas again not
to kind of mess with the learning
algorithm but to actually think about
how do we get better data from the user
cleaner data from the user so here's how
we can do it it's called fair pair and
it's very simple so instead of
presenting just the ranking as the
search engine outputs it what you do is
you take adjacent pairs and you flip
them with probability fifty percent and
then oops and then you only create a
feedback ranking if the user clicks at
the bottom of one of these pairs now
think about it each of the elements of
the pair has the same probability of
being in the bottom of the pair if the
users act completely randomly we're
going to get data great feedback in both
directions of the pair with an equal
probability in expectation they're
actually going to cancel out so that
means kind of in the worst case if uses
a completely random we still only get
feedback that kind of keeps us at the
position where we currently are in the
correct of preference perspective
because errors cancel out noise cancels
out if you collect data in this fashion
but if users are just a little bit
rational if they just are slightly more
likely to click on the more relevant
results we actually get alpha and
formative feedback that points us in the
right direction
so here's the same experiment and the
only exchange that we did was we
introduced this fair pair perturbation
in selecting and collecting the data and
now the system even just after a few
hundred learning steps is already so fit
substantially better than the baseline
and it keeps improving so it was again
about collecting the way that we
collected the data making interventions
into the interface that gave us cleaner
data that our learning algorithm had
much easier time learning with so this
is this the plot that is in the IML
paper and it's the all the data that we
had by the submission deadline but
actually about a year later karthik
three Catholic Rahman who was the lead
student on this on this paper one of my
office and we were thinking about a
different experiment to run on the full
text search engine and at some point we
wondered wait did you ever turn off that
experiment and was like I don't know
probably not so and indeed it wasn't
turned off it was running the whole time
so karthik went and collected all of the
data that the system had accumulated
over the year and and this is what it
did so it just kept improving and then
kind of converging to some very high
rate of beating the baseline I've tried
to build kind of real-world learning
systems that that can live in the world
and autonomously learn in the world and
be intelligent in the world for a long
time and most of the systems worked but
they all required kind of careful
maintenance and feeding and you know
once in a while a little you know
careful tuning this is the first system
that I could actually literally forget
for a year and come back and see that it
performed just fine so that really kind
of made me convinced that if I have the
right model of user behavior and I
collect the data in the right way for
the proper learning
and I can build these very robust
systems that can live and learn
autonomously in the world without human
intervention so that was the second kind
of episode here where it really given
from coming up with a good model of how
people make decisions and how that's
connected to utility the underlying
knowledge that people have and then
designing the learning algorithm and the
interface for collecting the data in a
way that actually goes together well and
that gives you reliable ruling results
so let me get you to the last one and
and this is about getting more data we
live in an area in an era of big data
but there's tons of problems where we
still don't have enough data so one of
these problems is that of session-based
recommendation so it's different from
long-term recommendation where you have
like a long term profile of what you
generally like session-based
recommendation is like the task of you
know which movie should i watch tonight
there's only one movie that I can watch
and for tonight there's there's a lot of
hidden context you know which mood am I
in tonight am i watching this movie with
a friend do I have one or two hours
right all of this is hidden from the
recommendation algorithm and you would
want to use a session based
recommendation algorithm to quickly get
this information within this one session
to then adapt the recommendations that
you give me an ow don't recommend to our
movies if the person has only one hour
of time to watch right so to be a
Schnabel Paul Bennett and pseudo may had
this brilliant insight of how we could
actually get more data for this kind of
session based recommendation and before
I tell you what that brilliant idea was
let me tell you what you're typically
dealing with so this is an interface for
you know as you would see it in Netflix
amazon as well for selected movies you
can browse through the movies you can
you know click on a movie and explore
more information about an elusive
director and so on and then eventually
you choose one movie to play so they did
the user study where they gave people
this interface very standard interface
and then collected all of the feedback
that they got particular you can you can
you can treat you know getting more
information about the movie as a
positive signal that this movie is
something that they were considering
that was interesting to them right so we
have positive feedback of movies they
explored and movies that they were they
saw but that they skipped that they
didn't interact with and then this
interface this is basically the only
type of kind of feedback data that you
have so they were trained a ranking SVM
based on this feedback data and then to
test how well this predicts we took the
movie that the user actually chose in
the end and embedded it in 99 other
random movies and then we had to rank
against with them predict which of these
100 movies was the one that the user
chose basically was a ranking task so
rank the chosen movie as high to the top
as possible and hear the results this is
mean reciprocal rank it's a measure of
how well it ranks and you get a tiny
improvement from learning right if you
take a random ranking that gets you mean
reciprocal rank of 0 point 0 52 if you
do this learning you get at 0 point 0
6-3 not very impressive and the reason
is simple right an average session had
only 2.75 movies that it had positive
feedback on and that's just not a lot of
data for machine learning algorithm to
deal with any machine
so here's the idea that Tobias Poland
sue had there's research from psychology
or theory from psychology that suggests
that for people to keep things and
short-term memory is actually work it's
cost and we are not very good at keeping
things in short-term memory so they came
up with the idea of providing a short
list to the user so a shortlist being a
kind of short term digital memory where
people could you know add movies that
they would like to have under
consideration for this one session and
then they could later go back and you
know make us a final selection from
these movies very yeah very interesting
but very simple idea right now we have
three questions first questions is now
what's this psychological theory
actually right to users appreciate this
kind of digital short term memory that
helps them augment their their abilities
to keep track of things within one
session second question is do they
actually make use of the short list so
they had to get more data and the final
question is does this additional data
actually get us better retrieval quality
so let's start with the first question
do people so in this user study the
subjects were exposed to both interfaces
the one without the short list and with
the short list and after the study was
over they were asked which of the two
interfaces they preferred and as you can
see they had a very strong preference
for the short list they actually really
liked the shortness there were a lot of
interesting quotes you know the people
explaining their their their choice here
why they know what they prefer and this
was a particular nice one in each round
it really made me confident that I
wasn't losing track of a good movie in
the shifting sands of my short-term
memory very poetic so people like the
interface interestingly if you ask them
you know how much do you how satisfied
are you with the choices the movies that
you're actually selected when you were
under these two conditions that too
different interfaces they also strongly
prefer the movies that they selected
when they have the shortlist available
so they the Medusa thought that we were
making better decisions there is the
shortest okay so that's good so people
like it does this liking it actually
translate into using it and the answer
is a pretty clear yes so in ninety-three
percent of the sessions where shortlist
was available people actually did use
the short list and that roughly doubled
the amount of data that the
recommendation algorithm had available
right so if you take now examined and
shortlisted both as positive signals the
amount of data positive data that you
have actually doubles if you know give
this to a ranking SVM and train them on
both kind of sorts of positive data you
actually also get basically a doubling
in the mean reciprocal rank so you're
doubling in this sense the
recommendation performance as well so
this was yet another example where
you're going from understanding people's
decision processes here in a sense of a
cognitive limitation that they have
designing the interface where's the
purpose of collecting the right and the
right amount of data so let me wrap up I
think all of these three examples are
kind of making the point that if you
want to build these intelligent systems
that gather intelligence from their
users and augment basically and and and
really kind of make an effort to go
towards gathering more and more
real-world knowledge that makes these
systems more and more intelligent I
think it's equally important to think
about all three components of this
machine learning system without any of
these components none of this would have
worked and the first one is of course
that we wanted to learn from user
behavior we have to have a reasonable
model how in
that particular situation the observable
actions are connected to utility to the
knowledge that we want to get from the
user and utility and basically in quartz
encodes the knowledge and an actionable
way of biases and limitations that
people have to augment their abilities
then we need to design based on these
insights we need to design the interface
and we can do interventions like
interleaving or fair pair to get the
data that we want we can shape the data
distribution and with the wrong data
none no learning algorithm is going to
work so we have to get the right data
and so we have to be worried about these
two things and then finally we can think
about the learning algorithm but I said
we try to make the point here in this
top really much of the action happens
well before you actually start with the
learning algorithm so this is really a
way of getting large-scale data much
larger that you could ever get by hand
labeling directly from the users and I
think this is an interesting path to
building these intelligent systems it's
also an interesting example of what
information science means right it's
this idea that or this necessity
actually that to make these
computational systems work you have to
think about your users you have to think
about humans and this relates to all of
these different other disciplines all of
which were relevant in making these
systems work thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>