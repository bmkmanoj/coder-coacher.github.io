<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Deep Learning - Max Jaderberg | Coder Coacher - Coaching Coders</title><meta content="Symposium: Deep Learning - Max Jaderberg - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Deep Learning - Max Jaderberg</b></h2><h5 class="post__date">2016-06-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/T5k0GnBmZVI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">materials supplied by Microsoft
Corporation may be used for internal
review analysis or research only any
editing reproduction publication
reproduction internet or public display
is forbidden and may violate copyright
law
so just introduced max yeah de Burgh
swirl as you're sure sort of said Wendy
organizers were located recommendations
from the PC members and feedback from
the community internally they were think
about the oral talks especially was we
wanted to identify the papers that we
thought pretty much every deep learning
researcher should know about
and so Max and his quarters developed a
I want to say you know remotely simple
intend learning algorithm for learning a
simple spatial transformation for the
input layer or for other layers that
gives remarkably good results so we're
excited to have him here today to tell
us about that work tickly ready max okay
cool
hi my name's max yada berg and I work on
yes facial transformer networks together
with Karen Andrew and Cori at google
deepmind's it's basically a new module
from your networks that adds some new
nice functionality for them so we have
convolutional networks and commnets are
awesome and you know especially for
vision they really just gain everything
and part of what makes them really
really good is that you have these
multiple layers of convolutions with max
pooling and having intertwining
convolution as max pooling gives you
some sort of spatial invariants but at
the moment is pooling is simplistic you
know you just take the max over a small
window in this window might be a little
two by two segment and this means that
you only get very small amounts of
invariance perth pooling there and which
means you can only be limited
you only spatially invariant to a very
limited amount of transformation also
another problem with max pooling at the
moment is that you pull from across the
entire input image and yet obviously
this is really really effective but the
question is still can we do better or
can we add some more functionality and I
think what I really thought we were
missing is this conditional spatial
warping that is conditional on your
input image
can you allow your network to spatially
warp and transform the data and you know
the reason why you might want to do that
is that means your network can transform
the data into a sort of normalized space
which is more expected by the subsequent
layers of the network another nice thing
about this if you can do to conditional
spatial warping is that you can then
intelligently select features of
interest I you do attend ssin and so
hopefully your networks can be invariant
for more classes of spatial
transformations obviously this isn't the
you know first piece of work which we
wanted to do this has been a whole
history of work which wanted to do I
don't know achieved spatial invariance
depos normalization having attention
systems and detection systems what I
think is maybe something new that we
bring to the table is this is quite a
simple and efficient module which can be
trained just from gradients just with
backdrop there's no extra supervision
and there's no reinforcement learning
involved so here's a here's a preview of
some results basically this is a spatial
transformer network trained to classify
distorted m-miss digits and here for the
purposes of video I'm just smoothly
varying the input and you can see that
automatically just by minimizing
classification area classification error
the spatial transformer networks learn
to attend to impose normalize objects of
interest so there are three main parts
of a spatial transformer the first is
this localization Network which takes an
input feature map and gives you the
transformation to be applied to that
feature map now there's a grid generator
which produces a sampling grid I use
sampling in this sort of graphic sense
here and then as a differentiable
sampler which applies that sampling grid
and performs the transformation so first
is this localization Network this is
just a mini network maybe a two layer
fully connected network which takes the
input image or the input feature map and
then regresses the parameters of the
transformation to be applied to that
feature map so for an affine
transformation for example these are the
six elements of the
find matrix or if you're doing attention
then this would be three elements this
would be the scale translation x and y
you can also do this in place spline
where you'd regress the control points
and or something like that so the next
is the grid generator the grid generator
takes si takes the transformation
parameters from the localization Network
and produces this sampling grid I don't
have a cursor but okay the the sampling
grid is basically for each output pixel
the XY coordinates in the inputs where
they're out per pixel gets its value
from and so here you can see an example
for an affine transformation to produce
this transform sampling grid and in
green you take a regular prior grid to
this gray black grid and which is just
one one one two on three coordinates and
if you multiply the quarters of that
grid by this you do matrix matrix
multiplication with the parameters
progressed by your localization Network
you get this transformed sampling grid
in green and then the third part of this
framework is a differentiable sampler
and that takes a the sampling grid and
actually applies it to the input
producing the transformed outputs and so
if we just used the regular prior grids
this would be the identity transform as
you can see on the left and you just get
a complete pass via the image but by
applying the transformation on the
sampling grid you actually get a
transformed image on the right and so to
do this resampling you can do for
example nearest neighbor resampling or
famosa worker used bilinear
interpolation and so you can actually
write image by linear interpolation
resampling out in this form and if you
write out in this form it's really easy
to see how you can define gradients with
respect to use the input and also
crucially with respect to x and y which
are the coordinates of the individual
sampling points in this sampling grid
and because we have all these gradients
defined this is a fully back palpable
structure and so we can actually back
drop our classification error for
example our global task loss
all the way through this structure
and so we can learn we can train this
localization net within our network just
to minimize the global task loss without
any extra supervision no post
supervision no localization supervision
and you can just drop this module into
networks wherever you want wherever
makes sense so you can have them you
know straight after the input you can
have them deeper in the network acting
on convolutional feature maps rather
than the image you can have multiple
transformers in parallel a reason why
you might want to do that is um that
would allow you to attend your transform
multiple objects in different ways in
your data okay so that's pretty much the
crux of the technical stuff in the paper
now it's just having fun with some no
transforms endless and toy data so here
the model is basically a spatial
transformer acting on the input image
followed by a small classification
network and so we call this a special
transformer network and so this is the
results of training this simple
classification model on these distorted
M missed data sets so here for the
purposes of a video I'm just smoothly
smoothly changing the input images and
this is trained on images and so you can
see this for different types of
distortions of the data and also
different types of transformations
applied to the data so for example we've
got rotated and this on the top left
with an affine transformation and for
example in the bottom right you've got
translated rotated and scaled em nests
with a thin plate spline transformation
and what you can see the raw data is on
the left and you can see the
transformation visualized on the input
in the middle and then the output of the
transformer which is fed to the
remainder of the classifier is always
stable pretty stable for all of these
models which means the classifiers a lot
easier time to to learn and so here's
some quantitative results basically we
just focus on this call at column which
is the rotation translation and scale
distorted data you can see that a fully
connected network does pretty poorly you
get 5% error it's because there's no
mechanism for spatial invariants in a
free connected network if you use the
CNN with max pooling where you do really
really well you get point 8 percent
error what's interesting though is if
you put a spatial transformer before
fully connected Network
you recover the same performance as a
CNN so the spatial transformer is giving
you that spatial invariance and you can
do even better by adding a spatial
transformer but before CNN with max
pooling as well and so you get better
again
and so you can have some fun with this
framework and to show you why you might
want to have two transformers in
parallel here's a task where you have a
two channel input image and in each
channel of the image you've got a
different digit which is randomly
distorted in different ways and here the
model has two transformers in parallel
both transformers look at both both
input channels but what happens during
training is that one transformer
specializes on one channel or the other
on the other so you can see the channel
1 output of spatial transformer one is
stable while the channel 2 output isn't
whereas for the other transformer its
channel 2 which is stable whereas the
other isn't so this makes it a lot
easier for the subsequent freely
connected network to classify and add up
these two digits
and you can also do this unsupervised
co-localization experiment which is a
bit like unsupervised tracking where the
goal here is to find the common object
in this data set without actually saying
what the object is and so here you can
set up for the problem as a triplet loss
using a transformer to find bounding
boxes and you want the distance in some
embedding space to be small when you use
a transformer between two samples where
use a transformer compared to if you
just took a random bounding box and on
the bottom you can kind of see the
optimization process that is happening
okay so then we tried it on some real
images on Stevie House numbers which is
this competitive computer vision dataset
I basically took the state-of-the-art
CNN architecture and for example for the
last spatial transfer model I just added
four spatial transformers one before
each of the first four convolutional
layers so what's interesting about this
model and the bottom is that the last
spatial transformer actually acts on the
convolutional feature maps not the raw
image and if we use this we get
state-of-the-art results and what's
really interesting is that as we move
from 64 pixels of background context 128
we barely lose any performance so you
know these spatial transforms are doing
what you want they're actually attending
to the objects of interest and you can
see this happening it's actually
progressively pose normalizing the
digits and this just makes it easier for
the remainder of the network to classify
we also tried this on fine-grained bird
classifications so this is 200 Way
classification of different bird species
we started with some fine-tuned there's
some imagenet pre-trained networks and
then fine-tune them on birds and the
model that we used is this mother way we
have a single localization Network which
outputs multiple transport
transformation parameters so we have
multiple transformers in parallel acting
on the input image the output of each of
these transformers is in fed to its own
inception architecture whose features
are concatenated and fed into a final
200 way
linear classifier and again we get
instead of the our results here even
though we have a very very strong CN n
baseline and you can kind of see why
this works on the if you look at the the
first row of the images this is the case
where we have two transformers in
parallel and we also do experiments
before transformers in power but if you
look at the two transformer images you
see the red transformer always focuses
on the bird head and the green
transformer always focuses on the bird
body and that's somehow pose normalizes
this bet for the remaining classifier
and this is learnt just to minimize
classification error there's no extra
supervision there's no part annotation
there's no bounding boxes at all and
what's nice about this model is it
allows you to use much higher resolution
inputs and transform high resolution
inputs and then down sample them and
still use your lower resolution
classification architecture so you can
get better performance and there's
nothing really constraining us to just
doing 2d transformations with this
framework you can extend this to for
example 3d where your third dimension is
depth or it could be time for a video as
pretty simple to do and then you can do
a 3d to 3d transformation and something
cool that you can do with this is
actually once you've done your 3d to 3d
transformation is then flatten the the
depth dimension by for example summing
over it or doing a max operation over it
and this gives you a 2d output but it's
it's a projection of this 3d space so
you basically created this
differentiable camera where in your
network can now decide where to take a
picture of this 3d input from and so the
nice thing about this why you want might
want to do this is that you can convert
your 3d data problem from yeah 3d use
this differential camera projection to
2d and then use your nice fast 2d
convolutions to actually do
classification and so if we're going to
do this we need a 3d deysa setten
because we're addicted to em list we're
just going to take em list and then
extrude it in three dimensions and
create
vocalized M&amp;amp;S which were then going to
randomly place in various locations in
this 3d space Fey scimitar like what we
did in 2d and so if we train a network
to do classification on this data these
are the sort of results we get on the
left you can see the raw the raw input
again I'm just smoothly bouncing this
digit through it with the spatial
transformation the 3d to 3d
transformation visualized in the raw
info it brought in it input in the
middle is the output of the the
flattened output of the 3d transformer
the output of the differentiable camera
which is then fed into the classifier so
yeah this kind of works and yeah I mean
that's all I've got time for this here
we've just created a differentiable
spatial transformation mechanism which
has new functionality to newer networks
it's a new building block for you to
play with it performs conditional
spatial warping which is learned without
any extra supervision just by minimizing
your global task loss and there's some
third-party open source implementations
around such as in torture lasagna and
cafe I'm sure there's probably more
floating around and they probably work
as this new work which makes some nice
use of them so yeah thanks
Thanks so um we have time for a few
questions there are a couple might say
the back view on the line up there hall
a call says a Mikan can run two people
oh go for it yes very interesting um I
have been learning much during this week
about attention which I realize is a
constant addressable memory typically
now what you're doing here is a
coordinate addressed memory much better
for runtime but often more difficult in
terms of finding the right content do
you feel that this is struggling to
identify the right bit of data to
extract in these cases in these data
sets that I've shown it doesn't struggle
at all especially if you use a nice
optimizer like atom for example but
you're right if you if you have very
very at a very very big image and a very
small object of interest your signal
from the gradients might not be enough
to actually attend to that object so it
can be harder then you might need to
resort to some reinforcement learning as
well make sense thank you
very simple question when you initialize
do you initialize based on a
perturbation around just a standard
identity transformation or is it
completely random Gaussian no no so we
initialize so the bias of the
localization Network gives you the
identity transform then with random wait
so it's basically the identity transform
with a little bit of perspiration as you
said yeah hi fantastic work I'm curious
if you wanted to make the
transformations even more stable under
the sorts of animations you were showing
earlier what approach you might consider
yes so all these animations by the way
we're just trained on single images they
weren't trained on movies it was no
recurrent net it was just a feed-forward
nets so you know if you want to start
classifying video and stuff like that
with a transformer with an RNN you can
start being smoothing costs or you can
start I'd like yeah if you had a model
which was actually trained for this it
would be smooth I wouldn't make you did
you find the optimization to be easy or
difficult did you use batch
normalization or something for the for
the birds experiment we use special
normalization and that definitely helps
in general if you use atom the
optimization is pretty straightforward
if you use SPD you need to do a bit of
gradient scaling on the localization
Network because you effectively have you
might have a thousand sample points
which will have gradients some will call
point coordinates and they all
accumulate for example in the six
parameters of the affine transformation
so this is huge accumulation so you just
need to scale that if you use SGD but
yeah maybe listen um Sergei is not
setting up and meanwhile well but in
stairs here what is a one last question
do you think how broadly applicable do
you think this is would you try it on
audio data yeah so I think this is work
on I mean your your data has to be
smooth that's that's kind of what you
assume with this framework say audio
data would probably work any sort of
smooth data I think you would have a
chance working thanks a lot let's take
max icky thank you
you
each year Microsoft Research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>