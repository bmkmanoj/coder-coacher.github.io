<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Entity-Centric Search: Querying by Entities and for Entities | Coder Coacher - Coaching Coders</title><meta content="Entity-Centric Search: Querying by Entities and for Entities - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Entity-Centric Search: Querying by Entities and for Entities</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/glQaIUmvaxc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright let's get started it is my
greatest pleasure to welcome young way
our to come back and knee and have an
interview with us mua has been
outstanding in turn with is RC before
and he's graduating this summer I lope
and the his work is on the entity
centric search and so this is happens to
be the area that we are focusing a lot
so we are very happy to have him to come
and tell us tell us what he has been
doing so without further ado young wait
okay thank you I think it's constant
yeah so thank you for coming to my job
talk today so I'm very happy to share my
working my PhD study to you yeah so
topic as we show here it's about NTD
centric such currying by any TSN for
entities so should I speak louder or
this voice would be fine okay sure yeah
so you know nowadays the web has become
a very huge database they storing all
kinds of like different entities so by
any design means life example a person
like products like company all the
things i need is and because of least
most of the information retrieval
operations on the web and i know also
related abilities people are searching
all kind of information for example for
latest event for some person some phone
number for some company some historical
factor so all these kind of information
receive operations are energies and
check according to some statistical
reported in the previous vocalized 70
two-point-nine percent of web queries
actually or entity oriented and 71
percent of web korea contains the name
entities well in order to get the
information so nowadays people get used
to use the changing so as the
traditional search engine framework
people use
keywords to characterize what they want
and then the search engine will return
on top documents but we also find that
these kind of searching framework it's
not cannot better support cannot will
support its kind of entity centric
search operations so for example the
given enquiry died when did michael
jordan graduate if in terms of input if
the search engine are not aware that
this career actually trying to search
for energy and also have some energy
involved in this query then you may not
be able to better judge whether
witchetty law is correct and also for
example you should use the keywords you
do you have some ambiguity issue there
and in terms of result for those kind of
like operation when people try to search
some particular entity they went in
returning a document or may not be a
good option as well because the users
have to look into a document to find
answer yeah so so you might PhD study
actually I try to focus on which I can
move beyond this kind of traditional I
even ritual framework so we want to
study when the concept of entity
involved in the input curry or in the
output result how can we actually change
this kind of research behaviors such
ranking result or something so here I
use a diagram to show the position of
our work so use the X Essex to show the
input and y x 62 to the output so in
this way tradition I are will become
lighting list in Terraria and what we
were starting to study when the console
entity appear in the input or output so
we call this kind of operation and it is
such an eccentric search operations so
specifically according to whether the
entity or peer in the input or in the
output we can further categorize these
kind of such operations so if the
concept of entity appear in the input
usually people will note some entity and
then they want to find some information
about this initely we call this kind of
operation is aquarium
I entities for example find some ladies
event was in person for some useful
reviews on film product and in conscious
with querying by entities we also have
querying for entities that means the
output of results should be the that
means that users who expect the output
result condensed energy and in this case
for example won't find the highest
building in the world or the most
expensive handbag or something like that
yeah I including that at least two areas
you have some call overlapping area so
in this kind operation people usually
try to search some entities that match
some desire relation for our input any
keys so we so query by entities for
example like the phone number of some
company some year or graduation
graduation year of some person so this
will have entity involving both the
input part and also the output part so
basically my research worker can be can
pick into least three categories yeah so
for the querying for entities we have
some work about it already content query
system according by an TD pass we have
an engineer's actual document filtering
and the overlapping area we all have the
relation we need is such and in this
talk I will specifically focus on this
area least work and this work the
occurring by natives work in the
querying for entities work for the
relational entities search although we
have some publication but when Radames
they're all actively working on that so
i will probably introduce it in the
future work so this is going to be the
overview of my talk today i will start
with acquiring for any keys and query by
entities and then followed by our future
work yeah it's pretty much the overview
of my talk today so let's start with the
querying for entities part
so the position of this work is here so
in this work we try to design a general
content data oriented content query
system try to support at searching
directly into the document for finding
the relative energies the input will be
some key words Alfred will be some
energy the motivation of this work is
that so nowadays we are actually
witnessing many different kinds of
efforts trying to search entities inside
the text data so for example that type
ID search proposing 2006 try to use them
keywords to fight to dirty return any
case result like what information you
traction people would try to use some
simple pattern and then use some idea
light snow ball to iteratively get a lot
of relational data from the web test
like web based question answering people
given some query ask some question as
input that I want to find some
uncertainty as a result well you can see
that these kinds of different efforts
are all doing similar stuff they will
have different keywords therefore
different patterns try to find out some
entities inside or text data just
belongs to the occurring for entities
applications well since they are this
project I develop independently so these
kind of like keywords or patterns or
scoring function are hard-coded in their
projects and it's their limitation so
one the code with the project with
available before when one project cannot
be used easily in other projects so how
can we saw the motivation of works that
can we do a more general system to
better support is I this kind of
different applications so this is the
goal of our work since they are doing
similar stuff so we want to build a
general system that can better support
this kind of different querying for
entities application for curry for
energies projects
so this might actually much similar to
database so before database is invented
people when they try to do some data
manage to operate they need to write the
data storage data indexing in different
projects and then the invention of dbms
actually largely proceed at their jobs
so similar here we want to try to view a
general system to better support these
kind of different querying for entities
applications so how this system should
look like couple of requirements as you
have mentioned before different projects
actually will have different kind of
like entity extraction and different
kind of scoring function to find out the
correct entities and also they are and
the needle support different entity
types so specifically will require that
in order to pure general system that
should support very flexible anything
instruction there are scoring functions
with customizable for different
applications and the LED types should
also be extensible every time when we
try to support some new entities we
don't need to change the project code we
can directly extend it easily from
England in the system so this is the
purpose of our requirements of our
system so the handle is actually our
solution is kind of valuation model or
the key idea is that we try to view
these kind of keywords and titties is a
lot of tables in the in the coppers in
the back end and then we'll buell a
system beyond that and then we design
language to querida and find the result
as we turn the result is kind of a table
result I know a little bit unclear here
so I want to show you a demo to see how
actually worse
so this is a system that I developer for
this project and here's the interface
this is a this system is filter based on
ponder this system is built based on
wiki video covers and have some like
entity types it is checked it and index
in the caucus and then it support a
query language to carry those kind of
entities for example a person location
in the text data so how do we work for
example I'll a simple query first so for
example I want to find a number the
other person entity from we like to have
some pattern here and then say we want
to find all the person that which you
have the computer science this kind of
no fridge appear close to it then you
will process it and then get other
entities that match these kind of
patterns so this is the idea so we can
light some different kind of patterns
here and then to specify what kind of
entities we want so here use the bracele
means that you see the computer science
is a sequential raised here use the
square bracket i means that is the kind
of window patterns this random know so
in ranking we need to have a something
like a group i order back so yeah we
meet so i can first show how this kind
of entity match the
so you can see I can see the context
function I can see how they match our
patterns so for example I here we get
the passing them and they don't we see
how the computer science it is kind of
word up here around it so this has some
similar patterns to that and then also
you will if one to the support of
waiting we need to order a result so for
example shown your query here so in this
query in connection between Jim who's
covering the signs in two seasons so
yeah so computer science so I just know
a person here so yeah it's no connection
but I just say we won't we'll collect so
this is basically is collecting phase
use the whale plane tsv complex means
that we showed that for example perch in
here is present in here computer science
we find to all the by all the kind of
lap entities the match that would have
this word appearing around yep tending
say their chambers at all yeah it worse
wrong yeah yeah there was a pot will in
10 words actually what's the raw data
not a nice so this is a small a demo so
this demo is just built along the
Wikipedia covers I think is a Tucson and
I Wikipedia covers we all have a larger
data excited we have a larger system
build on last rehearsal user crew web
data there was speaker context is the
document and the person is the title of
the document suppose identity I want to
is checked the fear of example are you
check a lot of ND here from this kind of
course these kind of contacts here
person is that you have to do you
understand right because this is a paper
that read them pies yeah yeah yeah
unfortunately in this journal is like a
monograph seen come the size yeah yeah
so this is no prank result this is just
some just so sleepy then match this
pattern all is fine yeah so if we won't
rank a result we still need to do some
aggregation and ranking so if I dumb hoe
here I would like to find the population
China that maybe our eyes I'm seem
apparent population off and then close
to China and then I can do some group
back with ample because like those kind
of entity
that match up here frequently will be
more likely to become correct without
right so to some aggregation and then we
can do some ranking for these kind of
table constructing and then uses a thank
you yeah yeah yeah it's inside that but
actually in the back end is no table you
just do like working index yeah so in
this way you can also kind of
support like a natural language Isis
also know so this I just say this is a
general system to better support this
kind of project so it's a kind of like
that database natural yep so so what are
the keating acknowledged here here is
how do you get an inverted index or to
you ability check out this on to you
basically the season here yeah so I
think yea in this club actually I didn't
go into the details though tecnica so
but in a paper that key technique is now
for example when this kind of come
pattern becomes very complex what kind
of key inverting index I need to choose
so hurry pearl n and also yeah which
index should be used yeah so these are
general idea so basically try to support
carrying the energy inside the text data
so i see i already show so how you were
good work actually just like for each
kind of occurrences of the keyword or
ending here I just viewed it as a table
things like here we have this position
we have spent we have some some
confidence like major whether these kind
of instruction is correct or not we have
story kind of information in with the
inverting index and then given some so
as I say before we need to fulfill two
requirements like flexible energy
extraction customized scoring function
so how can we do that so basically so
use a relational model for example we
were from Christ to a from class to
specify which and icky we want use some
pattern to filters and result use some
aggregation to equity the result work
frequently mentioned and then some some
customizable scoring function we can use
tutoring the result so basically it's a
general system to support it and another
interesting idea for this work is that I
mention about us not a requirement
extensible entity types so in a lot of
projects when we try to support a new
entities they need to review the index
to a lot of other things well for this
project we use an idea of view click on
the concept of view to the view concept
from database to support extensible data
types so like the example shown here
from we only index the number location
person right but we can use the idea
with them how we specify some pattern to
characterize a number from popular to
defy a population type of from a number
type so i can show example here so here
you see we would defy a population of a
number but we can also divide it as a
view like here we can specify a lot of
different kind of patterns like
inhabitants population of and define
this population is the number by doing
that then go back to the query we can
directly query the population you
actually although these kind of entities
are not indexing or back-end system but
we can still carry that and then get the
same result so this is the kind of that
virtual entity types from the existing
types and the finest hypo actually is
pretty easy we are going to write some
like ballet is some type of here and
then simply save it you will appear in
the left part and then you can directly
querida so yeah so let's define some new
types used entities based on some
existing any keys
yeah so this is a requirements higher
fulfill this kind of requirements in our
system and then the main technique so
yep expectation value if you need any DS
120v by Eddie's entities so it's the
system here so in our system will you
pray is checked some entity and endings
that in the back end example is check
the number location organization
location is all very basic energies so i
define I means like I defy some
high-level energy based on this basic
one yeah yeah and your package to
instructor yeah and also this one this
is this time already show these four
types but in a larger type i use a lot
of dictionary to measure more entity
types yeah so the population you extract
it is it a unity is in origin of the
existing oh you scandal is a new entity
based on the first number descend by a
basic energy the number is pretty
strictly indexed but while the
population is no index at all we just
use this kind of pattern to is checked a
new type to check those kind of numbers
days are more likely to become an entity
yeah so some new Thai entity types based
on the existing pipes existing energies
since I one question so problem
cannot be solved by keyword search and
can be self I this is this demo oh no I
I just say this is just pack a back-end
system to better support those kind of
project I didn't say I solve a new
problem I just pure general system who
has supported so they don't need to
write too much code report there are
projects yeah
okay so this is the first piece of our
work so as I say the men technical focus
on the index design how can we like Bill
the emerging index for those kind of
keyword enter key to make this kind of
carrying faster and also time query
optimization which keywords to be used
what your entities should be searched on
internet yeah so I this pi wouldn't go
into the details so if you have question
we can discuss appliance yeah so
basically this is the first piece of our
work is about caring for entities so our
work is about data or enter content
credit system so the second work i'm
going to introduce this this pot and
this will be the main focus of my talk
today is about entity centric document
filtering the idea here is that we try
to input some entity and then we want to
output the document and then the problem
is that we given an entity which is
characterized by an identification page
how can you identify relevant documents
so indeed eccentric document filtering
so as I have introduced before many kind
of like information retrieval operations
on the web you are searching information
search in documents about particular
entities for example for common people
they might want to know those kind of
ladies inventor for their favorite
celebrities for the million is people
they want to know that we know more
after they released some products they
want to know some feedback or review of
their products for so recently they also
recently proposed the check task is
about knowledge base acceleration they
want to help wikipedia editors to
identify which kind of documents are
relevant for which with some particular
entities and then help have them to
enrich the content of wiki video in a
fast manner so we can see that
kind of scenarios are all about carrying
by entities they want to find document
that related or entities so the goal of
our words try to build a system to
better support this kind of to
facilitate these kind of applications so
in order to purely system what
information should we do we need the
system need to know what kind of entity
we want to write in to know the
information about these entities and we
will say that actually using the N
kingdom is insufficient because if I can
be very ambiguous given Michael Jordan
you can refer to different people so in
our project our idea is that we will try
to directly keep the Sun page that
characterize these energies as input so
for most of the entities in the world
nowadays we all have some pages on the
web characterize this entity for example
for a movie star you have IMDB page for
product we will have the specification
page for academic people where they have
homepage or popular energy will have the
wikipedia page we will have some
descriptive page that characterize this
any key so using these kind of things as
input will have two advantages one first
ever a result ambiguity problem and
second you also provide more information
about each entity we can better have the
system better understand the input curry
in terms of output given some entity we
will try to know what kind of documents
are relevant or irrelevant for its
energy so this is a simple example for
example if la plus 1 this is relevant
well the second one is irrelevant
because I is mainly talking about steve
job instead of Bill Gates and similarly
here for Michael Jordan the first one is
related well a second one is unrelated
because it refers to communal science
people
oh so the input will be a page the
character page we see this yeah yeah
yeahs characterize this yeah but you
know that I should define this one these
kind of relevance or England is pretty
subjective different application
different label even they will have
different criteria so in all work which
I could let that from user labels
instead of like design some defined some
criteria for weather is relevant or not
yeah so these were peeing police will be
the output and this will be the scroll
problem we will study given a query page
out of documents we want to fight which
one is relevant or irrelevant we sell
uses call ng centric document filtering
problem well this will be the problem we
were studying this work so why this
problem is challenging because like the
key challenge of these problem is that
we will have a very noisy and D pages
input the query is very long so let's
see the traditional I are in usually in
traditional information which people
what people will do is like usually they
already use this kind of very short
actual courage Akio Corey only contains
one or two key words or when sentences
and then people have already develop a
lot of information receiver models for
these scenarios I be entered if I
language model vector space model they
all worked pretty well for this kind of
shocked keyword curry however at the
same time also people has also observed
that this kind of information receiver
model usually their performance would be
great when the query becomes longer with
importance consists of like two or three
sentences when the query becomes longer
this kind of I Armada wouldn't work well
well in our problem it becomes more
difficult our query neither becomes a
very long page is not national is not
just a few tendencies it's a very long
document so I in this document
definitely it provide a lot of
information about this any key however
at the same time he also includes a lot
of noisy keywords which may not be tell
we'll list entity so in this case
definitely traditional information
resume or model with the work very well
here so this will be the key challenge
we will focus in this work yeah the
noisy query page as input so how can i
how to stop this instead of directly
proposing some solution as some some
some quick solution we actually start
with investigating the fundamental
principle actually behind document
scoring it's a very pretty much a very
simple question given a query given
document how can we just when the
document is relevant or not to willys
query answer is that we need to check
the contrary basically we need to check
how this document contains keywords that
are important polish query it's very in
straightforward intuitive idea we call
it as a document scoring principle the
relevance of document depends on how it
contains keywords that are 11 for this
query yeah but actually you might not
notice that this kind of very simple the
principal actually contains two caches
first which keywords are actually
important for this query so in this
scenario for example a microstrip the
Bill Gates these kind of keywords will
be very important for this an input
energy right and second question how the
document can how the keywords are
contained in the document so you ready
we will see if this kind of document
contains more important keywords is more
likely to become relevant document so
you actually can be decomposed into two
questions and traditional non learning
scoring model actually for Felix
principal quite well for example IBM 25
language model they actually followed is
kind of principle life example which QR
importantly will assume that those kind
of keywords that are pure a lot in the
query and less in the whole caucus will
be more important keywords those high
inverse document frequency keywords will
be more important keywords
and how they acted then the people
usually this kind of model will count on
frequency in the document like so this
is how traditional non lending scoring
model actually will do that but we will
say that actually this kind of simply
using simply the simple answer for this
question actually is insufficient for
our scenario so different from
traditional information retrieval and
our model becomes all kuru becomes a
long page so simply using IDF is not
enough in order to identify these kind
of important keywords we need to make
use of more other information for
example like we need to see the Kentucky
or how you appear whether you appear in
title or in the info box or in the
opening paragraph we have more
information to leverage to identify
which keywords are important for this
entity and similar here because our
target document is a web page and web
page is also somehow structured will
have a slide title we have the UI oh and
angle tags and also something like a
picture world so they also give a lot of
other information to characterize how
these kind of viewers are contained in
the target document and once we want to
incorporate more information also we
want to incorporate the more information
about these kind of questions then
simply manually fulfilled and print the
principal becomes impractical and
because of that we will sing about is it
possible can we try to learn to answer
these two questions so is very natural
to sing about learning to rank well when
we look at a learning to rank we found
that a Jewish traditional inka actually
doesn't is fail to learn to fulfill its
principal all right nowadays community
the academic community has already
proposed many learning to rank models
link SVM on the mud or limb post a lot
of different kind of learning to rank
models and all these kind of learning to
rank a model actually follow the same
objection they try to learn a ranker for
a query and document and then I will
have some input features to carry two
that and then they try to output the
document elements for that and from this
objection we can say that see that
actually this kind of objection don't
learn to feel all of principle because
in the principal the most important
concept is key words how the keywords
appear in the query and how the key
words are important for the query and
how the keywords are contained in the
document the concept of keywords not
model in here and so how the exact of
learning to rank a model actually score
the document should be key words means
like the maybe I should use the word is
that it's better so it means like the
word in the query each word in this
query and down is kind of word up here
in a document why is not here why isn't
keyword is not also a cure not appear in
the objection this objection so this
model abstraction we only have Korean
document we downloaded on the word then
the concept word here yeah so that i can
say that you can bother that teacher
yeah so that's my point here so in order
to make this kind of system work that I
need to use this kind of feature be
entered if I language model right or
they need to in this official design
they need to characterize how the
document contains keywords from the
query so actually they require the
designer to fulfill the principle in the
future design yeah instead of letting
that so and as we have seen that before
for some scenario manually design these
kind of features manually fulfilling the
principle is difficult as I have claimed
before because Aquarius complex in the
document is also complex then menu
design is feature is difficult so this
kind of traditional learning to rank a
framework just simply leave this button
to the visually the feature designers so
it's not good so that's why my claim
that they didn't learn to fulfill the
principle I simply lipid to the
designers so so the purpose of our work
here is like question
we try to facilitate the feature
designed for people and the first
important concept we proposed here is
called a visual decoupling idea the key
idea is that traditional own feature
design they need to carry describe how
document contains key word there are
important in the query and which is
difficult because like in this feature
did that people need to adjust these two
questions together in the visual design
so our intuition is that actually we can
decouple this feature design into two
types of more elementary features such
that a one type of feature or just one
type of question one question or 10 so
this kind of feature will become much
more easier to be designed so this is a
key inclusion of our work can we try to
decouple the features into two direction
one is for meta feature characterizing
which keywords are important and ensure
feature characterize how the keywords
are content yeah so here is that some
example features for our entity centric
document filtering example of which can
be like for example 0 of each other IDF
can be using the meta picture before the
query side and release how whether this
list keywords and now it's a verb is
mentioning the entity or not I also have
some structural features like the
position of the skewer in this page how
they are mentioned in the infobox they
are all made her feature the query site
features similarly we can defy the intro
features how the document how the
keywords appear inside the document so
final different position in the URL in
the title different position of that in
different representation besides that
simple accounting the term frequency we
can have a log scale TF normalized here
we have different kind of ten frequency
representation characterize how the cure
appear in the document yes so this will
be that you've done for feature with
design for our applications and because
of this decal for the feature we are
land we are facing a new learning to
rank a framework so in the yep
that's essentially the left hand feature
I like curry level each card i will be
dry hand societies hurry URL features a
document preacher talking hurry re that
he won't talk in my future so you can
also put the m25 oh its features in
traditional gym 25 I think you already
combined are the key words inside a
quarry the m25 yeah so it's a summation
all the keywords in santa clara CA i
know i think so the feature met official
would be fw and cute for the correct
word i query and then 4pm 25 is like the
featured if I don't document enquiry
already so this game 25 will correspond
to our traditional learning to rank a
picture for each of the world makes it
as a time for return and some overall
turns it's gonna be a game 25 oh yeah
you can say that but be m25 usually the
sum of over different keywords in the
quarry yeah all the key were seen you
read this is a hard game 25 is designed
yeah so here I just like this one is
designed over one single keyword this
isn't her ELP happy mother okay also to
our party also p pi over one keyword and
a pen I think believe me she's the right
are so mean yeah yeah this area no no
parade not only the key word document or
the keyword Logan how come in here yeah
yeah yeah and that is polished but a car
key word require it I assume if there's
no overlap pitching ladies don't know he
would overlap between left and right and
the NASA hope nothing uh so usually when
we are doing our information the Shiva
tasker we will see how the document can
take you only from the query so with
this keyword a dimension in the car you
would encounter right yeah so yeah
basically we just find overlapping
keywords between the query pot in the
document come yeah so yeah the basic
idea just language I'd rather try to the
cup of the feature did not and then we
will have a new learning to rank of
problem so this setting is still like
traditional including we have some query
and wherever you relevant document for
this query and then given a new document
also characterized by a video page we
want to judge where the document is
relevant or irrelevant by learning this
model this is pretty much similar to a
traditional
2-ranked yep this is one assumption
original intention so yep select is the
key word exists preparing independent of
the documents or none are independent
we'll talk right for the hazy okay the
key ways always pillowcase and micro Sol
even-handed of the conference over what
you are going to dynamically Rikuo ok
sonic even tournament direction oh
actually no yeah I just I check those
kind of Q so keep on a document I
already checked those few words that
mentioning is document and also
appearing the pirate that they are
overlapping you were given a Korean
document i will check their overlapping
keywords okay yeah but yeah definitely
we can extend it for dental given and
curry we can extend more using some sin
ánimo other techniques to find more
keywords related for this query we can
also do that yes i say so p consequence
it acabo feature we all have a new link
ranga framework so in this time for the
query and document pale we described by
two types of features instead of one
future function let people yeah so how
can we stop this kind of the couple
feature based learning to rank a problem
will be the focus of our work since they
are decoupled so we need to recover them
back so let's revisit the documents
going principle little bit no document
depends on how it contains keywords are
important for the query mathematically
we can translate it on here there are
relevance of a document to a query can
become a summation of the contribution
of each keyword for this document for
this query yeah this will be like you a
contribution for the curry and document
at the same time and what I mean Rico
holding is that how can we define this
function based on the meta features and
ensure features this will be the focus
the first requirement in the our
learning to rank of framework and then a
second requirement is that these kind of
function should be noise aware so as I
mentioned before the key challenges that
I'll carry is very noisy so this
contribution function should be aware of
those kind of noisy worse so how can we
so these fulfill these two requirements
the second concept we propose is called
info the sparsity idea the idea is very
straightforward because like this query
is long so there will have a lot of
overlapping keywords and this kind of
know a lot of naughty words are noisy
they may not be really scary and because
of the existence of these kind of noisy
few words if we are just a giant as I
some low value to least no idea he was
kiosk or my accumulated and then as a
result they will child will affect the
final scoring accuracy so because of
this we will require that for those kind
of noisy keywords they should fulfill
the requirement satisfying contribution
this contribution function should be
called equal to zero we call this is
inverse masky what oh yeah this is a
good question actually I will mention
that because we don't have fewer labels
say this is noisy or not right so yes
this is a typical key change I need a
handle in the later solution I winch
dousset person so I first introduced why
this is calling first basket because
that it's pretty related to traditional
smash learning in smash learning people
for in order to select features in order
to improve the accuracy prediction
accuracy they all were required for the
future waiting they won't get those kind
of feature waiting to be very fast only
the important keywords will have nonzero
waiting well on the important ones that
you have zero value we were special
function to become non zero only for
those kind of important keywords so they
actually pretty related but they also
somehow similar other different so for
traditional smart learning alpha here is
a feature waiting it's a free parameter
so they can divide real and make it
spots by using techniques such as a 1
legger addition well here the function
we want to specify use of the tacky want
to specify the function instead of a
free parameter this function actually is
the Defiant based on
our meta feature in the capital features
so traditional technique we don't work
well in our scenario so to achieve that
is in first batch gir a key idea is that
we will use a two-legged scoring model
so basically i will try to learn a qo
classifier to judge whether keywords is
important or not if so here c means i
keep a classifier if we go 0 means less
noisy keyword and then it is important
instead you will have some nonzero value
and based on the output of this keyword
classifier will further judge determine
its contribution based on how you appear
in the target document but try to design
is model is not that easy as you have
vision we don't have fewer labels here
so it's not easy to learn the cuba
classifier here and also how can we
design a feature to determine these two
aspect yeah so how can we solve it our
solution is use a graphical model the
suitable machine model to handle that
the zero ideas led is given a document
we try to use the random variable Y to
denote Weatherly's document is relevant
or not and then you will connect to a
lot of keywords you can net a lot of
keywords right so you for each keyword
will also have a variable they know that
whether these keywords is an important
heuer for this query or not easy if we
go to non zero value that means that is
important if it equals to zero that
means like a skewer is a noisy keyboard
and then we have a couple of men three
type of different factors to determine
how this kind of cure how the crowd
defined a crack you a classifier and
then also how the Constitution should
look like so first we have a qo
classifier right so we will use this
factor to represent that basically
according to our definition of features
for meta feature we use met efficient to
characterize how the fewer I important
for the query or not so here we will use
this factor to incorporate this
information and this part will represent
a contribution of a keyword to the
document so if this is important
keywords then we will further look at it
will further look at how you appear in
hugga document so we measured it here
and if it's a no it's not important
keywords then this kind of keywords
should be limited in diminutive so it
should not continue any Scott with the
final document relevant so we have zero
here so basically it's a Malcolm
negative line and we also have document
factors this part we can try to
incorporate traditional letting to rank
a feature for demo page rank documents
or even beyond 35 vector space model we
can have this kind of auto
traditional features inspector and then
our algorithm just try to stop here so
this podiatry well handles the lack of q
labels problem because we can simply
view this part is a hidden variable and
then based on the document labels only
to learn which keywords should be
important and then also as well as
holidays to consider Scott to the final
document relevance and the key advantage
of this model is that it should
restructure actually so when I try to
solve this problem try to optimize this
model we only we can even use the belief
propagation to efficiently for furious
so this is a key idea
so what I want to collab that's on to
mega summary in all technical there are
two important ideas the first one is
called a fissure decoupling idea try to
decouple official designed such that
people can easier to design the features
and in second is the inverse plastic we
can achieve that we can filter those
kind of noisy keywords and only use the
important one and then we say that
actually this kind of framework is a
very general framework can be applied to
other applications so for example I use
the recommendation system try to
recommend some items for users usually
we can have a similar called item
ranking principle here because like we
can connect the user and item by their
HQ usually when we try to measure
whether this item is good or not for
user right we will see how this item can
how this item match that edge edge view
that these users are interested in right
so they connected by the edge view so
the user he becomes a query HQ becomes
our keywords before and items become to
the document similarly we can have a
locked up meta featured in the
insufficient to characterize these two
links for made official can characterize
how the engl uses light leaks kind of HP
for example some long-term preference
some short-term preferences how he
sprang lightly different kind of feature
characterize how he or she lightly is
kind of different attributes and four
incher feature we can similar have the
intro feature for the item page builder
for example do we have the exact value
for the attribute or we just have some s
key information estimating information
or whether this value is missing or not
we can characterize all these things as
some feature vectors and so I also
investigate my the try to apply on my
worker to domain application framework
so here is a one application we try to
we have lot of reviews coming from some
domain so we have lots of the views that
positive and negative for the most
remote domains we try to learn the model
which can be adapted to the kitchen
appliance told me yeah
and this is the dominant occasion
problem because the different reviews
will have used different keywords for a
book review they will use keywords such
as interesting boring well the kitchen
appliance reveal will use the
high-quality leaking broken they use
different keywords and then previously
how can you match that pretty as
previously one people used to try to use
for this application people propose an
algorithm called a structural
correspondence learning the idea is try
to match these kind of keywords based on
they are mutual information the
correlation with those kind of like
common keywords good or bad so they the
intuition is that similar correlation
will have the similar sentiment here
yeah and then they propose their
solution well what I want to say here is
that actually their solution is pretty
specific to the application well we can
say actually this just made her features
they can be viewed as meta features in
all framework and similarly we can have
intro feature characterize how this
keyword printed document so we can apply
our framework to this kind of scenario
as well so in terms of experimental
actually in my paper submitted to KD
recently I actually try to evaluate our
framework in two types of different
applications one is the entity century
talking a few Turing and secure wines
the domain application idea so basically
user data set try to learn the model
from with some domain and then try to
adapt it to a different domain and then
we compare our solution with many
different kind of bass lines so for
example first the standard learning
algorithm basically just like ug
traditional learning to rank a framework
by using feature we designed for that
for example for NDSU document filtering
I will use a lot of learning to rank of
future beyond 25 because the base model
and then compare you so basically there
are a leech you are the learning crank
that has set in MSR so in that page they
are not officially design so I just
follow that feature desire to design a
lot of traditional learning to
banka features yeah and then beauties
baseline the second one is lies I simply
combine the meta feature in the intro
features by multiple item so much like
the TF IDF like Kiev a given much simply
multiply she have any ideas so in this
baseline I didn't use a la mode or our
graphical model solution ideally
multiply two types of features and the
disadvantage of this is that they didn't
fulfill the inverse busty requirement
they will introduce a lot of noise
contribution because I simply multiply
them together and this solution is
boosting promoted I proposed in my
previous paper and actually my solution
is extended is so my graphical modeler
is tended from this model and also
upload from that and this egg with the
message was really designed for the
domain habitation so yeah so we can see
that we also can achieve that similar
domain adaptation the performance
compared with this can work yep so thus
far as i know that the acop is quite the
ring was very in a lot of experiment on
this thing I said yeah try to compare to
the moment a straight line and then your
neighbor 10 knock me off and the other
base much okay yeah this part I haven't
considered because I I in this paper
especially I just want to show the
possibility of applying this approach to
auto meditation yeah so I didn't yeah
yeah the department look at that yeah
maybe in the later experiment I can try
to use more strong base line for that
yeah but I just I include the comparison
we recommend to this work I just want to
say it can be applied to that scenario
using a modular framework yeah so this
is the instrumental result for our
framework
yeah so in this workout we are studying
acquiring by entities problem and it is
essentially document filtering problem
and then we propose a general framework
to stop that yeah so as the future work
so first we are studying the aquarium by
entities inquiring foreign keys will try
to move step one step forward we try to
start it the overlapping pot and also we
want to try to connect it better back to
the traditional information which your
problem this will be our the direction
of our future work so first for the
occurring by and foreign keys they
overlap in part we are starting a
relation or any research problem
basically idea is that we given some
entities we try to find a kind of other
entities that is related to that
according to the attribute for example
given the Bill Gates we can follow that
his wife is education background or his
founded company have different college
costs for that and learn how this kind
of project as been realized in industry
for example a google they have knowledge
crop and in Microsoft that they have
tutorials this kind of knowledge
spacecraft idea however they are this
kind of output is supported by our back
and knowledge graph so you know so if
the entity is not a popular then if it
is not start in the knowledge graph then
you cannot be showing a result so in our
future work actually what we are
studying is that we try to appeal a lot
of relational searcher for that we try
to do it online the relation extraction
for this kind of inquiry for example we
will learn a lot of searcher one soldier
is ultimately finding the people the
wife of a person and some education we
have different calculation specially
designed for this person for this type
of entities and then we can try to rank
the entity according to this kind of
little results yeah based on how these
kind of entity or pure in the in the web
document
so as the cover in progress we have a
sub-module lens a model try to learn
these kind of lencurp based on some data
from the knowledge base with a more free
base we use a lot of is check a lot of
relational data here and then use that
to learn this kind of searcher and this
is already been published however there
are still a lot of unsolved problems so
this is what we are working on right now
so for example in the previous work we
only work on one relation at a time so
randall tyree will try to Hulk we knew
it to efficiently search multiple
relation together and to both improve
the efficiency accuracy and also how can
we result identity ambiguity so given I
in pokkiri so this can be a long story
in person so how can we search their
relations to discriminate different kind
of entities so this will be the first
generation of our relation any research
and the other future work I'm working on
right now it's about the NT intended
passage scoring so basically we try to
better understand the entity concept
heater inside a quarry and also we want
to search passage that's the pot this
kind of question answering so the
problem is how can we rank the passage
according to the impressively the entity
in 10 hidden inside a query so this is
also a problem at right now I'm started
working on actually yeah so this is my
work I basically focused mainly focus on
an entity centric search and I also have
some other relation work on the some sun
protection perdition and also on
socialization so if you're interesting
other direction we can also discuss it
offline yeah thank you we have time for
questions guess a lot of work that we
also do probably all you want more
in-depth discussion when you're meeting
with the candidates but if there is any
question quick question how do you
envision this inner product example
mention something soon in value of which
p which work at you know I'm just
creating bring these foreign keys
evaluate that not evaluate but how do
you envision it working in product
rankings for the leg okay so for example
like the first piece of work I say this
i dosing is a kind of i propose a new
problem i just propose a new platform a
pack and prac form that can be used to
help other different kinds of carrying
four entities applications so i'm sure
so if we double wing with these are
different current projects and we can
easily for them just lie some similar i
set aside to fulfill a lot of different
kind of difficult relations before yeah
so this can be used as a back-end
support for different kind of
application online application which
which we can use Excel to can be
language yeah yeah so for example
actually one sample application can be
simply and it is such like I simply
define our so we can still use a keyword
query but this can be translated not to
be translated into our current is that
query or design query and then try to
return result so it's build a based on
our previous but vida mia Oh platform
hundred thousand people oh I just rang
the result the second one maybe not good
just like the most correctly so yeah
they because I they also mentioned my
inhabitants based on these kind of
patterns I designed and made why in
China okay I think it's because like
mention about Sonny mr Mackey for in the
city so I feel a lot of Arrakis yeah
based on some point instructions yeah we
can define design some application on
top of a general practice and for all
the second piece of work so I think this
it can be very useful as I say for those
kind of motivating scenario i mentioned
here trying to find document relevant
more that and it's also easy to get the
home page or descriptive and some text
data for anything right so you can be
used to improve the accuracy fold the
pot right so if there's no further
question that we thank the speakers</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>