<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Freeform 3D Interactions Beyond the Living Room | Coder Coacher - Coaching Coders</title><meta content="Freeform 3D Interactions Beyond the Living Room - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Freeform 3D Interactions Beyond the Living Room</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/QxMbGI60iJE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
good afternoon I hope you had a great
lunch break and some rest for the second
round of talks today I'm David Kim from
the interactive 3d technologies group
and I work amongst many others at
Microsoft on natural interaction and 3d
technologies and today I'd like to talk
about our most recent efforts in taking
3d natural interaction out of the living
room so when we think about it the games
Indra see always has been the major
driving force in pushing the state of
the art of human-computer interfaces and
actually bringing these to our homes all
the way to the living room and that's
not just because of the very competitive
nature of the industry but it's because
of the user the industry needs to
constantly reinvent itself to keep the
gamers entertains bound immersed into
the overall gaming experience and
immersion is one of the key aspects of
modern games and we achieve this through
realistic 3d graphics through clever
gameplay and the way we mimic and
incorporate parts of our physical world
into the games so a joystick steering
wheels acceleration based motion
controllers there are always to mimic
the parts of our physical world and the
way we interact with in our real world
and but connect has been a major game
changer that decoupled the user from a
single point of input and let games
observe what the user expresses with
their bodies in previous years we worked
on a number of projects that attempt to
further blurred lines between the real
and virtual with Kinect fusion for
example we gave connect legs human legs
and lets the the deaf sensor freely move
around and understand the complex
geometry of our world we also worked on
an augmented projector that combines
input and output in a single
Enza and that digitally augments
physical surfaces anywhere and hollow
desk it's a miniature holodeck that
allows users to literally use their bare
fingers to push pixels around they're
floating in mid-air okay so now i will
show you few example video clips that
actually shows what this system can do
so connect fusion is able to reconstruct
the geometry of the world tracked itself
but it also changes these changes
happening in the environment and links
physical and virtual objects together so
here the user still interacts with a
real teapot but with a virtual
counterpart attached to it so you can
imagine what is possible in the future
of gaming and because we now know the 3d
geometry of the environment we can just
simply let users draw and interact with
like normal services like what the table
the wall and ordinary surfaces and just
turn any object to a multi touch sensor
and both the scene and the user there
the tracks simultaneously so that allows
the user to digitally interact within
their own physical space but with
objects that purely virtual
and with the augmented projector we also
decoupled the output from a single point
in the room and gave it again human legs
which allows us to turn any surface
highly interactive so here we use a
coaxial infrared camera for input
sensing motion sensors and a pico
projector in a single device and
demonstrate direct interactions with
shadows and here the hand itself becomes
a interactive menu which makes object
selection intuitive and in another
application we show that the finger 10
turn into a pointer and a marker that
lets you present an adult aside slides
in any room and in any surface so we can
also make paintings graffiti stick on
specific physical locations that can be
revisited later so they just remember
where they are and the projector itself
becomes kind of a digital / light that
reveals hidden information on on
surfaces in halle desk we allow a user
not wearing any data gloves or even
digital glasses to interact with virtual
objects in the same space space as if
they would coexist in the physical world
so the user can just touch and juggle
with virtual balls and with other other
objects but we are not limited to
interacting with our hands we just
simply remove the glass barrier
separating the pixels from the atoms
that we can use like ordinary physical
objects to interact with them and lastly
we demonstrate the use of the system as
a virtual prototyping tool so it gives
the user an instant sense of the size
interaction and the look and feel of an
object that is physically non-existent
yet
so what common themes that has driven
our research has been looking at
utilizing the primary tool we have to
interact and manipulate in our physical
world our hands our goal is to unlock
the expressive power of our hands and
create a more natural way of interacting
with user interfaces so I process the
Microsoft labs around the world we have
investigated different strategies for
sensing touch and enabling new forms of
interaction using camera based
technologies and we enabled more
realistic surface interactions by better
understanding the underlying physics in
real physics based interactions and we
made existing devices smart and
context-aware through clever placement
of proximity sensors and capacitive
sensors and applying machine learning
algorithms and we also enabled a free
hand natural interaction on and above
the surface through sensing through the
display and since the advent of our
in-house deaf sends our connect it's now
possible to intact on any arbitrary
surface we can directly interact with
virtual objects as if they would coexist
in our physical world with Windows 8 we
are clearly sending out the message that
we are committed in making desktop
interactions feel more natural and
seamless and disagreeably presented a
number of works around machine learning
and keyboard hardware to bring that kind
of interactions to our desktop space but
they're going forward we want that kind
of interactions this personal computing
experience also in our everyday lives
whenever and wherever we are so across
labs we worked on our body 1 sensors
that can to some degree recognize
certain hand poses or even arm sense
touch through our own skin and we also
made an effort to take 3d national
interactions away from the living room
and two more mobile scenarios and other
scenarios so in the following two parts
of the presentation I'll talk you
through our most recent sensors for 3d
natural interaction digits and flood
light so digits is a response sensor
that captures the full 3d pose of the
hand without any external sensing
infrastructure or instrumenting the the
hand itself and it enables both discrete
gestures and continues hand interactions
on the go so it could be used for sign
language detection for example and so
the design goals in this work were to
have a continuous real-time 3 3d hand
post recovery with a self-contained
mobile device that doesn't require any
instrumentation of the hand itself and
this is intended for mobile use so we
had to consider aspects like the
computational cost power consumption the
form factor and that made unfortunately
the use of a deficit sensor like a
Kinect prohibitive it's a big for you so
so instead we have off-the-shelf
components that capture only that data
that we absolutely need to with a
low-power infrared laser around the
wrist a infrared camera that captures
the inner side of the palm diffuse
infrared LEDs around the camera and an
motion-sensing inertial measurement unit
so as you can see here the laser
projects the horizontal line across all
thing us you see there's a clear
correlation between the finger band and
whether laser projection lens on the
finger and to get the exact location of
the projection we first have to
eliminate the infrared contribution
coming from the background from like
ceiling lighting and from the Sun and to
do that we quickly capture images with
and without our infrared illumination
and perform the background subtraction
step which gives us the clear like image
of our own illumination and then the
next step is to segment these points and
track them and finally we triangulate
the 3d location of these are based on
the cameras intrinsic parameters and the
calibrated location of the laser line
so the laser lines occasionally merge
however when the fingers are pressed
together so to address this issue we had
proposed a digit separation and
algorithm step which uses the diffuse
limit lit image so not the laser
projected image to detect boundaries
between fingers so H at each concavity
of the hand we trace multiple vertical
path along the fingers and pick the best
one using an dynamic programming
approach so at this point we have a very
sparse set of data with only five
distance measurements so only one
distance measurement finger and from
this we want to recover it naturally
looking hand one from this to it is it
possible so just some basics about
fingers a finger has three joints are
simply abbreviated as the dip pip and
mcp joints and we want to know the
angles to recover the overall handshape
and we have to infer all this from a
single distance measurement per hand but
scientific papers on biomechanical
property fest properties has shown have
shown that all three joints are
interrelated when we are performing a
like natural fist flexing gesture like
shown in the video so in our initial can
forward kinematics model of the hand we
ran a simulation where we proportionally
articulated all finger joints together
as we would do an initial sex motion and
simulated the laser distance measurement
for the hand so the laser line the red
line shows where the laser would land on
the simulated finger and then we map
this behavior in a graph and
approximated this further in a
polynomial function which gives us a
summary of the mathematical properties
of this behavior which outputs a single
joint angle for a given laser distance
measurement but because there's an
interrelationship between all those
joints we can simply mathematically
derived remaining two finger joints so
yeah we make a really strong assumption
about how you are flexing your hand but
this often holds true when we are
gesturing we don't make sudden weird
movements they are all natural and fluid
but with this approach we can already
sense discrete dentures hand gestures
and continuous movements so imagine you
are just walking around in your house
and then it detects certain hand
gestures that would be could be mapped
to certain functions with your mobile
device for example and the recency even
allows you to comfortably hang down your
hand or leave it on the surface for more
subtle interactions for example in
public areas so well this is simple to
implement it does have some limitations
for example our kinematics model starts
to deviate from the true hand pose when
we for example keep the knuckle joint
straight but only try to move the upper
two joints so a slightly less natural
way of flexing the finger and it also
holds true when we do it the other way
around and to alleviate this we came up
with another inverse kinematics model
which is slightly more elaborate and an
approach extract to the fingertips only
based on the infrared images that we
already capture so our 2d fingertip
extraction uses the same diffusely lid
image that we use for the digit
separation and association step and you
can see here that the fingertips the
round areas clearly stand out in the
image so we just need to find a way to
computationally extract them and so in
this processing pipeline we first
generate a depth estimate of each pixel
by modeling the light fall-off by seeing
how light at units over the distance and
then from that we generate a normal map
that reveals a depth invariant image
features and then from that we can find
any detect the figurative through simple
template matching and filtering and so
at this point we have additional 2d
fingertips and
and measurements from a single hand so
what do we do with this so in inverse
kinematics angles of the joints in a
kinematic chain they can be arrived
given the end effectors here so the the
knuckle joint and the fingertip however
we only have one of the anti factors
which is the knuckle and the other one
the fingertip is just the 2d projection
of the thing a tip so we don't exactly
know the exact 3d location so the 2d
fingertip could be anywhere along this
rate that you see here so we have an
optimization problem here given the
laser distance measurement and a 2d
position that can lie anywhere along the
way to derive to the joint angles from
this and we also enable all the joints
to be articulated independently so it's
like a highly complex model but we came
up with a objective function with
basically minimizes the distance of the
simulated laser distance measurement and
observed laser distance measurement and
the absurd fingertip and the arm
simulator fingertip in a very in a
brute-force manner but with a lookup
table which makes it really fast in
runtime so this gives us even higher
degrees of em for sensing with wider
range of hand poses steffa that are
difficult to predict with our simpler
kinematics model so now I'd like to show
you what these kinds of interactions
enables in for example real-world
applications so one interesting
interaction possibility digis enables is
3d spatial interaction around the mobile
device that is seamlessly integrate with
touch input so here the user start
semantic zooming by performing a pinch
church and country with you continuously
controls the zoom factor and another
gesture can be used to like pan the
interface or like manipulating object
after it has been selected to spur
through touch
another interesting design space is I
screen to action where the where there
is no visual feedback required so in
this example mobile device is left in
the pocket and the user just performs a
gesture to turn on the radio and mimics
the turning of the dial to change the
channel of the radio and changes the
volume by moving an invisible slider up
and down and the finally answers the
call was just simply doing a thumbs up
gesture and that they just going also be
used for fine-grained 3d interactions
for example as a variable gaming
controller in this example the user
mimics the shape of a gun with his
finger and shoots at targets and
navigates within a virtual environment
by just performing a grabbing just
removing the arm around so this is
currently a purely explorative research
project and I've to admit it it's a bit
chunky to be used in a daily basis it's
like sticks in your clothing and stuff
so so we experiment with I experimented
with other camera sensors with embedded
processing and found that the tiny
infrared camera that is also used in the
wii remote control that tracks infra red
blobs over time and sends it by
bluetooth to another computing device so
this already shows the potential of this
technology to be more in a shape of the
wristwatch in future so our latest
system is called flatline which was
developed for 3d salute sensing for
high-precision input on and above
physical surfaces so as you can see it
on the slides there already many ways to
interact more naturally on a pc with
touch on surfaces with the stylus for
more precise input with a with attract
tangible objects and also since connect
leap motion and similar senses also in
3d 3d in midair however we need separate
sensors for all these modalities and if
we want to combine them together it's
hard they don't play well together and
all these things that need to be sensors
need to be calibrated to each other so
the question is is it possible to
combine all these modalities so we're
good for example use a depth sensor like
connect or similar senses but they don't
offer the necessary precision as they
were developed with tracking cause body
motions in mind and a 3d motion
controller likely leap motion it they
are quite precise but they are only
limited to tracking fingertips that are
in midair so motivated by this problem
we accept the challenge and we developed
a flat line so flat light it's a single
sensor for robust pressure enabled touch
stylus tangible objects and media
interaction and here we demonstrate the
manipulation of virtual objects using on
surface and as well as media gestures
and we can also use custom physical
widgets like this color wheel to control
parts of the user interface and we also
allow for more precise input with pen or
any other pensioner please so by letting
users seamlessly switch between pen
touch and freehand 3d interactions as
well as the use of physical interaction
widgets so that like of us a really
extremely rich interactions in a variety
of styles within a single package so
here a little trackpad is used to
control the camera of the scene
and the hardware setup itself is fairly
simple it's cheap and it's fairly easy
to reproduce so here we use a pair of
infrared cameras diffuse infrared LEDs
and off the shelf retro reflective
material a material that can be found on
sports wear or like on bicycles and this
material just aids the segmentation of
objects by enhancing the contrast in the
camera images for objects as a position
between the camera on the surface so the
property of the material is that light
coming from other sources that will
simply bounce back and only light shot
from the direction of the camera will
bounce back to the camera so so with
this simple trick of with optics we can
keep all the smarts in the camera and
leave the whole interaction and stand
touch area are free from any electronics
so that reflective material is also
tracked and we can sense 3d contours of
the hands above the surface we can also
extract the fingertips as well as
they're pointing directions and we can
also extend this mechanism to even
detect pen and other pen shaped objects
and when tracking hands and fingers
slightly eyes also offers touch
detection with submillimeter accuracy
and as well as information about the
distance of the the fingers relative to
the surface it's interacting with so it
gives us have estate and we can also
reconstruct the full baths map from the
3d contours of of the tract objects for
example for more sophisticated
applications so this is the image
processing pipeline in motion so we see
two rectified images from the stereo
cameras this is the all the input signal
we get from from the flood light system
and the objects placed on top of the
retroreflector they can easily segment
it by masking out the convex hull of the
reflective material and once the objects
are segmented from the background well
novel stereo algorithm efficiently
recovers the deaf
from these conscious and which we
visualize here in false color mapping
and we can also interpolate the depths
between the contour points so the 3d
motion tracking is very robust and fast
only because our images are virtually
already segmented and binary us we have
a Kia segmentation and in our processing
pipeline we simply trace the contour of
the objects and don't use the whole
image so we don't use all the megapixels
of the camera images but only use the
counter points to extract the depth and
this simply removes the need to do a
search over the whole image and make
this approach unique and very fast and
the country path is also used for some
post processing steps for example
smoothing and filling in some basing
measurements so in detail the stereo
matching algorithm works like this so we
extract the horizontal disparity of a
certain contour point as observed from
the left and right camera so we have the
concert here we have one from the left
and from from the right camera and we
look for the disparity the difference
between the two images in parallel for
all the rows and as there can be many
other control points in this in this
single row for examples to counter
points for a single finger and contour
points for five fingers we have to tell
for their compatibility to make sure
that we are observing the same point on
this relate from the two images and so
we the compatibility is tested just
based on the direction of the concert
point and their location relative to the
contoured a sit-in and this is a
combinatorial problem and we solve it
again with a dynamic programming
approach which gives us the best
solution for all country points along
the row so in the diagram right we see
the contour points order it by their
eggs
direction in the x-axis and the control
points from the right image in the
y-axis and with this cost metrics we can
simply trace the path with the lowest
cost and find the best combination of
matching the left and right come to
images together so the precision we get
from the flight so that's defamation
also enables in a variety of features
for example with a malleable reflective
surface we can detect hover touch as
well as pressure information so the blue
column you see is the amount of pressure
you put in and we can also use
flashlights a to detect and track pen or
pen drag objects for like drawing
applications and this enables
appreciation quite similar to comment
graphics tablets and the robust tracking
also extends through physical objects in
3d space allowing us to directly
manipulate virtual objects that that are
mapped to physical tokens so to get a
better sense of the performance of blood
lab with real users we conducted a
comparative study with to evaluate 3d
the 3d pointing task with other
state-of-the-art commercially available
system so we compared floodlight to
connect with floodlights fingertip
detection pipeline which we call connect
light and to leap motion and 3d gear
system which also utilizes the kinect to
detect certain hand poses and we
conducted to study with 12 participants
and we had 3888 selections in total and
maybe not that surprising hip fly fly
performed the best we crying only 1.95
by 1.5 5 centimeters approximately the
width of the finger for the minimum
target size which is shown here on the
left and black
closely followed by connect light and
that leap motion and three gear system
which requires a minimum target size of
5.3 by 3.15 centimeters and then we also
compared the touch accuracy in a sec
separate second study and we asked
participating participants to touch 312
grid points on a plane and here we came
up with a new physical setup where we
enable touch sensing even for the Kinect
and Alec motions which were originally
not intended for touch sensing so we
used a infrared absorbent paint a sheet
for the leap motion as you can see here
on the bottom picture and a transparent
acrylic for the kinect set up with laser
edged touch points and this study also
yielded similar results to the first
study connect flashlight ranked first
followed by connect light at the end of
motion and three gear systems so our
satellite is a very flexible sensor
platform and there are so many ways to
avenues we can follow in our research in
a future research and as I said before
all the smarts and suchlike are in the
camera itself so we could allow users to
simply augment any existing physical
device with touch and proximity
awareness and we can let users to simply
cut and shape a virtually infinite
number of input devices like a 3d
multi-touch mouse a 3d trackpad sends us
trip touch pads buttons and more and in
an experiment we also found that common
printing is transparent in the infrared
spectrum so they don't show up in the
input images so we can simply visually
augment the reflectors with a custom
user interface so the computation part
of flat side is fairly lightweight which
means that the whole system could be
ported to a mobile device like a tablet
and mobile phone and with 3d input user
user interaction wouldn't be any more
limited to the physical size of the
device itself we can just use the whole
3d space around the user and again we
did some initial experiments and found
that the camera and the built-in LED
flash and mobile devices already give us
a very strong signal so both videos were
taken simultaneously and here we can see
on the right that the retro reflective
material only appears as a strong signal
on the camera with the LED flash turned
on and in the other camera we don't even
see the LED flash turned on so the
signal is strong enough for the sensor
to pick up but it wouldn't disturb or
bland the user from like normally int
acting in the same space and we very
recently we got hold of the phone with a
stereo camera and LED flash very close
to the lenses and so we again did a
quick experiment and found that in the
two MA images the reflective material
again sticks out fairly brightly so we
can juice for example in a mobile use we
could have a smartphone which we carry
around along with some other DIY widgets
and like malleable sheets that you put
in your pocket and you could have access
to Street in Section anywhere but
potentially we don't even require a
stereo camera phones we also
investigated using utilizing only a
single mobile phone in satellites we
could use small mirrors and a red green
blue splitting prism that is commonly
used in the 3lcd projectors and and
cameras and we could just encode the
left and right images of the scene into
the two different color channels in the
RGB image and we wouldn't sacrifice on
the field of view or the number of
pixels to capture left and right images
with a single sensor ok so now I'd like
to conclude this top with a fairly
simple sound
question what is the future of tests of
interaction we don't know it yet but
we've tried to hook up flat light to
windows 8 anyway so injections work
seamlessly with all multi touch enabled
apps so this is Bing Maps on in Internet
Explorer and we could just use normal
multi-touch interactions on it for
smooth navigation but we can also use
MIDI air gestures for for gaming
scenarios for example and with a printed
keyboard layout on top of the perfecter
we can also type on it if we really want
to if we want to keep our hands on on
this interactive area and then after
finished typing we can simply switch
back to multi touch mode and continue
with our natural interactions and
finally we let users reach for and grasp
objects in a virtual environment that
mimic the behavior of our own physical
world so will any of this really take
off I think we'll have to see and and
wait and with that I'll finish my talk
and take your questions
any questions yep at the back there is
it conceivable that the thing that is on
the desk that you're interacting with
could actually also be a screen so that
you could projective things up like that
keyboard or some other stuff yeah so
what really interesting thing about the
retroreflector is that it is highly
angle dependent so it's really good for
the sensor but for the output for the
output it would be really bad but
there's one way to have a really good
output and that's by putting a pickle
projector next to do I so for example by
having to pico projectors like next to
your eyes you could have stereo 3d
output without like having anything in
front of your eyes just by putting the
projectors on the other I either side of
their the ice so that could be used but
we can also use a normal LCD to use
similar kind of interactions I don't
know if you've noticed but you can use a
polarizing filter like the one in 3d
cinema glasses and then pointed at an
LCD display and then the LCD just turns
black you don't see any light coming
from the display but the hand and so we
could use a blast screen and use cameras
with these polarizing filters on top and
then could have again have really clean
segmentation of the hands itself so we
can so the street e contour sensing is
not limited to the reflective materials
but there are so many ways to get a
similar result with other ways of
segmenting the foreground from the
background ok any of the questions very
good thanks again really appreciate that</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>