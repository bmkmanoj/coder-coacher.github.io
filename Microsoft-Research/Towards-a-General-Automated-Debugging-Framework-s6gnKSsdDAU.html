<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Towards a General Automated Debugging Framework | Coder Coacher - Coaching Coders</title><meta content="Towards a General Automated Debugging Framework - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Towards a General Automated Debugging Framework</b></h2><h5 class="post__date">2016-07-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s6gnKSsdDAU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright it's my pleasure today to
welcome to of visitors from the
University of Illinois suru and Vikram
are both here for today and tomorrow and
so if there are other people who aren't
on the schedule want to talk was a
little bit of time but it's my pleasure
to welcome sir OOP who's going to be
giving the talk today about some work
he's been doing on debugging frameworks
thank you thank you good afternoon
everyone so the title of my talk is
towards developing a automated divining
framework and using automated software
fault localization by filtering likely
invariants this work was done along with
my colleagues become at the John
Criswell and says giggle at University
of Illinois yeah so the broad goal goal
of our work is to develop some kind of
automated or semi-automated system to
help programmers developers fixed box
and let me give some motivation for our
for our work according to an East report
software failures cost nearly sixty
billion dollars of losses every year and
large applications contain so widely
used application contained large number
of bugs for example Mozilla they get
nearly 300 bucks every day so then it's
so some kind of fell to roll it triage
and diagnose this box and is also known
that cost of fixing box increases
exponentially as the software
development lifecycle progresses so bug
fixing costs are very high during
operational or maintenance phase so it
is very important to fix quickly fix as
many bugs as possible before the
application ships and gets deployed and
the process of dividing involves three
key steps first is reproducing the
failure then trying to locate and
understand the root cause that is
responsible for the failure and then
finally trying to fix the root cause and
divine is complex job for many reasons
reproducing the failures may be very
difficult in many cases and the point of
failure may be very far off from the
root cause and that kind of complicates
the process of debugging and hence
divine is mostly a manual and
time-consuming process now and automatic
fault localization which mainly focuses
on the second step and it can
automatically identify the root causes
or the program's statements that is
responsible for for a failure and it can
also extract other valuable information
that may help the programmer debug or
find or fix the bug and automatic fault
localization can reduce the development
costs and time significantly so the
broad over Olga our broad of overall
goal is to develop some kind of
automatic or semi-automatic system to
help programmers fix bugs and one one
great example where such a system can be
applicable is doing software testing
doing any kind of automatic software
testing which takes as input programmed
test inputs and expected output and
produces failing tests so they have all
the ingredients that is required for a
automated divine framework specifically
all the failing tests have some kind of
Oracle to say whether which can detect
whether some input actually for the
program fails and some inputs or not and
using all this information an automated
debugging tool can try to point out
possible locations of root causes and
some other also informations like 40
program values faulty execution paths
and the cause effect chains which
produces the symptom and all this
information can be greatly valuable for
fixing and debugging and fixing bugs in
particular currently we have worked on
actually trying to localize the fall
in one of our recent as plus papers we
developed the automatic system to
identify the root causes of the failures
and it is very scale it is scalable and
it reports very few false positive and
our tool takes program and faulty input
as the input and tries to produce the 40
locations in the program in somewhere it
also provides some other valuable
informations available information
though we do not yet present them out
put them in a presentable way ok so our
technique is based on many basic
techniques one of them is the pop
well-known popular Delta driven strategy
which tries to compare memory states of
two different runs to isolate the root
causes however is pretty expensive to do
that and likely program invariance can
be a a very efficient way to compare to
summarize and compare memory states of
different runs and what are likely
programming means they are program
properties which are observed to hold
true for some set of successful runs but
unlike sound invariance they may not
hold true for all possible future runs
for example we can say return value of
some function is always positive or some
stored value is between 0 and 100 or is
all some load value is always 10 these
are some kind of some examples of likely
invariance and although like all the
likely invents that fail during a during
the failure failing run shell can give
us a set of candidate root causes but
even if after this step we we have we
needs two lots of improvement 24
actually effective for localization
so the key is some important
contributions of our work was that we
used a novel mechanism to train in bands
in particular we use automatically
generated similar input which are close
to the felling input to generate this
invariance and we combined our approach
with the dynamic slicing dynamic
backward slicing and software and we use
two noble heuristics for reducing the
false positives further and we use many
bugs in four large application like
squid Apache MySQL and Clank for
evaluation and we got around five to
twenty eight locations as candid root
causes even for programs of 100,000 to 1
million lines of code and after that we
applied some trivial manual manual
filtering steps which gave us only 2 to
14 program locations and in many cases
we had only two to four locations so the
results were excellent so yeah sorry
what does it all mean it doesn't mean
that you can change these
yes kind of that okay route I guess
formally route because there any kind of
any program statements which are
responsible for the failure but in for
evaluation purposes we saw the patch
which was which were fixed which will
which statements were change to fix the
failure so for evaluation we use only
those statements we call those
statements are root causes and I guess
yeah yeah somewhere I a multi-threaded
though we don't have any concurrence
about sex some of them are multivariate
your final enforcer province nice or
several statements I'll tell maybe I
will show the statements here some
results as show yeah okay okay so i gave
the motivation and contribute
contribution of our work and now i will
be talking about some problems with the
existing fault localization work which
we try to address and after that i will
give details of our bug densities
framework then i will give some key
experimental results then i will talk
about some of the future work we plan to
do towards developing a usable double
automatic divil debugging tool okay so
before going further i will give some
definitions which i just talked about
the definition so we define all the
faulty program statements that are
responsible for the failure as root
cause of the software failure and for
experimental evaluation purpose all the
modified statements in the patch we call
them as the locations of two rows root
cause of software and all the candidate
root causes which are not the true
locations of the root cause they are
called false positives yeah timber if i
try locations and you find one of them
there a success or you know where and
all then yeah yeah but only only out of
them which are really actually sometimes
they actually try to fix other things
and somewhat irrelevant statement so so
if the if it finds out all the relevant
statements that needs to be changed then
we call it success yeah
but let's find all of them so there is a
lot of a lot of work on automatic fault
localization and we have classified them
into six categories here and I will talk
about only two of first two which are
more most relevant to our work and if I
knew you were interested in I can talk
about anything else so so Delta living
is a very popular work 24 Faldo
coloration where is a smart approach for
which compares different memory states
of different runs but doesn't scale well
and there have been some improvements to
Delta leben so that can handle larger
applications and this aims to find for a
cause-effect chance but in many cases
still in 55 five percent of the cases
can still miss the root cause and as it
said in science is a good weight is a
compact and precise way to compare
different runs but most of the previous
work sorry I think all of the previous
work have many issues first the test
inputs they use to train the invariants
may not always be available and coverage
of test inputs is often low for training
and they do not have any solution to
make the likely invince narrower tighter
so what so when the invents become very
broad so the root it may miss the root
cause so some of the key insights for of
our work which try to improve upon the
previous work was we use like Lee
invents as a compact besides way to
summarize and compare different runs and
in this way we can quickly isolate the
difference in behavior and and and give
the programmer an initial set of
candidate set of root causes and we
instead of using general test
Oates we automatically generated similar
close korean posts to train the
invariants and because of this we can
now use very few clothes good inputs to
turn the invariants and because of this
we get much more tighter and relevant
invariance so we have very few false
positives that means we do not mean
sorry so false negatives that means we
do not miss many root causes but this
may result in many false positives and
hence we developed a sequence of novell
filtering techniques to reduce this
false positive set but much smaller set
ok now let me give some more details
about a buck dang sis framework so this
is the overall architecture of our to
our tool takes program the original bad
input and an optional input
specification and then uses them to try
to generate many similar inputs ok men
similar good inputs and this good inputs
are used to generate the invariants and
this invents have been instrumented back
into the program then it is run with the
bad input and after that all the failed
invariance will give us the initial set
of candidate root causes then we apply a
set of false positive filters to reduce
this set of initial candidate to a much
smaller set in particular we apply three
filtering steps first one is dynamic
backward slicing second one is
dependence filtering and third one is
multiple faulty input filtering and I
will talk about them in little more
detail later so let me give a concrete
example to explain some of the concepts
later on so this is a bug from mysql and
this bug happens when an SQL query you
just say
specific debt filled with your value as
0 and this causes a segmentation fault
in MySQL okay and the segmentation fault
happens at line 7 okay when the weekday
value because it becomes negative and
this results in a buffer overflow but
actual root cause actually is starts in
line at line 3 we're in unsigned your
value is used and because of this when
here is 0 Y minus minus becomes a very
large value instead of minus 1 and this
value propagates through various
intermediate values and then goes to the
dnr value then DNR is used and this
value propagates to it day and finally
week there becomes negative and it
results in segmentation fault okay what
I showed is a kind of simplified version
of the code actually it is this code is
split across three different functions
and this is where the buffer overflow
occurs at line 16 and there is a
function which computes the weekday
value i showed in the previous slide and
there is an the function which computes
that dana value i showed earlier and the
faulty values flows through the green
arrows here and i will use this example
to illustrate some concepts later on now
diagnosis with invariance so in this
work we use likely range invents to find
potential root causes and what is likely
ranging than likely range invents a
range of values which are computed by
individual instructions in the program
in the correct runs and when these
indents get violated during the faulty
run they give us the set of initial
candid locations currently we have
invents only on load values Torvill
lives and and the function return value
since they are the most critical
locations these are some of the examples
of the imbalance kind of
so here this is a so return value of
weekday is between 0 &amp;amp; 6 &amp;amp; here some
load value is always positive and here
store value is always 100 so these are
some kind some of the example in brands
we can we kind of use in the source code
example we have invents on the return
value of this to which day and Dana
functions in line 9 and 12 and here the
invite influence where return value is
always positive and these actually fell
during the faulty run and they gave a
give us kind of initial set of candid
locations one important point to note
here is that we are trying to we are not
actually trying to observe invention
intermediate program values like for
example let us say it Dell some any or
temporary value here so the bog maybe
actually anywhere in those intermediate
values which feed value to the function
return values or the invariant
instructions so when we present the
result we give all these statements also
we output all these intermediate
statements also we give them to the
programmer since the bog may be anywhere
in the expression and we call this
actually expression tree of the
invariants other return value as I said
earlier we train invents using very
similar inputs which are close to the
failing input and which this in this way
we can capture the key relevant
differences between different runs and
because of this we have we use very few
inputs to train the advance we get much
more tighter and relevant invariants and
we are less likely to miss the root
causes though it may result in many
false positives and we have many for
false furtive filters for them I will
briefly talk about how we construct
inputs and one important point is this
may not be the best way to actually
generate inputs tho tho this techniques
work and one of the key regions of
we visited here is that we want to
really would like to collaborate with
kind of testing teams like says INPEX
who are doing dynamic symbolic execution
and and fro other tools and and using
them probably we can have much more
system we can generate the input in a
much more systematic manner currently we
have three approaches to generate inputs
one is delicious with specification
independent approach which is kind of
variation of the well-known determine
algorithm and where we try to apply
character label deletion and and the
second approach is replacement based
specification dependent approach and for
this actually we need some kind of info
specification like which specifies what
are the lexical tokens that are present
that can present the input and for each
token what are the alternative set of
tokens that we can replace them with so
depending upon the token type we try to
create many variations of each token and
then we combine one token at a time to
create the inputs and the third one was
for compiler box we use AC reduce based
approach so she reduces a tool which
tries to automatically create minimal
test cases for compiler bugs and while
it does so actually it produces many
similar inputs along the way so we
actually modified the stress test
scripts in the she reduced or to keep
track of the good and the faulty inputs
and classify them accordingly and then
after that we can actually select a
small set of inputs which are close to
the original felling input and I have
some slab if anyone wants to know more
details for them I can actually explain
later so so right now what we have is we
have
set of similar inputs then we select a
few clothes good inputs from them then
we generate invents using those good
wooden poles and then what we will do is
we will insert those in balance back
into the code and drawn run it and run
it with a bad input ok now the FEL
invents will give us the set of initial
candidate ok but we still have hundreds
of candidates after this step this is it
is a significant reduction but still is
too much for the for the programmers to
analyze hence we apply three different
filtering techniques first is dynamic
backward slicing which tries to remove
any kind of candidate invariants which
may not be influencing the symptom then
we apply something called dependence
filtering where we try to discard the
dependent failed invariance if there is
no intervening passing in variance
between two failing invariants and third
is multiple faulty input filtering where
we run this techniques for many
different similar for faulty inputs and
then try to take an intersection of the
candidate would cause us off from all
such inputs and I will talk little more
about them now ok first is dynamic
backward slicing
here we try to build a dynamic backward
slicing starting from the failure
symptom and any in brand instruction or
in any initial candidate root causes
which does not fall under the backward
slice we remove them we implemented the
N pwc algorithm in two phases and we
handle both the data flow and control
flow dependences and at the runtime we
record all the memory locations accessed
all the basic books that are travers and
the function call and returns and build
a programmed dynamic program dependence
graph using this stress and the SS form
and this tool what we call it as
Gideon's available as open source and
other people's are using it and also
there is a google Summer code of project
this year where we are trying to
actually make it more general and widely
available ok and we compute the this
dynamic backward slice on the original
failing run since the root cause is
likely to be in the failing run and in
our example the two invents which were
on the return value of dnr and weekday
function they lie on the back dynamic
backward slice so they are not filtered
out now let me talk about the next
filtering step which is dependence
filtering so the main idea here is that
the return value of the day in our
function is actually used by the return
value of weekday function so the fault
so the in this case we say that this
value actually fail not because this is
faulty but since it used a faulty value
from the previous dependent instruction
so most likely the root cause is here
not here hence actually so we say that
this is
a possible root cause because the return
value the inventors locator than equal
to 0 got violated here by a negative
value but this one is probably not a
root cause so we can roll fitter this
out so in general the idea is we go
through the dynamic backward sorry we go
through the dynamic program dependence
graph and we check for invent failures
for example if FL invent you just value
from another fell in the end we say that
the dependent imbalance actually is
probably not a root cause it only failed
because it used a faulty value from the
previous invariant so we can filter this
out in the in other cases when there is
they are passing in variance between two
failing invariants in those cases we do
not filter this dependent in the end
because it used a past value from
passing in brand so our assumption is
that this value is correct so this used
a correct value and fails so this is
also a likely root cause so in this case
we keep both the root causes for
eliminate the top one
this thing instead of the code city of
the color
yes that it may be it may have recovered
and one more thing is actually wasting
one path it might be going through
another part to the other same first
symptom that is that is one important
region and I guess in song this is
actual not a also a sound technique our
filtering techniques currently are not
sound so it is possible that actually as
you said this may not be a root cause
also and it's possible that this may not
be also to actual this is a root cause
so our technique is currently not sound
so it is possible that sometimes it can
filter out the true root cause yeah but
in this case we do not know if it may be
going through other values to enrich the
same and may be affecting the symptom
and multiple faulty inputs filtering
step this is a very simple simple idea
so we assume that root cause is same for
all the faulting all the similar faulty
inputs which caused the same failure so
assume that the root cause must be
present in the candidate root causes of
all such inputs so what we do is we use
the similar input generation methodology
to create many similar in polls and we
repeat the previous three steps to
construct many different the candidates
root root causes for each different
input then we take an intersection of
all all those candid root causes which
gives us the final set of candidate root
cause locations any questions
okay now I describe some of the key
details of a bug diocese framework so i
will talk about some experimental d
results key experimental results now
so I would so here I would like to
address two key important questions
first is how effective our overall bug
localization framework second is how
effective are our filtering techniques
okay so for the experimental /
evaluation purposes we use 13 bucks from
for applications and there are five
bucks with which were missing code box
that means there's some parts of the
code which you missing and we didn't
consider them since ephemera currently
can't handle them so we need kind of
additional kinds of incidents like
control flow invariants to handle them
so we didn't consider them for this
evaluation and we used LOL beam for
compiling programs running our passes
this table gives some key
characteristics of the eight server box
from the server bugs we use three server
application squids apache and mysql and
the third column here gives total number
of static lines of code that are
exercised by the faulty run okay so we
have around kind of thousands of lines
of code that that gets executed and the
fourth column gives the distance from
the root cause to the symptom in terms
of number of dynamic number of llvm
instructions and this column gives the
distance in terms of number of static
number of local lines of code and this
gives the distance in terms of static
number of functions from the root cause
to the symptom
and the important observation here is
that the thousands of lines of code that
gets executed in the failing gun and the
distance along the slice from the root
cause to the symptom spans several
functions and this distance is
especially high for the incorrect output
bugs you see for the inquiry or put bugs
there are kind of tens of fun functions
between them root cause and the symptom
and for such bugs and then diagnosis
process is more difficult this distance
is showing along the dynamics lies I
think oh sorry so this distance right
yeah this is along the dynamic slice
actually okay i am not include the other
instructions here instead there may be
other irrelevant instructions so if we
take the slice from the symptom to the
root cause so it will span through this
many functions for the buggy input
Oh for these are like MySQL is kind of
some kind of query like ice for the
example MySQL query and for squier and
apache there is some kind of HTTP
requests HTTP or ftp requests so so we
take the inputs to the application so
now i will let me talk about the how
effective was our overall bug
localization framework if you see each
of these bugs exercise thousands of
static invariants and when we run our
environments pass so we had around
hundreds of failed in variance in each
of those bugs so we can see it is a kind
of significant reduction from thousands
of invariance to hundreds of invariants
but I think still I think this is lot
more for the program was to analyze each
of them and figure out the root cause
and when when we apply all the three
previous filtering techniques we got
around five to twenty eight program
locations as candid root cause so the
filtering steps were quite effective and
then what we did was we actually
manually went through those root causes
and we applied a trivial filtering step
there which I will talk a little bit
later then we could reduce it to only 22
for 14 program locations so what the
approach was Freddy effective for these
bugs and we missed root cause in one of
the cases and here the user root cause
was so inside the visit function he said
the ski process uses to false and we
call this visit expression function here
and here the the condition in this
branch was wrong and hence it did not
set the skip uses skip process uses to
true so it remained falls and
comes back and it incorrectly calls the
process uses function and it results in
a assertion violation so to handle this
kind of box we will probably there are
several ways to tackle this first is we
can do a better input generation so that
it differentiate clearly differentiate
the failing runs from the good runs or
other kind of invariants may help here
like control affine invariance and also
in vents on the intermediate values
right now as I said we have only
invention the function return values
loads and stores not on the interval
temporary intermediate values in when
some intermediate values can also help
in such cases so and that is the kind of
future work for us yeah
kanpur flowing balanced by a control
show environment I mean any kind of
general invariance which takes into
account which branches the program take
one example we can I can think of is
kind of diffusion variance so people
have okay so basically if I have some
use which definitions it is using so
that depends on the control flow of the
application so some kind this kind of
invariance so for some classes bugs it
may be pretty useful for example like
missing code box this can be useful now
okay now how effective where our
individual filters so if you see the
slicing filter is pretty effective we
had it was able to reduce nearly eighty
percent of them false positives in the
second was dependence filtering it was
also pretty effective and reduce nearly
fifty three percent of the remaining
false positives and the third one
multiple faulty input filtering was
somewhat less effective reduce fourteen
percent still I think we can say since
it appeared after a set of very
effective filtering stairs also still a
significant improvement okay now one
important thing was for one of the box
the last filtering stave actually means
the root cause since for for some of the
faulty inputs we generated it didn't
contain the root cause and so finally
the last actually we had the root cause
for 11 out of 13 box
say hello did it to retreat the control
flow of the industries after fixing
controls home
yes some in some will not all but some
of them will change the controls oh yeah
for sure yeah I do not sign up but
though sorry slice and dependent on each
other yeah it's all right yeah
difference filtering actually uses the
slicing stay picture the dynamic program
dependence graph that is built doing
this so multiply independent you
evaluate that keep myself no okay we
applied up okay we applied this previous
steps on all those faulty inputs and
took the intersection so it oh just by
itself you yeah right we are not try
again yeah that you can see ya it may be
much more effective then then if we
applied at the end yeah right okay now
ok so i will let me talk about the
manual manual filtering step we did we
program is can actually manual look into
those root causes and quickly try to
filter out false positives for example
when we look through the candidate root
causes we found out that invents in many
which we didn't do any kind of
sophisticated processing we just looked
at the function name where those felt
candidates invents we're okay by just
looking at the function names we could
figure out that they are very less
likely to affect the root cause yes
we'll let us try to affect the symptom
for example legs and passing functions
actually many candidates there will fail
if there is a slight difference between
the inputs okay but it is kind of very
less like graphic then input similar m
is the case with the input/output
functions also for random number
generation they can randomly get
valuated without really affecting the
symptom and we also observed that many
time related functions fail okay and
they can fail if you run them different
times but very less likelihood after the
root cause and we did this debacle did
not element the false positives so after
applying this make sure
from five to twenty eight actually we
could reduce them to actually 2 to 14
locations and this is actually one of
the bugs actually the candidate
locations in one of the box and here I
have simplified them to just include the
function names here okay so first
actually we we can remove that all the
candidate causation root cause is
because of time function then if you
then I remove the candidate of the
random my under square end is some kind
of random number generator in MySQL then
there are two functions which were
actually input/output functions and
there are finally two functions which
were lexical analyst legs and parser
functions and after that we had only
three candidate root causes here and the
root cause was in this function
made another agenda
oh you mean there shouldn't be any kind
of invariant there yeah but since we are
not doing it kind of sounds you are not
wheezing sound static and laces we are
doing it at the dynamic one time so any
kind of value it observes there it will
try to form some kind of range there
yeah we will come out with something but
it's was not a really valid invariant
there but will come out something okay
it's more like a observed range of
values in some small number of runs
close to the feeling of the period of
their properties Megan already you know
but I was surprised even you saw these
dynamically generated properties you can
construct arrange for anything there
that's really all on something 10 times
a hundred
so okay so i will talk about from there
okay so let us say okay for one of the
bug like the squid length ball it worked
in in our case but if you use kind of
general test inputs it may not work so
why that is because the input field in
this in the faulty input actually uses
many special characters to reproduce the
failure and if you have any large larger
username in the training set then the
faulty input will actually miss the root
cause for example in this case the
failure in failing input was something
like this where they it had many
specific special character in the input
and if you have in the training set if
you include kind of many different
inputs and you have kind of somewhere
very large input it will fail because
there were some environments which you
are based on the lengths of the parts of
the input okay and this will become very
broad if you use many different large
inputs but our approach is kind of more
likely to find the root cause in this
case some of the Cabot's over about a
work first thing is i talked about the
expression trees so how we output to the
program one second is input sensitivity
so so we output to the program or all
the failing candidate set at each
filtering step and for each candidate
said we also improved the maximal local
sub expression tree rooted at that
candidate which includes all the
intermediate values that fits value to
them in the end instruction okay and we
do this since we only track the
load/store values in the function return
values not the intermediate values where
the bog may be actually so as I as I
have shown earlier actually for this
return value for the environment this
return value the expression tree will
include all the statements that are
Martin read
and for the number of candidate root
causes we had in our last filtering step
the total number of lines of code that
which the source expression tree maps is
here and this is these numbers are
somewhat high for some of the box
however I think we can actually reduce
this size of the expression tree to a
much smaller set by putting in vents on
the intermediate values and using some
other in more in balance like address
based in variance and the control flow
in variance because the when you form
the expression tree actually till the
values it can actually escape through
the addresses and make the expression
too large for some of the candidate root
causes and also we observed the bog
behavior was somewhat sensitive to the
inputs we use like for example as you
saw in one of the bhagat one of the bugs
the root cause we missed the root cause
in the last filtering step because of
some of the similar faulty inputs we
used and the under bug SQL convert bug
in one of our earlier set of experiments
when we used Manuel Manuel Manuel
generated inputs to train the invariants
it had missed the root cause but in our
automatic setup actually it didn't
remove the root cause so overall why
does our approach work so well so we had
initially few thousand static lines of
code which were exercised by the faulty
run and our tool was able to reduce that
to 2 to 14 locations okay so some key
regions where I think the likely range
in bands were effective for comparing
runs of comparing successful and failing
runs of many bugs and we use few similar
to inputs to train the invariants so we
had very tight and relevant set of
invariants which avoided the false
positives sorry
I have been talking and false positives
most so which prevented false negatives
and we also had some very effective
filtering steps to reduce the false
positives okay now let me talk about
some of the future what we're planning
to do and what is developing a really
usable debugging tool okay first of all
our analysis already actual extracts
many useful information for example the
FEL invariant and its value can be very
helpful in debugging and the bad inputs
and good inputs also can pro and the
differences can provide crucial clues
about what the root cause might be for
example the when you observe the year
field is always 0 in all the bad inputs
that will give some strong clue for
example in that in one of the box the
parameter to the aggregate function was
always negative for the to reproduce the
failure I think such information can be
pretty helpful and we also have the
dynamic education path from the infinite
failure to the symptom and all this
information will be pretty useful and we
can additionally use some costume but
simple static analysis to extract more
information from the same time symptom
invariance and execution path for
example we can do some kind of symptoms
specific analysis for example memory
bugs have kind of some different from
specific particular type of root causes
so if you can do some kind of seem
symptoms specific analysis so we can
probably pinpoint the root causes better
and second thing is five bucks false
testing uncovers a large number of bugs
and today is difficult to find out which
box to fix which bugs not to fix and how
and diagnosis can actually significantly
help to decide which bugs to fix and in
such a case the in such environment
testing environment for example like in
in a Knightly testing environment our
team tool can be much more practical
because one of the problems which our
tool is we need some kind of detector to
find out whether input causes the
failure or whether it's a good input so
especially for incorrect output bugs we
do not have such a detector so we
currently use some kind of some other
versions of the applications or some
other similar application to compare the
output to and to find the to detect
whether is a fail failure run or not
however in the nightly testing
automatically provides us with a easy
detector for all the bugs and also it
gives us begin put so in this can make
out will really practical and and also
we plan to pursue any future research
directions to make this tool really
usable and 2.2 kind of really pinpoint
the root cause first is our current
input generation is not really general
so we plan to do some kind of robust
input generation and also our filtering
techniques are not really sound so we
can we are planning to do some kind of
more robust filtering steps and we want
to explore a broader category of
invariance
so currently the many testing tools
automatic testing tools like sales pegs
etcetera which uses that type of
approaches and uses constraint solvers
to construct new good and bad inputs and
such kind such techniques can be
leveraged to build a more robust general
and automatic input generation for our
framework however there are some crucial
differences for example we need a sub
strategy which which can quickly search
education path for similar inputs in
contrast this the existing tools the
testing tools they try to explore
different paths so that they can
increase the code coverage but our goal
is kind of the opposite and we also need
a selection strategy to increase the
likelihood of finding the root causes
and I have some examples during how we
can do this for example in this bug so
the root cause was here in the last
return step and there are two
conditionals here chillin in this
function okay so if the c12 CN are the
path constraints on the conditional to
reach to this function okay and we can
actually combine the other two
conditional statements here
and now we have a complete path
constraint up to this candid root cause
and we can try it if we try to negate a
certain constraint and solve up to that
constraint which we can get a new set of
inputs for example if we invert the last
constraint here okay and each of this
conditions here is a branch condition
which through which we'd is the
particular point and if you solve this
constraints the sabra may be able to
give it a different input here which is
not a failing input but well this
similar input to the failing input at is
away next we can actually try to negate
the previous constraint and try to solve
and the solver can give us another input
which is similar to the previous input
but as I said our goal is to actually
explore similar paths not increase the
block of arrays so thus our strategy
will need to minimize the differences
not not the other way around next one is
building a more robust filtering step we
are trying to investigate use of a
dynamic symbolic execution to reduce the
false positives in particular given a
failure inducing input and so given a
failure inducing input and a particular
statement is there a different value of
that statement which can avoid the
failure so we would like to ask this to
the constraint solver and depending upon
the answer further from the solver we
can categorize them into three different
sets the candidate statements for which
it's a different value can avert the
failure and some candidate statements
follow is no different value can avert
the failure and third the solver
Mitchell may not find any solution
within certain time durations certain
time bound and this very this is such
approach may not be actually possible if
we have
if you try to apply that on the whole
program but if we have a small set of
candidate locations already then this
may be a feasible approach and one issue
here is the scalability issue because of
because the execution can diverge from
the point from where they we change the
value at the invent which a novel you at
the immigrant location right so the
execution and divers and it can possibly
use the successful and failing runs to
and control the scalability for this
example actually we can give some set of
constraints like temp is equal to Dana
plus five which kind of sorry okay okay
sorry yeah so here it was computing Dana
plus five so so this constraint kind of
models that statement then this
statement actually models the cast from
temp to to int and now we have a
condition which says the array index
actual lies within the rebound and
dependent the if the solver actually
finds a solution which it finds for this
case then you can say that this
candidate is a this is a possible root
cause and if the sabra can't find the
solution we can filter them out salford
on this some sex angelic people
so be found
okay they have a similar
yeah I do that I don't know how it is
similar okay the tool can automatically
identify the five to twenty eight
candidate root causes and likely range
minions were effective for comparing
runs and we had very few similar good
inputs to get the tighter invariance and
we had novell filtering techniques to
effectively reduce the false positives
and after we applied the manual
filtering step we had only 2 214 can
delay locations and one important
question would like to ask and we'll
address is can the con colic execution
be used to make this approach more
abortion pinpoint the root causes of
failures machines
it's interesting to think about how
what's good for your technique is good
for search I mean in things like chess
and sage there's a certain strategy by
which you searched as or you searched
schedules and this has the property that
you're searching and essentially as long
as things are good you're doing some
pruning and you're you're you're you're
searching then a very similar the next
path in the next grace is often very
similar to the previous so then once you
cross the threshold from good to bad
once you find a bad trace it's very
often the case that that since the
traces are so similar you already get a
very good candidate through pause
because of the search strategy because
you're already close to a good trance
when you find the bad traits so I think
it's interesting to think about you know
search and and what the notion of
closeness is similarity because I think
yeah that's what cases you're randomly
generating tests or I can see the eye
you just leave it up to the users to
generate good and bad tests and then you
don't have necessarily
of course but in search you do you get
it sorted for free and it really helps
in our experience I think that you can
get the walls almost for free if your
search strategies has nice properties
yeah one one way we may be to see how
the the exaction part difference between
is different between different runs to
find out the closeness between the
inputs
looks like our speaker okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>