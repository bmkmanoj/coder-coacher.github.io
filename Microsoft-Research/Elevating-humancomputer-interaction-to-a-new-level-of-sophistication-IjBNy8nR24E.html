<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Elevating human-computer interaction to a new level of sophistication | Coder Coacher - Coaching Coders</title><meta content="Elevating human-computer interaction to a new level of sophistication - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Elevating human-computer interaction to a new level of sophistication</b></h2><h5 class="post__date">2014-04-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IjBNy8nR24E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">the goal of this project is to
understand how to build computing
systems that can see here interact
reason in a way that they can understand
surroundings people social settings and
engage in fluid conversations with one
or more people just as another person
would the system's draw upon a large
number of areas in computer science you
know things like obviously Auto sensing
going on vision and speech recognition
the higher level processing like natural
language understanding and dialogue
there's a lot of machine learning and
planning you know there's a lot of AI
technology we have an opportunity with
this inner creativity AI project to
bring together projects that we've done
in a standalone manner over the last 10
years each of which was a separate area
of research the elevator is in a region
of the lobby area of our building where
most people can't really tell whether
someone want the elevator to walk past
the elevator and people can even walk
very closely to the elevator and not
want the elevator so simple sensors
would not work very well we need a more
of a sophisticated approach to
telegraphing someone's intentions in
advance and taking action on their
behalf we learn to anticipate based on
how the scene looks like when this
button that calls the elevator will be
pushed and we learn to predict that
about three seconds ahead of time so
then you know the camera just sees these
patterns it actually relies just on a
simple background subtraction in some
sense like it just sits the silhouettes
of blobs moving into the scene and based
on that information it can anticipate
this button will be pushed or this
button will not be pushed we use this
common framework across these systems
that look at some of the same problems
but in different settings
it lets us sort of identifying in time
what are the commonalities here what are
the really core bits the core
competencies that we need to have in
place to generalize across these
settings to be able to do systems that
that can
work well in a variety of settings with
people do you need directions yes or
what are you looking for Eric Horvath
the task of the directions robot is very
interesting and it highlights the notion
of situatedness the notion of pointing
directions in a physical space
understanding where the viewer is and
the viewers perspective on what
directions mean understanding what it
means to say you're right or on your
left the robot can track multiple
participants so you can track and
understand where people are how far they
are it understands their trajectories
and it reasons about that to reason
about engagement which is a basic
process that we actually don't think
much when we interact but that happens
all the time how do I know I'm stealing
this conversation with you a machine
will need to reason about all that and
we communicate that with various kinds
of signals with how fast I'm approaching
you am i paying attention in that
direction or somewhere else so trying
with sensors using camera and you know
the sound source localization
information all different kinds of
sensors we have we're trying to make
inferences about these aspects an expert
executive administrator working on
behalf of someone understands over time
the nuances of whether somebody is
interruptible or not
hi I was expecting you the robot told me
you were coming are you here looking for
Eric yes this one really integrates a
lot of different knowledge sources it
draws upon a lot of work that has been
done in the past here by Eric and others
on building inference models and
predictive models so it knows for
instance about Eric's availability it
knows to predict will he likely be
reading his email in the next hour or so
so I have all these interesting
different streams of information and
we're trying to integrate all of these
together each of these systems that
we've built and experimented with let us
sort of investigate the boundaries in
the state of the art and kind of try and
push against tests in different areas
and in some ways each of them highlight
different aspects so for instance with
respect to the situated aspects of the
project today to this idea that machines
will understand better what happens with
people in physical space in the dynamics
of all the human interaction
getting machines to understand the sort
of subtleties of the way people behave
and interacting these different settings
is it's also an interesting exciting
space we see the vision of where we're
going we see some of the challenges
along the way
it's very exciting research we think we
make great progress and I'm certain that
these technologies will be ubiquitous
someday</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>