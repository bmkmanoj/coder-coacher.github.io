<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>X1-Locally Non-linear Embeddings for Extreme Multi-label Learning | Coder Coacher - Coaching Coders</title><meta content="X1-Locally Non-linear Embeddings for Extreme Multi-label Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>X1-Locally Non-linear Embeddings for Extreme Multi-label Learning</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/g_-aJamcQRY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so next speaker is a Koosh Bhatia he is
currently a research fellow with the MSR
India and before that he did his b.tech
from IIT Delhi where he got directors
gold medal or presidents gold medals for
silliman and yeah so he will also talk
about extreme multi-label classification
okay so hello everyone I am going to
talk about algorithm x1 for the same
extreme multi-label classification
problem as he has talked about so as yes
I already explained but just let me let
go over this briefly the various kinds
of classification problems which arise
in the multi level in the machine
learning literature the most common one
is the binary classification problem we
are given two choices and you need to
select one of them so for example in the
US elections you can either vote for the
Democrats or the Republicans a
generalization of that problem is the
multi-class classification problem for
example in the Indian elections you are
given more than two choices and as a
voter you can vote out for vote for one
of the political parties that contest
elections and even more general problem
is the multi-label classification
problem well you are given C choices and
instead of selecting one you need to
select all those which are applicable so
for example in this tagging the
celebrity task if you consider all the
people on this earth to be your set of
categories you need to identify the
correct people who are there in this
what they're in this photo so this is an
example of the multi-label
classification problem and as you had
already mentioned this also comes up
very frequently in document tagging
tasks for example this is the wikipedia
page for Bharat Natyam and you need to
target with the relevant categories or
tags so what makes this problem
difficult and interesting at the same
time is that you have around around half
a million leave this present and you
need to select only five or six which
are most relevant to your which are most
relevant to the document so this makes a
problem are both interesting and
difficult to solve let's try and
formulate this problem mathematically so
if you if you let X represent the space
in which all your documents lie and why
represent the discrete space of labels
the objective in extreme multi-label
classification is to learn this function
f which when presented with the document
it it identifies the correct subset of
labels which are relevant to your
document
and so and there's been a whole lot of
research done in this area and the kind
of techniques that are available in the
literature can we divided into three
broad categories the first one is the
one versus all our binary relevance
which assumes the label to be
independent the second class of
techniques are these tree based
techniques which josh talked about and
the third the third class of techniques
is the embedding based techniques which
will focus more on in this talk so let
us go over them one by one briefly so in
the one versus all read binary relevance
technique you assume all the labels to
be independent and you learn functions
separately for each of the levels so
when you have a new document what you do
is you pass them through each of these
functions and each function predicts a
score for the document this core tells
you how relevant that label is to this
document and the final predictions are
made by thresholding these four values
so there are some problems with this
method for our extreme multi-label
classification problem so the thing is
even if you assume this F to be linear
classifiers the storage space comes out
to be ordered LD because you need to
store a classifier for each label and in
the setting for Wikipedia where you have
a million labels and more than 100k
documents the storage space comes out to
be 300 gb which is it makes it
practically infeasible for such
large-scale classification tasks also
the test time is l times D hat so let me
just go with this notation once d @ is
the average number of features which are
active for a document for example if you
see the Wikipedia dataset as a bag of
words representation the number of words
which are important in each document are
typically very less as compared to the
total number of words in your vocabulary
so in the wiki Ellis HTC data set which
even just mentioned about jihad is
typically 50 while the whole
dimensionality of the data set can be as
large as 1.6 million the next next set
of techniques is what he has talked
about there are the three ways
techniques so each each
leaf node in a tree has a distribution
over the labels and when you present it
with a new document you pass it down to
so this is a forest of trees and when
you are presented with a new document
you pass it down to each of the pass it
down through each of the trees at each
leaf node you get it you get some
distribution over the labels you
aggregate those distributions and you
make the final prediction for this
document so this is in general how tree
based techniques work so and we are
interested in these embedding based
approaches so as you can see the
document lies in a very high-dimensional
bit sparse sparse waste and the label
vector lies in a very large l
dimensional binary space the key idea in
these embedding based approaches is to
project both of these on to a very low
dimensional K dimensional space so the
key task is to learn these
transformations W and H so as to
transform both of these two common space
and then do prediction somehow most of
the different emitting waves methods
actually differ in the in the way they
learn these transformations wnh so what
are the key advantages that this method
has over some other words so typically
this rank rank of the subspace on which
are projecting is much less than l lordy
so where L and D can be of the order of
millions this case typically a few
hundreds the storage space comes down by
a factor of L or D because it's not only
L plus B times K and the prediction time
is l plus D hat time scale so if you see
this carefully if K is greater than B
hat the prediction Lang is actually
worse than what's there for one versus
all techniques so this is something you
would like to address in this talk is it
so let's let's have a comparison of the
tree and embedding based methods both of
them have their own advantage advantages
and disadvantages so fast XML is like
the state-of-the-art tree based method
and LML is a state-of-the-art embedding
based methods as you can see in terms of
prediction time and physician the tree
based methods typically outperform the
embedding based methods by a huge margin
however when it comes to model size
these three base
it's tend to have a very large memory
imprint when you compare it with
embedding based methods just in summary
we concerned we want to work on these
four parameters accuracy prediction time
model size and theoretical understanding
of these algorithms so typically
tree-based message to do very well on
accuracy and prediction time however in
terms of model size and theoretical
understanding or internal theoretical
and decide not much is known about them
so even for the state-of-the-art methods
you do not have consistency guarantees
whereas for the embedding based methods
although they tend to perform poorly in
terms of accuracy and prediction they
are very well understood theoretically
and they have reasonable model sizes
which you can work we want to propose an
algorithm x1 which does well on all
these parameters so that's something
that's going to be like our objective
going forward in this talk since since
you are going to basically work in this
embedding space it's it we should take
some time to understand why these
embedding based approaches don't have a
good good accuracy on such large-scale
performances so this second plot that
you see here is actually a label
histogram of the wiki Ellis HTC data set
so what it shows is on the x axis you
have the labels and on the y axis you
have the number of documents in which
those labels up there note that the
y-axis in the logarithmic scale and you
can see there are hundreds of thousands
of documents which are current less than
five documents hundreds of thousands of
labels which occur in less than five
documents this presents this
distribution a very heavy tail
distribution and in the presence of such
heavy tail distributions it is not
reasonable to to be able to capture this
distribution through a low rank through
a low rank subspace and this can be
verified empirically as well so if you
see the if you see the blue line in this
in this plot the what this is trying to
capture this if you try to have a low
rank if you try to project the label
matrix on to a low-ranked space using
even a rank 500 approximation you can
capture only ten percent of that matrix
however if you look at
red line this is the local SVD line what
this does is it tries to it tries to
cluster the documents into different
regions and tries to fit a low-ranked
subspace into each of those clusters so
one of the key insights that you get
from here is that this this label
distribution is not low rank in its
entirety but if you start looking at
local clusters of labels you can
approximate this very well with the help
of a low-rank approximation so the data
is locally low rank and not globally low
rank as these and bedding materials have
been trying to assume so yeah this is a
recap of our objective slide so we want
good accuracy prediction times model
sizes and our algorithm should have some
theoretical basis to it these are
objective going forward well up in our
algorithm x1 so let us just spend some
time on what was the key inside that we
had from that from two slides back in
the presence of such a heavy too heavy
tail distribution k-nearest and k
nearest neighbor algorithm is known to
be a consistent algorithm in such
settings so basically what you can
expect is the kenya sneh nearest
algorithm to give you good results
however in our large-scale setting this
is practically infeasible because the
test time is order n times d hat so even
if you have like a million documents
you're going to spend a lot of time
going to each document and seeing
whether whether it is a nearest neighbor
or not since K nearest neighbor is
inconsistent is a consistent algorithm
in such a case what what we would like
to do is we would like to design an
efficient and accurate method to perform
nearest neighbor search basically and
our solution would be now to have low
rank embeddings which preserve the which
reserve this neighborhood let's delve
into our solution now so all the
embedding methods that exist in the
literature train now can be classified
under the broad umbrella term of linear
embeddings what they are implicitly
trying to do is if you have these
documents if you have these documents in
original space and these are the
embeddings for those documents so what
they want to do is this objective this
is the objective function that they try
to capture they want to say that the
distance in the embedding space should
be well should well approximate the
distance in the original space and they
try to do and they try to model it for
all pairs of documents so it does not
matter for them if the document is
closed or file they would want that
distance to be well approximated in the
embedded space however if a main
objective is to actually do nearest
neighbor search what we care about what
we care about is a is our neighborhood
and not distance distance in the in the
documents in the neighborhood and not
and not to every other document so what
we propose is a locally nonlinear
embedding method which tries to preserve
for each document the distances to only
those documents which are in its
neighborhood in the original space and
if we just try to formulate this
mathematically you want to minimize for
each document for each document you want
to minimize the the difference between
the distance that is there in the
embedded space and that is there in the
original space this turns out to be an
instance of the low ranks matrix
completion problem and we solve it using
the projected gradient descent method
also called SVP so this is how we obtain
our embeddings using the label vectors
we solve this objective objective
function using the projected gradient
descent methods so right now what we
have is you given a label vector you
have to get the embeddings all you need
to do is you run that projected gradient
descent method and to go back to go back
to the label vector from these
embeddings all you need to do is in
nearest neighbor search what remains to
be done is to learn is to learn a way to
predict these embeddings using the
feature vector of the documents because
when you have a new document new test
document which comes you would ideally
want to predict its embeddings and then
go to the label vector from there so
this is the formulation that we propose
for learning these regressors w so i
will go over briefly what each term
means in this objective function so we
want
regresas w using X which can predict
these embeddings very well so this is
the standard least squares loss function
you do not want this regresar to overfit
to this data so you put afro bingham
regularizer and finally there is an l1
regularizer / xw so we are going to use
xw as our embeddings for the training
data and we want those embeddings to be
sparse l1 promotes such sparsity and
that is why we put an l1 regularizer
over there we use the ad mm approach of
spray common at all to solve for this w
so that was a logarithm x1 in brief not
chill and now let us see what happens
when you have a new point coming in so
you use the w that you used the w that
you learn to obtain these embeddings you
embed this in this low dimensional space
you do nearest neighbor search in this
low dimensional space and you obtain
your label vector however is and and for
this large-scale implementation what
happens is the testing it comes down
from ND hat to NK but it's still very I
its linear in in and you don't want your
test time to go linearly with n DRL
because all of them can scale to
millions and also if you try to
approximate this whole structure whole
global structure using a low rank low
rank subspace the dimensionality of that
load of that first subspace can be very
high so what what we proposed in this
large scale implementation is a
divide-and-conquer approach where you
cluster the data initial data that you
have in the feature space and in each
partition you separately learn a
regresar and embeddings separately for
each of these clusters as soon as you
talk about clustering in such high
dimensional spaces the curse of
dimensionality starts to follow you and
because clustering in such high
dimensional spaces can often be unstable
so in order to avoid this curse of
dimensionality instead of learning a
single x1 learner what you do is you
learn an ensemble of x1 learners and
these ensembles differ in the way you
seed your initial clustering so X 11
would differ
ex1 any in the way the initial clusters
are initialized and it would how it
would go on now what happens if you
aren't presented with this new test
document and you have such an ensemble
of learners so the test document is
passed on to each of these each of these
X 1 learners and inside each EX one
learner you have a clustering of the
initial data you figure out the exact
partition in which this test document
lies you embed that using the regressor
the local regresar that you had learned
for that partition you embed it into the
low dimensional space you do nearest
neighbor and finally you get the
predicted label using this learner you
do this for all the learners that you
are learnt and finally what you have is
a label prediction from from each of
these losers so you agree get those
label predictions and you make a final
prediction in terms of this ensemble
method aggregating labels so let's say
one of the ensemble tells you that it
thinks that label 1 and 2 are active and
the like the last one tells you that
levels 1 &amp;amp; 4 are active and this course
for all these levels are very high so
you would want to predict 12 and for all
these labels together so let's go over
some some nice properties that our
algorithm x1 has the prediction time is
independent of n D and L all of which
all of which can be can go up to
millions it is only dependent on this
quantity NC this quantity NC is the
number of points that you have in each
cluster and appropriately using
clustering techniques you can control
that number we have final sample
consistency guarantees for X 1 wherein
we guarantee that even if a new test
point comes the embeddings would indeed
preserve its in population neighbors in
terms of storage all you need to store
are the embeddings for each clusters and
the sparse predictors the only thing
that needs to be done is to check how it
performs as an algorithm when compared
with these other baselines so again we
did an extensive X
evaluation over a variety of low medium
and large scale data sets where the
label cardinality varied from 100 to 1
million labels all of these are publicly
available except for this adds 1 million
data set which is a proprietary
Microsoft dataset so I will go with the
results one by one from from the medium
scale and small scale data sets to the
large scale data sets so let's let us
compare x1 with other nearest neighbor
bass lines that we have so vus obby is
is an embedding based method which does
a nearest-neighbor in the embedding
space it is very similar to our method
but it differs in the way it learns its
embeddings and Kanan is the vanilla
nearest neighbor classifier that's
present and as you can see it's almost
we have almost ten percent improvement
over over the scanner and wasabi based
methods on bib tech and another it is
your list we perform consistently better
than both of them comparing this with
state-of-the-art embedding based methods
so le ml is the state of the art
embedding based method and I put in one
versus all classifier here as well for
comparison purposes as you can see again
this is this is a huge margin between or
between our method and these existing
baseline methods the third category of
methods which yes presented a fast XML
even even in this regard our algorithm
x1 is doing much better than any of
these existing methods so on small and
medium scale data sets on all the
existing categories of methods are our
method is able to perform the best let
us move on to some large-scale results
so I've shown results for three datasets
Vicki LS HTC delicious large and adds 1
million most of the methods were not
able to scale to such large-scale the
KNN one versus all in wasabi so I've put
in only numbers for X 1 fast XML LPS R&amp;amp;L
EML even element was not able to scale
very well to the ads 12 ads 1 million
dataset and as you can see in terms of
large-scale data if you have a thirty
percent improvement over the best
embedding based methods that out there
and even in terms of the tree based
method we have a
seven percent improvement so
consistently we are doing better in
terms of small medium or large scale
data sets in terms of precision accuracy
what happens in terms of prediction time
so the state-of-the-art embedding based
method le ml takes around two hundred
and eighty seconds milliseconds per test
point to predict where as we take around
eight milliseconds is that is an order
of magnitude improvement as compared
with tree based methods were still far
off and this is something we would like
to work on in the future comparing model
sizes to both x1 and fast XML our
ensemble methods and so we vary the
number of ensemble number of learners in
the ensemble and we plot the precision
accuracy versus the model size as you
can see 2 to beat 5850 fast XML learners
at 10 GB we require only 2 X 1 learners
at around 1 GB and over there only a
single x11 is sufficient to beat around
50 learners of fast XML so in terms of
even module sizes we are doing much
better this was summarized what I have
presented so extreme multi-label
learning is it is a very important
machine learning problem where there are
three broad methods nearest neighbor
embedding entry based in situations
where i showed with a heavy label space
has a heavy tail distribution Kanan is
possibly the best method however does
not scale very well to large-scale
methods and we presented our algorithm
x1 which is a nonlinear embedding which
preserves only local distances rather
than preserving all pairwise distances
and we showed each state of the art
accuracies with the moderate model sizes
prediction times and proved theoretical
guarantees for the same I would like to
thank my collaborators himanshu
Purshottam pratik and manic who helped
and work with me on this
distance Podrick so let's hang pushing</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>