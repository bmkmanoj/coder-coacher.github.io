<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Optimizing Mobile Application Performance through Network Infrastructure Aware Adaptation Techniques | Coder Coacher - Coaching Coders</title><meta content="Optimizing Mobile Application Performance through Network Infrastructure Aware Adaptation Techniques - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Optimizing Mobile Application Performance through Network Infrastructure Aware Adaptation Techniques</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lFxBBN5ZbyQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so I'm here to introduce Chun soo to you
Chun is going to be giving his interview
talk on mobile application performance
optimization he is a fifth year student
graduating by end of April and he has
worked on cellular network
characterization and mobile application
optimization and he interned with us two
years ago and has also worked with other
people and I'm sorry to develop 3g test
which one the FCC open challenge or a
myriad network measurement applications
and um without further ado here's X
channel thank you good morning everyone
and thanks for joining talk am very
happy to be here today to present an
hourish research effort about mobile
application performance optimizations
the roof network infrastructure where
deputation unlike some existing
performance adaptogen techniques ours
are more familiar a more a wire of
server network or unique characteristics
of said network infrastructure the
Tuesday that since last year smartphones
have overtaken feature phones on the
market of several natural device and out
of those smartphones the traffic
generated by those web applications
start to dominate HTTP traffic rather
than web browsers the increasingly
powerful and popular smartphone devices
do not necessarily guarantee acceptable
mobile application performance so very
often will use those applications our
interaction with application will be
easily interrupted by broken network
connectivity or so natural connection
even ways acceptable network performance
without utilizing the seller network or
the computation resource very carefully
those applications will have any
consumption much
higher than we can expect it when we see
our acceptable performance the
performance actually refers to end to
end performance so we know that entrant
performance is affected by a multitude
of factors the networks that device
application the implementation will all
affect the application performance for
example in network the routing the
topology the naming addressing
resolution of them will affect
application performance basai network
application is affected by the hardware
the software the application
implementation there are so many
different types of applications
different type of applications have very
different resource utilization
requirement so we expect those
applications will have different
performance even for the same type of
applications they use different
optimization resource adaptation
technique so we can see sometimes their
performance differ significantly so what
we want to do in our research we want to
improve mobile application performance
again we want to use those updated
optimization adaptation techniques but
different but different from existing of
previous optimization techniques we want
to do something different we do we are
more aware of those fundamentally unique
characteristics in cell networks such as
really our routing restrictions issue
performance variation issue and radio in
efficiency so the workflow for our
research is path we identify some unique
characteristics in cell networks then we
determine we try to determine okay the
impact of those individual factors on
the application performance then when we
develop our design those optimization
techniques we take those into
consideration let's have a look at what
has been done so far in 2011 we have a
project named the yellow page in this
project we identified routing
restrictions issue in cell networks then
we investigate what's the impact of this
reality restriction issue on latency
since
application such as CDN after that we
have a project metope curate and its
follow-up project in this project we
identified our reverse engineered some
middle box policies in several network
and we investigate the impact of such
media middle box policy our networking
applications besides we also have a
system namely o'clock which address a
particular issue faced by seller network
operator when we want to localize
network performance to lower aggregation
level switches are antsy or base station
level very recently we start to work on
project name the pleura routing we
anticipate that in the future internet
mobility will dominate we will damage
the future internet and so we we propose
some up inter domain routing plus with
some internet architecture features to
keep inter inter-domain Alateen flexible
scalable scalable still incremental
deprived incremental deployable upon PGP
those projects are related with network
infrastructure and device that are we
have a project in the 3d test and retest
we try to isolate the impact of
different factors on web application
besides 3d test we have a project in the
Proteus proteins in Proteus we identify
the predictability of server network
performance in the time granularity of
seconds then we try to see how much
benefit we can achieve if we want to
leverage such produce ability for
real-time interactive applications
real-time interactive application is
just one type of application for other
type of application that those are non
real-time interactive we have an
infinity infinity is a scheduling
framework that schedule uploading and
prefetching request for applications and
take energy efficiency into
consideration if we want to move beyond
optimizing a single application to many
applications we need to understand ok
the usage patterns of
those application so we have a diversity
project diversity is a measurement our
study it I to try to understand using
patterns of all the applications that we
can observe in cyber networks for
cellular network operator if you want to
understand app usage if you want to
identify applications a first thing they
need to do is for all the network flow
they need to know which application
originates each network flow so flour is
such system that it's helped certain
network operators to identify those
real-time natural flows to the ipad to
the applications individual applications
that originate such a traffic flow in
real-time way super ways minimizing
supervisor learning average for your
projects primarily compared to the ones
new group that you were associated with
this one the yellow page 1 infinity
Proteus diversity o'clock and flour
which is nice to see that's a product
photos project AB am the lead students
for others and me hoped for three tests
as you know AB the major contributor as
well buying this talk I'm only going to
focus on two of them Proteus and the
yellow page in Proteus we work out your
problems first we quantified are we
identified the predictability in Sarah
network performance particularly in a
temp grand narrative seconds and then we
we investigate okay how much benefit we
can see it will average such predicts
bility for real-time interactive
applications we know mobile user
experience can be significantly enriched
by those real-time interactive
applications such as what I preview IP
video conferencing online gaming and
even potentially popular product based
on adaptive space such as school glasses
five years ago or ten years ago when we
start strategy to use smartphones
windows phone iphone or a blackberry at
that moment the computation capability
or natural technology even couldn't
support was over will appear well but
now we think in all even in future we
expect such applications will play a
more important role but we know those
applications are very performant
sensitive so let's have a look at how
sensitive those applications are to
network performance a very common
feature has been overlooked by us for
those real-time interactive applications
say that those application can actually
tolerate to buy network i teach fact
natural condition to some degree for
example they can add forward correction
to prevent information loss they can
adjust the digital preferred site is
smooth teachers they can even vary the
South coding route to lower down the
requirement on the network bandwidth but
those adaptation assumes that ok the
network performance the network change
is somewhat can be estimated or somewhat
predictable so that they can adapt to
the performance they can do adaptation
if the performance change is in any
arbitrary way then performance
degradation will happens but we know in
certain networks performance variation
is really high at this moment round-trip
time can be up to 100 millisecond but in
the immediate next moment can be 200
milliseconds are even higher so let's
have a look at some numbers railways
southern network performance variation
we have a public available application
management application name the map be
perf also known as 3 it has and 40 test
which was developed in collaboration
with Microsoft as well based on the
measurement collected from 300k users in
the US we see in 3d networks the
round-trip time is around 200
milliseconds when standard deviation 150
milliseconds so the variation really
high similar force to push even for LTE
networks the performance relative batter
but I see the Roxy the round-trip time
is 20 music basically
division is up to 50 milliseconds sure
median ah this is immediate with median
this is the stent evasion click directly
from your 4g test paper in Moby since
right armor you're showing their the
median was about 70 milliseconds you
know and then the standard deviation was
quite a bit lower than 50 wreck that's
papers for Street 444 the faulty test
paper that paper we measure using a
Beretta phone in our local in our local
location because at that moment we only
have LTE just delightful horizon and 440
test the application at a collection
measurement from the entire United
States this is computed based on the
measurement from the US users why is a
local number why is the overall number
represent was it was covering more than
local test fit LOL double check that
sure clarify the first number is a
median order me the first number is a
median and they sit a second number it's
a standard deviation to the point which
others we've actually done a bunch of
experiments right here for a different
project you know we were involved the
treaty test not stuff but we did our own
thing and these numbers don't pan out
the numbers we are seeing it in
washington state with a quality
measurement do not say 20 milliseconds a
more in the direction of 73 seconds I
think yeah I think it's sure that those
numbers will differ based on locations
but this is you know con with that if
you want to present it and you might
want to put some caveats in there too
right okay yeah because because a lot of
our calculations based on a lot of the
systems we built our business on Google
Terms the rules of thumb say
approximately 17 seconds what that the
users and I also keep in mind that the
speed test data shows that
in the US for LTE the median is around
10 to 12 megabits per second on the
dum-dum darling ok yeah so that's
confirmed that when we do measurements
um it depends on the location depending
on here on the data set so ok regardless
of the actual numbers we can spy that
the high variation is the nature do is a
nature from style networks and given the
heavier asian it's pressure to think
that reliable adaptations for those
performance sensitive application for
those real-time interactive application
may always be necessary insulin networks
if we want to let those application run
well on smartphone device so what's
problem do it address we first want to
figure out if there is any produce
ability in sarah network performance or
in other words what performance metrics
are predictable our house to drum
predictability they have particularly in
the time granularity of seconds because
we want to let real-time interactive
application to adapt to that
predictability then beyond that we
position we want to position the
predictability in applications we want
to see how to efficiently do the
prediction in real time and how to
supply those applications where the
profit with the performance prediction
and eventually how much benefit we can
achieve to to let those applications
have performance predictions there are
several challenges we need to address
first there are so many hidden factors
in the networks or own devices with the
natural condition the network congestion
level the load balancing the resource
allocation scheme i totally unknown to
us without knowing such information is
difficult for us to develop some
sophisticated a performance model to do
the prediction so what we do is we use
regression trees we heat treat all those
hidden fans together as a black box we
latch the regression trees automatically
learn the performance patterns then to
the prediction what advantage of using
regression trees is steve as gritty
effort in
the performance model are in tuning the
parameters the second challenge is the
cause of learning projects ability for
real-time interactive applications for
real-time application even without si
without the real time constraint we can
do whatever necessary we can do active
probing we can we can do offline
training but for real time applications
since we know that the nature the high
variation is a nature of the seller
natural performance so to support this
application we think the prediction has
to be are continuously in real time and
the lightweight so what we did is we
passively monitor the traffic improvised
/ flow yeah or gdp rose ah / blow UDP
follows actually I'll talk about later
is that specific to a single divisors
the performance is actually performance
model is not a limited device but the
scope of the performance model only
covers a short time period so you known
as racial it's not necessary to cover
device at a time i think i will
introduce later maybe has a better
understanding so we pass in mind to the
traffic generated by by those iPad
applications assumption we have it as
since we are doing the prediction at the
time granularity of seconds the close
contacts information like the location
the user a ton the congestion level in
the network should be very stable so we
so we think the traffic to track a
pattern of this application at this time
granularity should be very stable that's
why we use passive monitoring rather
than active probing the search
challenges the awareness to produce
ability assume that eventually we can
okay predict predict the performance
metric fairly accurate today we still
need to feed those applications with the
performance prediction
so wait so so we have a way implementer
farcaster library to feed those
applications which is performing cost
the library name is Proteus let us have
a look at what has been observed in
previous studies to crack some
confidence on the performance predict
village we know that the resource
allocation happens at each aggregation
levels instead of network-based agent
level duran sea levels ggsn level so
expect there should be some predict
predictable patterns either resource
pattern of performance pattern in the
tank linearity of hours are there are
Daniel patterns in network load which
has been observed by many pipe papers
such as a single stigmatics paper in
2011 in a shorter time for energy of
minutes the switchboard paper in my
piercings from Microsoft as well they
observed that somewhat the latency can
be predictable or has some
predictability at a time scale of 15
minutes in a further a shorter time
granularity of hundreds of milliseconds
there is a map become paper observed
that a divide tend to stay in the same
modulation rate for hundreds of
milliseconds to us for as the most
suitable time granularity its second
because we are targeting for real-time
interactive applications for example a
typical chunk size for adaptive bitrate
streaming can be two seconds in order to
verify the predictability at the second
time granularity we run some control
experiments in the control experiments
we collect the more than four hundreds
of hours more than 400 hours of packet
trees it covers three sarah network
operators AT&amp;amp;T Sprint and t-mobile and
also a 40 bathtub in covers Android and
iPhone where it didn't succeed on
Windows Phone because at that moment for
windows phone 7 attends the party CP
dump or other packet sniffer
we run most of our experiments in a
napper some of them were collected in
Chicago one thing to mention that we'd
run most of our experiments over UDP
instead of TCP because TCP already has
congestion control and the
retransmission wait what we do
performance prediction bility we want to
have the maximized visibility to natural
performance that's why we're on UTP so
so 400 + 1 hour crisis is nice it can
give you a sense for like the locations
like where did you do them in one spot
in both of those locations or did you do
it in both of those cities or did you do
it in multiple positions in those two
cities so for yes for the location for
for each location for Chicago will do
just at West but for another way to add
several network locations for example
office apartment labrie wait wait eight
o'clock several you know five or seven
locations not like 20
no move during the one-hour traits or
was it stationary
evangel cracks smu some some packet
trees when they are moved but when the
pepper tree when the one way to
experiment since it's me last for well
we need to power supply during mobility
it's difficult so we do experiments in
stationary condition this is a quick
evidence showing the performance
predicts ability at em Granero t of
seconds so we come here this we are
showing the distribution of ultra
correlation coefficient the correlation
is we compute the correlation at this
time moment for the super latest a
moment the correlation between the
current storage and the throughput let's
see T seconds ago and t is a 10 mac so
give it a time lag we know the ultra
correlation coefficient for one flaw
since we have so many flows we have
distribution given for each given ten
lakh and we show the distribution based
on percentile values apparently
somewhere around one second we can see
the autocorrelation coefficient is
nonzero but if we move the time lag
222nd the other correlation coefficient
is close to zero it's too hard to is
hard to tell so one nice then we learn
from this figure that the performance
current performance is highly correlated
with the performance most recent
performance but for the history see 20
seconds ago the creation is already weak
to tell that's why I see it's not
specific to a location because I just
have 20 seconds the location Minaj the
location probably do the same or
specific or specific to a device so we
just do performance modeling at is 10
granularity
we do different time update but later I
will show is not actually affected by
time of day issue much so we thinking
our 400 tracy's way to collect the
experiments over a different time of day
within a specified pick our unknown pick
our conclusions are drawing from this
craft which is that a team that you
believe based on this the location
doesn't matter and and the reason I
disagree with you on that is because i
think in conditions where a signal
strength gets particularly poor or there
can be interference from either other
devices that are on the same spectrum or
or not or you know catherine was adam
oasiz okay but but also in sort of
contradicts with your previous assertion
on the numbers on LTE for instance we
were talking about well those previous
numbers were from lab experiments and
you know we should we should be looking
at numbers from from broader scale
experiments so so can you tell us
philosophically you know why in this
situation we should believe those lab
numbers whereas when looking at overall
team performance numbers we should focus
on a broader set of data such as
um you mean when we only for example for
LT numbers which we should rely on more
like the lab experiments or are the
proud experiment calculus perfectly like
which which is a better choice in one
situation such as collecting LT prongs
numbers and looking at those and looking
at very fine-grained information like
this I think I think eventually the
axiom ago is try to optimize for
application performance so it depends on
the needs Eve with the granularity for
the optimization see it's just a very
proudly lectures such as CD inserts this
we don't care about okay the performance
measure is from the lab we we probably
more care about the performance major
priority over the over the country but
if we are here currently focus on that
application the granularity is about
seconds then probably we care more about
the experiments collected in lab because
see we see the variation in the
variation in the time grin ority the
temp dimension for the variation is we
see at this moment at next moment so it
covers seconds or minutes so probably
from that perspective lab the
measurements capsule from lab is also
valuable difference in this to
experiment in 40 test as you are
focusing basically for more why don't we
have the answer I mean we are basically
that thing about this small I've you're
a fun run again return hilarity I am
learning the question
so what is the celerity he has a
scheduling update them underneath it in
which you know you've got packets coming
into Cuban and lt sort of master
sleeping cream so how much of the effect
you see inside these numbers Chrissy
about 0.5 seconds and this might
actually be with the way of these
scheduling the package to the client as
opposed to nail correlations were for
example sing and that that actually
depends on how the packets of the extent
to the base station or to the plan right
so you will parse that out you mean how
much contribution of the scheduling in
the cell network affects the overall
performance right so I think for 3g
that's definite a very very big impact
and for LTE the impact for those
scheduling is low but in turn that's in
terms of absolute value that means when
we see the scheduling in style network
in LTE networks compared with where line
is not where is not the bottleneck pass
deal is affects the variation
so here we could actually experiments
using control experiments is
over different types of device we will
carry out swimming f1 and joy we didn't
see clear difference in terms of critics
believe we're at experiments from 400
more than 400 packet trees but you want
to make I said conclusive cut our
representative conclusion we need to
figure out the root cause so that we can
apply this conclusion oh we can sink
this conclusion is a common feature in
several networks at the time granularity
of seconds the we know that the user the
congestion level will be asked she'll be
stable very likely to be stable and we
think the performance is highly affected
by the radio resource allocated to
device by the base station and the base
station allocates the radio resource
based on proportional fair scheduling a
balance between the device with the best
natural condition and the fairness of
multi vices the base station has
motivation to encourage the device to
occupy continued attempts throughout to
finish the transmission and we did some
survey on some infocom papers in some
basis base station vendor manuals we see
the aggressiveness factor for the folder
Penn Station to do proportional fair
scheduling acer is as small as 0.001
that means a base station encourage the
device to occupy the channel for a
certain amount of time and if we do a
quick back of Amalur parnassus the time
further divide to occupy the same
channel will be roughly the time slot
for the allocation / the aggressive
aggressiveness factor which is 0.001 and
based on TV shin it will be one point
one point and 67 / 0.001 it's around one
second so that makes us confidence on
seeing there is predicts bility our
performance of stellar network
performance whilst we are convinced
about the predictability with structural
design are the regression tree
is to learn performance patterns
intuitively the input half the
regression tree will be a history window
the performance patterns in the history
then we predict okay was a performance
in the next time window and in the
history window we split the time into
individual time window for each time
window we know the through hood to delay
the last rate and based on the based on
the based on thurs performance in the
history window then we predict the last
to delay or the throughput and we also
know that the current performance is
mostly correlations with most recent
performance rather than performance ten
seconds ago our 20 seconds ago so we use
exponential back-off that means that
includes of the regression tree will be
the performance in the previous one time
window to time windows for time windows
8 and windows also forced to choose the
site for the time window of the history
window for 10 window will choose half
seconds because we are talking for
real-time interactive applications for
history window we choose for 20 seconds
because from the previous figure we see
the correlation between the current
performance and the 120 seconds ago is
two weeks to tell but definitely those
parameters are subject to tune now we
know the design of the regression tree
we run the reveal to tree over other
packet reese's we want to see what's the
performance what's the prediction
accuracy way to prediction and training
simultaneously that means once we can
see a complete history window which is
20 seconds we can do prediction
immediately wait values a prediction
accuracy by comparing the predictive
performance metrics with the actual
performance metrics based on the package
trees to have a baseline of the
prediction accuracy we compare ways to
adaptation solutions why is 81 82 ed one
always ask you mates the performance of
the
next time window based on the current
one and 82 is less aggressive is as you
made the performance of the next time
window based on the average of the
history window so let's see the
prediction accuracy here that we are
showing the prediction accuracy for last
occurrence not last rate for last
occurrence the first positive happens in
a time window Eve ok if the prediction
you would predict your packet loss but
actually there is no we can see the
first positive rate for protests the red
curve is consistently low it's no more
than two percent perfil other two
solutions for 81 is up to you three to
twenty percent for a d2 is even up to
eighty percent actually which is this is
expected because 482 it's assuming the
current performance based on the
performance the average performance in
history but we know the performance
correlation between the history
performance is just 10 seconds or 20
seconds ago it's alright it's already
too weak similarly for first- Proteus is
here is our it's a little bit overlap to
wait up with with 81 82 and the first
negative is still consistent low is one
to two percent it's always is also the
best besides binary prediction say last
occurrence we can also are predicted the
exact numbers fourth root last root and
delay the exact numbers can help
application to address Alexi for error
correction
thanks what right so false positive rate
for that is like almost two percent does
it mean that if you there's some kind of
negative correlation of what they say
about your poisons that it's deposited
them if you just flick me whatever
decision this figure thanks Fred so for
this figure which we see that the
performance at this moment the
correlation between the performing at
its movement and this moment is too weak
to tell so that means evilly say a way
to estimate performance based on the
average the history that will be really
inefficient see the correlation it's
almost a zero so that means even way to
an estimation just a base naive
estimation let's deal when you to look
at a very short time window otherwise it
won't be helpful 0 you expected to be
fifty percent that's right exactly
Oh see
fifty percent you have fifty percent of
false positive rate right I think what's
missing is that you have a defined here
what you mean a false positive do you
mean is it is it an accurate prediction
if it's within X milliseconds over the
true legacy is what what is what what
your definition of false positive
articles I think you have entry number
does that mean that it has to be exactly
correct I think it's wont to be fifty
percent it will be fifty percent Eva
quantitatively to the predict predicts
the last rate but you wait to the
prediction for the last occurrence in
the time window see if there is one
packet loss or to pack loss is we always
consider is a packet loss so that in
that case that's where we fifty percent
but if which is a pretty a las
occurrence is now fifty percent but
there are meeting I mean what's the
probability that the loss just accursed
if you're without giving
this arm here I also want you to mention
that last rate is actually now the last
where it's based on the package it's an
a3 space down window so that means the
given for this time window if there is
packet loss we consider there is loss so
this last rate is higher than the actual
loss rate based on our face down number
of packets also this is also one reason
why this first positive rate it's not
like you said fifty percent ok so this
figure I want to show that we set binary
prediction I can quantitatively predict
the exact numbers for the exact numbers
i use through protect sample as example
so for each super huge I only showed a
15 flows in the figure but when the
random sampling otherwise each the
figure will be very unclear so for each
flow is show that every recruit on the
x-axis and since of flow has so many
time windows we show the average arrow
instant deviation of prediction error
which is you can see the network
performance varies a lot from 100 kbps
to 800 kbps but the prediction error is
still is very stable it's around 10 KPBS
with standard deviation of 10 kbps
LT right this is 476 for 3g it's only
only only key that experiments in our
location we don't have LTE yet but we
eat some quick experiments we found for
LT there's still predicts ability I just
explained because there is a base
station scheduling but a time window set
and hit your window size will be
different aero bars so for one flow
there are so many time windows right for
each time window we have a prediction so
aggregating all the time windows we have
a distribution of the prediction error
the average the meeting all the standard
deviation so for each flow actual one
bar this is the this is the average
instant evasion and the acts access I'm
showing the every distribute of the flow
and i want to show over soap that
diverse natural condition the
performance prediction is stable the
prediction arrows not production values
would predict compare with the actual
one because we have the package trees we
know the actual performance of the
projection of actual
comparison
this is the arrow is using the
predictors reputed manners the actual
circuit and the way and we see are the
difference not that not just madness
because it can be negative so we show
the difference here and then we show the
meat the average of the difference and
the standard deviation of the difference
actually for it not for each super value
for all the flaws we have difference
through values it's it will be
distributed over the x-axis but I only
show 15 of them to make the figure clear
are there different flows of the same
what is it so how the photon generating
the packets the mo more than 400 packed
with loads retracting our control
experiments today are different they are
different because I different time of
day and even you do not characterize the
flows themselves how to 1 i'm sorry
jennifer is the clothes and how fast
facts and intervention generated oh yes
decide the UDP Sandy Ridge you mean it's
actually difficult lack first gap
previous gap was the gap around / UDP a
first do tcp probing to ask me the
network bandwidth then to UDP waited
very similar thing at HTC for each
experiments we ran a short time tcp to
roughly estimate was a network condition
then we send UDP at this at this
standing rate certain characteristic
closed achieves whatever error bars
versus others oranges
uh how's their culture looking at all I
don't know that so if your floors are of
different characteristics right are you
honoring bits are different from yes
then you're gonna win you when you
measure all this stuff it may be
different because again a packet if the
queue so filled up and it wasn't there
Norfolk depending on how fast the
network is responding balloon so you
need to sort of say this is the kinds of
flows which achieved this is the kinds
of errors versus some other kinds of
flow tubes are other kinds of error
unless suppose your confidence that all
the flows are pretty much very similar
to one another and so in which case we
can actually look at the apartment
circles is what service yes I think the
four out for all those flows loss flows
Catholic infra Tom different location
that's why distribute distribute the
distributed over the x-axis they have
different characteristics definitely but
we also show that in terms of different
characteristics the prediction area
roughly roughly other prediction the
arrow of the prediction in terms of our
agents and division roughly the same
very stable we adjust introduce how to
how to build how to construct the
regression trees to show the existence
of predictability and prediction
accuracy of using regression trees so
next I am going to show how to leverage
such produce ability in applications
first the way you will suck the rapper
to add our secuencia information because
first revision tree is a very important
information to compute performance
metrics data package sequencing and
timing information see if a sender one
to send XYZ to the receiver the XYZ
through the stock the rapper we add
sequencing and timing information over
the network the receiver setup to repair
will see the sequencing timing and
distract the sequencing timing to the
performance to the regression tree and
extract the content for the content of
XYZ to the application this is a this is
all our very original design 442
collects the practice sequencing and
timing information I think this way
probably is actually
ball two most type of applications so we
think this did their solution can
potentially maximize the applicability
and the West the regression tree has the
package sequencing and timing
information it feeds to the application
the application for example can adjust a
digital buffer size and also send other
information to the to the sender's to
the sender sac so the sender will get
out for example that allows the soup you
to adjust the verbal error correction
for example do the adaptation and but we
know that many sensitive applications
already included sequencing our timing
information so we extend our solution
little bit as well for those we wait now
those application has supposing timing
information why don't just select those
applications feed regression tree with
the sequencing and timely information
this way it's minimize over head off
without bothering the stock the wrapper
very recently we we we think we realized
that many those applications real-time
interactive applications Ussery a TP
okapi already has security and timing
information and install we can easily
locate those sequencing timing
information just using leave pcap and
leave using leap ich hab the application
that just send and receive as the word
without knowing Proteus library just get
performance prediction from the Proteus
labrie so within taste the very out the
very recent design potentially max my
the transparency to applications now we
evaluate the benefit of such
predictability using video conferencing
video conferencing the rate of
conferencing application does two things
first aid class of performance
prediction from different prediction
solutions like Proteus 81 or 82 why
ensuring that since we are comparing
across or different prediction solutions
how to guarantee the
your condition is the same once it get a
performance prediction 8 adjust a source
coding rate the forward error correction
and the digital buffer site to make
performance adaptation there is the end
our approach on desktop we can use H dot
264 reference software to achieve that
but currently there is no such open
source platform to form for open source
including decoding platform for mobile
so with illustrations to address those
issues the goal is to achieve equivalent
mobile video conference that have first
we modify the HTC for reference software
to enable per frame adapt Asia that
means for each frame we can adjust the
source coding rate the amount of
efficacy forward error correction and
the digital paper size then we run the
modified h.264 reference software on a
laptop this is our limitation actually
we write on laptop and in order to
guarantee that different different
solution different produced solutions
have the same prob pre producible
natural conditions we replay those 400
packet reese's which dynamically encoded
our content over different prediction
solutions and see how I do that this is
an example of the original packet trees
we collected in our culture experiments
and in each package we have a sequence
number temps temps damp and the random
placeholder a random content in a packet
on the receiver side where pure
articulate the receiver periodically
send those those priests holder package
in our experiment those placeholder
packs are some random values buying our
evaluation we reveal those placeholder
package awaits the prediction loss
throughput are latency using different
prediction solutions understand ur side
once it sees our prediction prediction
packet8 compute the source coding rating
and forward error correction accordingly
in
go to the frame according to Turks start
coding rate then replace the content of
those random content with a newly
encoded accountant so we can evaluate
the perceptual video quality on the
receiver side this is a quick example in
visual comparison between Proteus 81 and
82 and also TCP I'll explain later how
we compared with TCP this is a quantity
comparison we use 5th percentile PSR to
show the perceptual video quality and
similarly for for each flow we have a
distribution of PSN are we show the
fixed represent fifths % LPS and
everything because we think is reflects
user this fact is that factory work we
can see for Proteus and opt means up
hypothetical optimal they are very close
the hypothetical optimo is that at this
moment I always know the performance in
the next time window so i can add
forward error correction again at the
jetty teacher / first set and just
starts cooling rate you can see they are
almost identical just slightly worse
definite is much better than 81 and 82
we also compared with TCP but there is a
issue how to guarantee that seem network
condition between TCP and UDP what we do
is for each TCP flow we found we try to
fund the most similar UDP flow based on
averages route then we show on a figure
to compare with different solutions for
TCP is even worse than the case without
performance a prediction that's actually
well known it's roughly 20 DB the
previous 5th percentile PSR are
I cannot even sure for tcp and UDP for
HTC p fellow as search fall I try to
find out ok when OTP flow is the most
similar to the current tcp flow so I
based on averages servers
ayee why maybe two or purchase that's
how we do here and wait for different
prediction solutions this is a regional
package choice in original package sure
we have placeholder package those are
pleased holder package are filled with
random values at the very beginning but
what we do prediction we're using
different prediction solutions we refill
those package with the performance
prediction this is under control
this is control experiments because we
want to guarantee same network condition
so we replace those pack the trees
collecting cell networks previously and
see over those pack the trees how much
our benefit we can achieve unity is very
real mobile members our real mobile
networks there's another difficulty is
the encoding decoding stuff don't work
on mobile platform currently is only
works on desktop we try to cross compare
that wait didn't succeed with USB dongle
that's possible that's definitely we can
run this over those USB dongle but
probably that again we cannot guarantee
seem natural condition we can see we
receive the video quality we receive the
video better in rizal prediction then if
you run it or enough time oh you should
see us just a difference significant
difference yes is the true yes just a
few very simple past we didn't we don't
have enough number of our repeated
repeated experiments to show it's not
much different from wrong you're doing
intros you just run it for some time or
some
p different time of day then should be
able to see some difference between
should be actually if there is no if
there is no such limitation of running
those encoding decoding sweet on our
mobile platform definitely we will do
that just so there is no such testing on
mobile platforms so we think just widens
just let using pack the trees repeating
to show the comparison across different
prediction solutions the fifth sub to
fix the present he'll show more like the
quality video quality affected by loss
that means we the pretties loss is lower
than the actual lot so we lost we lost a
frame then we see bad video quality we
also want to show how if we
overestimated see the packed up the
packet loss rate we add more forward
error correction if we met add more
forward error correction in to waste
network resource and energy definitely
so we want to show we want to compare f
you see overhead due to over protection
the bottom part is actually are the
necessary forward error correction need
to add to prevent our information laws
and the top part is showing the
additional for error correction for
Proteus is around 50 BPS and for the
other two roughly you can tell 20gbps
even higher so in Proteus again with
each three things we first to verify or
confirm the predictability of thorough
network performance and at em
granularity of seconds then we design
the Proteus to provide applications with
performance per class then we evaluate
how much benefit we can achieve by
leveraging such such performance
predictability it's almost identical to
help settle optimal as an ace a little
over asked me to because we replay the
trees rather than do the computation on
mobile device and if we compare ways
existing
adaptation solutions we in terms of PSN
are we have 15 DB improvement we say
vision accuracy delight what what
inputted there bears in the delano and
it's the last their loss rate or just
whether you lost a packet and i have
second here i only show the results
based on Las occurrence that means for
110 windows 498 of those 10 windows we
can predict the inert any packet loss or
if there is no packet loss first time we
know is half second about the way great
in a similar scene if delay is higher
than the human tolerable delayed see 150
milliseconds then we consider similarity
to lassic we consider is a we consider
false positive and false negative also
we can quantitatively produce those
numbers as well but this here i'm just a
show the binary prediction actually
predict the loss rate how how is this
done
I mean you mean continually produce and
lacerated if so rather than not
predicting a binary yes there will be
loss or they won't be laws in a half
second window by developing it will be
very close to the actual last rate
that's why from this fit from the
previous figure we see is almost
identical to up to the hypothetical
optimal otherwise it will be lower than
the ten the blue points
the second project i'm going to
introduce quickly introduces a yellow
page in this project we identify the
routing restrictions you instead of
networks you may already have known at
this moment but at that moment is
unknown to most of us and we investigate
the impact of such routing restriction
issue on latencies it's sensitive
applications such as sagia the
motivation for the project is actually
from the observation about style network
cellular IP dynamics this figure is
showing this observation so for all
those blue points is showing the
geolocation of a single cellular IP
address you can see the single server IP
addresses geolocation overtime can cover
the entire southeastern part of the
United States in comparison we plug the
location of internet IP address is
limited to a pin point on the map so we
want to figure out what's the reason
here what's the reason for the cellular
IP dynamics and we think this may affect
those typical IP based applications on
the Internet so let's reveal are the
cellular network infrastructure this
figure is showing the important network
elements along the path from a
smartphone to the Internet the base
station the RNC and ggsn according to
3gpp ggsn is a gateway for the first IP
IP hub from the underpass from the
smartphone to the internet where IP
allocation happens as well so we think
there must be something related ways
ggsn that results in the cellar IP
dynamics so to speak to specify our
problem we want to determine to internal
of sterile network infrastructure
starting with inferring the placement of
ggsn inside the cell networks and weeks
package will be the root cause for the
cellular epi dynamics then we
investigating implementations
implications of such rotten restriction
issue in Sarah network where like we're
geographically toasty juices are
actually being topologically where we
are it's actually not important that it
we are exactly location of the ggs
imprisonment we are more care about the
network location of those tgs and so we
know how it affect the network
applications or IP based application so
prima sprays mentor is not a physical
physically placement swirl agent with
the facility we we had some observations
we confirm with AT&amp;amp;T poway with the
solution we propose tend to rely on AT&amp;amp;T
information or other carriers
information so we have to the challenge
comes from two aspects limited
visibility and epidemics in terms of
limited visibility one way so intuitive
solution away want to discover the
internal network way to trace route way
to treat our firm provides your internet
we found mostly after miss most of the
hops it's just a private IP address
without house and hostname or domain
information which are difficult for us
to infer topology co information in the
reverse direction we do trees are
fumbling tonight to the to the phone
very often the probes approached by
drops fairwater the nest boxes in the
middle another in queue is solution that
okay I have a server device I determine
which space station connect with my my
device and then which rnc connects with
my with those base stations eventually
we know which dgs and connection weights
RNC's to do the paste on the propagation
but it's are those lower education
levels those are transparent to us those
are appropriate information for Sarah
network operators we don't know the
second challenge aspect is epi dynamics
we will our experiments we want to
control IP address choose to to do some
experiments but we see for
while or even with the reset form we
will lose that p it will appear as
somewhere far away we don't know so our
solution that we consider geographic
coverage of IP it's a sin channel we
expect as those ip's where it's the same
joke we have two patents or seem
geographical location distribution just
should be highly highly likely allocated
or managed by the same ggsn so the
question say that if we can determine
that you'll get the coverage of each
server IP address then we can roughly in
work infer the placement of those ggsn
so the question is how to determine the
geographic coverage of IPs we use to
decide what it asset is a location
search service name the Yellow Page is
very similar to Yelp on the server side
we know the IP address of the HTP
request and the GPS location of the
request the second thing has that is 3d
test no need to mention is the it
collects the some like the carrier
information the IP address of the each
time the user run our experiment tell
how to infrared ggsn placement based on
geographical coverage I think there
should be some correlation between the
ggs imprisonment and the geographic
coverage so once we know the geographic
coverage of IP address we group them and
clattering clattering rhythm to figure
out some patterns of those geographic
coverage then further in vascular ggs
imprisonment are going to show some
examples first let's see how do we
determine the geographic coverage of
those cellular IP address in terms of a
number
I understand that the top boxing ivory
you're saying that you you expected that
IP praying that the device is using for
any given our communication that as the
view from the public internet side is
going to depend upon where the device
physically is located in the world is
that what your kindness for all
communication or oh we think okay for
what I p address it can be allocated to
so many different devices so for each IP
address for so for each IP address then
we can have a geographical distribution
or geographical coverage even the device
will always use the same IP address i'll
PM worries to your application is or
does it there's any what I'm questioning
is there doesn't seem to be anything in
here about who it's talking to what do
you do you see that the for a given
device about the phone here right now I
try to connect to say some website in
California versus some website in
Florida am i'm going to be using the
same IP address on the public internet
for both those communications or might i
go through some different egress point
into the public internet that allocates
completely different IP address but a
communication
I think you will be allocated by the
same gateway but II view user device you
see connect communicate with the server
and Washington and the server I see at
Michigan over the over a very long time
the IP address may be different issue
and another one and so technically you
can have different IP addresses so for
each each phone you can have different
classes of service that are essentially
defined by the APN settings so you can
have multiple ap ends tied to each 3G or
4G connection and those can technically
be served by different Teachman sense so
you could contact different services
aren't on the internet via different IP
addresses so so I think your point is
about it that in some situations that
can happen I don't know whether the US
operators actually do that I do know
they have a multiple apns but I don't
know if phones I'm not aware of any
phones of us that connect to multiple
you can simultaneously with their
international ones that do do that will
they beat sort of the architecture issue
yes that was my second question is
basically is there an assumption here
that all GD ascends offer the same ap
ends that the provider has because I
know technically you can have let's
you've got three ap ends you can have
some GD a sentence serve Akins one and
two are the ones that serve two and
three and so you may not necessarily
have this correlation direct correlation
between geographic location of users and
what block of IP addresses there
we serve god I can't leave the service
where a Piet's can be given to the
corporations like gravity private
appearance can be done soon educate you
will see the conversos i think that's it
i think first i need to clarify that how
to define GG i said if GG s in just a
single machine then use yeah yep then is
right that's the earth so different AP
and they should probably go to different
single machines by you receive g GS and
probably it's a cursor of machines it's
a serving our same roughly the same
geographical area then you can consider
them together at the same GDS and
cluster so in that case you have
different AP and profiles although you
connect to different different the tgs
and individual machines but still you
are talking away the same ggsn clustered
least the definitely one weight design
system we want to a select the
functionality of different GG as a GG GS
emotional some of them may serve what a
pia some of them we save the up with
will serve the other I mean they're
they're still it's still a valid
configuration to have gps ends of
cluster pjs ends in one data center
serving a subset of ap ends and GES ends
in another data center serving
potentially overlapping the different
set of Indians you mean at the same time
at the same at the same time it only
happens when there's there is some some
specific service because some there's
still some motivation for device to keep
IP addresses then in that case even if
you move to another location you're
still talking to the original ggs that's
probably happened in roaming the earth
in other words even also in other
aspects of load of balancing the
sometimes the at this moment the IP
address may be allocated managed by 12
GS in the next movement it may be
managed by the other ggs so you will
take time domain into consideration that
doesn't move answer is there more than
10 miles with its lesson 10 for AT&amp;amp;T is
for ya so it hasn't changed in the last
few years well I stiff a culture change
action you can you can you imagine they
need to set up a very big data center to
serve all those RNC's they only have
I searched online for like Cisco you may
provide such tgs and machines it's not
it's not it's not it's not that large
egg is it's yes more than we can expect
it I heard the name they are going to
expand their significant be asking you
that I've been told that they've already
expanded from the four that we should we
show with me I think it's probably one
reason why they want to acquire t-mobile
because evilly okra tmobile you can
share the network they will combine
those data center together share common
network resource but anyway I'm going to
speed up so how to determine that you
have a coverage it's our first questions
is the first question so we have a
location search search service we have
the server logs we have the logs from
the server side for each happy for each
HTP request we know the IP address and
we know the location of the HTTP request
by in terms of number of records for
each single IP address it will be small
so we aggregated IP address in the same
vgp prefix well use ratios data and the
trio's data give us at least a bgp
prefix so we first occur if you stood in
a sec we have for each bgp prefix we
know couple of GPS locations so wait for
each bgp prefix we know that geographic
coverage as the retired similarly we
know the carrier name answer it has
because it's the application class such
information so for each prefix we know
who's the carrier and weaker leave those
the intermediate mapping table together
we for each carrier we know at least
I'll bgp prefix and the coverage of this
of the bgp prefix so the next question
is how to infer the GS and placement
based on the geographic coverage here i
show some examples clearly in the top
two figures those two figures those
those 2 c-i 24 address box share very
similar geographic coverage they're
almost identical covering the eastern
part of the US and for the bottom two
this year another geographic coverage
covering the southeast southeast part of
the u.s. we looked at the entire our
entire data set we see there are only a
limited say four or eight geographical
coverage over the southern South Beach
p prefix so intuitive solution here that
we why don't we just a cluster those PGP
preface based on geographical coverage
so we think they're joke you a cluster
them we can see which probably in which
GTS and covers which accounts for which
at PGP prefix so way to clustering I'm
going to skip the details how will you
run those clustering algorithm how to
retune the parameters but here I'm going
I'm showing example using AT&amp;amp;T for each
cluster we aggregate the geographic
coverage of the BGP prefix in the
cluster we can clearly see that when
covering the western part when covering
the sauce sauce sauce part when covering
the southeastern and the eastern part
for AT&amp;amp;T there are four GTS and clusters
for other carriers there are roughly the
same number for tmobile six for sprint
eight and the four for verizon six so
this means that every time we set up
communication between our mobile device
to a remote server it goes through some
ggsn which is far away the injuries
which reach to the content server this
is the routing restrictions you we
validate an hour nowra clustering
through full method i can give one
example for example dangle patents for
the bgp prefix be down to the class of
carving the western part we count the
number of records for the bgp prefix we
can see a diagonal pattern and we do the
same thing for the for the BGP freaks on
the eastern part clearly we see there's
three hour outside so we also use other
solutions to write to validate in our
classroom result well I'm next I'm going
to focus on the implementation the
implication of such restarting
restriction issue and latency sensitive
applications such as CDM from CDN a very
important decision for their design is
where to place the content there are
several options we can press content in
radio access network I know some startup
companies are working on this but it
requires infrastructure support because
the IP that inside the radio access
network
it's lower later than I p and the second
option is freely to you cellular
backhaul the static IP networks it
requires approval approval from southern
network operators then the current
that's why I currently cell network
operators they are doing their own
content city in service for normal
citizen service providers a
cost-effective approach may be just a
place close to ggs but intuitively we
know the latency is most likely decided
by the widest part so you will place of
the content close to ggs then the
benefits may be very trivial so we want
to see how much the benefit is which way
to some experiments in 3d test we send /
over to the first epi hub then we sound
proper to 20 landmark servers for those
20 landmark servers will choose the one
with the smallest latency and we compare
this modest latency which ggs then here
this is a comparator this is showing the
distribution of the difference we can
see even place content codes through
ggsn we can roughly have 6 6
milliseconds benefit the 16 millisecond
when we did a study is very trivial
because the 3g through impurities the
round-trip time is 100 to 200
milliseconds 6 milliseconds is too small
but in current LTE networks we know the
latency is reduced as significantly the
16 milliseconds may be non-trivial avoid
consider raising content close of GDS
because there are so many GG essence
there there are so few GG essence we
just need to press content creo two
dates for example for locations when you
don't need to consider other locations
so this can be a cost-effective approach
the second thing is as we know there are
some inconsistent usual between the
network location and the physical
location away place the content
physically close through the Tuda to
those users to those devices we may make
it may be non optimal so we compare the
latency to the first ap hub and the
latency to the geographical
the closest landmark server and we show
the difference pressing continent close
geographically close to users it will
result in roughly named me namely second
additional agency again this Regency may
be considerable for LTE networks so in
conclusion way in this work we identify
the rutting restriction issue in cell
networks you explain why this week never
matter I mean it would seem that the the
topology closest Network the one that's
got the fastest connection to the GS tjs
em whatever they would be more important
than what what its physical location is
right in England ektoras there's a
proximity between the network location
and a physical location so sometimes we
just simply put appraisal content as
code wireline we're not internet there
is a proximity between the network
location and a physical location so you
waste your place content the six
similarly just based on the physical
location it will be not optimal right
seems to you that the difference between
this slide then the previous slide is
simply showing that the connectivity of
the landmark servers to the GS gigi SMS
doesn't exactly correlate to where
they're physically located but rather
correlates to what their quality of
network connection is
this figure is now showing the quality
of the natural connection this figure
see we have 20 landmarks service we do
proba to the 20 landmark server some of
the landmark server is geographically
close to the cellular device so we see
okay for this latency we compare avoid
the latency to the ggsn see it was how
much is latency inflation go to the
phone to the phone yes yes ah well that
then well again why would that ever
better because i sat in varna you just
answer I mean you're basically just
showing that the Jeju sons are
definitely play store that there's too
few of them work yeah season is the only
one which can reach the phone for our
practical yes
I'm not a surprising the difference
isn't worse Yeah right the difference is
worth compared to I would have expected
this craftable worse because because as
we know and with a relatively small
number of ggsn you've got a relatively
large distance between where phones
actually are where the GSM is in reality
it should be worse because we do 420
landmark servers we select the way 20
landmark servers way we choose I think
for all for this figure for the preview
figure I think they should be worse
because we do 420 landmark servers and
we select the minimum the minimum
latency and the 40 tiessen we just do
once so if you do more number of probes
definitely you will see if you will have
a better chance to see smaller latency
so if way to stay 20 probes 2g GS a-- we
probably will see higher benefits after
after of the latency i ordered that
you're trying to tell us your company's
like Akamai already put caches in
network
and that works you know so what's what's
something that you're telling us here
catching can now solve all the problems
rights so this you need to least you
have the motivation to push the club
push the content close to the to the
user as close as possible and there are
several options but maybe the
cost-effective approach it's just a
priest content kill 2g GSM if they move
the content further inside and network
it will require some additional over
high so we see ok how we press council
at ggsn intuitively we think the benefit
will be very marginal because the worst
part was do you think Dominic's the
latency pier with what if I that we see
this penetrates may be marginal to set 2
3g networks but in the future in LTE is
now the case and it is not necessary the
case anymore and also people are
thinking about okay let's move the first
app you have a forward to the device
rather than GTS and because ggsn you too
far away if we move move the appt you
hop forward to close to the physical
physical device there may be other
chance for us to place content just give
people a idea of raising contents it's
not necessarily intuitive
the contribution here our way identify
the Alton restriction issue in cell
networks the conclusion that for all
those major seven network operators they
only have four to HDD assets and we also
investigate the impact of such routing
restriction in shoe latency assistive
application taking example as content
delivers a network and which we think
that precinct on turncoats 2g GS s still
beneficial for particular in the future
and the benefits can be non-trivial in
LTE networks in other research project
we have some observations may be still
beneficial Subban beneficial to our to
mobile application may be for for
example for not speculate we have a way
we reverse-engineer the middle boxes and
we found that some middle box policies
just too aggressively interrupt the TCP
connection are very very very quickly
the interruption to TCP connection will
lead to energy waste and application
performance degradation also in a
diversity project we found that actually
twenty the local application you will
consider that most of the traffic most
of the content request is from a certain
geographical area that will be a local
application we actually found the twenty
percent of applications are local
applications so this bring up the
questions how to place content to close
to for some application there are
significant local application how to
press content further close to the to
those users may be will you cry some
infrastructure support you if the local
applications are significant we also
observe some applications that were
always uses the other we previously up
we often do optimization on a single
application but it may be not optimal
optimal if we know that though they
always into interference with other
application problem probably some one
way to optimization we need to take
multiple application together we also
observed that non-trivial applications
are used when the user are in a movement
we know that when user new movements
there are lower layer events like
handoff and the connection will be not
very reliable so when with one weight
design
or as for those application optimization
for those applications we probably need
to take a reliability into consideration
how to do performance adaptation for
those applications since they are used
they are user very likely in mobility we
also have a system named deflower this
flower as I mentioned it identify the
app identity of the real-time network
flows in easier seller network or
enterprise network if they want to apply
certain policies on the app base for
security issues for like user for
content customization issues and we
found that eighty to eighty four percent
of those applications are identifiable
with our supervisor learning that's in
mobile in mobile networks so I think
overall the contribution here in our
researches we develop optimization for
mobile applications and we also address
the several challenges faced the
particular by network operators and for
some of the solutions require some
effort in terms of light large large
system scalability issue and that's all
thank you probably posted an eye
disorder chance to ask questions
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>