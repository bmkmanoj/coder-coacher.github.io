<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Introduction to Large-Scale Optimization - Part3 | Coder Coacher - Coaching Coders</title><meta content="Introduction to Large-Scale Optimization - Part3 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Introduction to Large-Scale Optimization - Part3</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YwaWFto9KoQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
i think i'm going to start so welcome
back to my last lecture on introductory
stuff about large-scale optimization 12
minute recap hopefully you do remember
so somebody had also emailed me
yesterday I did put up all the slides
they might be some your URLs missing
because I didn't spend too much time
making the webpage for that yet but the
slides are online at the promised URL
which is there so these slides also
eventually I will put up and I will
improve that webpage with details that
are available on this page so check them
out and if you have if any at any point
even after the course you have any
questions comments criticism feedback
anything about my slides or the material
or you want to know more pointers to the
literature or anything feel free to
email me eventually I will reply to your
email related to that and just to end
you so yesterday we talked about some
introductory concepts of convex analysis
and for those of you who like challenges
there couple of interesting challenge
problems in the slides or convex
functions see if you'd like to think
about them they are not research
problems I know the answers to both of
them and when we talk about some basic
concepts going up to duality and some
optimality conditions but the slides
contain further derivation of some very
interesting concepts like minimax theory
saddle points from which then I right
include a short derivation of the famous
KKT conditions of a necessary and
sufficient optimality etcetera so all
that material is there for your
reference and the second thing we looked
at after building this basics of convex
and
are some optimization algorithms and a
few comments about their rates of
convergence or rather what is also
called the global iteration complexity
you run an iterative procedure after K
steps of the procedure approximately how
far is this from the true global optimum
and we looked at gradients time methods
descent based methods for both
differentiable and non differentiable
problems and I didn't give very else
annex pooper explicit application of
those things i will show you a toy
example or a toy actually an example
from an application that i worked on a
few years ago which uses many of the
kinds of algorithms that i talked about
yesterday as subroutine so that if you
are curious to see in what bigger
frameworks you could be using these
types of methods you may like that today
i am going to talk about something going
beyond the gradient and sub gradient
style methods of yesterday and this is
the class of methods that we also
discussed a little bit yesterday during
the panel that these are super simple
methods and we like them like them
because they work well on big data
problems and these happen to be of great
importance that's why to current day
which in learning actual applications so
the ones that I mentioned yesterday
those you may think of as small to
medium-sized problem methods and the
ones that I will talk about today are
for truly larger and larger problems and
I'm not going to spend any time
discussing because we don't have that
kind of time but if if you're interested
you can talk to me later about that a
lot of effort has been spent by people
both in industry
academia to implement these methods in
on very solid platforms to engineer from
the systems and networking and computer
architecture perspectives how to realize
these algorithms in very practical
setups we don't have time to get into
that but that's a very interesting part
of the whole world of large-scale
optimization in a couple of days I think
there is a lecture on trade-offs ability
of communication and computation for
distributed optimization that should
time nicely to stuff that comes after
looking at some large optimization
problems so that's where we are today
and just to remind you of the context
within which we are operating so look at
the standard regularize machine learning
empirical risk minimization problem if
some loss function you have a
regularizer and this you have by now
seen that several times and this is a
special case of the composite objective
form that we saw yesterday for which I
showed me that is this prox operator
based methods we have a loss plus
regularizer like block the least squares
loss or logistic which happens to be
quite popular for click through rate
prediction in computational advertising
and standard problems like lasso and
some stuff with the multi-level CRFs and
in this context the way this so this one
this just remember just a little caveat
here i am continuing to use the machine
learning notation to aid your if you are
doing pattern recognition by looking at
the formula because they w is written as
the parameter and x and y are training
data pairs here i am just assuming X
belongs to R to the D X could be some
data and actually work with the feature
map of X which may live in some other
space but i will just write x and
somebody did ask yesterday so what do
you really mean by large-scale ml and
what we typically mean by
our scale mmm is that both the
dimensionality of the data and the
number of points and that you have in as
your training data both of those are
large and the kinds of methods that you
care about so if you assume the training
data sparse actually the storage will be
typically much smaller than d times n
but when we talk about scalable methods
we can we we mean methods that do not
cost you more than linear in the site
entire input therefore if your sub
linear even better for some problems but
you have to if you wish to see the data
at least once then this is the
complexity so in the risk minimization
framework so the training cost is the
regularized empirical risk and the
actual cost function in machine learning
is for me the one that allows you to
generalize on unseen data and the one on
unseen data is not the empirical cost
function this is the empirical risk
that's a regularizer over the samples
but an expectation over all possible
samples that could come from the space
over which you are trying to learn so
these are kind of related but different
problems so you can see that the sec the
first one is obtained by say pulling iid
samples out of the second formulation so
the second problem the one for
minimizing the generalization error or
the testing error that one we solve by
making a single pass through data in
that what that really means is you pick
samples randomly one by one and go use
these samples to update your estimate of
the parameter w and because we use the
word iid because we're trying to then
prove some theorems about how well
you have estimated it you essentially
get to use each sample only ones and the
empirical risk there you have already
all the n samples you are free to use
them Brazilians of times if you want so
this is slight difference and so I will
first talk about the harder problem
because the harder problem has been
studied for longer within machine
learning and statistics the easier
problem of minimizing just the training
error that has been studied for very
long in optimization and it has been of
course fairly well studied now in
machine learning so the minimizing the
generalization error is written as this
stochastic optimization problem where
you are trying to minimize a cost
function which is given as essentially a
summation or an integral over all
possible training data so the randomness
is in the training data samples given
back see so the setup is that you are
trying to minimize over some compact set
that we keep that compact set there it
allows us to prove some bounds and the
samples are some random variables
following certain probability
distribution and that this loss is well
defined so this is just the usual setup
of the machine learning and we are
assuming the simpler case that we are we
have a convex model because it's easier
to optimize so assumption in this
optimization framework that you do have
ability to generate samples according to
their distribution and you need not know
the full distribution
that defines that expected loss or that
expect that risk that you are trying to
minimize but you have the ability to
pixel and get samples from them and you
have access to a subroutine which I
yesterday called an Oracle we just got
or you can call it any subroutine that
can compute for you stochastic gradient
switch is just another word for some
kind of noisy estimate of a gradient
which in expectation so on the average
is actually a true sub gradient almost
surely so so you have a black box that
can give you a stochastic gradient and I
in fact mentioned this theorem yesterday
when I showed you that if you have an
integral each of whose components for
each of those components you can get a
sub gradient you could get a sub
gradient for the whole integral in a
certain way when we looked at some
differential calculus this is related to
that that sub differential of the entire
function is just given as the
expectation of the sub differentials of
the individual guys in the integral so
it means a single of those guys without
taking the expectation is a stochastic
sub gradient so the expectation just
think of it you are averaging over some
samples and so there are two ways of
addressing this stochastic optimization
problem both are well studied and well
known in optimization the first way is
called stochastic approximation which is
essentially what is relate called the
stochastic gradient descent method by
originally by Robbins Monroe from 1951
in fact we are still using it and so
this method operates in a highly
sequential fashion pick
sample so you get to see a training data
point compute a sub gradient of your
cost function at that point and plug
that sub gradient into the sub gradient
method that we saw yesterday the
counterpart of that is what is called
sample average approximation SI which is
insane machine learning language called
building the empirical risk or empirical
cost function so pick some n iid samples
and look at the empirical objective
function so this is not the expectation
over all possible samples is just the
expected cost over just n samples and si
a what I say the sample average
approximation this is the name that is
given in the stochastic optimization
literature to the act of creating this
cost function it does not refer to any
algorithm for solving the problem si on
the other hand is of course is a
stochastic gradient algorithm so once
you have the empirical objective somehow
you have to minimize that also and we
will look at methods to minimize that
that's one of the most interesting
things that people are paying more
attention to these days so the
stochastic gradient method is fairly
simple so you sample and you've just run
the stochastic gradient update be
wrapped around a projection operator
because we had that constraint set so I
will just simply write that update so
instead of carrying along so just GK
here yesterday when we were looking at
projected sub gradient method gk was a
true sub gradient today it's a
stochastic sub gradient that means GK
actually depends on the cck sample but
to avoid writing a very long symbolizes
right JK here and of course the question
is does this method work and as it
happens with any of these gradients of
gradient based methods
more or less unless the cost functions
are really nasty the more or less work
so the difference between the sub
gradient gradient methods that we saw
yesterday and this stochastic world is a
is a natural and very straightforward
simple kind of difference and that is
yesterday the when I said you method
converges and it produces the iterates
today because we are working with
stochastic problem if things will
converge only in expectation so you
won't have exact convergence but in
expectation so then after that pretty
much the same math works out same types
of results work out but because we are
not using true gradients but we are
using noisy gradients it happens that
the speed at which we converge that gets
slowed down which is intuitively not so
surprising and there are functions which
are so nice that even if you use a
stochastic gradient you don't have
slowed down but that happens when the
original problem was already so
difficult that you couldn't solve it
fast so I will make that explicit pretty
soul so convergence analysis let's go
through this one convergence analysis
because yesterday I ran out of time I
couldn't show you what I think is very
nice and clean convergence analysis of
the proximal gradient method which
includes projected gradient projected
sub gradient etc special cases so that
analysis is on the slides but I couldn't
go through it so let us look at a much
simpler kind of analysis of this
projected sub gradient method and so of
course this myth analysis is really
analysis of projected sub gradient
method which I did not describe
yesterday go through but it's exactly
the same analysis with expectation
wraparound stuff so but let's go through
this and I not in terrible detail but
just so that you get a flavor that
analyzing these algorithms is actually
not so complicated so nicely the
stochastic optimization algorithms they
turn out to be much easier to analyze
then the meth l grew thumbs for
minimizing just the empirical risk
though mathematically this is a harder
problem to solve so the point XK it just
depends on the sample seen up to the
point K minus 1 so it is itself random
but it doesn't depend on the chaos
sample it's XK plus 1 which uses the sub
gradient and the KH sample so XK plus 1
depends on c FK but it doesn't xk itself
doesn't depend so that's useful so the
sub gradient method like many other
optimization methods that one tries to
analyze the convergence analysis when
you're trying to prove that this method
converges which means it goes towards
the or minimum one popular way or one
basic technique to do that analysis is
to come up with a function which is
called a Lyapunov function which
characterizes progress of your method
and for sub gradient style methods the
progress function is really hinges upon
how far is the cath gas at the solution
from a true answer for many other more
complicated optimization algorithms
sometimes it can be fairly tricky to
come up with nice in a fly up no
function that allows you to measure
progress and probe convergence so this
one is simple so I'm showing it here
because now we instead of using true sub
gradients we are using stochastic sub
gradients the Lyapunov function actually
becomes expected value of their distance
to the optimum so let's write their
distance
this random variable capital R K
distance to the optimal like that and
little ARCA justice expected value and
the strategy is very simple we
essentially are doing some inductive
type of argument that suppose we have
some idea what RK is we just want to see
what our k plus 1 is and come up with
some measure of progress that at time
point k plus 1 how much have you
progressed so our k plus 1 is just that
and i think i will remind you today of
the that very important property that i
mentioned about projections yesterday is
that
projections when you project so this is
like projection of X and this is
projection of Y so px minus py they can
only shrink distances this can be proved
as by using just some of the optimality
conditions it's actually also one of the
example exercises in my slides for you
so XK plus 1 is just that because X star
is a feasible and optimum point it it is
its own projection so projection of on
to X of X star is equal to X star so I
can wrap a P of X around X star now the
standard trick p of x apply to something
minus P of X apply to something else
things can only come closer so you can
get rid of the P of X in there and now
you can see the inductive step taking
shape because you have XK plus 1 minus X
star here xk minus x star here so you
just expand that so you get the first
term which is our k xk minus x k star
square the gradient term in the third
term okay so so far so good right it's
really nothing happened but a very
important property that got used is the
shrinking property and very useful to
know that when we are using proximity
operators they also have this shrinking
property so if instead of projection we
were using proximity operators this step
would still go through and if instead of
using proximity operators you were using
any other operator that has this
contractive shrinking property this step
would go through as long as it has some
nice behavior there so it's it really
hinges on that nice property
so we have managed to prove that
inequality so now we make one more
assumption to enable us to control that
inequality and that assumption is
roughly saying that somehow the sub
gradients the stochastic sub gradients
are bounded that the which roughly
amounts to saying that my convex
function that I am trying to optimize
doesn't vary too crazily from one part
in space to the other let roughly what
it says so it is an assumption it is a
knowing that we have to add that
assumption but let's it's the one people
commonly use so now we put the
expectation operator around this so the
expectation is taken over whatever
randomness is in there so is this change
of notation and if you have an
expectation around that this part is
constant this part is bounded by
bounding that this is just change of
notation and you have that expectation
so now we only need to figure out how
does the last term behave everything
else is so far so good right so since XK
is independent of the KH sample you take
that expectation now I have expanded
that GK to explicitly show you that GK
looks like that so you just use nested
expectation to write it as an
expectation / ck of the expected
conditional expectation of things inside
now dot product is linear so this e
operator can be pushed inside so this is
just the expectation of some random
vector which is we call the stochastic
sub gradient over the other samples and
we assumed by construction that our sub
routine
gives us stochastic sub gradients which
means it gives us sub gradient switch in
expectation are true sub gradients so
this expectation gets simplified and
replaced by a true sub gradient fine so
we have managed to turn that random
thing into this still random object so
we need to now control this term so it
remains to just bound that and once we
bound that then you can plug that in
here and we'll have some kind of
recursive bound on our case in the after
they just to do your bit rate or induct
on it so because f is convex that helps
us now bound this term if ever if f were
not convex at this point the proof would
break down so if I because F is convex
it satisfies that bound because GK is a
sub gradient this is just the sub
gradient inequality we talked about
yesterday so in particular then you plug
that in multiply both sides by like
positive number put this back into the
inequality I just rearranged it I
brought that thing to the left hand side
so there's a type of here we had this no
that's not a typo sorry we had that I
brought it to this side and I replace
this by this quantity which comes from
this bound which one this one maybe the
sign got flipped yeah yeah yeah sorry
but it will come here but after you do
that so what so from this point we need
to somehow what we really want to do is
now ultimately we want to see after K
steps how far is f of xk from x star so
if you work with the summations you may
notice something nice on the right hand
side this is our k minus r k plus 1 so
if you were to add up for k equal to one
up till k equal to some number they
might be a lot of cancellation happening
here so we try to make use of that
cancellation so you sum up over i is
equal to 1 through k those terms they
telescope lot of stuff cancels out and
this is the bound you end up pertaining
of course that thing is positive so you
can drop it that you still have their
say in inequality and I'm just trying to
simplify and rearrange this bound to
make it look as simple as possible
that's all I'm trying to do right now
because I want to replace this summation
by something simpler to understand so i
divide both sides by the summation of
the Alpha eyes those were the step sizes
remember we are using step sizes in sub
gradient so i divide both sides by that
and so you notice nice thing of dividing
by that that the gamma eyes they
arise after dividing both sides they are
just convex coefficients or probability
vectors or whatever you want to call
them so in other words what we get is
that the expected value of gamma I times
that difference is upper bounded by
thread quantity that's one of the most
familiar looking kind of bounds once you
start looking at subgradient methods and
such related methods and this is still a
bound which involves all the X I let's
simplify it into a statement about just
the final XK that we have reached that's
all and this is here where I again
invoke convexity so we have this convex
combinations so it's easier to talk
about the averaged XK that you have so
rather than talk about the exact XK it
turns out easy it turns out to be easier
to simplify this summation in terms of
the average XK that you attain after K
steps and notice that to maintain this
average value you don't need to store
the previous solutions you can make a
running average which is important
because if you had to maintain the
previous case solutions then that would
kill the benefit of the large-scale
thing because these are really long
vectors typically large problems these
are so long they don't fit in a single
XK may not fit into single computer's
ram anyhow so f of the average is
smaller than the average of the f's
thanks to convexity that was our
definition of convexity and hence
plug-and-play gives you that bound that
the expected distance to optimality of
the average iterate is upper bounded by
that so if you now want to get some
corollaries out you just pick plug and
play with some nice values of alpha and
you'll get some bounds out so you can
select some nice values of alpha and by
plugging in a typical value of alpha
which is like one divided by root of K
you end up getting convergence rate of 1
divided by root K and the impressive
thing is even though this is noisy
stochastic problem this is the same
convergence rate that projected sub
gradient method has even in the non
noisy case so the stochastic and the non
stochastic versions both have the same
complexity 1 by root K this this stuff
goes further if you were looking at
problems which actually differentiable
but not not just non differentiable then
one can actually compute gradients
rather than one can compute stochastic
gradient stars in stochastic sub
gradients and a different kind of
analysis ultimately leads to a
convergence rate of the form 1 by K the
analysis ultimately boils down to
proving inequalities which capture the
net amount of you know distance to the
optimal they follow fairly similar
structure but now there is convergence
that happens as a function of K so the 1
by K convergence should remind you of
the 1 by K convergence we saw for
gradient methods yesterday but there is
an extra one divided by square root K
term so that comes from so if the
stochastic gradient because they are
stochastic the amount of variance that
or the amount of noise there is is upper
bounded by Sigma or their variances
upper bounded by Sigma square that shows
up here so you can see from stochastic
gradient convergence proof that if Sigma
goes to 0 the stochastic method turns to
the non stochastic one and we recover
the one bike a result so this is an
annoying 1 by square root K is removable
dependence and it's is removable so
there is
a lower bound proof which says you
cannot get rid of that however that is
as more pessimistic story there has been
recent work maybe two years ago and even
some of it this year by different people
one is by Frances Bakken moolah and the
other is by I think Aaron Sid food and
shampoo curry and some other people
where for some special cases even for
the stochastic problem they can avoid
the lower bound punishment caused by 1
by square root K because the lower bound
applies to an entire class of problems
so they identify a sub class with some
nicer properties where you can beat the
lower bound and that's the recent
progress in the stochastic world in
particular it turns out i think for
least square loss in logistic regression
both are smooth differentiable problems
with more careful analysis you can get
around back and the other the third
class of functions we looked at were the
strongly convex functions and for those
functions also with a clever convex
combination so rather than using uniform
averages of the excise like we did when
we did the x-bar k right now in our
analysis we use a non-uniform
combination which gives more weight age
to the more recent points strong
convexity seems to prefer that we can
get a 1 by K convergence rate and this
this previously people had proved with
uniform averaging you actually get an
annoying extra log effective but you can
get rid of that by non-uniform averaging
but only in 5012 somebody proved it so
that's kind of my summary foster
commands and of course they extend to
proximal methods then accelerated style
method so like yesterday we saw that you
had gradient method going at one bike a
lower bound by was 1 by k square then
they was nesterov our accelerated
gradient method which could attain the 1
by k square so similarly people have
invested
gated that at least the non stochastic
part in these methods so the part that
doesn't depend on the noise can we
accelerate that to be optimal and that
can be done people have studied all
these variants so there's a large body
of work on this topic and fairly good
summary of that work leading up to work
i think around 2010 you can find in one
of the optimization for machine learning
book that i listed on the first page
that has two chapters which describe all
the stochastic stuff in greater detail
ok so now after the stochastic let me
just speak a few quick words about batch
problem and let's see how because I also
want to show you at your request an
actual application where I used some of
these algorithms but that application
happens to be different from convex
optimization though but we are coming to
it so the batch problem says collect
samples make an empirical objective to
empirical risk minimization the slight
confusion that may happen when you read
some of the literature is that after we
have made the empirical objective
function we may still minimize it using
some randomized algorithm and often
people just call it that we minimize
that empirical risk using a stochastic
method and they that is also called
stochastic gradient so that can
sometimes cause confusion because the
empirical risk minimization just
minimizing the empirical objective
function regardless of which method you
use whether it's batch of stochastic
it's a different problem from minimizing
the true generalization error but anyhow
so the theoretical guarantee is that we
will have our only on sub optimality on
the empirical objective if you use a
randomized method to minimize this
creature then your have guarantees in
expectation only on F hat and when you
want to make a generalization theorem
you wish to take the finite sample
to make a statement about the
expectation but that requires work but
this like strength studied stuff in
machine learning and statistics but
these days people said well actually
when we work in a real machine learning
application I already have pretty much
not always except for online settings I
already have all my training data
sitting out on whatever distributed file
system or something so I'll just process
the I'm minimizing empirical cost
function all the time anyways let me
stop worrying so much about the
generalization error for now and let me
just deal with that generalization error
by doing more regularization or things
like that so because paying more
attention to just looking at this
empirical risk and trying to do the best
job of minimizing it and of course this
is just f of X like yesterday you can
throw any of the methods from yesterday
to minimize it there is a regular riser
you can even throw the approximate
proximal methods if F is are non
differentiable you can use sub gradient
stand method fine we already did that
yesterday right but the stochastic
gradient way of solving that what is now
I used to call it incremental gradient
to maintain the distinction between
minimizing generalization error problems
with troost expectations and minimizing
finite sums but there is no there's no
need to maintain that distinction so
what you do now instead is at each
iteration you randomly pick an integer
from sorry that should be n I had n
components in their summation to exploit
that summation structure and instead of
using the full gradient just use the
gradient or sub gradient of the
component whose index got picked so
so in expectation if you because we are
picking uniformly at random in
expectation even that gives a true
gradient but now because we are using
just the gradient of one of the n
components potentially each iteration is
now n times faster so you can make much
faster updates so of course then the
question is well you can make much
faster updates but if you need billion
more updates then what's the point and
so here is the summary then stochastic
gradient I'm just comparing that now to
this for the full gradients and Foles to
cast a great fall subgradient methods
and the stochastic versions so if you
were using the sub gradient method from
yesterday and you had just general
convex function then it will converge at
the rate 1 by root K in just a few
minutes ago we saw that if you use
stochastic sub gradient it also
converges at 1 by root K and if you were
using the sub gradient method with them
slightly more careful analysis and you
had strong convexity it will go as 1 by
K and the so does the stochastic method
so from a theoretical complexity
viewpoint it seems that the stochastic
method is as fast as the non stochastic
method but with much cheaper iteration
so maybe we just go for the stochastic
method for these problems anyways the
picture is slightly different or
substantially different for smooth
problems seems that for smooth problems
the methods that use the full gradient
and step size tuning can benefit much
more and the benefit much more because
the full gradient seems to carry more
information and you can actually the one
of the main reasons it benefits more is
you can take longer steps so you can
more make more progress per unit of work
and so if you had just a smooth function
the full gradient method goes as 1 by k
+ v
saw that stochastic gradient method goes
as one by root K because of the Sigma
part and if you had strong convexity the
full gradient method has linear
convergence there is a square root term
here if you use the fast faster gradient
methods that I talked about yesterday
but the stochastic method even under
differentiability and strong convexity
still manages only 1 by K so for smooth
problems stochastic gradient method
theoretically needs way more iterations
to give you similar level of guarantee
of course the stochastic problem methods
also have guarantees only in expectation
but stochastic methods are of course
vary widely stochastic gradient had been
and still is very widely used in machine
learning because it gives you really
rapid initial convergence so you may
have convergence looking like that that
you know the gradient method maybe goes
like that stochastic gradient may go
like that and then get really really
really slow so this is time and all
sorts of speed up technique steps I
steering and lots of different things
have been studied about those but more
or less in the end the worst case
remained the same like the worst case
complexity remain the same so that led
people to think that well why not just
make a hybrid method which tries to
borrow some of the good things from
gradient methods and plug them into the
stochastic gradient methods that's an
oversimplification but that's pretty
much what it is so there is one method
called stochastic average gradient which
says well we will average the gradients
so the way that runs is it stores the
gradients of all the components so the
way this method actually runs is at its
first iteration it makes one pass
through the training data and stores the
corresponding gradients so this should
already sound very very some to you we
cause I have n amount self-training data
samples and maybe say 10 million or
whatever millions you want it's storing
so many additional gradient vectors each
of these gradient vectors can be of
length D so sounding it is pretty nasty
but let's see if if you could pay for
more storage what do you get in return
so then it stores the gradients and
doesn't does the same stochastic
gradient step that you pick a component
and instead of updating using just the
single stochastic gradient you picked it
uses the previous ones it uses an
average of the previous ones that you
have stored and the latest information
so it uses this average and it turns out
this is just a randomized version of a
method called incremental aggregated
gradient which was proposed by blood at
all in 2008 which is one of the methods
in a long line of incremental gradient
methods that have been studied in
optimization since 80s and this has
nasty storage overhead but in some
machine learning type settings it is
acceptable in the sense that if you are
each fi is just a loss over some label
and X transpose your training data so i
just wrote that as a feature map as over
the training data then the gradient is
as the gradient of the loss and the
gradient of the loss just depends on the
dot product x transpose fee so it
actually just depends on one number so
in those cases you could actually just
store the only n numbers because you
anyways have those training data you're
paying for those anyways so you just
need to store an additional number so
you can deal with it in some cases not
always if you don't have this nice
decomposed ability then not so it's some
sometimes this can be a decent choice
benefit this is summary of the benefit
so the gradient methods in the convex
case we saw 1 by K strongly smooth
strongly so of course gradients which is
different
she will strongly convex case has a
linear convergence the accelerated
gradient has square root in there the
stochastic one gives 1 by K as we saw
and this method which uses additional
storage and rather than using a single
gradient averages things so that helps
it d noise and allows you to take longer
steps ends up getting linear rate of
convergence so this speed up is actually
it shows up in practice also so if you
compare it so I don't have lots in there
but if you look at the authors papers if
you compare it against the ordinary
stochastic gradient this guy does really
go way faster so that's quite nice there
are many other but it has really ugly
convergence analysis I think some
cleaner analysis of this method is about
to appear but the original analysis is
essentially unreadable similar rates
have been shown similar fast rates have
been shown for a bunch of other methods
so if in the dual setting there is
things like stochastic dual coordinate
ascend stochastic coordinator sent but
those apply to only limited problems but
they have such linear rates there are
there's something called stochastic
variance reduced gradient I'll just show
you the iteration this guy runs and make
a comment on that and a bunch of other
methods and new ones keep appearing so I
think like one week ago I saw a paper on
accelerated versions of large family of
this incremental gradient methods this
is like what one week old and there's
even an incremental Newton method around
these which attains linear convergence
and so on so it's a very active area
currently this mini batch parallel
etcetera things also around this out
there and so I just mention related
method called stochastic variance
reduced gradient and it's called
variants reduced gradient because
ultimately it's what is trying to do is
instead of using just the original
stochastic gradient as
a random variable whose expectations
gives you the true gradient so which is
just approximating the gradient any time
you do that every approximation you
there will be some variance there will
be some error in that approximation if
you can change the approximation either
either make it biased or maybe not bias
but add something to it which helps
decrease the variance that means your
gradients become better proxies for the
true gradient you will see a benefit in
the convergence and a much simpler
analysis then the SI g method was given
by for the that's this SVR g method
which does still feel a little
disturbing but it it shows the structure
of using more information so what this
method does is it runs a double loop in
the outer loop it's really computing the
full gradient so this is like truly a
hybrid method the outer loop computes
the full gradient and in the inner loop
you run a stochastic gradient type
iteration where instead of using just
the stochastic gradient you add
information from the true gradient to
reduce the noisiness of the stochastic
gradient and then every once in a while
you of course reach the outer loop and
under some assumptions this method you
can show that it converges at a linear
rate however notice the difference from
stochastic optimization problem this
method makes multiple passes through the
data so if you happen to be in a setup
where you can afford to go through
training data more than once you could
benefit from this and many times you can
but not always some people work with
problems that are so large or data
changes so rapidly that you would not
like to go through the data more than
once so that version of problems is also
still very interesting and important
even more so
all of these methods they are typically
in the practice initialized by run going
through data ones using just the
ordinary stochastic gradient method so
understanding and improving the ordinary
stochastic gradient method which just
makes one pass through data will impact
all of these faster methods also in a in
an empirical sense so I wanted to you
know not rather than spend all the time
on convergence rate bounds I wanted to
actually because I was it was suggested
to me yesterday to show an application
where these things could be used I
wanted to show you a toy application of
mine so let's so I'll show you an
application because which I worked on
few years ago but that got me more
interested in to incremental methods and
that was the following application we
were looking at images of distant
objects we were actually looking at
images of planetary of bodies through
atmospheric turbulence so what you see
through the telescope is something very
shaky like that and if you have a look
through a telescope you'll see and it's
distant stars or planets because the
atmosphere is in continuous turbulence
it's very shaky all same thing happens
if you are taking MRI images the heart
is beating if the person or animal is
alive motion causes turbulence and what
do you observe sorry this noise so you
have some noisy sequence and you want to
denoise it and d blur it to obtain
to be able to reconstruct a good
approximation to the underlying signal
image or medical image or whatever you
want this is related to if you have your
iphone and you're taking the video with
that now technologies improve if it's
very shaky could you just do dynamic
processing to stabilize it or improve
the resolution or do super resolution
whatever so what we looked at that
problem as was now we have those images
so each image so we have a video the
video has say 10,000 frames each frame
is say an observation vector each vector
is disturbed by some blur plus some
noise and the assumption is there is
some true underlying answer X so you
observe a frame of noisy versions of a
true underlying answer X so you have
10,000 whatever number of observations
and these are noisy versions of the same
underlying object so that's what you
want to benefit from so I write it as a
matrix factorization problem you have
these columns each column is an image
each of the columns of this matrix is
the blurring operator that describes how
the underlying signal is messed up so i
can rewrite it to look like a matrix
factorization and the aim was to solve
this problem in real time ultimately and
for a typical example so if you 5000
small data problem all almost frames of
size 512 by 512 it turns into a matrix
approximation problem which without
exploiting any structure turns out to be
unnecessarily large with structure it
becomes smaller because it's blurring
it's not just an arbitrary linear
operator that messes up your
observations but even then it is still
five million parameters and we are
trying to solve this essentially in real
time so here pratik may be angry with me
but for me with alternating minimization
was impractical so I still I'm not going
to prove any Theory here other than that
I can make this thing converge so that
was the only theorem at that
point in time so I write it as a matrix
factorization problem you look go
through all the images each of those has
a blur operator and I want to get a true
underlying image you can add some
regularization both the blur and the
image are unknown so it's a non convex
problem you add some regularization on
the image the only regularization we
used was that the image pixels are non
negative which is a pretty strong piece
of information but it's an important
piece of information so and for the
blurring function we use some very
simple regular risers we just try to
maintain the property that the blur
function is not going to change the net
amount of energy in an image it just
spreads the energy out but it doesn't
change it so the way to solve this is
pretty trivial is you guess an answer
sure sure so we actually even if you
drop all of that it doesn't so the thing
is the important message here is with
large enough number of observations you
can throw out even these priors doesn't
matter so the physics people were using
very complex price based on physics it
didn't work because those were too hard
to satisfy in an optimization framework
here we just made such a large number of
observations that there was enough
signal available to reconstruct but yes
if a more faithful consider prior would
give an even better reconstruction so
that is true so you observe an image use
that to estimate the blur solve some
optimization sub problem and that
optimization subproblem is one of the
convex optimization problems that we
solved yesterday so the sub problem is a
convex optimization problem and modeling
the N estimating the blur comes back to
your question is a completely different
activity step three is a convex sub
problem that is one of the things we did
yesterday and the overall procedure is a
stochastic gradient procedure the kind
we are looking at today and this is an
a nice example let me show you why so
the key idea is the optimization problem
is over two variables X and a blur an
image and we are trying to minimize over
both I just write it as min over X of
min so just min over x of f of x where f
of x is the minimum value of something
so this is clearly a non-convex function
and this is in matrix notation from that
I reach the problem in terms of f of X
this again looks like the composite
objective minimization problem from
yesterday except it's a non-convex
composite objective problem so that does
not stop us you know we'll still do what
we do with convex problems and the ideas
from convexity they many of them
translate over to the non convex
large-scale problems and for some of the
non convex problem then you can even
prove a theorem that they at least
locally converge and they at least
locally converge at a certain rate so
one can prove those theorems I didn't
yet I didn't prove for these because it
wasn't giving any particular insight and
I couldn't at that time so so the key
thing here is which also gives back a
more general picture about the
stochastic methods as we have already
seen if we were to run a proximal method
using the full gradient f of X computing
the full gradient is very expensive
because computing the full gradient of f
of X involves going through all the ten
thousand images that we captured so of
course it's a natural candidate for
using noisy gradient so I just call some
of them sometimes also in exact
gradients and that's a common idea in
many optimization problems too i use
stochastic gradients stochastic
gradients have one kind of noise but in
general noise can be of various kinds
some noise could be because you just
solve the subproblem in exactly but you
want to be able to benefit from doing an
exactness so you can scale up so we had
a bunch of in exact nas's in there
and f of X decomposed over all the
images and instead of using full f of X
i am using the incremental f of X and
this turns out to be the key to
scalability and i think i actually
already showed you that but let's see if
the video is still playing that actually
does this is a synthetic data we
collected to show that it does give a
reconstruction and on clinical data also
so this was the raw sequence and just
going through it incrementally and
updating your estimates in the
stochastic setting they end up as you
will soon see reconstructing a
reasonable estimate a stable image the
most challenging thing that remains here
is I am doing video in image out
ultimately I'd rather like to have video
in video out but then I need much more
data to be able to do that but any also
this thing does work and I think and
actually the Astronomy people many of
them do like this because it helps them
solve one of the problems they care
about and I think I'm just wanted to
make one more comment about parallel
methods but I think I don't have time to
do that but I will spend five more
minutes to tell you a few important
perspectives that remain at this point
so the stochastic gradient idea is
important in the non convex problem
computing a single stochastic gradient
required me to solve a convex
optimization subproblem the kind of
convex optimization problems we looked
at yesterday and that's kind of a way to
put your building blocks together it
involves proximity operator so you need
to have really good subroutines like
projections and proximities to do those
things so the structure and knowledge we
build up for the convex world it proved
in handy for even the non convex
problems and I think what I wanted to
tell you was I had wanted to tell you
about how to do these things now on
parallel and distributed architectures
but maybe I am in minus five minutes of
time right now so i'll just mention the
most important part there that i think
would be a pity to miss out so you have
seen block coordinate descent in chi is
chi jail in stocks i think and block
coordinate said okay well f cost
function is function of n coordinates
and at each step i will go through the
coordinates one by one or maybe i can go
through them randomly or using some
other more clever strategy and optimize
single variable problems and in machine
learning people ask the question will
what if in addition to being kind of
separable over coordinates like that
your cost function also has this nice
separuh bility that we were so far
exploiting for stochastic gradient
setups well if this is truly fi x i and
all the excise are independent these are
n independent problems you can solve all
of them independently but if the there
is a little bit of overlap so if your
cost function cannot always be
separately written out like that there's
a little bit of overlap then also you
think that it's almost separable and I
can use the stochastic gradient ideas
that we have been looking at right now
or coordinate descent ideas that we have
looking at right now and still solve it
in parallel and incur a little bit of
penalty for being in exact because we
are not we have not doing the things
because they were violating due to the
lack of separable ax t and in fact
because in machine learning you we often
work with training data
that are sparse so it may be ultra high
dimensional but it's sparse each
training vector may only have a few non
zeros that means and many of the cost
functions that we have they involve
terms of the form w transpose X i if
this were a training data point so that
means if you had the loss function of
that type this loss function depends on
only very few coordinates so this really
is almost like that and it is this
structure that people have exploited by
combining stochastic gradient ideas with
coordinate descent by really truly
exploiting the way data looks for many
large-scale machine learning problems
and to do that you can write it slightly
more formally that if you have your cost
function you can partition the
coordinates in two bunches which are
independent and typically data sparse
you will be able to partition things
into a large number of this essentially
independent bunches of losses and
because they essentially independent you
can plug and play this on a parallel
computer and the sample coordinates from
each block and update all the
independent blocks in parallel the only
thing that is atomic is the shared X so
this is roughly the underlying idea
behind a large number of parallel
coordinate methods and the hog while
stochastic gradient method and they
several other asynchronous coordinate
descent methods and all sorts of methods
which many of them they just hinge on
really exploiting this separuh bility
this is called the sparsity assumption
in machine learning and it's also
closely related to the idea of
real-world data having power law
distribution so that even though it
doesn't have physical zeroes it's
essentially sparse and using this par
City assumption is what has been one of
the most let's the easiest ways to
extend all the stochastic etcetera stuff
that I talk about right now including
the faster SI g sv RG whatever including
these faster methods to the parallel
setting so i think this was the most
basic way to keep scaling up and i am
out of time so i am not going to be able
to tell you how about doing this on a
distributed computer where different
concerns are at play but fortunately
there will be a separate lecture in the
summer school on distributed
optimization so you'll learn more things
there so i think with that i will stop
and not stop you from having your coffee
thanks
any questions oh I already disconnected
so what is the effort required in your
example from reducing with structure
from billions to millions we are able to
reduce it right so is it a slow process
or rough so no so the I just said that a
naive way to solve it would have
billions but we assume that the
distortion in the image is not just a
completely arbitrary linear transform
but it is some kind of space varying
convolution so typically people use
convolutions but those are too
simplistic for atmospheric images we use
we introduce a more generalized thing
than just a convolution it's like a
non-uniform convolution but it still can
be done using something called
time-varying fft and so on so ultimately
it boils down to saying that the full
matrix a is parametrized by a much
smaller number of parameters that makes
the thing smaller but there's a lot of
other engineering that goes in there to
make this thing really ultimately run on
a GPU in almost real time thank you any
more questions
yeah can we have a quick comment on the
initialization for non-convex problems I
think you mentioned that we initialize
sometimes using a CG figure do we have a
concrete analysis on the initialization
itself okay so I think a 2 replies he's
asking about the role of initialization
for the non convex problem and there was
a hidden implicit sent there which is
talking about the role of initialization
in the convex problems so I showed you
stochastic optimization which makes one
pass through the data and then we looked
at the empirical risk where we can make
multiple passes through the data and the
first pass of those multiple pass
algorithms is just the ordinary
stochastic gradient one past of the data
so they do make that one pass and they
use that to initialize the data
structures and whatever the need for
making the subsequent passes so doing a
good job hopefully on that one pass is
important because it already makes a lot
of progress even after that one pass and
so in some sense it's it's somewhat of
an open concern to understand the impact
of that initialization even for pure
convex problems for non-convex problems
as it's known they are quite a bit
sensitive to initialization however we
happened to be in the lucky case of a
somewhat easier non convex problem which
is made simpler by have by actually
having big data so we have so much data
that eventually the thing fixes itself
so it's not that sensitive to
initialization but it is still we we did
experimentation it is still sensitive to
the amount of in exactness we are using
incremental gradients but that's not all
we are not even using a true incremental
gradient because I said to get a sub
gradient to get a stochastic gradient we
need to
volvo convex optimization problem and
convex optimization problem is never
going to be solved exactly so there is
already in exactness even there and
those things add up but initialization
in general it's a non-trivial topic for
non-convex problems and for our cost
function we did a few simple things
guided more by physics than by math but
yeah
so but let me say again i will put the
slides up again and feel free to email
me any point if you have anything to say
this material or if you want any more
information so i will certainly respond
to every question that is sent to me
thanks / you can so many things also
with four amazing tutorial on convex
optimization</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>