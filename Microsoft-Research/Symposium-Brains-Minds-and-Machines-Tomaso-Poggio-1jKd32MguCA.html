<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Brains, Minds and Machines - Tomaso Poggio | Coder Coacher - Coaching Coders</title><meta content="Symposium: Brains, Minds and Machines - Tomaso Poggio - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Brains, Minds and Machines - Tomaso Poggio</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1jKd32MguCA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
all right I think we can start so
welcome everyone to the symposium on
brains minds and machines um let me
first say a few words before I start
with the first session so this symposium
is organized by tomas aperture gabriel
crimen me I'm max nickel we are all part
of CB NM the center for brains minds and
machines which is an NSF science and
technology center funded for 10 years in
multi-institutional MIT Harvard and so
on and in fact tomorrow is the director
of CD mm so the idea of this symposium
today is to discuss current results in
the scientific understanding of
intelligence and how we could use these
results to possibly create more
intelligent machines and away but we're
interested in is how today science
enables us to create new methods to
engineer intelligence and this
understanding of intelligence require
series of different levels spanning
neuroscience cognitive science machine
learning AI and we are very excited to
have what I think is really fantastic
list of speakers today so in
alphabetical order we will have talks
from soy yo ganguly from Stanford day
Mississauga's google deepmind kristef
 from the N Institute for brain
science gave you a crime on from Howard
tomasz departure from MIT and ruzek's
from Howard and Josh Tenenbaum also from
MIT so in a few words about the format
we will first have almost years when
writer talks if there's time we have
possibilities to ask questions and at
the end we will also have a panel
discussion of the symposium and there
Gary Marcus and Tara sinofsky will also
join the discussion so I'm without
further you let me hand over to Tomas on
max welcome to everybody and the theme
of the workshop is really in the title
and the subtitle here is today's science
is tomorrow's engineering I want to
explain why this workshop and and this
is also an explanation of why this
Center for brains minds and machines
which is organizing this symposium this
is more or less the plan what I want to
tell you the main messages are that
recent advances in a high especially
deep learning have come to basic science
of decades ago and that we should as a
community or at least part of this nips
community should realize that there is a
need for basic research on human
intelligence I stress human intelligence
and of course if you are a scientist you
don't need an engineering justification
for doing scientific research on the
brain and the mind but there is also an
engineering justification and I will
briefly mention a few recent results
that point in this direction so let me
start with thinks everybody of us knows
that we have seen in the last 10 years
quite a number of surprising and great
successes of AI from deep blue to Watson
to drones that can land on an aircraft
carrier like the x-47b in general the
appearance of machines that mimic human
performance and are even better than us
in narrow domains of intelligence and of
course the planning
was the story of the last two years from
deep mind which is one of our industrial
partners in the center for brains minds
and machines to this review on deep
learning of young lacuna and joshua
banjo and Geoffrey Hinton now even more
than scientific paper I think that the
real success of a ir things like
system
recognizing large
traffic signs in traffic
for assistance systems
the greatest challenges so this is
mobile I is an Israeli company started
by Amnon shot shows a student a postdoc
of mind and it's giving vision to the
car using machine learning so that's a
very difficult task of course and very
successful at it this of course was
based on machine learning old work 20
years ago in at CMU for instance Dean
Pomerleau in my lab from which this
sequence come this was a system trained
with images of pedestrians working in a
Mercedes in Alma to detect pedestrians
and of course at the time were very
happy about performance it was only one
error a very free sec every free frames
that means 10 arrows per second so
completely unusable whereas mobile I
today as a about one error every 30,000
kilometers of driving so that's 20 years
now my main point is all these kind of
advances started from suggestions coming
from actually neuroscientist as you
probably know it was from the work of
Hubel and Wiesel in v1 primary visual
cortex which is on the back of her head
is the beginning of visual information
processing in the ventral stream is that
that gave the ideas of your article
models of neurons doing doc products
convolution like and then pulling for in
variance in a year article way and the
first quantitative models of such the
kind was much later why Fukushima in the
80s a modern version was H marks in my
group which is shown here which is
really more sophisticated and more
biological version of Fukushima model
and the architecture is exactly the same
as the deep learning network
today with dot products convolution like
and pulling to give him variance and
iterating this and the system work
pretty well but of course we need to
make similar advances in order to enable
the next generation of intelligent
machines between beyond feed-forward
deep learning networks and this is the
justification for this symposium
justification for our STC Center and so
our central focus is on the problem of
intelligence which i think is one of the
great problems in science together with
the origin and the nature of the
universe the origin of life and so on
probably the greatest of all and so our
mission is to make progress in
understanding intelligence that this
means understanding how the brain makes
the mind how does the brain work how to
build intelligent machines and so our
goal and i hope the goal of many of you
here is about science of intelligence
first and engineering of intelligence
second now i think is the convergence of
significant developments in various
areas of science from computer science
and especially machine learning and
neuroscience and cognitive science I
think this is what will allow us to make
progress on this great progress in
problem in science and we tried to
represent all these disciplines in the
talks of this were a symposium today now
I want to make a brief point about the
fact that I think the problem of
intelligence in general in the abstract
is ill-defined from a scientific point
of view and with scientific I mean
natural science this is a point that
Francis Crick used to the difference is
creek with david marr
me many years ago used to make that
science is only natural science is the
study that thinks that exists in nature
and so if we want to study intelligence
we really have to study human
intelligence and this is really why the
Turing test makes a lot of sense now
human intelligence is not one problem
this would be like saying that biology
is one problem it's what just won Nobel
Prize that's it human intelligence is
many problems and some of them actually
correspond to different parts of the
brain think for instance the number of
questions that any one of us can ask
about an image just a simple image like
the one shown there you can ask about
who is there what is there you can ask
many more complex questions about what a
person is thinking about the thoughts of
another person and so on we don't know
how to build the machines that can
answer all this question we cannot
possibly have supervised learning
pre-trained for all the infinite number
of questions you can ask so we want to
understand how the brain does that we
want to have models that explain the
computational behavioral level how
people dude answer this infinite number
of questions and what happens in the
brain in the circuits of the brain when
people answer there is one example of
such a question its face recognition we
don't need really engineering to
motivate this question how does our
brain recognize faces and this is the
question why we probably are closer to
answers at all the levels the level of
knowing which parts of the brain are
involved when a human person recognized
faces these are face patches in the
human book or tex that are active when
you recognize a face then we can
localize similar AMA log patches in the
brain
of the macaque this is work by dorost
sow and derek fry valve anarchism part
of our center and then you can study
with electrodes and fMRI the interaction
between this network of face patches and
record from these neurons and come up
with models we have done that recently
with village that can emulate the
performance of humans in recognizing
faces and predict some non-trivial
properties of the neurons involved like
mirror symmetry in patch al so that's an
example of the kind of problem when the
question we want to answer and the
different levels at which we want to get
these answers this of course is a much
higher bar than one Facebook or Google
have who do not care about with what is
really the circuits in of neurons that
are implementing the various algorithms
now there is more than than this kind of
purely scientific question coming from
the ventral stream work that we've been
doing and trying to understand what's
going on and the kind of computation
that are carrying out in v1 v2 v4 the
fact that they seem to be geared towards
producing representations that are
invariant to transformations we'll come
up with results which I give you only
one example which make tell us why
yerkes are better than shallow networks
so the I mentioned this briefly if we
have a function of n of D variables then
both a shallow network what in one
hidden layer like this formula shown on
the left extreme top left with the sum
of CIA this is a network with linear
rectifiers we know that such a network
can approximate a function of D
variables
which is continuous on a finite domain
bounded domain as well as you want we
also know that the hierarchical network
on the right can do the same thing but
the new result is that if you want to
approximate functions on D variable that
have this compositional structure shown
on the top right of being the
combination of computations done by two
variables at the time then it turns out
that the deep network has a much lower
VC dimension can have a much lower VC
dimension than the shallow network
giving a big advantage in sample
complexity and part of the other results
show that there are several visual
computations for which compositional
functions are important so I think there
is going to Tier II of your wire are
keys and what are the parameters there
how many layers and so on is developing
and they should have been ported not
only from the scientific point of view
understanding the ventral stream but
also for the development in the
engineering so let me skip this point
which i think is important I don't think
deep convolutional networks or other
computer vision systems are really doing
object recognition the way humans do and
this is mainly because of the very
peculiar eccentricity dependence of
everything in the winter cortex this
will take some more times or skip this
let me just finish summarizing what I
tried to tell you there are mainly two
or three messages one is that the
engineering success of today usually
comes from idea motivated by the
scientific advances of yesterday with
believe this will continue in the future
that
one of the reason but not the most
important one for looking for doing for
working to the science of human
intelligence and I gave you a couple of
example one more on the scientific side
one potentially more on the engineering
side of those kind of scientific work
and let me finish with just saying that
I think the next stage we are in the
stage of computer science and machine
learning which is labelers you know used
to have programmers now we need teams in
India Sri Lanka like mobile I has for
labeling images in order to train with
millions of labelled example modern deep
learning system sorry
okay and but I think this is still big
data is the number of labels example
going to infinity in the sense that the
more labeled example you have the better
but I think the goal the idea is to have
system that can learn like children do
mostly by themselves not only but mostly
by itself and then the real challenge is
to work with small data so the metaphor
which is actually due tues to gman is
this is an going to one let me finish
here Thank You Heather could you please
I'm just clarify focus on human
intelligence as opposed to like any
organism solving a problem be it a human
or a macaque or of Joseph illa fly or
something so I'm not saying it's wrong
to focus on humans but can clarify the
the logic behind that emphasis
so I think first of all you know just in
terms of a strict definition if you take
for science natural sides you can study
only things that exist in nature okay so
how many things are intelligent in
nature I don't know I would choose as at
least one of the best example the human
brain that's you know the strict
definition of this but the other part is
I think it despite attempts to do that
you can try to define some general
intelligence in the abstract I think the
problem is imposed is ill conditioned it
has too many possible solutions are many
ways to be intelligent and that's why
you know the Turing test is so
attractive to many people because that's
basically definition of human
intelligence that we can study it which
we can refer to
oh I think time for one more question um
well so I want to tell me um at the end
you mentioned the number of examples
going into one super was learning on the
role of unsupervised learning in that
regard can you comment on that so the
question is how to do it and I always
thought that the fact that you need the
millions of labeled examples to train a
deep neural network was from the
biological point of view a real problems
for the biological plausibility of deep
learning I now think that there may be a
way around it and the way around it
could be something like let's call it
implicit labeling or implicit
supervision you could imagine that you
have simple mechanisms in the brain of
organisms based on a small number of
simple heuristics that allows you to
label in an implicit way different for
instance images if I look in our at
Pierre Pierre Baldy here and I get over
a few seconds several frames several
images of your face I don't need to your
net to know your name in order to label
those images for training a network I
can just know if I know that you are the
same X face has not changed then all
these images are labeled with X right so
the simple heuristics of say time
continuity there is no big
discontinuities in the optical flow in
what I I see can be a ristic to allow
implicitly it's not the only one there's
just an example and so in you know in
two years a baby may get back of the
envelope optimistic calculations about
10 million images
suppose that there is it manages to do
implicit labeling of ten percent of them
it's already 1 million that may be
enough to get a neural network up to
speed so that's just one
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>