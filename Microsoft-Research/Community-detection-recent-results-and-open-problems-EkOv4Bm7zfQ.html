<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Community detection: recent results and open problems | Coder Coacher - Coaching Coders</title><meta content="Community detection: recent results and open problems - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Community detection: recent results and open problems</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EkOv4Bm7zfQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so I'll start with a quote that i came
across recently that that i found
amusing and that will give you a clue
that it will be mostly about theory a
little bit only about experiments but
i'm not taking such a hard position as
Reddington was taking and the theory i'm
going to talk about is hopefully
affected by experiments and anyhow so
this is all about community detection
and community detection is roughly
speaking the identification of groups of
similar objects within a another
population and based on the observed
interactions so typically the
observation might be a graph of
interactions between objects and this is
of course closely related with the
objectives of clustering nodes in a
graph or embedding the nodes of this
graph into some ambient space and the
motivations that i have in mind in this
context out of the following they are
mostly about recommendation
functionalities so for instance if the
graph is the observed friendship graph
an online social networks such as
Facebook one may want to identify latent
communities of users with similar
interest or profiles and use those
inferred communities to fuel the
recommendation and gene that will
suggest people potential contacts that
they want to then they want to connect
to and these contacts if they are they
have compatible taste they will act as a
suitable filters for relaying
information to those users of the online
social network another similar kind of
application also about recommendation is
when we have data such as the bipartite
graph coming from the netflix ratings of
movies by users so for whichever movie
they have rated the gay veto
one two five stars mark so we have a big
graph connecting users to movies with
marks on the edges and again we may want
to group the users or group the movies
and if we have groups of movies that are
similar in a proper sense then we may
want to recommend to users who like to
an item similar items all right so this
is the the motivation but we'll go to an
abstract version that can encompass
other applications as well and my
outline is the following I will start
off with a generative probabilistic
model for studying this community
recommendation problem that is the so
called stochastic block model and then i
will start describing positive results
on the performance of what you might
call vanilla spectral methods and
subsequently i will consider how there
are instances where people do not know
whether the vanilla method works but
actually we can save the day by coming
up with more involved methods all right
so let's start with the stochastic blood
model it's a very simple random graph
model that's been introduced in the 80s
by people working on in social sciences
as a means to model our social networks
and we have a number of other categories
and nodes and and such nodes are grouped
into categories we have a fraction of
the nodes going into each category and
based on this we we will randomly create
edges between the nodes the probability
of there being an edge between two nodes
will only depend on their their
categories so here you have the this
probability there's a B term which
captures the community's the two nodes
belong to and then we have some scaling
that is convenient for this purpose and
being the number of nodes if you ignore
if you if you look at the average number
of neighbors people will have it will be
about
the magnitude of B which might be
constant times s so that s is to be
thought of as the average number of
neighbors in this model and it's also
convenient to think about this average
number of neighbors as the signal
strength in the observation and another
thing to note is that when we have this
model and if we take the adjacency
matrix we may think of it as being its
expectation plus the difference of
itself minus its expectation so there's
a noise that is added to a very
well-structured matrix which structure
reflects the underlying blocks in the
mother okay so that's the the basic
model introduced in the 80s we may want
to augment it a little bit for instance
there's a very cheap generalization that
is convenient if we think of the netflix
price data which is to a labeled edges
and so we can just do that by saying for
each edge we had in the graph who will
then draw a label from a distribution
that may again depend on the types of
the end nodes of that edge and that that
yields the labeled stochastic blood
mother there will be another
generalization that is motivated by
experiments actually that's the
stochastic block model with much more
general types here I've been describing
the basic version with a finite fixed
number of types and we can assume an
infinite number of types so for instance
types may be taken from the interval 0 1
and then again we made raw edges
according to a kernel that depends on
these continuous types for instance
those numbers between 0 and 1 and but
apart from that it's very similar so
here's an example of the shape of how
sorry to determine this function that is
the probability of creating an edge
between nodes with the types x and y
between 0 1 and again we can do the
trick of
labeling the edges it comes for free all
right so this has been considered in
various papers last all of us has a book
on this kind of mothers but he was not
studying community detection instead she
was studying graph limits when you
consider families of graphs and you let
the number of nodes go to infinity in
once what sense do you have a
well-defined limiting object and these
kinds of models are useful to address
this question so let's move on to the
performance of spectral methods and in
particular let's start with the vanilla
version so the disc consists in the
following you have these adjacency
matrix let's say so you would just
extract the leading eigenvalues and the
corresponding eigenvectors and let's say
you take our such I ghen vectors and
then you can do an embedding of your n
nodes into our dimensional space by just
stacking the coordinates of the
eigenvectors does that correspond to
that and then you can run in this
embedding k-means clustering or any kind
of clustering that you may may like and
so here's an example of data drawn from
the stochastic blood model with four
blocks we know exactly how to how to
color the notes because we generated
them like that and this comes from the
netflix data set taking are equal to in
those two plots and so then k-means
would color things like that all right
so a first result in this context is
that we'll be able to do well using this
approach the vanilla approach so long as
the signal strength is at least
logarithmic in the problem sighs and so
here's the statement the spectrum of a
has a very special shape namely there
are a small number of eigen values that
stand out and
we can quantify that so there are order
of the signal strength ads while the
others are of a lower order namely the
square root of the signal strength and
furthermore the if you do the embedding
corresponding to this leading eigen
values then the the representatives of
the nodes will cluster according to the
hidden blocks so k-means will do
extremely well on this and this instance
and it's useful to say a few words about
why that works because i'll return to
that later on so thinking back at this
picture of the adjacency matrix having
this nice structure of a block matrix
plus a random noise matrix here we know
that this characterizes the blocks we
know that its eigen structure is as I
described so namely as other as I ghen
values in a small number Utley at most
as many such eigenvalues as the number
of blocks and so if we could get at this
matrix and that its eigen elements we
would get back at the blocks so the the
whole game then is to show that the
addition of the noise does not affect
much this picture and indeed this is the
case and this can be shown by using a
standard arguments on the control of the
eigen structure of a matrix when you add
it at Twitter matrix with small spectral
radius so the the game is to show that
there is a small spectral radius for the
noise and actually there is a body of
literature looking at these kinds of
controls namely at the the spectral
structure of noise matrices arising from
random graphs I think that the the
template for the kind of properties we
are after is given by this so-called
notion of a graph being Ramanujan which
was a term coined in the 80s and this
applies to regular graphs and graphs are
called Ramanujan when the gap between
the largest eigen values and and the
second ones is as large as possible
and as large as possible means that you
have a jump from the top eigenvalue
being all down the signal strength s
here to the square root of that so this
is a the template and it turns out that
random graphs tend to have a similar
spectral separation property so for
instance s regular graphs taken
uniformly at random have almost this
property so with high probability they
are a quasi Ramanujan if you take add
those photographs which are not regular
graphs but have an average with average
degree s if s is large enough namely
logarithmic then we have the same kind
of behavior we have a jump from all the
rest well the square root s in the eigen
values and the noise matrix
corresponding to these adjust to these
other sueno graphs has a small spectral
radius an idea of other square root s so
this is all we need we can leverage that
plug it into a model use some kind of
argument to show that this control gives
us the control we want and so this is
how you get the positive result for
vanilla spectral methods however this
will fail if the spec the signal
strength is smaller than his other one
say because then the spectral radius of
the noise matrix becomes too large and
it dominates the signal and so it's is
going to lead to a spurious I ghen
elements and it will completely destroy
the the ability to recover the clusters
with this method so if you have labels
you can do exactly the same game by
picking random numbers for each label
and so you construct so labels may be
categorical so you could transform them
into a numbers and this gives you a
weighted adjacency matrix you can do
exactly the same game and with
logarithmic cenotes friends again you
will be able to recover the cluster the
planted clusters exactly
so long as the signal is logarithmic but
if you step back and look say at the
netflix data set you will see this kind
of eigen value distribution whereas if
you take a stochastic blood model with
say four blocks you have as the theory
predicts a gap in the spectrum and so
the theory works well for this but
clearly there's a mismatch so you don't
have this nice spectral separation in
data coming from say the netflix data
price so this this is one reason to
consider general types and so let's
let's do that now so with these these
general types it turns out that so this
is again a reminder of what this model
is with general types so it turns out
that in that case we can again analyze
this spectral structure of the matrix we
are considering quite precisely namely
we can relate the spectrum of the matrix
we are considering to the spectrum of an
operator that is a derived from the
model parameters and it's an integral
operator it's got a number of nice
properties and so I won't detail that
too much but there is a theory one can
develop and one can really characterize
how the spectrum of the matrix is a
determined from the spectrum of this
operator and a byproduct of that is that
we can generate spectra that look much
more like what the netflix data suggests
and for instance we can generate power
law spectra which is a feature that was
present to some extent in the in the
netflix data set and on top of that we
can make formal claims about the
accuracy of the vanilla spectral
procedure namely we do the embedding
with let's say our eigen elements we can
characterize that
this embedding is done according to some
distance that in a sense is not very far
from a distance that is intrinsic in
capturing the geometry of G of the
problem so I will not give a formal
statement in that but I will just show
one application so using the theory we
can develop for this we can show that if
say we want to estimate the distribution
of the label that might have been placed
if there had been an edge between nodes
I and J in this setup then we can just
do a local averaging in the embedding
and we can control the bias that this
kind of procedure will will suffer so
the does does a nice theory if you like
for spectral methods the standard ones
so long as the as the signal strength is
is logarithmic and I will now turn to
the case where the signal strength is
weaker than that because we have seen
that these models these methods will
break for for smaller signal strength so
in the weak signal strength case that's
the signal strength of order 1 we have
random graphs of average degree of
constant order then we know that it will
not be able to categorize accurately
everyone because a feature of those
random graphs is that the they have
plenty of disconnected notes and there's
no way we can categorize correctly those
disconnected nodes so we set up we set
out to do something a bit less than
vicious instead we want to categorize
correctly as many nodes as possible and
so this is what this overlap metric
captures it says let's check how many
nodes we have types predicted accurately
but by any scheme we might propose and
we subtract an offset to make sure that
if we put every no
in the same bin then when this creates
zero for this particular metric so with
this metric for for characterizing the
performance of any clustering method one
might venture guess as to what can be
achieved and the gas the natural one the
naive one would be to say well if I
decrease the signal strength may overlap
okay we'll have to decrease but maybe it
will keep on decreasing until the point
where I no longer have a huge connected
component in my graph so it's a feature
of random graph that there is this
transition between the emergence of a
giant component and below and this
critical connectivity you have only
small connected components so the naive
conjecture is the following one that you
could have a decreasing of our lab till
you hit this critical connectivity point
but in fact it's not like that the
picture is more intricate and this is
what i want to tell you next as you
decrease the signal strength you will
hit zero with any method the overlap
will have to be zero even if you have a
large connected component in your graph
that will just not be exploitable
information in there so there is another
transition point above the critical
point for connectivity so let's let's go
into that so to explain this I will
describe the simplest possible
non-trivial scenario which is two
classes of equal sizes and so we have
two parameters dance one parameter
characterizing the intercommunity
connectivity and another parameter
characterizing the in also a was intron
B's inter and so physicists have
conjectured this threshold that I was
just mentioning in that particular model
they said well four parameters a and B
that that are too close by then any
method will fail to achieve positive
overlap there is just not enough
information in the data something that
was proven
in 2012 by mussolini madman sly but
above the transition this was just a
conjecture until until 2000 till last
November actually and the conjecture was
that not only one could achieve positive
overlap above this critical value but
that it could be achieved for instance
by running a belief propagation
algorithm on by running an involved
spectral method that the authors called
spectral Redemption so in may I in
November sorry I developed a proof that
I described the results of the
intermediate results of and this comes
with a modified spectral method all
right and it was this interesting kind
of circumstance where I informed the all
the people interested in this game of
the fact that proof was now available
and the authors told me well we are
nearly there and the week later they
came up with a another proof which is
very different we don't have the
embarrassment of having found the same
thing the method and the proof are
different so here's the method I'm using
true to do informative reconstruction on
this mother I'm constructing a derived
matrix from the adjacency matrix which
basically counts the number of self
avoiding paths so but we need to know
the I and J I want to count the self a
number of self averaging paths the
typical situation is that there is at
most one such paths this is when the
neighborhood is a tree like locally but
by definition it could be a modern one
such self avoiding path so here between
I&amp;amp;J you have two lengths for self
avoiding paths so that's the first thing
to do which will be a polynomial time
step if you have a sparse enough graph
and based on that you do spectral
analysis
this derived matrix and so what one can
show is that this matrix enjoys a
spectral separation that is close to the
spectral separation properties I
described earlier I la a la Ramanujan if
you like namely you have to I ghen
values that stand out what I should say
it's for sufficiently long path length
but so you have two eigenvalues that
stand out and the remaining ones of the
other essential you have the square root
of the leading one so again this
dichotomy with a square root from the
informative eigenvalues to the noisy
ones and the other statement one needs
to make use of that is that the second
eigen vector vector in that case is
positively correlated with the vector
encoding the community's you're
interested in so you can just apply
fresh holding on the second eigenvector
and this will give you a reconstruction
of the planted communities that which is
positive overlap so some experiments the
experiment confirms the theories as I
very my my parameters I see that this
overlap metric does kicking roughly at
point when when the threshold is crossed
with parameter tau equal to 1 and what I
would like to describe before I conclude
is a number of remaining problems so we
have there was this question about
reconstruction down to a threshold well
it was not known whether it could be
done we knew it could not be done by
vanilla spectral methods so we could
come up with a spectral method that does
achieve the goal down to the threshold
but there are many more interesting
questions in those models so here's one
I've talked about this toy model with
two blocks so it's natural enough to say
well what about a larger number of
blocks and we could again have two
parameters one parameter
connectivity within a blog within a
block and then another for connectivity
across blocks and in that case when you
go to more than two blocks there are
conjectures mostly so here's what we can
prove we can prove that there is a
transition above which the modified
spectral method I was describing will
again achieve some positive overlap so
it will be a successful it is known that
if you have not enough information
detection will be infeasible whatever
the method that will just be not enough
information but it is conjectured that
in this set up there is an intermediate
phase where all the known methods namely
a belief propagation and spectral
methods will fail but some more
intricate method let's say maximum
likelihood which would have a horrendous
complexity will succeed so that's one of
the remaining mysteries can one really
prove that the picture is like that so
identify these boundaries something that
is not done and moreover so proving that
reconstruction is impossible is not the
hardest part figuring out what can be
done in the intermediate range is really
the challenge here whether a polynomial
time algorithm can be identified that
would that would achieve useful
inference here's another interesting
open problem on these models so coming
back to a two community problem we may
consider a tiny community of size K
where everyone is connected to everyone
so this is the planted clique problem if
you like we plant a clique of size K and
we assume for simplicity that all the
other edges are present or absent with
probability a half and so it is known
for a while that if the size of the
planted click is above
we're root n times a constant then there
are plenty of algorithms that will
succeed very easily just looking at the
degrees of the nodes in the graph will
tell where the planted click is if you
go below a square root n nothing is
known it is known for I mean in terms of
polynomial time algorithms to identify
this planted click it is known that it
will be detectable through impractical
algorithms like maximum likelihood or
finding the largest click will will give
away the click but no polynomial time
algorithm is is known as of now and it's
yes it's not clear so people conjecture
some people like you Val Perez
conjectures that I don't know if he will
back that but he was optimistic that
there may exist polynomial time
algorithms but others take this problem
as a measure of the hardness of other
problems saying if a problem is as hard
as finding a clique of size say and to
the one-third in this model then that
problem is hard because no one has known
how to solve this one so that's that's
the last open problem i wanted to
describe so to conclude vanilla spectral
methods are efficient for logarithmic at
least regard mixed signal strength this
is something we can show alternatives
are needy that lower a signal strength
so but there are a number of open
questions for instance belief
propagation which is conjecture to the
optimal is does not come with any
guarantee we have guarantees only for
the indo hard regimes for the modified
spectral methods I was describing and
okay as I was saying it would be of
interest to to know of computationally
efficient methods for the two open
problems I was describing and I have
further questions I had to cut this list
at some point but one important one is
whether the sbm model does reflect
correctly a real-life data so we took a
step in that
direction in going for the continues
types which allows to feed spectra much
more accurately than the spectral block
model with a small number of blocks does
but it's not clear yet it will be good
to have evidence from professional
statistiche ins about the accuracy of
those models and with this I will I will
stop and take questions if any thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>