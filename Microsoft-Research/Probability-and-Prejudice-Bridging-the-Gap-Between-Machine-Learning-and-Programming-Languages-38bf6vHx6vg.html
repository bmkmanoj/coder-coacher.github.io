<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Probability and Prejudice: Bridging the Gap Between Machine Learning and Programming Languages | Coder Coacher - Coaching Coders</title><meta content="Probability and Prejudice: Bridging the Gap Between Machine Learning and Programming Languages - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Probability and Prejudice: Bridging the Gap Between Machine Learning and Programming Languages</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/38bf6vHx6vg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
can I get a show of hands of the people
who would count themselves as machine
learning types machine Oh excellent and
programming language types we have about
half and half which is exactly what I
wanted very very cool all right so yeah
I am going to talk about the gap between
our two research areas or the gaps
plural I'm not really clear on that in
the slides I guess but and how to close
them and bridge them I also want to
illustrate probabilistic programming a
little bit this this what we usually
call an intersection between machine
learning and and programming languages
and and that the gaps are actually both
technical and cultural so why because
everybody knows we have a hard time
working together and I have not seen
anybody or any classification of the
reasons why anybody compile you know the
kinds of put together you know
information on the kind of attitudes and
things that cause this our problems I've
heard that from like for people now yeah
it's a dangerous talk so I can I have a
ready participation for just a little
bit I I would like you to look to your
right to the neighbor on your right and
look to the neighbor on your left okay
thank you I'm going to mischaracterize
and possibly offend both of these people
yes but not you sir well I've been
thinking for a long time about how to
present information like this and I
think it's best done by telling my story
I was born at a very young age no we
don't really need to go that far we can
just go to the end of my master's
research I was fortunate to have an
advisor who counted pretty much any kind
of solving difficult under constrained
problems machine learning and that
included computer vision so he basically
let me do what I want that was really
cool how I was so what I worked on was
super resolution and how I did it was by
making a Bayesian model the model would
allow me to take a low resolution image
and infer a scene that could have
generated this image
using a known capture process and then
it would simulate taking a picture with
a better camera maybe a higher
resolution camera as a Bayesian model it
looks like this and the objective was
then to get a sample from I prime given
the given image I this is the math that
that describe that the model describes
here and and I still think it's
beautiful one of the reasons I think
it's beautiful is that it's utterly
reproducible the only thing that's
really kind of missing is this h
function here happens to be a blurring
function it's in the paper but I mean
given this an H and a few tips on how I
did inference you could completely
reproduce what I did it's kind of one of
the hallmarks of doing Bayesian
inference but the code the code to
implement the math was was not as
beautiful it was 600 lines of evil
Python and pythons really not supposed
to be evil looking when you're finished
with it however I got some good results
here was my main competitor I had
identified some areas where where I
thought we could do better so in the in
the and the gradients here there's some
stepping and along the edges it doesn't
look quite straight and my algorithm
didn't have a didn't have those problems
and it also beat the state-of-the-art on
some objective measures you know quote
unquote objective
I had a co-author who pestered me
endlessly about doing this so he said
you know Bayesian inference this is
really good
missing data problems and super
resolution is like a missing data
problem right you're filling in the
pixels that are missing so once you
tried CC DD mosaicing what is that so
what it is is in a consumer camera over
the over the CCD array there's a Bayer
filter that filters out light and so the
pictures that as a consumer camera
actually look like this the red green
and blue are not all in the same
position so they have to be D mosaicked
or there has to be some interpolation
that goes on to fix that and so with
very very few changes to the code and
note
changes to the model I was able to do it
and it comes up with some lovely sharp
edges and very few artifacts in painting
was another thing he suggested mark all
these things is missing that are over
the top of the image and run a very
similar algorithm and exactly the same
model and it just sticks them all back
it misses some details it just puts
plausible things in so it only looks
like magic it's not magic it is what
happens when you model something as
precisely as you can using a framework
that is tolerant of uncertainty I was
really only mostly satisfying I'm still
not sure the program was correct
for example and it's not because it was
not just because it was long it's
because every time I the model wasn't
something that I just made up one day
and tried it's something I iterated on
over and over again I think I tried 50
versions of it and every time I'd made a
change to the model I had to make a
change the code to keep them in sync and
that's just the opportunities for error
is there just rife the other problem was
I could not model things the way I
wanted to
in real life objects include each other
when they put each other there is a
sharp discontinuity the thing that makes
anti-aliasing happening the smoother
edge is is blurring by the by the lens
and we I could not figure out how to
make that happen here's the fine print
from the paper that makes it sound kind
of like a good thing but without the
fancy tuck guess we can't figure out how
to model blur as part of image captures
we hack it into the scene model is
satisfying well I heard a programming
language professor J McCarthy was coming
to BYU I sent him an email and asked him
to take me on I was kind of ready for a
change and it took me an embarrassingly
long time to come up with this idea six
months hey I could use this to fix my
own problems you say if I had a compiler
for my for a probabilistic language then
I wouldn't have to write the code right
it became clear very quickly that we had
communication issues I love this comic
now human beings I think have it better
than dogs because we can fill in using
context
however if we think we already know what
the words mean we still have some it
might lead to greater issues so Jay
would say something like the curry
Howard correspondence is only a general
observation except in the calculus of
inductive constructions in similar
languages and I would hear Indian
cuisine correlates with integration and
possibly something about making learning
algorithms wait where so all this comes
from Indian cuisine that's currying
obviously correlates correspondence
integration calculus making learning
algorithm ducted right so in machine
learning and programming languages we
use the word induction and inductive in
very different ways so I would say
something like metropolis Hastings
algorithms are inherently sequential
because they step according to a
distribution conditioned on their last
location and J would here you can't
paralyze algorithms for walking around
cities and possibly something about hair
care we write we talk past each other a
lot and this is just vocabulary so this
is where the gaps come come in our two
cultures are separated by fairly wide
gaps and our and the the the formalisms
that we use in the math we use they
separated by some fairly wide gaps too
and some of these make it pretty hard to
work together so my objectives today are
to identify the major ones to classify
them as technical or cultural and show
how to bridge or close them because I've
had some experience doing this and I
want to pass it on so first gap
vocabulary so Peola theorists are kind
of like the Inuits of north america 30
words for snow we have 30 words for
meaning and well to be fair they
actually have a language where you can
smash a bunch of words together to make
new ones that's kind of like Finnish so
these are the ones I came up with off
the top of my head now again to be fair
to us we usually post fix this with
semantics semantics just means meaning
but yeah we have a lot of words for the
thing that we do this is the major thing
that we do the word we use depends on a
lot of fine distinctions that somebody
from the outside really doesn't know or
even care
about sometimes oh no usually so usually
these days I just stick with
denotational and operational and when
that's not enough it's the meaning of
programs as the values they reduce to
the meaning of programs as mathematical
functions the meaning of programs as
probability distributions now in the
machine learning side just the
statisticians have 30 words for
distribution which is the thing that
they work with all the time are the
Bayesian z' and those are the ones I
came up with off the top of my head
there are a lot of them and again the
word that you use it it depends a lot on
a bunch of fine distinctions that an
outsider really doesn't know about or
care about so I usually just stick with
distribution and conditional
distribution nowadays and that seems to
work just fine and if I have to I get
more precise the distribution of the
parameters given the data the
distribution of the things I haven't
seen given the things I have seen now
these this vocabulary refers to even
stranger mathematics now to somebody who
is familiar with Bayesian inference this
is may not be perfectly legible but it
is at least decodable see as soon as you
see that little tilde right there you
can start working out what it is and the
only thing that might be a little odd is
this which is a potential I was mixing
Markov random fields with generative
models this is the kind of thing you
would see from programming languages and
if you are used to reading denotational
semantics this is again decodable the
only kind of odd thing is that that
these are defined in terms of a narrow
calculus which is not a typical thing to
do but it's totally allowable um to
close this gap the only real option we
have is to take some time to explain to
each other what these things are I have
some tips about how to do that so
explaining semantics to somebody from
machine learning here are some things to
avoid which are probably the most
interesting things we make semantics
about that's kind of annoying but yeah
okay lambdas continuations type systems
no avoid philosophy do not say this
sentence okay programs mean nothing
until the language has a semantics
mathematically strictly speaking yes
that
is true but if you say that to somebody
who's been programming for 20 years and
thinks they have a pretty good handle on
what their programs mean then the rest
of what you have to say is not going to
actually go through their ears and into
their brains do use arithmetic draw an
intuition familiar things at a
non-trivial language feature I know that
I've already disallowed lambdas
continuations and type systems but there
are some non-trivial language features
you can use and emphasize the need for
an explicit mathematical model so for
example why do Bayesian this is so this
is part of explaining semantics to
somebody for machine learning why do
Beijing's create models of processes
uh.well reproducibility is one we should
be able to reproduce each other's work
that's good science formal manipulation
you might want to take that joint
distribution and you know move some
parts around proof of properties you
might want to prove that two parts of
your model are independent and therefore
you can do inference on them separately
and then combine the results okay so why
do PL researchers create models of
languages exactly the same reasons we do
slightly different things well not
slightly different things we do
different things with them but for
exactly the same reasons this is some
common ground we can exploit we both
believe that explicit is better than
implicit if you name something and give
it a mathematical model you can
manipulate it so a semantics is a
mathematical model of a language so
here's our language we'll start with
starts with a grammar this is just BNF
this is something that as computer
scientists we all are familiar with our
language has the natural numbers in it
which is not a very interesting language
yet so let's make it a little more
interesting
we'll add addition which is still not
that interesting but we'll make it
interesting in a second all right so
what should add for five mean uh it
could mean like four minus five right
okay you probably don't want to do that
but it's allowable in general though we
have two main options an operational
option we can say that add four five
what it means is how it computes
that's an operational semantics it's
like sitting down at the Python repple
typing four plus five and getting a nine
or we have a denotational option and
that is to turn our program into
mathematics of some kind and then we can
reason about it using mathematics that
seems like a fine distinction right now
it's going to get less fine in a second
so as soon as we add something a little
more interesting to our language
non-deterministic choice choose just
means choose between these two things
what should choose 10 20 mean again we
have two main options an operational one
either 10 or 20 it could be the faithful
implementation of this semantics decides
that it's 10:00 on Tuesdays and 20 on
every other day we don't know we're not
modeling how it makes the choice we have
a denotational I'd approach we could
take and and here's a good denotational
approach it could be a mathematical set
containing all of the all of the
possible outputs for this program we're
going to go with that to define a
denotational semantics we define a
semantics function and already it looks
a little bit weird it's got those double
bars you know but it's just a function
it's an out fix function like like floor
and ceiling and absolute value you you
stick something inside of the bars in
order to evaluate the function on some
value and it just happens to be a
function that operates on syntax and
gives us the meaning of that syntax what
should its return type be well here's
some guidance we like uniform
interpretation Bayesian models we always
interpret them as a joint distribution
if somebody came up to us and said
here's my Bayesian model and it means a
prime number we'd be a little bit
worried about that person so we should
pick a uniform representation in
particular we don't want the meaning of
10 to be a number and the meaning of
choose 10 20 to be a set what we can do
is make the meaning of 10 be a singleton
set and this will work just fine
so let's define the semantics it is yes
say element of two's 1020 in the set
1020
uh let's see which side of you from mo
appeal this could make a difference
general computer song general computer
science so why didn't I say in 1020 we
are really fond of equality's they're
easier to reason about them membership
with peaks of 10 and 20 yes you couldn't
advance well if we did a distribution
then we would be saying something about
the relative frequency with which things
are chosen and we are not saying
anything at all about how they're chosen
because we don't know does that make
sense
you could totally do that and I'm in
fact that's kind of one of the reasons I
like this example because you can easily
imagine extending it with probabilistic
choice instead of non-deterministic
did I do okay answering your question
well there has any of the standard
meanings of equal or as I think in would
have the standard meaning we can talk
about this offline afterwards and you
know try to get it sorted out or it
couldn't be that if I show you more than
it'll just magically click or something
so we go on okay
we need to know the expressions meeting
it soon you want to know the whole set
1020 maybe at the end of okay all right
so well maybe if we define the function
it will help
so here's its type it takes expressions
as inputs and then it returns a set of
natural numbers so this is the meaning
of programs as the set of possible
things that can return it's defined by a
few rules so these are definitional
equals you know if just the meaning of a
number is a singleton set containing
that number because that's the only
thing that could evaluate to if we made
an actual programming language out of
this to choose between a couple of
things means that we take the things
that they could return to us and make a
Union out of them addition is a little
bit more complicated we have a nested
Union in here it's basically just a
doubly nested loop so we loop over the
things in or the the things in the
meaning of e1 and that things in the
meaning of e2 and add them together and
then collect them all and it set some
test cases let's test it yes 10 the
meaning of 10 is a set containing 10
just like we wanted the meaning of
choose 10 20 is the Union so all we do
is copy this down from the semantics
from the definition of the semantics so
we copy it down substitute U 1 and E 2
and then we we can substitute further
and then just manipulate it till we get
a set of 10 and 20 addition coffee the
definition and then inside here we still
have the meaning of 4 in the meaning of
5 so we'll substitute those definitions
to get the set containing 4 in the set
containing 5 and now it's just
computation to get the second 89 ok
there is semantics in 5 minutes well is
denotational all right so our language
actually has an interesting property
that's kind of difficult to find in
other languages and that is a
distributive property if we add 4 to
either 10 or 20 we'll get either 14 or
24 which is the same thing we'd get if
we chose between adding 4 and 10 and
adding 4 and 20 in general
for any program II 1 e 2 and E 3 this
distributive property holds addition
distributes over non-deterministic
choice now it's easy to see that using
numbers it's a little bit harder to see
that just kind of Intuit it if you're
thinking in terms of any possible
expression or any possible program you
could write in this language which is
why I like this but you can prove this
using a semantics it's possible also
what if you added a new kind of
expression to your language is it
possible to falsify this property is it
possible that this would no longer hold
any pl people want take a stab at what
you might do to this language expression
of the same yeah so all right exceptions
or side effects of some kind right ok so
I I was thinking in terms of side
effects so a side effecting operation
like a print maybe
oh yeah that's true
hmm ok different side effects I was
thinking in terms of more like a
mutation or something because if you use
this if you use this rule in a in a
programming language that has side
effects the side effect would happen in
two places and then you know your
property would no longer hold ok reasons
I like this example it's easy to imagine
implementing it's a stupid language but
it's easy to imagine implementing it it
only draws upon knowledge of grammars
arithmetic and sets and maybe you have
to get used to the funky meaning of
notation the distributive property is
just not obvious enough that you'd think
yeah maybe I should go to the semantics
so check it out and see if it holds and
it shows that semantics aren't just for
running programs sometimes we want to do
something else we want to know something
about a program instead of running it
and we can make a semantics to do that
and it's also easy to imagine replacing
queues with probabilistic choice ok
so how do you explain probability to
programming languages people this is
something that I've struggled with a lot
okay because I have mostly been
publishing my papers in programming
language conferences so things to avoid
all your favorite stuff right don't
don't use these okay again philosophy do
not say these words people in
programming languages they have a very
different idea of what it means to
believe something than people in machine
learning so an idea that some
probabilistic law could prescribe what
you should believe about something it's
actually a little bit yeah okay don't do
it zero probability conditions it means
you have to talk about their I have
actually been called out on
philosophical issues with this and and
about how you derive Bayes law for
densities from Bayes law for for for
probability probability mass functions
it's some it turns out that you can't
without an extra axiom it's very
interesting
so anyway avoid do use coin flips and
uniform distributions everybody
understands flipping a coin and
everybody who's programmed has well
probably is used a programming language
with random choice in it where you get a
number from zero to one illustrate with
physical processes which we all have in
common and draw upon intuition for area
and volume to explain what probability
is so here's an example that I gave in
the context of probabilistic programming
languages at ESOP earlier this week
here's a probabilistic program not a
very good one it just flips a coin and
it names the result of it X and then
returns X oh okay
well so with half probability we get
Thomas Jefferson's head and with half
probability we get his tail and then
it's not very interesting so let's flip
another coin so and we'll call it Y and
because we're in England we'll flip a
pound coin and with probability 1/2
we'll get the queen of England's head it
was probability 1/2 we'll get her tail
except I think she only has a tail in
some episodes of Doctor Who
no that's the Royal coat of arms so I
have arranged the outcomes of this
program conveniently so that they take
up the same amount of space on the slide
and this and you can use that to think
about or to think about the that
corresponds with their probabilities
okay so each one has equal probability
of happening this is not an interesting
program again in particular x and y
don't have anything to do with each
other they're independent so let's
introduce some dependents we'll make it
so if we flip the first coin and get
heads then we'll flip the second coin
we'll flip a fair coin for the second
one and otherwise we'll flip an unfair
coin that has a point three probability
of being heads and our output space now
looks like this
now critically even though we've changed
the program and done some weird things
introduce independence it still
satisfies something called the law of
total probability which is if you add up
all these areas you get one always the
probability of getting the queen's head
is the sum of the probabilities of all
the outcomes in which you get the
queen's head that's pretty
straightforward now here's something
that Bayesian statisticians love to do
they love to restrict their attention is
one part of the probability space and
reason about just that one part okay so
for example we might say given that we
saw the queen's head what's the
probability that we've got thomas
jefferson's head now because we're not
dealing with the entire probability
space in order to answer this question
we need to renormalize
and so we divide by the probability of
getting the queen's head and we can
answer this question and it just so
happens to not be 0.5 it was a the
original coin was flicked with
probability 0.5 but by restricting our
attention to part of the probability
space and asking the question we get a
different answer or in the bayesian
philosophy by observing that we got
heads for the second coin that tells us
something we learn something about the
first that's Bayesian inference here's
another one of my favorite examples to
illustrate probabilistic reasoning so
stochastic ray tracing stochastic really
this just means that we're doing it in a
physically based way we're going to
simulate physics so imagine we're
were in a room with a single light
source and it's just sending photons
everywhere in random directions
uniformly random now imagine the
percentage of them that actually go
through your pupils that's very small
it's vanishingly small and but those are
the ones we're interested in those are
the ones that make a picture on the back
of your eye that your brain can
interpret so we really only want the
photons that go through an aperture
which is an event that happens with some
very very small probability now we
actually want this aperture as small as
possible too because the smaller it is
the more focused our result is going to
be we really like to do something like
this where we only simulate the ones
that actually go through the aperture
this is what a stochastic ray tracer
does then it sends them all back onto a
virtual screen and collects them to form
an image now it's critical that when we
filter these things out or that when we
when we sample in this way that we
maintain the probability the original
probabilities in some sense of of the
original photon paths because if we
don't we'll get garbage or we'll get
something that's worse than garbage have
you ever actually watched this I I
couldn't get through it all okay so what
I like about this this example is that
it's visual it draws on intuition that
we have about physical things it
motivates conditioning why are we
restricting our attention to this tiny
part of the output space well because we
want to form an image it illustrates how
rare events make things hard it's really
easy to imagine that oh wow we only want
that vanishingly small proportion of
them yeah that is hard and and sometimes
we want the that probability to be as
small as possible it shows why the
conditional distribution matters and not
just the values that get assigned
probabilities and motivate sampling
methods it makes them look like the
obvious thing to do which sometimes in
explaining probability to people in
programming languages is a hard thing to
motive
it also motivates probabilistic
programming languages hand coded
stochastic raytracer as thousands of
lines of code that don't look anything
like the physical process that it is
simulating it because that constraint
the photon path must pass through the
aperture is weaved in through the whole
thing it's kind of like it like infects
the program and from a personal
standpoint it allows me to distinguish
my own work because as far as I know I
have developed the only probabilistic
programming language in which you can
actually code it up that way or in a
nice way so in dr. baize which is the
one I am developing it's still just a
simple physics simulation this is the
most complicated part of dr. baize but
not dr. baize the most complicated part
of the simulation the rest is basically
a vector math library in other
probabilistic programming languages it's
either not possible or it's just as hard
as in a general-purpose language in dr.
baize you just say this is the physical
physics simulation give me all the
photon paths that go through the
aperture and in other ones you can't do
that now my first probabilistic
programming language no no I got stuck
way earlier on the simplest things
imaginable max point five random this is
my favorite example of something that's
difficult to do but you it seems like
you should be able to do this right it's
not because that people who implement
probabilistic programming languages
aren't good at it I know these guys they
are very smart but it's the reasons
we're almost entirely theoretical let me
explain why so we have a random function
that we can call and because of course
we're doing a list P language you put
the parentheses on the outside and it
returns a value uniformly in between 0
and 1 now to quantify the the
uncertainty here to give it a
distribution we can use a density
function now the density function
doesn't give us the probabilities of
outputs because if it did then the
probability of every output would be 1
and if we tried to sum them all together
we would get infinity and we're supposed
to sum to 1 according to the law of
total probability what a bin city is is
a change in probability it's like a
derivative and so we have to integrate
to find actual
probabilities if we want the probability
that random returns a number between 0.5
and 1 we need to integrate its density
from 0.5 to 1 and we get 0.5
now critically if you try to integrate
it on a single value you will always get
0 because there's no function you can
integrate between a and a to get
anything but 0 is just how integration
works so let's look at max point 5
random it looks a lot like random on the
right side as it should and on the left
side it never gives us anything between
0 and 0.5 so there's no density there
but what about at point 5 so what it
does is take all of the this this
function you can think of it as taking
all the probability between 0 and 0.5
and piling it on to one point and so
what we'd really like is is this we
would like the probability of max point
5 random to be point you know on this
singleton to be 0.5 but we know that if
we integrate any density from a to a we
get 0 and so this density can't exist
ooh okay
well here's our first technical gap we
have a limited theory probability that
we typically use when doing
probabilistic modeling yes it's not
programming obviously talk to you like
you're a program I'm not sure what you
mean can you ask a more specific
question exactly is the problem again
with the v-max 0.5 okay so max 0.5 it
takes anything that's below point 5 and
makes it 0.5 okay so it's a real number
from 0 to 1 nothing max 0.5 random land
another thing max 0.5 random yeah okay
that returns a real number between 0 and
1 right ok so oh oh right it's so it
depends on
that we're thinking operationally or
denotational II if we're thinking
operationally it returns us a number
we're thinking in terms of an
implementation if we're thinking
denotational in terms of mathematics the
it means a distribution yeah many many
times
yeah and half the time you get 0.5 and
half the time you get something between
0.5 and 1 good question okay so the the
okay so the the strict mathematical
answer is that those functions are just
a formal device and they don't actually
exist it would be really interesting to
try to formalize them I can give you
better motivation than that though on
this slide in fact well well let me let
me show you these other problems first
okay and we'll see what other things
densities can't model so I talked about
discontinuities and this is very
interesting to me that that if you're
doing if you're Bayesian and you want to
model a thermometer and the fact that it
can't show you anything below zero and
anything above 100 you're kind of hosed
that's that's fascinating variable
dimensional things so if you have
something that says flip a coin and if
it's hit heads know if it's true Wow my
coin flip got different if it's true
return a list with one random thing in
it and otherwise return lists with two
random things in it there's no single
density function that describes the
distribution of this so the Dirac Delta
function wouldn't help you with that
another thing it wouldn't help you with
is infinite dimensional things there are
no density functions for infinite
dimensional things in general
no non-trivial ones and that makes that
makes it really hard to describe the
distributions of streams and when you
get into the guts of making a
programming language recursion or
unbounded loops in general densities are
just insufficient to model the out
puts of probabilistic programs and you
have to restrict your language quite a
bit to to use them there is a theory
that handles everything but it's
extremely general and when I went to the
so I went to the the math department to
take courses on it measure theory and
they never computed anything well
they're mathematicians that would be
dirty so there it's really hard to find
any computational content in measure
theory and I actually spent about three
years trying to find it before I finally
found some that was that was effective
but we need to talk about it because
this is this is the only way that I know
of and really anybody knows of right now
to get around this problem of density is
not being expressive enough so a
probability measure is kind of like a
density that's already been integrated
for you so let's take the measure of
random the uniform measure between zero
and one so it Maps subsets of the unit
interval to probabilities and it's
defined this way so we can define it
using just intervals and counted as
defining any possible pretty much any
possible subset of the unit interval
that we could write down we can define
it using integration but we don't have
to because this is a measure this is a
primitive concept the measure of max
point five random is this it actually
exists and if you squint hard enough you
can see where it comes from this max
point 5b - max point five a that is a
lot like B minus a it just happens to
all happen above 0.5 but in particular
this is the interesting part this term
here assigns probability 0.5 to that
singleton point 5 which is the thing
that the density could not do for us
so to bridge the gap this Technical gap
we need to derive measures from code and
then somehow compute them here's how
I've chosen to do it this is the way I
finally discovered so we're going to
interpret max point 5 random is a
deterministic function that takes a
random source and outputs a value and
define it this way so this is our random
source it's a uniformly random number
and the function is computed in kind of
the obvious way
so Haskell programmers in here
anybody program in Haskell okay this is
just like the random monad okay there's
no different except this time instead of
a stream of random numbers or a random
generator we have just a single
uniformly random number just for
simplicity's sake to compute to derive
the measure of max point 5 random we can
do this simple thing well maybe it's not
that simple
it's defined in terms of the probability
measure over the input space and we
compute this after the minus 1 of B
which is a pre-image
a pre-image is all the points this will
give us all the points in our input
space that would produce something in
our output so in other words to get the
probability of a set B we find all of
the inputs all of the random sources are
that when we apply F to them would give
us something in B this can be factored
into nice parts it factors nicely into a
into a random part and a deterministic
part and one thing I really like about
this is with PL people this means that
if we concentrate on this we can
entirely forget about probability it's
all just sense so in other words we can
compute measures of expressions by
running their interpretations backwards
on possibly uncountable sets which might
seem like a tall order all right well
what about approximating so here's a
pre-image that comes up in some of my
tests this is if you really want to know
it's the normal normal model with a
circular condition which i think is kind
of fun it looks like a football it is
not linear in any way so linear
approximations are going to do very well
but we're gonna try some kind of
approximation anyway what if we
approximate with rectangles this is a
sound approximation of the preimage and
it is not helping us very much there's a
lot of empty space that this covers and
so it's measure for its area would be
very different than the area of the
actual parameters that we're trying to
measure and if we sampled inside of it
we'd mostly miss the preimage but what
if we cut it in half we could cut it in
half and then restrict the
interpretation or program to one half
and then the other and compute preimages
just on those half
it would shrink and then we could I'm
gonna call that refinement we could
split and refine and split and refine
and split and refine split refine and
it's getting closer and closer this is a
fairly good approximation of the
preimage if we measured it they would be
pretty close now this doesn't scale very
well so in Bayesian inference we like to
have you know a hundred thousand random
variables and this is an exponential
time algorithm to to one hundred
thousand is not good but there are
techniques of scale like sampling
techniques so what if instead of
enumerated of these what if instead of
splitting and then choosing both halves
we just chose one of the other with some
probability then we could sample from
them and from inside each of those we
could sample a point if we did that
enough we would get samples just inside
of this preimage hmm okay this brings up
an issue
this brings up a cultural issue a pretty
big one um sampling is difficult to
swallow all right so early on when I
joined Jays lab he asked me to just
write something I was taking a long time
learning learning measure theoretic
probability and and he needed me to
write a paper just so we could get some
practice and so you could see what I
wrote what my writing was like and so I
chose to write on probabilities the only
good way to model uncertainty and how
sound approximations don't scale but
sampling does and his review was
basically this I had run headlong into
some philosophical differences between
us our ideals and motivations differ
significantly between programming
languages and machine learning in
programming languages five-nines is not
enough we must have complete certainty
in machine learning usually 99%
certainty isn't feasible okay you're
lucky to get 95 programming languages
only sound approximations are admissible
and we have to provide guarantees and in
machine learning they've learned from
long experience that logical reasoning
doesn't scale the real-life problem so
we can't provide guarantees in
programming languages correctness on all
inputs is a desirable and achievable
machine learning perfection one domain
usually implies poor generalization see
the need for bias in learning
generalizations by Mitchell the problem
is all of these of our statements are
true in context in our own specific
research domains they are all true we'll
look at the contexts so here's a silly
example I came up with let's say that
you were writing a language for writing
creating a language for writing end-user
license agreements I'm not saying this
is a good idea I'm just saying it's
something you might do alright so you
can have named entities in it such as
the software and Acme corporation and
and it's based on well-known research on
contracts by findly at all the compiler
creates Damons that answer questions so
or yeah so anything on the system might
ask the question like am I allowed to
copy this file does it violate the
end-user license agreement am I allowed
to send this like the network stack
might ask him am I allowed to send this
personal data to Acme corporation now
the query language has to be
meticulously defined so that you can ask
very precise queries and soundness means
always denies a contract breaking action
it might deny something that that is not
a contract breaking action but you're
really after not breaking the contract
and then it becomes used as a target
language for compilers for all kinds of
human contracts like rental agreements
and marriages and then people installed
on their phones and the dusters and the
big brother apocalypse so this is what
our research context is like in
programming languages we work on
well-defined problems and if our
problems right if our problems are not
well-defined we add definitions until it
is a well-defined problem our languages
tend to be used in the middle of a giant
pile of abstractions like this end user
License Agreement language became a
target language for other kinds of
things JavaScript is now a target
language for compilers which is a little
bit scary actually so I mean how do you
keep this big pile of abstractions from
falling down hmm okay
so because we work on well-defined
problems we can get correctness and
because our languages are used in this
big pile of abstractions to keep them
from fall
down we have to provide guarantees to
the things above them all right let's
look at the machine learning version of
this we're gonna create a classifier
that determines whether actions violate
Acme's and years of license agreements
in particular so we could use keystrokes
as data sources harddrive contents rank
processes the webcam we could use some
recent research on emotion recognition
and faces to see if somebody's doing
something sneaky and of course we need
examples of users violating and not
violating the end-user license
agreements and what was going on in the
computer so we can train the classifier
we would hand write a watchdog daemon
that tries to determine when the users
were breaking the contracts and it's
correct whenever it reports a real
violation the system is clearly AI
complete and that's dangerous so here's
the research context for machine
learning ridiculously under constrained
problems they are it's amazing how under
constraint problems you guys did you
know I'm machine learning too that we
tackle okay
the algorithms get used near the top of
the pile of abstractions not in the
middle of it and so correctness is
almost never achievable but we can make
some strong assumptions that give us
good performance and we can tolerate
errors because our stuff is used near
the top of the pile so now this is one
of the most difficult gaps to close I
think because whenever you do research
in probabilistic languages you need to
satisfy both sides and they're both
pulling at you with their ideals so the
programming language ideals that you
need to provide guarantees and the
machine learning ideals that you have to
provide approximations of scale are both
kind of difficult to meet at the same
time right so here's how dr. baize does
it well if satisfied or dissatisfied
both sides bye-bye
you know compartmentalizing them so at
the very bottom there is an
interpretation of function of programs
as pure functions that operate on random
sources and return some value and this
is assumed correct this is our baseline
built upon that there's a pre-image
semantics that interprets programs as
functions that compute preimages and
this is proved correct with respect to
this semantics and in other words if you
these two semantics the same program
then this will return something that
computes preimages under what this one
returns now because these things are
horrendously uncomputable they need to
be approximated I use rectangles
currently so there's another semantics
that outputs things that compute
approximate preimages and this has
proved sound with respect to the correct
semantics so that yeah I'll show you why
in a second now in order to implement it
I needed a rectangular set library a way
of doing something like interval
arithmetic but in Reverse and this is
also sound uh and then we draw a very
dark line that we do not cross it is the
soundness barrier and on top of the
soundest barrier or on the other side we
have sampling algorithms which converge
in probability this is the best way I've
found to resolve the tension there are
guarantees on the bottom exactly the
kind of guarantees that we like to see
in programming languages and there's
there's scalability on the top okay
so motivating soundness to somebody in
machine learning is actually pretty easy
you just need to show them something
that it enables that is useful so so dr.
raises way of approximating preimages by
using rectangular sets and then sampling
from the or sampling from them and then
sampling inside of them is one way to do
that convergence of the sampling
algorithm requires that the
approximating prima semantics be sound
if it weren't sound there would be
places inside of the preimage that it
might miss there'd be places that would
never get sampled from and so the
sampler wouldn't converge motivating
unsoundness is somebody in programming
languages I think is significantly
harder it's harder to get somebody to
loosen loosen they're the word I'm
looking for their standards so in fact I
had this conversation with my advisor
just last week I asked him when if every
accepted sampling is a viable way to
represent the the meaning of programs
and he gave me this answer I actually
call this a win actually I I don't think
I will ever convince him that sampling
is a
worthwhile thing to do but he trusts me
and and that's okay
now even with somebody like Jay who will
probably never accept sampling you can
start small there are people out there
that want this sampling stuff it's kind
of weird but this is what they wanted we
can give it to him yeah yesterday likes
quick checked that's the thing he likes
fuzz testing and randomized testing oh I
I think it's I'm not sure the why that
is I need to nail that down all right so
now we get to the difficult part and
that's when we have to talk about
prejudices preconceived opinions that
aren't based on reason or actual
experience they are really easy to
develop and all you have to do is take a
good idea or a good attitude or good
anything in one context and apply it
without conditions in another so for
example it's really easy to start with
we provide we must provide sound
guarantees and go to sound
approximations are the only useful
approximations and it's really easy to
go from I know what my programs mean
well enough to write great programs to
semantics are unnecessary these are not
good implications we can't actually
conclude them but we do because I think
we over generalize it's just easy to do
how do you address it I have not found
great ways to address these yet sound
approximations the only useful ones
there is overwhelming evidence against
this at least at the top of the
abstraction stack not in the middle in
the middle you must have provide
guarantees of the stuff above you or the
entire thing falls down I used to have
floating-point on this slide but then
somebody at the last place I gave this
talk to you said no we don't believe in
floating-point and most of them agreed
that was very interesting so semantics
are unnecessary to make a probabilistic
language this one is probably a little
bit easier
start with reminding them of the
paradoxes statistics is full of these
paradoxes these weird unintuitive things
that happen that when you dig down into
it you could figure them out but until
then you're like whoa why did that
happen this is the conclusion I make
from that if you iterate on a
probabilistic programming language
till it never surprises you anymore
you're probably doing it wrong it's got
a it's got errors in it somewhere
because statistics should surprise you
occasionally alright so in fact I've got
a new one for you this this one's fun I
discovered this while making dr. base it
doesn't allow zero probability
conditions I am almost finished so it's
good you have to use intervals instead
of equality constraints so here's a
standard normal normal model with two
observations so we have X distributed
normal 0 1 and y 1 distribute normal
centered on X and another one centered
on X and we condition X on this being
true whoa y1 has to be between something
near to and something else near to so y1
has to be near to and y2 has to be near
negative 1 so that's like saying y1 is
equal to 2 I want y2 is equal to
negative 1 except with a small interval
now these epsilon epsilon 1 and epsilon
2 control the width of the intervals and
it seems like they should at least be
proportional to each other or something
or they should be equal but it turns out
that doesn't matter at all that's weird
to me as a statistician I would expect
the narrow interval to make more of a
difference to be weighted higher in the
evidence or something so here's a true
density for it for this model the the
true posterior density and so with
epsilon being point 2 and epsilon 2
being point 0 1 the estimated density
from the samples is almost exactly the
same both ways and if we look at the
preimage space they look different like
on the right side it's wider on this one
on the left side it's wider on this one
but they give us a pretty much the same
distribution of samples on the other
side width matters a little as long as
you get it small enough but
proportionality doesn't really matter at
all
that's strange ok the general property
is actually due to something called the
big differentiation theorem you can go
and prove this oh it's just kind of nuts
all right
so speaking of sampling here's a
technical gap probabilistic program
distributions are weird compared to
distributions you encounter in regular
machine learning they are strange so for
example one point on the
support of the program the distribution
of program domains is an infinite tree
in dr. Bates this is its source of
random numbers how do you sample from a
source of random numbers especially when
you're doing Gibbs sampling Wow okay
the current inference algorithms are
developed for very different
distributions nice rectangular or you
know Cartesian product D kind of things
on the other hand proper list of
programming languages can provide a lot
of extra information to help these
things out but algorithms is one place
where there is a big gap that we need to
fill to make our probabilistic
programming languages work so in summary
when I move from machine learning to
probabilistic languages it was a big
culture shock because of the cultural
gaps vocabulary formalisms ideals
motivations even prejudice made made it
very difficult to communicate to get
along to collaborate and these gaps keep
us from working together as people who
do machine learning and probabilistic
languages but they can be closed and
bridged probabilistic programming
languages are hard to create because of
technical gaps but you know limited
theory probability I didn't get to talk
about limitations in programming
language formalisms that i've had to
overcome but i did talk about our
existing sampling algorithms and how
they kind of struggle but these are the
gaps that are the opportunities for us
to do foundational research and if we
can close the culture gaps we can work
together and make this happen
thank you thanks so much Neil for a very
interesting talk very insightful I think
we've got time for one question because
of another talk coming in at 2:00 right
after this yes if I can use machine
learning terms your posterior has some
sort of limited support right says
there's only some possible regions of
your original parameter space that could
lead to the observations well that's one
in one case I'm interested in in general
I'm interested in the cases that you
can't model with the density which are
were you here for the beginning of the
talk yeah
if I'm asking about your rectangle
strategy and - you've sort of drawn this
limited region with oh well support this
in the posterior but in most problems oh
actually in most problems it's still one
little tiny stripe it just happens to be
along an axis and you can forget about
the other dimensions the ones that you
observed so it's still very sparse
logistic regression that's not going to
be the case yeah we can talk about that
afterward I can draw some pictures and I
would make more sense I can't draw
pictures right now great let's Knight
mule again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>