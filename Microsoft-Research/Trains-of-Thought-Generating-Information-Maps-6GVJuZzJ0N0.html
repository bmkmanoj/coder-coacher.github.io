<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Trains of Thought: Generating Information Maps | Coder Coacher - Coaching Coders</title><meta content="Trains of Thought: Generating Information Maps - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Trains of Thought: Generating Information Maps</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6GVJuZzJ0N0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
it's an honor today to have definition
half with us Daphna is finishing up a
PhD at Carnegie Mellon University
working with Carlos gastrin memo aplomb
she did her bachelors work in Tel Aviv
University her master's at UIUC and she
has many interests and and directions
but ended up focusing in her in her
dissertation work on methods to help us
deal with the large amounts of
information and to visualize it and
understand it going beyond lists of
search results for example in navigating
the web and in driving information from
the web into a richer stories fabrics
and maps and she has one base paper
awards k DV 2010 has been microsoft
research fellow that we're proud of to
have our our label for him I guess and
also a siebel scholar so I'll just turn
it over to Taft nuh who's talking about
trains of thought generating information
maps thank you ok first of all can you
all hear me at the back a flat so my
name is Daphne chef I'm from Konya man
and I'm indeed going to talk to you
about my recent research project called
trains of thought generating information
maps so was this project about let me
tell with my new favorite quote the
abundance of books is a distraction this
was said by seneca who lived in a first
century okay now a lot of things have
changed in the first century but senegal
film has only gotten worse and evolves
in the numbers right here are just some
of them so first is the google estimate
of the number of books outer and now i
have no idea how they got that specific
resolution also the number of blogs
skyrockets and even if you just look at
scientific publications right pubmed has
19 million papers and Eddie one by the
minute Scorpius has twice as much now I
was browsing the internet looking for a
figure she's on the slide one of those
explained
actually growth figures you've all seen
them and I came across this one and just
had to share it with you okay so this is
from a paper from the 80s x-axis is a
timeline solid line is the number of
papers about some topic that they found
interesting but what I really like about
this figure is the dashed line ok and
the dashed line is what they call
innovative papers now I think the water
trying to say here politely it that the
number of favorites grows exponentially
the number of papers Wars reading not so
much ok so hopefully you're convinced at
this point there's a lot of data out
there now suppose you want to learn some
complex topic might be news like Youker
by their financial crisis in Europe or
it might be some research area that you
want to start looking into so where do
we go now most people I know their
answer would be we go to a search engine
right we all love search engines but
sections are really great at retreating
nuggets of knowledge but in all seven
trying to show you how those what 30
million results seeking together what's
the big picture ok and the events of
systems in the past trying to summarize
and visualize complex topics for example
news junkie that came out of here but
usually try to construct a story line or
a timeline and I'm going to claim that
this simple stuff some real
summarization only works for really
simple story that are linear by nature
right and real stories are nothing like
Lennar it is spaghetti into branches and
side stories and intertwining narratives
like if you just think about research if
I had to come up with a picture for to
research is like it wouldn't be aligned
to be more like something like this come
on i'm sure you all know the feeling
right one day I'm going to make this
screen turn on and then i'm going to get
my dissertation anyway so you're dealing
with this type of messy tangle so what
do you do let me show the inspiration
the holy grail how many of you have seen
a few maps before ok i haven't seen it
until not too long ago do so this is an
issue map it's a set of seven poachers i
first stumbled upon dometic corridors of
CMU and this one specifically charts the
big old AI debate can computer sync
again let me just zoom in and zoom in
some more miss h note in this graph is
an argument like machines can have
emotions and you can follow patency this
argument is supported by that argument
or its disputed by that argument can you
supposed to just sit there and read it
beginning to end and understand the big
picture can I first stumbled upon it I
was fascinated if the topic relax
earlier but reading it just made it all
fit nicely in my brain okay and then I
started reading a bit about how they
made it and turn out they need about 20
men years to generate them so the next
question was hey can we actually build
automatically just think have one if you
map for every query you want when I
that'd be nice so I don't know how to
contract issue maps yet but in this talk
I'm going to tell about the first few
steps we took towards this and our
system is called mushroom apps can the
ideas again you start with a query like
Greece debt crisis and output looks like
a metro map where each line tells a
coherent story and different clients
focus on different aspects and you see
how they intersect and overlap okay so
this is a very very simplified and
financial crisis in europe map you can
see the blue line tells us sorry how
greasy status would reduce to junk they
had to come with austerity plan to get
the belt red line here is how are they
had all those processed and strikes
because dogs are your plans and you see
how the both lines intersect at an
article about austerity plans motivation
clear sort of good so how do I do that
yes this simplistic for example protests
oh so this is just for you each one of
your mine and so on each node actually
corresponds to an article I just
couldn't feed the title but you'll see
the real map likes later in the talk
good each node here is an entire article
yes is the linear structure of work
oh it's chronological the red line is
supposed to be slanted but I need some
great people anyway anything else ok so
again you'll see a map later and it's
all going to be much nicer so how do I
do this now maps are complex creatures
so we start with a simple prof. how do
you construct a single metro line what
makes a good line ok then we will move
to max for the news domain and finally
I'll tell you how to adapt it to the
scientific domain ok so let's talk with
lines and we tackle the spell in a kdd
2010 paper call connect the dots where
we made our life even simpler by
assuming that we know the endpoints ok
so here's the situation you want to know
more about financial crisis this time in
the US so you pick two articles they
were stars and your goal and idea that
you vaguely remember it had something to
do with your house key crisis and the
bailout okay so this is the input to the
system that would is a smooth chain of
articles that bridges the gap between
them okay for example output might look
like this so you have a chain telling
you that people board money from the
bank to pay for houses and mortgage
crisis begins to spiral because banks
rely on that too much investors want the
Congress to react bailout plan starts
rolling and finally bailout yeah this is
the type of out what we're looking for
fair enough okay so how do you go about
finding such a good chain and when asked
people this almost always their reaction
is just too short it's fast it's not a
real problem right just build a graph at
the node for each article and edges
based on your favorite similarity metric
and just find a short pass between them
or a bottleneck path or your favorite so
why isn't it good enough let me show
what happens when you actually do this
so we try to combine those two articles
one about the Monica Lewinsky store in
another about a Florida election and
recount everybody familiar with those
stories remember sorry the data set was
a bit old so let me show you what
happens when you try to connect those
two shortest path
and let me just show the important parts
here you done it read it the important
part is this jen is rather erratic okay
it goes from monica lewinsky to
microsoft to Palestinians to flora late
it doesn't make any sense but if you
look at each transition out of context
it starts to make sense because the
first two documents are back trial they
share a lot of vocabulary by judges and
lawyers and you know Jory terminology
and the next two are about Microsoft but
oh it's not for you i've been using
those slides for the last three years
okay it's in the paper from 2010 so the
point is that what you get is a few of
consciousness effect where each
transition makes sense out of context
but overall effect is there is no global
sim running crowded okay now what would
you like the chain to look like same two
articles ideally the chain would look
something like this Clinton admits the
story he's about to be impeached he is
impeached he's acquitted I'll go starts
his companion tries to break away from
Clinton because it's a messy sing
election draws near and finally election
and recant okay so hopefully you all
agree that this chain is better but why
and it looks like what we were looking
for really we call it coherent okay this
chain is more coherent than the other
okay so that's the property were after
but of course that just different across
now instead of looking for a good chin
and looking for a coherent chain em so
how do you define coherence I just give
an overview into this talkin my work in
general a huge chunk of my work is just
formalizing crafting good objective
functions okay formalizing all those
fatty terms i've been using our orders
coherence even min yes after you find an
objective you're happy with you need to
come up with an algorithm to optimize it
to find good chains and finally i need
to convince you that it works good so
crafting an objective so i decided in
order to see why this second chain was
better than the first one to look into
word patterns what you see here this is
the shortest paint a pass chain and the
bars correspond to whether the word on
the left a present article above it okay
so clyntahn appears beginning an end and
you see this stair alike behavior here
okay this means that the topic changes
with every transition the v2 documents
are related because of a different set
of four
now compare it to the second chain and
you say that everything is smoother and
nicer I transitions everything is longer
Clinton is herb beginning to end
Lewinsky they're almost everywhere and
Al Gore start showing up and keeps going
on for a while k everything is smoother
and nicer and consistent so I decided to
use this as our intuition when we
redefined cohere step back quick list of
dizzy grata so coherent chains are going
to have strong transitions we're not
giving that up but also something global
running throughout that okay so let's
away from transitions how do you do that
and perhaps the most nave way is to
start thinking about it just say well
for every transition just count the
number of force that two articles sure
but you see something here of rewards w
indicator function does war w up your
box in document the eye candy i floss
one yeah does the faith i always making
when I furthered us so when you want to
evaluate a coherent of the entire chain
you take the minimum right because the
chain is only as strong as its weakest
link now this really doesn't work and
why just take a look at this indicator
function and I'm going to claim that
it's way way too coarse it completely
ignores the importance of different
words based on Corpus level and /
transition and also it has a staff
missing words because again if the
document has the word judge and jury but
not Florida nearly implicitly their case
we had to replace it with something
software which we call the influence of
document di on document di plus ones
forward w okay what do I mean by that so
just intuitively you want to think about
this influences hi if the two documents
are related and W plays an important
role in what makes them related Kevin's
w doesn't appear in either of them
you're with me I think I lost some of
you okay
what do what yes well I guess for this
actually in the next slide so just to
tell you that there's been a lot of
influence notions in the literature but
the usual I assume that there are some
edges and in front yeah I believe in CI
plus 1 yeah now this relation became
super freak became what submit form I
thought symmetric because it's doing it
I'm very rare related that's a little
bit oh but I only competed for a
chronological order okay so di will no I
will not even compute influence if it's
after the f +1 they all change go
forward in time yeah but actually the
way we competed it not symmetric but I'm
not going to go into this yes the
beginning in the end of the dog face
is no order in the document no
documented the unit okay okay so are
they saying oh that we don't have a
single edge in our data set so we had to
come up with our own notion of influence
which I'm not going to get into but it
basically uses word co-occurrence k to
achieve those two properties okay so
yeah sorry when you say so what I was
really doing I was constructing a
bipartite graph between words and
document I was just looking at path
between ok so you connect the word if it
appears in the document or some weights
and edges and we're looking at when you
want to go from di to di plus 1 you
zigzag on this bipartite graph how often
do you need to go through w ok enough
secretion if you want I can talk about
it for a whole lot more I knew I could
have kept us back up sight anyway what
Nikhil said was corrected ok we have
strung transition border but everything
i started with john you you know you
have to have this global sing smooth
nice transitions and the thing is with
our current objective the GTA change and
shortest path chain consists go really
well ok but the important thing to
notice is that it needs a whole lot more
wort in order to get a score yeah well
the good chains can usually be
represented by a much smaller number of
words ok let's play again suppose I tell
you that I allow you to choose only
three segments ok this is a segment and
I'm going to pretend that these are the
only words that appear in the document
and then computer score ok like these
are the only words what do you do so
Ford article on the right you can go
with for example Lewinsky impeachment
and gore and still get a really good
score the Arctic the chain on the left
or there's no three segments that will
give you a good score ok precisely
because each transition uses a different
and set of four segments rather than
words the fact that next year
I do mean segment because you might have
this like document one and two are
related because 4ward and also like
three and four but not two or three so
you have this zigzag pattern okay so it
just we tried actually both ways this
work better sign instead of like we did
before summarizing all over all words
you're taking all words into account now
we all I take active words into account
the segments that we picked and we turn
this into an optimization process which
segments would you pick to get the best
score yeah and we have some constraint
on us and activations because we want to
simulate the behavior of good chain like
you can use too many words you can't use
too many words for transition and what
you just said you can have words
zigzagging like turning on and off and
on and off yeah you with me good so we
finally have a coherent definition I
told it would be a big chunk now
regarding algorithm how to actually find
a good chain bad news is that is np-hard
here all gastic good news is that if you
don't care about having binary
activations if you're okay with choosing
point five of this word over here then
it had a very natural formulation is a
linear problem i'll in our program LP
yes just before the words yeah suppose
the world is Cleveland hmm but article
is really about healing this one but
another thing is this supposed both
articles about healer mm-hmm here is so
Hill's old wife I would advocate
likewise is clear that whose wife is she
yeah you're talking about NLP groans and
they're people who spend their entire
research career on this disambiguation
thing so part of the point of this
article was to see how far you can get
using just the most basic features just
words see if it's good enough before you
start throwing in the big cannons like
like what you said about disambiguation
and reference and wife and actually
since that just words can do quite well
we also tried it with some more
interesting features like topic models
but hey if it works okay the answer your
question good so we have an LPN we have
a rounding schema and I'm just going to
tell you that we have some approximation
guarantees that in expectation we can
control the length of chain we want to
get and that we type for builder we can
tell exactly what how close were the
optimal chain okay next thing I need to
convince you that it works so the sad
part is that we can't do the standard
thing we don't have ground truth we
don't have golden standard I can draw
those nice precision recall curves so we
had to do user studies to let people use
our chains and competitor chains and see
what they like best so these are our
competitors and we have shortest paths
after I spent 10 minutes or so bashing
them we have google timeline and we have
a system called events reading or key
BTW okay and just to give you an idea
this is one of the chain which showed
you the users we're trying to connect
then oj simpson trial to the verdict and
this is the chain simpson strategy there
are several killers book deal
controversy April transcripts and
something completely unrelated about it
under a murder case yeah this is one
chain we got from google news timeline
second chain same articles if you are
free from erupting the sims on trial li
police has some right racial tensions
more bias in LA police and finally
lawyers try to use this in order to get
acquittal and
direct okay so this is one of our chains
now we run several years of study I'm
just going to talk to you about one of
them and the deficit with the new york
times I've told it with an old one from
1995 to 2003 we had 18 users we chose
five prominent news stories like the oj
simpson trial and we show them two
Chainz generated by a different methods
double-blind and first thing we asked
them just which one is more coherent ok
cool we wanted to see if we captured
their notion of coherence also despite a
fighter who not directly optimizing for
it we also asking which one is more
redundant and which one is more relevant
yeah let me show the results and y-axis
here is the fraction of time one method
was preferred to the other people could
say two things are the same so it
doesn't have to sum up to one hundred
percent and first thing is coherence and
all I should really get out of it is
that we're doing better which was the
entire point of this paper it's over
happy now if things like a bit not as
good when you looked at redundancy okay
but then we looked at relevance and they
all started to make sense because if you
think about there's a very clear
trade-off between relevance and
redundancy right if you want like high
and you want to remove redundant
features pick random articles and your
relevance rocks or if your entire
relevant just stay really close to sr22
to your input articles and then your
redundancy get up even you can sit in
the change that I showed you right
because the tender murder case is
definitely not redundant but it's also
not relevant you think this is what
happens here that we pay for relevancy
with some redundancy okay again this is
just to give you a flavor of what
questions you can ask about with change
what I'm we're seeing you yeah I get
question so you looked into new times
yes
hmm because that kid let me buy us I
made that ethical they use their
favorite words so that don't you now
Google looks white or also on so you can
actually restrict it to New York time
with you you can restrict it to New York
Times ah then what you did to Google
restrict its new york times I think
that's what I did it was too easy to go
back to I think that was otherwise it
may be even better about what you said
it's actually because we're just using
words and because different writers do
tend to use your own word you can
actually see that sometimes it prefers
changed by the same writer article the
same day in the same issue from Wall
Street Journal
yeah I actually don't have any other
Dana said but the new york times but it
would be fun to play with the wall
street journal the five new stories i
think we went to one of those you know
website of what were the top news
stories of the year or something like
this and we pick the top and talk to
from a review or something okay released
so one thing i really like about change
that they allow some interesting forms
of interaction okay for example though
Jay Simpson have so many ways to connect
those two endpoints and tell a coherent
story right so what we did is we added
some interaction mechanism where users
were shown a tag cloud they could say
given more about this word or less about
death word okay and ideas are from
online learning I'm not getting into it
again for it but just to give you an
idea of what it looks like in practice
so this user got a chain focusing on the
verdict day okay and they say I don't
care about the verdict give you more
about the racial aspects then they got
the chamber similar to what you saw
earlier about racial issues in LA please
I'll further the way they could say do
you more about blood and glove and then
they get a chain about DNA expert in
fiber evidence yes there's a lot of
playing room here good so I hopefully
convince you I know how to install good
lines but like I just deadlines are not
good enough hey just again oj simpson
there are so many different aspects to
be said so next thing is we should
switch to map can just as a quick
reminder this creature is a map lines
are coherent and different lines focus
on different aspects and so and they
overlap the intersect now let me just
define it family formally sama is just a
graph G and a set of path by and all you
need to know is that vertices correspond
to news articles and that the edges are
really the underlying edges of the path
okay so the graph is just the union of
all for the bath now so how do you
define a good map well the first
property I gave you for free right
its coherence every line should tell a
coherent story but is it good enough so
can I just return the top three coherent
story in the dataset and call it a map
so let me show you when you actually do
this this is the map we got for the
query Clinton again all data set so
hillary was not around and the first
line is worth Clinton's visit to Belfast
and then you have two more lines about
Clinton's relationship with some
religious leaders now just take em out
and started and what's wrong with those
maps and the thing is that there are two
things wrong here right first of all if
I don't know how to say it but those are
not really important story okay there's
so much to be said about Clinton's
presidency and his visit to Belfast is
not one of them also there is nothing to
go against redundant right those and
green and red lines are pretty much the
same and yeah they're both coherent but
they don't give me anything that the
other one did not ok so there's
importance and then redundancy or in
other words the challenge is relative
balance is coherent with what we call
coverage case the reliance should be
coherent but they also be able topics
that the user care about and as many of
them as you can with me yes so how do we
do that and we tackle the very similar
priming kdd 09 paper called turning down
the north in blogosphere where the idea
is to just find a small set of articles
that are both diverse and important okay
so if this is a ted cloudy but
everything that happened on january 17
2009 you can see that obama was very
frequent okay the size of word
correspond to its frequency and this was
Obama's inauguration also they Israel
Gaza conflict and New York because
apparently some airplane Lebanon the
Hudson River so the idea to peak
articles that are about those important
sword can just one slide the summary of
how we do this all you need to know that
the idea is the documents cover concept
again concepts you can think about them
is words for example this flow document
covers some of Obama's some of
Washington some of you
you throwing day oh the orange one you
completely cover in your can add some
coverage to you the US and some point
you start looking for documented cover
other things okay everything you need to
know that we use this coverage notion it
addresses both the problem that we had
both importance and redundancy oh it
means it when you look for when your
algorithm works for another document to
increase coverage they're not going to
pick something about New York because
they're not getting any additional gain
from it don't understand what I mean to
do in the oil reverse containment to do
so it means that New York play was
important in this document ok so this
document was about something in this
case the Hudson River so they mention
you're quite often so in other words
you're not going to actually let go with
a Bama tune is your example you had an
article about the inauguration it
covered Obama some words it covered on a
Grecian somewhat you keep on picking
articles that cover those two things and
you have this diminishing returns
property here's at some point you just
stop adding yeah what is diminishing
oh so yeah that's because I'm hiding it
in my slope so the notion of coverage is
a similar function that has diminishing
returns okay so i don't have the formula
here but it's basically do you want me
to go into the formula i can better say
what's the idea because everything yeah
I guess I song documents mention Obama a
new york how do you know each document
mentions okie first document which you
got what the important words are this is
just tf-idf standard NOP stuff and then
what you do is you say well this
document covered Obama say a third okay
then you add another document and you
don't want if this one also covers the
third you don't want just to keep adding
Obama and Obama oh no known so if you do
is you turn it into em probabilistic max
coverage problem which document flips a
coin and with that probability covers
the concept Obama okay so when you have
more documents as for belted just one of
the men do it is closer to one and then
you don't get any additional coverage
from a new document and you need to go
into this Gaza Israel stories it's still
not convinced you can't discuss it all
on the teller that we have a coverage
notion but not a part of the main main
line of work here that helps us figure
out if a set of documents about high
important things and also not redundant
fair enough and this is what happens
when you incorporate coverage in when
you look for a map that smokes high
coverage and coherent okay so this is
about Greece again and you have a line
about strikes align about Germany and
the line about IMF now what's wrong with
this map come on you're all thinking it
yes precisely they're not connected and
especially annoying because we have this
article here about Germany and the IMF I
mean for crying out loud it just those
two should have been resected okay so
our last property is exactly
connectivity if two lines are connected
then I want to know about it ok and a
multiple way through formulas
connectivity we experimented with users
and it feels like the only thing they
really care about what those two lines I
know they're related but the map doesn't
show it to me okay didn't seem to care
if they were connected or is it was at
the beginning at the end one article
multiple articles just are they
connected or not so we want for this
real simple objective of just count the
number of lines that intersect okay so
now we have objectives for coherence
coverage and connectivity or three C's I
guess and now how do you turn it into
one big objective function and idea that
it's fairly game of trade-offs because
like I told her if you maximize
coherence you get all those clintonb
alpha story of low coverage and if you
try to maximize connectivity then again
you're going to get those are the lines
that are almost the same enter
definitely connected but they're about
the same thing so again connect to the
end quick too many properties coverage
drops if you try to maximize coverage
your connectivity drops and so on so
here are the three properties how would
you combine them and let's start with
coherence now hopefully I convince you
that we're not after maximizing
coherence we don't want necessarily the
most coherent change that we have rather
it it's really constraint that you only
want a change to be coherent to be above
some threshold now we're Lazarus
coverage and connectivity we really want
to maximize both but think about it if I
tell you here's a map here change that
you care about but I don't tell you how
they're connected versus here is a map
here a change that you don't care about
but i'll show you how they are connected
what do you prefer so hopefully i agree
that coverage is more important than
connectivity okay so this is our primary
objective and connectivities are
secondary listen so the way you would
write everything down it like kappa b
the maximal coverage you can achieve
with coherent maps and you try to find a
map that's coherent and that maximizes
connectivity given that coverage is
already maximized hmm you look skeptical
so dala far this is that it generates
disconnected maps because coverage is a
set function so relatives no reason ever
to use the same article in two different
lines because you don't get any external
coverage from it okay so we had to enjoy
some slack we're willing to sacrifice an
excellent fraction of the coverage if it
tells us something about the
connectivity then there's a map
objective now let me just give you a
very high level overview of how to find
good maps okay so we start from says the
document next thing remember I told you
that coherence is a constraint that we
only care about coherent chains so we
need to find a way to represent all the
candidates chains to be used in the map
okay so what we do is we encode all
coherent chains as a graph which you
call the coherence graph invest each
node here corresponds to a short
coherent chain and edges between the
nodes correspond to candles to be
concatenated and still remain coherent
get in as a transitive property so real
each path in this graph is a coherent
chain okay next thing you do is you try
to find a set of high coverage chains in
this graph okay so there are also
coherent now if you think about the
finding a path in this graph you really
look for a path that maximizes some
function of the nodes visited right
which luckily somebody already and solve
this from for us it's called
orienteering and it's a hard problem but
luckily again our coverage notion is
submodular what I was trying to tell you
about diminishing returns and didn't
completely work so i can use this
algorithm of samaja orienteering by a
korean pal it's a nice algorithm it
gives us quasi poly time it's recursive
greedy and it has some approximation
guarantees okay so we know how to find a
set of high coverage chains in this
graph to their coherent final step we
just have a local search step that tries
to increase connectivity without
sacrificing coverage now the perfect
time you don't yes my house
yeah and some for some topics you may
care about reliability for other
problems you may not sometimes you need
to do a trade off between covariance
coverage drivers technology can do those
kind of trade so here we just said hey
the direct I've we trust everything they
have to say which yes has some
limitations we actually came across this
valley in the previous paper where we're
dealing with blogs so yeah there was
some really bad things we had the
fielder but here it's nice it's New York
Times we're going to come again to trust
when we talk about scientific papers
okay yeah the whole process of
formulating adjectives and the
constraints Chris morning just for
giggles seems mostly creative and sense
that we sort of look at the output and
see what's wrong any thoughts on i-15
either from and possess feedback or from
pairwise comparisons or something or is
that a scope how I can quantify hearing
other than the user studies at the end
French so coming up with this book here
at the unseen so on it seems like sort
of look at it and that you say what's
wrong with this um so it seems largely
sort of looking at it it's very it's a
very human and very subjective look at
banging my head against the wall for a
few months coming up with a notion I
like that's a pretty good summer did you
give something the users and then use
their clicks or use fairways or
otherwise basically create that justice
so yeah so you might be actually one
thing that would have rather will help
me would be giving users a chain telling
them coherent not coherent and if not
coherent if you're a small change you
can make to make it more coherent and
just see if there's some I don't know it
going to work but just if there is some
locals and gradient that you can follow
like what would make this more coherent
or is it just beyond repair and but now
actually that's pretty much what i've
been doing so far just trying to come up
with objectives it would actually the
thing is there's too many possible
chains outer to do the standard machine
learning techniques just give me a cup
of chains that are good in a couple
their bed it's not going to work so
maybe feedback on a more
on a final level that sort of answer and
I just think about it some more okay so
this is the algorithm now just let me
show you an example for the math looks
like and this is the real grease math
you know the simplified version you saw
earlier and acceder is the line about
what is it yeah deficit-cutting planners
they have to make cuts they're rated
junk and so on next thing if you have a
lion about the strikes and riots and
next you have a line about Germany and
finally a tiny line about the IMF coming
out at the end okay so this is what the
maps look like and this is really sorted
chronologically okay thanks how do you
know that those matter any good so again
very high level overview of the user
study here we had the New York Times
degas it again this time slightly newer
and what is it eighty thousand articles
or so and we try to see what myths are
good for really so first thing we decide
just what we call my chronology okay so
just using maps with information
retrieval tools suppose the user has
some questions I have in mind like who's
the prime minister of Greece isn't up
any good for helping them located faster
and the answer was that it did show some
improvement but it wasn't my nerd
compared to their competitors okay we
didn't really do a lot better definitely
not statistically significant and like
some people told us in the study like if
I wanted to know the answer to this
question I would just search for it
there's no really need to start looking
at the map so second thing we tried with
what you calm a chronology seeing if the
map can help people understand the big
picture and how do you test this so we
decided to wait to see if somebody
really understand the story if you see
if they can explain it to somebody else
okay just think about the last time UT
aid and we asked people to summarize to
look at the map or look at competitors
and give a one paragraph summary of the
story again then we threw the prowl on
Mechanical Turk and ask them here's two
paragraphs one of them generated by map
uses the other by competitor
which one tells a more coherent and
complete version of the story ok and ok
here the results for the grease debt
crisis seventy two percent of the
turkish / and preferred our aim at peril
gasps and it looks less good for the
haiti earthquake when you only got fifty
nine percent then actually had to go and
look at the paragraph and although the
map did have a lot of other aspects of
the story like what was it some
kidnapped orphans and some laws
established in the u.s. to help like
temporary lows immigration laws pretty
much of them also the user that
summarize those paragraphs just follow
the main storyline okay earthquake lots
of damage distributing AV so on okay so
our conclusion for now is just the
bottom line that maps are useful for
those macro summers gets of tools to
understand the big picture especially
for stories that are complex that you
can't really say have a single dominant
storyline yeah I wanna stay here I was
guys were amazing no actually I'm gonna
treat you the next book okay so what
were their competitors here let me see
so there is google news again just type
greece debt crisis or whatever they
wanted to type and just read the first I
think up to five pages yourself and
second was TBT that I talked about
earlier and we're reasoning you on
Tuesday priests haiti and chile the
miners trapped underground again for the
pretty small scale study there's a
limited the number of undergrads that
are convinced to come for pizza just
these three did you look at the maps
that were generated
haiti and chile and see if they sort of
whereas high quality in your in as
degrees now they're actually again i
think there are a good quality just that
people did not care about the side
stories as much sounds like when you
talk about the earth squared reuters
just the main storyline and everything
else is distractions okay well increase
the somehow liked other lines more
there's more because the topic not
because of the quality yeah
okay that's what I think um okay so we
know how to construct maps I hope and
now how do you adapt it to science and
this point you're supposed to stop me
and ask where I've and butter adapting
it I mean those techniques should still
work right for scientific papers why
even changing them and you'd actually be
right those techniques work out of the
box the real nice thing is that science
just gives us all this wonderful
additional structure okay in the form of
the citation graph and maybe we can do
something smarter with all this extra
information let's let me just walk you
through how I would modify the maps for
a scientific domain okay so done and
this is a quick summary for yourself so
far we have three objectives that
coherence coverage connectivity and this
is what we did in new domain now let's
just go one by one and I'll tell you how
I would modified for scientific papers
so coherence first hopefully remember
the slide this is the CBR echo here is
objective if you take a close look at it
are really two main ideas going on one
is this compute the influence of warts /
transitions and that is choose a small
set of words its capture the story well
can it since that the second thing still
works but how about computing influence
remember how I told you there is lots of
influence notions in the literature but
we can't use them because we don't have
edges well now we do have edges okay we
have all those people citing each other
and really telling us who influenced
them so maybe we can use that so we're
going to change influence and the idea
that we want to capture the ways ideas
travel in the scientific literature okay
so when you write a paper your ideas are
influenced by your previous work the
papers your site hopefully some novelty
involved but really we we use this
notion of influence from beyond keyword
search from kdd 11 and the idea again
briefly is that for each word you
construct a graph okay notes or papers
and edges means either citation or same
authors any real attention to where the
idea came from kay there's a weight on
each
that you come up with what's the chance
that this world for which the graph is
constructed like Pete got this idea from
cure from our something novel and then
use this graph so in beyond curators
they defined directly implants which is
just a probability that paper p2 got
this idea directly from p1 okay maybe
through a bunch of other papers in the
middle but it originated at p1 and when
we use this notion of info just plug it
in our coherence notion it really limits
the type of change we can hope to look
for right because if you will only give
you change of paper to directly
influence each other that build on top
of each other usually from the same
research group okay so it's not as
interesting therefore we replace it with
this notion of and suffering flies we
don't care if p1 directly influence p to
as long as they both got it from a
common source key from a common ancestor
kandis gives us some nicer and chains by
the way circulation people in the
audience I could use some help on coming
up with better algorithms for this okay
so this will influence how would you
change coverage now all I really wanted
you to know about coverage is that our
original notion just covered concepts
get documents cover concepts well it's
not really good enough in scientific
domain because we like contest is not
enough right think about those two
papers sbm in oracle database versus
support vector machines in relational
database under books they have very
similar content okay this is our content
and you see SVM data database what else
performance efficiency but they had very
different impact again if you are cured
that this is a siyum tag cloud over the
papers citing them okay so if your paper
cited you will get the authors and
venues in this cloud now what you should
see here is Jess it first of all the
paper on their left affected a lot more
authors and venues just because there
are more words here also and despite a
factor solving the same problem there
really related there's very little
intersection I think there's only a
single paper citing both of them
okay so we decided it in the scientific
domain instead of covering words we want
to cover the papers themselves case your
paper will really cover the paper that
it had a big impact on so if you think
about it what I'm saying that if you
want a high coverage map it's a small
set of documents that together had
impact on a large chunk of the corpus
and some of you might think that
descendants is counterintuitive right
because how can a paper cover future
contributions like how can number theory
papers cover RSA but we always think
about it really looking at the ancestors
only gives you some idea of the context
where the paper was written well if you
look at the descendents you can really
get the gist of what the contribution
was guess all recovering papers instead
of concepts lasting into and
connectivity so previously we just had
county number slander and intersect and
it can work for example this is a detail
from a map about support vector machines
and you can see there is the line here
but what is its large sat on a large
scale SVM's and align about I can't wait
for multi-class lesbians and they both
intersect at the paper but a sequential
dual method for large-scale multi-class
linear SVM here sometimes this notion
does work but more often it doesn't okay
because really in scientific paper
there's a much rich palette of
interaction possibilities you might say
to paper for many reasons again
coherence works against that's all for
the time ago see this blue line here
it's a coherent chain about linear
classifiers perceptrons SVM's and then
Colonel lesbian and the orange chain is
about SVM applications to vision facial
detection facial recognition and there's
not a single paper that can comfortably
fit in both chains right you can't
really get them to intersect and remain
coherent but are clearly related right
all those papers about vision sight the
theory papers okay you with me so we
decided to reward change not just for
direct intersection but also for having
high impact on an hour okay so just to
show you a dis resultant
this is a map but reinforcement learning
let me zoom in first line is about MVPs
and palm leaf isn't something called EMB
peas and you can see how it effected a
line about coordination and cooperation
of multi-agent systems you can see here
that this paper cites this one they said
it from bp's extent stochastic games can
you can also see how the MVP line
affected assignable and robotic arm
movement other end of the map there is a
line about the exploration exploitation
dilemma and bandit problems and you can
see how it interacts with this line
about analysis and bound for
reinforcement learning oh so if if this
is actually a direct citation not just
some couple of levels impact this is the
text around a citation with limitations
of PDFs text extraction
okay so we know how to adapt Maps very
scientific literature one last thing
this is the user studying actually going
into a little bit more detail how do you
evaluate those and evaluating metric
maps of science is really tricky hey
first of all you can do double blind
there's no way that which is just too
unique which means that you need to get
the group of users ask them all the same
questions let's half of them play with
maps that ever have with competitors but
this means that you have to find a
research domain that the entire group
can both understand they need to read
those papers and also they must not be
expert in it in advance okay so we chose
reinforcement learning and we
constructed mass over the ACM corpus
about 35,000 papers I just tell you how
they use your study was so we had people
stepping into my office and I told him
to pretend they're a first date grad
student all excited about doing a
project in reinforcement learning and
they step into the professor's office
line yes teach me everything you know
about reinforcement learning and the
professor gives them a survey paper to
read now the last survey paper I know
about in very forefront la needed
actually it's fitted for a first a grad
was written in 1996 so their task was
really to update it okay to find some
more recent research directions and some
relevant papers to fit in this new
survey and they're given 40 minutes
which is not a whole lot of time it just
to simulate a quick first pass on the
data they could use google scholar they
could use our Maps and Google Scholar
they're given no instructions whatsoever
just you stumbled upon the thing and we
also had to baseline which is the map
itself and the wikipedia entry okay with
a snapshot of their progress and we
recorded a browsing history next thing
we did well first of all we ended up
with 30 participants we had to get rid
of four of them but didn't quite
understand the task and wrote me really
really nice ahsoka but reinforcement
learning and then we took the papers
it's also the people mentioned we
combined them into one really long list
and we send this to a judge who's an
expert in this area and the judge just
had to for every paper again they didn't
know where they come from tell me
relevant irrelevant or seminal
they also since they put every paper
under some research direction category
ahead labels and the judge also tell me
whether the label is good or bad yeah
sub precision all I should get out of
heat were the blue lines way to a way to
green lights and we're doing better both
in score of the papers and score of the
labels okay and if you want to know
about the baselines well wikipedia did
quite poorly really first of all they
had 15 citations and only four of them
qualified what we were looking for
meaning research papers written after
1996 now out of those four only toward
in relevant although in Wikipedia
defense a lot of those references were
books they could have been useful for
our hypothetical first year grad student
now with the map it is a bit hard to
compare to map the users out but just
because there are more papers but just
to give you the flavors there are 45
favor earth seven of the more dim
seminal and another 21 relevant and
interestingly enough many of the
irrelevant papers were seemed like they
were used to bridge between two relevant
papers hey just to form a chain also
it's somewhat concerning that the map
has all those seminal papers and users
didn't quite see off than they didn't
list all of them okay so there is
definitely some research going into this
area of how to show people what's
important in the map I actually get how
to know what's important in the map
first last thing is recall because it's
really nice that they get paper that are
relevant but are they completely
overlooking some really important
research direction and we do we compose
the list of the top ten areas of
reinforcement learning and we just
computed the fraction of areas if each
user found and again we're beating
google scholar users alone and just at
the end of using this snap we asked
people to stir Mina just tell us what
they thought so just a samurai's they
sought the with the maps were helpful
and noticing directions that they didn't
know about youthful way to get the basic
idea for science is active and a lot of
the negative comments can be checked up
to Miami nanako sangre skills Frank like
casings like the legend
is confusing or it's hard to understand
from the paper title alone okay just to
remind you were we had at that okay the
direction is still the same we're trying
to build issue maps automatically for
every query you have in mind and one
thing I think could really add a lot to
issue maps is this interactive component
okay this personalization there are so
many ways you can interact with this map
right you can do me into something you
care about or zoom out or maybe like the
chains give me like I want to know more
about Germany's rolling that that crisis
just increase the importance of death
word another thing i was playing with
recent place and just having a map that
reflects what you already know your
background k cuz if i search for
reinforcement learning with some expert
searcher they're really looking for
different things so maybe just give the
map as input your beep text file and hey
those are the papers i know about can
you just give me like can you use it in
a query somehow i think it would really
make it much more useful okay one more
thing how am i doing that time hmm do I
have time for one minute demo just to
show you and I don't have internet
access here for some reason so you're
going to have to survive with what they
did at the hotel this is our site
currently you see there's a map okay I
can do both and you can click on an
article and you can read the article
like I said not too much grease kills
and wait yeah anyway that's what i
wanted to share we have a website we
hope to launch it soon after we finish
fighting all the html5 kinks and i guess
i'm going to get a lot more data and see
what works and what doesn't wait okay
conclusions a huge chunk of my work was
just like you said formulating those
metrics just coming up with good
objective what's coherence what's
coverage how do you measure connectivity
and then coming up with some efficient
methods to actually compete them with
some theoretical guarantees now we have
some user studies that highlight the
potential of this method
the website under way hopefully soon now
if there's one thing I want you to take
out of the stock it fell out this one
okay search engine looks like search
engines are great but sometimes you need
more defenders you have more complex
information needs and then hopefully
you're going to use maps yeah thank you
now if you have any question
yeah we consider altering wages to be
like bigger automatic evaluation so that
you mentioned it surveyed seems like an
obvious one driver you can take the
papers before the survey see if you your
recall on the basically website in a
survey by generating from paper before
and even in the map separating into two
sections relevant to the servant right
do your math line separate by the way I
love to get some point you can use the
survey at the point of evaluation right
so you can basically from the papers
beforehand do you recover the seminal
papers by doing automatic analysis do
you segment them in the way that the
surveys 30 years available for
vegetarians I was actually looking at it
I was looking at planning service and/or
different services have completely
different ways of segmenting this and
they mention different papers so maybe I
should go like one level higher just
Lucas they find the good authors I guess
this is less controversial maybe but no
Alice I guess can you serve Vidya wait I
want to write this down yeah so anyway
we haven't done it actually like it yes
can you remind us one more time the
definition of connectivity services yes
it was ok for the first one it was Daisy
widest every tool answer didn't was
intersect you get the point ok so in
other words another way but it is it's
simply the number of edges in the
intersection graph the graph those nodes
outlines so why don't you use actual
connectivity is
this graph connected because sometimes
it just it really can be connected
because of what I showed you in there in
the scientific don't mind that some
lines are just especially when the
queries wide some lines are just too out
there they can't be connected why do you
use the number of components because we
say you have ten lines
the following to go actually no measures
you eat the pair of them is connected so
there's all the chain and alternatively
four of them have nine edges together
and all that separate can use the number
of connected components but i would
actually it also interesting to see how
the things in the connected components
are connected right right about it but
I'm just it seems just a number that's
my boy right since we're doing it
there's a local search this is actually
the easiest objective to change so
mm-hmm hey I should the cliff get
something ready yes batteries I don't
agree from Paul suggestion earlier when
talking I was thinking about well the
union is the document well we could have
as a unit to paragraph so imagine you
take all the paragraph in to shuttle
them and then you try to reconstruct
to the paper is for young story and now
you can see how all the papers which
offers multiple going story hopefully
basically score with the different
metric
Patrick let's go well be actually your
foundation is a good metric assuming
that the papers had some courses which
is a different assumption for most
authors yet now a wikia eric remember we
talked about at some point so yeah this
is somewhere on my to-do list and I wish
is long I admit and I think that a
single paragraph is not enough for you
to to actually see that those two
progress are coherent but maybe a third
of the article would be good enough so
yes this is that the wavering yes the
same question is that you seem to listen
to be an assumption that all the items
have no some were comparable that you
could imagine feature access one would
be the same verbiage in person
the charge was not emotional
and and so grigio you restrict to some
of these axons and see
or you could look for diversity through
these things of
I really like this question so one thing
that this is what I think
personalization can come in really
useful because then well not off the
excess you were talking about but you
can definitely best towards Republican
or Democrat like if use the word
Obamacare its particular what you think
about it okay so and I was just ducking
this morning about maybe you should try
doing this complete the map for the same
query once for the New York Times one
for all street journal or something else
and see how they defer or just try to
give different point of view I don't
actually know how to have to find the
other access that you're talking about
there I live low level that's that's
doable yeah I guess that kind of car you
can do it with that and personalization
as well right just increase their weight
of some words that are very charged why
do you want that right away that's a
perfectly good reason okay those knots
are really entertaining especially if we
try to connect change that are between
end points that are not really go can i
it's good you really get something like
a conspiracy theory generator right good
the most coherent story is not very
coherent buganda
sheen from the crank image any
connections between you know the gypsies
of Europe North torn out the downfall of
the banks anyway any other comments or
questions thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>