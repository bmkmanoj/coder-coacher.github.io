<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Making Genomic Data Analysis Faster and More Accurate | Coder Coacher - Coaching Coders</title><meta content="Making Genomic Data Analysis Faster and More Accurate - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Making Genomic Data Analysis Faster and More Accurate</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VupsmtIJTSU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright so I am very pleased to
introduce to you crystal and matei or
both fifth year grad students at
Berkeley although apparently this does
not mean that their equi distant from
graduating because mateas finishing next
year and crystal just told me to but
they have spent quite a bit of time
working with a bunch of people including
me on gene sequencing and I can't speak
for them but for me this is one of the
most fun things I've ever done it's just
just amazingly cool to learn that
knowledge about hash tables translates
into biology so so they're going to get
to tell you about the project that we
worked on and I they're going to
teamspeak I don't know what order
they're in yeah all right so I'm matei
I'll start enough so this is a project
that we you know a bunch of people are
interested in a lot of the people you
see up here are from the amp lab at UC
Berkeley which is a new lab that started
a year ago that's focused on big data
and large-scale machine learning and
data processing and we're also working
with Bill and Harvey from Microsoft and
with some folks from UCSF including
Taylor sidler who is here today he's a
medical researcher they're so so yeah
let me just jump into this why we're
doing this so I think if you've you know
seen some biology in the past you'll
know DNA is the molecule and the code
that orchestrates all the activity of a
living cell and because of this DNA is a
central element in a lot of diseases it
either has you know differences in the
DNA that cause the disease or it's like
actually involved in in propagating at
somehow so in particular what DNA does
is it encodes how to build these worker
molecules called proteins in the cell
and also signalling information about
essentially when to build them so
everything the cell does is guided by
that and two ways in which it ties to
diseases is first of all in cancer
cancer
essentially caused by the DNA in some
cells accumulating enough mutations that
the regulatory mechanisms break down and
those cells start multiplying without
without the right regulation and and
just start consuming all the resources
of the body and on top of that the
hereditary diseases which I adjust if
there's some mutation that's passed down
that causes a problem DNA also affects
susceptibility to various drugs and you
know it's an important thing to
understand what's going on in the cell
and one of the things that's happened in
the past decade is that the cost of
actually reading the DNA from a cell has
fallen dramatically and it's fallen
enough that it's starting to actually be
used in clinical medicine for treatment
so here's a picture of that and this
compares it against Musleh this is the
cost of sequencing one human genome and
if you go back to 99 and 2000 the human
genome project cost several billion
dollars the sequence the first human
genome and that was a great
accomplishment if you fast-forward a bit
this year you can actually get your
genome sequence for about three thousand
dollars and next year a couple of
companies have announced that they will
do it for one thousand dollars so this
sort of magical thousand-dollar genome
is is is really on the horizon and I
think if you work out to eight the cost
has been fouling by something like a
factor of three or four per year and
it's actually quite a bit faster than
Moore's Law so we were able to actually
eat this stuff now and being able to eat
this stuff can impact a lot of areas of
Medicine and of course of biological
research as well so here are just some
examples so in cancer the most exciting
thing about this is like cancer is
typically caused by a bunch of mutations
that affect many different gene pathways
like these are groups of genes that work
together to perform a function in the
cell and often cancer is that you know
that that look very different like a
skin cancer and a lung cancer might
involve the same pathway and by actually
sequencing it you'd be able to figure
out which pathway is in
involved and also which drugs you should
use that target that particular pathway
so there are many targeted cancer drugs
now that will work really well if one of
the possible things is going on and will
have no effect if something else is
happening for infectious disease like
just you know common viruses and
bacteria one of the nice things you can
do it sequencing is you can really
quickly find out which pathogen a
patient is infected with you don't have
to do these sort of crazy diagnostic
tests where you look at the symptoms you
can actually get some DNA and see what's
going on and another thing this has been
useful is identifying new diseases such
as the h1n1 flu by actually sequencing
and saying hey this looks different from
some stuff that we've seen before and
finally you can personalize medicine to
individuals genome so you can identify
things they're susceptible to an advance
so for example some dog allergies or
side effects only happen to people with
specific genes you can know which which
inheritable diseases they have and you
can estimate the risk of different
things going on and there are some
examples of this happening already um
just a couple of weeks ago there was a
series of New York Times articles about
you know sequencing driven personalized
treatment for cancer so this was the
case at Washington University where they
sequenced the cancer tumor that actually
what one of the researchers they had and
they figured out that it was actually
susceptible to a dog that had been
designed for a completely different type
of cancer and they applied it and
actually they managed to really induce
her or omission in that patient there's
another one that was well this this was
a case where you know in the end it
didn't work out the remission was only
for a short amount of time but similarly
they they were able to use a dark from
one cancer to treat a different one that
it normally wouldn't be prescribed for
and there's some examples in sort of
non-cancer things as well so for example
diagnosing metabolic diseases which is
usually very hard they can sometimes be
due to
mutations in mitochondrial DNA and here
they were able to just sequence
mitochondria and figure out some of
these diseases okay so so this is all
great in terms of affecting medicine but
we're here talking at a you know
computer research lab so what is the
computational challenge so it turns out
that although the cost of the wet lab
part of sequencing is is dropping
dramatically you have to do a lot of
processing to actually put this data to
use both to understand what is happening
like actually how to put together the
sequence for one human and of course
beyond that to understand which genes
affect which diseases we were not even
going to get into that in this stock but
that's also very hard kind of machine
learning problem but the reason that it
the processing is difficult is that the
way the sequencing machines work is
essentially by using massive parallelism
by reading many small sort of random
substrings of the genome in parallel and
you get together these little random
strings kind of like puzzle pieces and
you have to put them together to see the
whole picture of what was actually going
on in that person's genome and I'll show
I'll show what that looks like and and
what are the steps you need to do that
so the bottom line is that the current
pipelines for doing this take multiple
days for each genome and cost if you
actually I work out the compute time it
would cost on something like like Amazon
or agile or to some extent even if you
buy your own machines it can cost in the
thousands of dollars just just for one
genome so this is starting to exceed the
cost of the actual sequencing itself and
it yeah question just the
yeah just the chemistry boy exactly yeah
and in the other so this is you know
it's it's not great for clinical use but
it's especially bad if you want to scale
up the amount of genomes you sequence
for research and and like really
understand these diseases because people
have to pay that many times over the
sequence you know a hundred cancer
patients and see what's going on so the
goal of our team is to build faster and
most scalable and more accurate pipeline
that does this genomic construction and
that can be used both in actual like
medicine and in in research where you
get a lot of different genomes and you
want to compare them or you just want to
do this kind of computation at skill and
we have a bunch of people involved from
a bunch of different areas so we have a
lot of systems people this started out
essentially with a bunch of systems
people talking with Taylor and figuring
out that we can do some of these steps
faster and bill and Xavier from
Microsoft we have Taylor and Arun from
UCSF and we have some folks in machine
learning and in computational biology
and and see that are helping out as well
at Berkeley all right so what will we
have in this stock so I'm going to start
with just an overview of how the
sequencing process works and what are
the computational problems and then
we'll mostly talk about the first
processing step which is sequence
alignment and also turns out to be the
most expensive in terms of CPU time and
we'll talk about a new algorithm we've
developed called snap that cuts down the
cost of this this step by a factor of
ten to a hundred compared to the
existing tools so this is something that
it's takes a problem that was
essentially cpu-bound initially and took
more than a day for a human genome and
it turns it into a problem that's now
basically i/o bound and we can do it in
in an hour and a half then we'll talk
about some ideas we have for further
improving alignment especially the
accuracy by taking advantage of the
structure of the genome and finally
we'll also talk about some some
downstream processing steps beyond
alignment
so hopefully this will give you a taste
of like the different types of problems
that exist okay yeah so so let me just
show real quickly with some pictures how
the actual sequences work and what you
have to do with with the data so the DNA
itself that you have is a long molecule
well it's actually a bunch of long
molecules but in total it's three
billion of these letters or bases long
that encode the sequence so you have
sequence of these three billion things
and the first thing you do is is you
have placated so you have a bunch of
copies or you just read a bunch of
copies from for myself and next to
actually sequence it everything is done
in parallel so you split it randomly
into fragments you can just do it by
heating it up or something like that and
you get these little fragments of DNA
and after that you can read the
fragments in parallel there are machines
that do this by you know doing some
clever chemistry to be able to heat the
sequence off of each of them and the way
it works is you know they kind of like
attach it to a little thing on a plate
and they float around the complementary
basis that the ones that are part of
your DNA and they eat what actually
sticks to it and in what order and as
they do this they end up eating the
sequence so these little fragments are
called weeds and about a hundred basis
or letters log today about the whole
thing as i said is the-- billion and in
a typical you know human genome
sequencing on you're going to get 10 to
the 9 of these reads which is basically
covering each location in the genome
about 30 times and the h100 bases long
so now how do you actually put these
things back together you can view it as
a puzzle so you just get this sequence
of weeds and one way to treat it as a
puzzle is like let's see what the
picture on the top of the puzzle boxes
you can use the reference genome which
is the genome that was sequenced you
know back in 99 and 2000 and has been
refined since then where people use
longer heat technology and know-how
everything ties together
and when you look at the reads from the
person they're going to have some
differences from the reference genome of
course because it's a different person
and they will also have differences
because of varies in the sequencing
itself but what you can do is you can
take each read and see where in the
reference genome it matches the sequence
best and and then just just place it at
that location like placing the piece of
the puzzle you know on the sheet in
front of you and so this step is called
sequence alignment this is the one that
will talk a bunch about and what this
lets you do is once you've aligned all
of the weeds at each location you'll get
something like this were a bunch of them
align together and you know they might
i'll show some difference from the
reference genome or you might have you
know some of them show different some of
them show show noise because there is
some air in the actual reading process
but then you can do kind of a voting you
know algorithm to actually figure out
what the base at that location was this
is really simplified it turns out
there's a lot of stuff that makes this
harder so this step is called variant
calling and we'll talk about it a little
bit later and just to give you a sense
of the rate of differences between these
two people only differ in about one in a
thousand basis from each other so that's
pretty small but the sequencing machines
can can have a rates of up to a few
percent although it depends turns out
some of the areas are also pretty biased
so so so this is kind of the eight
you're looking at okay so let's let's
jump into the first step of this which
is alignment and you know what we ended
up doing for that so the alignment
problem as i said is given you know one
of these reeds and the reference genome
which is a big string find the position
in there that minimizes the edit
distance to the genome and again to give
you a sense the genome is the billion
bases and the reeds are about a hundred
ok so in if you look at the current
status alignment is today the most
expensive step of processing
and it's also important because you
really have to map the reeds to the
right location to do anything downstream
of that so depending on the accuracy of
the tools you use it can take a few
hundred to a few thousand CPU hours and
if you work out you know the math it can
be takes out of hundreds to a thousand
dollars of compute time and the issue
there is also that the faster aligners
lose accuracy so they typically don't
align as many of the reeds and they
support fewer differences insid treat
and the problem with that is if you have
a place in the genome where the person
really has like five differences in a
hoe and your aligner doesn't support
that you're going to systematically miss
all the reads that map there and you're
not going to see that difference in the
downstream analysis so what we build
snap the scalable nucleotide alignment
program is a tool that's ten to a
hundred times faster than the current
ones and at the same time it improves
the accuracy so it has higher accuracy
and it also has a richer error model
that allows for more types of
differences and basically you can give
it a parameter K the number of edits you
allow from delete the reference genome
and it will find you know a location
with with the best location with at most
K edits and as a result as I was saying
we cut down this step from about one and
a half days to 1.5 hours and this is
done while reducing the number of errors
in half on sort of real human data okay
so what what do current the liners do it
like how do we do this better so there
are two two methods to do alignment the
one of the earliest ones well if you've
heard of blast this is what it does is
based on seeds so the idea here is you
have the genome you you index just short
sub strings of it of say 10 characters
and what you do is then you take your
weed and you take every 10 characters in
here and look for an exact match in the
index so you noted here we have just for
say these were the first four of them
this is where it matches so you get some
candidate locations to try and then you
place the read at each one
and compute the edit distance and you
end up picking the best one at the end
okay so this is this is the seed based
method d yeah and of course you don't
know like maybe the first and bases
match somewhere but actually does just
by chance maybe there were some errors
in there so you have to drive multiple
seeds to really find the best location
and so you keep doing this for
continuous you know continuing seeds
this is one method the other methods
people do actually many of the faster
tools today user burrows-wheeler
transform to encode kind of a prefix
three of the genome of like all the
substrings in the genome and then they
searched through this tree using
backtracking so this is you know you're
just going down a tree and trying to
insert that it's at different locations
but I'm not going to talk much more
about this because it's it's a bit more
complicated to explain so what what we
do in in snap is we we've actually taken
the seed based method but we've done a
bunch of algorithmic changes and also
sort of systems changes that reduce the
cost of the most expensive check oh the
most expensive step which turns out to
be the local edit distance sticks so we
leverage on the one hand we leverage
just improving resources one of these is
the actual heat length so the read
lengths used to be about 25 bases and
now they've gotten longer to about 150
and using this it turns out you can
really change the form of hash index you
have and do quite a bit better we also
leverage higher memory so I algorithm is
designed for surveys with you know about
50 gigabytes of memory so we can use
that and on the algorithm side we have a
way to prune the search to reject most
of the local alignment locations without
fully computing the score and this turns
out to save a lot of time as well so let
me just explain the first part so we're
going to use these seeds to match a hash
table of just exact matches in the
reference genome but we we've chosen to
use longer seeds than then a lot of the
previous
liners and there is an ink for this is
that there's a trade-off in general
between this the seed size and the
probability of actually finding a seat
in the read that matches the genome and
the amount of false-positive hits you
have but yeah so if you have so the
human genome is about 4 to the 16 bases
and if you have a seat of ten bases and
if you have a two percent sequencing ER
it turns out there's a nineteen percent
chance that you see doesn't that receive
contains an error otherwise eighty
percent of the time it will match but
also you'll have 42 the 6 or 4,000
matches just by chance you know it
against the whole genome just on average
so this is why people used in the past
and it made sense when they had very
short to ease you couldn't take much
longer seats than that and expected to
match if you go up to a 20 base seed you
have a higher chance of an error but you
also have almost zero chance of it
matching just randomly at a particular
location so at least if the genome is a
random string you expect to test a lot
fewer candidate locations and the reason
this makes sense for longer it is
because the heat is longer you can take
many more independent seeds to try yeah
any assumptions about high repeat
regions all right there this is yeah
this is very simplified so actually
highly paid two regions mess this up so
some regions are you know some seeds are
much more common than others and just
big regions are replicated yeah so you
still have to search a lot you know you
have to search more than one hit for
each seed usually yeah well we'll
actually talk about that idea and that's
one of the things were looking at yeah
cool yeah so just to show this you know
say you have this short video of 25
bases you have an everyday well if you
take a 20 base seed there's pretty much
no place you can put it without touching
data but if you have a long lead of 100
basis and you know there are a bunch of
errors even with a two percent error
rate you do expect some seeds to work
well and so you can actually find
matches for this in the index so this is
you know this is like just an
observation with some math of
something we can do yeah what technology
allow them to go from 25 to under all
right yeah yeah definitely lead links
are improving and basically what it is
is I showed that picture at the bottom
or like you attach one end of the DNA
string and then you float these
molecules around and there's actually
like the fluorescent and there's a
camera pointed at it that see which one
is Dutch and what happened is they've
been able to attach more of them before
they have to stop so like before after
about 25 of them there was too much
noise to be able to attach more and like
see which one actually got put day and
they've improved both the chemistry of
like how those things float around and
the camera technology to be able to do
more yeah electronic read by a man of
war yeah there are new other technology
after more electronic reader you have
way less complications so we will see
longer reads coming yeah yeah what is
the order of magnitude we can expect
yeah I think that's it yeah 10,000 busy
and how did there are other people
working on megabass reads mmhmm yeah it
has a ways to go but yeah it'll be
interesting yeah the 10,000 a spare are
they seem to be doable
they're sort of prototypes that are out
there yeah turns up that rock is a bad
strategy for life all right even for 100
see ya yep yeah it's a good question
here so these things are getting longer
yeah okay the other thing we do is very
simple on the index side but it actually
helps a lot is a lot of the existing
tools were designed at a time when
server memories were a lot lower and for
example if you index 10 base pair seeds
they only took the non overlapping ones
like the one you know position 0 to 9
and then 10 to 19 whatever all these
these disjoint seeds what we do is we
just do a sliding window and index every
substring of that length and if you
think about it this means our index has
you know 10 times or actually 20 times
more seeds in it because we're using 20
base pair seeds but it turns out that if
you you know back two bytes nicely into
a hash table you can fit that in a very
reasonably sized memory so we are able
to do this with 39 gigabytes of memory
for the human genome and this is
important because looking for a seed and
not finding it in the hash table is
actually really expensive it's like
hundreds of cycles because it's an LC
cache miss because even with a small
hash table that they had before it's not
going to fit in your processor cash so
it really helps to actually have this um
but the other part so the algorithmic
part that's really different beyond this
is the way we do the local alignment
check so as as we talk as I mentioned
before it turns out that the genome is
not a random string there are a lot of
areas that are quite similar to each
other so for many seeds you'll find them
in a lot of locations and in general for
many weeds you'll have to test a bunch
of candidates before you find the best
one and this is where all the cycles in
the algorithm go essentially at least
ninety percent I are going into this so
what is our insight here so the thing we
figured out is that at the end of the
day to actually map the read you only
care about the best and second base
locations where the line
so if the place where it aligns best you
know say it has added distance one and
the second best place it has at a
distance 5 you can be pretty sure that
it came from that best one and you're
just going to align it there if the best
is at distance one and the second best
is a distance 2 then maybe you're not
sure and you're just going to tell the
downstream analysis okay i'm not sure
where this heat goes you might give at
both locations or something like that
but these are kind of to confidently
caller he do you just need to know the
best in second best locations so how can
we use this we replace the traditional
edit distance algorithm which always
computes the full edit distance at each
location and it's a quadratic time
algorithm with one way you can give it a
limit on the edit distance and you can
tell it if the distance is bigger than
this I don't care how big it is just
stop early and tell me it's bigger and
so we have this this algorithm the
complexity is only n times the distance
limit and we lower the limit as we find
more hits and improve the best and
second best locations yeah yes yeah n is
it is the larger except humor only
campaign locally at each place so n is
like a hundred basis it's the length of
the heat it's just we mapped it to one
place with a seed we take those 100 and
the 100 that we have yeah so yeah so we
have so with this algorithm you can
lower the limit and you can also arrange
things so that the first hit you find
actually has a low edit distance and
start out with a low one van didn't
search it just it's a little bit so it's
it's kind of like filling in only the
diagonal of the matrix but it's actually
it's a bit nicer than that because it
only uses like d order d space so it
only like tracks how far you can go down
each diagonal so that's actually kind of
cool because it fits in the l1 cache of
the processor as well yeah but that's
that's what it is yeah okay so here's
here's like the actual algorithm I'll
just step through this to show the the
way we actually do the pruning so
basically what we do when we start we
start the delimit to be the max
distance Plus this confidence threshold
see which is like how far away we want
the best and second best needs to be to
call it as unambiguous and we go out and
extract seeds from it and for each seed
so this is the confidence special yeah
for each seed we find the locations
where it matches in the genome we
actually prune seeds that that have too
many locations because some things I
just do repetitive we look for a better
seen that doesn't have that properly and
we had candidates to a list once you've
tested a minimum number of seeds we look
at the candidates that matches the most
of them and the idea here is we're going
to do an expensive edit distance
computation let's find the candidate
that will give us a lodi limit for the
next one so we score that candidate and
next thing we do is we update the
distance limit so so there are two cases
for this one so the first so we look at
the best and second best hit we keep
tracking them as we go along and there's
two possibilities so if the best is much
better than the second best and we have
this confidence special see then we only
care about finding other hits within
best plus C because the idea is if there
are hits with a bigger edit distance
then this one it's not going to change
our result but if we find one in this
window of up to see more we're going to
say maybe we're not confident about the
match so that's the first case their
second cases of the best and second
already within distance see then there's
no way that finding guys bigger than the
best is going to help us the only thing
that will help us is if we if we find
something much better than the best
that's up in this window here and and
there's nothing between it and and best
so we can be confident about it so we
only search up the best minus one in
this case so yes so that's what we do
with that and the final thing is there's
a trick you can do to stop early so if
say you found you know the best is in
distance to and maybe only cup the
things up the distance for beyond that
what you can do is if you if you've done
at least five disjoint seeds
from the heat you can actually stop
right there because if you've tested a
bunch of disjoint seeds you know that
any lead you haven't yet put in your
candidate list so it didn't match any of
the seeds has at least that many errors
in it so if you've tested five seeds and
there's a location that that you haven't
yet found through the exact matches on
those seeds then it must have at least
one area in each of the seeds so it must
be at least distance five so you can
actually stop or the over here so so
this let us let us stop the search as
well okay so just in terms of the
results here some numbers comparing snap
to two of the commonly used aligners
today bwa and soap they're both actually
based on the burrows-wheeler approach
and we can see what the existing aligner
is this so we're showing three numbers
here this is on simulated dana which
lets us know where each reactor Lee came
from and we're showing the percentage we
aligned the error rate and the speed and
in heats per second and here you can see
bwa and soap give you a trade-off
between accuracy and speed you know soap
is a little faster but makes a lot of
errors and snap actually matches the
percent aligned of beats the percent
aligned of bwa it has half the error
rate and it's also going about 30 times
faster ok yeah that is yeah yeah ok so
this was 100 base leads with two percent
differences this is kind of the current
to each you have today another cool
thing so in in snap you can actually
tune some of the parameters that the max
hits you check for each seed and trade
off between the accuracy and the speed
so if the speed it or if that's too fast
for you and you are ok waiting for you
know a day to align this stuff then you
can tune snap to give you higher
accuracy so we show you can you could
align with only 0 point 0 1 percenter if
you go about three times slower or if
you were ok with the error rate of bwa
you can tune it to get higher speed so
you can trade off between these yeah
so the remaining others they know
they're just leads we couldn't align so
so the main reason why we couldn't align
them is so these are the ones we
confidently turned one location for for
the other ones there were multiple
locations where they matched well
there's also usually like maybe like
half a percent where they have too many
differences and we don't find any
location for them yeah but this is how
many were confident about like how yeah
yeah no it would actually be only like I
think in this case it would only be like
ninety three percent or something like
that because you can Oh 94 okay yeah
because some agents are exactly
identical and you can't know why that
you so we're not counting those but it
would have zero air yeah yeah and it
actually even dear you know because of
the model you might have two areas that
put you closer to something than where
you actually came from but I think you
could calculate what that is so it is
yeah if you get other mutation or an
error in the in the read by the machine
it may move the string from the place
that it came from to actually closer and
edit distance face to the wrong location
and then it seemed you know it's
impossible to do this I mean you might
be able to just say I'm not going to
call it because it's too close to many
of them yeah yeah okay so this is kinda
with today's data 11 other neat thing is
that if you have reads with a lot more
areas which some of the future
technologies will have and also which
which will happen if the person has a
lot of mutations in one place snap still
performs pretty well so here with with
ten percenter the existing aligners kind
of fall over they align less than twenty
percent of the weeds and you know they
actually go a little faster because it's
easy when you're not aligning anything
to go fast but but we still do okay yeah
nice if you could combine this into just
one hundred percent error you'd need to
create the downstream processing do you
would do yeah so what happens when you
take just an off-the-shelf downstream
processor alright so if you can best nap
with others that's a really good
question yet we don't actually have a
complete answer for that yet we've have
so we've looked at does misalignment
affect downstream college and we found
places where it does like where bwa
misaligned some weeds consistently and
you call mutations that aren't actually
there but I don't have a good sense yet
but of the issue is there are other
things that are hard in the downstream
color that you have to like get right
first before this makes a huge
difference yeah but we think it will
it'll help eventually yeah yeah okay
yeah what there's a most of the current
processing pipelines there's a second
step what happened it's done after
I think I didn't actually feel ugly this
week as I guess about thirty thirty-five
percent reads the expect don't location
they're actually yeah this is this is
now looking at cancer genomes which are
a little bit more complicated than
origin
and in can significant in cancer also a
lot of the DNA like an application and
sort of checking mechanisms break down
so you get a lot more mutations yeah
like weird things happen yeah okay yeah
and let me see what else I wanted to
show oh yeah so this is another thing we
wanted to show he is as weeds get longer
which they will in the future snap
actually scales better as well so so the
existing tools the fastest ones today
have this backtracking which is a bad
idea when dudes are longer because you
have to you know backtrack add more
locations where snap actually does
better because you can use longer seeds
and you can get more disjoint seeds and
filter out a lot of locations just by
number of seeds matching okay and we
also did some analysis of the speed up
so one of the nice things is this
heuristic where the font we we test the
read with the most exact matches of
seeds means that usually the first
candidate we score is actually the best
one will find and we get a load delimit
at the start and we eliminate
ninety-four percent of the locations
without actually scoring them and forty
percent are just because the number of
seeds didn't match so this cuts down at
the time and the adaptive edit distance
threshold also helped by about a factor
of four okay and finally just to wrap up
this part we've been doing a bunch of
stuff after that you know I don't have
too much time to go into so one of the
things is we've generalized the
algorithm to work with what i called
paired-end reads this is when you this
is actually the most common type of feed
you get a bigger molecule like 500 basis
and the machine reads a hundred bases
off one end and 100 off the other and it
can't go all the way into the middle so
now you get a line kind of two strings
in a place and you know some constraint
on how far they can be far meet other
and it does a bit you know you can use
similar ideas in this problem to
actually align them as well as bill was
saying we've spent a bunch of time into
making this scale will and it does this
is an up to 32 Coy's and part of it is
because we're careful about
how many memory accesses and cache
misses we do we're careful to pay fetch
data and basically like having systems
people look at this actually does help
improve the speed to the point where it
matters to practitioners so that's kind
of cool and we've also hunted on some
real data it says numbers i showed
before so these are real reads
interesting thing with real data is a
lot fewer of them aligned because
there's some contamination and just like
stranger things happening in them but in
this one we're able to again sort of
married the results in the simulation
and get higher accuracy and also go
about 20 times faster okay so that's the
part of the dog that I had crystals
going to talk about what we're doing
next yeah consider this a salt problem I
mean given the lower areas and the
speeds that you're saying you're at IL
bel all right are you guys done all
right it's a good question i think the
so i think the accuracy can be improved
even further and we want to explore this
more to see when it matters but
especially for things like cancer there
will be more mutations and also there
will be things like pieces of the
chromosome that usually aren't together
get get cut and paste it next to each
other and you might want to detect that
so now it's like one have to read the
lines in one place and half in another
in terms of speed i think the speed is
pretty good although there's always like
you know this is one genome in one hour
but there are people who have like a
thousand genomes data said if they want
to realign that using snap or using
higher accuracy that might take a while
yeah but so we'll talk a bit about like
what we're doing for accuracy but but we
also want to look more at the downstream
steps next because there's more unknowns
in there yeah well okay so yeah as
matane mentioned we're kind of capping
out on speed but the accuracy is
actually something that we still want to
look at more because so you know as far
as alignment goes there are two parts of
the process and one is finding the good
candidates to check the read against and
the other is quickly checking against
all the candidates so you can find the
best match so in the first case we've
pretty much narrowed that down and we're
good list of candidates but in the
second one we still want to make some
improvements there so what we've noticed
in doing a sensitivity analysis to what
parameters tend to affect the
performance of snap is it really the
only parameter that makes a big
difference is this max hits and so as
mente mentioned this is the kind of the
cutoff for when you have a seed how many
hits it has in the table as to whether
you'll consider it or not and so what we
noticed when we very been that max its
parameter is that the error rate does go
way down as you get a higher max hits
and you can also align a higher
percentage of reeds so what this means
is that at first cut you would think
that a seed that matches in a bunch of
places is just indicating that your reed
is just matching to you know it's going
to be ambiguous because it's matching so
many places but what we actually find
when we are willing to consider more and
more places is that for some of those
reeds we actually can find an
unambiguous best match in the genome and
the reason this happens this way is
because as you're kind of alluding to in
your question the genome is not a random
string it's actually highly redundant
and what makes it difficult as well is
that it's not exact duplication that's
really the main factor it's similar
duplication so this is something that we
really want to be able to consider more
hits proceed but then of course the
downside is if we do consider more hits
proceed if this speed takes a big hit so
how can we improve our accuracy reduce
our error get a higher percent of line
while avoiding dropping off to the
bottom right of that curve that's the
part of this that we're looking at now
so as I was kind of mentioning we have
these similar regions in the genome that
make alignment difficult this back to
the seeds explanation and the false
positives the odds of finding too many
matches in the genome are so low if we
had the random string assumption but the
fact that we have similar regions are
what makes alignment take longer for us
so this is an example here you see that
it's colored by the
the base so when we see a solid color
column that means all the strings are
matching exactly and when we see
different colors in a column that means
we some of the strings have differences
from each other and these are some
strings that we found via clustering in
chromosome 22 and they're all unique
it's around 400 of them however if we
find the consensus string for that group
of substrings and we find the average
distance from each string in the cluster
to that consensus it's very low only
about six edits so this is what this is
the phenomenon that's causing us to
spend most of the cycles in the
alignment process so are those all in
the same person so this is from the
reference genome and he'll always
identical brands one part four thousand
different
yeah so um no it's actually kind of a
mix because different sequencing centers
were working on it so they all kind of
submitted part of it and it's kind of a
stitch together so um what we really
wanted to be able to do is test that are
read against the entire group of strings
however that takes a really long time so
we have this trade-off then which we've
kind of looked at and if we are too
aggressive in lowering the max hits then
we're going to be paying the price in
error so just to illustrate this
graphically what we have in many cases
is these reads that are matching against
these similar regions and so the based
on how we set max hits we have some
different problems so if we take it too
low that means we won't try any of the
locations that the reed is going to
match against because all of the seeds
have too many hits so we'll have we
won't be able to line that one at all if
we take it too high we'll try all the
locations but then we'll spend an
inordinate amount of time trying to
align that read so what happens in
practice is because we have some middle
of the road setting for Max hits what we
end up doing is we test some of the
locations where that read would match
but not all of them and therefore this
is where most of our alignment errors
are coming from so you know what happens
is we test it against a few locations we
find one that works well and we report
that however there was actually a better
one somewhere else that we didn't test
so this is the main thing that we're
trying to address and this is important
definitely for the downstream processing
so we really want to be able to get this
right so what is our approach to fixing
this well first we're working on pre
computing those similar regions in
advance so that we can use that
information quickly during the alignment
process second works flooring an
algorithm to rather than you know you
get the cluster and then you compare
individually against each string in that
cluster we're working on a
representation that kind of reuses the
work in that comparison so that you can
efficiently do that so currently this is
a work in progress but what we have so
far
is we have a parallelizable algorithm
for detecting these similar regions
using spark which that's a project that
mentes worked on with some folks at
berkeley and is a really good framework
for cluster computing and we also have a
group at a distance algorithm that gets
quite a good speed up over I'm comparing
against all the strings fully so it's
the way that we're detecting these
similar regions is very simple but i'll
just quickly illustrate it graphically
so what we have is for each substring in
the genome we represent it as a note in
the graph and then we look at the other
strings in the genome and we compute
adjacencies so this one only has two
edits from the original substring so we
draw an edge however in some cases that
have you know too many edits we don't
drawn it and then we whenever we find
that two clusters share a member that or
have members that match each other well
we're going to merge those clusters so
we continue to do that and eventually we
end up with these disjoint clusters and
then each one of those would be the
groups that we want to compare a read
against and so how so of course this is
expensive because we have this n by n
adjacency matrix that we want to compute
over the whole genome so it takes too
long to do it naively so the way that
we've worked on this is we've
partitioned the this matrix so that each
spark task will be working on a separate
piece and what we do is we index this
only the the short part of the the
genome that this task is responsible for
and then what we do is we run our Union
find so that we get a set of clusters
and we do this for each of our tasks so
then now we want to be able to merge
these clusters so that we get the actual
set for the entire genome and the way
that we do this remember each cluster
has a bunch of different strings in it
and whenever we find strings that match
across clusters will merge those and we
continue that for the whole batch and
then we get some final set of clusters
over the entire genome
so very simple idea so species uh yeah
so definitely you would um so far far
why not just do it hey you look take a
few take a few weeks yeah I mean I guess
certainly you could do that we've been
focusing on only human uh-huh yeah we've
been focusing on only human um if you
yeah you could certainly run this on
whatever reference genomes that you
wanted to work with but so we've won't
focus on getting a parallel
implementation just because it's it's
proved to be intractable to work on for
you know in a single no one's for
species one is done you're developing
your brother one super species so look
Christine own sanity getting a mic the
clusters might change you don't know
what that's right oh yeah and this is
also something we're still kind of
working on tuning the parameters to so
you know we would like to have a shorter
or cycle for running it so but yeah it's
definitely this is something we
precompute so it's not going to affect
alignment performance okay so then how
do we use this information in snap once
we've gotten it so back to the case
where we have a read that we is going to
match against a bunch of different
places in the genome what we do is we
notice that one seed is matching against
one of these flagged locations so we
kind of flag them in advance if they're
belonging to a cluster and then what we
do is we just grab all the fellow
cluster members of that location and we
just use that at a distance kind of
aggregate algorithm I was mentioning to
test against the entire group rather
than testing against some fraction like
we're the current version of snap dose
so in this way we can kind of get some
of the best of both worlds and that
we're saving time but we're still able
to compare against all the strings in
that group so we're able to avoid errors
and in fact what we see is that once
we've incorporated this into snap if we
compare against the standard form of
snap we get a big reduction in error so
this confirms the fact that really the
errors that we were getting from the
standard version of snap are
caused by these similar regions so and
as I'll kind of get into in the last
part of the talk these alignment errors
can really be important downstream so
even though it's kind of a small
fraction overall it can lead us to
drawing some bad conclusions if we have
those errors okay so what is the rest of
the pipeline so back in the beginning of
the talk might a kind of talked about
how the idea of reconstructing a genome
is taking all these reads aligning them
that's the part we've been talking about
so far and then but the the main goal is
to know what that individual that
actually has for their genome Eddie to
position so that's the process called
variant calling and in the kind of the
simplest view it's just this idea of
taking a consensus of all the reads that
map to a particular location and seeing
what they indicate about the genome but
actually this is very difficult process
and we really are focusing on trying to
improve the accuracy of this so why is
the variant calling difficult there are
several factors and first there are some
that are just inherent to the genome
that make it difficult so these aren't
going to be going away the other ones do
have to do with technology so they can
potentially be targeted but there are
things that we have to cope with for now
so one when we're actually taking the
DNA out of the cells and producing the
sample that's going to be sequenced by
the machine we have this process called
PCR polymerase chain reaction and this
is not an unbiased process for for kind
of amplifying the DNA so that we have
enough to sequence again we also have
systemic problems that the sequencers
make when they're producing the DNA
reads and as we've talked about the
alignment errors that's also going to
confound variant calling somewhat ok so
hetero zygosity what this means is that
for you know for all of your chromosomes
you have a pairs of chromosomes that are
the same length and will you get one
from your mother or one from your father
and so for at each point on those paired
chromosomes if you have the same value
at each of on Bo
the strands of that chromosome then this
is called homozygosity but if you have
different values on the two then this is
called heterozygosity and so why is that
a problem well when we get reads that
map to that same location on the genome
they could tell you different things
about what the person actually has at
that site and so we it's not always
straightforward to know is this actual
like truth heterozygosity or is this
just that we've gotten sequencing errors
from a couple of the reeds so this is
one of the challenge another challenge
is as Mateo was kind of alluding to we
have something called structural
variants which are larger scale changes
so so far we've been talking about
mostly mutations that are one maybe up
to a few bases in length but these are
changes that can be arbitrarily large
even up into the mega bases so and and
especially in the case of cancer or you
really see a lot of these large-scale
changes so what do they look like well
first you can have a deletion from the
reference genome and the way one way
that you can detect this is that you
have a bunch of reads that map normally
but then you have a segment of your
reference genome where no reads map at
all and also so matane mentioned that
you get these reeds and pears usually
you know how far apart those pairs are
and sometimes you see that the distance
between pairs is off from what it has to
be given the parameters of the sequencer
so in this case reads that actually come
from you know a certain amount are
looking like they're way too close
together so that's that's one large
scale problem another thing is you can
have a sequence inserted this can be
from somewhere else in the genome or it
can be of completely novel sequence that
you won't be able to detect it all so in
the case of a completely novel sequence
being inserted I you'll have some some
reads from the patient's genome that
can't be mapped anywhere in the
reference genome because it's a novel
sequence then also you'll have some
reads that came from close to get that
should be close together in the
reference genome mapping way too far
apart and the page
genome so so these are also some of the
things that make variant calling more
difficult another is you can have how
would you know where they met in the
patient's genome to know they're far
apart in that napping ah so reserved to
pier dance yeah yeah you definitely
always use the paradigm okay so um yeah
that's a very valuable signal so they
wanted to trick your types of structural
variants to look at is these
duplications so in this case this is
called a tandem duplication because you
have a part of the sequence duplicated
so it's adjacent to the original
sequence and what makes this difficult
is that one thing is that you see too
many reads mapping to that portion on
the reference genome but then you also
have some reads that are kind of from
the the overlap of these two that are
not going to be mappable because they're
just going to look funny so these are
you know very difficult to detect and
they're still a definitely an open
problem for developing better tools
there we're looking into that another
thing that experiential engine is that
as I was kind of alluding to the process
to converting the DNA to a bunch of
short fragments that can be sampled
during the sequencing is is biased and
what are the biases so one is the the
molecule length and another is the
composition of the molecules so are
trying to find those are they mostly
looks
yes so they're definitely it's been a
lot of work and finding structural
variants but the accuracy is still
pretty bad and also their each tools
kind of like a point solution so some
are good at finding short insertions
some are good at finding large deletions
you know some can find more types of
structural variants but not very
precisely you know so it's still kind of
like not clear how to do it for you know
in a way that is holistic and can find
all the different structural minds so
yeah that's why that problem is still
very interesting and especially because
structural variants actually have a lot
of interesting influence on cancer and
other types of diseases kind of in a
mutation doesn't kind of look like
heterozygosity or just a mutation
typically go to both yeah both can
happen I mean of course usually you
would have only one but then in some
cases you might have both of the the
genes knocked out on both of the
chromosomes right so then because I
guess I don't know what causes mutations
I think of cosmic rays and would
probably affect one chromosome but yeah
they can but then that's kind of why
cancer takes a long time to get because
it's just like accumulating the bad luck
over like many many years right in the
human population there were mutations
under gota now a lot of the population
test gets hard there isn't really like
one gold standard and human genome when
people have mistakes out of it there's
just very person right and so you can
wander
wound up inheriting two copies of a
radially look at the population tree
maybe there's some patient that happened
in chromosome 14 and anyone one
individual copying of that in the
germline and then it spread out the
population but it may actually both of
your parents may have gotten that
through you know the fact that that your
great-great-great grandparents sometimes
are the same as each other right
necessarily true for everybody right is
the old baby extra coming back
generations and you do too to that power
it's bigger than the population of the
planet and people get confused let's see
what happen well what happened is you
somebody is multiple times or
grandparent and that kind of thing can
lead to getting the mutations in both
copies of zebra chromosome it came to
you
okay so um yeah so for sample prep what
makes this difficult is that there's
biases toward them sighs and the motif
and what this looks like is you start
with your original full strand of DNA
that you've replicated and sorry that
you've gathered from the sample and then
you send it through some heat process
that fragments the DNA but then what we
do is we apply pcr to amplify those
fragments so that they can be sequenced
throughout the sequencer and you see
that some fragments are preferably
preferentially amplified so this means
that like I was showing you here a
higher coverage depth at that
duplication well sometimes a higher
coverage steps can just be because of
the way the sample was prepared another
thing that's pretty subtle is strand by
so I'll walk through this this is from a
visualization tool from some folks that
created a lot of libraries for looking
at genomic data and well you see at the
top this T ttc etc is the reference
genome at this position and this is in
chromosome 20 and then what you see
these little dots and commas these are
reads so the dots are reads that are
mapping in the forward direction and the
commas are reads that are mapping in the
reverse direction yeah so so yeah I mean
we talked about paired-end and what
happens in Para Dende is you read them
in the reverse directions so one gets
read forward and one gets red reverse
right DNA's Jhene is it is paired right
into the double helix thing that you
want to see pictures of and then in the
double helix the the bases there's
there's faces that go together like
types
together and their directional on the
direction one alexus one way and once
the other so the information and the two
copies of helix are exactly the same
that they're in opposite senses and when
the machine does not reach them you
can't tell which one you got so one of
the props that comes in an alignment and
mapping that we didn't mention is that
you have to check both senses whether
it's you know forward or reverse
complement and then this just tells you
what sets the
Thanks so so yeah so what we have here
the reeds that you can see that are
these comas are showing a bunch of tees
so the where you see letters the reeds
are reporting a difference between the
individual and the reference genome so
but you see that these tees are only
showing up on the ones that are going in
the reverse direction and so this is
this is a case where we don't want to
trust the the reeds when they're telling
us the difference from the reference
genome because they're only showing up
in one direction and not the other so
this is this is just one and probably
the simplest example of a strand of a of
a sequencing systematic error that you
can get but definitely we have some
other cases as well so also this
systematic errors in the sequencing can
also show up based on the motif so what
we see here is that so for one thing we
see more errors when the reference
sequence is a tee and which we see with
the red tea there the other thing is
that we see more errors when the
preceding two bases to the position are
both geez so just because of the way the
chemistry works in reading off the DNA
from in the sequencer it tends to favor
or I guess it tends to have a harder
time in these cases so the fact that
these errors aren't uniform makes this
process more difficult and a further
thing is that variant calling is
actually really difficult to evaluate as
well because we don't have or we have
very limited ground truth for this and
so one thing that you can do is produce
samples of the genome with different
sorts of technologies and then compare
what you get with the shore reads to
what you get with these other
technologies but there really aren't a
lot of standard benchmarks that you can
use to evaluate for hand calling and in
fact all the variant calling works thus
far has kind of relied on ad hoc methods
of evaluation so one thing that we're
working on is developing more standard
benchmarks for our own development just
so
we can know if we're actually improving
based on the things that we're doing so
then another thing you can do if you
have the data that's really cool is
using this trio data so what's a trio
that's two parents and their child and
the reason this is really valuable is
because there's certain assumptions that
you can make that the child's DNA has to
be consistent with their parents DNA and
the reason you can make this assumption
is that the rate of novel mutations from
the parents DNA to their child is very
low so therefore if you're seeing
something strange it's much more likely
that it's a problem with the sequencing
the variant calling and whatnot rather
than actually a true novel mutation so
just in some of these examples you see
that the child's DNA is consistent with
the parents DNA because each of the
child's sites on their gene is
consistent is from one of their parents
but then for example these cases are
invalid because their DNA doesn't match
against one of their parents so this is
another really valuable signal when we
have trio data that we can use for
evaluating snip calling so this is still
very preliminary but one thing that we
did we wanted to check against the
existing tools and get get more of a
clear benchmark on how they were
performing and also be able to try out
some simple ideas and see if we could do
as well or even better than the existing
tools so what did we do we took some
some real data this is it's called the
aruban trio it's from some individuals
from Africa so we have two parents in a
child we looked at only one chromosome
so we aligned all the data to the
reference we took the reeds out then
back to chromosome 20 which is one of
the smallest ones we realigned it then
once we got those reeds that filtered on
to chromosome 20 and then we did this
this nip color that we developed as a
prototype and we evaluated it based on
those Mendelian conflict assumptions
that I just mentioned the other thing
that we did is we looked at simulated
data so the same
related data this is a simulator we've
been working on in our group and it's
it's kind of meant to be more realistic
than existing simulators because it uses
some data of with real snips that people
have found in various sequencing
projects but we restricted the simulator
to only include single substitutions so
it doesn't have any short in dolls or
any structural variants so in in our
thinking this the simulated case should
have been pretty easy and we did
something similar but of course in the
simulated case we know what the variants
are at each site so we can actually get
the accuracy there so what are the
preliminary results well so we looked at
two different pipelines this is a kind
of a preliminary evaluation but what we
looked at is the cassava pipeline which
is produced by alumina which is the main
sequencing company that we've been
talking about and they called out about
you know about 128,000 snips and they
had 162 conflicts so then we also looked
at the gtk on the simulated data and the
gatk it stands for genome analysis
toolkit it comes from the Broad
Institute in Massachusetts and it's
pretty much the state of the art for
variant calling this is the one that's
most widely used by practitioners
throughout the field and what we found
on the simulated data even though it was
very simple is that there was still 66
false positives there in you know a
pretty short amount of DNA just from
chromosome 22 and it's also very slow it
took four hours to do this process so
what did we do well on the trio we while
calling about the same amount of snips
we got about half the conflict rate and
on the simulated data we got you know in
order of magnitude less of errors and
this is something that you can run in
two minutes instead of four hours so we
think that we have some insights based
on this that we should be able to
further improve this to work on the full
genome n and also extend to other types
of mutations besides plain snips and so
looking forward that our efforts over
the next few months are focusing on kind
of formalizing this feasibility study
into a full
scale pipeline that takes us from DNA
reads to a fully reconstructed genome
and so looking at the way that this is
currently done people have broken this
up into individual stages and then kind
of separately optimize each stage and
then in order to get the data from one
stage to the next they use a lot of
threshold links you know whenever
there's uncertainty you have to just
kind of cut it off report one value and
then push the data on to lift the next
stage handle it so on kind of leveraging
all the work that's been done we're kind
of hoping instead of having this kind of
disjoint pipeline work working on
producing this end-to-end system that
will rather than truncating information
very early on in the process we're
working on propagating it through the
entire process so therefore we're able
to leverage that uncertainty in a better
way than just throwing it away early
okay so just to pop up a level as matane
mentions DNA sequencing is getting
really realistic in terms of the
affordability it's improving accuracy
and this is really going to have great
impact on medicine and variety of cases
and one of the things we're really
excited about is about producing better
treatments for cancer and the cool thing
for us is that it's actually involving a
lot of computational challenges in
addition to what love challenges so we
feel like we can participate because a
lot of the existing tools lack in both
speed and accuracy and that's becoming a
big problem if you really want to use
this in a clinical setting and based on
our work mostly with snap but also our
initial work with the rest of the
pipeline we have some evidence that we
can help with both of those cases and so
if you're interested you can check out
our website we have we've made this
open-source we have it available for
download actually we presented it at a
conference a few days ago and so we've
already kind of gotten a lot of interest
people trying it out yeah so I don't
know if there are any more questions
about the end end process so you guys we
saw the error rates for alignment and
I'm curious if you run any studies to
see how that propagates down to vary in
color yeah I mean we've we've done some
benchmarking but it's been mostly just
kind of running bwa just since that's
kind of the standard benchmarking and
just seeing how that you know what our
ultimate variant calling results are but
we we haven't really done something
where we kind of vary the parameters so
that we get better better or worse
accuracy and seeing how that gives us
the variant calling but yeah I mean
definitely that's something that we're
interested in looking on i rescind of
say snap from universes beauty wed you
see that the error is the difference in
the areas is actually helps with the
americana process it in the in the thing
that was shown that there were at least
there was at least one example like a
hint at BW in this map that we didn't
that caused it to calm mutations but
this is a way like you know it's a very
anecdotal thing we need to do the same
thing on a whole genome and I you know
we just haven't kind of thing oh yeah
and part of the obstacle is just that
gtk is so slow right we can't we haven't
even been able to run it on a whole
genome because it's so expensive and
conference we were at I listened to the
1000 Genomes project talking about this
and the errors that have come out of a
wa that caused them such problems that
he was talking about giving up on
pre-shopping altogether
assembly for all our genomes because it
was dreaming in Bunker's so that's some
evidence that I reserve problem we're
hoping we can do better and by building
an integrated pipeline you can correct
for the errors later I think I don't
know this does ghe taking anything
without that reads um no not with on my
plan is to use on that breeds yeah and
to remap stuff leave it as you're busy
looking at it because there's lots of
stuff you can tell it's just wrong yeah
I mean other you know what sets of
reasons one yeah so other tools do look
at unmapped reads but they're not
integrated with gtk so yeah I mean
that's another obstacle is just kind of
making everything work together and I
mean just in our benchmarking process it
was pretty painful to try some of these
things out so about something I haven't
wanted before but to see the earlier
part of the target so the approach is to
take seeds from the reeds and mapped
into the genome and depending on that
that's the coverage you're going to have
many more seeds to look at than the
places in the genome to compare right
but the complexity seems to be the same
if you I mean theoretical complexity not
nice if you could go the other way
around if you actually take the seeds
from the genome and try to find them in
your ease and again you can do the same
trick with stopping early but now the
stopping would be based on a different
criterion that you want to see a certain
number because you know what the
coverage is roughly so you won't stop
after you've matched set number of times
so has anybody thought about if that
could be worked out to be almost as
efficient as the other way around
because then some of the structural very
internal issues would be perhaps easier
after the fact
so you would pay a price a little bit
but I don't know well I haven't thought
about it watching was just occurred to
me now so so we've thought about doing
something similar to that or trying to
trying to do de novo assembly of
structural variants so the thought was
you go through get rid of the easy stuff
because most of it's easy most of the
stuff max unambiguously with few errors
and you can look at it call in to get
rid of it and then go back through the
remaining reads and try to build them
into context which is means if you find
overlaps there's to do it but
essentially if I press if they roll out
that you build long strings of DNA that
they seem to represent then you try to
fit them in good places we've got
breakpoints in the genome so at that
point
whether it makes sense to invert the
process from the beginning I'm sure he
issued I don't think I don't understand
is like that's it it doesn't really help
with structural variance because it
still tells you like these look like
they came from here but it seems yes
like altimas do you need to do this
detective work at the end maybe it will
help you with like the reach that I
split across both ends but symmetric but
some algorithms have done this so for
example some versions of glass do this
because they just take to eat like say a
million at a time and the index those
and then they scanned through the
reference genome when it's when it's too
large for them so in a way it's being
done but I'm not I have to think more
about that how it affects about your
cognitive haven't thought about it much
but it is always really think well it's
probably always think about we're gonna
have to leave yeah yeah but but I don't
know in the end if it's really alignment
yeah then maybe in learning the process
could be worked out it's not be
straightforward yeah I mean they're
definitely have been some aligners that
choose to index the reeds instead of the
reference I mean they're a little bit
old so I don't think we could really
compare directly against their
performance but also someone a liner
that whose goal is to find actually all
the matches for any read index is both
the reference and the reeds and then
kind of walks through it that way and
then there's also been some work for
detecting structural variants where use
assembly to reconstruct the patient's
genome and then you align the reeds both
to the reference and to that
reconstructed genome and then that's
kind of a signal for how you can locate
these structural variants that may have
been done in the past just like the way
blast was using seeds
use them use that idea much better
because a lot of these things look you
know they're imperfect so maybe right
yeah that could be an interesting thing
to incorporate subset of the reasons for
which it it works very well I mean if
you're if you're dealing with reads that
only have a couple of matches in the
genome the current process is probably
the best right it really depends on how
easy it is to find the match in the
address and strength but they're great
but for reads which have a bunch of
matches in the genome the reverse better
if suppose you have a specific goal in
mind for one particular vision you don't
need the entire sequence if you want to
know which of the seven being colon
cancers he has can you it can you make
it holistic even beyond that and say
okay this is the end goal I want to have
to update the whole pipeline to optimize
for for answering that one question I
have I say yes so one thing people do is
they narrow their focus when they kind
of gather the initial reads they get it
from only what's called the exome which
is the coding sequence or the genes that
are in your genome because that's
actually only about 1% of the entire
genome you also have all this other
stuff that orchestrates how you know
which genes get expressed or could have
just been randomly inserted by evolution
we don't really have a good sense for so
some people do just pull out that exome
sequence that and then you know see how
if they have mutations where they think
they might be but the problem with doing
that is that you there could be a lot of
other mutations that are related to that
one that you are missing if you do that
so it's kind of like the looking under
the light post approach where which is
somewhat problematic when we still don't
understand a lot of the other things
that could be related but yeah in terms
of how the pipeline might change I mean
if you're focusing more in certain areas
and everything becomes a lot easier
because you know you've really narrowed
down the uncertainty so you know which
parts of the genome that you're looking
at so and then it's also going to make a
lot of the stuff we were talking about
with similar regions not as much of an
issue so
yes
figures</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>