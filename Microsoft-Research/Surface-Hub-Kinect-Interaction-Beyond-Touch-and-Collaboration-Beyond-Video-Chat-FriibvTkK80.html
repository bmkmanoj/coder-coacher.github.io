<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Surface Hub + Kinect: Interaction Beyond Touch and Collaboration Beyond Video Chat | Coder Coacher - Coaching Coders</title><meta content="Surface Hub + Kinect: Interaction Beyond Touch and Collaboration Beyond Video Chat - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Surface Hub + Kinect: Interaction Beyond Touch and Collaboration Beyond Video Chat</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FriibvTkK80" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
welcome everyone and today we are
honored to have dr. Daniel John come
here and Tennille it's it's very very
good
I have great experiences well known is a
Tripoli fellow and it's a manager here a
research manager of multimedia
interaction and experience and she also
published 200 papers is I Triple E
fellow and ACM fellow he's a founding
editor in chief of the I Triple E
transaction alt enormous mental
development and served on the Editorial
Board of I Triple E TR Am I
I Triple E TCS ft i triple e TM and many
many conferences also he has served as
program chair general chair in the
program committee member of numerous
international conferences in the area of
computer vision audio and speech signal
processing multimedia human-computer
interaction and he's a general chair of
international conference on multi meet
multi model interaction 2015 and the
general chair of I Triple E conference
on computer vision and pattern
recognition 2017 and he received a
true-blue a Helmholtz test of time award
and has for his paper publishing 1999 so
he let's welcome distinguished saying
your talk thank you thank you sorry for
the introduction and the kadhi unis
thank you for coming to my talk this is
you know part of the IPC Auto section
Tech Talk series we have this monthly
event today I will talk about how to
combine surface hub and connect to
achieve more natural interesting with
big display as well as immersive
collaboration between people who are
just you reach either geographically
so surface hub is a large display I will
talk a little bit later it's like a
giant iPad but it's better connect
Tonetta is a RGB D sensor it will
directly capture the depth information
so if we take a look at the office space
you see a lot of white board
why bother is a great tool for
collaboration because it's big so
everyone can focus your attention to the
sheer space and they can write freely as
you can use it if you want but most
importantly you know when Z collaborate
Z brainstorm so you can see each other's
facial expression hand gesture body
posture gives awareness intention etc
okay so this is important at all and
that's why in the modern obvious you see
quite nice y ba the inner ins office at
the same time more and more electronic
displays installed in the office because
they are getting cheaper and cheaper
those displays now could be large and
could be also touch enabled ok so how to
leverage this the trend is an
interesting theme for collaboration
remote collaboration and interaction
Mouse oft has announced a new line of
product called the surface hub it will
be leased on January 1st next year
service hub is as I mentioned earlier
it's like a giant iPad but it's better
it provides the best pen and touch
experience on larger display so there
are two there will be two versions one
is 55 inches and it's always 85 inches
okay and the 85 inches surface hub has
the 4k display so that just play quite
it would be very good
and I also mentioned we'll use the
Kinect sensor to enhance the service hub
so the Kinect sensor directly captures
to the information ok the tabs
information this means now we can see
the surrounding environment not just
from the viewpoint of the sensor we can
now move out of the viewpoint of sensor
to see the environment and this is a
video in the person try to see from
different angles so the idea is that by
based on the fact that we have a depth
camera we can really do so traditionally
in computer vision we use our G B D RGB
camera ok it's a color camera to do the
computer vision tasks and third sensor
now offers a number of advantages for
example the Kinect sensor can work in
low light or even in the complete back
room because it emits you know infrared
lights and RGB will not work in
low-light condition and the Kinect
sensor can you know the Conchata
immediately is a test information so the
person in front will be automatically
pop out from the background and if we
use RGB if the back one is cutted then
it's very hard to segment as a
foreground from the background and also
because we directly get to the depth
information is a skill is known with the
depth sensor in RGB because the 3d
environment is projected to the 2d
sensor so the text is lost
ok so we don't have that scale but as
the two are really complementary so
that's why even in the Kinect sensor we
also offer RGB camera so that's why the
kinetic is RGB G sensor yes
since the sensing mode of the depth
sensor is it off the sonic it's a
infrared infrared the post is also seen
as to version files the original Kinect
is projecting random thoughts and sent
to the 3d icon charging and the second
version is using the pulse you know
different frequency and for us actually
here is the diagram we think of could it
be a new surface hub could I be okay by
integrating the tab sensor into the
touchscreen so we can have a depth
sensor on the wing okay so we can from
this one we can see the people or all
the environment on this side force from
this one we can see from the left side
and the tape sensor on top can see the
whole room okay that's a back yeah I
would say they okay we bought about here
you so yeah I need to put what's the
okay I would hear you later
so yeah while experimental setup we
don't have the integrity that device yet
so we just put a Kinect sensor next to
the large display and you can see
there's a little gap because you know
the connection sir has a minimum sensing
distance so here we need to put them a
little further away and by combining
them we'll get a more natural and
immersive interaction with touch bot so
the V border is stand for vision
enhanced immersively interaction with
touch bot the system considered two
subsystems so first one is called the V
touch it
it's about human-computer interaction
the second one called image pod is about
human human collaboration across systems
okay and we will show you in more
details so in V touch
we're leverages cues of the user from
the Kinect sensor for example how far
the user is from the surface hub or will
he is standing
what who is the person whether the
person is in the left hand the right
hand and what just shows cetera so to do
that to have a better interaction for
image spot because the step sensor can
reconstruct as a remote person in 3G so
we can manipulate the person in sweidy
and the render it appropriately so we
can achieve real-time immersive
experience to feel that we see the
reference point of the remote person and
shield feeling that we are standing in
the same space and being aware through
the case of demo person predicting demo
person's intention so we really try to
achieve the experiences that is the two
people are standing side by side in
front of the same whiteboard so now let
let's take a look at about a retouch the
first a subsystem vision has the touch
experience so as I mentioned the earlier
we really want to leverage the Kinect
sensor to extract cues about user
position proximity person ID hand ID
just your ID any attention okay and here
is the system diagram from the Canadians
we apply a number of computer vision
based algorithm and they check to the
cues and then we use the cues of the
user to design interactive applications
to work with the large display and full
key computer vision technologies are the
sensor display calibration human
skeletal tracking hand judge finishing
and personal condition so I will explain
one by one shuttle a so first technology
sensor display calibration so we use the
Kinect to sense the user
as a sensing result is expressed in the
coordinate system of the connect
connoisseur
okay and the interaction is done in the
coordinate system of the display okay so
two need to be brought together and sets
the calibration process so we needed to
determine the location matrix and
transition vector between two and this
can be easily done by typing a few
points on the display because by tapping
on the display
we'll get the touch point that's in the
coordinate system of the display and
there's a fingertip at that time is the
XYZ is obtained from the Kinect sensor
so then we can compute the rotation
matrix and translation so insecure
fishing is done can be done very easily
so second technology is about human
skeletal tracking a human in our case is
represented by a few joints like as a
head neck sugar etc and each joint is
the present either by the XYZ
coordinates and we can compute from the
X Y Z we can computer the angles etc and
Kinect sensor 20 body joints in your
time and here is the pipeline how it
works so from the depth image each piece
of each step pixel is classified into a
body parts okay so for that but if we
take this piece of we say okay this is
probably the right sugar okay and this
Siskiyou so is para para into the you
know right hand you cetera so that's a
per pixel inference for each step sense
peps pizza but this could be very noisy
because it's determine the purpose oh so
where is M
aggregated the information in a
neighborhood and this will generate
hypotheses of the body joints
and this is still noisy too noisy and at
the last stage we apply the kinetic
constraint as well as the temporal
coherence and this will give us scatter
checking result and here show you some
example here okay this is the input
depth image and that's the infer the
body parts and this is a different view
of the joints okay and the full
human-computer interaction hand that
gesture is very important so here I will
talk about how to do the hand gesture
recognition we in our current system we
need only a few gestures there are two
gestures actually one is the gesture
when the person is very closer to the to
the bot okay this includes the color
palette in order to choose different
colors or typing pointing and for the
farmer when the person is pretty far
from the body then pointing or clicking
etc is needed and what we do we first
from depth image to segment the hand
again cut out to the Han region and then
we build this local occupancy pattern
around the hand okay and paste answer
whether is a we divided into a number of
cells and count the number of points in
C R and then we do the multicast support
vector machine to determine the new mod
left hand near more the right hand
different gestures and the training data
we have now is only from four people we
are working with more people now and
each person is asked to walk in front of
the large display with each gesture
and at the wrong time based on the
position of person we can determine
whether we need to apply the new mode
the judge your farm or the judge you and
then we apply as a classifier that's for
the hand gesture for personal
recognition I'm sure everyone knows you
know we do the fist clinician but we do
actually a little more than fist the
machine we combine this past body
appearance like a closing okay so reason
is when you walk in front of the in
front of the surface hub sometimes you
are not in front of who you are not
fishing the Kinect sensor so you don't
get as a front of view of the person
right but body appearance the closing
you can see from everywhere okay and
this is usually stable at least for the
whole day you know you don't change the
clothes during the day okay and even you
don't see the face the body is that as a
clothing will help you and if we
sometimes this is a ambiguity between
different people once you see the face
it will be it can be easily just
disintegrated so we really try to
combine the advantages of the two
modality I get two parts face give you
accurate recognition for front of you
but very bad for side views and
occlusions and body appearance is not a
robust you can be confused when people
we are in different clothing similar
clothing but is robots are for side
views and for the face recognition part
we use so see is a fit usually you know
get to the features and then you have
the classifier in our system we use
multi skill LBP okay
local binary pattern get crater we
choose a lot of features but we use the
PC to reduce to 2000 and then we use I
joined Beijing method to determine
whether this is
as a user or not the pizzas over the
back okay if you want to view them
hungry for the body appearance we you
know as we are tracking the person so we
have the upper body lower body hands etc
so we just usually a few parts of the
body and for each body part we also have
different angles you know based on that
or it pulls orientation in with respect
to the Kinect sensor okay and then for
each part and each orientation we have
the color histogram and the pizza
ecologies again we can construct a model
after the appearance so here I will show
you some result so the person is coming
and initially it's unknown but as as
soon as the one as the front of you is
seen then the person is recognized
unknown and then quickly it's recognized
item okay and as it's a low bar to
occlusion and now the impelling you
trying to defeat a system by changing
the clothes so now he's coming with
different clothes then it's initially
unknown but as soon as a person is seen
front of you
the probably scenes and it would be
associated so named piece with the
appearance model so usually so you know
I you know our system usually is you
know a team member so ten people above
yeah yeah you have a hundred thousand
dead several points for facial
recognition using PC 82 to reduce the
rank of that problem in 2000 yeah that's
a great big matrix it's a bigger metric
how often you have to invert that is it
once for appearance of a new face was it
regularly down or periodically done so
it's a model in successes that's pretty
faster so it's now would be the
projection is really fast but but in our
case actually we don't run every frame
it's about of you know five six frames
because in the middle time we can track
right so it's a running muchif red
threads the situation here so we you
know everyone it's not everyone it would
be in the in the database you know isn't
it either part of it is that you want to
identify whether this guy that's whether
some the object belongs to whatever said
so if the database is infinity is end
yeah a lot of confusion so they when you
have a smaller size in the database that
you want to recognize much easier still
you know one side of you is a chaser is
yeah it's harder right lighting
condition yeah so now we'll show you how
the retouch works here are a few
applications for example can bring up
menu without touch you can just play
menu wherever you are and I can augment
touch with hand ID puts an ID and
gesture ID and hover
I would explain those however here
pointing auto lock of the display so
when you walk away from display it can
automatically lock the screen for the
for the privacy of the content and when
you come back if you are part of the
session it can unlock for you to
continue the succession okay
and here are a few screenshots I will
show you the video okay so like a
showing the palm open it will bring
automatically is a color palette menu so
you don't need to go to the Start menu
in a drawing color etcetera okay you
just show the permit same thing we know
for typing and we recognize different
people so we can determine who is right
who wrote what
okay like this case is a user one this
is a tool okay and here when the two
people write a player game automatically
we know who is who so you don't need to
change the color it will automatically
give assign different color for
different people and here's the screen
launching etc and when the person is
pretty far away we can have the pointing
and we distinguish whether you are using
left hand to point or right hand to
point and we also distinguish whether
you are writing with the left hand or
right hand or with a pen and another
thing which usually is missing in the
touch touch display is the hover so in
having when you move the mouse in front
of the object then automatically some
content menu will come out right but
touch you you have to touch in order to
do anything
yeah but with the border we know whether
you are close to the object on the
screen then we are bring with them in
this case we are moving closer to the
Chicago's and the context menu will come
out and other part of we know there is
no context meaning okay and then you can
touch the content menu cetera and also
you know menu can follow you so wherever
you go so menu will come handy okay so
you don't need to run to the start part
you know to get me if especially when
you have number 85 inches just play so
now let play a short video
and the feel here he's a pathologist and
he's now meriting that sensitive this
video thanks to a Kinect pointed across
the display he touch can understand
where the user is who the user is and
what the user is doing even before the
user touches the display gestures such
as an open palm may bring up a color
palette from which the left hand may
select erase the pen may select green
and the right hand may select red these
selections are remembered a race with
the left hand right green with the pen
and write red with the right hand
another gesture a pair of high-fives
opens a keyboard making it easy to enter
text anywhere on the screen hovering
over the text brings up a menu of
attributes which may be selected to
change the color or size of the text
different users can also be
distinguished using vision another user
may select his own color and stroke
width when the first user returns his
settings are remembered
the strokes and text are tagged as
belonging to user 1 or user 2 V touch
knows the user's location not only can
it bring up a menu near the user but it
can keep the menu near the user so he
does not have to hike back to the far
end of a large display pointing for
interaction when the user is away from
the board
it's also possible so what did courage
to whether gesture detection is robust
against other gestures the right hand is
distinguished from the left selection is
also possible v touch requires the
Kinect to be calibrated with the display
but calibration is easy simply by
touching four points on the board v
touch uses vision to enhance touch by
detecting gestures whether far or near
to the screen distinguishing between
hands when they touch the screen
restoring hover to touchscreens
distinguishing between users and
tracking their positions v touch vision
enhance interaction for large touch
displays so we have contacted user
studies and people really like it okay
most people just one person you know
feel 40:14 is not is a derivative
shocking and one person feels that left
hand the right hand is too difficult to
remember so you know when you to
something see just people just touched
with the point always with one hand
rather than left right okay but other
than that you know people feel it's very
very good yeah okay so now let's move on
to the second sub system called the
immerse pot using enhanced image remote
collaboration so before we talk about
remote collaboration let's look at cool
or heated collaboration so for them two
people standing in front of a physics
whiteboard what
you have you see different people okay
in front in front of you you see the
fistula finish feature expressing ideas
JETRO etc okay so you really have the
full bandwidth across people and also
you see immediately it's a content
creation okay because it's user
sim space right and now let's look at
the current situation about a remote
collaboration so what are you you you
have you have a window of the demo
person it's a talking head of the demo
person and you have a shirred space to
collaborate with the more people but the
tool are disjoint they are not connected
with each other so what do you have
between people you have you only see the
face okay
very seen bandwidth is draw here and the
for the content you don't know how the
content was created ugly you just use a
final resolved pop-up you know screen
right because you don't see such
gestures pointing is a traffic okay so
what we want to do is really trying to
bring the two space together and they
create a much richer bandwidth between
the between the faces and between the
content creation so here is one example
in the person point here okay so now the
person can move out of the box now and
little EMU pod can see okay and I wish
you later home so again just to repeat
because the connect has 3d information
so we can manipulate so sweet
information of the limb or person to
render it in a appropriate way on screen
and then we can see the reference point
of new person
people feel we are shown in the same
space and I will GE is the predator
intention and we have implemented two
metaphors so first one is on a
whiteboard okay you see how this is done
second one is about two people writing
on a mirror so rather than look into the
side you can look at the remove the
other person through the mirror so
without turning the head you know okay
so this is a little bit different if
some if you have seen in a class war
video of a video then you know you have
to write backward right and here in our
cases we don't need to write backward
because you write in the same side just
so you look at the person on the outside
it's a circular on in the middle and we
have implemented the arches three okay
so that's the two people writing in side
by side that's the writing in front of
the mirror and here is the exactly same
thing as the kernel system you know it's
a touchy head and you shoot a space but
now we manipulate the hand to touch the
reference point so you see now let's
play the video is a system for remote
collaboration through an electronic
whiteboard with immersive forms of video
thanks to a Kinect camera mounted on the
side of a large touch display we augment
shared writing with important social
cues almost as if participants were in
the same room this two and a half D
hybrid between standard 2d video and
full 3d immersion shows the remote
participants 2d video on the side but
his hand is able to extend out of the
video frame in a non physical way to
write or to point to a place on the
shared writing surface this
visualization shows the remote
participant in full 3d as if the
participants were standing shoulder to
shoulder in front of the physical
whiteboard I gaze Direction is improved
as well as gesture direction body
proximity and overall sense of presence
so what do I do here is you know the
Kinect capture the 3d information shape
of the person right plus the color et
cetera and the Zen we have this wide bot
okay-y body in 3d space and
essentially the surface hub surface and
then the person then we move Q to the
whole thing inside and the project on
the screen that's why we get this tooth
border process the person so you can see
now the I gaze of the more person the
participants are able to look at each
other to check for understanding
finally this visualization shows the
remote participant in 3d as if reflected
in a mirror on which the participants
are writing eye contact and gaze
Direction are further improved as well
as overall sense of presence averse
Board makes remote collaboration almost
as good as being their natural remote
collaboration so that is the final one
you know the video quality is live with
the pad because it is really easy you
know you observe a person from a side
but you render really from the front I
can't you see only the half of the
person but you know it's a IKEA stretch
how is the interaction will be more
different
so we are now trying to improve the
result by adding another Kinect and this
one you know people you know feel good
like a lecture you know it's pretty cool
and this one has the highest video
quality because the video is exactly the
same as the original one but we kissed
Otto's hand people she neutral interview
is very awkwardly not to Caesar's this
long stretch the harm but after wire in
a people feel comfortable and also in a
full see you you see the long stretch
arm from your point of view but for the
person standing here because of the
foreshortening which is a view right
it's not very long actually so it's a
steel except when you work a little bit
in front you forget to stretch the arm
but the waist you need to make us a hand
the more stable etc
okay so to summarize I just presented a
system called the V border which
combines the service hub so pick a touch
board with the Connexions
IGBT sensor and they actually enhance
the immersive interesting with touch
board so by leveraging is accused of the
user using connect now we can interact
with the large display beyond the touch
ok even before you touch we know a lot
of things about user and the using
gesture precisely you know it's it's
very natural and rich and also because
the canet captures the information by
rendering appropriately we really feel
the more people can work together as if
we are standing side by side and as a
future research will be inner to try to
unify it's expensive from largest play
to surface - you know for maybe and this
is a joint work with the implant future
he's here and to say ok thank you for
your questions yes you said something
about using mobile connects yeah so the
interference actually you know if you
can synchronize you can avoid the
interview and the connection is not
really for your search purpose right so
it is no synchronization to make the
chip actually the interference was
pretty bad in the ins for the older
canet you know when you put a random
dots so when you add another connectors
the red dot will be confused again with
a new one actually it's much better she
has still some interference
yeah actually you tried to the render
randomization publishing right yeah so
when he was at a use even see me
you can't explain you vibrate both the
camera and the projector the two units
are rigidly attached well we see their
own projective pattern sharply with the
floor out with all the other patterns so
you can you can have a little accent oh
man how much family sorry how far did
you collaborate here to China oh no this
is I mean well you because we want to
see some other person's experience right
so we really put a side-by-side yeah and
sometimes we simply to buy a cotton you
know but a bitter bitter the idea is
really try to have the system deployed
from China to here yeah how many points
to the body in Jonesborough you when you
for the second one for the second is a
subsystem right you need to render so so
you know it's a about 500 times 400 and
then you have a smaller portion of the
person so I don't know you can take
better yeah resolution is so tapes since
the second of our IV it's a 500 by 400
something just the RGB I know I'm
talking about the depth sensor yeah yes
smaller so the RGB is HD cuz you need a
lot of pixels on the face in order yeah
yeah y'all got to have the good of you
yeah because the geometric pattern if
you can still using triangle okay
patch and the color will be met with
China
yeah questions no okay so I'm not clear
on how to connect depth sensor works are
you projecting fiducials just for the
first connect generation generation is
projecting random fiducial points and
that pattern is known
it's okay so then you capture the
projection one from the second image is
from the camera IR camera and then you
need to met match observer the one with
the other one then you do the
translation in 3d space so second one
you the camera in the protector offset
metric yes but that one would be in a
plane right so giving the third
dimension requires it's not a plane
sensing just oh no they're offset so
here's the protector over here
projecting up that way and here's the
camera over here and so it's like a
stereo camera sure just that's only in a
plane right how do you get the out of
plane resolution when you're doing when
you interpret metric in a plane because
there's a whole locus of ambiguity yeah
so now the pattern is really random is
very unique okay
quite clear here and for the second
issue maybe feel you can explain the
it's a very different chronology okay
it's fast it's there are different kinds
of time of flight cameras mmm one is
pulse mm-hmm
pulse is gated mm-hmm this one is
actually a third nine okay oh yeah
modulated cycle soit and you measure the
phase space ranging but it's still in
correct
right yeah then my question was how much
bandwidth do you need with this
so it depends on how you implement again
for flexibility you want to send the 3d
information to the other side and render
in that case in that case you need in
those tabs information passes the hollow
so you have additional you need
additional bandwidth but another way to
do is render if you notice that
complication is fixed for the demo site
so you can render from this side right
and then send that the 2d images in the
other side and then it's almost same so
this is most people you know for two
people and the country we are only is
the system work it's only for two people
actually but if you think about it it is
tending to more people
since the rendering will be different
for different people because each one
will have their own location their own
ideas etc is that his better to send the
broadcast the 3d information to the demo
site and then you know they will be
rendered in your time according to a
user's location and the poles so the
bandwidth is not much higher in that
case Benito will be a little more yeah
if you're on dub sensor is like 500 by
500 in your colorist HD yeah actually
the most pen with depending how these
properties the noise in the tab sensor
so we need to you know annoys most oft
anything here oh yeah okay you okay
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>