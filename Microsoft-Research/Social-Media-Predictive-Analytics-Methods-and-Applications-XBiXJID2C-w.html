<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Social Media Predictive Analytics: Methods and Applications | Coder Coacher - Coaching Coders</title><meta content="Social Media Predictive Analytics: Methods and Applications - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Social Media Predictive Analytics: Methods and Applications</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XBiXJID2C-w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so it's my great pleasure to welcome box
econo is from Johns Hopkins University
interviewing here for a postdoc position
thank you so much you mean today we'll
be talking about social media predictive
analytics methods and applications I
don't think I need to sell you on the
importance of social media nowadays a
social media gives us access to
personalize timely diverse multilingual
data on such a large scale you can think
we are talking about millions of
messages and a billions of users and
this kind of data allows researchers to
build a variety of social media
predictive analytics as soon as
researchers that in my started mining
this data and media picked up and you
could see headlines like fake facebook
data know you better than your mother or
computers judge personality better than
your friends or even scary ones like
social media your social media profile
can cost you a job and a really
controversial study that was done by
Facebook this summer were researchers
manipulated user emotions to study the
emotional contagion and social media so
um people who are working this kinds of
research have to be really careful about
ethics and privacy let me tell you about
my research I use natural language
processing and machine learning to work
with social media I'm primarily focus on
author attribute prediction I also do
emotion and sentiment classification and
a bit of attitudes towards events and
entities you can think about an example
of Russian Ukrainian crisis for example
and this research ended in a variety of
publications I have also been working on
unsupervised dialogue learning and
language grounding and in
extraction and named entity recognition
in the medical domain so let me tell you
she is so interesting first of all it's
very short it's only 140 characters it's
very lexically divergent and abbreviated
and multilingual we have many many
languages and actually more than eighty
percent of twitter is in languages other
than english so you can mine tons of
languages easily but doing this research
is very challenging first we are working
in a highly dimensional feature space
and also you should another issue is a
data drift so for example one concept
that was popular a year ago like a euro
my down in Ukraine is not discussed
anymore another issue is user activeness
and since users are active in social
media to a different degree it reduces
the generalization of the models the
next issue is topical sparsity people
don't really talk about politics in
their everyday messages people who are
politically active they are talking
about politics but but random Twitter
users that they don't care about
politics the same is the relationships
and so on and the last but not the least
is dynamic and streaming nature we have
to work on the models that allow to take
into a cabin to take into account
evidence that is coming over time
iterative Lee and that's what I'm going
to talk about today so here's the
outline of my talk I will start with a
machine learning and NLP for
computational social science and tell
you about our study relating user
properties and emotions on Twitter and
then I will move to machine learning
methods and we'll talk about online
streaming inference as well as dynamic
learning and inference from the mix
streams so users and social media are
connected
and you have your friends you have
people you repeat you follow you
mentioned and imagine we have predicted
user emotions so things that they feel
and think that they feel their opinions
and also we have demographics for this
users predicted or ground truth so we
want to study two questions first
whether we want to figure out whether
there are emotional differences between
users and their neighbors and second we
would like to if there are such
differences we would like to correlate
these differences with user demographics
and this study was inspired by this
controversial research on emotional
contagion on facebook so let me tell you
how we sample the data and sampling is
really important when you work with
social media because there are so many
sampling biases you can bring while
working in social media so for this
specific study we sampled
english-speaking Twitter users from the
US and Canada and we were looking
specifically for average Twitter users
we didn't want any celebrities or we
didn't want empty accounts we sampled
people to meet between four and ten
times a day and have more than 20
friends and it's not a random number
this was empirically estimated on a
different data set and confronting the
literature to get random to your users
averaged sirs two following day it got
10,000 random users was there a 200 most
recent weeks after that we use the same
strategy to sample their neighbors we
sampled five track on average of people
they mentioned and five of people they
retreat at the end we ended up with
having 100,000 23 users 25 millions of
their messages this is the size of the
data set we are working with and it's
comparable to the this emotional
contagion study on facebook it's huge um
so let's talk about the approach first
we would like to be able to predict user
attributes user demographics to do that
we have developed supervised models and
to get labeled data we use crowdsourcing
then we use this annotated data to
perform mmm attribute classification the
next we developed emotion we used a more
data annotated with emotions and
actually to annotate this data we use
distant supervision and by distance
supervision I mean that here is the
example of the tweet that includes the
hashtag sadness and we say that this is
a sad tweet of course this is not
perfect but this is the best we can do
as of today as for sentiment we used
publicly available sentiment datasets
collected tweets both as positive
negative and neutral using this label
data we developed emotion and sentiment
classification models and applied
emotion and sentiment classification
models as well as or attribute
classification models to make
predictions for for Iraq users and their
tweets so imagine you have a database
where you have users labeled with their
demographics and you have tweets labeled
with emotions and sentiments yes
sourcing next slide really good question
so now let me tell you first how did I
get the annotations and second how did I
build these classifiers so how do we get
this label data it's really challenging
because we are coming to a new domain
and we are doing supervised
classification we need to get
annotations it's really costly and
difficult time-consuming to get there
are several ways you can get the ground
truth first you can think about fun
psychological tests the people take
voluntarily on facebook they have
nothing to do they just take this test
and researchers are collecting this data
I
and I think it's really clever um next
way you can collect this data mine you
can go look for self reports you can
look for tweets that say I'm a
Republican or happy x birthday to me but
this kind of approximation will bring a
lot of biases to your data data
collection the next possible approach
will information so for example like
like toward with people from the
Cambridge for faith-based would mesh
like gender age relationship status but
unfortunately for Twitter there are no
such data and it's extremely sparse
another approach you can use distance
supervision or alternatively you can go
and do crowd sourcing and actually
crowdsourcing have been successfully
used for a variety of natural language
processing tasks that I'm more
challenging than author attribute
prediction for example machine
translation or a predicate argument
agreement really difficult challenging
tasks that require a lot of intelligence
and to mention that all of this data
annotation approaches have a lot of
biases if you want me to tell more we
can talk about it later so how did we do
the annotation we decided to go with
crowdsourcing we use the trusted crowd
on Mechanical Turk we paid six dollars
an hour and in addition to that we also
embedded a lot of quality control
questions to ensure the quality of the
annotations we showed 5,000 profiles to
these users and we asked annotators to
label a variety of attributes for us and
on this graph i am showing you the inter
annotator agreement which I which we
estimated or on a random sample on a
random to present sample of the data due
to budget constraints we only underrated
5,000 profiles by one annotator and we
calculated the inter annotator agreement
on a sample of it so as you can see
of the attributes are highly agreed on
like ethnicity and gender at the code
the agreement is really high and then
for the majority of the attributes like
children aged life satisfaction income
Optimus level at the agreement is solid
but for some attributes like a political
preferences religion intelligence the
agreement is fair so you should think
about it these attributes are really
difficult to predict em and when
annotators are looking into this data
it's really difficult for them to find
the signal in the profile the slide
answer your question right so this
objective the answer is it's a
subjective thing improvisation exactly I
will mention this issue later yes so we
are looking for subjective perceived
annotations yes so yes so did you did
you let people express any sort of
uncertainty or kind of confidence in
their know this crowdsourcing experiment
but it is for a variety of the of the
annotations for my thesis actually I
don't think it helps they are to
expressing the confidence if I just say
first of all we didn't work with the
local winery annotations we collected
multi-class annotations so you can think
about it that we gave them a freedom to
express the degree of confidence on the
scale so for example for narcissism will
say do you agree that this guy is
narcissistic and we say strongly we
agree in either agree or disagree agree
and agree strongly right on a scale of 5
for example for this specific attribute
all as well know for me it was binary
for binary attributes binary so now let
me yes can you tell us what this measure
is on what's Cohen's kappa and what does
point for of going to
so it measures the how annotators agree
on their annotations usually you don't
collect one annotation you collect more
than one and then you see how often they
agreed on this on this measure on the
annotation income is discretized in some
to some number yes I often do yes they
agree yeah it just yes it justice for
trans agreement isn't right yes so we
see essentially how much more they
agreed it would be exactly yes it's a
standard measure for this kind of tasks
so now let me tell you how did we
actually build the models we are doing
nothing fancy here we are working with
log linear models or logistic regression
and we are binary singh all of the
attributes so you can think about it as
a binary classifier and we are working
with a range of features so you can
think about word and rams character and
grams or stamps costums linguistic
features you can also think about so
chilling with syntactic and stylistic
features like part of speech dogs
punctuation parse trees different
Mexicans another kind of feature is
effect like emotion sentiments attitudes
topics communication behavior and
network structure features but in this
work we we have only used the top three
but if you interested in talking more
about the features i can tell you more
and so back back to the question about
the subjectivity and here we are trying
to perform a more ethical study compared
to what Facebook did but this study has
several limitations first we are working
with subjective annotations these are
pretty perceived annotations and some
attributes are more difficult to predict
than others and to honor
like income it's really subjective
intelligence is really subjective also
we are using distance provision to
collect this hash and to collect this
emotion annotations and when we use it
we are not really taking into account
the irony so for example imagine there
is a tweet where you say it's Monday
hashtag Joey versus it's a hashtag joy
so the hashtag was using a sarcastic
ironic way and we are not actually
taking it into account more aware we are
not we don't have any no emotion has no
emotion class which is again biases our
results a bit so how do yes Chris hash
time yes so how to interpret our results
correctly um let me warn you that we are
drawing conclusions from predicted
attributes not ground truth and also we
don't imply any causalities we are only
looking for correlations so here are the
results for attribute classification and
and I'm showing you the area under the
curve because the data at the data sets
where I'm imbalanced for for the
attributes and as you can see race and
gender or the most are easier to predict
them for example religion and like
education intelligence is not bad and
actually we these numbers are really
good they did the state of the art and
my models from my thesis were the state
of the art in this field and we actually
bring these models and the evaluations
were done using 10-fold cross-validation
next let me tell you how did we do in
terms of emotion and sentiment
classification so in addition to using
bag of work features for as we did for
attributes we added lipstick and
syntactic features like negation for
sentiments it's extremely important to
distinguish the feature versus happy we
also add allegations capitalization
hashtags convicts punctuation and you
wouldn't believe but these features
significantly improve your f measure so
regarding of measure
remember we are doing 65 classification
it's basic Eggman's emotions which is
joy sadness fear surprise disgust and
anger and we are getting really good F
measure which is now the state of the
art and we are bidding a variety of
previous work on that and I can even
tell you why we do that first we have
much more data a second we have much
better features nothing fancy but it
works regarding sentiment classification
we are classifying tweets three classes
positive net neutral and again we are
getting a really good at measure which
was actually estimated on a standard
test benchmark test for this kind of
tasks and actually the first the first
team got sixty-eight sixty-nine percent
of measure and we are getting 66 it's
not really fair to compare because we
were using different training data but
for the purposes of this study give you
an idea how well we do in terms of
classification and that's the way to
compare so let me recap a bit of the
task so what we're trying to do we are
trying to measure the emotional
differences between users and their
friends for example and imagine you have
a Twitter user for whom you have a
variety of predicted attributes and you
have a set of incoming tweets per user
for every incoming tweet you predict
emotion and sentiment and we call it the
environment emotional tone and then you
have a set of outgoing tweets per user
and for every outgoing tweet you predict
emotion and sentiment and we call it the
user emotional tone and we are looking
at us via for differences between its
distributions give your demographics so
given that this guy is a male how their
these two distributions are different so
let me tell you about the methodology
first we would like to measure the
similarities between two distributions
and we do this using Jason Shannon
divergence which is a symmetric a KL
divergence so basically we are looking
between
we are comparing the incoming
distribution of emotions and outgoing
asian of emotions for each user and then
the next thing we do we look for this
deltas so imagine we take one emotion
for example sadness and then we say what
is of your outgoingness minus your
incoming sadness normalized by the Sun
and we do this for every emotion and for
every user we have this Delta numbers
and then really simple to to compare
this distributions of Delta's we perform
a meant Whitney nonparametric test on
Johnson Shannon divergences and emotion
differences so for example let's take
people who are predicted to be male and
like let's take people who are predicted
to be female and have a distribution of
sadness from in the mail group and the
distribution of sadness in the female
group what we would like to compare we
would like to compare to means of these
distributions and say whether this
statistically significantly different so
that's what we do for the analysis any
questions here okay so let me show you
the results so we found that the media
of half of the emotions actually we're
amplified and let me tell you what do i
mean by what what i mean by
amplification so if the amount of
sadness that i am outputting is more
amount of sadness that isn't coming it
means I'm amplifying sadness vice versa
if the amount of joy or if the amount of
sadness again is is is lower than the
amount of incoming sadness it means i am
a dampening sadness that's the way we
will use these terms of amplification
and dampening so for these three
emotions for disgust sadness and joy
they are always amplified to a different
level by users with contrastive
demographics accept people who are older
and have kids they've always dampening
sadness which is very interesting
and actually in this psychological
literature will find similar trends
regarding age and having kids the same
is for joy all of the all of the groups
of people with suppose predicted
demographics always amplifying joy
people who are predicted to be stressed
and pessimists they are damp and enjoy
always for the next three emotions
people are dampening surprise fear and
anger except for people who are
predicted to be dissatisfied with life
and not excite people who are
dissatisfied with life and not excited
they are amplifying I'm sorry they are
yet they actually amplify anger hey you
can think about stereotypes but that's
what we are from the data and it's in
the literature and given the scale of
the data i think these discoveries are
really interesting and then we found
really nice things that haven't been
reported in the literature but it's very
interesting first regarding sentiment
let's talk about neutral sentiment
people all of the people amplify neutral
sentiment so they they say more neutral
things that they get accept people who
are older with higher income and degree
you can think about it like that people
who are older get more money and more
educated they are more opinionated they
express more opinions positive or
negative it's like in the reviewing
process you don't say them the number in
the middle you either don't like the
paper or you like the paper so I think
people are just more confident by having
mine you and being more smart regarding
positive a neutral sentiment it's always
dumpa except from mail users and people
who have kids mail users and people with
kids they are amplifying a positive
sentiment they are more positive
compared to the contrastive attributes
okay so given this results we decided
first we find these correlations they
are really interesting
let's use these correlations and with is
what we decided to do is to to build
back better features for our models in
addition to using bag of fort features
we decided to go with sentiments and
emotions and that's what we got so I'm
showing you a similar graph that I
showed you before for the variety of
attributes this is again the area under
the ROC curve but i'm adding i am adding
a more outgoing emotions and sentiments
as features and the emotion and
sentiment differences and you can see
that you can compare in the column on
the right how and what doubt again we
get for every attribute for some
attributes like gender and ethnicity the
deltas are lower because we did good job
before in predicting these attributes
but for some attributes like for example
political preference and age we are
getting 17 point improvement which is
really good also to mention you can
think about emotion and sentiment
features as a meaningful dimensions to
reduce your to reduce your feature space
because this emotions and sentiments
were predicted from the from from the
tweets from language and you can think
about it as basically as a
dimensionality reduction tip but if you
use SVD you don't know that I mentioned
you give the meaning to this dimensions
so to summarize first we show that
different demographics leads to varied
reactions in the emotional between users
and their neighbors even that I am a
female I will react differently to angry
tweets compared to too am a user and
also when we use emotion sentiments and
and this Delta as features we are
significantly improving also attribute
classification which is really important
but let's let let's ask what if
and let's think about the real world
area um in the real world scenario have
twists that are coming over time you
don't have uh you know you don't have a
setup where you have a set of training
day day and a set of test data like we
use be in the real world scenario you
will also want to take into account the
actual the relations between the users
so far we predicted users independently
but didn't look into this users as a s
connected in the social network so right
now I want to switch to the machine
learning methods and tell you more about
streaming in France how to make
predictions on the fly in a streaming
fashion and then in addition to that I
want to tell you about then learning and
predictions so we want to learn and
predict on the fly so unlike previous
approaches which use static batch
predictions which means that we had
offline training and offline predictions
and we didn't really take into account
network structure we would like to move
to streaming online in France which
means that we will still have offline
training but we will try to make
predictions online in addition to that
we would like to explore six different
kinds of relationships between Twitter
users like friends followers retweets
and so on going beyond that we would
like to learn over time as well as make
predictions time which still means that
we are making online predictions and
relying on neighbors but in addition to
that we use iterative retraining we use
active learning and we also do some
effective rational annotation and I will
tell you what are these rationals adding
these advancements to our approaches
will will help us to handle the dynamic
and streaming nature of social medium we
will also explore the network structure
by making predictions from you from
joint user neighbor streams and also we
will look more into the tray
gulf between production time and model
quality which is really important for
for practical applications so let me
start with iterative Basin predictions
so we have a task we want to predict
user political preference and to make
the spread to make these predictions we
are using simple Beijing rule update
where we start with some prior and the
issue of priors on twitter is extremely
important and if you want I can talk
more about it so we start with some
priors and and every time we get a tweet
we update our likelihood so we're
getting the probability of the tweet
given the class given a Republican and
every time we update our posterior
belief about the label for this specific
user let me show you an example so at
time t0 we only know the prior let's say
we start with the balance prior 0.5 it's
a binary classification task as soon as
we get a tweet / user we update our
posterior and then we get a tweet from
user neighbor we update our posterior a
tweet from the user we update from the
neighbor and so on and this posterior
actually is actually telling you our
confidence in the prediction so you can
think about it as a time tau 2 I am
sixty-two percent confident that this
user is a Republican and a time tau okay
I'm seventy-seven percent confident that
this user is a Republican so what which
you get you improve your posterior
belief and actually you I'm developer
you can think about it when you should
stop and I will tell more about the
strategy when to stop so whether you
want to have less confident productions
versus and were confident productions
you decide when to stop now let's move
to learning how do we predict this over
streaming data as well as learn better
models again we are doing political
preference prediction we have a small
set of users labeled as Democrats and
Republicans
and we have a bunch of unlabeled data so
the simple the simplest brute-force
approach will be as soon as we get new
tweets we train the model we trained to
look listen you're a model we predict
for the unlabeled users you get more
tweets Yuri estimate your weights you
you use it again you get more tweets and
again you apply it in the streaming
fashion I just described you before so
this is a simple scenario just
retraining your model it requires a lot
of computations again your feature space
is really extremely sparse and it's
highly dimensional but you can use this
iterative batch retraining in two
scenarios with and without rational
filtering and rational filtering it's a
totally NLP concept so in a dip to
asking people to annotate a user's as
Democrats and Republicans we actually
asked them why did they make this
decision and they highlighted and grams
in the tweets that are predictive of
Democrats versus Republicans so for
example a unigram like waters president
passes senators are predictive of users
that are Democrats and economics palin
drilling representatives word slip
street journal hunt actually kiss owed a
predictive and grams for republicans and
and when I say we apply rational
filtering which it means that when we
retrain the model in a dish instead of
adding all of the tweets that are come
that are available up to this time we
only add tweets that include these
rationales doing this allows us to to
shift the classifier to make more
accurate decisions so your training data
is extremely accurate it's really
precise am and it's lower dimensional so
you it requires less computation and it
actually gives you better predictions
better quality of your predictions so
now let's move to active learning the
active learning scenarios similar to
iterate battery training you have we
have a set of labeled users and a set of
unlabeled users connected
network and again we are using the same
patient will update when we make
predictions and as soon as we predicted
a label for a user and we are confident
in this prediction and you can think
about confidence you decide about this
confidence whether it's high low you can
play with this threshold 5575
ninety-five percent confidence in this
user being a Republican as soon as we
classify this user we can add it to the
label set we can also ask Mechanical
Turk errs to highlight the rationals and
to confirm the label and then we retrain
the model apply to the rest of the
unlabeled users again update your
posterior for the last for the rest of
the users and so on as soon as you
classified more users and you're
confident in your classifications you
add this users to the label set as
annotators to confirm if you want
retrain the model apply it again and so
on update your posterior and so on until
you classify all of the users and in the
active learning setup you also can think
about the rational annotation approach
you can also think about classifying
without the Oracle and by Oracle I mean
that if your mechanical Turkish are
always give you the correct decision
they are one hundred percent correct all
of the time you can do it using the
majority vote you can ask many chokers
to do that you're gonna five label for
five people to let same user but again
it's a trade-off in a time and cost and
the quality of your productions using
this this streaming online approaches
you should think tour about optimizing
for targeted advertising for example and
it's really interesting and we have a
description about this experiment in our
research we play I paper but let me just
summarize it for you so you can you you
might want to optimize tour with your
production quality you might want to
have models that are accurate always
over time and they should be accurate
today tomorrow
get to tomorrow always so you you in
order to do this you would like to use
models with rational filtering so really
really precise models and you want to
hire you're confident threshold so
basically you will always say I will
only take people whom I ninety-five
percent confident that they are
Democrats or Republicans on the other
hand if you want to deliver your ads
faster in time so you want you want to
have correctly classified users faster
tomorrow not not in one month you would
rather use models that don't use
rationals they give you higher rico and
you use lower confidence for example
fifty-five percent so you allow for some
misclassifications but you want to
deliver ads faster and finally data
section we run a bunch of experiments
for use when we only take a user stream
from all the user tweets versus user
neighbor tweets and we combined with
different neighbors and there are
different strategies you can you can
select your neighbors always we found
that always it's better to get
predictions from the user neighbor
stream neighbors tell many things about
you and even if you don't have any
tweets your neighbors can predict your
attributes so join streams are better to
summarize our results show that online
learning is my much better than batch
productions and it's actually really
suitable for this kind of predictive
analytics and social media active
learning is better than iterative batch
predictions in terms of quality of the
productions and your neighbors
completely give you away so always use
mixed streams it's not noise it's actual
signal that is coming and finally wrap
significantly improve your
classification performance so go into
more linguistic features when you do
this kind of text analytics to recap let
me tell you why my models are good so
this approaches actually moodle
streaming nature of social media which
people just started doing
and I think it's really important
instead of just using train and test and
build classifiers next our models allow
to make predictions for the users for
whom you have limited content for
example I don't beat and even I don't
tweet you can say many things about me
and actually different kinds of
neighbors make different predictions for
like gender age versus political
preference for example my people whom I
retweet and mention can tell political
preference better than the rest of the
neighbors people who might follow a
predictive of my age and people whom I'm
friends with people who are really close
to me I'm a friend's business with your
immunes friends with me these people are
predicting a predictive of my gender so
differently different neighbors a
predictive of different attributes it's
called attribute assert activity and
it's actually really interesting and to
see that we can get the signal from the
data also we actively learn from
crowd-sourced rationals and this kind of
active learning allows us to account for
the data drift and the same has been
this one is typical sparsity so since we
are making predictions from multiple
streams instead of just a user stream we
are also considering Fran tweets
follower tweets user mention tweeds we
allow for this topical sparsity which
means that i rarely talk about politics
but my friend yoram talks about politics
Oh Sofia talks about politics more and
if I'm getting tweets from these people
and I'm actually there are some strategy
how to do this it's called exploration
versus exploitation and we have some
actual experiments on that it's better
to explore your neighbors it's better to
get one tweet from each of you rather
than getting all tweets from Europe for
example which is again really
interesting and it helps to solve the
issue of topical sparsity the next is
learning on the fly we learn on the fly
we change the dimensions of hours
feature space which allows us to take
into account this data drift if there is
a new concept that will appear tomorrow
I will be able to pick up this concept
because
I am learning new models and I am
extending my vocabulary while I'm making
predictions and finally we build a
really really flexible framework which
allows you to add more features easily
you can think about word embeddings
which is really hot right now and it's
really helpful actually you can think
about interests profile information
feeding behavior and so on we can add it
easily with that said i would like to
thank all of my collaborators i'm really
grateful to them and thank you for
having me and i'm ready to take
questions questions good um I was
curious but be the end of your energy I
didn't quite understand how you select
um tweets that you want to be labeled
could you say a bit more about that you
gotta play different strategies first
you can think about the threshold so if
you say for example there are a bunch of
people I'm seventy percent more than
seventy percent confident in their
predictions you can randomly sample you
can use any different strategy like you
can ask you can get the majority voting
from the annotators there are standard
active learning but like their standard
weight you can select next examples to
annotate in the active learning set up
what we did in this experiment we
actually just took randomly from the
people with the highest confidence you
can also think about taking people with
the lowest confidence so if you want to
make predictions of if it's a really
difficult example and you you haven't
been you haven't been able to classify
this user / like 10 tweets and it's very
difficult you can think about getting
the users versus the lowest confidence
is that to get labels for them did you
compare
it seems like them and not not not yet
yes but it's really i didn't it's a very
nice question yeah it's really
interesting to explore the next step yes
you go back his first experiment for
you're talking about you have your
arrows about amplifying yes sure yeah
right so trying to understand what went
into these Anissa when you have
something like distressed pessimists
seen thursday i'm going to Joey the
correct interpretation but is it the
case that you're predicting whether you
are stressed or pessimists based on the
same feed data now using to measure very
good question of course when people when
they asked for annotations and when
people labeled a completely separate
data set which is not relevant to the
data set we analyzing people might in
their head they make inference and they
think okay this is like this person is
expressing a lot of joy it should be an
optimist right so of course I I agree
that there are some embedded
correlations between emotions and
sentiment that were used while people
made inference during their annotation
process but since we are using a
completely different data set we are
just picking up the signal that was
present before that was in the data we
learn from this data and we are making
predictions from for the test set so I
don't think it's an issue because we are
using completely different data set to
train and test so for a user in Tessa
you
the same tweets are used two strands
vessels and the amplification yes yes
but that's the assumption you always
made you think you're worth I
independent their ID when you make from
you when you make predictions right or
your features are independent which is
not always true you know i mean maybe a
similar question would then he said you
can think of you know one definition of
a pessimist is somebody that looks at
their environment whatever other friends
are saying and they only retweet
messages if they're sad so you know it
could be the case that people that are
doing this task on mechanics orgasm is
this a pessimist right they could be
looking at all you're the friends of
this person saying what they looking at
that person you know it's a matter of
interpretation they could be doing that
I don't think it's possible to rule this
out of course we since we showed them
all of the like all of the information
in the profile they might clicked into
the link they might follow followers and
check if they do really sort of work
while annotating so as I said there are
some bias but I think your question is
different it's more like the features
you use for attributes are similar than
the features you use for emotions yes
but yes and no because the feature
vector u bei you built for the attribute
production is a user base so you
aggregate all tweets per user it lasts
sparse and give a it's the dimension is
the size of your vocabulary but for
tweet base productions you have a
different completely different
vocabulary and then the features are
really start it's done on the tweet
level so these are the difference so I
don't save the future ii i totally think
that the predictions are independent in
this in this scenario please see what I
mean let me tell you one more
I still don't see how it's ruled out
that you know some people just have more
pessimistic reading behavior so they're
classified as fastest and classified as
they are not classified as dampening joy
we we found that they are dumped and
enjoy based on their tweaks based on
their emotion predictions that we are
not classifying these people as
dampening tweets there is dumping
emotions let me start again so when we
train our attribute classifiers we have
a vocabulary over 5000 profiles
annotated with attributes this is the
vocabulary of size x and we build a
future we build em every time you get a
new infant straight test instance we
just build a feature vector over this
vocabulary let's take binary features
word features right of size x for each
user we are trying to make a prediction
of in comfort or like you want pessimist
we want to label this users as a
pessimist or object when we are when we
are predicting emotions we are training
different classifiers this classifier is
a tweet level so we have 75 tweet 75
thousand tweets labeled with emotions
completely different vocabulary of sighs
why and ever every tweet we get from the
user from the leg from the users we
convert to this feature vector which is
really sparse because it because it's on
the tweet level of size why so for
attribute productions we worked with VT
vectors of size x for emotion
projections we worked with feature
vectors of size why different and I
think that's why these two productions
are independent even given the fact that
they are projected from the same data
the I'm Phi Delta then that is a person
between email david yes
you should be fine brother so I believe
let's this simple example let's take
sadness I'm getting sixty percent
sadness from my friends and I'm
outputting thirty percent sadness am I
dampening your amplifying I'm dampening
sadness right yes for sadness define the
ratio of those weeds that are able to
set a set freely to this end let me show
you this example again um quickly you
should ask me before um so you have a
user for this user you have a set of
tweets that are in coming right for
every tweet you have a sentiment and
emotion label the feature vector is of
the size y on the tweet level tweet
based production we label every tweet as
sad fear disgust and so on for this user
you say you have a set of outgoing
emotions and sentiments outgoing tweets
we label it with emotions and sentiments
and we look for normalized deltas so for
every emotion you get sixty percent
sadness incoming thirty percent sadness
outgoing the difference is 0.3 and I say
let's take all female in my data and
let's take all males in my data and see
whether the mean proportion of sadness
and the milk for female users and the
mean proportion of sadness from mail
users is different and battery it's this
means a statistically significantly
different and that's what I do and when
I say dampening it means that all of the
female like the average estimator for
the female group is dampening versus the
average mean for the male group that's
what I meant yes
take a trip you said okay you know there
are some things that are topical in the
data and the way you got around that in
one one of your papers was use people to
annotate save Republicans versus
Democrats people read the news all the
time they know you know if they're
talking about the candidate they know
whether it's a Republican or a Democrat
candidate so they basically use people's
judgment oh you plug people into the
system yes Oh to solve some of the
problems and I was just wondering
whether there was a way of automating
this something you specifically
publicans and Democrats USD search
Wikipedia you would know who the
Republican and Democrat candidates are
ya so maybe one way of replacing people
throughout this process which are very
expensive is using some automated tools
or some knowledge we do you think that's
doable totally doable and actually did
it for one of my papers but it implies
several biases when you do it like that
so in one one of my papers we used
distant supervision labeling strategy
where we say people who follow Obama and
Biden during 2012 election process are
but don't follow Romney and Ryan are
Democrats and people who follow these
two guys but don't follow Obama and
Biden are Republicans but using the
strategy and brought a lot of biases in
the day that we actually sample users
who are really politically active base
since they are following this candidate
they are talking about politics really
really a lot and when we tried to
generalize these models to a random
sample of twitter users these models
don't really generalize unfortunately
that's why I told you that every time
you you try to do analysis in this kind
of on this kind of data and sampling is
extremely important the way you annotate
this data is extremely important and
unfortunately people don't share the
data they collect in social media that's
why I couldn't do any any comparison
with the existing work I actually have
some numbers I can show you where we
stand in terms of
annotation but it's not the fair
comparison just one sec because the data
is different so the training and test
data is different yeah so where do you
stand this is the slide yeah so first of
all in this work we worked with a
variety of previously unexplored
attributes people have been primarily
working with gender age and political
preference on Twitter and here are the
results for all of the existing models
and and so far like my models were the
best but the model sweet build with
Yoram actually outperformed the state of
the art again I can't make a direct
comparison except with with testing or
training on on ms our data and testing
on on my data because i have my data
right and actually when I do this and it
was a guy test question before when I do
this we are getting higher or compel
comparable performance um but I can't
compare with this with these people or
this person or that person and so yes
there's that Collin's comfort as well
what is accuracy that uh this is the
accuracy I'm sure this is the prediction
errors if we want to predict users male
or female this is the accuracy this
people get different data set includes
chance as well no ma a.m. no I'm not
taking the thing is that all of the
previous data sets were balanced it's
5050 so the prior distribution is 50-50
and actually here I'm showing you the
road curve actually it's not accuracy so
it's again it's not fair to compare but
it gives you an idea where we stand well
you know this like you're talking about
so you take your machine learning
algorithm we run to 10-fold
cross-validation what procedures yeah
what sort of the case it was right when
you're talking about integrate your
agreement you're not running any machine
learning god you're just looking at what
other people said and you measure how
many cases they agree bro yes okay yes
exactly and rock is basically taking
like two random people to make a like
one male and one female I'm and labeling
them correctly you have a sense of
coming back to the question do you have
a sense of where your algorithms would
stand in comparison to UM this inter
annotator agreement thank you
essentially reached the accuracy of what
could be found in the data or you close
re4 with this is really interesting
question and which gets back to a new
paper at pnas where where I researchers
found that actually machine learning
models are better predictors of user
personality than annotators and
Mechanical Turk it's a really
challenging task I mean you can compare
versus self-report or their personality
Oh question here and you know their
family sometimes doesn't know them
goodness can do better job you don't
believe in ER i mean if you get your
neighbors from from sourcing then how
could you what oh no that's not my or
less now the people discovered with
their personality data they collect the
playbooks completely differently um but
yes gonna take the models that you train
into results that you're observing say
you're a questioning at the last bit
again so so we know that crowdsources
make a lot of mistake yes they have
biases so how does that affect them
aside step
I know that they make mistakes but but
yeah I think on a large scale since we
collected five thousand profiles it's a
lot i say i'm hoping that this noise
will disappear and my models will still
of course i agree because when he won be
i wanted to make the sanitation's we had
to decide whether we got 2008 profiles
annotated by three people or 5,000 by
white one person we have one budget and
we decided to go large to go big and get
more annotations but also back to do
this question like a regarding inter an
interior agreement I think because
people really agree on ethnicity and
gender it's like 0.5 0.8 kappa so we get
really accurate labels the accuracy is
good is much better but for subjective
attributes like religion for example
where they agreement a solo and the
actress is a lower yeah any more
questions all right</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>