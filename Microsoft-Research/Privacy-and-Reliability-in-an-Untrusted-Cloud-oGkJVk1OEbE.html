<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Privacy and Reliability in an Untrusted Cloud | Coder Coacher - Coaching Coders</title><meta content="Privacy and Reliability in an Untrusted Cloud - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Privacy and Reliability in an Untrusted Cloud</b></h2><h5 class="post__date">2016-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oGkJVk1OEbE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
alright welcome everybody coming out and
listen to Yuri's talk I'm Christian bird
I have the opportunity of hosting Yuri
brune today your room is currently an
assistant professor at University of
Massachusetts Amherst he got his PhD at
USC and then had a postdoc at close to
us University of Washington with David
Nutkin and Michael Ernst we had some
collaboration with him in the past and
he's here today to talk to us about
privacy and reliability in an untrusted
cloud so I handed over to you thank you
Chris thanks everyone for coming so I'm
going to talk to you today about privacy
and reliability in the cloud and those
of you who know my work will find that
this is quite different from the work
that you know most of you know me for
doing stuff with developers helping
developers to particular kinds of
actions but I'm going to be talking
about today is how do we illuminate hold
classes of actions developers have to do
from the set of things that they have to
do so I'm going to be coming up with a
technique that tries to inject privacy
into systems and try to inject
reliability into systems without the
developer worrying too much about how to
do that ok so let's jump right in i'm
going to start talking about privacy
first so let me just go ahead jump right
in and talk about what i mean by privacy
in the cloud so the cloud is a
well-known term today there's lots and
lots of things out there that calls
itself the cloud and the problem that i
foresee with the cloud is the following
if I used to do my taxes I used to get
my computer I would enter download a
program into my computer I would enter
stuff into this program and if I wanted
to make sure that nobody stole my
private data what I had to do is make
sure that nobody broke into my computer
and stole the data or nobody physically
stole my computer now today that's not
the case if I want to do the taxes I
don't or i rarely download a program to
my computer but I really do is I go to
something like TurboTax calm and I enter
my data into this program and it's not
stored on my computer it's stored
somewhere else out there in the cloud
right and so now I have to worry about
two things first I have to worry that
nobody breaks into into its cloud into
it as the company makes two about tags
and steals the data but second I don't
actually even know where these computers
are I don't
into it is outsourcing this to some
other cloud and actually these computers
live on a shore somewhere else and so
what I would really like to do is I'd
like to make sure that the data that I'm
entering isn't known even by those
computers themselves so i would like to
distribute a computation onto a cloud or
onto just some large network without
having those individual computers know
the private data that i'm entering into
the computation right like someone to do
work for me without them knowing what
they're doing so that's the problem i'm
going to try to tackle today so there's
lots of different ways that I could try
to talk to you about what is privacy me
and I'm going to focus on a particular
thing today I'm going to tell you about
a technique called style and it's a
technique for privately solving
computationally intensive problems so
i'm not going to look at taxes i'm not
going to look at gmail i'm going to
start out with empty complete problems
in particular i'll talk about three set
and i'll talk about how do we solve an
instance of a three sad problem on the
cloud without the computers on the cloud
knowing that input knowing the
particular formula that i want to solve
and in particular this is this is an
even harder problem of taxes because
there's a very small input right there's
a bit not not just how i see smells in
the audience it's not just harder
because it's empty hard it's harder from
the point of view of privacy as well so
there's a very small input and a lot of
computation has to happen on that input
and i'm going to try to keep that input
private despite the fact you know in
taxes maybe you have to add a number
somewhere and then never deal with that
number again it might be easier to keep
it private so the hope is that if you
can do it for and pick complete problems
you can expand it too much wider range
of problems alright so how are we going
to do this well turns out this is a hard
problem and in fact there's lots of
people working on this problem those of
you in the audience were familiar with
homomorphic encryption it tries to solve
exactly the same problem I'm trying to
solve the idea is you have lots of
computers you want to distribute a
computation on two computers without
them knowing what they're doing and give
you back an answer that you can actually
use but they don't know what the answer
is so I'm going to make a circumcircle
bent the problems that people have
identified so in particular so child's
in 2005 is prone the proven that for
certain kind of problem it's actually
not possible to get help from somebody
else without telling them what the
problem is and what you input is right
and you showed that for NP complete
problems so how am I going to get around
it this is what I'm doing I'm going to
take a problem and i'm going to
distribute it into lots and lots of
computers
now it's going to be the case that if
you look at all the computers if you
were to compromise all the computers all
together they're going to know my data
but if I look at any one computer or if
I look at any several computers and I'll
show you later that as long as I don't
control up to half of the network it's
very hard to reconstruct the whole input
all right so that time circumventing
these hardness proofs from before is
that it is the case that the whole cloud
is going to know my comp my data but if
you look at reasonably large chunks of
the cloud they won't and so it provides
these guarantees all right so in
particular that i'll come back to
homomorphic encryption later in the talk
but in particular the distinction is
that the privacy guarantees are weaker
than with homomorphic encryption because
the entire entity would know their
computation but it is it's a systems
approach it is a much more efficient
approach on homework encryptions today
now I think very highly homomorphic
encryption and I think that decade from
now we'll all be using it hopefully but
today this is a much more efficient
approach all right so how do we get
there what we're going to do is I'm
going to take a computation and i'm
going to divide it into its very basic
very elemental pieces will talk about
how you do that and then i'm going to
take these pieces the sub computations
and distribute them in a smart way on
the network so that they all sort of
self assemble will self compute and
everything comes together and out pops
the answer for you right that's the high
level approach so in order to do that
I'm going to need to describe to you a
theoretical model of how do you take a
problem and separate it into little
pieces so let's talk about a particular
theoretical model so here's an example
of a model you need an input to this
computation and I'll talk about each one
of these pieces on the next few slides
you need an input that you can encode in
some way and i'm going to encode things
using these tiles these little squares
with labels on them you can think of
them color like cellular automata and
then you need a program that in some way
acts on the input so here's a program
that happens for addition again I'll
talk about that in a second and then in
this view of computation I'm talking
about you take an input that encodes
your input you take a program you mix
them together there's lots and lots of
copies of this program and these little
pieces are going to self-assemble and
somehow build up a large what I call
crystal and assembly of these tiles that
encodes the answer so that's what it
means to compute okay so let's go
through this in detail this example so
you understand what I'm talking about
so first of all I want to talk about the
program the program for addition here's
an example of a program this is the hard
part this is the part you have to write
using tiles to perform a particular
calculation that you're interested in so
this program is for addition and we
don't have to understand too much of how
this works but you can sort of see that
it each one of these tiles encodes
information codes bits their zeros
there's ones so I'm going to give you a
little bit of a hint as to how this
works each one of these tiles is a one
bit full adder so this tile right here
it's adding 0 and 0 and 0 and it's
coming up with the answer is zero and
the carry bit is 0 I'm not very
interesting if we take a look at one of
these guys it's adding 1 &amp;amp; 0 &amp;amp; 1 and in
binary 1 plus 1 plus 0 is 0 with a carry
bit of one okay so we'll see how this
comes together in the future let's take
a look at what we're going to do with
these tiles let's take a look at an
input so suppose I want to add 10 and 11
so let's encode 10 in binary that's 10
10 and 11 in binary 10 11 you build
something like this this is an input to
the computation so here's a 10 10 the 10
encoded on top here's the 10 11 encoded
on the bottom and now what's going to
happen is we're going to look through
this program and these tiles are going
to attach under certain rules here the
rule is whenever tile matches on three
sides with this crystal with is growing
assembling it will attach so there's one
place here where there's three labels
that are available for attachment
there's 0 0 1 so the 001 tile is going
to attach their right we're thrown in
here and now what you see it's done is
it's added this bit in this bit and this
incoming 0 carry bit and so it's said ok
0 plus 1 is 1 so it's added the two
least significant bits and they carry
bit from one from that is 0 so then the
next thing you'll do is you'll add this
one zero one one zero one right here is
going to pop in and you can fill it in
all the way in there and now if you read
in the middle you get 1 0 1 0 1 which is
21 in binary right so this is a very
simple example I'm just doing this full
adder but I'm trying to illustrate how
this you can take a computation break it
up those tiny little chunks all right so
now we can do this everything I've
described so far is in this crazy model
it's in a its model it's not in a
software system so what do I envision
doing this with a software system well I
envision that each one of these tiles is
going to deploy it on some computer all
right there's going to be computers out
there we'll talk about how they're out
there and they're going to deploy these
tiles and so you can envision that every
single one of these things is deployed
in a particular computer and now if you
come to the system and you compromise
some computers let's say we've
compromised this guy this guy in this
guy we get pieces of the input we get
pieces of the output we get pieces of
the intermediate computation we're sort
of getting these chunks an entity that
controls these three computers can get
these pieces but all they see is a few
bits and I'll talk a little more about
this later but they don't actually even
see that this one and this one are apart
so the old a knows that they're not
connected they know these guys are
connected they can connect pieces that
are directly connected but as soon as
there's a disconnect you can't piece it
back together so in order to regain the
whole input you have to piece together
through maybe you have multiple copies
of this edition going on at once you
have to piece together all the pieces so
that's where the hardness is going to
come from okay so this is a very high
level intuition now so far I've been
talking about addition the same thing
you do with addition you can do with
more complex problems all right edition
was just for explanation purposes so
with addition you take a tie you take a
way to encode the input and you can
build something that will automatically
find the answer for you you can do the
same thing with satisfiability so here's
a system that solves three-set now you
don't this is way too small to read so
you don't need to read that and I'm not
actually going to focus today on how the
three sad tile system works I want to
focus today on how the software system
the distributes it works so but I
encourage you to take a look at the
natural computing paper if you want to
see how this works but the idea is you
encode 3sat formula using these tiles
and then there's a program it's very
similar to the addition program that
I've written instead of eight different
tile types that addition had it has 64
different tile types and so then these
tile types can come in and they attach
and they grow and they find the right
answer to the problem now this is an
np-complete problem and something that's
important to understand is I'm not
trying to solve NP complete problems in
polynomial time all I'm doing is I'm
trying
put NP complete problems out on the
cloud to solve them faster but I'm not
looking for polynomial time algorithms
so this alga is non-deterministic it's
going to guess an assignment it's it's
not the dumbest non-deterministic
algorithm it's not going to get 2 to the
N assignments it does 1.7 something
something something to the ant
assignments because it makes them smart
choices along the way and you can prune
things along the way so tiles are
touring Universal you can implement any
algorithm you want using tiles but
that's really the hard part of the
approach and what I'm trying to talk to
you about today is let's say we go
through the trouble of implementing our
code and there's actually you can
compile it you don't have to actually
write with tiles but let's say we go
through the trouble of converting our
programs to this crazy tile assembly
mode if we do that what can we get out
of it right and the answer is privacy
but it's at the cost of efficiency at
the cost of a couple of other things so
that's what we're trying to compare all
right so the same thing there happens in
addition you can do with np-complete
computation encode the input you have to
make lots of copies that then input now
because it's non deterministic and then
each one of these assemblies kind of
grows I'll show you the process of that
happening and then eventually there's
this black tile in the corner if the
black tile attaches that indicates that
it's found the answer it's found a
particular assignment that will lead to
the satisfiability of this formula if
the assembly can also get stuck
somewhere in the way and if it gets
stuck then the black tile will never
attached to that assembly and that's
going to happen to most assemblies right
there's only a few special ones that
find the answer ok so how does this
process actually work when we're dealing
with computers the idea is the first
thing you do is you go up to a network
and you want to make this network deploy
your style computation you have to tell
the computers the program they're going
to run so you have to go out to a
computer and you have to say you are
going to be this type of a tile you're
going to deploy a particular type of a
tile which is defined by those side
labels it has you go on to the next one
on the next one and the next one and as
I said before there's 64 different title
tabs for 3sat so you can take this
system and now using some gossip
protocols they can spread this
information out and they can assign
every computer on the network that's
going to participate in your computation
to particular tile type and then you
build a single seed so you build one
seed out of computers and network to
deploying these tiles
that are deploying each one of these
individual tiles and they know they're
connected and that's all you have to do
is the client and now the system is
going to take over on its own so the
first thing system is going to do is
it's going to go out and create copies
of these seeds so it's going to go out
and it's going to find other computers
that deploys the same tiles as itself so
each one of these tiles is going to go
out and find another computer to plays
the same tile and ask it to replicate
itself to create a copy of itself so the
seat is going to create copies out there
and then they communicate with one
another using these neighbors in order
to create an exact connected copy to
this seed right so this isn't to to
trick you probably believe me if I said
that if I have an assembly of computers
that know about their neighbors they can
go out and they can create a copy of
another set of computers that are going
to look just like them okay once you
have each one of these assemblies built
they're going to start growing so each
one without waiting for others to finish
is going to start growing and under
particular conditions in this corner
here these two tiles are going to say we
need a neighbor we don't have a neighbor
we have a couple of exposed side here we
need a neighbor so they're going to
start querying nodes on the network to
see whether or not they can attach and
when they do this querying they're going
to use secure multi-party computation
protocols so Yahoo's garble protocol and
the idea here is that when this node
when this tile tries to attach it's
going to learn to only two things it's
going to or one thing it's either going
to learn that it can attach there or
that it cannot attach their so either it
matches on the sides or doesn't it's not
going to learn any of those bits of data
that's stored within each side each one
of the tiles all right so you only learn
the interfaces and that's how you
prevent information from spreading okay
so maybe this guy doesn't fit or less
somebody else somebody else does fit
there's a intricate algorithm in there
to make sure that you don't end up
asking too many nodes you can pretty
quickly find out whether anyone fits or
not and so once the nodes start fitting
this assembly is going to start growing
and so there's a multiple levels of
parallelism in the system first there's
lots and lots of these copies of these
assemblies going in parallel and those
can be completely independent and second
of all each one of these little growth
places each time you have a step you can
have a tile trying to attach there as
well so lots of these things can happen
in parallel and then once this crystal
grows
to get to the end so some lucky crystal
will get one of these black check mark
tiles and then that check that tile can
contact back to the client and say hey I
found the answer and the idea of this is
you think about it there's lots of these
inputs that are going in parallel and
most of them are not finding the right
answer and the hard part about this
problem is fishing out the one that is
finding the right answer this assembly
here is encoding precisely the the
answer to your problem precisely what
the 3sat assignment is to satisfy the
formula so once you get the right one
with the right authentication you can go
the client can go and they can find out
what the assignment was with some NP
complete problems all you really care
about is whether something is
satisfiable or not it's sort of a binary
decision so then you don't even have to
disassemble this crystal you just get
that bit yes that track if you say oh
this child is here and then later you
realize oh there's no answer can factor
so that's a good question the question
is does the algorithm backtrack and in
this particular implementation a doesn't
because what's happening here is that
you have lots and lots of copies of the
seeds and they're exploring all the
possible paths now you could implement
something smarter where it actually does
backtrack it goes up and gets stuck
things fall off there's some
implementations of system like this not
in computers but actually do you name
where does precisely that but for this
purposes I didn't bother I just sort of
created lots and lots of copies and
they're all exploring different paths
and the goal here isn't efficiency and
we'll see that in later slides that the
system is not super efficient turns out
it's efficient enough to be used today
for some purposes but the key is that
you can actually get really good privacy
and this is kind of a proof of concept
of that sense all right so this is how
you report the answer to the client so
what I'd like to do now is I've tried to
give you some intuition about how the
system works I really want to talk about
copy the system that I actually built
and talk about some empirical evidence
of empirical evaluation of that system
before we get to the empirical parts
first I want to talk about where does
the privacy come from why is it hard to
break the data to crack the data in the
system all right so let's talk about the
formal proofs of privacy before we get
to the 40 improves pretty graph so for
three different sizes of problems for 20
bit of 38 bit in a 56 bit input what I'm
showing here
is as the fraction of the network you
can compromise increases or here you've
compromised half the network what's the
probability that you can crack the data
and so there's a few things to notice on
here one is that the bigger your problem
is the harder it is to reconstruct all
the data right and that's a good thing
you want that because for small toy
problems maybe it's easy to reconstruct
all data but as soon as the problem gets
them to some real space where it's a
media problem it actually becomes harder
to reconstruct the data another thing to
notice is that is you come over here
towards one-half if you compromise half
of the network there is SM Tata kaleu
get 1 over e 1 minus 1 over e or a
sixty-three percent chance of being able
to get the data out of the system right
so that's not very good I wouldn't be
happy if my taxes had a sixty-three
percent chance of being hacked but as
soon as you push over here to the point
2 or point one so ten percent let's save
the network's compromised the
probability of hacking the data is very
low so give a 56 bit input at ten
percent you're getting 10 to the
negative 40th roughly probability so
very low right and that's the
exponential drop off that you want so
let's take a look at where these graphs
come from so first of all what is my
threat model in the threat model i
assume byzantine nodes that are trying
to actively collude with one another in
order to figure out the private data and
so you can figure out the probability
they could can collect enough pieces of
all the different inputs that are
floating around to put back together
your entire input and so there's three
arguments here there's the fraction of
the network you've compromised there's
the size of your input the larger input
the harder it is to put together but
because this is an np-complete problem
the larger your input the more seeds
you'll need the more copies of the input
will you need and in fact that number is
exponential in the number of in the size
of your input so you get this kind of
all is very pretty formula but it's a
very ugly interaction you get this
fraction here it's being exponentially
pushed down by the size of the input but
then it's being doubly exponentially
pushed up in the number of inputs that
so that you can collect different pieces
from and so in the end result what you
get is that graph that you saw it's best
I think shown with an example if you
have
tara grid a hundred thousand machine
network and somebody's compromised
twelve and a half thousand machines of
that network and you deploy am i using
here a 17 variable formula so a pretty
small 3sat formula what you have is one
in ten billion chance that somebody can
crack your input and remember as the
formula becomes bigger it becomes harder
and harder to reconstruct that input
okay so that's what I want to say about
privacy let's talk about it can the
system actually run so I've built a
version of the system it's called
mahjong it's available for download it's
open source I encourage you guys to use
it if you'd like it's actually a
reasonably small it's built on top of
prism MW which is this middle or
platform and takes care of all the
network communication and really all
that mahjong does is it imposes the
rules for when things can it cannot
attach so it's only about three thousand
lines of Java code the input to the
system is an np-complete problem
instance and it compiles that NP
complete problem instance down to a
distributed system that you can now take
and put on computers and it will run it
will provide an answer for you to solve
that problem instance right so in
particular it's limited right now to
empty complete problems because it
translates them using a polynomial time
encoding into either three set or subset
sum which are two programs for which
I've written tile systems and the key ID
here is that when the when somebody
wants to use the system they never have
to worry about tiles tiles are an
underlying thing just like assembly is
that the developer doesn't need to worry
about I'd like to provide for them a
system that they can use if they want to
prove the properties the privacy
property system sure they need to
understand tiles but if all they want to
do is deploy a system privately they
don't need to worry about tiles it's an
automated compilation procedure does it
for you alright so i used this system to
run it on three different networks i
have an 11 node private cluster so
imagine a graduate student in a room
with 11 computers he set up that was me
a few years ago I also have a 186 node
USC high-performance computing cluster
so this is 186 computers that are all in
Los Angeles but there are two different
locations in Los Angeles and they're
pretty homogeneous and then there's also
a hundred node planetlab subset so
planetlab is about
thousand machines i have about a hundred
of those machines so planetlab is this
globally distributed system there's
computers all over the place different
organizations can contribute to three
computers to it for the rights to be
able to use some of these computers to
join as a test bed right so planetlab is
actually not an ideal resource for this
particular computation because planetlab
is not computation intensive there's
lots and lots of experiments running on
it it's very overloaded so planetlab is
really for measuring the reality of the
Internet's communication overhead and
things like that but I think that for us
for me it's really served as sort of
this is the limiting factor this is
pushing style to the limit were you
using computers some of them are
overloaded and doing other things and
some of them are faulty and might be
running viruses so I think it was a good
evaluation from that point of view all
right so what did I do with this I
wanted to show two things to demonstrate
two things by using these systems the
first I wanted to show is that the
system can actually be used because I'll
show you something in a second that
fundamentally you should all be thinking
this is going to be way too slow right
and on the second thing I want to show
you is about scalability so why is this
going to be way too slow well it's going
to be way too slow because I'm taking
things like I'm taking instead of even
just adding two int's I am splitting it
up into individual bits and I'm taking
things that normally happened within
gates within your CPU and I'm moving
them to the network right grossly slowed
one hundred two thousand times slow in
fact probably even this is an
underestimate network communication is
much slower than things happening inside
the CPU so this is a this is the right
intuition but it actually turns out that
it's right in tuition for the wrong
problem the reason why this kind of slow
down doesn't affect style is the
difference in throughput and latency so
if my system we're doing the following
if you were saying I need to add this
bit in this bit great let's take care of
this tile sending a message out on the
network for some tile to comment
attached which is essentially that
addition and then waiting for that
message to come back when the message
coming back st. great you're attached
let's go on to the next bit if it were
doing that then in fact this would be
the right intuition but it's not doing
that we're dealing with these very large
computations with lots and lots of tiles
so my system never sits there and waits
for communication it deals with the tile
it sends out
message saying I need an attachment and
then it moves on to dealing with the
next aisle and the next aisle and the
next aisle in the next aisle so every
single one of the nodes is spending
almost noted in fact spending no time
waiting for the communication to come
back in fact the communication waits for
the computation to further yet for the
computation to finish on the nodes so in
particular this is somewhere we're
dealing with NP complete problems helps
me there's just so much computation that
you never end up waiting for the
communication okay so this is a nice
little story that I told you but I can
actually verify this empirically so what
I did is I took an 11 node subset of the
three networks the private clustered
with very low latency the HPCC cluster
in the planet lab cluster and I solve
two different sized problems on them the
small one and a larger one and you can
see that there's never more than six
percent deviation from the mean so I can
tell you at six percent but the times
despite the fact that planet lab has way
bigger lag the time doesn't change very
much in the amount of time it takes to
compute to solve the problem i also
created a simulator to run my system on
top of and in the simulator i can
control how much it's a discrete-event
simulator I can control how much the
communication takes and so I used no no
delay in the communication to 10 100 500
millisecond to also a Gaussian random
distribution of the communication and
then also for every node I assigned a
random location on Earth so somewhere in
the middle of the ocean but that's the
world we live in today and then I said
the communication is going to be
proportional to the distance and again
you can see no more than six percent
deviation from the medium we're all the
earth to be able to go right so well so
it's not faster the same way that the
converse is not faster there is lots of
factors that affect your communication
the the sort of computation speed and
the most important one is that we're
solving non-deterministic we're using on
to risk Algrim so sometimes you'll get
lucky other times you won't get lucky so
it's not fast I mean here it's working
out faster sometimes but if Iran enough
of these all these numbers that the
error bars would decrease basically the
latency is not affect negatively
affecting the running time is the key
here okay so that's the first thing that
I want to demonstrate empirically the
second thing I wanted to demonstrate is
that one of the big reasons I built this
system
is that I wanted to get good good speed
up good scalability so I wanted to be
the case that if I throw twice as many
computers at my system it would compute
twice as fast and there's so much
parallelism here that we should be able
to get pretty close to that so I
designed an experiment to do that as
well I solved some problems again a
private cluster HPCC planetlab and also
in simulation and for each one of them I
picked a half of the network and then
Twite well twice half of the network to
compare how much the execution times
were and what I found was that I get one
point nine times speedup so almost to
not quiet there's a little bit of
overhead on the real physical Network
and I believe that these numbers are
pretty accurate for the physical
networks in simulation there's a little
more variability here you can see I'm
actually getting super linear speed-up I
think that this comes from the fact that
I solved a very large problem so this is
a problem for where you can't explore
all the seeds really just exploring a
small portion of the seeds and so
basically the error bars are too big
here so this is really just an artifact
I think that the numbers that i got from
the physical network of 1.9 a much more
accurate and the simulation just sort of
shows that the algorithms do what you
would think that they would do right so
this is not quite perfect we're not
getting twice the speed up but I'm
pretty happy with 1.9 number considering
it's not really optimized for that
overhead okay so great I want to talk a
little bit about some related work and
other ways to try to solve the same
problem so the first thing first and
foremost if quantum computing were a
real thing today you could do private
computing the way I've described it by
using entanglement so this is great I
look forward to the day when we can do
quantum computing similarly homomorphic
encryption tries to solve the exact same
problem today actually microsoft
research has done some great
advancements in homomorphic encryption
and there's certain types of problems
that we can actually do already when
homework your first homework encryption
first came out we needed more memory
than all of our complete probably than
particles in the universe in order just
to solve a simple problem today we can
actually do some addition some even
multiplications using homomorphic
encryption but I'm truly trying to
target much larger problems that today
are not possible with homework
encryption so I fully believes that
decade maybe sooner if we're lucky we'll
can use how morphic encryption but for
now I'm taking a system
approach to trying to solve the same
problem that's actually usable today
there's lots of work out there on trying
to distribute competition a non private
way and this work is complementary to
mine because you can actually take these
approaches and you can compile them down
into style and you will be trading off
efficiency for privacy should be getting
a private system out some of them are
trickier than others but it's
complementary in that sense there's lots
of work on how to make systems fault
tolerant and I'll actually get to that
next we'll get to the reliable part of
the cloud and then there's also lots of
lots of work out there on how to do
private storage and private access to
data on the cloud and of course the big
difference here is that you could for
example encrypt your data but you can't
then compute on it you have to decrypt
it to compute on it and I'm really
trying to get the cloud to compute on
the data produce something useful for me
without without telling them what you're
doing ok so what I told you about in the
first half of the talk is about style
it's this kind of crazy idea that we're
going to get privacy through
distribution by distributing the problem
that's what we're going to get privacy
problem and I've shown that this is
actually possible it's not very
efficient but it's actually possible to
do and I can give you a specific number
so in my not optimized prototype it cost
about four thousand times more to solve
the problem using style than on a single
machine so what that means is if I have
it's roughly the cost of owning one
machine I could do just as fast by
owning four thousand cloud machines
right so that's a big difference it's
probably not financially doesn't make
sense to do that right now but I believe
that about half of that or i should say
the square root of that about a factor
20 is because of inefficiencies in the
prototype that I've built and then the
other square root of that has to do with
the fundamental cost of doing privacy by
distribution right so basically the
project can be increased quite a bit and
even even with this 4000 number there's
still places where people want to use it
Microsoft has tons of computers that are
internal to the company that are not
being used at night they may want to run
something on them but they may not trust
those individual computers to not have
spam wear or something installed on them
so they may need to do it in a private
way so there's places and particularly
what I think about
as is this is a bound this has shown an
upper bound of we can do privacy through
distribution by doing it this way by
doing it through style right we can try
to improve on it we can try to build
more efficient systems this is orders
and orders of magnitude more efficient
than home morphic encryption already is
but it you know it's a very different
approach all right so I'll refer you to
a couple of papers one from I cdcs from
last year one from transactions
ependymal secure computing if you want
more information okay so I want to shift
gears a little bit right now and I want
to talk a little bit about how do we
make systems not just private but
reliable how do we take a system and try
to make it more reliable without the
developer having to worry about it so
I'm going to start out by asking you a
couple of questions so the first
question is let's say I want to compute
some very simple function I want to
compute what three plus five is and I
can ask you guys in the audience but
let's say some of you or Byzantine some
of you or mean or maybe some of you just
faulty and you may not give me the right
answer let's thank you yes that is a
good example so let's say that there's a
seventy percent chance if I ask and you
want to be you'll give me the right
answer but that seventy percent is not
enough for me so what's a good way for
me to bring up that reliability right
ask multiple ask three of you and have
you vote or ask five of you and have
your vote right that's one possibility
okay let me ask you a separate question
let's say that I would like to send a
message to chris and in sending this
message to chris the channel among which
i'm going to send it is noisy so let's
say about thirty percent of the bids get
flipped so how can i send a message to
Chris send it a bunch of times anyway
have another idea so we know a lot about
from information theory about sending
messages we could do things like he
could send me an acknowledgement we
could encode our message in a smart way
such that what we get back sorry such
that we can correct errors or at least
detect errors and it's not sending the
message multiple but could send them as
multiple ways but that's not very
efficient so the question I want to ask
is if we know in in fact there's lots of
theory out there on how to optimally
send a message so that you're squeezing
as much information as you can out of
every non noisy bit
so if that's what we do for sending
information why don't we do the same
thing for computation can we improve our
computation computational channel if you
will of the cloud so that you're getting
as much reliability as you can out of it
so it turns out you can here's the model
that I'm going to be talking about you
have a pool of computers the computers
can be Byzantine so the computers can be
actively trying to collude and break
your computation but not all of them but
the fraction and the identity of the
nose of the Byzantine is unknown to me
and in fact it can change computers can
join computers can leave they can become
Byzantine they can become reliable again
all of that is is unknown to me all I
know is that some fraction of them is
faulty and my goal is to build a
technique called smart redundancy so I'm
going to try to use redundancy and that
redundancy is going to try to maximize
the task reliability given a particular
cost so basically let me say that I'm
willing to spend three times the
resources for my computation of just
doing it once but I'd like better than
seventy percent reliability from adding
three plus five what's the best way to
do that what's the best way to be
willing to spend three times of
resources to get as much reliability as
I possibly can alright so let's spend a
minute and looking at what kinds of
systems this would actually be
applicable to so there's lots and lots
of systems that need this kind of
reliability MapReduce systems that have
lots of individual subtasks there's any
not anything but most things built using
the globus toolkit can be can benefit
from this kind of work and then as a
series of Boing systems so these are
things like folding at home study at
home all of these systems have lots of
individual tasks that can happen in
almost arbitrary orders not quite
arbitrary but almost arbitrary order and
so anything that has tasks like this
they can be repeated multiple times to
improve reliability and can be reordered
will benefit from my technique and
there's a whole other class of
techniques that's very different from
the ones we normally think about but
they're actually even more important
they're crowd sourcing techniques things
like recapture or folded which is a
protein folding game which has led to
some advances in cancer treatments
there's software verification techniques
out there there's techniques on
involving humans in every day programs
and things like that so this is an
interesting area because the resources
here or even more expensive
the humans are the resources and they're
unreliable they might even be Byzantine
and maybe trying to break my computation
but because their resources are so
expensive we definitely want to squeeze
out as much reliability as we can order
that unreliable channel all right so
these are the application let's get back
to how much you're going to do the
reliability so let's start with
something very simple this is the voting
redundancy this is what happens when I
just ask three or five of you to vote on
the answer and for now we're going to
assume that that we know the average
node reliability let's say that the node
reliability is seventy percent so if I
ask a single node I get seventy percent
reliability and let's say I'm trying to
hit a system reliability of ninety-seven
percent that's my target okay if I ask
three nodes we can compute the
probability that will get a reliable
answer right and that probability is 1
minus the probability that all three
nodes failed or were Byzantine minus the
probability that two of the nodes failed
on there's three different ways in which
that can happen and so we get an
eighty-four percent chance probability
eighty-four percent confidence in the
answer if we ask three nodes and then
have them vote in order to get to
ninety-seven percent reliability we're
gonna have to ask 19 notes right to have
to pay a cost of 19 a factor of 19 in
order to get to the desired reliability
right so that's our baseline that's what
we're trying to beat it turns out we can
beat that by quite a bit okay so let's
talk about smart redundancy here's the
flow chart of how smart redundancy works
it takes a computation and it says let's
assume the best case let's assume the
best possible thing is going to happen
and if that best thing happens how many
jobs do we need to distribute I'll
explain this with an example on the next
slide but how many jobs are you to
distribute in the best possible case ok
so we compute that number we go and we
actually deploy that many computations
and then we find out how close to the
best case are we how close is reality to
the best case and if the reality is the
best case then great we achieve our
desired reliability we're done if it's
not we go back to that step and we say
now we know a little bit more about
reality let's readjust our expectations
let's say how many more jobs that we
need to deploy in order to achieve in in
the now known best case in order to
achieve the desired reliability so
main idea here is that you only deploy
jobs if you definitely know you're going
to need them you never deploy a job if
it's possible that will contribute or
maybe it won't contribute so let's take
a look at this with an example I'm going
to need to throw up some numbers here on
the right for us to use this example we
already know what happens if we ask one
node and it returns an answer to us we
are seventy percent certain in that
answer and what happens if we ask two
nodes if we ask two nodes and they both
returned the same answer to us alright
so we got to sort of one answering zero
of the other agreeing answers then we
have an eighty-four percent chance of
getting the right answer right we asked
three nodes we're even higher has three
nodes they all give us the same answer
we're at ninety-three percent chance all
right this is great but now let's look
at some disagreements what happens if I
has four nodes and three of them give me
one answer and another one gives me
another answer well that's going to
undermine our probability so you can
look at you can calculate that
probability and you get eighty-four
percent chance so let me dive into this
formula just a tiny bit how do you
calculate this probability what you look
is what is it you look at what's the
probability that it's that seventy
percent thing happens three times and
the thirty percent thing happened once
divided by that same probability that
it's the 70 thing happened three times
and 30 happened once times the
probability of plus the probability that
the thirty percent thing happened three
times and the seventy percent thing
happened once so how likely is it that
you got three bad nodes and one good
node to get this answer so that's how
you can compute this let me throw out
some more numbers here if you ask four
nodes and they all give you the agreeing
answer then you're in this magical
ninety-seven percent reliability that
you wanted right so this is the best
case we'll come back to this if you ask
five and and you gotta for one split
then we're back down to ninety-three
percent 51 also happens to give us ass
ninety-seven percent reliability okay so
let's we're just going to use these
numbers to try to figure out how this
technique is going to work so let me
throw back up this flowchart we saw
before so the first thing we do is we
start up here and we say in the best
case what do we need to do to get the
reliability we're looking for well the
best cases we deploy for jobs in the OL
come back with the same answer and that
gives us 97% reliability so that's what
we'll do we'll deploy for jobs that's
our best case
now we deploy the four jobs and they go
out there and let's say we get a 3-1
split back so we weren't in the best
possible case but now we know where we
stand we haven't achieved the desired
reliability so we loop back around and
we say given the fact that we've seen
three of one answer and one of another
answer how many more jobs do we need to
deploy in the best case so the base case
here is that we'll ask two more nodes
they'll both give us an answer that
agrees with the three and we'll get this
51 split that gets us to ninety-seven
percent so great let's do that will
deploy two more nodes there it is will
deploy two more jobs and if they come
back to us and let's say in this case we
get lucky we get a 5-1 split we're done
we've reached the reliability we wanted
the confidence in our answer that we
wanted and we spend six times the
resources rather than 19 right now of
course this is just an example execution
I'll show you the expected value in just
a minute okay so that's basically how
the technique works so what I'd like to
do right now is I like to take a quick
aside to talk about something
interesting so here I've been talking
about knowing these probabilities
knowing that the network overall is
seventy percent reliable but actually
turns out that you don't need that you
can run this kind of conversation
without having any idea how reliable the
nodes are so the basic idea is I
saturated smart redundancy is that you
assume the best case and you ask the
minimum number of nodes and you only ask
more nodes after you learn how much
reality differs from the best case so no
one here does it say that you need to
actually know how much your confidence
is all you're trying to do is you're
trying to improve the confidence as much
as possible given some set of resources
so here's how you do that we're going to
play a game a fake game virtual game
there's going to be two rooms and what
I'm going to do at the end of this after
I describe the rules is I'm going to ask
you guys to vote which room would you
rather be in so in each room you will
get to make a bet a virtual bad right
and so basically the difference is do
you want to get more information from
what you know in room wanna do you get
more information from what you know in
room two to make this bet okay so let me
ask you let me describe the rooms for
you so in room one we're going to take a
30 70 biased coin so you either comes up
heads seventy percent of the time or it
comes up tails seventy percent in time
but we don't know which we just know
it's 70 30
and I'm going to flip this coin four
times and I'm going to get four heads
and zero tails and then the bet that
you're going to make is whether it's a
seventy-thirty heads tails or tails
heads coin so what's the more likely
thing right so presumably you would you
would guess that heads is more likely
since we got 4 and 0 but you have some
amount of certainty that that's the case
after seeing for Flip's now in room
number two we're going to do something
very similar we're going to take another
7030 coin again we don't know whether
it's heads or tails seventy percent
we're going to flip it a thousand and
four times and we're going to get a 504
heads and 500 taels split and again
you're going to be asked to make a bet
whether it's a seventy percent heads or
seventy percent tails coin so let's vote
who would rather be in room number one
to make this bet I'm seeing four hands
okay good you guys split up down the
middle how many who wants to be in room
number two nobody wants to be in room
number two so you may have any other
answers so look so incredibly unlikely
at room to will only exist in probably
one in ten to the hundred that's right
that's right we tend to the hundred okay
so room to may not exist and therefore
you don't want to be there that that's
exactly right you expect to get a
roughly seven hundred three hundred
split but you're getting this really
crazy split so let's take a look at the
probability that this is a head coin
versus a tail corner head favoring which
to hit fair and coin you can write down
this whole formula it is the number of
different ways that you can pick out 504
out of a thousand four times the fact
that it's the point seven percent thing
so it's the heads that happened five
hundred four times some tales that
happen 500 x divided by that same
probability x flipping it so it's the
heads that was point three percent of
the time and yet that came out 500 four
times and the probability that it was
the tales that comes up seven percent of
the time and yet happened five hundred
times right so this is it's a very small
number as I mentioned so here's this
probability but we can notice that
there's a bunch of things here we can
cross out so all of these guys are the
same so we can cancel them out and
there's a lot of these three hundred
point threes to the 507 2004
it's a this is a binomial coefficient
this is a thousand for choose 500 so so
they're symmetric it hasn't fortune 500
the same as a thousand for tues 504 yeah
there's also this point 3 to the 500
that we can get rid of so there's just
four of them left down here and there's
this point 7 to the 500 that we can get
rid of and we'll have a couple of fours
over here all right I've made a total
mess here let me simplify it out what we
get is this formula which happens to be
exactly the probability in room one
right so what happened here I mean you
had a very good intuition but this
intuition is driving us down to the
wrong decision the reason is this room
is incredibly unlikely to happen right
but given that this room is what
happened when you send out a bunch of
answers and you get some responses back
given that the responses you got back
the question is how likely am I to have
gotten more of the right answer than the
wrong answer so it's very unlikely
you'll be in this room but given the you
are in this room you get exactly as much
information out of it as you do from
this room so the key point to take away
here this is a this comes from Bayes
theorem it's actually a pretty direct
implication from Bayes theorem but it's
very counterintuitive but the
implication for our technique here is
that as long as you get a stain split
remember how the 40 split gave us the
same conferences of a 5-1 split it's
because that difference was four so
really when you're specifying this
reliability technique you can just
specify single number you can say get me
difference of 40 get me a difference of
20 and it's very parallel to this voting
technique when you say deploy to 19
nodes and have them vote right that
doesn't actually tell you how much
confidence you're going to get how much
improvement conference you're going to
get all it tells you is how many
resources you're going to use right so
it's a single parameter that tells you
how much improvement in relative terms
do you want to get out of it all right
so let's take a look at how the system
actually works so I'm not going to
describe how I built a system but I
built a system it's built on top of
boink we took a SAT of sat at home and
we ripped out the redundancy technique
that it uses which is voting redundancy
and we put in our own smart redundancy
so what I did here is I took a system
and I varied the reliability of the
underlying nodes this is again deployed
on top of planetlab i varied reliability
of the underlying nodes from point nine
five so ninety-five percent down to 75
percent and then I deployed voting
redundancy boinc with voting identity on
top of it and you see the voting done
see ask seven nodes each time and then I
also use smart redundancy with a
difference of two so I'm trying to just
trying to get a difference of two
between one answer and the other answer
and what you see is that when the nodes
are very reliable you're getting down to
almost too sometimes you're still
getting unlucky and get disagreement you
have to ask extra but you're getting
down to almost a cost of two and then
when the nodes become less reliable down
here you're shooting up in the cost that
you're spending and what's interesting
here is that the technique automatically
adjust your never specifying all the
nodes are unreliable you should do
something different it's just that
you're getting more disagreement the
answers are coming back or disagree more
often and so your cost shoots up but if
you look at the reliability the role
system with voting redundancy the
reliability drops just like in
reliability underlying nodes drops quite
a bit but with smart redundancy you're
staying and the scales are different by
the way so watch out for that one but
the smart redundancy the reliability is
staying pretty constant and in fact the
only reason that there's this jiggle
here is because the discreteness of the
system you can't ask three points two
nodes that you really want to ask you
have to ask you the three or four nodes
and so you're getting these little jumps
here sometimes time so that's the goal
to try to keep the reliability of the
system at a particular level regardless
of what the underlying hardware is doing
and then use the resources optimally so
they use the least possible resources in
order to achieve that reliability so you
can actually show that was one example
that was a particular change you can
show that for any desired system
reliability smart redundancy will always
outperform voting redundancy and this
graph I'm showing you here is
theoretical results this is
mathematically what should happen I did
the same thing using discrete event
simulation and you can see that it's the
same graphs and then I did the same
thing empirically using an actual three
sad not three sets or a SAT solver on
top of boink and you can see that it's a
little jump here in fact it's only jump
here for voting done see but you're
getting the same results yeah
whistling smart urgency makes an
assumption about what the actual live
reliability is right it doesn't it
doesn't need to do that so it can you
just specify parameter for how much more
reliable in some relative term so right
so that's a good question so I didn't
quite go over that so you specify what
do you do with voting redundancy you say
ask 19 notes or something like that so
with smart redundancy you say get me a
difference of answers of two or get me a
difference of answers of four and so
smart Nancy goes out and it asks for
nodes and if it comes back as a 3-1
split it says oh I need more so it
doesn't actually know what the
reliability of the underlying nodes are
is just trying to get a difference of
four right it's a way it's a weird thing
to specify but I would argue that it's
just as weird to specify this 19 because
you're specifying the cost you willing
to spend rather than reliability you
want out of it perfect formula
difference of four is effectively cool
to employ an employee of reliability of
I don't know it would be some number
like it's a date it's not it's not quite
that the a4 means a relative improvement
if i gave you nodes that are sixty
percent reliable you'd go from 60 to 80
to if i gave you notes that are eighty
percent reliable go from eighty to
ninety seven or something like that
right so it's an amount of improvement
just like the k foreboding redundancy is
an amount of improvement okay all right
so you know these graphs I don't think
that they're that interesting really
just here to emphasize that the
theoretical analysis is consistent with
the empirical results the system that
would build it actually provides you the
same level of reliability is
theoretically predicted okay but there
is you know I've talked about lots of
good things about smart redundancy
they're not all good there is a cost
that you pay the smart redundancy and
that is with voting redundancy I get to
go and deploy 19 jobs all at once and
they take some time to come back but
roughly speaking I get one time unit of
until I get my results with smarter done
see you can't do that you have to deploy
your some number of result as some
number of tasks and then you have to
wait for them to come back before you
decide if you want to deploy more and
more and so it grows logarithmically in
the number of
steps a number of stages but if your
your job is one that the can't move on
to the next task until you finished with
the task and smart redundancy may
actually cost you more in resources than
well not in resources but in time then
voting redundancy would so for some for
a large number of tasks lots of
reCAPTCHA tasks out crowd sourcing tasks
lots of MapReduce tasks where you can do
things out of order it can be helpful
and you can get all the benefits but
it's not always better so there are some
situations where it's not always better
all right so there's two different kinds
of related work there's one kind is
other types of redundancy techniques and
for the most part what I found is that
these types of techniques work really
well in models where there's random
faults and models were there particular
kind of faults but what I'm really going
after is Byzantine faults where
somebody's compromising my cloud and so
things like credibility baseball taller
and said watches the system for a while
and says this node has never given me
the wrong answer they don't work there
because a malicious agent might give you
the right answer for a long time just to
screw screw you at just the wrong time
right so those kinds of techniques fail
on the Byzantine models where a smart
redundancy does not and then there's
also a number of techniques that are
complementary to our work so things like
primary backup and active replication
these use redundancy and so you can plug
in smart redundancy to tell the system
how many backups you need in order to
achieve a certain level of reliability
things like that so you can actually
improve those kinds of systems all right
so when we talked about a smart
redundancy I basically showed you how
you can use this idea of a computational
channel to try to boost optimally in
some sense the reliability of your
system and there's lots of future work
what I've talked about today deals with
essentially one big channels I get to
ask you a computation I get an answer
back and it's either right or it's wrong
and I grouped all the wrong ones
together but you can actually get a lot
more if you allow these companies
communicational computational channels
to be larger width so in particular if
I'm allowed to deploy for jobs to Tom
and I have some reason to believe that
he's either going to give me the right
answer or the wrong answer for all of
them then now I can try to squeeze even
more reliability out of that channel so
my
I proved that I didn't show you the
proof today it's on the paper but the
proof of optimality assumes that there's
a 1-bit channel if you're allowed to
send multiple jobs to note and assume
correlation between their answers you
can actually be even more optimal you
know even more reliability out there's
also lots of things can be done with
using the history to improve so now
we're assuming non Byzantine models but
you can be even better at using the
resources and then also there's lots of
applications to crowdsourcing that might
have a lot of serious challenges that we
may not have thought about before with
it with computers because people may
work together in different ways and
people the errors may be correlated and
things like that are there any questions
I made you guys answer questions it's
only fair
this is a way to combine these two it's
like the ultimate private and reliable
service orders right so the question is
can you combine style and smart
redundancy in fact smart redundancy fell
out of style when I was working on style
and I was dealing with planetlab some of
the nodes were faulty and I was trying
to figure out how would i use the planet
lab nodes in a smarter way so that when
somebody returns a faulty result to me
the whole thing doesn't fall apart and
so the first the first answer was voting
redundancy but it's very inefficient so
yeah it this came out of thinking of how
would you do reliability to redundancy
in a smarter way I haven't combined them
yet but it definitely plugs right in and
it's possible to do that but I think
that this has much broader applications
and just for style right yes
if the attacker just wants which you
happy
yes that's a good question the question
is about if somebody is trying to get
half of your data or some fraction of
your data how hard is it so the numbers
I've shown you the the big drop off and
then the concrete numbers had to do with
trying to reconstruct the whole input
and you're exactly right if somebody's
trying to come up with just half of the
input what would happen is you would
shift down in that scale you still get
an exponential drop off but it's still
very hard to get any constant for any
fraction of your input what's easy is to
reconstruct say four bits of your input
or 11 bit is very easy but it's actually
getting one bit of the input is a funny
thing so you know that there's a zero
sum where an input that's actually less
than one bit of information right one
bit of information is to know there is a
zero in a particular spot to notice a
zero is almost no information right you
just know it's not the old one input so
but if you're trying to get a constant
piece like five bits it's relatively
easier you don't get the exponential
drop off if you get some privacy but
it's possible if you're looking at
fractions of the input then you get this
fast exponential drop off and it's very
hard to reconstruct them yeah so I
thought about different ways to encode
the input I haven't done a lot of work
and figuring out if there's a more
efficient way so that really when you
reconstruct these chunks you really
can't get much information out you know
one very simple thing you could do is if
you ensure that the 0 0 and the old one
input aren't legal then when you find
out when there's a one or a zero
somewhere in there you get nothing you
get literally no information you already
knew that the input can't be all ones so
that's a very simple encoding but yeah I
think there's a lot of room there for
thinking about how do you encode the the
input in such a way that it's still
computable upon but you get less
information out from reconstructing
chunks of it ok thank you very much for
your attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>