<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Generating Descriptions of Visible Objects | Coder Coacher - Coaching Coders</title><meta content="Generating Descriptions of Visible Objects - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Generating Descriptions of Visible Objects</b></h2><h5 class="post__date">2016-08-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pfm7CuqbdpM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
thank you for coming I'd like to
introduce you to Meg Mitchell she is
joining us from Johns Hopkins where
she's doing a postdoc right now working
on sentiment analysis named entity
recognition and other things that are
not generation but that's actually
because she did her PhD work at
University of Aberdeen specifically in
generation on the topic that she'll be
talking about today which is generating
referring expressions for visible
objects which is something that she's
been pursuing also since her master's
degree at University of Washington so
she's no stranger to being here in
Seattle and I think it's it's
interesting to watch how meg has been
very very you know setting this
direction as an interesting direction to
pursue specifically for generation
really opening up a new field for
generation meg would really enjoy
fielding questions during her talk she
says it's a dynamic talk so we can kind
of go where you know some kind of
combination of where we're interested in
what she can talk what wants to talk
about but I'll be the policeman in case
we rat hole too far in and and and don't
don't get to where we need to be but
please ask questions as we go along
thanks Nick thanks for coming thanks all
right hi I'm Mick um the work i'll be
talking about is work that I've worked
on through at least three universities
um with a bunch of people particularly
Brian Roe work from ohsu um a food
writer in case venting tur from the
University of Aberdeen and a lot of this
work was done as part of the Johns
Hopkins 2011 summer workshop okay so um
before I start I want to say that i've
been doing additional research on
sentiment analysis in social media with
like which I can't talk about here or
not talking about here but if anyone has
a sentiment system I'm organizing the
tax sentiment track so
you should submit okay so okay well I
can make vision to language so say that
you want to search for something on the
internet and as it stands it's easy to
do enough when you have some textual
information but ideally you could go
further than that you can start giving
images and search for things based on
image content and also search for
pictures based on given textual
descriptions right so ideally there's
some sort of connection between getting
information textually and getting
information visually at least you can
work together I started working in
connecting vision to language with a
particular interest in assistive
technology so am at Aberdeen there's an
assistive device where kids with
cerebral palsy who can't otherwise
communicate can have a computer vision
system recognize objects and that gives
them things that they can talk about and
select so they can discuss them in a
natural way clearly this is also useful
for improving search so connecting
visual characteristics to language
characteristics so we can actually query
with text and query with images and sort
of go back and forth there it's useful
for devices that scan the world and and
interact with humans so for example like
GPS devices that can give further
information about what's around in the
world functionality within gaming
devices and this is kind of a nice area
to look at because you don't have to
have computer vision necessarily you
sort of have these objects already given
to you by the system so it's easier to
make distinctions between them it's also
useful for things like automatic summer
evasion summarization of videos so if
you have a ton of content a video
content you don't have to rely on a
human to provide information about it
but maybe you can actually extract that
information texturally based on the
visual characteristics this also can aid
summarization along with textual
descriptions taking also the visual
content um it's fun for things like
caption generation and creative tasks
like story generation and also I find
that working on this ends up helping
computer vision so computer vision
a lot better when you can start
constraining its output based on
linguistic information so my plan for
this talk I'm going to talk about a few
things first I'm just doing a broad
overview to a human object perception
and humans language production um which
has informed my work as I go and I don't
work towards cognitive models but this
kind of gives me some modeling hints and
future ideas that I can then incorporate
to try an approximate human-like
language um then I'm going to talk about
some work I did on pre nominal modifier
or during which is one of the first
tasks that sort of falls out of this if
you want to start describing objects
ordering the modifiers around them then
I'm going to talk briefly about what
computer vision provides and what
natural language generation provides
with the idea of starting to
automatically connect these two in light
of what human systems do what we know
about what people talk about and how we
can actually start describing the visual
world then I'm going to be talking about
two systems one is an algorithm just for
describing visible objects given some
ideal computer vision input and then one
is an end-to-end system called Mitch
which is amazing what happens when you
actually give it real visual input and
everything breaks and what you can do to
handle that okay okay so how does visual
object perception work so free viewing
in the scene is guided by rods and cones
that respond to light reflecting off of
objects this is passed to through the
ganglion cells to primary visual cortex
where we first start recognizing color
and size in addition to edge orientation
and contrast this feeds forward to to
that process these in parallel one is
the dorsal or where pathway which deals
with things like size location
orientation and the other is the ventral
or the what pathway which deals with
things like color texture material these
sort of absolute properties so this is
this nice kind of distinction in the
brain it seems between sort of relative
properties that require reasoning about
spatial characters
sticks and absolute properties of the
object themselves okay the interesting
thing about this is you can start to see
vision influencing languages from this
trend so color and size are some of the
first things to be processed in the
visual system color is available pre
attentively size tends to be available
pretty attentively things like shape
material are harder to recognize when
just reviewing the scene you have to
fixate on it and what we see is a
preference when people describe for
color above all and then followed by
size so these are just some graphs from
a couple of studies Iran where I just
had people describe objects and try to
figure out what visual properties I
wanted to focus on in order to
approximate what kind of language people
would generate and i found that across
the board they prefer a color and size
followed by things like shape and
material these sort of more difficult to
process attributes but there's a clear
preference for these like basic visual
properties so if we can get at those
than we can get a higher recall by by
tackling those first okay so how does
language production work um the
important bit about this is that there's
thought to be a sort of horizontal and a
vertical aspect to language generation
where we have three basic stages that
correspond to conceptualization thinking
of what you're going to say formulation
the sort of grammatical component where
you figure out the sort of syntax and
the articulation stage foot phonological
we're actually starting to to say it the
thing about this is that each stage
seems to have processes that run in
parallel so it's possible to start
blurting out articulating things all at
the same time you're formulating and
also conceptualising so you don't plan
the whole message before you begin
talking but you speak and plan at the
same time this has ramifications
specifically when you're talking about
describing visible objects because it
falls in nicely with what we know about
how people view scenes so pekeman 1989
so that subjects will begin referring to
objects before they have even begun
scanning the alternatives so they see an
object they start describing it and then
they start looking around they don't
tend to look back largely probably
because of a
edition of return and then this
complimentary finding in the vision
community tradesmen and Gallade that
visual characteristics of different
objects will tend to pop out without
focusing on surrounding items so you see
in this little set of X's there the red
X and the oh you can begin talking about
and describing without having to do any
sort of cereal search for them and this
suggests a possible intersection for
combining vision with language at the
level of the noun where how people view
an ounce or few objects and then talk
about them is something we can begin
modeling okay so to specify the task a
little bit more I've been looking at
descriptions of objects specifically
inanimate objects so I've largely
avoided working on descriptions of
people that's encoded in the brain
differently that brings up a lot of
different problems people in the
generation community tend to conflate
the two but I'd like to separate them
out at least for now and say just focus
on what's interesting of objects um and
then maybe extend the models to people
i'm looking at generating initial
reference so in a discourse this would
be the first thing introduced between
two speakers for someone else to read or
hear so the idea is that I want these
models to be usable for both
summarization or for captioned
generation but also when initiating
dialogue and beginning to to bring up an
object for a speaker to respond to and
I'm calling this form of reference an
identifying description which i believe
is identical to Searles identifying
description if anyone is very familiar
with philosophy of language and
disagrees let me know because I'm
starting to publish calling it that so I
hope I'm right okay so when you first
start thinking about the object and how
to describe objects in the visible world
a sort of low-hanging fruit that
immediately comes out is how you order
the different modifiers that you use to
talk about these objects so this was one
of my sort of first forays into working
on this problem and and basically my my
hope was that I could have a story of
these under
classes that gave rise somehow to the to
the ordering that you see there and
these might roughly correspond to
semantic classes um I ended up modeling
modeling it as an hmm where I just do
transition probabilities and observation
probabilities but it put hard
constraints about the possible
transitions between classes so we can
only go see for 2 C 3 C 3 2 C 2 and the
word classes themselves predict the
position in the sequence and so doing
this worked reasonably well and rather
than training to full convergence I
ended up holding out the data set to
empirically determine the stopping point
and I found that as I was doing this the
constraints on the transition
probabilities was tending to skew the
probability of the emissions so I used
this generalized procedure where I held
out some of the parameters train to full
convergence and then added in the rest
of the parameters specifically I just
trained emission probabilities until
convergence and then added in the
transition probabilities and so the
output model was one that didn't have
improvement over five iterations on the
held outset I'm just curious
so where the state's not really giving
them because between the emission
probabilities on their own Beach I don't
know which state correspondent no I so I
have I hope beliefs over all of them i
ended up using five classes c1 through c
8 and then just updated beliefs on the
probability of the modifier given all of
the underlying possibilities yeah so it
tells me what the classes are right it's
a generative model so this ends up doing
reasonably well I tried a bunch of
different techniques at the end of the
day what ended up working the best
frustratingly was an Engram model so if
you train on the New York Times section
of the giga word this is training on New
York Times and then testing on Wall
Street Journal it does better than all
the other approaches i tried
significantly and it actually is
relatively robust across these different
corporate so switchboard which is
conversational data brown which is a
balanced corpus including literature it
does reasonably well and then it does
even better if you start adding some in
domain training and you see significant
improvements so just adding like a tenth
of the Wall Street Journal automatically
parsed we saw a statistically
significant improvement just using an
Graham model um so sort of lesson learn
from that is that you know thinking
about this in a generative way maybe it
makes sense to think about these actual
underlying classes as being somehow
semantic so um something like color and
thighs giving rise to these actual
modifiers that you see and they might
not exactly correspond to the ordering
but maybe their corresponding to what is
generated and then they're ordered on
the fly or later with that kind of model
you can start you can start putting the
observation probabilities as the
probability that you see some kind of
modifier via post-nominal pre nominal or
whatever given some underlying attribute
so you can start generating things like
color and size and if you begin creating
this separation similar to the
separation we see in the human brain you
start to be able to generate them at
same time and you actually find over
specification that mirrors the kind of
over specification that people use so um
I ran this sort of side study I haven't
published on it's just sort of a small
thing and where I had people refer to
cubes and spheres that I manipulated for
color and size and I found that even
though only color or size was reasonable
to distinguish the referent in all cases
people tended to over specify including
size and color so you can get these nice
things in the sort of model it's also
something like this and which is also
suggest the idea that you get these very
strict ordering of
categories uh so I'm not housing that
there's a strict ordering of semantics
hmm or even if they're broken out in
those same ways you mean is it is it
true cross-linguistic Lee that will see
a preference for color oversized yeah so
that's I haven't tested it in other
languages I mean there are some
languages fall into their but yeah she
started inch into something like a
cognitive claim oh I was i'm not meaning
to make a cognitive claim right so I'm
just sort of trying to learn from
cognitive structures to see what i can
do to approximate human language but
specifically i'm testing in english yeah
i've moved on to spanish recently but
that's not different enough so there's
some argument that like you know some
languages have i think i think the the
smallest number of colors is three
there's a yeah so that would be that
would be fun to try to see what happens
i'm not sure experience it people use
for this for you saved in there
oh so a big blue cube right so you can
say big cube or you can say blue cube
but people just a big blue cube and this
really goes nicely with this story that
people tend to parallel processing so
they're not making sure that they're
uniquely identifying the reference but
they're actually seeing visually salient
things as it occurs to them um so this
so this study um if i go i'll tell you
later can I tell you later okay i'll
just get forward a bit to get to that um
but they were fillers in another task so
it was part of a task where they were
describing how to put together a
configuration of objects and they had to
tell the hearer which object to grab it
was an identification task they were
when they specifically asked you talk
about characters differentiated than
measures aimed at totally open domain
yeah yeah I don't control this as well I
could control the properties but I tried
not control what I asked people to do
other than explain okay so um so we have
the sort of idea of what we can do in
order to generate descriptions of
visible objects kind of at a high level
and we have some sort of model to order
descriptors once we have settled on them
so what can we actually do to start
plugging these together what does
computer vision provide the thing to
keep in mind about computer vision is
that it generally doesn't work very well
um so this is a bunch of detectors we
ran on just an open domain image from
flickr um just don't know if you can see
here but you see it recognizes things
like plants and tiger faces and dogs and
chairs I think there's a sheep there um
I don't really see that there the
computer sees it there but the thing
about this is if you tell the vision
what you're looking for right you tell
it to run the mountain detector whatever
it can do reasonably well um this is
especially true on the pascal data sets
that ever
when uses and tests on kind of falls
apart in our open domain but general its
thumb if you just run arbitrary
detectors that doesn't do so well but if
you can constrain it in some way then it
does a lot better um so um so the task
of converting some object detection to
some known is a bit trickier until you
can start understanding what you expect
there to be in the scene semantically
and you can use sort of like
distributional information to do this
sort of thing and generally in these
approaches on their discriminative
models that just figures out the
different categories they're sort of
binary classification tasks um so
particularly recently relevant work is
this work on attribute detection so that
plugs in really nicely to the kind of
stuff I've been working on so um this is
the farhadi at l 2009 paper just a
pulled out a screenshot of it where they
started taking gold standard object
detection xand then within those
detection running these binary
classifiers to figure out whether or not
it has text um let's see they look at
materials like has would they look at a
lot of parts here has metals and other
material has plastic another material
they're looking at shapes a bit as well
is 3d boxy is vert cylinder um it's a
bit haphazard like which properties they
decide to pick out so that kind of
speaks to the need to being clear about
what people are actually going to be
talking about rather than training
arbitrary detectors um but the nice
thing about this is that it starts to
provide a connection to talk about
objects in the in the computer vision
world so now we can actually start
describing aspects of those objects so I
worked on this a bit in 2011 as part of
the Johns Hopkins summer workshop and
RDR dia was this maybe we can start
getting closer to describing objects if
we can figure out the attributes better
and to do this what we could do is take
captions of images so we used flickr
parse it
pull out the noun phrases and then
figure out what the modifiers are being
used what modifiers are being used for
specific objects so given some object
that we have a detector for like bear we
can try and figure out where it is in
the image and then start training our
models to learn those attributes
automatically um so we did this um I
don't know how familiar people here are
with computer vision but the basic idea
was that we used in SVM where we used
histograms of oriented gradients texture
and color these sort of low-level visual
features and we treated the found
attributed detections as positive
training samples and captions that did
not have the given attribute as the
negative training sample and what we
found was that we could do pretty well
for color right that makes sense color
is also a low-level feature we can do
okay for materials and and that's
probably largely because of its
connection to color so materials are
often clear from color but what we can't
do well on our relative attributes so
things like size requires this
additional reasoning about the
three-dimensional characteristics of the
scene that is just not quite possible to
train a detector on yet especially
because object detectors tend to return
sort of areas of the areas of the object
we're sorry areas of the image where the
object may be is where it believes
object to be but not actually specific
segments of the objects then you have to
move to image segmentation um okay so um
so computer vision might be useful for
things like color and it's getting
better at material shape for relative
properties are still a lot of work to be
done so what does natural language
generation provide what can we actually
do to start taking what the computer
vision does and what natural language
generation expects in order to start
talking about objects so the
state-of-the-art natural language
generation right now actually I just
released this corpus so this is this is
the new state-of-the-art I guess but
before this recent release um it's
generally he's like very basic
images of objects that are
computer-generated they're very along
like properties color size orientation
location and that's about it so the sort
of mindset in generation here is that
given some clear specifications of the
type the color the size and the
orientation the question is how do you
select which ones to generate the truth
is that from a computer vision
perspective what you're getting is
confidence levels for different kinds of
object detection zor stuff detections
which I'm not using edge detectors
basically and then you can get a
likelihood of attributes within the
bounding box where the detective
detected object is likely to be you can
get a bounding box which is nice but you
can't get small so this basic connection
here is already not working um so yeah
so there are just different kinds of
object detectors basically so this is
from work by um tomorrow and alex berg
at Stony Brook University and they just
trained object detectors using like
histograms of oriented gradients and
they all and they use spells and swabs
object detectors and then for the stuff
they removed any sort of edge detection
to detect like sky grass things that
tended not to have edges
okay um so given this kind of mindset
within natural language generation and
how they conceptualize the problem the
two basic approaches that you use now to
talk about objects tends to focus on
unique identification one of the top
algorithms that everyone uses is the
graph based algorithm um which defines
the problem as a sub graph construction
problem basically cost functions are
defined over the edges and then self
loops mark object properties and then
these directed edges are used to mark
spatial relations between objects they
develops sort of the best way to do this
tends to be using a branch and bound
method where you try and find the vertex
of the objects that you're interested in
and then recursively construct subgraphs
around that so you make sure it's
connected and that's yeah learn from
data or is it specified and so basically
so this doesn't have any costs on it so
this is just yeah so this is just you
can hand specify if it's an absolute or
sorry actually no the clue says here if
it's a self property then it's a self
edge and then if it's a spatial relation
then it's a directed edge to the next
version yeah yes yeah I'll talk about
that so yeah yeah and so then the
problem becomes okay if you have this
nice structure then maybe we can start
seeing how we can model what humans do
by defining the weights differently
um
I don't think so I think it was just
something that they were playing with
and ended up working reasonably well on
the data they were working with yeah um
okay and then the other sort of basic
approach in referring expression
generation when you're talking about
objects tends to be this incremental
algorithm and it's it's more complicated
than this but the basic gist of how this
works can just be written in this sort
of four lines of pseudocode so you have
an empty set and then for every
attribute you have in some pre specified
order which you can learn from the data
based on frequency if a value for that
attribute rules out something else in
the scene so this is nicely defined for
you predefined for you already then you
add that to the set and then that
becomes your identifying or you're just
distinguishing description to identify
the reference okay so those are the two
sort of main approaches to generating
descriptions of objects in n LG ok so
now let's talk about sort of newer
models for describing visible objects ok
so as I mentioned before sort of across
the corpora I've looked at there's this
preference for color followed by size
and so I wanted to get a color and size
in particular process well in the models
I was working on and we know computer
vision can to some extent provide object
detection xand object attributes within
the founding boxes it does ok with color
size is still an open problem um natural
English generation is working on object
reference that's obviously a clear
connection using object attributes but
it expects size to be predefined um and
color the connection is already there
reasonably well so working on this
connection it seems like the next most
reasonable thing to do is just start
looking at size and how people use size
and this is kind of a fun problem to
think about um it's it's a sort of not
as simple as you think right so like if
you have two boxes and one is smaller
than the other if the if the y-axis is a
bit
a longer as you go like does that become
small and long now what do you call it
now it's thin or smaller now it's
definitely thinner not smaller I guess
this point what is it is it larger the
volume might be more but so there's this
interplay between the height and width
and the depth and that's interacting
with other objects in the scene to
determine the kind of size language that
people are going to end up using so you
can just use this in a discriminative
approach to figure out what sort of size
language to use so I'm taking features
like height features ratio features
distance between the objects and things
like that you can use segmented images
so these are clearly segmented and in
order to try and predict the kind of
language that people are going to use i
ended up distinguishing six different
basic size type so there's two sort of
overall one for large and one for small
and then there are four that correspond
to these sort of like thinner fatter
shorter taller and you can train this as
a bunch of binary classification tasks i
use ended up using decision trees i
tried other things but they ended up
working reasonably well they over fit at
first but if you start taking away the
measurements of the of the target object
specific measurements and just
converting these two ratios um you can
actually do really well at generalizing
even to new domains so i tried looking
at how people were referring to objects
in these groups using their size and i
found that instead of just proposing
that there are two objects right so it
was sort of the basic case you have one
and another and how do they refer to the
size of the one on the right in this
case you can actually just take the
average of all of the heights and widths
so the average height and the average
width of objects of the same type and
you can do even better actually at
predicting the kinds of size language
that people describe so the sense of
working reasonably well so as soon as we
can start getting computer vision to the
point of reasonable segmentation in
addition to having some sort of sense of
distance that's sort of one of the
crucial things we can actually start
using both color and size line
which in a way that mimics what humans
seem to be doing okay so that's one step
right so this is the state of the art in
our eg a referring expression generation
they're acquiring things like small I
moved it to a bounding box okay um so so
then thinking further about this
generative story we have this color we
can learn probabilities to generate
color we can learn how to generate size
and then we can use this to build some
set for an identifying description um in
order to keep the model from over
generating so generating a ton of
different properties just putting a
basic length penalty ends up working
reasonably well so people tend to not
include more than four modifiers when
they're describing objects and then I
just tuned this based on just
empirically based on the training data
and so at that point a likelihood of
generating some set of properties for an
object is a function of the likelihood
of those properties and the and the
length penalties going further with this
you can actually start using these for
non deterministic generation so instead
of just generating the top n you can
generate a distribution over possible
things that people will say and then you
get a ton of different forms and capture
speaker variation which is one of the
key components if you want to start
generating language that sounds natural
right you need to be able to vary it so
if you want to get really fancy and make
sure that you uniquely identify the
object you can just compare against
objects and then use a basic incremental
approach to compare against some
frequency based properties and if one
rules out the object that you want to
roll out then add it to the set okay um
so now I'm how do you evaluate all these
different systems so the sort of general
approach in referring expression
generation looking at Jenna firing
descriptions or generating descriptions
of objects assumes these you know these
sort of nice two poles here
I'm moving this for my algorithm so the
size is actually the bounding box of the
image for the other two algorithms I
talked about the graph based algorithm
and the incremental algorithm I had to
say small large but that's okay that
just makes it harder for mine and I
still be them okay so um so I compared
against these these sort of algorithms
that the people tend to use so there's
incremental algorithm was just you have
to specify a preference order in order
to figure out how to iterate through the
attributes um and for that I just based
it on training frequencies and then
adding object type at the end the graph
based algorithm obviously requires a
some sort of definition of the costs of
the edges for this i just used k-means
clustering i wanted to encourage over
specification to bring it closer to sort
of descriptive reference so i just used
two clusters where one corresponding to
zero cost and one corresponding to one
cost cost of one just on the frequencies
of the properties and the data and then
to choose between edges when there's a
tie there's a preference order in place
similarly to the incremental algorithm
and then I'm comparing it against this
other um this newer algorithm that I've
been working on that generates the
attributes from some sort of underlying
probabilities and I just learned these
from the training data because it's non
deterministic I run it for all test
instances and then can compare the set
of human produced descriptions against
the set of generated descriptions just
just so it's like they're but they're
all seen the scene in
this petrified sir no okay so small
difference is that the incremental
algorithm and the graph based algorithm
this says small and this is large but
for mine I'm making it actually decide
yeah I'm pump from plugging in the size
algorithm there so she could the graph
model and this is a long answer get off
plenty of industries like so exciting if
you know where you go and the conditions
you go this is going to be dependent on
what what emissions you observe in the
world looking at food yeah these five
things light up and you wouldn't finish
and all the other things here just read
about those and it seems like the
eduation have to have conditional
probabilities of the orders of those
being talked about in a language purpose
or something the orders of them I don't
know about the order because it's it's
obstructed from the order you're just
looking at the frequency
okay well I guess I'm just thinking that
like it's like four or five attributes
or something and there's no person would
say something about them I think you'd
imagine that the transitions this is a
directed graph right so you see that the
traditions would put they come from what
those transition probabilities what I
like in the language about yeah so I
mean that's just a finding that the cost
right the when you're when you're trying
to construct the sub graph how you're
going to choose which edges you're going
to connect right yeah that's the cost
the weight yeah sir keep a certain set
of nosy not yeah so it's yeah so you're
trying to find a sub graph that uniquely
identifies the reference so it has to be
a sub graph that only corresponds to the
referent and can be and is not
isomorphic to any other referent okay so
to evaluate these algorithms um commonly
in machine translation people use things
like blue and Rouge which tends to
another measure and graham based string
comparisons but it's been shown that
there's sort of a reasonable measure of
fluency but they're not a very good
measure of content quality so they
correlate well with human judgments of
how fluent it sounds but it doesn't do
very good at capturing whether or not
the content is reasonable so in the
generation community people have largely
moved over to dice which requires you to
have attributes so rather than measuring
string overlap it's looking at the set
of attributes that are generated by the
system the set of attributes that are
generated by the person and what the
overlap is the one of the sort of
nitpicky things i have about this is
that that score depends on what's been
annotated so um so obviously if you have
a very fine-grained annotation and you
know you're not going to do as well then
if you have a very coarse grains
annotation so my thought about this was
why don't we just evaluate over boolean
values for fixed set of attributes so
say
we are interested in color size location
orientation that's four and we're going
to evaluate the overlap over those four
sets the nice thing about this is now
now it's equivalent to accuracy and
precision and recall right because a
false positive is false negative and
there are no true negatives so all
metrics become equivalent that's nice
okay so for a non-deterministic
algorithm like the one I've been
introducing it becomes a more difficult
problem because now you have a whole
bunch of possible expressions that
you've generated so to approach this i'm
just doing i'm trying to find the best
alignment between the algorithm produced
expressions and then the people produced
expressions um and then measuring the
dice overlap for those um this might
look familiar this is the same as an
assignment problem for finding a maximum
weighted bipartite matching so you can
solve this in polynomial time rather
than having to look at all permutations
using the Hungarian method which is
really fun and cute and and in the case
where we don't care about non
determinism and we just want one answer
we can just take the majority whatever
it predicts the most plurality in some
cases okay so I evaluated on this sort
of this corpus these corpora that are
available now to generate in terms of
specific objects and their attributes
and this is the GRE 3d three corpus and
the tuna corpus and I find that on the
GRE 3d three corpus split into two sets
they're just sort of mirrors of one
another my algorithm performs
competitively with a incremental
algorithm in the grass based algorithm
at least as I've implemented them I
tried to you know make them perform
optimally under the development
conditions and then for majority it's
it's pretty reasonable as well sort of
tying with the graph based algorithm and
similarly in this other domain the tuna
domain we see that across the sort of
different conditions of this domain they
had a condition where people were
discouraged from using location
information which i think is a bad way
to go because no you're just priming
your subjects into knowing that you're
looking at these kinds of things but I
evaluated on that anyway and then in the
condition where they weren't primed with
anything it also does reasonably well
the incremental algorithm does really
well in the minus location can
just looking at majority agreement um
but it's not robust across both of them
so mine is kind of giving this more
stable values across the different
domains so it seems to be working
reasonably well okay um so now let's
talk about how to put this all together
and then to end system actually using
real visual input so this is output from
some detectors that we ran as part of
the summer workshop in 2011 with object
detection that were primarily fills in
swab stuff detections that included like
hog and texture these low-level features
as well as canny edge detectors and this
is the sort of bounding boxes that is
giving for this scene so we see a bus
sky a road and these have sort of
various attributes associated to them
the road is thought to be wooden perhaps
the bus is thought to be black and but
there's also some likelihood for for all
attributes right because these are
binary classifications so my idea here
was we should just take the computer
vision output and stick it into
naturally language generation input so
instead of backing off to some sort of
underlying representation let's see what
we can do to just connect the two
together and so this is the top caption
that my system ends up generating for
this image the bus by the road with a
clear blue sky it's a cherry picked
example obviously but it's a nice one
because it's kind of showing the kinds
of things that I'm trying to do
specifically I'm trying to put together
a syntactically well-formed sentence
based on the object detection by
figuring out what the relative what the
reasonable spatial relations are and
what the most likely attributes are
given with the computer vision detects
right so I mentioned earlier that
computer vision is very noisy but one
thing that you can start doing is
actually using distributional
information to
strain what the computer vision sees and
just take the intersection of what the
language predicts to be likely and the
computer vision system thinks that it
sees and then you end up getting
descriptions that are relatively
reasonable I'm going to talk about
previous work just very briefly to sort
of scituate what I'm doing with respect
to prior work there's sort of two modes
of generation for this kind of task one
is this um this approach where you try
and find a caption of a similar image so
you can use low-level visual features
and from there try and generate just the
given caption that you've already stored
as the new caption for this novel image
it obviously looks human-like because
it's written by humans but it's often
incorrect and misleading you know it
could be like two girls walking in China
but it's like somewhere else but you
don't know that because these are things
that are just ground straight from the
caption and then the other approach that
has the people have been sort of working
on is this template based approach right
where you can just fill in verbs you
just set up a basic declarative sentence
fill in object detection as you go
although anyone looks natural the
difficulty with that is that it's not
varied as it talks about different
images right so it'll just keep telling
it you can start seeing that it's a
template the more you see and one sort
of interesting thing about this work
that that sort of stuck out to me was
that the perceived relevance of
description was highest with a simple
baseline that did not add any verbs so
computer vision can detect objects
there's also work on action and pose
estimation action imposed estimation
tends to perform not that well but the
idea was that if we take some sort of
distributional e similar or likely a a
verb for some given object detection
then we can just add it in even if we
haven't seen it in the computer vision
and create a full sentence but actually
people tended not to like this it didn't
end up
ducing the correct verb in a lot of
cases and the preference was to just say
what you do have evidence for okay so so
I'm so in the input to the system i'm
using is the front end of this previous
system called Baby Talk which is from
kulkarni at out 2011 and that provides
object detection staff detections and
attribute detections and basically how
this approach has worked before is just
to fill in template slots similarly to
the Ewing Adele paper but the nice thing
about it is it's a set of you know 20
different object detections and a bunch
of attribute detections and you could
start using these to see what else you
can generate it also is a nice way to
directly compare automatically generated
descriptions with template-based
descriptions using the exact same input
ok so I ended up being able to compare
three different systems I luckily knew
the authors of all of these so it's able
to like annoy them into giving me their
systems the Kulkarni al system for this
picture they're generated something like
this is a picture of two potted plants
one dog and one person the black dog is
by the black person and near the second
feathered potted plant the yang Adele
system for this generated the person is
sitting in the chair in the room and my
system generated a person in black with
a black dog by potted plants you can
kind of see how these two are a bit more
templatic if you compare this one to
this one so this one says three people
are showing the bottle on the street so
you can see they're taking the
preposition prepositional phrase or
sticking it at the end and they're
taking the just coat yeah they're just
collapsing the object detection for
people into one and the Kulkarni Adele
system has this it's a bit of sort of a
room it's a bit of a sort of recursive
template and so they say this is a
picture of three persons when Baldwin
one dining table the first tricity
person is beside and then I kind of tend
not to read the rest like I just get
bored at that point I glebe is just
wrong and so for my system which uses
the exact same visual input we can
generate something like people with a
bottle at the table
okay so that's nice not tagged with the
particular object recruit hey active so
they are yep okay bed so that was just a
miss miss classification in this case
then they're up steverson the rusty
person yeah so it detected rust um yeah
yep that's a visionary right yeah and
that's the whole little nice missive of
working on this vision to language thing
because even if you have an incorrect
vision detection you can have a language
model if your language models good
enough to say yeah you don't see that so
let's not generate that more vague you
are many cases some whatever sentence
will be so basically when you become not
vague basically bullying it does take a
chance the question is when should you
do that yeah so there's so that gives
you special bonus points for not being
okay yeah yeah that's that's a good
point and so there's there should be
some sort of trade off between the
confidence level of the vision detection
and then the likelihood of the language
model so you don't want to rule out
things at the vision detection the
division is pretty confident about right
how accessible is symbolic if you
single input for most of the pictures
that your captain is okay people reacted
with that they really want these long
weird descriptions right so I haven't I
haven't tried just giving people only
template generated descriptions but for
the goals I'm working towards like um
you know I had this ongoing goal of
trying to make a generation system for
kids with cerebral palsy I don't want to
just give them the same sentences to say
over and over again I want to be able to
sort of capture human-like variation so
they sound more like people indeed what
our focus of captions okay no but if you
look at the properties of that corpus
are they really tectonic do people say
the same kinds of things when they're
talking about photos over and over again
well I don't have a direct measure of
that but I can just say by looking
through them like no and and this is a
problem I don't want you guys to ask me
but I'm going to tell you about it
anyway so there's so there's like this
under not lying knowledge of the person
who takes the picture and they might
actually describe things that are not
visual right like Sam's birthday party
and I mean maybe there are cues that you
can pick up or like Sam is having fun
these things that are a lot harder and
and then there's also the captions that
are just sort of describing the scene so
I'm just working towards these captions
that are just describing this question
here like that where do you want to be
in the Oval grand
depends a lot on the application you
know and it seems like inferences appear
to write like a human judgment on the
end with this it's nothing useful sir
correctness might not but might depend a
lot whether someone's looking for
pictures like for instance like even
though this rusty person when it's a
little bit wacky it does have a lot of
details right yeah yeah i mekim useful
so in the minister she asked if I was
looking through the corners innovators
yeah yeah but on the other hand if I
want something that's going to appear
automatic in a news story yeah your
translation like you know and these are
simple parameters you can just set in a
system so i just have a threshold on the
probabilities and the confidence that i
want as an input and so you can decide
if you want to accept something like
rusty person from a computer vision
system given that it's above some
likelihood threshold in your language
model syrian you know i'm thinking of
human or Turing test or something kind
of like again if you gave somebody cares
of beans yeah yeah of course you know
thousands yeah well that's how I
evaluated it I can't talk about
evaluation yet but I will because I want
to talk about him actually generating
first okay so the basic idea behind this
generation approach is that an image is
processed by computer vision algorithms
and it can be characterized as a simple
triple where a is the set of object or
stuff detections and with bounding boxes
and associated attribute detections B is
the set of action or pose detections if
those have been run and see is a set of
spatial relations holding between each
bounding box um similarly a description
of an image can be characterized as a
triple where a is the set of nouns in
the description be as the set of verbs
and c is a set of prepositions that hold
between them right so if you look at it
that way connecting the two is simple
it's just a matter of figuring out how
to sort of order the things around it in
a way that sounds natural but the basic
idea is to Center the generation process
on the object modifiers and attributes
are defined given the object same with
verbs and actions or defined given the
object
and prepositions and spatial relations
ok so the problems to address in this is
first how to order the object so they
are mentioned in natural way so what's
the subject of the sentence and what are
going to sort of be in the relative
clauses that follow and how are we going
to order them how to filter out
incorrect detection so this is just a
current problem with the state of the
art just dealing with the noisiness of
the data how to connect these together
with in some sort of syntactically and
semantically well formed sentence so it
sounds actually natural and then if you
want how to add further descriptive
information even if you don't have a
specific detection but you think it's
likely that this is the case so the
first problem I handled in this is just
learning an ordering model to figure out
given some set of object detection how
am I going to talk about them and I used
flickr captions from this and wordnet i
found that after parsing it through
ninety-two percent of sentences had no
more than three physical objects so that
kind of gives you just a cap on how many
objects you're going to want to put in
one sentence um and then using word in
it you can just estimate the probability
of the position in a sentence like first
second third given a hyper and in class
and the number of objects in the phrase
and that's the way to learn fun things
like and animate objects tend to be
closer to the beginning of sentence and
inanimate objects so here's an example
of how this kind of works so we have cat
in a box contemplating the light cat
from word net we know is an animal box
we know as a structure animals tend to
prefer first and second positions
structures tend to be sort of equally
distributed across the different
positions may be a slight preference for
a second and you see this sentence is
reflecting these trends right so animal
which is prefers to be at first is in
first box and second light is um put at
the end
both inanimate object and and a cannon
object Pierce's overhaul
its overall flickr description so it's
animate and inanimate yep took the car
the dick you scoped it to the set where
there is potent I'll just leave these
effects might even stronger potentially
yeah its publisher yeah but I mean I
wanted to work with noisy data I just
wanted to take I think the set the set
now this flickr corpus is a thing a
million captions at the time i was
working with it it was 700,000 and i
just wanted to just parse it all and see
what i could learn from it without
filtering other than you know HTML and
you know norming it a little bit um so
this just says what I just said but
preparing the data for generation so I
used 700,000 Flickr images with
associated descriptions parse each
caption using the Berkeley parser and
then the key to this is that once it's
parsed you can start to extract word
co-occurrences within local subtrees of
given object nouns so this is somewhat
similar to tree substitution grammars if
anyone is familiar with that it's not
quite the same way and I've been working
with Matt post at Hopkins to to bring it
closer to the tsg framework but
basically the idea is that nouns are
objects there's these seeds that give
rise to the rest of the syntactic
information around the description so
they they're used to define all the
different things with that they co-occur
with the adjectives and nouns or sorry
adjectives on the determiners so so we
can learn if we take the parse we have
these basic template structures / sub
trees that we expect to find for these
declarative sentences so we expect there
to be an NP it'll be it might have a
determiner or not it will have an be
headed by an N and can have any number
of things in between this is the pantry
make syntax which has these like really
flat phrases but just defining that we
can start to learn like well given box
the probability of a versus that versus
another versus no determiner and we can
automatically figure out what's a mass
noun with a count noun these kinds of
preferences like that we can do
something similarly for adjectives so
this is helping to filter out the
incorrect detections from the computer
vision system right so we know that
boxes tend to be red wooden old um small
whatever i'm using a distributional
information we can associate these
adjectives to their attributes and then
we're actually starting to get closer to
what referring expression generation is
expecting where you can reason over the
attributes and then select values
depending on that and that's just
attached to the to the adjectives that
we learned in the corpus okay so I cast
generation in the sort of three tiered
procedure that is common it's also
roughly reflective of how humans seem to
generate on basically the idea is in
content determination you figure out
what you're going to talk about in micro
planning you sort of associate this to
linguistic structures and then in
surface realization you put it out in
some nice fluence fluent sounding
sentence and so for the content
determination stage I'm collecting all
the object names from the vision
detection and then I'm grouping and
ordering the nouns using this word net
model that I learned and to order them
I'm just doing a greedy search over the
maximum probability of the position
given the class for each object so
things that really prefer to be first
can be first and things that kind of
prefer to be third can be for third and
then I build these likely sub trees
around object nouns so from the data
I've defined a sorry in the data this is
like all parse so i can define these
basic sub trees in order to figure out
what is the likely verbs given the noun
what is the likely determiners given the
noun what is the likely prepositions
given the noun all within these nice sub
trees and then from there you can start
creating full trees so if you have a
duck and grass you've ordered them so
doc is at the beginning grasses at the
end you pull out the likely sub trees
that they tend to co-occur in along with
their clothes class information to get
prepositional information so ducks tend
to be above on and by grass tends to be
on by and over then you can just take
the intersection between the two and
start saying things like duck on grass
and duck by grass
there's some kind of in order to
maintain the penn treebank syntax you
have to add these auxiliary trees for a
concatenation um betta but that's the
general idea and the fun thing about
this is that you can also start
hallucinating like some of this other
work has looked at where you just take
something that's likely given given the
subtrees that duck tends to occur in
given the subtrees that grass tends to a
current and given this overlap we can
actually stick them together to say
something like a duck sitting in the
grass all right so and then after that
its surface realization so a predominant
modify ordering ended up being a problem
yay so I could solve that I use the top
scoring and remodel from our earlier
work and this basically thing turns
things like blue clear sky into clear
blue sky so it kind of has this roughly
more fluent sound and post modifier
generation so if a person is detected to
be green I want to say something like
person in green rather than green person
that still needs work for now their hand
coded rules um and then and then the
question is select the final tree so you
have a distribution over all these
different possible trees which one are
you going to select we tried a bunch of
different approaches the one that ended
up working reasonably well was just
taking the longest character length this
tended to be the things that were the
most descriptive so to evaluate I used
840 Pascal images and I compared it
against the human written the Kulkarni
at al baby sis BB talk system the yang
at Al system which is this template
system in my system i put it up on
Mechanical Turk where each subject rated
3 descriptions from each system and used
a five-point likert scale from strongly
disagree to strongly agree with a
neutral middle position which has been
shown tips to get reasonable judgments
from humans um when I had them judge on
a bunch of different levels one was
grammaticality this description is
grammatically correct one is main
aspects correctness order so getting at
the ordering model and human likeness so
that's the Turing test one so humans win
participants could tell the human
written descriptions from the mid rich
written descriptions but midge
outperformed the other state-of-the-art
systems that were based on these sort of
more template based approach so I'm
correctness and order it outperformed
all automatic all these automatic
approaches on human likeness and main
aspects it outperformed the yang Adele
and so this is the more simple template
one and people are finding this
automatically generated structures a bit
more human-like which of these two
senses more human-like which of these
pieces yeah I didn't that would be nice
yes yeah that's um so I have the spirit
advait Siddharth in who does he that's
how he tends to evaluate things I like
that idea it would be fun to do but for
this i just gave each subject just had a
randomly selected set of images and
descriptions and they each judge three
from each system so just sort of they
would not seem
images description under different
systems right yeah yeah okay um great
okay so that's a that's putting it sort
of together in an end-to-end system the
problems of generating these nuanced
descriptions of objects sort of take
second stage to the problem of figuring
out how to constrain and control the
computer vision system but we can put
them together and start approximating
sort of attributes and values that
natural language generation tends to
work with um okay so I've talked about a
bunch of things one is just this basic
overview of visible object reception and
language production and how we can sort
of use this to learn models and features
for generation computational perspective
of what computer vision provides and
what natural language generation
provides starting to put two and two
together at just the basic level of
ordering modifiers around a noun to
figure out how to make a description
sound fluid then I delved a bit into
describing visible objects specifically
looking at size and generating
identifying descriptions so not
necessarily attempting to uniquely
identify the reference but generating
things that are visually salient and
then I described an evaluation approach
and an intern system plugging this into
computer vision on which I call Mitch
and okay that's it and you can try the
midget system online if you want to see
how goes
results all the same so Jenna this is
considered a pretty hard benchmark so
this data set did you try to solve the
vision thing manually we use manual
threshold thing and so let's say you say
that this is what one let's think of the
perfect vision I'll god this is what I
want the output to be yeah and then if
you plug that into the language model
doesn't do significantly better every
time I had mentioned the system someone
has said I should do this i still
haven't done that i need to do that yeah
clearly that would be a really nice
thing to see how it works in the other
comparison with conquering it all those
doses large body of text where there was
a lot of Jesus I was wondering if the
vision task if you do it at a very fine
grain level whether it's a bad thing for
language generation that's something
that sort of thing by my life because
you could do you could do the vision
task at a fairly abstract level yeah and
we would produce maybe a very nice
Enders yeah I think that's I think
that's right and I think that that would
be a really nice way to just push on the
generation the goal of this work was to
connect the two together like I was very
focused on like what do I have to do to
make these fit but I think that as I've
been working more on the language
generation component and try to make it
a bit more nuanced and sophisticated I
think it I think that would be a really
nice way to go I'd like to do that in
yet so from what description i get
impression that
basically you had that deal fly
constraint of Ishmael put orchestrating
actually division up by your generation
system but I think you're doing is an
influence travel right so I socialism
and is very interesting to constrain the
training of the vicious yeah what the
n.o Jesus yes what's gonna do that is
the Legion you get because you could
actually avoid waste them live yeah for
this way some issues yeah on extra
predicting the things that you would use
later yeah yeah and now that I have this
um this visible objects algorithm it
would make it really easy to just stick
that in with at the object description
level so yeah that's that's clearly yes
that would be a nice thing to do yeah
you've talked about generating the
expression for inter she's for visible
do what work is therefore generating
referring expressions for not this cool
and would that be like when we're just
talking for example yeah what research
is there in that area so there's been
similar jordan and walker we're working
on developing dialogue systems that
helped you that sort of modeled how
people would talk about furniture and
how they would decide the costs and
weigh those kinds of things so it wasn't
all visual and they ended up just using
something just like a decision tree it
was it was two thousand i think it was
it was sort of a it was a step in the
direction of these non visual domains
but non visual features were handled the
same way as visual features and that's
been the philosophy of most of the work
in referring expression generation on
the examples that they use and then the
corpora that they build but the examples
that they use they tend to talk about
visual characteristics like say you have
a large black dog and a small white dog
and what without without sort of
focusing on the fact that what they're
talking about is a visual modality and
there's been a lot of work
psycholinguistic work showing that
reference is different whether it's
written or spoken whether the here and
the speaker know one another the
register stuff like that but in terms of
the state of the art for an expression
generation it's just mostly been
focusing on give me a set of attributes
I don't care what they are and I will
order them using one of these algorithms
so what I'm doing is saying well you're
only evaluating on visual domains so
let's just say this is visual reference
and see see what we can do when we
actually attack that task head-on
no a number if you will have a chance to
talk to make thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>