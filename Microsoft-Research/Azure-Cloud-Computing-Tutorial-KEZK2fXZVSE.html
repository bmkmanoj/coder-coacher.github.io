<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Azure Cloud Computing Tutorial | Coder Coacher - Coaching Coders</title><meta content="Azure Cloud Computing Tutorial - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Azure Cloud Computing Tutorial</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KEZK2fXZVSE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
started and women has more demos to show
I guess everybody had a nice little
break I always like to make sure that
you guys get your full 10-15 minutes
full break so that you can actually
absorb what the talk is all about a lot
of times actually go to a lot of talks
and to be very honest half of the time I
probably get lost after I would say 30
minutes of the talk and I know I'm you
know very primitive in terms of
understanding new technologies but it
turns out that it's I would call it the
conference anonymous turns out that my
colleagues and other people that I know
didn't quite understand a lot of talks
either so I would like you guys to ask
questions if you like and we'll just try
to make it a bit more interactive I have
we have an hour and I'm you know the
person that's between you and lunch as
well so I completely understand
um so let's kind of take a look at
what's going on so here's what I did I
actually remote terminal service into
the head node of that cluster that we
created so I typed a simple command
called node list really simple oh I just
wanted to know is what's going on with
all these notes that I just created with
Azure so I asked Ashur for 10 nodes and
initially I asked for 20 but it says
your subscription does not hold 20
because it's somebody else's
subscriptions I think they put a
restriction on me so I could only have
10 so I said ok start 10 and so I typed
note list and as you can see I have 3
nodes initially so they're all online 1
2 3 so these guys are online and there's
a bunch of them offline which means they
Ashley got I got these nodes running but
they're offline they're just waiting for
me to issue the magic command and I know
there are 6 of you
and you have to identify yourselves I'm
kidding that submitted jobs while I was
away
so while you know during the break or
maybe you know one when I was talking so
the first thing I'm gonna show you is
that you know this is the cluster job
manager which means it's showing you
what's going on actually on the cluster
so you know what has finished what has
failed you know four hundred jobs have
been submitted so which means four
hundred jobs Webb simulate cold weather
simulations has been done and prior to
that there is actually a few more
hundred a few hundred more the
interesting thing is that I can kill the
entire thing right now and still have
all the data available because they're
stored in the blob storage and also
table storage for all the jobs so I
don't rely on this cluster to I assume
it's gonna come and go so it doesn't
really matter so tomorrow I can spin up
a entirely different cluster as long as
I have I still have my storage account
so if you lose your storage account you
know you're gone gone but if you have
your storage account you can turn them
off turn them on and always save your
data back to the to the storage so story
just simply a copy operation or you can
also mount the blob as a as a drive it
will show up at the drive if you want it
to do that so it's really simple to
operate in that environment if you want
to learn anything about how to do that
go to the azure training kit it will
teach you all about the basics so there
are lots of jobs that finish so what
we're gonna do is we're gonna take those
nodes online so I'm simply gonna type
node online anything that says offline
I'm gonna turn them on so let's do a
node list again so most of them are
actually starting to come up so as you
can see here's something interesting so
you can see notes 11 through 14 here's
the artifact of when I started
allocating them initially I said I want
20 notes but the scheduler comes back to
me and says I don't have 20
so what it did is tried to do as many as
it could which means it allocated 14
notes and then I went back and says give
me you know 10 so as you can see the ten
would actually be online so up to here
so one through ten should actually be
either online or offline and 11 through
14 I had never really allocated so
that's kind of how it's working so what
I'm gonna do is I'm gonna start taking
them all online compute node let's see 4
Oh oops
I put a little space in there so you
guys can tell me like 5 if I missed any
6 7 &amp;amp; 8
and you don't have to do this manually
it's just that there's a default
configuration where you can turn them on
immediately I just chose to have them
offline so now I have all of them online
and you should see that the other jobs
that were cute
there were 4 of them that were cute are
now running so you can see that all
these jobs are actually running and now
we can go back to the portal which is
the web portal and see that these things
are just running are going to start
running so there's one caveat which is
that as you can see it's still in a busy
status inside the azure portal what it
means is that I'm actually preparing
these notes in a startup tasks the start
of tasks are installing Python
installing you know partial cygwin which
I which I needed to run jido and some of
their utilities and also it has to set
up the you know set up the data and some
of the other operations like you know
make sure that the scheduling is
happening so there's some preparation
that needs to happen here but when it's
all done it will start running these
jobs so let's go to the portal
and let me see if I can uh I think I
killed it so that's the nice thing about
web-based application is that once you
killed it you can just go right back and
let's see lists all forecasts as you can
see that all of them are starting to run
so you know what you submitted so I'm
not gonna ask but you guys are feel feel
feel free to submit a few more and it
takes about eight hours so by the at the
end of the day this evening when you're
done with our drink party you will be
able to see weather forecast the
interesting things if it's actually
queued up it will wait until you know it
sits in the queue but it will always
grab the latest data initial data from
NOAA and run it and once I have it set
up it will continue to run and I don't
have to worry about the IT department
turning it off I don't have to worry
about losing the code I don't have to
worry about pretty much anything because
you know we we have to keep this running
there's a lot of businesses running on
it so the the fact that it's it's
mission-critical
I mean these public cloud systems will
ensure that it will have much better
what are they called ninety-nine percent
one of those service yeah a service
level so it will be better than a few of
your students running a couple of Linux
boxes and hopefully the sysadmin has
enough money to pay for electricity so
it will just you know keep on running
and if you don't need to use this turn
it off and it doesn't cost you anything
so my understanding is that all desktop
computers are only five percent
utilization and I think servers are
around ten twenty which means if you
know how to manage turning this thing on
and off properly you're gonna save a lot
of money in the long run and also you
know really in terms of the reliability
and also writing your soft soft work
correctly and in a stateless manner in
the very first time is also really
money-saving in the in a long run
because one graduate student graduates
your project just going to sit idle I
think I heard that story yesterday or
maybe I was on Monday so so you're not
people dependent it's just going to run
um so I really enjoyed doing this
project I think having it running for -
a couple of months really gives you a
lot more confidence before I just
probably wouldn't have done that I think
it's really about doing a project
where's the cloud and be able to
understand the benefits it will it will
be a lot better so I encourage you guys
to take this and you know play with it
and see you know if you can take your
scientific workflow and you know make it
right without writing a whole lot of
code actually be able to deploy that
into the cloud and try to you know make
it into a service and the second nice
thing is that you can share it was the
world so right now I actually have
random people submitting all kinds of
stuff on here and some of them are just
me but like there's just hundreds and
hundreds of interesting things and it's
also makes a very a good educational
tool because I bet I don't think there's
another service in the world that allows
you to do this sort of stuff so you can
you know play back the precipitation I
this is in Miami okay and also you know
when speed a lot of events actually I
found that some of the folks found
Ankara Ashley logs on and types that
there's a little simulation and I would
know what kind of events there are so I
think they were looking for validation
so it's also a way of keeping a golden
copy of your code that you know has been
working it's got the production
production copy so I really found that
to be a very good software engineering
practice so any questions
so the theta is in blob storage so I
keep all the data in the blob storage so
some of them I actually started deleting
because it produces about 5 gig per run
and it's gonna overrun my storage very
quickly so what I instead did is I keep
all my gifts and animations the other
overlays so I keep the images but I
don't keep the data but I can keep the
data if I want to so what that is you
know it's very easily accessible you all
you need to do is to use one of these
tools like as your storage or something
as your storage so we have you know a
lot of interesting open source projects
that's out there that you can you know
some play run so yeah I have a lot of
fun little projects but let's see if
this one allows me to log in so here's
the wharf storage if I can get this
thing to load so if I so it you know
once you log in you'll see them as just
simply files and you can you know mount
a blob storage over as a drive and keep
writing to it so that's the persistence
store and it's actually pretty cheap I
think it's a if yeah when do i scrub it
it depends on you what would you know it
you know you run when your credit card
runs out or or or when the data is no
longer important you know you can do
whatever you like was the data that you
store on there so for example I know
people that store medical you know some
of their medical data on it and what's
interesting is that they kind of gone
exponential so they're storing I think
it was 18 gigs initially and then it's
like 60 gigs and you know 130 gigs and
then you know what's what's really good
about is that you we replicated three
times
and also with Jill Jill replicate that I
don't know what is going on today but oh
I know why I probably have the wrong key
I don't have it stored in this storage
but when you browse it looks just like
an FTP site or any of these
I just recently cleaned out a lot of my
a lot of my machine so so these accounts
are no longer valid but if you log on
use just simply you know dragging and
dropping files it's not really complex
you can also automate it so we provide a
rest service and you can use curl if you
wanted to in fact I was you can use
Python you can use curl and you can
pretty much use any any anything that
supports rest the nice thing about
writing about net code is that you don't
have to go parse you know XML right I
mean that's just not very exciting
so there's Python tools out there by the
way Microsoft actually has something
called the Python tools for visual
studio which turns out turned out to be
super slick I've been using it a little
bit and I really found it to be very
cool so I I do a lot of Python code for
a big data type of you know manipulation
you know clustering of data sets but you
know we have we have Python so Python
will have a lot of azure support which I
think would be very good for people who
want to do it front another platform so
just the other day I was just sitting
down thinking how do I access management
and upload management certificates and
all that through through through on
Linux so I was sitting on a Linux box so
it took me about I would say 35 minutes
to figure out how to use curl to access
the management API it's really simple
you're simply running against a rest
service and you can use that for
computer storage or anything you want
but in terms of cost it does cost money
I think it's uh I don't quite remember
around 10 cents per gig per month
which is you know but it's replicated
three times we ensure it's always gonna
be there
and in fact a lot of my colleague stay
pay there they use their own personal
credit card to store their family photos
and all that on on storage because you
know you you have a habit geo replicated
so anything happens your your data is
gonna be be there so so and also I think
at some point we talked about how to
access things like a blob what is it
called a Dropbox and that are other type
of things so in a cloud a lot of things
are rest based so you can access Dropbox
from as your compute services as well if
you really want it to and it's just in
play you know you you have an API key
and you probably use H Mac for
authentication you you know you would be
able to grab data you know operate
against other storage services even
Amazon s3 so so it's all interoperable
SkyDrive I don't know I haven't tried it
I don't know if they have a REST API but
I would assume they have one SkyDrive
has was a 25 gig free that's really nice
I should probably look into that um but
but this doesn't cost you much either I
mean it's just what is a Dropbox is
getting expensive I think I have it I
think it cost like a hundred bucks or
but anyway the point is that this
ingress is free as well so if you upload
lots of data on to the cloud for storage
it's free and also you know I know this
is like kind of a public venue but I
certainly don't mind telling you guys
that we have some new HPC had we're in
the indie works so they're gonna be
specialized very fast machines I can't
disclose what exactly what they are but
you know you guys should be on the
lookout for these new hardware which is
actually really going to be really
really nice I can probably take my
simulation and run it and take it from
you know six hours down to a few minutes
once I have we
the hpc had work but the important thing
is the abstraction the the ability to
you know share and also the ability to
collaborate and the convenience of the
cloud is really important at the end of
the day it does save you a lot of time
and money and also I encourage people to
write their develop their software in a
stateless manner regardless of what
platform they're on they're on so it
will always save you a lot of money
I remember my my previous my advisor
always told me that you always have time
to do it right the first time and you
know that has worked out yeah save me a
lot of time
oh is that advice so so that's kind of
what I you know see from a lot of
scientific computing places that you
know code is written in kind of a
hard-coded way because people think that
they don't have time but in the long run
and will you will save yourself time
especially for the grad students here is
that it will come back to haunt you if
you write your code in a sloppy way it
has come back to haunt me yeah
yes sir that's a really good question
so MATLAB has we're actually working
with MATLAB so what their model is that
they're gonna put licensing servers in
the cloud probably do that and also they
could also you know sell you a desktop
version it's kind of the burst model
where they launch their cloud instances
from from your actual desktop
application so that's that's a
possibility I mean the simplest way to
run a MATLAB code on the cloud would be
can you max it take the binary and shove
around to the cloud and run that yeah so
but yes they will have MATLAB running on
the cloud I mean we we are trying to
solve a lot of these problems so there
are a lot of different scenarios running
stuff on the cloud at the simplest way
by you know far is something as a
service
I think that goes a little overboard
over here as a joke sometimes but
anything is a service essentially so it
would be you know probably office 15 is
obviously a I mean what is it called 365
sorry that's a service you know Google
Doc is a very clearly a software as a
service and then you go to platform a
service which you know it has a lot of
the global services down for you to
enable to you to write a cloud
application so for example we have you
know service bus we have you know you
know these these blob you know the
storage services we have all these
infrastructures allows you to make your
application stateless I have to explain
our service bus a little bit so
essentially it's a pub/sub model of
getting notifications and also we have
just you know things things that really
enable you to build a global global
application so these things are not
necessarily applies to everyone it's
just like it's an overkill for me to go
buy a bus right now
but it's there once you want it and it's
you know for rent and the fact that the
public cloud is available today enables
a lot of interesting scenarios so the
next piece I want to talk to you guys
about is Hadoop on Azure so this is a
data conference and I do want to you
know to explain a little bit about
Hadoop so how many of you actually have
heard of Hadoop or have started using it
I started using it would you guys like
to come over come up here and talk about
so I kind of started looking at Hadoop I
would say about a year ago and I found
it to be you know very interesting I
think the main the main thing about
Hadoop is that it has a has a ecosystem
where you have you know a lot of
interesting things think that's built on
top of it so for example if you want to
do a machine learning there's mahute and
if you want to do page ranking there's
pegasus there's a lot of
stuff that sits on top of it so it
allows you to take advantage of all
these tools and also there's a lot of
applications that are starting to be
built on top of Hadoop a lot of
bioinformatics type of applications gene
sequencing and also anything that has to
do is graph so so we actually so Hadoop
is really about MapReduce taking a large
amounts of data and distribute it onto
the cluster and be able to move your
compute code on to the nodes that
contains the data so it's a pretty
straightforward concept to understand
but what's what's what's interesting
about it is that it kind of goes from
the same route where HPC stands so it
kind of take the database you know the
the monolithic database story where you
have to buy these giant you know Super
Dome machines was you know MPP
processing and just simply say well I
have a lot of data and they may not have
a schema and I honestly don't care that
much about them but I like to extract
information from this massive amounts of
data so what do you do so you buy a
bunch of commodity machines and I have
to note that when they say commodity it
doesn't actually mean you're you know
ten year old PC that's sitting under
your desk what they really mean is
server class machines that are you know
cheaper than these database boxes but
not necessarily you know as cheap as
just you know you're you're you know
$200 pcs so so let's kind of go over
that and I have all of these
presentations in the training kit so
it's easy to go do so the way you find
this training kit is you go to download
Microsoft comm and simply search for as
your training kit you just find it and
it has all this stuff in there and and I
keep I like to kind of keep referring to
that is because we spend a lot of money
on this so I like to you know
have as many people take advantage of as
possible so let's open this presentation
and I'm gonna run a little run a little
demo of it real quick so here's the
Hadoop so 1.7 zettabyte that's a lot of
data and I honestly think that well
we're not really storing all that data
we spur most of them away so people
actually think that we store this data
no we don't so what ended up happening
probably 1% or 2% data ever gets saved
but the real problem is that it gets
bigger and the velocity goes much bigger
as well so for example one of my former
boss works for good nip which is a
boulder based company so I'm from
Boulder Colorado so I work for a tech X
Corporation which is the do-e contractor
so one of my former boss started working
for that startup and I kind of learned a
little bit out what what that company
does they have a full pipeline they have
one what one of the two maybe full
pipeline of Twitter so essentially
anything that tweets every day I think
it's 155 million per day so that's not
just that 144 character 40 characters
it's also metadata it's also web pages
that the the asshole you know maybe the
text or the PDF or anything that it
links to it comes with a link so you
have to include all that stuff so your
pipe suddenly becomes huge so big data
the the rise of Big Data really came
from social networks where just suddenly
you have a globalized application where
you're storing a huge amounts of data
which is not necessarily structured so
they're not structured meaning that
they're not they don't have a schema for
it so it's just data and before that
that was search engines search engines
also Big Data and Bing Bing has I think
it was petabytes in pet of it was yeah
seven petabytes a month I mean that's a
lot of big
and I don't even think that being would
be able to you know do a lot of this
in-house immediately because this is
just a huge amount of data 155 million
messages that's a one terabyte a I mean
it may not seem like very much but if
you multiply it out and it's one
petabyte at s1 tire by per day and it's
still growing there's just a lot of data
that people want to process not just
simply by batch systems but also in
real-time so you guys probably have seen
this issue of data delusion that's
essentially saying that how do you you
know how what are you gonna do about it
with all that data so here are some
example scenarios traditionally there's
this you know this database world where
you have your enterprise where a data
warehouse and you process some of them
and you store them and you throw away
the rest and now people are starting to
realize that all data has value to it
what's interesting is that Facebook and
Twitter they can figure out a lot of
interesting things about you and also
there's location data and I don't really
have to explain data to you guys because
you guys have a lot of scientific data
how many of you have more than one
petabyte of data you have okay well yeah
I can't even imagine how we're gonna
store that I think the the IBM was going
to do a tellus telescope or astronomy
project which has I think it was a
thousand pedable what exabyte I think it
was one exabyte worse than beta oh okay
woohoo that's a that's a lot so the idea
is that you use these commodity machines
and use their hard drives to distribute
the work work out and and it's really
not just about amounts of data it's also
about the variety there's different
types of data that comes in right so
there's social network data there's log
data as many log data is a big big deal
people really mind their log data to
figure out where you know they know
exactly where you've been browsing for
the last 12 years
and they could actually probably
structure a timeline of what you're
doing every evening that's starting to
sound really scary but on the other hand
there's also can be used to discover you
know interesting good things and which
is what you guys are focusing on which
is how to use big data for the cue for
the good of mankind and also at the
velocity right so we just talked about
that I mean everybody not just you know
simply tweeting and the data from your
sensor network is probably going 24-hour
continuously being stored somewhere um
so who who's doing sensor network here I
know a bunch of you guys are doing so
where do you store that is it stored in
a database or is it stored in one of
these you know no sequel type of data
store you do both okay yeah as flat
files yeah yeah yeah into a database so
do you delete something do you pre
process them first because scientific
data in general is already structured
okay so it's already structured in some
way okay so because that's a big
difference and actually that's one of
the distinction I like to make which is
big data the the industries you know the
big data growth is really about and
structured data which means you know the
social network is big part of it and
also being able to do things in real
time that's another hot area where a lot
of financial institutions invest a lot
of money in and also scientific data
which has always been around but that's
nothing new that we have to do so so
here's the how things work I mean
obviously you guys know how data works
and I don't have to explain that very
very much but it's really about
MapReduce is really about taking your
code and map it to where the data is
without having to move it very much and
instead of you know using a database
you're using you're using the local
storage
of these machines and the map Map Reduce
workflow can be you know changed into
many many different layers or not really
layers but pipelines or you can
yeah pipelines so you can do MapReduce
MapReduce MapReduce for at many stages
to to produce a some results and I think
there's a term called iterative
MapReduce which Microsoft Research has a
great project called Daytona I think
it's still running which allows you to
which is optimized for the cloud so the
cloud system is a little different from
on-premise meaning that the storage is
actually not necessarily the local
storage you can use the local storage
because that's temporary so or your
storage actually has to go sit in a
centralized storage like the blob store
so that the operation and the algorithm
becomes a little different what they do
is they add caching layers they add
different types of optimization to make
sure that it's it's fast enough to be
able to solve your problem so I would
encourage you guys to look at Daytona
you know not necessarily the actual code
but look at their paper to see how they
optimize you know MapReduce for for the
iterative MapReduce for the cloud but
Hadoop is you know even simpler than
that so it's just simply MapReduce
okay so there's you know this is
actually exactly what you guys do which
is data acquisition you know you do a
collaboration visualization is a big
deal how do you visualize a huge amount
of data do you do sampling or do you
actually try to read all demand so you
know those are the challenges that the
inter industry has and I think they have
a lot to learn from the scientific
community about you know how to
visualize a huge amounts of data so we
have tools like NHPC we have tools like
visit and you know pair of you to do a
parallel visualization so a lot of that
would eventually be done in the parallel
parallel fashion for for visualization
as well
so visualization is also you know really
expensive so here's how Hadoop works for
those of you who don't know which is
that you have job trackers which tracks
all your MapReduce jobs that gets
divided up onto the cluster and then you
have your data nodes that's controlled
by a name node this looks very familiar
because you're operating in a clustered
environment HDFS is sort of a meta file
system and I'm pretty sure it's
replicated three times and you guys
wouldn't you know you guys who have
played with it probably know it a lot
better um do you guys keep it rep at the
replication three times as always or do
you guys shut it down to or reduce it by
a to one only how do you afford a file
system what do you guys do with the file
system in terms of how many times your
data is replicated twice ok ok yeah and
you can start running Hadoop on your
local workstation yeah you know like
even if we had just one node you can
just download this it's just a Java
bunch of Java code and runs a bunch of
services and you can start you know
running your code playing with it
just on a single workstation so it's not
really hard to start playing with Hadoop
here the differences between our DMS
versus MapReduce MapReduce is really
about linear scaling and keeping the
cost low more than anything else as far
as I'm concerned it's a it's a batch
system and you don't you don't read it
very often but you know that's all going
to change in the future I'm pretty sure
so with the new HBase HBase tool a tool
set they're modifying it to be you know
to sustain a lot of writes so all these
things can be optimized for what you
want to be want to be doing but the main
difference is that it doesn't have a
schema and it tries to scale linearly by
using cheap commodity software that's
hardware that's clustered together and
here's the ecosystem that I talked about
a little bit so Pig is for workflow and
hive I actually have a quick demo
that to show you and then scoop is for
importing and exporting data in and out
and then there's stuff like HBase which
is a column based database a ya column
DB where you can just shovel a lot of
data in there there's something called
Cassandra which is kind of equivalent so
there's a lot of tools that sits on top
of and then there's the analysis tools
like Mahal I don't see anything here
that's not in here but there's like
Pegasus a lot of these tools that allows
you to do it provides algorithms for you
do do clustering of your data set and
one of the things that I used to run
quite a bit is running information
retrieval retrieval using SPD's so
apparently my Hult has a has a disk
based SVD solver I thought that was
really interesting it just runs really
really slow but it will be able to
operate on a much larger scale than your
linear algebra library that's sitting in
memory so that's also a difference
between HPC and big data which is
everything is you know you're focusing
on the disk i/o side of things so we
actually provide a high bode BC driver
which I'll show you a really quick demo
of and then there's the rest of all
these interesting things that you
actually saw there's power pivot and you
guys probably then we show any demos
about Power View so far we haven't okay
so Power View is part of sequel server
2012 Denali and you know basically what
Microsoft is doing is actually I really
think it's it's great it allows you to
take open source software and
interoperate that was the commercial
quality software that we produce allows
you to you know have a lot more
flexibility in terms of cost and also
you know development time if you you
know if you want to use the commercial
side of things as you can see that the
open source code is generally a little
flaky in terms
stability but you know as we support
Hadoop we'll make it a lot more robust
and also a lot easier to use for people
on the Microsoft platform but we also
will allow contribute the code back to
add Apache foundation to make sure that
you know people on other platforms can
take advantage of our changes as well so
so I what I want to show you is that
maybe maybe a demo a real quick I think
that would make it more interesting than
going through slides all the slides so
the portal is called Hadoop on Azure and
you can actually go get it go get a free
subscription so you could if you go to
connect out Microsoft comm you'll be
able to log in I'm gonna type Hadoop on
Azure comm so once you go on there it's
really really simple just think of of
Hadoop on Azure and there is an
invitation you can click on it it may
take a few weeks but you actually get a
cluster think about it's the free adjure
you wanted so it creates a little
cluster for you once you sign in so I'm
gonna sign in here with my login so this
is a typical example of software as a
service in this case the software is
Hadoop itself so I think I'm trying to
sign in or do this I mean oh there you
go
yeah so I think our product team is very
funny but so here's the very simple UI
since I'm showing you on the very large
largely amplified screen I can show you
basically you know you're you have a
little interactive console where you can
play with your code a little bit and you
have a remote desktop you can log into
the cluster where you just initiated and
also you know keep the ports open you
can FTP in I know everybody loves FTPS
you can use curl and other tools to FTP
in and you
your data you can connect to your blob
storage which is the azure permanent
storage or Amazon s3 if you wanted to
and you can import data from data market
and we're gonna show you an example of
that how did how to do that so it looks
like I have a little bit data that's not
very much so I have a four node cluster
which means it's you know fairly small
you can have up to 32 but in general you
probably don't need that much so this is
more of a development environment than
anything else right now so this is a
preview of what you know Hadoop on Azure
looks like so let's just run some simple
samples or actually you know at least
look at the samples that I ran so here
is a 10 gig terasort that I started a
while back and it's still running
apparently so you can Hadoop is Java
base so you can you know you simply tie
a type in Hadoop command jar and you can
you know one of the examples and you
just run it and that's how do peer just
writing Java code I mean more than
anything else
but what we did is we actually added a
little bit of Java console to it a
JavaScript console you can actually
script against Hadoop if you really
wanted to I have a I think I was showing
a little example earlier but we can do
it again ok so here's a JavaScript
console so you can't actually access all
the file system isn't that cool to me
this is a very much like a UNIX file
system but it not really it's not really
a UNIX file system because you're
operating against the distributed Hadoop
FS so the command you're actually typing
going back and forth is Hadoop FS LS you
know right right the command is LS you
can see I have some files there but what
I'm gonna do is I'm gonna run some
simple examples and let me see if I can
find my demo script here I'm just gonna
type that in real quick it's gonna take
me a while to actually type it so so
what you can do is you can type a pick
command Pig is the workflow engine so
you can type something like this
obviously this is wrong
I have to type so they changed it a
little bit so it's not quite the same as
before
so what I can do is I can say you know
word count so I have four files up there
which I do want to do a word count of so
I'm gonna run run a MapReduce job so
Gutenberg is a directory which you can
see it's a Gutenberg I have four books
in there so I'm gonna do a word count
down for books and we're gonna run a
MapReduce job which you know uses the
JavaScript code and I am going to order
them by the you know the sand and take
ten of them so essentially I want ten
ten words to be in there so I can just
go ahead and run this job and and then I
can show you I'm gonna call it top ten
2.2 and I already have a job that I
finished which I can you know visualize
very easily pound ALS so I have the
results as a Gutenberg topped him from
before if I want to visualize what it
has I can simply type
I think it's file file equals FS to read
a GB top Gutenberg top ten let me see if
I can type that in so what I'm doing is
that I'm reading in the results and try
to visualize it right here on the
console so there's ten words and there
are currencies and then I can do a
little plot I can say data equals parse
I don't remember what it was I think it
was a parse see if I can find this yeah
that's right so you want to parse the
data file itself so this is all just you
know simply pick commands but you did
but we allow you to do that in
JavaScript so I'm gonna paste that in
data equals parse they have that right
so I just parsed you know that dataset
and then I can say graph graph bar data
what did I do
oh well now once again capital D oh I
didn't spell it right okay
OOP there you go so I can you know just
do a lot of interaction right there
on the console without having to you
know deal with all the jars but at the
end of the day I think people just
really want to run something you know
have a little development environment
login you don't want to man you managed
your clusters what's what's funny is
that I'm not gonna mock you know things
on premise once once this I'm the cloud
evangelist is that I actually was
looking at you know some of our hands-on
labs that that you would what you would
actually have to do to to you know get
your cluster set up just you really
under any environment this is no
different from no different from you
know developing on any other platform so
there's just look at this this hands-on
it's 44 pages and 40 pages of it was
setting up the cluster look you know how
to set up your VM or VM or VM and all
this up and by the time I spend that
time playing with it I'm not really
focusing on the problem at hand so
that's the power of the cloud and it
just keeps on going so the point is that
you Hadoop on a sure it's a really
interesting direction of you know taking
taking things into from software
reserves and we also have done a very
good job of being able to import and
export data so the simplest thing you
can upload a data so how do you upload
the data to to the file system so you
can simply say in the JavaScript console
F stop put and browse locally and you
have a file up there I don't know what
would I want to there you go so I can
that's my five gigabyte email archive
okay but let's just find something that
we can upload I do bond - sure how about
this I will just upload a big movie and
upload and it's just asynchronous
because I can actually you know still
browse what's in there but it's
uploading in the background just simply
by typing a JavaScript command um so so
that's that's the simplest way of using
it so how do you use that with Excel so
I'm gonna play that video that we showed
you earlier so this is something that I
built a while back maybe half a year ago
well four months ago so what this is
doing is taking data from data
marketplace so I log in and you know get
into my account and I will go to data
marketplace and import some data set so
it's really really easy all you doing is
that your username and password you can
create a free account on data market and
you can use mine if you really wanted to
but I think that key expired and then
you can you know put a query it's really
simple you go to one of the data set and
come back down here and just grab the
you know the the HCP API it I think this
is old data you just grab the old data
API and and put it into the query and
you want to import that into the hive
table hive it's there you know into
enterprise data warehousing think that
sits on top of Hadoop it's a column you
know it's not a column DB it's a fully
featured database tool so I said import
so I imported all the data and then I'm
gonna go back to the interactive console
and I'm gonna run a query against the
data that I imported through all data
it's real simple I'm saying is that I
want the maximum crimes through the city
and then I just type in the query on the
screen without having to touch anything
don't have to log into the machine just
type it in and go to Excel install the
hive ODBC driver is just executable
reinstall that you know that's already
installed obviously so just type in the
machine you
to you the cluster you want to connect
to MapReduce and my name and password
and hit OK and you can query against
cluster right there from your desktop
obviously I did a you know I don't want
to move all my data so but only the
results so I did a select and I you know
told it to limit to ten once I did the
actual query ok so I'm just you know
simply doing a select from that they did
you know data query that already ran
so I can just use the normal Excel stuff
to run it and if you want to use power
power pivot you could do the same right
and will be very simple so so that's
kind of an introduction of Hadoop on
Azure and what Microsoft has done in
terms of making it easier to use it's
currently in CTP and that you can go to
Hadoop on Azure calm and get the get the
actual uh you know login and password
and play with it a little bit and people
might you know ask me about performance
we're not currently the way it's set up
we're not really doing the performance
part yet but later on we will make sure
that that improves so right now it's
more of a develop environment just
showing you how things look just try to
set up one of these at home it will take
you quite a quite a bit of time but just
log on and get coffee and you're back
with a cluster ready to go so alright so
I think that's the that's the
presentation on Hadoop and I we can you
know go back and look at who has
submitted more jobs for the weather
simulation let's see where they are
right now wetter did I spell it right
okay all right there you go so we can
list all the forecasts and okay so you
guys can submit a few more if you want
it to oh wow okay well they're all
running so seven percent eighteen
yeah so submit a few more and let's just
and a for those of you who have
submitted and you may not like you know
you are wondering where the heck it is
there is a it's not owning a bug it's a
feature there is a feature that it hides
all the jobs that has are queued up in
the backend it only shows the ones that
are running so look there's somebody put
that MSP MSP in there so it's kind of
queued up so so tonight check the
results on weather service cloud app.net
and i'll make sure that this source is
gonna go on github as soon as possible
and i know a lot of people are trying to
run use Azure to do compute without
having to learn you know all these tools
this is a great example of just simply
scripting but I you know but also adding
a front-end to it for visualization so
there's a lot of pieces that's involved
there's being map that's involved and
also there's a new implementation that
I'm doing using MVC for what that means
is that it's Microsoft web system it's
very similar to Ruby and the Django for
those of you who know Python and Ruby
well so the idea is that you would be
able to connect Web API to provide old
data provide JSON private provide XML or
you know data format of your choice by
simply creating a web application it's
very very powerful and it makes it super
easy to be able to do and you know
making your data sources available
another project that I'm working on is
was the hdf5 group so we're trying to
take using f-sharp to do intellisense on
datasets that you bring in so f-sharp is
a strictly typed programming scripting
language but which is you know perfect
for data and the nice thing is that a
lot of times we make errors in terms of
you know programming but but you know in
terms of data that also happens you know
you may have selected a wrong column you
may have done something wrong so having
intellisense on scientific data sets
like you know hdf5 format what
be very valuable to reduce error and
also make people more productive as a
scripting language so that's all I have
to say and I make sure that you got
seven minutes to get lunch Thanks
questions yeah go ahead
Hadoop
right so the problem also becomes that
you have a probably not a very fat
pipeline between your machine and and
the cloud so we have we have a much
better you know pipeline do you probably
once you have your data in the cloud you
probably want to run you know stuff in
the cloud and also you know there's
there's a little bit of a problem with
with some of these you know models in
the in the cloud like Hadoop obviously
doesn't run as well in the cloud as
on-premise because of the fact that
we're not really I mean you can use the
local disks but ultimately you still
have to put your content back into
storage so you know up having that
optimized I mean that optimized meaning
that you know physically move your
machines closer to the data center or
doing doing interesting things will
eventually happen you know I I don't you
know I'm not embedded in the product
team so I can't really tell you anything
I because I don't know but I think a lot
of these optimizations are gonna be in
the works but yes you ultimately want to
make you know keep keep your stuff in
the cloud and run your analysis in the
cloud but it shouldn't prevent you from
marching up local data so you can still
keep your local cluster and run run that
but you probably wouldn't want to send
everything up you probably want to send
you know maybe a selected few items and
then joying it with what's in the cloud
so it's really about common sense and
and you know data locality so the
program you think is draining on the
data volume and the communication
between the kiosk you know the
universities generally are on internet
too so they have a really fat pipeline
several key maybe yeah yeah maybe it's
actually quite feasible but you also
have to keep in mind that latency may be
a problem even though latency is less of
a problem for a big data problem because
your batch already but you know it's
it's definitely possible and you know it
also depends on how much machines you
can keep on premise
right I mean I'm premise people have
limited real estate I think that is the
main thing right now is that I can't
keep all three craze in my in my office
I have to keep it in data center if I
had 40 of those it would have to be in
the cloud so the ability to get a
concentrated amount of resources or
continuous amount of resources is
ultimately going to be in a public cloud
or you know even a do-e type of setting
any more questions or all right well
thank you all very much enjoy lunch</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>