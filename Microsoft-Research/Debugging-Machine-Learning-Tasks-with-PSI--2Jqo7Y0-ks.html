<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Debugging Machine Learning Tasks with PSI | Coder Coacher - Coaching Coders</title><meta content="Debugging Machine Learning Tasks with PSI - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Debugging Machine Learning Tasks with PSI</b></h2><h5 class="post__date">2016-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-2Jqo7Y0-ks" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
thank you I'm a member of the
programming languages group here at MSR
and today I'm going to talk to you about
a tool called sigh that aims to help
developers of machine learning fast
debug errors in their data and this is
joined to work with various colleagues
at MSR and our interns alex chakra of
and char x n so let me start with an
example so imagine that you want to
build a classifier for images of
vehicles and animals and one could do
this by using and off the shelf machine
learning light duty for instance a
library that implements logistic
regression so i have label set of images
which i pass on to library and the
library learns a classifier for instance
the classifier shown over here and as
you can see and this line over here is
supposed to separate the vehicles from
the animals and as you can see that this
isn't quite the right classifier as it
would miss classify this image of the
car as a animal and and oh yeah i got it
on the wrong side so yeah yeah so I I
think I got it all wrong so in any case
right so the the what you'd like to
learn instead is a classifier which is
the so I should have put the dashed line
before but what you'd like to learn
instead is this dashed line which really
separates the vehicles from the animals
and our object and the reason why the
classifier doesn't figure out this
dashed line in the first place is
because as far as the training set is
concerned this set of vehicles has been
incorrectly labeled as animals and our
objective in this work is to
automatically figure out such miss
labels in the training set and one naive
way of doing this is to look at labels
in the training set perturb them that
has changed a labels retrain and observe
the outcomes in the test set and there
are various issues with this approach
and for example the most most notable
ones being that changing any training
instance does not
I certainly fix the error in fact there
could be multiple labels responsible for
a Miss classification in the test set
and as you can imagine there an
exponential number of such subsets and
also every change to the training set
also involves rerunning the training
algorithm from scratch and this is also
computationally quite expensive and I
hope that in this talk I will be able to
show you that we will be able to tackle
these kinds of issues and before I go on
to explain our technique let me also
introduce some very brief notation so we
are going to look at machine learning
tasks which are these triples where
gamma is a training set a delta is a
test set and is a ml classification
algorithm so the training set consists
of these pairs of vectors from RN and
labels from set minus 1 plus 1 and the
test set is similarly defined and
generally develop developing a machine
learning task consists of two phases as
shown over here the training phase where
the training says gamma is given to the
learner or the training algorithm whose
output is a classifier H and this
classifier is evaluated in the
evaluation phase over the test set and
the performance of the classifier is
estimated based on how well it does over
the test set so in our setting we are
given a machine learning task and the
user of the consumer of the machine
learning task is unhappy with some
outcome Phi of H where Phi is some
predicate over they learn to classifier
H for example it could be Miss
classification over a test point x t and
x sub T or more generally it could be a
predicate that basically bounds the
number of Miss classifications of the
test set and for each point in the
training set gamma what we would like to
measure is the possibility of it being a
root cause for the unhappiness of the
violation right and the question is what
is this measure of the root cause and
for this we borrow this probability of
causation introduced by introduced to
formally should buy Judea pearl and let
me try to briefly describe what this is
in this slide so for each training label
why I we have a corresponding random
variable capital y sub I and the output
variable X is
some function of these random variables
y1 through yn and this is the structural
equation and let us assume a world or
the situation where the capital wires
given values equal to small vice soup
eyes where the output is X capital X
equal small X so this is a world and
what we like to ask is whether a
particular label why so pi is
responsible or the cause for the output
X having this value X small X and for
this pearls defines this measure which
is called the probability of sufficiency
for a label bicep I causing this output
X equal small X and let me try to that's
even by this expression over here and
let me try to illustrate this with the
help of an example so so what you do is
that you consider a world where the
label for the random variable Y sub I
has a value different from the current
word which is minus bicep I and also the
output variable has a value which is
different from the current world which
is x equals minus small X and flipping
the value of the label why I results in
two situations a world where the value
of capital X is minus small X or the
value of capital X is X we like to ask
the question what is the probability
that upon flipping the label Y sub I you
go into a world where x value of x is
restored to is restored to the value X
which is given by the dashed region over
here and this is and this probability is
what we are going to use as our scoring
metric and I will connect this notion to
the notion of debugging in the next few
slides and our vehicle for implementing
the scoring system is that of
probabilistic programming so in the next
slide let me quickly introduce you to
what I mean by a prob listing program so
prob listing program is a regular
program with two additional constructs
the first is a probe list assignment
statement as the one shown over here C 1
equals Bernoulli point 5 which samples a
value from a Bernoulli distribution
which is fair and the second is an
observed statement that conditions
values of variables
so the observed statement over here
blocks all runs of the program that do
not satisfy the predicate specified by
it so in this case the only valid the
the value c1 equals false c2 equals
false is an invalid run another program
and the meaning of the program is the
distribution over the expression
returned by it so in this case the
meaning of this program is this
distribution shown over here where c1
equals false and c2 equals false is not
allowed so as far as going back to the
probability of sufficiency it turns out
that you can actually encode the the
pearls formulation is this problem stick
program and let me go through this
program over here so we'd like to
basically fix our attention to a
particular label Y and ask the question
how responsible that y is for a
dissatisfaction or violation of the
predicate that is so earlier which is
Phi right and what this program is
saying is that the way to do that is to
sample a set of labels from some
distribution and that's given by this
first statement with the condition that
the ight label has a value that is
different from the current label and
that's exactly the situation that you
saw in the blue square in the earlier
slide and then the line number 3
basically says with this training set
retrain and get a classifier and insist
that the class learned classifier does
not have a violation on the test point
and that's given by the observe not Phi
of h and then what we like to do is that
we would like to basically do this thing
which is an intervention which set the
label of the of the I training point to
the current label and line number 63
trains the PPP the classifier on the new
training set and we would like to ask
the question well upon resetting the
label of why I to the correct value what
is the probability that you get the
violation so this is basically just an
imperative way of encoding pearls motion
of probability of sufficiency and well
if you do influence for this program and
learn the posterior distribution of five
of H prime then that's exactly the score
that you want to compute
but of course this is easier said than
done in the sense that there are lots of
challenges with respect to doing
inference over this program the first
challenge is that well if you want a
sample labels from this prior
distribution need to marginalize over a
really exponential space and furthermore
we have one such program for every label
in your training set and that can also
be quite expensive and the third
roadblock is that inference would have
you need to perform influence over this
function call a which is actually the
implementation of the training algorithm
in the library so you have to you so in
fact a is not some nice closed form
expression but it's actually code that
you wrote in your machine learning
library so let in the next couple of
slides let me briefly say how we tackle
each of these problems so the first
assumption that we make is that that
that there is a very small fraction of
the training set that is actually
corrupted so essentially what we do is
that when we sample from the spier
distribution we make sure that there is
a heavy bias towards the existing
training set we are assuming that bugs
are rare and therefore the training set
is by and large correct and small
perturbations to the training set are
likely to help you fix the error and the
second one is is something which is
particular to programs the fact that we
write up this probability of sufficiency
expression as a program allows us to
perform various transformations in a
semantic preserving manner in particular
we're able to transform the the program
that you saw in the earlier slide this
program over here over which you can do
really efficient inference so in
particular we split up this program into
a first part which is one which is
statements one two and three which is a
part which is not dependent on I so
essentially in that part is common to
all training points so if you look at
let us say n training points you have
one program psi for every training point
it turns out that you can share
statements 1 2 &amp;amp; 3 across all those
programs and then run statement number 4
in parallel which which helps scale the
inference and I'm also assuming here
that the inference is quite naive where
we actually do some kind of rejection
sampling so you're on the program
multiple times and further further for
the part about
doing influence over the actual
implementation of the machine learning
library what we do is that we rebuild
simpler models of the of the algorithm
so we actually look at the code manually
build models and also simplify these
models by looking at runtime values that
we profile while running the machine
learning library and and these have to
be done carefully so that certain roles
robustness assumptions are met so in
particular what we have done is that we
have designed these gray box models
which are well combination of white box
which is done by way of looking at
static information and dynamic
information as well and we have done
this for both logistic regression in
boosted decision trees so we evaluated
our implementation which is quite
preliminary over number of benchmarks
and we have initial promising results so
here is the experimental framework so
what we've done is that we've taken
these benchmarks look at the training
set added noise and I will say what this
what I mean by this exactly in a bit and
then looked at the ability of our tool
to recover that noise that's metric
number one and metric number two is as a
result of identifying the noise and
flipping the labels we also measured the
impact on the validation error right so
so let me let me focus my attention to
the data set which is sentiment which is
the IM TV review set for sentiment
analysis and there were two thousand
reviews classified as positive or
negative the features were the word
counts and what we did in order to
introduce noise is to pick a word that
appears in ten percent of the reviews
and set all the reviews that have that
word to be negative reviews so that
that's an example of systematically
introducing noise and and and we ran the
our tool both for logistic regression as
well as decision trees and as far as
accuracy is concerned in for logistic
regression we were able to recover
around fifty eight percent of the noise
and for decision trees were able to
fully recover the rise and what the next
column shows is that follows is take
regression the validation error without
the noise was fourteen percent and upon
introducing the noise the the validation
error was
four percent and upon running our tool
identifying the noise and flipping the
labels we reduce the noise back to point
one nine percent and the same is true
with the decision tree column as well
and the time taken to do all of this is
not too bad it's around two minutes so
and and these two graphs basically show
that so because graph to the left
basically shows that as we increase the
number of root causes so essentially
what we deem as root cause there's a
threshold through the score so for
example if the score is point three two
five seven then that's what the
validation error looks like and so on
and so forth and what is interesting is
that after some point when we will keep
lowering the threshold you know the
results get worse which makes sense
because the probability of sufficiency
for those points is not high enough to
be deemed as a root cause and that graph
over there shows how the validation
error how the proportion of errors
recovered improves as we give more and
more feedback so in terms of ongoing
work we are looking at you know more
implementations like SVM zero networks
and one disadvantage of course with this
approach is that with every new
algorithm or every new implementation of
an algorithm we are required to look at
the code and write the models ourselves
so it would be interesting to come up
with a more general approach to sort of
automatically coming up these models and
we also looked at the barely scratch
scratch the surface as far as debugging
is concerned as you all know that this
this is a wider class of root causes to
be explored here like feature values
incorrect feature values over and under
fitting of data incorrect parameters and
so on and we hope to look at these
issues as well and currently there are
two scales data sets with thousands of
points and you know we need to do more
work in order to get our tool to work
over larger data sets through that and
swap thank you
any questions the core problem you're
trying to address is this work at me so
far is label noise yes right but why not
just build label noise models on top of
the existing models that you have and
then you can infer from the noisy
labeled data the inferred ground truth
label so it's sort of a modification at
the end of each of these models that
seems to be a method that a lot of
people have tried before I said you
looked at the nore has not having been
I'd be happy to sort of take the
references from you yeah having any
publicity classification right he's
actually doing that basically because at
the end is sort of going to assume some
form of probability distribution over
the observed label which incorporates
the forward noise and you could robust
robust apply that to allow for noise is
very far away from the class boundaries
that's right that's right so in fact the
the reason we started looking at these
label error so to speak was to see if we
could at the very least debug these
kinds of things and we also have done
have preliminary results for looking at
root causes which correspond to
incorrect computation of each of values
for example which is you know which
requires a different kind of problems
take model any more questions yes trying
to correct some of the labels right that
you think we're incorrectly labels um so
putting that to an extreme because it
because it's going to look at your your
learning algorithm your classifier and
then see which labels are the most
inconsistent with it I think that to an
extreme what prevents your classifier
from just saying I'm just going to label
everything positive and then correct all
the labels to be positive which is like
a form of everything like obviously
that's one extreme how do you know where
you are so the one use case the baby
expect that will be used is that
they're not going to automatically fix
the labels essentially what we are going
to do is to show these labels to the
user saying that these are the
suspicious points and the user has to be
as a geek Savin them and then correctly
alright so thank you a DJ again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>