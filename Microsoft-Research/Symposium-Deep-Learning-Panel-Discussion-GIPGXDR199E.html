<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Symposium: Deep Learning Panel Discussion | Coder Coacher - Coaching Coders</title><meta content="Symposium: Deep Learning Panel Discussion - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Symposium: Deep Learning Panel Discussion</b></h2><h5 class="post__date">2016-06-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GIPGXDR199E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
um I i'm going to maybe start things
rolling we had some questions earlier
this morning about the control questions
and we could have deferred to this this
later this evening what do you see if
there's some of the challenges for the
kinds of approaches that you've been
looking at and especially the thing that
I'm worried about is the number of
training samples you can get or the
robot is always going to be very small
so I think there are a couple of
questions in there I think one of them
is that the exploration right now is
very very slow still I think so being
more sample efficient most likely would
require different representations for
the action space something hierarchal as
opposed to what's typically being used
right now which is how r Kell on the
input but not Oracle on the action space
another related kind of line of work
that I think will be very relevant is
the work on domain adaptation where
envision there's a lot of work already
on you learn in one domain you make it
applicable to another domain and so the
question is key do similar things for
control where simulation is much easier
to come by than real experiments and so
can you learn to adapt quickly beyond
just fine-tuning but have a strategy for
adaptation that could be used to learn
more quickly on a real system
so okay so my question is for Peter so
in the last talked one King Regent was
the multiscale approached the fact that
in that case you know you start from a
very low resolution thing and then
slowly you you get a detests do you
think it's possible to do something like
that for reinforcement learning for your
poor deployment that you your technique
were you know to use something
hierarchical where you look at different
time scales so yes I think I think the
last presentation was in some sense kind
of sketchy an idea that at the abstract
level i think is absolutely applicable
to action except that it's maybe not
immediately clear how to make it into
real actions as opposed to out putting
images but so i think if there's a way
to up with abstract actions which would
be likely sit in memory in some sense
which in turn with then output the low
level and get lower level actions i
think if somebody can make progress on
that will probably require more than one
paper but if it could be a series of
papers making progress on that i think
that would go a long way in in advancing
the ability to learn more quickly for
action we have a question over there hi
there thanks for your talks to what
extent are deep methods state-of-the-art
across NLP across NLP well they're very
popular in NLP and there are clearly
state of the art in language modeling
and now they are state of the art in
machine translation they've been shown
to be said at the art on various tagging
tasks maybe you have other thanks to
head i think a relative relevant data
point is that almost all of the papers
are published in the most recent NOP
conferences use deep learning in some
way
okay thank you a high have a kind of
technical question there are a lot of
frameworks for deep learning commutation
now torch Tiana and Winnie others and it
makes some kind of mess for searches and
developers did you consider creating one
framework that will unite all these i
well understand it it requires
collaboration so first of all it's good
to have diversity because different
frameworks have different advantages
some were meant to do more for research
and more flexible and others allow you
to get results quickly on specific
architectures for example we see some
convergence towards using automatic
differentiation or symbolic
differentiation so I think that this is
this is an interesting direction and I
think it's good to have diversity but
we'll see maybe tensorflow will take
over so you know to diversity is good
but I see that the number frameworks
grows like I don't know exponentially so
maybe it's just not the time yet to
cooperate but I'm some day but I mean
would you do about it I'm you can't say
you don't write in this framework right
so unfortunate but my friend does okay
thank you
yeah this is I think one of the
questions that was on the list on the
website I thought it was a great one can
you speak closer to the microphone sorry
hello I think you can take it out of the
aisle just sounds like one of the great
questions that i saw a listed on the
website is what happens to unsupervised
learning now there was this brilliant
talk here that probably not too many of
us really understood but it sounded
great but other than that there was very
little about unsupervised learning any
thoughts I think as provides learning is
very challenging and it's back as a
really exciting topic I think it's been
left a little bit for a few years but
what's exciting is that they're very
different approaches that people are
looking at completely different training
objectives and ways of thinking about
the problem it's kind of wild west right
now how should we do is provide learning
there's so many possible ways you know
every year or we get several new ideas
and how to handle that so
actually can I respond to that yeah so
so with respect to language modeling
being unsupervised learning there is
really something different about images
and language say because language was
meant to be understood so it was it was
designed I mean that there really is
supervision and so there's a really huge
difference between like you know you
want to understand the density of all
image patches of size 32 by 32 and I
want to understand which we're just
going to follow the next because and the
problem with trying to model the density
of image patches or whatever it is it
just hasn't been particularly successful
and there is lots of weeks supervision
so it's sometimes hard to justify a
fully unsupervised you know approach
when other things are working really
quite well
so is another big difference is that
language is a natural sequential
structure where the joint probability
can be decomposed into the product of
the next word given the previous ones
and it's kind of a natural task if you
try to do the same thing with pixels it
works for EM nests actually this gold
made but it doesn't scale that well
probably because you really want to take
higher level decisions about the content
of the image and that's a more natural
thing there so the nature of the data as
Arthur saying is really different but
there is a sequential structure and
images it's just coarse to fine coarse
to fine but we don't observe well I
guess that's what the laplacian thing is
doing I think like many of us appear and
in the room I'm very excited about
unsupervised learning these days I got
started into it by the recent trend in
method of moments based learning in
machine learning community but i think
the what we saw in last year and a half
in terms of recent advances in learning
variational auto encoders and the like
have opened up a wide range of doors for
foreign supervised learning and an
incredible number of questions so
they're going to a couple of workshops
at Memphis where I hope we're going to
get into the nitty gritty details of
these but I think almost nothing is
known now conditioned on the recent
advances I hope every one of you in the
room starts working on supervised
learning
yeah I I mean it somehow it depends on
what you count as unsupervised learning
so in general you know prediction if
you're just predicting the next Fame in
a video or if you're predicting the next
frame of audio or if you're doing text
prediction to me these are all
unsupervised I mean the distinction for
me is whether or not a human had to come
in and give labels that help you solve
the problem and maybe this is more like
a pragmatic thing because it if it's
unsupervised in that sense it means you
essentially have limitless data but that
I mean that doesn't answer I mean most
of the prediction models you you're
mostly just minimizing log loss so it's
it's there's a sort of there's a
standard loss function there which isn't
the only loss function you can consider
for super unsupervised learning so I
think I feel like going to verizon the
big question is always like you know
what is it you're actually trying to
optimize and for me personally as soon
as you get away from optimizing just you
know reproducing the data becomes very
murky
did you want to see something well I one
thing that made me particularly exciting
about the supervised learning even
though I worked for 10 years on
unsupervised representation learning was
the fact that the networks that have
been trained on imagenet actually
generalize to new tasks to other data
sets and that's one of the motivations
where you might want to study
unsupervised learning because you want
to avoid overfitting on a particular
data set and rather want to learn
generally used for representations and
so I think what would be particularly
exciting to see us say that we test
unsupervised learning models well see me
supervise Lee learn models on imagenet
and see them transferring to if they
succeed only imagine it they are likely
to transfer also to other data sets as
we have seen in the past and so a new
benchmark would be that we want to
achieve we fix the performance we want
to achieve say less than ten percent
error on image net but with less and
less labour data that could be one way
to approach unsupervised learn there
many other types of data sets other than
languages or or vision we're on to
propose learning is particular
interesting so for example we do a lot
of work with healthcare data where you
might just have very noisy signals but
what's gone with the patient coming from
it's like billing codes or from
laboratory test results and one wants to
try to learn try to discover new
subtypes of the disease try to discover
a hidden trajectory through which
patients as hidden state clinical status
is evolving over time and deep learning
can can tremendously contribute to to
these unsupervised learning in
completely new areas
let's go to the next question I had a
question it's basically targeted at
Peter I mean one of the hopes is
unsupervised learning would be useful
for RL but you mentioned that you tried
doing this went by a bit quick but you
did some sort of unsupervised
pre-training and it wasn't helping and
then you took a pre-trained eqn it still
didn't help can you elaborate yes that
went a little quickly but in a little
more detail what we were hoping was a
training essentially dynamical systems
models but in pixel space in this case
or in an encoding space thanks to some
auto encoders or something like that
would result in representations that
helped learn a Q function more quickly
because you would hope that the things
you need to extract from the image to
predict the next frame would maybe be
the same things that you need to extract
to predict what action will be a good
action and then my there might be true
but it wasn't helping to use to train in
parallel on prediction and a Q training
a DQ n network and so then at some point
we decided to say well we don't really
know even if we can hope this would work
even if we had a really great way of
learning to predict frames and so then
we said let's let's take it to the
extreme let's pre train dqm a DQ n
network on a bunch of different games of
course separate networks until they're
fully trained once they're fully trained
zapped out the last layer the weights
there re randomized the way it's there
and just train that layer but going
feeding through everything that has been
so to say perfectly trained the head of
time and the learning curves that I was
showing was comparing standard DQ n
where you don't have anything in the
initial layers but random with where you
just retrain the last one retrain the
last two or not just retrain but you
also find you in the earlier one so
that's why there were four curves that
were our experiment and then one curve
well their average over multiple runs
but one curve that's DQ n and so the
finding there was at least in our
experiments that
initializing with what ultimately needs
to sit there didn't speed up the
training at all now what to conclude
from that is I'm saying clear but it
definitely does conclude that focusing
on improving our learning of video
prediction to share that learning with Q
networked learning was not what we
wanted to focus on we wanted to think
about like better understand what's
going on there and why even that is not
helping so this was for the same game
right so this is carty ways are if I
trained on space invaders and I keep
everything fixed except the last level
the last level is a linear function
approximator so you can use sort of
off-the-shelf good yes sir so likely at
least part of what's happening I mean is
that the initial Q values are pretty far
oven it's a self supervisory algorithm
so it has has some trouble so it might
be a matter of revising the key learning
algorithm to account for the fact that
it is self supervisory and maybe once
that's revised it could learn more
quickly that way but kind of using it
the standard DQN way wouldn't have it
learn more quickly that because you're
still doing sort of the sqd objective on
even on the final layer you didn't use a
more classical no we used mused exact
same objectives that we would have used
otherwise if we were training everything
which is the bellman error objective
okay okay mark my question relates to
testing generative models the way I see
we have on one end of the spectrum
testing maximum at the likelihood on the
generated on the generative distribution
but we see algorithms like nice that
generate that achieve very very good
likelihood but the samples don't look
very well and on the other hand we see
generative adversarial networks which
create insanely good samples but might
be massively overfitting and just
copying the training set do you have an
idea of where a very good quantitative
measure of testing generative models
might arise from yeah so let me first
start with the premise
because actually one of the points in
Emily's talk and you can also see it in
sumits most recent paper is if you take
paths through the z space and the again
everything is different if you she had
the one slide where you kind of added a
little bit of noise and do the thing and
you kind of look at the images there
there it is completely clear that you're
not memorizing neighbors and actually
for example in sumits paper what he did
was you make a single pass to the data
set with very small gradients so one
image can't possibly um you know affect
the final outcome that much I am sorry
so so it's quite likely that began is
giving up on a huge part of the space
but it isn't it isn't memorizing and the
second thing so but more to answer the
general question of how should you
evaluate these things and sometimes I
think this is one of the real problems
with unsupervised learning by itself you
should have another task that you you do
it i mean what we should have is a huge
Bank of tasks as he was saying with
important unsupervised learning is good
when it lets you do the thing that you
want to do you shouldn't I think
log-likelihood is actually even when you
say oh there's some models that give
very good local likelihood um love
likelihood isn't a isn't a canonical
thing and that it depends on the
representation of the input you can
change the representation of the input
and which models give better log
likelihood gets completely flipped
around so it there isn't any the problem
with unsupervised learning is there is
by definition not well by some
definition of unsupervised learning
there is there's not a good way of
measuring it you have to have other
tasks that used to measure it you have
to have outside signal that says this
thing was good for helping me solve this
other task otherwise you're going to be
con tomorrow's one thing I agree on what
you say and the experiments are awesome
but that's a why I said quantitative
measure of how to measure for example
the fact that adversarial Nets cover the
whole distribution of venture training
on
maybe also one more comments we just
published a note on archive where we
also show that the general belief that
the samples the quality the perceptual
quality of the samples correspond to
high likelihood that is not true right
so then just have to be aware that good
samples can come with a bed like yacht
or vice versa I found that within a
single model there's usually a good
correlation between the log likelihood
and the quality of example it just
doesn't transfer between models that
does Eric yeah I'm grieve it's fair if
one thing i want to add is you can you
can think of the log like he was just
measuring the KL divergence between the
distribution of the data generating
extra bution as the reference and the
district of the model but there's
another KL divergence which is the other
way around which would also be minimized
if they were equal and that actually
would behave differently than then
minimizing the in particularly what it
would lend itself to picking some of the
modes and not necessarily paying too
much attention to all of the modes this
is more like what ganda and actually in
the Gann paper we showed that what what
is being approximately optimized is a
something like a mixture between the two
formal decay the Verge's that explains
why we see sharper images and we would
also expect that normal log likelihood
if we were able to measure it to be not
so great
so can you just come to a little bit
further so if you take the view again in
terms of being something very close to
moment matching where is that you come
from which you you mean the kale thing
yeah but that came from a sort of game
theoretical analysis asymptotic analysis
that that relates the objective function
to this Shannon divergence that that is
a mixture of the two essentially roughly
a mixture of the two divergences do you
think that using that inverse KL to test
generative model we can't measure it
because we don't have the data turn
against erosion and it's approving
inside the log right so we can't use the
data for so the the Gann objective
function is meant to be a proxy for for
that maybe we should go to the next
question hi um so you were mentioning
earlier front supervised learning that
one of the reasons that maybe one of the
reasons it works well for natural
language is that language is meant to be
understood or like kind of by definition
has a structure to it that we should be
able to find um is there another reason
is that we have a lot more data in
language than and images at least as far
as how many we actually use right now
right we have a lot of I mean well you
know a lot of maybe even it also takes a
lot of compute power to use all these
images a lot sorry go ahead so well so
maybe that partly inches but I was
wondering if um that means that like
unsupervised techniques would be well
suited towards like music tasks music
use in music yeah cuz I don't see a lot
of work here using deep learning on
music and I'm wondering if there's all
the work on using LSD m's for example
for generating music from Douglas ech
like I don't know eight years ago or
even more there's been a few papers in
that thread but probably you know it's
going to come back now that we current
nets are really hot
there's actually the problem with
generating audio is kind of hard to
represent audio right that was symbolic
yeah so if you using MIDI or something
like that is easy but if you if you want
to represent you know natural speech the
same way that we go straight to natural
images it's not so easy because you have
to model phase you know or you have to
find some representation that the
network and naturally represent it seems
to me did that once that problem is
solved and we'll start doing a lot more
audio generation thank you hello I have
a non technical question that I'm
curious about there's been a lot of very
impressive work over the last few years
revision tasks you know coming from a
different universities things like image
recognition and seem labeling so the
thing I'm curious about is um so who is
funding this research and why FBI I mean
facebook facebook that's fun some of
this and I mean they found me to do it
Google clearly fund some of that I mean
NSF funds all the usual suspects like
what are the problems that are funding
you to solve ah sorry who like what are
they funding you for it what are the
problems are funding to get an extra few
percent on image that like why I'm not
really cure I'm just not really sure why
they're funding that um you know there's
also funding for basic research actually
exists
okay I mean I guess I'm just curious
what's the end goal but fair enough but
maybe we should ask some of our computer
vision friends what they think about
this I mean faculty you are getting
grants and thought and so on no
alright it's next question a question
for Alex so this general problem of
learning an algorithm seems really
really hard because the space of simple
algorithms is just ridiculously huge and
so I was somewhat surprised that your
results were able to generalize and so
there must be something wrong with this
intuition that the space dog rhythm is
just too huge and is hopeless to like do
search on it so I was hoping you could
clarify like why this intuition is wrong
and why you're able to get some results
in this area I think to put it simply it
works because they're simple algorithms
so you know the space basically the
space becomes huge because it's
combinatorial because you can just lots
of the sort of small algorithmic
primitives that you can combine in
different ways and I think if you if you
try to learn a complex algorithm in one
shot that involves lots of different
primitives is it's guaranteed to fail
for that exact reason I think any any
serious approach I mean one thing i
should say is it wasn't our our
long-term goal to learn more and more
complicated algorithms with a neural
network after all these are these are
algorithms that are pretty
straightforward to code it was more to
have a neural network that could sort of
think reason algorithmically and apply
that to real life situations where you
maybe don't need such complicated
algorithms but if you do want to get
complicated algorithms I think it's
clear that you need something that
builds much step-by-step and transfers
them or is able to store them in some
way and reuse them so humans are able to
learn I mean to learn to to reason
algorithmically and it's probably not
something that we were explicitly
evolved for it's only recently restarted
to program so it's probably a side
effect of the sort of normal kind of
cognitive work that we do that turns out
to be useful for for these tasks and
humans aren't that great at doing it but
you know it gives us hope that you'll
Nets will be able to do it with maybe a
bit more tricks and computing power and
and so on I mean humans are quite poor
algorithmic reasoning if they don't have
you know paper to write things down or
computers the checker game is of course
and even if they do go from here
so it's it's clearly not something that
comes very naturally to us Thank You
enforcement
all right in RL sometimes it's easy to
learn polish than the Q functions I
wonder how that extends to the deep
learning setting and whether it's easy
to learn let's see deep key functions or
deep bonuses sorry energetically repite
yet so I mean reinforcement learning
sometimes it's easy to learn and policy
directly then let's say the Q function
itself and I wonder in the deep learning
setting when you do the parallel if we
could elaborate on this if you have
tried to learn let's say on deep policy
or a typical value function sure okay
first first a very simple kind of
intuition why sometimes learning the
policy could be more convenient and
learning Q function is because
ultimately you want a policy you want a
prescription or often that's what you
want you want to go from state to action
let's say your action space is discrete
there are a few options there right then
if you learn a policy you're focused on
discriminating between those actions and
deciding which one is the better action
between the set of choices whereas if
you try to fit a Q function you're
actually largely applying most of your
fitting power to the overall value of
each state action pair but you're not
really zoning in so much on the
difference between different actions in
a given state and so you might have a
really good Q function fit over all that
has small relatively small errors of the
say overall but that actually has
flipped values for optimal vs suboptimal
action but yet could still be somewhat
close and so you're kind of not until
your cue you're a bellman error
essentially becomes zero until that
happens you might have to still be
pretty far away from picking the right
action in some situations in terms of
why I think in terms of why we let's say
I mean all of the deep mind results were
at least initially were with Q learning
and I think in my group all the results
we had initially were with policy search
and I think one of the reasons was that
we just felt that the kind of
research effort of working with policy
search was easier it was easier to
understand that the policy is making
progress than it was to understand
whether the Q function was a better Q
function than the one we had before and
so I think it was it wasn't so much that
necessarily the policy representation
was better than acupuncture person it
was more than us looking at what we're
getting it was easier to reason about
what we get when we have a fixed policy
versus a fixed Q function and when we
change it how good that changes there's
another advantage to learning policies
which is it generalizes to continuous
action spaces in a more straightforward
way thank
hi again how kind of general question
your opinion what will be the next big
thing in learning he / parameters for
deep neural networks this should be the
last two questions by the way it's
almost nine
what happened writers learning hyper
parameters yeah what were the next what
is the future of this like Paul study be
on bayesian optimization deep Beijing
optimization
thank you I mean some something I would
say is that most of the work in this
area has been focusing on learning I
mean learning for one specific setup the
right exploring where experiments are
for like one neural net where you
explore different type of parameters
it's you only get a few you know tens or
hundreds of trials at most otherwise you
might as well do random sampling so you
can't gain a lot because you dont have
space to learn a complicated policy
essentially but if you if you are able
to learn like grad students do through
many architectures many data sets and
generalize across these then you could
have tens of thousands of points but you
know this this architecture for this
data set with these separate parameters
give me this result then yeah I think
you have something more powerful but if
gaussian processes there wouldn't work
simply for computational reasons you
have to use something else well maybe if
you push that a little bit further and
think about extrapolating right so
suppose you write down an architecture
and torture piano and you know 100
layers deep could you hit a black box
can you hit can you hit a button and
then come up with a way of trying to
extract out a much smaller model family
automatically from that and to the type
of experimentation that Yoshi was
talking about and then learn how to
generalize back to the one you actually
cared about that might be fun to explore
I just wanted to refer to in good
fellows I can't remember the first
author I apologize good filled with many
authors though it's SEL our submission
the dome in the new um ways it about
that but but basically it's basically
what you guys are talking about um so
it's how to take a net and grow it a
little bit by replacing replacing a unit
with two units that are you know give
the same output and then having a little
bit noise or adding an extra layer on
under the name um but it really is a
powerful technique for doing what what
you guys were saying you can't get back
down but you can go back up and it is
you can start simple and grow bigger and
bigger and bigger and that's super
powerful because effectively what you're
doing is taking out the optimization
surface and blurring it you're doing a
multi-skilled you know over the
optimization surface and the bigger you
make the model the finer of the details
of the optimization surface are so as I
kind of method for hyper parameter
search though what they have in that
paper is just it's really i think gonna
be useful last question okay just coming
back to neural turing machines one
challenge with learning algorithms is
that he said I either get it right or
wrong right there is no intermediate
sort of intermediate algorithm that's
correct at least I think so do you have
a sense of how they managed to kind of
build up the building blocks to get like
something like copying to work yeah so
it relates to having a kind of
curriculum again so even even when in
the first paper we didn't have an
explicit curriculum but we always gave
it a range so it was copying sequences
of length 1 to 10 right so a certain
fraction the time all I had to do was
copy a single vector and there the you
know the supervision signals very clear
and then it has to you know from
narrator might start to learn two
vectors or three vectors and then at
some point it learns a rule that
generalizes and actually the rule
doesn't generalize perfectly so like
that slide I showed where it generalized
the length 120 there was actually a
mistake somewhere after I don't know 80
points or something like that it misses
one out and then does all the rest which
means that the log loss was actually
very high so when I first looked at the
loss I thought it was just failing on
the long sequences and was only when I
looked closely and realized that
actually would be you know it would just
have missed one line out or repeated one
line does that make sense so in a sense
is true what you say it had that the
algorithm was either completely right or
it was completely failing quantitatively
but qualitatively I could see that was
almost successful I guess the problem is
though that is you're trying to learn a
procedure I think and so either sort of
have I mean ultimately right and so even
if you learn have to copy those three
letters that doesn't seem like it would
help you
and laying the procedure and unless you
just learn the procedure oh I mean you
know what you thought you'd want is to
have a generic notion of what copying
something means that you can you can
reapply in different situations and my
feeling is that that is possible you
know the all of these things are
ultimately encoded in the weights of the
controller network and we know that
neural networks are quite good at you
know generalizing and even transferring
across task but you know it's not we
certainly haven't we haven't explored
that really you know it's it certainly
would be challenging all right so we are
at the end let's thank again our
panelists what our speakers
thank you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>