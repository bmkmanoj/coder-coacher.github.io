<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS Poster Spotlight Session 9 &amp; Conference Closing | Coder Coacher - Coaching Coders</title><meta content="NIPS Poster Spotlight Session 9 &amp; Conference Closing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS Poster Spotlight Session 9 &amp; Conference Closing</b></h2><h5 class="post__date">2016-06-22</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GRSYHnn-XM0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
so our work is the title metric
completion with noisy siphon mention I'm
cayenne chai and this word is the junior
with Georgia in Georgia Dylan Frankie
Orton so the goal of this word is trying
to exploit the site information for in
metric completion so for example if we
consider a netflix problem where we want
to use the user movie you use their
protection user rating history to make a
prediction on the user movie ratings and
the standard way is to use the metric
completion where we can learn the lower
metrics which capture the observed
ratings and we can also use the complete
images to make a partition but in some
circumstances we also given some
additional information like the real
future like user feature in this case or
the movie feature in this case then we
can utilize these kinds of the
information to make a better prediction
so for example there is a model code imz
which you can incorporate a feature
information into your petition and the
question here is that how much can the
site information help to dementia
completion problem so to highlight our
work we observed at a previous result
for example and see that show that the
prophet features are very useful but for
general kinds of features that actually
can knock out and guarantee the recovery
of the underline metrics so compared to
them we propose a new model which we
leverage the featuring information and
the Opera observation information better
so that we can prove that using a good
quality of the features it is enough for
the to prove the features are useful and
moreover we can guarantee the general
feature can also be used for in our
model so to highlight this idea the
consider that here the yellow yellow bus
here are the mattresses that can be
recovered by the measure completion so
if we are given the site information
life SNY has the features then we can
divide the space here too
sub bases and the previous results shows
that matches like our one can be can be
can can be recovered effectively if we
are given the x and y but for other
measures like r 2 to r 5 actually it
fails to recover so we observed that for
for example here are through to our 5s
my steel column still can consider like
sufficient formation of an underline
metric so we basically handle these
kinds of the instances and to propose a
new model to handle the useful features
so if you are very interested in our
words you can come to our post fifty
five six hello the title of this talk is
fast and guaranteed tensor decomposition
valve sketching a meaning and this is
the John work with value alex and an
email so we consider it as a metric
tensor CP decomposition problem in this
work for an import density of dimension
n the goal is to find K Y value and I
came back two pairs lambda and you I so
that the reconstructed low rank tensor
approximates we are original data center
keygen for being is known so this
problem has white applications in data
mining and more recently in prime to
recovery for late and rebel models one
popular choice was the decomposition
problem is through the 10 to power
iteration method at each iteration we
compute a normalized sensor multi linear
form based on the factor of the last
iteration it can be shown that under
mild conditions this tends the power
iteration procedure will converge to the
true underlying back chair with high
probability however this method is not
obtained using practice because it is
computationally intensive for example if
the original 1030 has n dimensions we
would require n cube operations to
each iteration this is prohibitively
slow for most large-scale data
applications so in this work we propose
an accelerated 10 to power iteration
method based on the idea of sketching so
the general idea is to first precompute
a compressed sketch of the original
tensile team afterwards each test multi
linear form can be approximately
computed in near linear time by using
the trick of fast fourier transform
either without the time complexity of
each 10 to power integration is reduced
from n cube to m plus B log B where B is
the length of the sketch and is
typically much smaller than thank you we
also apply our proposed method on some
real world machine learning applications
such as touch modeling in this problem
we want to recover k late and public
distributions from unlabeled documents
so we compare the negative log
likelihood and running time of our
proposed method which collapsed gibbs
sampling and we show that a hybrid
approach will outperform keep sampling
in terms of both document modeling and
running time so for more information
please come to our posted thank you hi
I'm Archie and this is work with you up
front at UC San Diego we give an
algorithm to tackle the central question
of semi-supervised learning which is how
do you sort of generically and
efficiently improve upon fully
supervised let's say binary
classification using on label beta so
you might do binary classification I
first trying a few algorithms as we am
just in tree deep net using the labeled
information to estimate their errors and
then after that using that information
to decide how to predict maybe you had
picking the best single classifier
taking some sort of vote or so forth to
this we add unlabeled information in the
form of the ensemble predictions on
unlabeled data so we get to pull the
ensemble and what we give here is an
algorithm which when viewed as a black
box takes the labeled information in the
form
these estimated errors and the unlabeled
information in the form of ensemble
predictions and spits out predictions on
the unlabeled data now these predictions
these Y hats have error which is at most
a bad of any single classifier in the
ensemble so this is a sort of safe thing
to do and the paper is mostly empirical
but let me get into what the algorithm
is inside the black box it's an
efficient algorithm that processes the
unlabeled data through stochastic
gradient descent basically so you you
only need to handle one unlabeled point
at a time and it consists of a learning
algorithm and a prediction rule learning
algorithm is a convex optimization which
is this gradient descent as I said you
get from it a non-negative weight vector
over the classifiers that you input the
SVM decision tree deep net etc the
prediction procedure on each unlabeled
test example takes a linear combination
using those learned weights of the
ensemble predictions on that test point
and runs it through a rectified linear
unit to generate the binary prediction
on that test point so you can call it or
neuron with learned weight Sigma the
interesting thing about this is it's
it's actually a lot stronger guarantee
than I said on the previous slide in
fact no predictor at all has a better
worst-case error guarantee given the
ensemble errors and the unlabeled data
and most of our papers is empirical
running this algorithm on random forests
and showing that basically given an
ensemble of the trees in the random
forest we can outperform random forest
pretty consistently and significantly so
for more please come and see our poster
number 38 thank you
hi I'm t kahan from Marcus of research
New York this is joint work with Daniel
sue from Columbia hella Cockerill jon
langford and rob Shapira from minutes or
New York so we consider streaming active
learning in every round the learning
algorithm receives a new example X
central I ID from some source and on the
spot the algorithm has to decide whether
it wants the labor or not if it does it
sends the example to some later gets its
label back otherwise the algorithm does
nothing just skips and proceed to the
next example so the goal is to maximize
classifier accuracy per level query and
the important question is how do we make
the query decision so in this work we
develop a new algorithm active cover
this algorithm uses efficient reduction
to supervised learning so it can be used
with for example SVM's decision trees
deep nets or whatever super light
supervised learning algorithm you like
and the important thing is the query
decision is randomized so on a new
example X the algorithm only queries its
label with a certain probability and our
main contribution is we use a novel
clerk of ability obtained by solving an
optimization problem as a as a result
the curve probability has the following
form so suppose we choose a set of
classifiers H say linear classifiers the
probability is an increasing function of
a waked count of all classifiers H that
disagree with the ERM classifier on X
the ERM classifier is the one that
minimizes the training error on all the
data that the algorithm has seen so far
both the exact functional form and the
weights are learned through optimization
a nice property about this is that more
disagreement on X means a higher chance
of querying its label we showed that
this new algorithm prove only learns an
accurate classifier while making fewer
label queries the null previous
efficient
astic active learning algorithms we
performed any extensive empirical study
on 22 classification datasets for more
details please come to our poster number
61 Thanks my name is Annika Romanesco
I'm from courant institute at nyu and
this is a joint work with jon langford
from microsoft this work focuses on the
multi-class classification problem with
a very large number of classes consider
a simple example of this problem imagine
I give you a bunch of images and I ask
you to recognize faces on this images
this is clearly a multi-class
classification problem in fact with
billions of unique labels most
algorithmic approaches for multi-class
classification have a linear running
time in the label complexity the gain of
using logarithmic rather than they are
running time approach is negligible when
the number of classes is small like 10
or 100 but becomes significant when the
number of classes is much larger so for
instance at million classes the speed up
is of a factor of 50,000 the gain is
huge and cannot be easily ignored them
we attack the multi-class classification
problem by reducing its the binary
classification problem organized in a
tree the data splits in tree nodes are
done by maximizing a new splitting
criterion that was not considered in the
decision tree literature before
maximizing this new objective is
essentially equivalent with maximizing
the dependence between the left or right
splits and the label we judge the
quality of the entire tree using Shannon
entropy we obtained the boosting theorem
that relies on the weak hypothesis
assumptions and shows the number of
splits that is required to reduce tree
entropy below an arbitrary threshold
epsilon this guarantee under favorable
conditions directly implies logarithmic
training and testing running time here
we are comparing the performance of our
algorithm that we call a long string
with the common linear
next time approached one against all on
the Left plot we show the performance of
both algorithms when varying the number
of classes and the classification
problem and we're long tree is
constrained to use the same
representational complexity as one
against all and won against all is
constrained to use the same running time
as long tree by data truncation clearly
as the number of classes increases the
problem becomes harder and our approach
becomes dominant on the right plot we
are showing on the left on the
logarithmic scale the ratio of the fair
example test time for one against all
and the long tree the plot captures
nearly linear dependence and empirically
confirms the logarithmic running time of
our approach so to give you an example
on the full image net data set our
algorithm performs predictions in about
half milliseconds so if you would like
to learn more about this approach please
see poster number 37 thank you for your
attention all right so hi everyone I'm
Brendan and this is joint work with both
the determine in advance and I'm sadly
neither adicional Bob could make it to
nips this year but but don't worry um if
you like the paper I'm more than happy
to take all of the credit if you don't
like the paper and I mean I don't know
why you would sorry it wouldn't that's
what I meant to say oh it was really all
Odysseus fault and Bob and I really
tried hard to stop him so in the normal
theoretical analysis of binary
classification algorithms we normally
make the very strong assumption that the
training set and the test set come from
the same distribution now this is rarely
if ever the case and in the paper we
consider possibly the simplest deviation
away from this well studied model and
that is given by learning with symmetric
label noise so we assume that the
training set has had its labels flipped
with some constant probability and this
is on top of any noise in the underlying
pattern so it just it turns out that
naive approaches to this problem so for
example just pretending there isn't any
noise at all and running say a support
vector machine on the corrupted training
set can lead to substantially sub
optimal performance and in fact this is
true of a large variety of convex
approaches to the problem so this is led
to the folk belief that you need to
resort to a non-convex loss function in
order to avoid the effects of label
noise so what we find is that this isn't
actually true at all and in fact to
avoid the effects of symmetric label
noise you don't need a resort to these
complicated non-convex procedures at all
in fact all you need to do is take the
hinge loss function from support vector
machines unhinge it and use the linear
loss function instead so the linear loss
function is provably robust to the
effects of symmetric label noise and in
fact in a certain technical sense it is
the only convex loss function with this
property so the minimization of the
linear loss function enjoys many of the
same theoretical consistency properties
for learning binary classifiers as
minimizing the hinge loss function and
furthermore for the case of linear
function classes performing this
minimization is remarkably easy so as a
as a sort of a one-line summary of the
paper and with the sincerest of
apologies to wild while the truth is
really pure it can be simple and if any
of this is piqued your interest and
please come see me at the poster and be
more than happy to answer any questions
that you may have right catch everyone
hi my name is nickel and i'll be talking
about collaborative filtering
applications with graph information and
this is joint work with shampoo you
pradeep ravi kumar and energy to learn
from UT austin so consider the standard
matrix completion problem where you have
a partially observed matrix with just a
few entries and the goal is to figure
out what the rest of the entries are but
also assume that there are graphs
defined on both the rows and columns of
the matrix so in the case of
collaborative filtering or recommender
systems this could be like a social
network or among users or like a product
purchasing a co purchasing graph on the
items in the case of large-scale
multi-label learning problems the
Rosewood just correspond to data samples
but then there would be the labels would
correspond to the columns of the matrix
and then they can be a graph on the
labels depending on what labels are
correlated with each other in gene
disease association studies you could
have a gene gene interaction at work or
a comorbidity comorbidity network on the
diseases and so this is a problem that
comes up in a large number of
applications and the questions we want
to answer is how can we efficiently
incorporate this additional graph
information in making predictions in our
scenario and in doing so what is the
price we pay what is the sort of
statistical and computational trade-offs
that you can show that lets us that lets
us solve this problem so this paper we
first show that you can in fact
incorporate this information in a fairly
nice manner we appeal to the notion of
embedding objects and graphs and propose
a nice regularization framework we then
propose a scalable alternating
minimization algorithm which achieves
about one and a half to two orders of
magnitude speed up or stochastic
gradient method so on fairly large data
sets like one the figure that shows one
with about 16 million observations and
then finally we can actually be sure
that we can actually write this as a
simple convex optimization program and
in doing so that lets us prove some nice
statistical consistency guarantees for
the method and then you can see that the
sample complexity is actually lower than
what you would get if you just saw the
standard nuclear norm regularized
program there's also a workshop on time
series where we extend this to
large-scale time series applications
you're free to check that out for more
information comments questions peace
with it a poster or number number 62
thanks
so this has been a an extraordinary nips
a very special year we've had a record
3800 attendees to both the conference
and the workshops and we still have more
to come we want to on behalf of the nips
foundation we want to give thanks to the
general chairs running a big meeting
like this has of many moving parts and
if you don't watch them you know things
can really fly off in funny directions
but Karina Cortes and the Oh Lawrence
did a terrific job last year with the
program and this year with the general
chair let's thank them for their service
now this year's a record number of
submissions and the extraordinary number
of reviews over 10,000 reviews is sets
of sets the bar very high so of Danley
in musashi sugiyama thank you thank you
so much without you we wouldn't have had
such a great beating thank you
now the the conference hasn't finished
yet in fact we have a poster session
coming up and then as you know there's
going to be symposia which is a new
event that is occurring you this year
this afternoon starting at three but we
do want to make an announcement for the
next nips meeting which will take place
next year in Europe Barcelona and we're
very pleased that our program chair is
for next year Isabelle Guillaume and
Ulrika fun Lux Berg have agreed to take
on that task thank you so much they have
a few words to tell us about next year
yeah so we're really excited and also
bit scared of taking on this job I just
want to make two announcement at this
time the submission deadline is going to
be earlier than this year it's going to
move like all the decision procedures
are going to move two weeks in advance
so be prepared to have your paper ready
and may 20 and we also try something new
and we're all authors who submit a paper
are requested to also serve as reviewers
so we will have no no reviews but we
will also request authors of papers to
regular paper so if you want to submit a
paper please be prepared to also be a
regular for our conference and all the
other changes we will decide on during
the next year and you will learn them on
the fly thank you we hope to see you all
in Barcelona and thank you for coming so
numerous tell your friends to submit
more papers we're all ready to take the
big load
the way in case you were wondering we're
not going to let you review your own
paper okay so this is really exciting
and I'm really pleased to say that we're
going to continue on this is going in
new directions that I would have never
even imagined even ten years ago so
thanks to all the wonderful talks and
thanks for all the posters it's been a
wonderful year thank you
each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>