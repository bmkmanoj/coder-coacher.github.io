<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MSR-INRIA Workshop On Computer Vision and Machine Learning | Coder Coacher - Coaching Coders</title><meta content="MSR-INRIA Workshop On Computer Vision and Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MSR-INRIA Workshop On Computer Vision and Machine Learning</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7IxEIEQOkyw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year microsoft research helps
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
okay so good morning everyone my name is
Adrian a PhD student at the Microsoft
Research Analyst center and also in the
Lear team led by cada dia and I'm going
to present you to recent papers about
learning temporal information for action
recognition videos it's all joint work
with as I shall we and Cordelia Schmidt
there's going to be two parts the first
part is about acting sequence models for
efficient action detection so what's the
problem the problem here is you want to
find if and when an action is performed
in a video so we're interested in short
human actions like sitting down so
basically a few few seconds and we want
to detect them in a long and segmented
stream of frames so long and cemented
video sequences like several hours of
movie data for instance and so what
we're doing exactly is called temporal
detection that is fine short clips that
contain the action and the approach we
propose is to model actions as sequences
of action items that I called Acton's in
the paper so what are Acton's there are
atomic action units put it in another
way there are actions specific short key
events whose sequence is characteristic
of the action so here for instance you
have an example of three active
annotations of 2x annotations one for
the action opening door from the forest
gump movie and another for sitting down
so by action specific I mean that are
semantically meaningful so we obtains
annotations at training time only from
annotators and so there are sub events
that make sense for four people and so
they are specific to the action right
there's not much meaning shared between
opening door and sitting down here and
also what's important is there a
sequential aspect right sits there
sequence that is characteristic if you
just look at the middle one it's unclear
what first gump is doing and if you look
at the other way around it seems like
he's closing the door instead of opening
the door and that's really important
when you do detection because you have a
chance level of 0.001 percent so this
kind of confusion that happened all the
time so you need the sequential aspect
and so what we do is we propose simple
model that's called this active sequence
model so we model each act on using an
extension of bag of features so we
propose to use an Instagram of time
weighted quantized spatial temporal
interest points so basically we just
have like local motion information and
we make all each points votes for a
particular act on or sub event and its
importance for these sub events is
weighted by how far it is from the focal
point of these sub events okay so then
we can just concatenate the per act on
histograms and we get this one long
sparse sequentially structured model
that encodes soft ordering constraints
between actions so I'm going to go a bit
fast for for this first part so i won't
go into the details of the parameters
ation of this this model but basically
now when you have built this model of
actions you can learn a classifier right
so you take your favorite classifier
mine is SVM basically what matters is
that if classifier it outputs a posture
probability of an action knowing its
Acton's so knowing its temporal
structure okay but we're not done yet
because that test time we don't know the
atoms we don't know the temporal
structure we just have this unstructured
streamer frame so what you propose to do
is to learn to use the training items to
learn acting candidates right and we fit
a simple non parametric model basically
to learn this temporal structure
distribution and what we do is we use
this distribution over interaction
spacings as a prior on temporal
structure okay and we're going to i'm
going to show how we use this prior for
detection but just to give you a hint of
what is this temporal distribution that
we learn this is for the smoking action
you see here this discrete distribution
that we learn ordered by probability so
this is the estimated prior probability
and if you look at the 5th most likely
action it has two thumbs which are quite
short and quite close to each other and
the third one a third part of the action
which is flatter and further away and
basically this this captures a way of
forming the smoking action in movies
where people smoke with style so they
bring the cigarettes to the lips smoke
and blow slowly away so this is one of
the styles that we capture basically
okay so basically now you have two
things you can compute the posture
probability of an action knowing its
temporal structure and yeah this prior
so for detection we estimate the
probability of an action being centered
at a particular frame by just
marginalizing over this prior okay so
very simple but what this allows you to
do is to do a sliding central frame
detection approach so it's like sliding
window but you don't have windows you
just evaluate this probability every end
frame and you don't have to choose
multiple scales or have multi scale
sampling heuristics like people do for
siding windows they are captured by your
prior and so in our in our conclusions
of using this prior helps a lot and you
can use also use it for siding windows
so if you're doing sliding window
instead of having multiple scales and
then doing on my xmas repression like
usual learn a prior over your scales and
marginalize its robust defies a lot the
performance so i can show you one quick
example what can you do when you can
detect actions you can do an action
summary so this is the coffee and
cigarettes movie and it's going to play
in a fast forward except when the system
automatically detects a drinking action
where it's going to be played in slow
motion so it's too small for you to see
but here you have like the probability
score also on top and so basically this
is an automatically generated movie
summary that's oriented toward one
particular action regressions
you can use logistically shins totally
that's something that a lot of people
say that's exactly true you can the
thing is that logistic regression is a
bit similar to to SVM in the sense that
both use so large stick regression is
not a probabilistic model it's all its
mixer it just uses a sigmoid it says it
just uses a sigmoid as a model for
probabilities and if you look at how as
VM probably the outputs are made it's
the same thing it's drop the difference
is that for svms the training is in two
stages you fit the SVM then you fit a
sigmoid logistic regressions it's
everything together at once that it
outputs something like probabilities
yeah yeah it is it is no no you're
you're you are right it just uses a
sigmoid as probability round and as umg
Sigma is also but you're absolutely
right okay so just to conclude this
first part not going to present
constitutive results it's all in the in
this paper five minutes okay good so the
conclusion is that ASM is an efficient
model of actions with flexible sequence
of key semantic sub actions so these
actions what was interesting is that
they are semantically meaningful but
we're not sure they will be
discriminative and actually they are so
that's interesting and it leads us and
it gives us also like meanings to
interpret basically decompositions also
we saw that this sliding central frame
approach is a principled way to the
multiscale detection using this prior
and temporal structure and yeah and our
experiments we show it outperforms a bag
of features the rigid temple structured
extensions of the bag of features and
state-of-the-art methods including those
using tracking we don't use tracker here
but we could use it to boost performance
ok so this second part is about
comparing the dynamics of actions so
here the problem is different it's more
like the traditional classification
problem so we have clips of short
duration and we are asks the question is
there this action is like our people
falling in there yes or no ok so it's
classification and detection anymore
here and what
we want to do here is to define a good
similarity are good colonel between
videos that can compare videos right and
so far the state of the Arts is as bag
of features and for videos you can view
back the features as computing per frame
bag of features and then averaging them
okay so basically right now what people
do and what works best on this kind of
realistic data is to compare histograms
over averaging over frames and using
standard kernels on histograms like
intersection kernel or whatever and hear
what you want to do is say okay this
works really well this captures a lot of
information we don't wanna make silver
bullets that's going to replace this but
we want to make something complimentary
to it okay and one thing that is not
captured by bag of features and is
averaging statistics it's going to first
order moment statistics which are just
how frames relates to each other through
time so these temporal dependencies and
so how we propose to do that we propose
to model actions as time series of
frames okay and what we're going to do
and what I'm going to show now is a time
series kernel that can compare the
dynamics of to time series okay and the
core part of this dis kernel is auto
correlations so each time series has its
dynamics model by its auto correlations
so 11.2 understand is that cross
correlation is something that's used a
lot you know all these template matching
methods with cross correlation there are
similar things for videos but it's
different right they are cross
correlating their computing an alignment
score between two sequences here we're
using auto correlation per time series
and we comparing two auto correlations
okay so modeling actions as time series
is also something quite well known in
speech just a recognition but here in
general it's more like dynamic prolific
models like a hidden Markov models or
things like that so we're differing from
that and we're all so different from
kernels on time series which for
instance are based on dynamic time
warping so they compute alignments we
compare dynamics and don't compute and
then compare alignments okay so just a
quick slide about how we represent our
videos so i said actions as time series
of frames we extract
the feature Scalia mentioned is dance
track let's dance short point tracks
from dense optical flow compute motion
boundary histograms and what we do is we
counts visual words associated to to
those track lights for each frame so we
build a histogram yes what did you mean
by first order that yes I will I'll show
you actually I will in the definition
about correlation this will be clear but
it just means yeah the dependencies like
the covariance if you want some kind of
coherence it's in skates Auto currents
okay so basically you just have sorry
for the many basically you have a time
series of bag of features each frame is
represented as a bag of features but
doesn't really matter you could use the
awesome pulse estimation we've seen
before and have a tree representing
every frame what I will show you only
needs one thing it needs a basic mean to
compare two frames so here as there are
instagrams we're using basic colonel on
instagrams of using intersection Colonel
but you could use anything you want ok
so we represent the dynamics of one
actions of one time series using its
autocorrelation operator ok what is the
autocorrelation operator it's the
normalized cross covariance between the
time series ok and a shifted version of
itself so it's the same time series but
just shifted by certain number of frames
this is the normalization term and this
is the auto covariance matrix so this is
where the first order terms appear ok
but they are dependent on the time okay
and so what does this thing measures so
I say operator it's basically it's a
matrix but it can be potentially of
infinite number of dimensions it depends
on your basic colonel ok so it's an
operator and this this operator or
matrix if you like measures the
statistical dependencies between frames
so one simple way now that we model the
dynamics of an time series with this
autocorrelation operator we can compare
to time series by comparing their Auto
correlations their respective auto
correlations so here we can use standard
Gaussian RBF kernel with base distance
that's called the deco distance for the
difference of autocorrelation operators
this is the natural distance basically
in the space where these operators
it's the hilbert smith norm just think
of it as an extension of the Frobenius
norm okay okay so what's one thing
that's interesting is that this kernel
it works for series with different
durations periodic or not okay so it
works in very general in many cases
however the problem is that's estimating
this distance okay as a computational
complexity that is cubic but in the
dimension of the frame feature space
okay so we have this kernel on base
frames to describe Colonel corresponds a
frame feature space okay that can be
infinite dimensional and this complex
the complexity of evaluating the
distance is cubic in the dimension of
this pace so basically it's intractable
we cannot compute it just to compare two
videos okay but there's a solution and
it's a classical application of the
kernel trick so basically we're going to
write a dual expression of this distance
and after rolling the computations what
you can see is that this distance can be
expressed as a trace of products of
various matrices okay I won't go into
the definition of these matrices but
what matters is that they can all be
expressed with this big k capital K
matrix which is the kernel matrix of the
evaluation of all pairs of frames ok so
it's also called the self similarity
matrices it's the between frame
similarity matrix and this evaluating
this distance now is still cubic don't
know it's cubic now but in the number of
frames ok and as we deal with short
actions like something like hundred
frames this takes on the order of
milliseconds okay in Python so it's fast
okay so I mentioned that this kernel can
be expressed in function of just one
matrix which is this K matrix this
matrix of similarity between frames and
here you can see that this relates to
something that is very well known an
action recognition but also in other
fields it's called the self similarity
matrices ok let's studied you know a
long time ago for periodic actions etc
and so here you have two actions that
are the same action so it's bits from
also its to rugby
ball kicking actions different actions
and here this big K matrix right so this
is the similarity of all frames from the
first action the similarity between all
frames of the second and these are the
cross similarities okay so here you have
frames and frames and what you can
observe it's pretty obvious I think is
that these two matrices they show
remarkably similar patterns right so
this is not something new this is
something that is again known for
instance even did work on this and for
instance what even did is saying okay
let's view these as two images you know
they are remarkably similar and to us as
humans so let's view them as images
let's take hot features on them and
compare hog features okay a computer
vision view of things okay it works
really well and here basically what we
show is that this is related to the
autocorrelation operator of the time
series okay so it provides some some you
know that mathematical background to the
self similarity matrices and here's four
different actions you see it's much less
structured okay so like I said at the
beginning our goal is not to make silver
bullets and overcome the Emperor bag of
features it's just to complement it okay
and so we want what we want to do is
combine our deco Colonel our dynamics
kernel with an averaging colonel so use
a bag of features colonel not exactly
bag of feature is called difference of
mean elements I didn't find it published
anywhere before it's just bag features
but in the frame feature space instead
of the input space so subtle difference
but it boost performance a little bit
and it can be expressed also just in
function of K so you have it basically
for the same move the same operation and
here you have like the accuracy in
percent on three action recognition
benchmark so simple one kth more
difficult to on ucf sports really
difficult one youtube and what we wanted
so it seems that in in all cases almost
averaging works best compared to our
dynamics but we're just comparing
actions based on how their frames relate
to each other right so it was expected
however also what was expected and
and what's good is that the combination
of these two kernels the averaging one
and our dependency Colonel improves
performance okay so we have also now
like a bit more experiments and a bit
better results than this since then okay
so to conclude the second part what I
showed here is that you can use auto
correlation to represent temporal
dependencies and this is a principled
way of extracting information from
self-similarity matrices okay we also
showed what we wanted at first that
dynamics are complementary to average
statistics and for future work so that
it's quite preliminary so there's lots
of stuff that you can do for instance
you view actions as time series of
frames this is a kind of a signal a
multivariate signal what are its
properties that are useful it's like for
instance smoothness important right now
with vago features it's absolutely not
smooth maybe using Fisher vectors would
improve also we have to make some
assumptions to make the computations
possible like most time series paper we
assume stationarity this is violated in
our case but maybe we can make a local
version of our kernel or Windows version
to account for this assumption also
there are like variety of parameters
that need to be investigated this was
published at last year's be MVC actually
if you want more details and just to
conclude this dispersant ations a few
take-home messages basically what i
showed is how to leverage temple
information for action recognition first
part was to learn a model as a sequence
of temporal parts the second part was to
compare these temporal dependencies and
what I want to to highlight is basically
the time is not space so i should add in
digital videos okay because in real
world maybe it's true in our brain etc
but in digital videos really time is not
space that's at least quantitatively
true I think everyone would agree the
difference between two neighboring
frames is much bigger than the
difference between two neighboring
pixels and I saw I my personal belief is
that it's also quantitatively true
because time is sequential by nature it
goes into one direction only and I
believe and that's what I've tried you
know too
do in my PhD is to use this distinction
in order to improve action recognition
and I think I've shown a few ways to
exploit this and also another take a
message is that when dealing with
real-world videos I haven't shown a lot
of videos because don't have a lot of
time but you can look on my website
there are lots of videos when you're
dealing with real-world videos you want
to make structured models in order to be
able to predict more complex activities
right so use more information but when
you use more information more structure
you make more assumptions necessarily
okay and the thing is that in real-world
videos these assumptions are likely to
be violated so whenever you have to make
a model and a big part of my work
especially on the first part was to make
the model robust to the violation of
this assumption so it's really crucial
crucial to account for this thank you
very much so is there is there a very
natural way of comparing these
autocorrelation parameters because
actually you're throwing parameters are
the sufficient statistics for learning
the autoregressive model but exactly so
what you could do is take the when
you're comparing two models you take the
whole family of models that you would
simulate under one yep set parameters
calculate their likelihood under the
other model and take an expectation yeah
now that would give you a because it
taking just like L to normal between
burn and Australia not convincing yeah
yeah yeah you're absolutely right this
makes a lot of sense and also related to
your question we've tried something else
that's not here is so this is related to
auto regressive so arma models but not
exactly the same we tried also fitting
arma models and comparing the models
also this seems to work equivalently but
it's very preliminary results so but we
are definitely working in this direction
it's very interesting thank you okay
thank you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>