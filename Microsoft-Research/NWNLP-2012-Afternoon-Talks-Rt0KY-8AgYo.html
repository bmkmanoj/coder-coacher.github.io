<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NW-NLP 2012 Afternoon Talks | Coder Coacher - Coaching Coders</title><meta content="NW-NLP 2012 Afternoon Talks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/Microsoft-Research/">Microsoft Research</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>NW-NLP 2012 Afternoon Talks</b></h2><h5 class="post__date">2016-08-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Rt0KY-8AgYo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">each year Microsoft Research hosts
hundreds of influential speakers from
around the world including leading
scientists renowned experts in
technology book authors and leading
academics and makes videos of these
lectures freely available
so my name's Emily primo I'm a graduate
student at center for spoken language
understanding at oregon health and
science university and i worked with
brian work and today i'll be talking
about graph-based alignment of
narratives for automated neurological
assessment um so to motivate the problem
a little bit and I'd like to point out
that neuropsychological exams often rely
on spoken responses is listed by
different types of stimuli for instance
you might be asked to name as many
vegetables as you can in a minute or you
might listen to a narrative and then
have to retell it and of course these
are usually part of normed instruments
so there are standardized scoring
procedures that are usually carried out
on the responses but you can also think
of the responses as a source of language
data that could be analyzed for other
kinds of diagnostic markers um so I'll
be talking about narratives today there
are different kinds of narratives that
can be elicited personal narratives you
can do narrative generation where you
narrate a picture book or a story from
from a series of pictures or cartoon or
a video clip or this is one of the most
common ones is that the subject listens
to a story and has to retell the story
to the examiner um and that's usually
scored in terms of how many story
elements were used so there's
information in the original story and
it's their graded on how well they
replicated that information um no
seniors with Alzheimer's dementia
produce fewer of these key story
elements in their narrative retellings
and so the question here is whether we
can use these narrative scores to detect
very early stages of dementia the
earliest stage of dementia that's
usually recognized is called mild
cognitive impairment and I'll talk a
little bit about that in detail shortly
so can we identify it can we identify
MCI using these narrative scores and
also can we do it automatically because
we're computer scientists and we like to
do things automatically so the goal is
to develop an objective automated tool
for mci for detecting MCI that relies on
scores derived from narrative retellings
and the way we're going to do this is to
think about retelling as a kind of
translation so imagine that the story
you hear it and you translate it into
your own language it's the same length
but it's your own special idiolect of
that language so I'm going to use
existing alignment word alignment tools
that are used for machine translation to
create machine translation style word
alignments between retellings and a
source narrative and then I'm going to
improve those alignments using a graph
based method and then I'm going to
extract scoring out information from the
alignments and then use those scores as
features for diagnostic classification
so a little overview of the data i
mentioned mild cognitive impairment um
it's characterized by impairments in
cognition that are significant but don't
interfere with your daily life so you
can still drive your car or balance your
checkbook and you know who your
grandchildren are and all of those
things but it is clinically significant
it's real it's happening your cognition
starting to decline but because it's so
subtle it's very hard to detect with
something like the mini-mental state
exam instruments like that that are
usually used to screen for dementia so
MCI is usually diagnosed with a long
interview by an expert between the
patient with the patient and also with a
someone who can corroborate what the
patient says like a spouse or other
family member and we're going to use the
clinical dementia rating which is a one
of the techniques for diagnosing
different levels of dementia and it is
one of these interview based techniques
typical aging is the clinical dementia
rating of 0 mci is a clinical dementia
rating of 0.5 this is how will interpret
it severe dementia would be like a
clinical dementia rating of 3 and I'd
like to point out the diagnosis does not
rely on any narrative recall test so the
narrative recall task will be talking
about it's completely independent of the
the process by which mci is diagnosed so
this is our data we have at ohsu we're
medical school we have something called
the latent Center for aging and
alzheimer's disease research and they
have this very long niña funded
longitudinal study on brain aging where
people come in and they do they give
them the mini-mental state exam they
have them do a bunch of activities and
tests and their interviews and during
this exam they're they're given the
interview for which you by which you can
detect diagnosed MCI and so we have 72
subjects with MCI and 163 subjects
without there are no significant
differences in
age or years of education and there's
also additional twenty six subjects who
have more advanced dementia or we're too
young to be eligible for this study or
had a diagnosis that changed back from
mci to typical aging and so we didn't
want to include them in either
diagnostic group since it's not clear
which one they're actually in the
narrative task will be talking about is
the wechsler logical memory subtests of
the wechsler memory scale the wechsler
memory skills have widely used
instrument used to assess memory in
adults it's been used for 70 years I
mean the story that will be talking
about has actually been used for 70
years as well so the subject the
examiner reads a story to the subject
and the subject has to retell that story
immediately and then after 20-minute
delay and the score is how many story
elements were used in each retelling and
the identities of the story elements are
noted but they're not reported they're
not used as part of the score for the
wechsler memory scale this is the the
narrative itself and you can see that
the slashes denote the boundaries
between elements so there are 25 story
elements this is a sample retelling and
the underlined items are the recalled
correctly recalled elements so you can
see that an is a correctly recalled
version of Anna but Taylor is not a
correctly recalled version of Thompson
and that's because this published
scoring guidelines give pretty explicit
directions about what are acceptable
lexical substitutions and what are not
what sorts of paraphrases would be
acceptable and this person gets a score
of 12 out of 25 so i mentioned that i'm
thinking of retelling as translation and
that I want to do word alignment so
instead of translating from German to
English I want to translate from the
story to the way someone rendered that
story and so the way this is we just go
of machine translation work typically as
you begin with this sentence a parallel
corpus of sentences where you have
senses in one language on one side of
the corpus and translations of those
sentences on the other side of the
corpus in a different language and the
idea is that you need to figure out
which words are translations of which
other words and the we do does through
word alignment and this could be easy it
could just be like you just
monotonically go through and line them
all up or it could be really complicated
where there's lots of word order
differences and things like that so you
can't just use lavender tiny distance or
something like that you have to do
something smarter this is usually done
using the IBM models which were
developed in the 90s when they first
started when they started getting
interested in machine translation again
after many years i'm going to use
expectation-maximization to figure out
which words align with one another and
there are two widely used word alignment
packages actually Giza is really widely
used berkeley liner is not that widely
used but i found that it actually gives
much more accurate alignments and it
also is the advantage of you can save
out a model and then use that model to
align other data that you didn't train
on which is not if it's possible Giza
it's not immediately obvious how you
would do it and it also saves out the
posterior probabilities for those
alignments and that is going to be
really important for the graph based
method i'll be discussing so i'm going
to use these alignments to extract
scores from narrative retellings which
i'll then use for diagnostic
classification so i said you needed a
parallel corpus and i have three
parallel corpora I've created just from
the retailing data that i have just
people retelling the story so the first
one is small the second one is a little
bit bigger and then third one is huge so
the first one is the source to retailing
corpus all I have on this side is the
source narrative and all I have on that
side are the retelling so this is about
five hundred or so lines long because
that's how many retellings we have
corpus do is a word identity corpus or
just saying the word cook should align
to the word cook and because we're doing
monolingual alignment this is a good
assumption to make if you see a word on
one side and you see the word on the
other side there is some high
probability that they're going to align
to each other the third corpus the huge
corpus is a retelling to retelling
corpus so every pair of retelling so
this is 500 square 250 thousand lines so
what I do is I'd use the Berkeley liner
to build two models the small models
built just on corpus one in corpus to
just the retailing to source in the word
identities the large models built on
corpus all three corpora the difference
in the it takes like two or three
minutes to build the first one it takes
like 12 to 24 hours to fill the second
one very very big difference in time
required to build these kinds of models
so I'm going to test both models on the
two retellings for each of the
experimental subjects so we can see how
well they align to one another
and then I'm going to use both models
because you can save out the models with
a Berkeley liner to line every retelling
to every other retelling because I'm
going to be using that in my graph based
model so these are the I'm just looking
at time okay so these are the results of
the alignment the precision recall an
alignment error rate alignment error
rate is like word error rate it's sort
of a measure of the precision and recall
of how many alignment pairs were found
in the proposed alignment that existed
in the gold manual alignment and you can
see that the aee are you get a very
large reduction in a ER as you move from
the small model to the big model almost
four points which is very very large
improvement for word alignment but those
are still pretty high state of the art
on the Euro paarl corpus with the
berkeley liner is like four and we're at
20 so we can do a lot better so the idea
we had was to use a graph based method
that uses random walks on graph so you
probably all know about page rank
Google's way of ranking web pages and it
has to do with you you build this graph
in the nodes our web pages and the edges
of the hyperlinks between those web
pages and if you were to just walk
around on the graph you created like
that this the nodes that you end up on
would be the more prestigious nodes the
more important nodes Lex rank is a way
of ranking sentences for automatic
summarization so forward alignment the
nodes are words and the retellings and
the source and the edges are the
normalized posterior weighted alignments
proposed by the berkeley liners we know
the alignments and what their posterior
probabilities were so imagine if you had
these four sentences the source is on
the bottom these are sentences from our
subjects and you can see that the
bold-faced words should all be aligning
to touched okay but suppose they didn't
suppose that when you did your alignment
of just the source retelling you only
got moved aligning to touch and sorry
along to touch so then you align every
other retelling every retelling to every
other retailing and you uncover the
relationship that moved aligns with
sympathetic and sorry aligns with
sympathetic now you want to be able to
get from sympathetic to touched and you
couldn't do that in your original
alignment once you build this graph and
start walking around on it you can start
a sympathetic and be like okay I'm and
go to moved and then from there you
go to touched okay so the idea is that
it creates a connection between words
that maybe were unaligned in your
original alignment that you can now
uncover by virtue of the relationships
that word has with other words in other
retellings so we build a graph using the
alignments and posteriors generated by
the berkeley aligner and this is the way
the graph works the walk works you
started a node which is a retelling word
with some probability you move to a word
in another retelling and with some other
probability you walk to a source word
and you break and you do this a thousand
times for every retelling word and the
destination source word that you end up
at the most often is your new alignment
so you have a distribution after the end
of thousand walks of which retelling
word it was a source word you ended up
on you pick the most frequent one that's
your new alignment and you can tune the
value of this lambda on those 26 in
eligible participants they can serve as
a dev set for this and so I do this for
the alignments from both the small model
in the large model and we can see here
that if you take the small model and
then apply the graph based method to the
alignments proposed by it you actually
get over a four point reduction so it's
a large reduction then you get just by
moving to the large model and keep in
mind that the large model takes 12 to 24
hours this graph based model takes three
minutes tops it's very fast so you're
getting the same benefit with um you
know very requiring very very few
computational resources we also see that
the graph based models both outperform
both of their correspondingly size
models as well so this bodes very well
for using graph based models so now I
want to extract scores from those
alignments and I'll explain how I do it
this is the narrative the elements are
labeled with letters of the alphabet the
25 elements and this is a word alignment
that the Berkeley liner proposed so what
we do is we look at the word from the
retelling we got rid of the function
words because who cares about them and
we look at a word in retelling and it
aligns to a source word and we see where
does that source word appear in the
original narrative it appears in element
a so this person got lmna Taylor aligns
to Thompson where does Thompson appear
an element be worked employed on the e
and so on so that's how we get this
score so we know for every element did
they get it or not I'm so I evaluated
the scores that you can extract from
this and actually under all the models
the F measure is very very high in fact
i'm giving you the Cohens kappa this
that's a measure of inter annotator
agreement this is actually within the
range of human interest ask so this is a
computer performing as well as a person
on the task of scoring this test in
addition we see that the models with
lower lyman error rate produce the
higher scoring measures which we're glad
to know so now we want to use these
scores for diagnostic classification so
what we do is we extract spores from
each subjects to retellings then we use
a support vector machine to classify the
subjects we have to feature sets one
feature set is just the summary scores
so for each retelling 0 to 25 how many
elements did they get this is the score
that's reported as part of the wechsler
memory scale the second feature set is
the 50 element scores so for each
retelling there are 25 elements so we
create a vector of 50 scores each one
being 0 or 1 depending on whether or not
they've got that element correct and
then we're going to evaluate this in
terms of area under the receiver
operating characteristic curve using
leave pair out validation and so with a
you see point five as chance and one is
perfect so anything greater than point
five is good and the closer it is to one
the better it is so we can see here that
the summary score feature know the
summary scores what's reported normally
is it does pretty well it gets close to
its about point 75 for most of them but
actually the elements core features are
much much better and they're getting
very high classification accuracy and
another thing to note is that the
clinical dementia rating has a
reliability at about that level so you
could sort of say that this technique is
working as well as humans are at
actually distinguishing mild cognitive
impairment and again we see the large
models and the graph based models are
stronger than the ones that are small
and not graph-based so can I finish okay
it's a stop as like all right well you
know you're on your own for the
submarine people
so the methods outlined here show
potential is a screening tool for
neurological disorders because it's not
just this test that's why we use there's
another test called the Nipsey narrative
memory that's used with kids there are
picture description tasks things like
that so be easy to adapt this to other
scenarios and the other thing that was
good was that these graph based methods
yielded large alignment error rate
improvements it went requiring the
extensive computational resources that
scaling up to a really large model would
so other improvements in the graph based
model I right now it's just one to one
alignment because I just picked the
single highest a most frequent
destination node over the distribution
but there are plenty of one too many
alignments in our data so that would be
something I'd like to look into um those
would like to look into using undirected
links like having an or allowing links
out of the source word I feel like you
could be exploring the graph a little
bit more than I am I'd like to apply the
technique to other tasks as I've said
the nefse narrative memory task and the
cookie theft picture description this is
we look at a picture of a kid stealing
cookies from the cookie jar and have to
describe it I'm working right now
incorporating speech recognition into
the pipeline and with mitre who's going
to be talking about something similar in
her poster and I also want to try to
apply the graph based method to multi
lingual word alignment there's a you
know euro parley's huge it has lots and
lots of languages seems like something
like this might be able to be used to
improve word alignment in some way
that's all
hi
hello hi sorry um so what is the
accuracy of the alignments if you just
map same word the same word um it's like
40 maybe 30 or 40 so it should the idea
the thing is like it should be good
because the probability that a word
aligns to that identical word is quite
high it's like sixty percent but the
problem is that there are multiple
instances of words so you have to decide
which word you're going to align it to
and I think that's where the in accuracy
comes from um you talked about kids were
you talking about specifically about the
wechsler test for kids are another test
for kids and what is the neurological
impairment something it's not
domestically very very early onset
dementia it's a really terrible problem
um so the test specifically I was
talking about was the nefse narrative
memory test so the Nipsey is like a huge
battery of things that test not just
memory but language skills and executive
function and all of those different
things and actually I've already applied
this these methods to that and the
results they're not quite as compelling
but we don't have as large a dataset but
they're pretty good but also the if
you're familiar with autism at all
there's an instrument called the autism
diagnostic observation schedule that's
used to it's like a set of series series
of semi structured activities one of
them is called a wordless picture book
and the kid and the examiner together
narrate a picture book and this is
something this is a technique that could
probably be applied to something like
that as well in the neurological
impairments we're interested in our
autism and language impairment actually
the nefse narrative memory is very tied
to language impairment performance on
that
hi um one more question so so were you
doing the alignment on the entire like
the in so it wasn't like sentence based
and then match so wasn't sentence based
it was the full retelling to the fall
source narrative which is weird like
that's not what you would do a machine
translation machine translation you
would have it have sentences aligned but
because we don't know in advance which
parts of the story the kids are going to
the adults are going to remember you
can't do it you'd have to do a sentence
alignment first and the different story
elements might appear in one sentence
that appeared in two different sentences
and the source so we just put it all
together well I just mean it because
from the point of view of a source of
error like the IBM models are like kind
of designed to work on sentences mm-hmm
and kind of imagining that you're doing
an alignment on much much longer
dependencies yes so that is certain that
is almost certainly why I'm one of the
many reasons why the alignment error
rates are much higher than they would be
for machine translation thank you very
much hello hello everyone my name is
Congo and this work is done together
with Raphael Hoffman and the damn well
then is my advisor and very glad to come
here to talk with you about this work so
as you may know relation extraction is a
very important task in natural language
processing and artificial intelligence
hope this simple example can help you
understand go of relation extraction
suppose we have some raw sentences like
our captain-general jenkins is a
phenomenal athlete said the Gators coach
Urban Meyer so after human being read
ascenders he can get some interesting
facts like the general Jenkins praising
a football team like data an urban meyer
Colchester football team beta so the
Prussian years can machines do the same
thing as human beings can get these
facts and they put in put it into the
machines so formulae a racial
instruction is a problem that we have an
ontology with set of relations with type
signatures for example an athlete is
praying for a team we have we have
signatures se10 team we have a relation
praise for team so our goal is to our we
are interesting the facts of this
relation so we want an extractor to the
input of this extractor is some law
sentences the output of the uke
structure is some two posts satisfying
the relations you are interested in so
suppose the relation you extracted from
the raw sentence for the empo like
generally Jenkins and Urban Meyer in
this case is not existing in your
knowledge base you can just add this
relation at this instance back to your
knowledge base at this instance back to
your aunt told you and your ontology is
becoming a larger battery and more
useful so that's the goal of the
relation extraction and that's the first
step to build a better knowledge base to
use for other tasks okay at the first
glance you may think that we supervised
learning is the best way to to do this
task so so you hold attribute our
classify for the extractor what you need
is a lot of training sentences for each
shrinking sentences you need to figure
out the entity's English sentences and
their relationships for example like Ray
Allen and Doc Rivers they satisfy the
relationship culture by you you put it
you label it has a positive example and
the YouTube and Google you know that
Google acquire YouTube actually but
descendants like YouTube API in Google
Code say nothing about acquired
relationship so you have to label this
as a negative example okay soup for
learning is good but what's the problem
the problem is it cannot scale easily so
let's see example like the h20 2005
dataset it contains only 1500 articles
so the reason that supervised learning
is hard is not it the problem is that it
is not only hard but it's almost
impossible because the data is very the
positive data is very good so most
instances actually do not contain any
interesting relationships in your
ontology in your in your small set of
four relations so here is a data set so
the ratio of our positive sentences
actually is less than two percent so for
the top 50 relations in freebase which
means that if you ask a human labeler to
label your sentences they will meet one
positive sentence after 15 negative ones
so I don't think many people
have that patients to label an update
her fall for this task to avoid so
researchers want to avoid labeling this
very to avoid this kind of labeling
sentences stuff sells a proposed week
supervision to leverage the instances
the idea is that ok it's hard to get
labels sentences but it's easier to get
labeled relation instances for example
we know that's the gate i'm the Urban
Meyer they've satisfied the relation
team coach you know Google YouTube
satisfied duration acquired and it's
also very easy to get a list offer and
label sentences from whatever you you
can imagine the clever part of the week
supervision years that you Jen who
recently generate the the training
examples by matching the instances into
the unlabeled sentences in return all
sentences that contain this pair of
entities as the training as a training
example for your classifier for your
extractors of course it brings it some
noises to the geology of joy extractor
but since the number of a label sentence
is so huge that you can do a lot of
machine learning stuff on this kind of
interesting data okay life is so good
until we ask the question so what if the
training instances here is also small
because the previous work during the
week supervision they try to avoid this
problem by looking only looking at the
relation that's already existing in the
in the database which means that it can
get large amount of training instances
for free almost so but what if you want
to define a relation by yourself for
some bio task for some for some whatever
you want for some question answering the
relation may be very specific you only
have some examples for that you don't
know which database to look at you don't
know where to look at so we want to
solve this problem so our motivation is
that there are some very large
background on ecology and web you can
leverage this background on project that
contains like millions entities and the
thousands of thousands of relations so
what if we can build a connection
between our target and Roger our target
relations to this background real
ontology and that they are all very
likely that among these millions
entities a lot of entity peers set is
by my target relations if I can dig them
out use them as the training instances I
can significantly increase the number of
training instances for my task and I can
do week supervision very well so that's
the idea we called ontological smoothing
so the goal is that I have a relation I
want new training instances i generated
a training instance is wrong the
background ontology the method is that
we do an ontology mapping from my target
relation to the background knowledge
base to the background ontology ok here
is the overview of our system we call it
velvet the first step is to build a map
I built a mapping from the background
told you to the target on taja the
second step is to generate new training
instances and the training sentences
from the from the mapping and from the
unlabeled sentences and the third step
is to Train relation extractors with
these with these actual resist new data
ok so so what's the challenge to do this
job there are two major challenges for
our on logical schmoozing idea the first
challenging year is that so there might
not be some explicit mappings you may
ask a question can i use a very naive
way to just pick an automatic relation
in the background knowledge and return
all is instances as my s what you need
the problem is that's usually good
mappings is not explicit in your bearing
or background knowledge you need some
database operator like join like you
need like selections to to get what you
want here is a simple example for
example in your target relation you have
in your targets you have a relation
coached by it has an example Kobe Bryant
and Mike Braun so in your background
knowledge there is no direct connection
between kobe bryant and my bro what you
know is that code bryant is price for
the team le lakes and the coach of this
team is my brawn so for human beings we
can either in see that if we i joined
the price for team and team coach i can
get a lot of tuples if i take the first
argument and the third argument of these
tuples i get a lot of good training
instances for the relation coached by
that that's what i want but how came i
shouldn't do that so another beside join
union is also important because the same
relation can be spirited into different
domains in your background knowledge and
they may even have different names you
can very hard to get their relationship
at the first class for example like the
hack couch so team coaches are named as
a head coaching basketball domain and
manager in baseball domain what we want
is to put them together in our system so
so that so the output over the our
system is a view a database filled with
the operator of a joint union selection
and abilities I build mappings from the
target relation to a view over the
background knowledge base the second
challenge is that we do need to put us
do need to do the entity type and
duration mapping as a whole jointly do
the mapping the reason is that entity is
very ambiguous in the background
knowledge for example you can see that
they are remaining by browning your
background knowledge some is Basque a
one is basketball coach one is football
player another is a politician so so
without context how could you know which
one is the guy you are talking about in
your target relation but under the
another condition that the coach bar
relation is mapped from praise for team
join team coach you have big confidence
that the basketball coach my brownie is
the guy you are looking for okay so we
can handle these these difficulties
these challenges in our system bye-bye
to step by break down into two step the
first step is to generate the mapping
candidates and that the second step is
to choose these mapping candidates by
the joint inference so we have before we
first put the background knowledge into
a graph with each node stand for each
notice that entity in the background
knowledge and each edges that is a
relation in the in the knowledge so we
look for the instance periods in our in
our in our toggle relation we return the
past between them as the as the relation
back in candidates and we return the
type of these nodes as the type mapping
candidates such random of master user
sometimes has some problem is noisy you
can see that the microscopy is born in
USA and my nationality is also USA this
is a parse between the two arguments and
you will return this noisy parts as a
candidate so we need to specify the
likelihood
to these mapping candidates in order to
get a correct ones so what we do is to
use the mark of logical network model
which is good to do the joint inference
with with the evidence writing universal
logic the probability of the event in
mark of logic is written by the number
of satisfied rules times that are their
weight and in our work we have three
kinds of a predicate 14 entity mappings
414 relation mappings and another for
the type mappings we write these
predicates together in the first order
logic rules actually there are features
for our observations and with the
conductor NM AP inferences to get to get
the predicates to get the truth value
for the practice which means that the
true that the progress is true which
means the mapping is good okay to do the
maybe inference we simply cough caused
the problem into an integer linear
programming and then do LP relaxation
and randomized surroundings to get the
result which is quite standard method to
in the textbook so you can check it ok
so here we are what where we are is that
we have the employed ontological mapping
we have a lot of new training instances
and training examples what we need is
our relation extractor a week supervised
the matter to do the relation extraction
we use multi are in this you know this
project it is developed by Rafael hop
banking our group so it is in might be
to my best knowledge it might be the
only out of shelf relational structure
you can find now and it skills very well
to like millions examples dataset so is
a useful tools ok so here's our
experiment we compare our system velvet
against three bass lines by taking the
joint inference away by taking the
complex mapping away by taking the
smooth instance away from our system we
do the experiment on to target ontology
nail and I see there are 43 relations I
see there are nine relations we use the
freebase which contains like 100 million
facts as our background ontology so we
drew the experimental a label data sets
a label sentences offer New York Times
which
also millions articles and like 50
millions sentences here is our
performance so you can see that without
smoothing it's not surprising that
without smoothing performs very bad
because the system is trained only by
like dozens training examples by putting
without by putting complex mapping and
oncological smoothing into the system
the performance is improved
significantly and velvety is the best of
all so the last site shows the figures
average by instances we can also average
by relations this result the real thing
the last slide maybe a little too
optimistic because big relations is
easier to do than the small relations
this is a average by relations we can
also see that village is much better
than the baselines we also compared our
system our master to the current state
of our supervised X Russians on the
canal zero fourth it has that we use to
state of our soup revised approaches as
the as a comparison and the wii but we
do the experiment our system use very
few training instances only ten grand
instances poor relation yes no sentence
annotation is surprising like that we
can achieve comparable real out to the
state of state of our supervised the
method it's because we use large number
of unlabeled data set so they are
trained by like Stalin data set but the
silent example sentences we are using
like millions labeled sentences that's
why we can get comparable performance ok
we also evaluate the performance out of
ontology mapping itself we manually
label some mapping results of nail and
we achieve like 88 accuracy on relation
extraction and 93 accuracy on engine
mapping so with the with the result is
like five percent better than the
baseline which is freebase internal
search api to get entity mapping which
is about 80 h ok so so our system so we
notice that the previous week
supervision doesn't scale very well
because if you have very few training
instances so our solution is to use
background knowledge background ontology
to to generate antoje mapping to bring
you some a large number of a new
instance
it can enhance relation extraction
performance very well well here are some
future work for example we we are
planning to bring more multiple
oncologist to the mapping to multiple
charges we are also interested in the
river so not only binary relation but
also entering relations for weeks two
provisions there are also a lot of space
to improve like the data is very good
and there are and the future is like the
future is very extremely high dimension
so it makes sometimes makes the
performance quite quite hard quite hard
to improve so there are some future
works to improve various we have some
time for questions
so I wanted to ask you a question about
enery relations and then you put it as a
future work yeah and but I was also
thinking do you really need energy
relations because maybe you can do
everything with binary relations so
where do you see the the benefit of
dealing with energy relations so I think
the generalization is that you sometimes
so you can break the things you can
break the anti radiation into several by
narration and the zio some major and
major entity that's the simple way to do
the annular relation but I'm not sure if
it's is good for all situations maybe
there are some cases it will not work
very well for example if a lot of argue
if many arguments are I like the date or
numbers I don't think it will work
because i'm not sure if virtual work
because these arguments are related to
each other so if you treat each of them
as a binary there may not be many
linguistic evidence to to get them I
don't know yet occurs
I might have missed this but did you
evaluate the precision of the the
relations you're generating from the
smoothing are they sort of a hundred
percent correct so right so you have
some seed relations then you get more
from or some seed examples and then you
get more from freebase or those like
sort of a hundred percent what you mean
so so what's the position of the
examples oh I didn't label that yeah
this is a good question so we only label
that's why by using them and to do train
extractor what's the perform but I
didn't sample the instances and to see
what its performance is a good question
makes up we do that
all right then thank you very much we'll
move on to our last oral presentation I
next Whitney ok so I'm max and this is
work with my supervisor anup Sircar and
the topic of our paper is bootstrapping
which is the case of semi-supervised
learning where there's a single domain
and there's a small amount of labeled
data or seed rules yeah ok oh it should
be ok ok so bootstrapping and in
particular we're looking at the arouse
key algorithm which is a simple self
training algorithm with decision lists
so we start with some seed decision list
we label the data and we trade a new
decision list and repeat and a decision
list looks like this the running example
here is word sense disambiguation on the
word sentence so one sense is a piece of
text then once NC is a punishment so
here we have to see drools a Google is a
score and a feature and a label a sense
and the decision list works just by
choosing the highest ranked rule which
have the feature matching the example so
here when we apply the seed rules to
data the first two examples in the data
can be labeled because they have a
feature that Matthew the rule and the
third one can't be because it has no
feature that matches so that's the first
two steps and in the third step we train
a new decision list and the scores are
coming from statistics over the
currently labeled data so it's basically
co-occurrence which the previous rules
and you can see that now we have more
rules we can label more of the data but
we have a threshold we only take good
enough rules based on the score so we're
never guaranteed to be able to label all
the data just whatever we have features
for
and we repeat at the end of all of this
will drop the threshold make a decision
list with no threshold and then we can
leave all the data so we do that for
testing this is sort of similar to e m
or hardy em at least but the difference
is that we're training this decision
lyst model which contains type level
information whereas II am is a you have
expected counts over the actual
instances the token level so to examine
the behavior of this in more detail
we're going to continue with the same
example now the current decision list is
on that side and we're just showing the
two sensors with the colors and the
currently labeled training data is going
to be down there also labeled by color
so in this graph the left bar is going
to represent the decision list and the
right bar is going to represent the
currently label training data and up
here we're going to show accuracy that
tests the current test accuracy with the
decision list so as we proceed the
decision list that the left bar is
growing very rapidly and we've now
labeled I believe that's all of the
tests with the training data and you can
see that both are kind of skewed towards
the blue label the which ever since that
is and now that we've labeled all the
data the accuracy kind of plateaus and
we converge there about sixty percent
accuracy so the next variation is your
house key cautious which is some cones
and singer I didn't say that we're using
cones and singer for our particular
specification of your house key on your
house key cautious and here for cautious
when we make a decision list we take the
top five rules by score and then at each
iteration we take five more the decision
list is now going to grow linearly and
you can see that whereas in the previous
example the axis went up to 1,800 now
it's only up to 400 because we're
controlling the decision list so as we
run it the decision list that the left
bar is growing linearly and it's
balanced between the two features now
and the coverage and the
accuracy are going to grow more
gradually and we converge at a higher
accuracy so now this is our non cautious
and this is our cautious both will
change when we do a retraining step and
we dropped threshold and we drop
consciousness and train big decision
lists but usually we see this behavior
that cautious a voyage getting stuck at
low accuracy so the reason we interest
in your mouse key is that it seems to do
pretty well the the first four here are
our only implementation of common singer
and the last is a version from Adam E
which we'll talk about a little bit
later and you can see here that the code
training algorithm and your mouse key
cautious are the best of that of that
set and condon singer also have an
algorithm called Co boosting which does
comparable in their results we haven't
tested it and you can also see the
cautious algorithms you ask me cautious
and co training which is something
similar or the only ones that get up to
about ninety percent so precaution is
important ah and the reason we're
interest in your mouse key over the
other algorithms is that code trainee
and also Co boosting require two views
and the views are supposed to be
independent so it's nice to be able to
drop that limitation and just use self
training that was the upside the
downside is we don't have very good
theoretical analysis for your ski
there's no proven bounds abney in this
paper address that but only for certain
variants and those variants are not
cautious and he doesn't give empirical
results we don't think that they would
do as well without cautiousness added
Afari it's the car extend this analysis
and they use a bipartite graph
representation which we'll see you in
just a minute and we do have empirical
results but it's not cautious and
therefore does not do as well as what we
just saw
so here's the analysis we're going to
look at two types of distributions this
is the parameters so it's a distribution
for each feature over the labels and
that's the labeling distribution with a
distribution for each example over the
possible labels so the labels be in the
sanctions in the example we saw so the
labeling distribution is uniform of an
example is currently unlabeled and
otherwise it's a point distribution list
mouse on the label and the parameters
are just the decision les scores except
that we've normalized them to be a
distribution so a decision list will
make this choice just taking the maximum
scoring feature from a example and avni
introduces this alternate form where we
take a sum instead it's not quite a
decision list but it is easier to
analyze okay so switching topics
slightly Superman you at all have an
algorithm more recently which they use
on a part of there's the task I believe
it's part of speech tagging and it's a
domain adaptation thing so it's a
slightly different task and a slightly
different algorithms so we're not
concerned with the detailed if their
algorithm but a couple of interesting
properties and this is self training
with a CRF so you can see it's had the
same two steps we label data and train
but they've added these extra steps
wanted to get type level information and
want to do graphs propagation on top of
that so the two things we're interested
in here are first the overall structure
the adding of these steps and second the
particular graph propagation that use
so our own contributions for this paper
number one we have an object a u.s.
qvariant with a per iteration objective
which will see a minute and this
algorithm is cautious and we can show
that it performs well so unlike the
previous well analyze your house key
algorithms we show that it is that you
can do as well as your ASCII cautious
second we've unified all these different
approaches the different algorithms and
third more evidence that consciousness
is actually important so going back to
the graph propagation this is an
objective from afar in sikar for one of
that one of a bleeze your ass Calgary
times it's a upper bound on the
algorithm and this is the objective for
the graph propagation part of subramanya
it's not the objective for the whole
algorithm we just saw just for the graph
propagations yeah and you can see in the
first equation this is the labeling
distribution that's the parameters again
so if we compare these the first term of
each is going to be the distance if we
plug those distribution into this it's
going to be the distance between the
parameters the decision of scores and
the current labeling and the second term
is a slap or regularizer so they're
quite similar so if we do plug those
distributions into this then we can
dress the optimize that model the
bipartite graph model alternatively we
can not use that motivation for prefer
in sikar but we can just do graph
propagation over the the Thetas the
parameters where we take co-occurrence
in an example to be adjacency so it's
not as well motivated from that work but
it sort of corresponds to actually with
nanny at all do
so here's their own algorithm it's like
you're grouchy but it had this extra
step so first we trained a decision list
in exactly the way we saw with
cautiousness and second we do
propagation over that so we just take
the parameters and we make one of those
two grabs and we propagate that and
Superman you at all give iterative
updates to do that propagation so we can
either do it on the bipartite or the
unep are tight graphs that we saw or a
couple more but during the paper um if
we use the bipartite one we only take
the parameters at the end the decision
list part of the model and so because
we're doing this step we when we know
that at each iteration we're optimizing
the objective that we saw and we can do
cautiousness simply by copying the
decisions of this decision less the
original decision list so this one
determines which examples will label and
this one determines what the labels
actually are
so to look at the behavior of this
algorithm will do the same visualization
and so again the left bar is going to be
the decision list in the right the
labeled data so you can see that lichen
cautiousness we're forcing the decision
was to grow linearly and balance between
the two labels and now we're labeling
because of the propagation we can label
more examples sooner and the accuracy is
increasing sooner and comparing to what
we just saw for your LG cautious we're
actually doing a bit better in this case
again they'll both change when we do a
retraining step and make a bigger
decision list but the behavior is a bit
different we increase coverage and
accuracy quicker oh we can also look at
what happens to the objective this is
the objective from Superman you just to
go after application part and here we've
disabled cautiousness because
cautiousness by changing the input to
the graph it's changing the objective a
lot so without cautiousness on the
bipartite graph this is the objective
globally and it's decreasing and it kind
of levels off at the accuracy and the
coverage level off so we have two sets
of experiments the first is following
the running example so this is Yzerman
care costs is where extent this
ambiguous on data from the canadian hand
sides there are three words each of
which is and they give us in english but
not in french and each has two senses
and two seed rules the features are the
words adjacent to the word we're looking
at and some nearby context words and we
used Oh memorized and raw forms of each
so this group of algorithms is the ones
we've seen this is Safari answer cars
algorithm based on the bipartite graph
and that's a different kind of graph
propagation and you can see it doesn't
do as well and this is our algorithms so
in this case the cautious form of our
algorithms
this is the bipartite one and this is
the Unipart tight one and you can see
the unep are tight one is doing pretty
close to dl code training cautious it's
actually beating cautious and the
bipartite one is doing pretty well too I
the data here is a little bit strange
the data sizes are small the second task
is this named entity classification from
fountain singer and in this case we have
three labels person location
organization seven seed rules with some
for each label and the features are
spelling features which are from the
phrase we're classifying and context
features which are extracted from a
parse tree of the sentence so particular
words nearby in the tree and the
relative position in the tree and again
the algorithms we've seen this one and
our algorithms and again the theta only
the unit partite one is coming out quite
well and the bipartite one's not that
either well in this case our algorithm
is the top but we're not really trying
to show that algorithm is beating really
cautious of your code train here we're
just trying to show that it is coming
out equivalent in accuracy but as we
said it doesn't have the disadvantage of
requiring two views when I didn't say
we're reporting on seat adapter see here
so that means we take after see only
over the examples that the seed rules do
not label so the idea is to measure
improvement over the seed rules not what
we were given
and that's it so we have software online
if you want to say thank you sorry about
that wait for the microphone so we have
time for a few questions
hangry told I I wasn't able to figure
out if if the algorithm once you label
an example in later that examples to
escape from the labeling and go back to
the unlabeled yes yeah this the steps
are I mean I saw that the labels are
always increasing with yes but weary
label the entire data every generation
so example can become become unlabeled
it doesn't happen very often that it is
possible
can you elaborate on the problem of
requiring the two views so how is an in
practice what I don't know how bad it is
in practice but my understanding is that
the the theoretical requirements for
those algorithms are that the two views
are statistically independent and so you
only get the theoretical properties if
your fear features have that property
which is quite unlikely and so the idea
is that we can get the same performance
with a much simpler algorithm without
having to have that property I mean the
performance is on a specific data set
that um doesn't have those proper days i
guess well on the data sets we've seen
it's doing compared ibly to the code
trading algorithms so it's we don't know
that it's better but it's doing it
comparable and we we don't have to have
that the requirement on the videos many
different ways to split the features for
the word sense data and we beat the best
one but we had to do a search like you
know there was no natural so nearby and
far away is not necessarily a natural
split of features so it's better but the
machine learning technology you should
not have to think about it it you plug
it in and it should work so why do this
extra work if you don't have to so never
do Co training and always do your ski
okay then that's going to conclude our
second oral session</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>